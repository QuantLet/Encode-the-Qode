{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"JZal6ahJZQBU"},"outputs":[],"source":["#%pip install protobuf==3.20.1\n","%pip install transformers[torch]\n","%pip install -q sentencepiece\n","%pip install datasets==2.13.1\n","%pip install evaluate\n","%pip install rouge_score\n","#%pip install wandb\n","#%pip install git+https://github.com/huggingface/nlp.git@fix-bad-type-in-overflow-check"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O5Mlzgdzaliu"},"outputs":[],"source":["QPATH = \"Quantlet/4-qode2desc\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iaAT_m3NaEzp"},"outputs":[],"source":["import sys\n","IN_COLAB = 'google.colab' in sys.modules\n","\n","import os\n","if IN_COLAB:\n","  os.chdir(f'/content/drive/MyDrive/ColabNotebooks/IRTG/Encode_the_Qode/Encode-the-Qode/{QPATH}')\n","\n","sys.path.append('../src')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oRmp1O7SZgaI"},"outputs":[],"source":["import pickle\n","import json\n","import re\n","import sys\n","from IPython.display import display\n","\n","from tqdm import tqdm\n","import pandas as pd\n","import numpy as np\n","tqdm.pandas()\n","\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","import torch\n","from torch.utils.data import  DataLoader\n","from transformers import AutoTokenizer, AutoModelWithLMHead, SummarizationPipeline\n","from transformers import AdamW\n","from datasets import load_dataset\n","\n","from transformers import (\n","    AutoModelForSeq2SeqLM,\n","    AutoTokenizer,\n","    Seq2SeqTrainingArguments,\n","    Seq2SeqTrainer,\n","    DataCollatorForSeq2Seq\n",")\n","\n","import nltk\n","nltk.download('punkt')\n","import evaluate\n","\n","import importlib\n","import preprocessing_utils\n","importlib.reload(preprocessing_utils)\n","\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NmtbX1d3ZvBW"},"outputs":[],"source":["#with open('../../data/preprocessed/Quantlet/Parsed_Qs_with_code_25062023.pkl', 'rb') as file:\n","#  df = pickle.load(file)\n","\n","CLEAN_UP = True\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CbZX3Y3_q1pw"},"outputs":[],"source":["model_name = \"Salesforce/codet5-base-multi-sum\"\n","sample = 'test'\n","\n","# tokenization\n","encoder_max_length = 300\n","decoder_max_length = 100\n","RS = 42\n","LR = 5e-5\n","\n","EPOCHS = 4\n","TRAIN_BATCH = 4\n","EVAL_BATCH  = 4\n","\n","WARMUP_STEPS  = 500\n","WEIGHT_DECAY  = 0.1\n","LOGGING_STEPS = 100\n","SAVE_TOTAL_LIM = 3\n","\n","LABEL_SMOOTHING  = 0.1\n","PREDICT_GENERATE = True\n","\n","\n","EVAL_COLUMNS = ['eval_loss',\n","           'eval_rouge1',\n","           'eval_rouge2',\n","           'eval_rougeL',\n","           'eval_rougeLsum',\n","           'eval_bleu',\n","           'eval_gen_len']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h_VA_dBRZ3rj"},"outputs":[],"source":["model = AutoModelWithLMHead.from_pretrained(model_name)\n","tokenizer = AutoTokenizer.from_pretrained(model_name,\n","                                          skip_special_tokens=False)\n","model.to(device)\n","print(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3GVNBFl5vpdu"},"outputs":[],"source":["def batch_tokenize_preprocess(batch,\n","                              tokenizer,\n","                              max_source_length,\n","                              max_target_length):\n","\n","    source = batch[\"input_sequence\"]\n","    target = batch[\"output_sequence\"]\n","\n","    source_tokenized = tokenizer(\n","        source,\n","        padding=\"max_length\",\n","        truncation=True,\n","        max_length=max_source_length\n","    )\n","\n","    target_tokenized = tokenizer(\n","        target,\n","        padding=\"max_length\",\n","        truncation=True,\n","        max_length=max_target_length\n","    )\n","\n","    batch = {k: v for k, v in source_tokenized.items()}\n","\n","    # Ignore padding in the loss\n","\n","    batch[\"labels\"] = [\n","        [-100 if token == tokenizer.pad_token_id else token for token in l]\n","        for l in target_tokenized[\"input_ids\"]\n","    ]\n","\n","    return batch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D9Uaq7GN4-2Z"},"outputs":[],"source":["train_dataset = load_dataset(\"json\",\n","                             data_files=\"train_dataset_descr.json\",\n","                             field=\"data\",\n","                             data_dir=\"../../data/preprocessed/Quantlet/\")\n","\n","if sample!='validation':\n","  test_dataset = load_dataset(\"json\",\n","                            data_files=\"test_dataset_descr.json\",\n","                            field=\"data\",\n","                            data_dir=\"../../data/preprocessed/Quantlet/\")\n","else:\n","  test_dataset = load_dataset(\"json\",\n","                            data_files=\"val_dataset_json.json\",\n","                            field=\"data\",\n","                            data_dir=\"../../data/preprocessed/Quantlet/\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DDkF_-QV5Ycb"},"outputs":[],"source":["train_data_txt = train_dataset['train']\n","validation_data_txt = test_dataset['train']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ya5gJ5zyveay"},"outputs":[],"source":["validation_data = validation_data_txt.map(\n","    lambda batch: batch_tokenize_preprocess(\n","        batch, tokenizer, encoder_max_length, decoder_max_length\n","    ),\n","    batched=True,\n","    remove_columns=validation_data_txt.column_names,\n",")\n","\n","train_data = train_data_txt.map(\n","    lambda batch: batch_tokenize_preprocess(\n","        batch, tokenizer, encoder_max_length, decoder_max_length\n","    ),\n","    batch_size=8,\n","    batched=True,\n","    remove_columns=train_data_txt.column_names,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cy3q8oyu46q0"},"outputs":[],"source":["def postprocess_text(preds, labels):\n","\n","    preds = [pred.strip() for pred in preds]\n","    labels = [label.strip() for label in labels]\n","\n","    # rougeLSum expects newline after each sentence\n","    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n","    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n","\n","    return preds, labels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wz7TLntc69Mk"},"outputs":[],"source":["def compute_metrics(eval_preds, metrics_list=['rouge', 'bleu']):\n","\n","    preds, labels = eval_preds\n","\n","    if isinstance(preds, tuple):\n","        preds = preds[0]\n","\n","    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n","\n","    # Replace -100 in the labels as we can't decode them.\n","    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","\n","    # POST PROCESSING\n","    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n","\n","    results_dict = {}\n","    for m in metrics_list:\n","        metric = evaluate.load(m)\n","\n","        if m=='bleu':\n","            result = metric.compute(\n","              predictions=decoded_preds, references=decoded_labels\n","           )\n","        elif m=='rouge':\n","            result = metric.compute(\n","                predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n","            )\n","        result = {key: value for key, value in result.items() if key!='precisions'}\n","\n","        prediction_lens = [\n","            np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds\n","        ]\n","        result[\"gen_len\"] = np.mean(prediction_lens)\n","        result = {k: round(v, 4) for k, v in result.items()}\n","        results_dict.update(result)\n","    return results_dict"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lr3F9kub7A6E"},"outputs":[],"source":["training_args = Seq2SeqTrainingArguments(\n","    output_dir=\"results\",\n","    num_train_epochs=EPOCHS,\n","    do_train=True,\n","    do_eval=True,\n","    per_device_train_batch_size=TRAIN_BATCH,\n","    per_device_eval_batch_size=EVAL_BATCH,\n","    learning_rate=LR,\n","    warmup_steps=WARMUP_STEPS,\n","    weight_decay=WEIGHT_DECAY,\n","    label_smoothing_factor=LABEL_SMOOTHING,\n","    predict_with_generate=PREDICT_GENERATE,\n","    logging_dir=\"logs\",\n","    logging_steps=LOGGING_STEPS,\n","    save_total_limit=SAVE_TOTAL_LIM,\n","    report_to=None\n",")\n","\n","data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n","\n","trainer = Seq2SeqTrainer(\n","    model=model,\n","    args=training_args,\n","    data_collator=data_collator,\n","    train_dataset=train_data,\n","    eval_dataset=validation_data,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HrUfnGAe7FO5"},"outputs":[],"source":["results_zero_shot = trainer.evaluate()\n","\n","results_zero_shot_df = pd.DataFrame(data=results_zero_shot, index=[0])[EVAL_COLUMNS]\n","results_zero_shot_df.loc[0, :] = results_zero_shot_df.loc[0, :].apply(lambda x: round(x, 3))\n","display(results_zero_shot_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8aqAC3Dk7Hbt"},"outputs":[],"source":["trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dje2FJSSttjt"},"outputs":[],"source":["results_fine_tune = trainer.evaluate()\n","\n","results_fine_tune_df = pd.DataFrame(data=results_fine_tune, index=[0])[EVAL_COLUMNS]\n","\n","results_fine_tune_df.loc[0, :] = results_fine_tune_df.loc[0, :].apply(lambda x: round(x, 3))\n","\n","\n","display(results_fine_tune_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fvvY6h61X5_n"},"outputs":[],"source":["def generate_summary(test_samples, model):\n","    inputs = tokenizer(\n","        test_samples[\"input_sequence\"],\n","        padding=\"max_length\",\n","        truncation=True,\n","        max_length=encoder_max_length,\n","        return_tensors=\"pt\",\n","    )\n","    input_ids = inputs.input_ids.to(model.device)\n","    attention_mask = inputs.attention_mask.to(model.device)\n","    outputs = model.generate(input_ids, attention_mask=attention_mask)\n","    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n","    return outputs, output_str\n","\n","\n","model_before_tuning = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n","\n","test_samples = validation_data_txt.select(range(20))\n","\n","summaries_before_tuning = generate_summary(test_samples, model_before_tuning)[1]\n","summaries_after_tuning = generate_summary(test_samples, model)[1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"26ZtjQRAbh8C"},"outputs":[],"source":["for i, description in enumerate(test_samples[\"output_sequence\"]):\n","  print('_'*10)\n","  print(f'Original: {description}')\n","  print(f'Summary before Tuning: {summaries_before_tuning[i]}')\n","  print(f'Summary after Tuning: {summaries_after_tuning[i]}')\n","  print('_'*10)\n","  print('\\n')"]},{"cell_type":"code","source":[],"metadata":{"id":"8CQ7XcDJ4Bkw"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","private_outputs":true,"provenance":[],"gpuType":"A100","mount_file_id":"1T-Pg89lDUNwYNKSoHrP6J9DUoSxqwVA-","authorship_tag":"ABX9TyMzeH/HvGWkyUAeHE+p3MFV"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
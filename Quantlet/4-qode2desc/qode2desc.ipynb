{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JZal6ahJZQBU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "#%pip install protobuf==3.20.1\n",
    "%pip install transformers[torch]\n",
    "%pip install -q sentencepiece\n",
    "%pip install datasets==2.13.1\n",
    "%pip install evaluate\n",
    "%pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "O5Mlzgdzaliu",
    "tags": []
   },
   "outputs": [],
   "source": [
    "QPATH = \"Quantlet/4-qode2desc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "iaAT_m3NaEzp",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "import os\n",
    "\n",
    "if IN_COLAB:\n",
    "    os.chdir(\n",
    "        f\"/content/drive/MyDrive/ColabNotebooks/IRTG/Encode_the_Qode/Encode-the-Qode/{QPATH}\"\n",
    "    )\n",
    "else:\n",
    "    %load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "oRmp1O7SZgaI",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/RDC/zinovyee.hub/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import json\n",
    "import re\n",
    "import sys\n",
    "from IPython.display import display\n",
    "import datetime\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead, SummarizationPipeline\n",
    "from transformers import AdamW\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "import evaluate\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "kM604UgeBuMl",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/RDC/zinovyee.hub/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/RDC/zinovyee.hub/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'analysis_modules' from '/usr/net/zinovyee.hub/IRTG/MLSC/Encode-the-Qode/Quantlet/4-qode2desc/analysis_modules.py'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import analysis_modules\n",
    "\n",
    "importlib.reload(analysis_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "CbZX3Y3_q1pw",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeT5_domain_4_19500-20231017_42\n"
     ]
    }
   ],
   "source": [
    "# model_name = '../5-domain-pre-training/analysis_report_CodeT5-test-12-300-4-2023-09-26-v3/results/checkpoint-78656'\n",
    "# model_name = '../5-domain-pre-training/results/checkpoint-12290'\n",
    "model_name = \"CodeT5\"\n",
    "# model_name = '../4-qode2desc/no_bootstrap/analysis_report_CodeTrans_domain-checkpoint-12290-test-10-512-16-2023-10-07/results/checkpoint-2500'\n",
    "# model_name = '../5-domain-pre-training/arxiv_pretraining/results/checkpoint-12290'\n",
    "\n",
    "# model_name = \"../5-domain-pre-training/analysis_report_CodeTrans-12-300-15-4-202310145_1/results/checkpoint-58500\"\n",
    "#model_name = \"../5-domain-pre-training/analysis_report_CodeT5-4-300-15-4-202310145_1/results/checkpoint-19500\"\n",
    "\n",
    "\n",
    "DATE = \"20231017\"\n",
    "\n",
    "SAMPLE = \"test\"\n",
    "if SAMPLE == \"test\":\n",
    "    load_best_model_at_end = False\n",
    "else:\n",
    "    load_best_model_at_end = None\n",
    "\n",
    "# tokenization\n",
    "encoder_max_length = 512\n",
    "decoder_max_length = 150\n",
    "RS = 42\n",
    "LR = 5e-5\n",
    "\n",
    "EPOCHS = 10\n",
    "TRAIN_BATCH = 16\n",
    "EVAL_BATCH = 4\n",
    "\n",
    "WARMUP_STEPS = 500\n",
    "WEIGHT_DECAY = 0.1\n",
    "LOGGING_STEPS = 100\n",
    "SAVE_TOTAL_LIM = 1\n",
    "SAVE_STRATEGY = \"steps\"\n",
    "\n",
    "LABEL_SMOOTHING = 0.1\n",
    "PREDICT_GENERATE = True\n",
    "\n",
    "MODE = \"no_context\"\n",
    "\n",
    "EVAL_COLUMNS = [\n",
    "    \"eval_loss\",\n",
    "    \"eval_rouge1\",\n",
    "    \"eval_rouge2\",\n",
    "    \"eval_rougeL\",\n",
    "    \"eval_rougeLsum\",\n",
    "    \"eval_bleu\",\n",
    "    \"eval_gen_len\",\n",
    "]\n",
    "\n",
    "\n",
    "analysis_name = (\n",
    "    model_name\n",
    "    + \"-\"\n",
    "    + MODE\n",
    "    + \"-\"\n",
    "    + SAMPLE\n",
    "    + \"-\"\n",
    "    + str(EPOCHS)\n",
    "    + \"-\"\n",
    "    + str(encoder_max_length)\n",
    "    + \"-\"\n",
    "    + str(TRAIN_BATCH)\n",
    ")\n",
    "\n",
    "if len(model_name) > 15:\n",
    "    analysis_name = (\n",
    "        model_name.split(\"/\")[-1]\n",
    "        + \"-\"\n",
    "        + SAMPLE\n",
    "        + \"-\"\n",
    "        + str(EPOCHS)\n",
    "        + \"-\"\n",
    "        + str(encoder_max_length)\n",
    "        + \"-\"\n",
    "        + str(TRAIN_BATCH)\n",
    "    )\n",
    "\n",
    "analysis_name = \"CodeT5_domain_4_19500\"\n",
    "analysis_name = analysis_name + \"-\" + DATE + \"_\" + str(RS)\n",
    "\n",
    "print(analysis_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "FRHaw5S7X81D",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "9t6d_nrX_lyy",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeT5_domain_4_19500-20231017_42\n",
      "CodeT5_domain_4_19500-20231017_42\n",
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/RDC/zinovyee.hub/.local/lib/python3.9/site-packages/transformers/models/auto/modeling_auto.py:1322: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/RDC/zinovyee.hub/.cache/huggingface/datasets/json/default-5c480fa98444650e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f4144182b83480aa8416524577c5614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/RDC/zinovyee.hub/.cache/huggingface/datasets/json/default-b07033dede6aa2c1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15f05e8177b84ab4ad3b857e24a1959d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4336 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/492 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/RDC/zinovyee.hub/.local/lib/python3.9/site-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='124' max='62' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [62/62 22:39]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   eval_loss  eval_rouge1  eval_rouge2  eval_rougeL  eval_rougeLsum  \\\n",
      "0     10.056        0.143        0.038        0.122           0.126   \n",
      "\n",
      "   eval_bleu  eval_gen_len  \n",
      "0      0.001        13.264  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/RDC/zinovyee.hub/.local/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1360' max='1360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1360/1360 21:36, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>7.982900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>4.939100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>4.501300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>4.139000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.757900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.471800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>3.247200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>3.071400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.908600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.788800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>2.773200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.688400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>2.653300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   eval_loss  eval_rouge1  eval_rouge2  eval_rougeL  eval_rougeLsum  \\\n",
      "0      3.865        0.324        0.162        0.286           0.291   \n",
      "\n",
      "   eval_bleu  eval_gen_len  \n",
      "0      0.056        18.218  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/RDC/zinovyee.hub/.local/lib/python3.9/site-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________\n",
      "Original: Algorithm that identify the S&P500 firm tickers from news text. The algorithm only identify those tickers in the three most commonly seen cases. Detailed description refer to the paper\n",
      "Summary before Tuning: Symbols Extraction in Spx Tickers\n",
      "Summary after Tuning: This Quantlet is dedicated to gathering data on the input data from the SPX\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Inspect the derived variables from the News Network, Plots and Stats\n",
      "Summary before Tuning: Learning Attention Variable for SpX Artificial Networks\n",
      "Summary after Tuning: This quantlet includes two main code files. The first one named SPX_lead.\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Inspect the derived variables from the News Network, Plots and Stats\n",
      "Summary before Tuning: PlotKernelDensityEstimator: A new object for plotting a series of\n",
      "Summary after Tuning: Plotting Kernel Density Estimation for Epanonnikov Data\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Estimates and plots (yearly) empirical pricing kernels (EPK), risk neutral densities (RND) and physical densities (PD) of DAX 30 index return conditional on 20% (red curve), 40% (green curve) and 60% (blue curve) quantiles of volatility index VDAX-NEW (current year), and time to maturity 1 month. Local linear kernel regression is used for estimation of the conditional risk neutral density and local constant kernel regression is used for estimation of conditional physical density. Moreover, 95% confidence intervals have been calculated for PK, RND and PD conditioned by 40% quantile of VDAX-NEW and time to maturity 1 month. All results are shown on a continuously compounded 1-month period returns scale. \n",
      "Summary before Tuning: C_2012: Path to the CSV file with Data and\n",
      "Summary after Tuning: Estimates and plots (yearly) empirical pricing kernels (PD)\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Estimates and plots (yearly) empirical pricing kernels (EPK), risk neutral densities (RND) and physical densities (PD) of DAX 30 index return conditional on time to maturity 2 months and different levels of VDAX-NEW-Subindex 2 (20 equally spaced numbers from 5% to 95% quantile of VDAX-NEW-Subindex 2 in a given year). Local linear kernel regression is used for estimation of the conditional risk neutral density and local constant kernel regression is used for estimation of conditional physical density. EPK is obtained as a ratio of RND and PD. The panels on the right-hand side of figures depict EPK, RND, PD conditional on 20%, 50% and 80% quantiles of VDAX-NEW-Subindex 2 with 95% confidence intervals. Colors from red to blue correspond to increasing values of volatility within each interval. All results are shown on a continuously compounded 2-months period returns scale.\n",
      "Summary before Tuning: Time Series Dax VDax VDax2m:\n",
      "Summary after Tuning: Estimates and plots (yearly) empirical pricing kernels (EPK\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Conducts a statistical analysis, heteroskedasticity and normality checks on housing data provided by Sberbank\n",
      "Summary before Tuning: Examining and Normality: A Statistical Programming\n",
      "Summary after Tuning: Conducts a statistical analysis, heteroskedasticity and normal\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Descriptive statistical analysis of Moscow housing data provided by Sberbank\n",
      "Summary before Tuning: The Quantlet: A Data Frame for Summary, Descriptive\n",
      "Summary after Tuning: Includes functions to get an overview of summary statistics for the variables and analysis of correlations of\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Descriptive statistical analysis of Moscow housing data provided by Sberbank\n",
      "Summary before Tuning: Data-Clean: A New Dataset for Model Quantlet\n",
      "Summary after Tuning: We load the raw and messy datasets and run the models in quantlet. We load\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Contains the VIX index and the VIX future intraday analysis on the tail-event day of February 5, 2018. The next day, Credit Suisse lost 500m USD because of a structured product on the VIX. Market disruption can already be observed the day before.\n",
      "Summary before Tuning: VIXfutures: A Framework for VIX-based Mark\n",
      "Summary after Tuning: Analyse the VIX index and the VIX futures for the quote tick and the\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Contains the VIX index and the VIX future intraday analysis on the tail-event day of February 5, 2018. The next day, Credit Suisse lost 500m USD because of a structured product on the VIX. Market disruption can already be observed the day before.\n",
      "Summary before Tuning: restrict_to_start_end_date: Data Frame Restriction\n",
      "Summary after Tuning: restrict_to_start_end_date Select a subset of the data, based on\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Implements the statistics tests for the Efficient Market Hypothesis: Bartell Test, Ljung-Box Test, Variance Ratio Test, Von-Newmann Test.\n",
      "Summary before Tuning: Plotting Stock Price in a Jupyter notebook\n",
      "Summary after Tuning: This quantlet includes two main code files. The first one named CAL_Risk\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Implements the statistics tests for the Efficient Market Hypothesis: Bartell Test, Ljung-Box Test, Variance Ratio Test, Von-Newmann Test.\n",
      "Summary before Tuning: What is AAPL?\n",
      "Summary after Tuning: What is the Apple stock data?\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Plots time series from cryptocurrency database\n",
      "Summary before Tuning: Dshusd, ltcusd, xmrus\n",
      "Summary after Tuning: This Quantlet plots time series of all market market exchanges and their quantiles in\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Plots time series from cryptocurrency database\n",
      "Summary before Tuning: Plot Theme: A New Visualization Framework for Data-Table and\n",
      "Summary after Tuning: This Quantlet plots the time series of crypto_ts for the time series of B\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Split the entire dataset in two parts. the first one consists of all observations without the last 24 ones. It is used for the training process of LSTM with the optimal parameters. The second subset consists of the last 24 observations used for the prediction of the hourly returns in the next 24 hours. Once the predictions are generated for each of the coins, they are exported from python in order to be imported in MATLAB.\n",
      "Summary before Tuning: Predicting Hourly-Returns in Multi-Step LSTM\n",
      "Summary after Tuning: This Quantlet builds time series of predictions for the LSTM model for the D\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Define a parameter grid for the LSTM and choose the optimal set of parameters that leads to a minimal root-mean-squared error.\n",
      "Summary before Tuning: Training Predict Evaluate with EarlyStopping\n",
      "Summary after Tuning: Simulates and estimates Markov-switching skew t - t factor copulas with two\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Produces the estimation results using GeoCopula approach and saves it as RDate file.\n",
      "Summary before Tuning: A Dataset of Fivecountries and its Applications in the\n",
      "Summary after Tuning: Produces the plot of the copulae of the data for the fivecountries dataset\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: UNIT 3 and UNIT 4 of DEDA. Introduce webscraping and object-oriented programming\n",
      "Summary before Tuning: Web Scraping for the South China Morning\n",
      "Summary after Tuning: Web Scraping encompasses any method allowing for extracting data from websites.\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: UNIT 3 and UNIT 4 of DEDA. Introduce webscraping and object-oriented programming\n",
      "Summary before Tuning: Passport Applications by Fiscal Years\n",
      "Summary after Tuning: ['Demonstrate getting data from webpage API and scraping news information from\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: ['Demonstrate decision tree', 'Demonstrate applied SVM on NLP sentiment classification', 'Plot some activation functions for Neural Networks']\n",
      "Summary before Tuning: Training and Testing with Keras\n",
      "Summary after Tuning: Use Keras to perform a training and test suite of the same in order to run the\n",
      "__________\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(analysis_name)\n",
    "analysis_modules.scs_analyze(\n",
    "    analysis_name=analysis_name,\n",
    "    model_name=model_name,\n",
    "    train_data_path=f\"../../data/preprocessed/Quantlet/{DATE}/{MODE}/\",\n",
    "    train_data_name=f\"full_train_dataset_{DATE}_sample0.json\",\n",
    "    val_data_path=f\"../../data/preprocessed/Quantlet/{DATE}/{MODE}/\",\n",
    "    val_data_name=f\"test_dataset_{DATE}_sample0.json\",\n",
    "    encoder_max_length=encoder_max_length,\n",
    "    decoder_max_length=decoder_max_length,\n",
    "    random_state=RS,\n",
    "    eval_columns_list=EVAL_COLUMNS,\n",
    "    learning_rate=LR,\n",
    "    epochs=EPOCHS,\n",
    "    train_batch=TRAIN_BATCH,\n",
    "    eval_batch=EVAL_BATCH,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    logging_stes=LOGGING_STEPS,\n",
    "    save_total_lim=SAVE_TOTAL_LIM,\n",
    "    save_strategy=SAVE_STRATEGY,\n",
    "    label_smooting=LABEL_SMOOTHING,\n",
    "    predict_generate=PREDICT_GENERATE,\n",
    "    load_best_model_at_end=load_best_model_at_end,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "smrdGzzNyo5n",
    "tags": []
   },
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPZowVhs+ksZzaIPG80pu3+",
   "gpuType": "A100",
   "machine_shape": "hm",
   "mount_file_id": "119XCh3Q64Zw4MGlsZQuGrMLS78o5MeWS",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "encode_code",
   "language": "python",
   "name": "encode_code"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JZal6ahJZQBU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "#%pip install protobuf==3.20.1\n",
    "%pip install transformers[torch]\n",
    "%pip install -q sentencepiece\n",
    "%pip install datasets==2.13.1\n",
    "%pip install evaluate\n",
    "%pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "O5Mlzgdzaliu",
    "tags": []
   },
   "outputs": [],
   "source": [
    "QPATH = \"Quantlet/4-qode2desc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "iaAT_m3NaEzp",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "import os\n",
    "\n",
    "if IN_COLAB:\n",
    "    os.chdir(\n",
    "        f\"/content/drive/MyDrive/ColabNotebooks/IRTG/Encode_the_Qode/Encode-the-Qode/{QPATH}\"\n",
    "    )\n",
    "else:\n",
    "    %load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "oRmp1O7SZgaI",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/RDC/zinovyee.hub/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "import torch\n",
    "import torch, gc\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "kM604UgeBuMl",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/RDC/zinovyee.hub/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/RDC/zinovyee.hub/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'analysis_modules' from '/usr/net/zinovyee.hub/IRTG/MLSC/Encode-the-Qode/Quantlet/4-qode2desc/analysis_modules.py'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import analysis_modules\n",
    "\n",
    "importlib.reload(analysis_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_name(analysis_config):\n",
    "    name = analysis_config[\"model_name\"]\n",
    "    if \"checkpoint\" in name:\n",
    "        name = name.split(\"/\")[-1]\n",
    "    mode = analysis_config[\"MODE\"]\n",
    "    date = analysis_config[\"DATE\"]\n",
    "    if analysis_config[\"val_data_name\"].startswith(\"val\"):\n",
    "        sample = \"val\"\n",
    "    else:\n",
    "        sample = \"test\"\n",
    "    return f\"{name}_{mode}_{sample}_{date}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeT5_domain_test_20231022\n"
     ]
    }
   ],
   "source": [
    "analysis_config = {\n",
    "    \"DATE\": \"20231022\",\n",
    "    \"MODE\": \"domain\",\n",
    "    \"model_name\": \"CodeT5\",\n",
    "    \"encoder_max_length\": 512,\n",
    "    \"decoder_max_length\": 75,\n",
    "    \"random_state\": 42,\n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"epochs\": 4,\n",
    "    \"train_batch\": 16,\n",
    "    \"eval_batch\": 4,\n",
    "    \"warmup_steps\": 500,\n",
    "    \"weight_decay\": 0.1,\n",
    "    \"logging_stes\": 100,\n",
    "    \"save_total_lim\": 1,\n",
    "    \"save_strategy\": \"steps\",\n",
    "    \"label_smooting\": 0.1,\n",
    "    \"predict_generate\": True,\n",
    "    \"load_best_model_at_end\": False,\n",
    "    \"evaluation_strategy\": \"epoch\",\n",
    "}\n",
    "if analysis_config[\"MODE\"] == \"domain\":\n",
    "    analysis_config[\n",
    "        \"train_data_path\"\n",
    "    ] = f\"../../data/preprocessed/Quantlet/{analysis_config['DATE']}/no_context/\"\n",
    "else:\n",
    "    analysis_config[\n",
    "        \"train_data_path\"\n",
    "    ] = f\"../../data/preprocessed/Quantlet/{analysis_config['DATE']}/{analysis_config['MODE']}/\"\n",
    "\n",
    "analysis_config[\"train_data_name\"] = (\n",
    "    f\"full_train_dataset_{analysis_config['DATE']}_sample0.json\",\n",
    ")\n",
    "if analysis_config[\"MODE\"] == \"domain\":\n",
    "    analysis_config[\n",
    "        \"val_data_path\"\n",
    "    ] = f\"../../data/preprocessed/Quantlet/{analysis_config['DATE']}/no_context/\"\n",
    "else:\n",
    "    analysis_config[\n",
    "        \"val_data_path\"\n",
    "    ] = f\"../../data/preprocessed/Quantlet/{analysis_config['DATE']}/{analysis_config['MODE']}/\"\n",
    "analysis_config[\n",
    "    \"val_data_name\"\n",
    "] = f\"test_dataset_{analysis_config['DATE']}_sample0.json\"\n",
    "analysis_config[\"analysis_name\"] = create_name(analysis_config)\n",
    "print(analysis_config[\"analysis_name\"])\n",
    "\n",
    "if analysis_config[\"MODE\"] == \"domain\":\n",
    "    if analysis_config[\"model_name\"] == \"CodeT5\":\n",
    "        analysis_config[\n",
    "            \"model_name\"\n",
    "        ] = \"../../data/pretrained/analysis_report_CodeT5-test-12-300-4-2023-09-26-v2/results/checkpoint-88488\"\n",
    "    if analysis_config[\"model_name\"] == \"CodeTrans\":\n",
    "        analysis_config[\n",
    "            \"model_name\"\n",
    "        ] = \"../../data/pretrained/CodeTrans/results/checkpoint-12290\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "FRHaw5S7X81D",
    "tags": []
   },
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeT5_domain_test_20231022\n",
      "cuda\n",
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/RDC/zinovyee.hub/.cache/huggingface/datasets/json/default-7c5e6e4837b55da6/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "261ca949acc64b4ea138df2188c7e527",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/RDC/zinovyee.hub/.cache/huggingface/datasets/json/default-9aac8b7225e27cff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d2e4b846e3049aaa8541e3c75e378f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/RDC/zinovyee.hub/.cache/huggingface/datasets/json/default-7c5e6e4837b55da6/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-4ace09ff0c81cbaf.arrow\n",
      "Loading cached processed dataset at /home/RDC/zinovyee.hub/.cache/huggingface/datasets/json/default-9aac8b7225e27cff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-6b7d1c64f0adc5a9.arrow\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='246' max='123' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [123/123 03:59]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   eval_loss  eval_rouge1  eval_rouge2  eval_rougeL  eval_rougeLsum  \\\n",
      "0      9.529        0.123        0.026        0.107            0.11   \n",
      "\n",
      "   eval_bleu  eval_gen_len  \n",
      "0      0.001        13.967  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/RDC/zinovyee.hub/.local/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1084' max='1084' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1084/1084 15:03, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Brevity Penalty</th>\n",
       "      <th>Length Ratio</th>\n",
       "      <th>Translation Length</th>\n",
       "      <th>Reference Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6.083000</td>\n",
       "      <td>4.770382</td>\n",
       "      <td>0.266400</td>\n",
       "      <td>0.085100</td>\n",
       "      <td>0.226400</td>\n",
       "      <td>0.232500</td>\n",
       "      <td>17.689000</td>\n",
       "      <td>0.021500</td>\n",
       "      <td>0.313300</td>\n",
       "      <td>0.462900</td>\n",
       "      <td>5945</td>\n",
       "      <td>12844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.306300</td>\n",
       "      <td>4.241068</td>\n",
       "      <td>0.294700</td>\n",
       "      <td>0.110100</td>\n",
       "      <td>0.249300</td>\n",
       "      <td>0.256600</td>\n",
       "      <td>17.733700</td>\n",
       "      <td>0.035800</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.487900</td>\n",
       "      <td>6266</td>\n",
       "      <td>12844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.508500</td>\n",
       "      <td>4.039348</td>\n",
       "      <td>0.306500</td>\n",
       "      <td>0.136700</td>\n",
       "      <td>0.264300</td>\n",
       "      <td>0.269300</td>\n",
       "      <td>17.930900</td>\n",
       "      <td>0.049800</td>\n",
       "      <td>0.363000</td>\n",
       "      <td>0.496700</td>\n",
       "      <td>6379</td>\n",
       "      <td>12844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.131300</td>\n",
       "      <td>4.016418</td>\n",
       "      <td>0.310300</td>\n",
       "      <td>0.140800</td>\n",
       "      <td>0.267700</td>\n",
       "      <td>0.274000</td>\n",
       "      <td>17.987800</td>\n",
       "      <td>0.052900</td>\n",
       "      <td>0.365000</td>\n",
       "      <td>0.498100</td>\n",
       "      <td>6397</td>\n",
       "      <td>12844</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='123' max='123' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [123/123 00:43]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   eval_loss  eval_rouge1  eval_rouge2  eval_rougeL  eval_rougeLsum  \\\n",
      "0      4.016         0.31        0.141        0.268           0.274   \n",
      "\n",
      "   eval_bleu  eval_gen_len  \n",
      "0      0.053        17.988  \n",
      "__________\n",
      "Original: Algorithm that identify the S&P500 firm tickers from news text. The algorithm only identify those tickers in the three most commonly seen cases. Detailed description refer to the paper\n",
      "\n",
      "\n",
      "Summary before Tuning: Dense Text Cleaning and Parsing from Nltk Import concurrent\n",
      "\n",
      "\n",
      "Summary after Tuning: This Quantlet is dedicated to classification of the Solidity Price of the Solidity based Turkish website. We provide three methods to visualise the effect of changing the effect of the effect on the effect of the keyword. For each of the methods, we select one of the names of the publications and use the names of the publications. For\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Inspect the derived variables from the News Network, Plots and Stats\n",
      "\n",
      "\n",
      "Summary before Tuning: Analysis of Network Variable Data: Human Ways, Human\n",
      "\n",
      "\n",
      "Summary after Tuning: This Quantlet is dedicated to generating and preprocessing of the attention variable. We provide a way to fully correct the attention variable for the first time series of the results. We show how to fully correct the attention variable for the first time series of the results.\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Inspect the derived variables from the News Network, Plots and Stats\n",
      "\n",
      "\n",
      "Summary before Tuning: PlotKernelDensity Estimation: Modeling and Estimation of Model\n",
      "\n",
      "\n",
      "Summary after Tuning: Plotting a series of KDE with given Bandwidths and Kernel Functions\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Estimates and plots  empirical pricing kernels , risk neutral densities  and physical densities  of DAX 30 index return conditional on 20% , 40%  and 60%  quantiles of volatility index VDAX-NEW , and time to maturity 1 month. Local linear kernel regression is used for estimation of the conditional risk neutral density and local constant kernel regression is used for estimation of conditional physical density. Moreover, 95% confidence intervals have been calculated for PK, RND and PD conditioned by 40% quantile of VDAX-NEW and time to maturity 1 month. All results are shown on a continuously compounded 1-month period returns scale. \n",
      "\n",
      "\n",
      "Summary before Tuning: Rethinking the System for Precomputed Volatility: Pre\n",
      "\n",
      "\n",
      "Summary after Tuning: Estimates and plots the kernel density estimate of the DAX and VDAX levels for the DAX and VDAX levels based on the time period from 2000 to 2006.\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Estimates and plots  empirical pricing kernels , risk neutral densities  and physical densities  of DAX 30 index return conditional on time to maturity 2 months and different levels of VDAX-NEW-Subindex 2 . Local linear kernel regression is used for estimation of the conditional risk neutral density and local constant kernel regression is used for estimation of conditional physical density. EPK is obtained as a ratio of RND and PD. The panels on the right-hand side of figures depict EPK, RND, PD conditional on 20%, 50% and 80% quantiles of VDAX-NEW-Subindex 2 with 95% confidence intervals. Colors from red to blue correspond to increasing values of volatility within each interval. All results are shown on a continuously compounded 2-months period returns scale.\n",
      "\n",
      "\n",
      "Summary before Tuning: Rethinking the System for Epk3Vola\n",
      "\n",
      "\n",
      "Summary after Tuning: Estimates and plots the kernel density estimate of the VDAX and DAX volatility using the Kelly-Bernoulli kernel and the time series of the DAX and VDAX and the PCA.\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Conducts a statistical analysis, heteroskedasticity and normality checks on housing data provided by Sberbank\n",
      "\n",
      "\n",
      "Summary before Tuning: Sbera Sbera-test: Sbera S\n",
      "\n",
      "\n",
      "Summary after Tuning: Conducting a statistical analysis, heteroskedasticity and normality checks on housing data provided by Sberbank\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Descriptive statistical analysis of Moscow housing data provided by Sberbank\n",
      "\n",
      "\n",
      "Summary before Tuning: Towards an Overview of Summary Statistics for Multi-Variable\n",
      "\n",
      "\n",
      "Summary after Tuning: This repository contains code for getting an overview of summary statistics for the variables and analysis of correlations of variables. The script helps to get an overview of summary statistics for the variables and analysis of correlations of variables.\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Descriptive statistical analysis of Moscow housing data provided by Sberbank\n",
      "\n",
      "\n",
      "Summary before Tuning: The Messy Datamerge Benchmark\n",
      "\n",
      "\n",
      "Summary after Tuning: This Quantlet is dedicated to adapting the models for the time-series of the transaction. We load the raw and messy datasets, which are then used in the models.\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Contains the VIX index and the VIX future intraday analysis on the tail-event day of February 5, 2018. The next day, Credit Suisse lost 500m USD because of a structured product on the VIX. Market disruption can already be observed the day before.\n",
      "\n",
      "\n",
      "Summary before Tuning: Towards Fast and Accurate VIXf futures for\n",
      "\n",
      "\n",
      "Summary after Tuning: Analyse the VIX futures and the VIX index prices for the market on the 5th of February, 2018 to the past 15 seconds.\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Contains the VIX index and the VIX future intraday analysis on the tail-event day of February 5, 2018. The next day, Credit Suisse lost 500m USD because of a structured product on the VIX. Market disruption can already be observed the day before.\n",
      "\n",
      "\n",
      "Summary before Tuning: restrict_to_start_end_date, data, data\n",
      "\n",
      "\n",
      "Summary after Tuning: restrict_to_start_end_date Select a subset of the data, based on a given date range and a given time range.\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Implements the statistics tests for the Efficient Market Hypothesis: Bartell Test, Ljung-Box Test, Variance Ratio Test, Von-Newmann Test.\n",
      "\n",
      "\n",
      "Summary before Tuning: Plotting Stock Closer\n",
      "\n",
      "\n",
      "Summary after Tuning: Plotting the VRT test for stock prices over January 1, 2016\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Implements the statistics tests for the Efficient Market Hypothesis: Bartell Test, Ljung-Box Test, Variance Ratio Test, Von-Newmann Test.\n",
      "\n",
      "\n",
      "Summary before Tuning: The Quantmod library and the emh: Get the Qu\n",
      "\n",
      "\n",
      "Summary after Tuning: This Quantlet is dedicated to classification of the Apple stock data. The first line is the GARCH-based stock data, second line is the GARCH-based stock data, third line is the GARCH-based stock data. The second line is the GARCH-based daily daily data.\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Plots time series from cryptocurrency database\n",
      "\n",
      "\n",
      "Summary before Tuning: Application of Time in Data.table, Applications to Enhanced\n",
      "\n",
      "\n",
      "Summary after Tuning: Plotting time variables for the Dshusd, LTC, and Xmrusd data set\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Plots time series from cryptocurrency database\n",
      "\n",
      "\n",
      "Summary before Tuning: Modeling the Time Series: PWR, Quantmod\n",
      "\n",
      "\n",
      "Summary after Tuning: This Quantlet plots the time series of BTC and BTC of the USD\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Split the entire dataset in two parts. the first one consists of all observations without the last 24 ones. It is used for the training process of LSTM with the optimal parameters. The second subset consists of the last 24 observations used for the prediction of the hourly returns in the next 24 hours. Once the predictions are generated for each of the coins, they are exported from python in order to be imported in MATLAB.\n",
      "\n",
      "\n",
      "Summary before Tuning: Predicts for export: A novel model for time series prediction\n",
      "\n",
      "\n",
      "Summary after Tuning: Predicts the LSTM models for the time series data from 1992-01-01 to 2006-12-29.\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Define a parameter grid for the LSTM and choose the optimal set of parameters that leads to a minimal root-mean-squared error.\n",
      "\n",
      "\n",
      "Summary before Tuning: Training Predictive as ParameterGrid\n",
      "\n",
      "\n",
      "Summary after Tuning: Use LSTM model to predict the risk bound of a test set.\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Produces the estimation results using GeoCopula approach and saves it as RDate file.\n",
      "\n",
      "\n",
      "Summary before Tuning: Automatic coding of fiveciverse data in windows\n",
      "\n",
      "\n",
      "Summary after Tuning: Computes a variogram of the Econometric distribution for the French food data.\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: UNIT 3 and UNIT 4 of DEDA. Introduce webscraping and object-oriented programming\n",
      "\n",
      "\n",
      "Summary before Tuning: Web Scraping and Retrieval of Data from Website S\n",
      "\n",
      "\n",
      "Summary after Tuning: Scraping encompasses any method allowing for extracting data from websites. You wish to scrape the first page of the South China Morning Post   \n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: UNIT 3 and UNIT 4 of DEDA. Introduce webscraping and object-oriented programming\n",
      "\n",
      "\n",
      "Summary before Tuning: Unsupervised Learning of US GOV Data from the W\n",
      "\n",
      "\n",
      "Summary after Tuning: Unpacks US GOV data from webpage and builds a data frame.\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: ['Demonstrate decision tree', 'Demonstrate applied SVM on NLP sentiment classification', 'Plot some activation functions for Neural Networks']\n",
      "\n",
      "\n",
      "Summary before Tuning: Sample size and prediction of data in keras models\n",
      "\n",
      "\n",
      "Summary after Tuning: Use Knowledge Loss for training and test data.\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer = analysis_modules.scs_analyze(**analysis_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "smrdGzzNyo5n",
    "tags": []
   },
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_logs(trainer):\n",
    "    log_history = trainer.state.log_history\n",
    "    train_log = pd.DataFrame(columns=log_history[0].keys())\n",
    "    eval_log = pd.DataFrame(columns=log_history[1].keys())\n",
    "    for log in log_history:\n",
    "        if \"loss\" in log:\n",
    "            train_log = pd.concat(\n",
    "                [train_log, pd.DataFrame.from_dict(log, orient=\"index\").T], axis=0\n",
    "            )\n",
    "        elif \"eval_loss\" in log:\n",
    "            eval_log = pd.concat(\n",
    "                [eval_log, pd.DataFrame.from_dict(log, orient=\"index\").T], axis=0\n",
    "            )\n",
    "\n",
    "    logs = train_log.merge(\n",
    "        eval_log,\n",
    "        how=\"inner\",\n",
    "        left_on=[\"epoch\", \"step\"],\n",
    "        right_on=[\"epoch\", \"step\"],\n",
    "    )\n",
    "    return logs[\n",
    "        [\n",
    "            \"epoch\",\n",
    "            \"loss\",\n",
    "            \"step\",\n",
    "            \"eval_loss\",\n",
    "            \"eval_rouge1\",\n",
    "            \"eval_rouge2\",\n",
    "            \"eval_rougeL\",\n",
    "            \"eval_rougeLsum\",\n",
    "            \"eval_gen_len\",\n",
    "            \"eval_bleu\",\n",
    "            \"eval_brevity_penalty\",\n",
    "            \"eval_length_ratio\",\n",
    "            \"eval_translation_length\",\n",
    "            \"eval_reference_length\",\n",
    "        ]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPZowVhs+ksZzaIPG80pu3+",
   "gpuType": "A100",
   "machine_shape": "hm",
   "mount_file_id": "119XCh3Q64Zw4MGlsZQuGrMLS78o5MeWS",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "encode_code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

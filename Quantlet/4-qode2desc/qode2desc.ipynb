{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "JZal6ahJZQBU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "QPATH = \"Quantlet/4-qode2desc\"\n",
    "\n",
    "import sys\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "import os\n",
    "import gc\n",
    "\n",
    "if IN_COLAB:\n",
    "    os.chdir(\n",
    "        f\"/content/drive/MyDrive/ColabNotebooks/IRTG/Encode_the_Qode/Encode-the-Qode/{QPATH}\"\n",
    "    )\n",
    "\n",
    "#%%capture\n",
    "#%pip install protobuf==3.20.1\n",
    "if IN_COLAB:\n",
    "    %pip install transformers[torch]\n",
    "    %pip install -q sentencepiece\n",
    "    %pip install datasets==2.13.1\n",
    "    %pip install evaluate\n",
    "    %pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "iaAT_m3NaEzp",
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "oRmp1O7SZgaI",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "import torch\n",
    "import torch, gc\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "import importlib\n",
    "import analysis_modules\n",
    "\n",
    "importlib.reload(analysis_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeT5_no_context_test_20231104\n",
      "CodeT5_no_context_test_20231104\n",
      "cuda\n",
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/RDC/zinovyee.hub/.cache/huggingface/datasets/json/default-6aaeab545afde721/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f7b39afe8604c288852baf343314af9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/RDC/zinovyee.hub/.cache/huggingface/datasets/json/default-7f5c8e4d974366b6/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a38837cc93f6432787216d64d3c5d470",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/RDC/zinovyee.hub/.cache/huggingface/datasets/json/default-6aaeab545afde721/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-12b560e1043ec171.arrow\n",
      "Loading cached processed dataset at /home/RDC/zinovyee.hub/.cache/huggingface/datasets/json/default-7f5c8e4d974366b6/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-641af1d54cea832d.arrow\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='160' max='80' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [80/80 02:17]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   eval_loss  eval_rouge1  eval_rouge2  eval_rougeL  eval_rougeLsum  \\\n",
      "0      5.888        0.132        0.031        0.114           0.116   \n",
      "\n",
      "   eval_bleu  eval_gen_len  \n",
      "0      0.005        10.806  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/RDC/zinovyee.hub/.local/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10320' max='10320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10320/10320 28:32, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Brevity Penalty</th>\n",
       "      <th>Length Ratio</th>\n",
       "      <th>Translation Length</th>\n",
       "      <th>Reference Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.686100</td>\n",
       "      <td>4.102497</td>\n",
       "      <td>0.313700</td>\n",
       "      <td>0.131800</td>\n",
       "      <td>0.272000</td>\n",
       "      <td>0.279300</td>\n",
       "      <td>17.128100</td>\n",
       "      <td>0.047400</td>\n",
       "      <td>0.348600</td>\n",
       "      <td>0.486900</td>\n",
       "      <td>3942</td>\n",
       "      <td>8096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.772600</td>\n",
       "      <td>3.861991</td>\n",
       "      <td>0.338400</td>\n",
       "      <td>0.164000</td>\n",
       "      <td>0.299400</td>\n",
       "      <td>0.307100</td>\n",
       "      <td>17.628100</td>\n",
       "      <td>0.069700</td>\n",
       "      <td>0.366400</td>\n",
       "      <td>0.499000</td>\n",
       "      <td>4040</td>\n",
       "      <td>8096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.187600</td>\n",
       "      <td>3.785659</td>\n",
       "      <td>0.357100</td>\n",
       "      <td>0.178800</td>\n",
       "      <td>0.319200</td>\n",
       "      <td>0.324100</td>\n",
       "      <td>17.706200</td>\n",
       "      <td>0.079900</td>\n",
       "      <td>0.374800</td>\n",
       "      <td>0.504700</td>\n",
       "      <td>4086</td>\n",
       "      <td>8096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.768500</td>\n",
       "      <td>3.794013</td>\n",
       "      <td>0.369200</td>\n",
       "      <td>0.199200</td>\n",
       "      <td>0.332800</td>\n",
       "      <td>0.338900</td>\n",
       "      <td>17.971900</td>\n",
       "      <td>0.096800</td>\n",
       "      <td>0.397900</td>\n",
       "      <td>0.520400</td>\n",
       "      <td>4213</td>\n",
       "      <td>8096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.463600</td>\n",
       "      <td>3.830547</td>\n",
       "      <td>0.370100</td>\n",
       "      <td>0.215600</td>\n",
       "      <td>0.337800</td>\n",
       "      <td>0.342600</td>\n",
       "      <td>17.753100</td>\n",
       "      <td>0.103000</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>0.515100</td>\n",
       "      <td>4170</td>\n",
       "      <td>8096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.241300</td>\n",
       "      <td>3.849622</td>\n",
       "      <td>0.370500</td>\n",
       "      <td>0.217100</td>\n",
       "      <td>0.336900</td>\n",
       "      <td>0.342000</td>\n",
       "      <td>17.406200</td>\n",
       "      <td>0.097400</td>\n",
       "      <td>0.335700</td>\n",
       "      <td>0.478100</td>\n",
       "      <td>3871</td>\n",
       "      <td>8096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.084200</td>\n",
       "      <td>3.890765</td>\n",
       "      <td>0.383600</td>\n",
       "      <td>0.227200</td>\n",
       "      <td>0.349400</td>\n",
       "      <td>0.354100</td>\n",
       "      <td>17.937500</td>\n",
       "      <td>0.105300</td>\n",
       "      <td>0.374200</td>\n",
       "      <td>0.504300</td>\n",
       "      <td>4083</td>\n",
       "      <td>8096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.966200</td>\n",
       "      <td>3.910174</td>\n",
       "      <td>0.370200</td>\n",
       "      <td>0.220200</td>\n",
       "      <td>0.341100</td>\n",
       "      <td>0.345200</td>\n",
       "      <td>17.540600</td>\n",
       "      <td>0.104400</td>\n",
       "      <td>0.364200</td>\n",
       "      <td>0.497500</td>\n",
       "      <td>4028</td>\n",
       "      <td>8096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.881800</td>\n",
       "      <td>3.883279</td>\n",
       "      <td>0.391400</td>\n",
       "      <td>0.237100</td>\n",
       "      <td>0.359200</td>\n",
       "      <td>0.364400</td>\n",
       "      <td>17.771900</td>\n",
       "      <td>0.110100</td>\n",
       "      <td>0.367200</td>\n",
       "      <td>0.499500</td>\n",
       "      <td>4044</td>\n",
       "      <td>8096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.816400</td>\n",
       "      <td>3.876163</td>\n",
       "      <td>0.393600</td>\n",
       "      <td>0.239800</td>\n",
       "      <td>0.359500</td>\n",
       "      <td>0.366400</td>\n",
       "      <td>18.093800</td>\n",
       "      <td>0.116900</td>\n",
       "      <td>0.396000</td>\n",
       "      <td>0.519100</td>\n",
       "      <td>4203</td>\n",
       "      <td>8096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.763900</td>\n",
       "      <td>3.823864</td>\n",
       "      <td>0.396900</td>\n",
       "      <td>0.247900</td>\n",
       "      <td>0.365800</td>\n",
       "      <td>0.369000</td>\n",
       "      <td>17.865600</td>\n",
       "      <td>0.122000</td>\n",
       "      <td>0.384100</td>\n",
       "      <td>0.511000</td>\n",
       "      <td>4137</td>\n",
       "      <td>8096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.719700</td>\n",
       "      <td>3.863929</td>\n",
       "      <td>0.393500</td>\n",
       "      <td>0.233200</td>\n",
       "      <td>0.358500</td>\n",
       "      <td>0.364800</td>\n",
       "      <td>17.934400</td>\n",
       "      <td>0.112300</td>\n",
       "      <td>0.391900</td>\n",
       "      <td>0.516300</td>\n",
       "      <td>4180</td>\n",
       "      <td>8096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.687400</td>\n",
       "      <td>3.818481</td>\n",
       "      <td>0.401400</td>\n",
       "      <td>0.243700</td>\n",
       "      <td>0.367600</td>\n",
       "      <td>0.373600</td>\n",
       "      <td>17.868800</td>\n",
       "      <td>0.119300</td>\n",
       "      <td>0.385500</td>\n",
       "      <td>0.512000</td>\n",
       "      <td>4145</td>\n",
       "      <td>8096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.661800</td>\n",
       "      <td>3.821615</td>\n",
       "      <td>0.396300</td>\n",
       "      <td>0.249000</td>\n",
       "      <td>0.366800</td>\n",
       "      <td>0.371700</td>\n",
       "      <td>17.887500</td>\n",
       "      <td>0.122800</td>\n",
       "      <td>0.386600</td>\n",
       "      <td>0.512700</td>\n",
       "      <td>4151</td>\n",
       "      <td>8096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.645500</td>\n",
       "      <td>3.818995</td>\n",
       "      <td>0.396000</td>\n",
       "      <td>0.245700</td>\n",
       "      <td>0.363600</td>\n",
       "      <td>0.368200</td>\n",
       "      <td>17.859400</td>\n",
       "      <td>0.120300</td>\n",
       "      <td>0.386600</td>\n",
       "      <td>0.512700</td>\n",
       "      <td>4151</td>\n",
       "      <td>8096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='80' max='80' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [80/80 00:31]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   eval_loss  eval_rouge1  eval_rouge2  eval_rougeL  eval_rougeLsum  \\\n",
      "0      3.819        0.396        0.246        0.364           0.368   \n",
      "\n",
      "   eval_bleu  eval_gen_len  \n",
      "0       0.12        17.859  \n",
      "__________\n",
      "Original: Generates graphs and tables to evaluate prediction performance of naive predictor, LASSO, and LSTM models.\n",
      "\n",
      "\n",
      "Summary before Tuning: Load error measures from a file of saved error measures.\n",
      "\n",
      "\n",
      "Summary after Tuning: The model in which the error measures are saved and stored in three different models for all datasets.\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Generates plots of total over-/underestimation errors of naive, LASSO, and LSTM models for multiple energy consumer and prosumer data sets.\n",
      "\n",
      "\n",
      "Summary before Tuning: Returns a DataFrame with the time tibble with the values specified in the input.\n",
      "\n",
      "\n",
      "Summary after Tuning: Generates plots and tables to evaluate the outcomes of three market simulations of a blind double auction as implemented in a smart contract by Mengelkamp et al.  with and without predictions of energy consumption values.\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Computes and plots descriptive statistics of datasets containing consumers/prosumers' energy readings.\n",
      "\n",
      "\n",
      "Summary before Tuning: Returns a DataFrame with the time tibble with the values specified in the input.\n",
      "\n",
      "\n",
      "Summary after Tuning: Generates plots and tables to evaluate the outcomes of three market simulations of a blind double auction as implemented in a smart contract by Mengelkamp et al.  with and without predictions of energy consumption values.\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Conducts a simulation study of the proposed methodology and computes the MSE\n",
      "\n",
      "\n",
      "Summary before Tuning: Simulates the data using the Gauss - Marquardt model.\n",
      "\n",
      "\n",
      "Summary after Tuning: Simulates the variation of an ARMA model.\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Plots time series graphs of mortality and fertility of Japan and Taiwan based on historical data sets.\n",
      "\n",
      "\n",
      "Summary before Tuning: Clear history and close windows\n",
      "\n",
      "\n",
      "Summary after Tuning: Analyzes demographic trend  and forecasts mortality and fertility data for Japan, Fertility and Taiwan from 1999 to 2006.\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Analyzes demographic trend  and forecasts mortality in Japan and Taiwan using Lee-Carter method.\n",
      "\n",
      "\n",
      "Summary before Tuning: Make a plot of the Japanread and Female data.\n",
      "\n",
      "\n",
      "Summary after Tuning: Compares and plots forecast accuracy using Lee-Carter and Hyndman-Ullah methods for mortality data of Japan, and computes Diebold Mariano statistics.\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Inspects the LDA output and posterior output and attempts to investigate structures and insights\n",
      "\n",
      "\n",
      "Summary before Tuning: Load topic models and topic models\n",
      "\n",
      "\n",
      "Summary after Tuning: Plots the topic modelling analysis of the Latent Dirichlet Allocation  model on the localising data\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Generates synthetic data in form of a partial linear model to apply simulations for causal inference estimation. The parameter of interest is the treatment or uplift effect for a binary treatment assignment.\n",
      "\n",
      "\n",
      "Summary before Tuning: Generate the simulation data as a dataframe by Daniel Jacob.\n",
      "\n",
      "\n",
      "Summary after Tuning: Simulation of datasets for the estimation of the CATE. Different settings are possible, e.g random assignment, confounding, linear and non-linear dependencies.\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: 'Computes parallel coordinates plots for multivariate car data. It allows to capture the structure of multivariate data in a 2 dimensional space. Parallel lines indicate a positive relationship between the variables. The full black line marks U.S. cars, the dotted red line marks Japanese cars, and the dashed blue line marks European cars. Variables 7  to 11  imply that US cars are larger than European or Japanese cars. The strong intersection of Variable 1  and 2  indicates a strong negative relationship between both variables. As relationships between variables are only visible between neighbouring variables other data visualization techniques are recommended.'\n",
      "\n",
      "\n",
      "Summary before Tuning: plot carc. txt\n",
      "\n",
      "\n",
      "Summary after Tuning: Computes parallel coordinates plot for car data.\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: 'Provides a profile analysis of citrate concentrations in plasma.'\n",
      "\n",
      "\n",
      "Summary before Tuning: make sure all variables are clear and close windows\n",
      "\n",
      "\n",
      "Summary after Tuning: 'Tests the equality of parallel profiles.\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: 'computes the nonmetric MDS for the athletic data set. Performing the nonmetric MDS we observe a cloud containing most of the countries and at some distance Netherlands, Mauritius, West Samoa, Cook Islands.'\n",
      "\n",
      "\n",
      "Summary before Tuning: clear variables and close windows\n",
      "\n",
      "\n",
      "Summary after Tuning: 'Computes the distance between the athletic records and the non-metric MDS for the ATHletic data.\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Performs a cluster analysis for the US cereal data from the R-package MASS. On the transformed data will be performed a principal component analysis and a cluster analysis employing Euclidean distance with the Ward linkage algorithm. Plots of principal components and the dendrogram are presented. After extraction of 3 clusters, the principal components with the 3 clusters  are shown\n",
      "\n",
      "\n",
      "Summary before Tuning: Make a simple MASS plot with the given cereal data.\n",
      "\n",
      "\n",
      "Summary after Tuning: Performs cluster analysis for the US cereal data.\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Employs the centroid linkage using squared Euclidean distance matrices to perform a cluster analysis on an 8 points example\n",
      "\n",
      "\n",
      "Summary before Tuning: Plot the clustered linkage and the total conciousness of the centroid.\n",
      "\n",
      "\n",
      "Summary after Tuning: Employs the Ward algorithm to perform a cluster analysis on an 8 points example\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Performs cluster analysis for US health 2005 data. On the transformed data we perform a principal component analysis and a cluster analysis employing Eulcidean distance and Ward linkage algorithm. Plots of principal components and the dendrogram are presented. After extraction of 4 clusters, the principal components with the four clusters  are shown. The graphs and clustering differs from R due to different algorithms.\n",
      "\n",
      "\n",
      "Summary before Tuning: shows the US health and the cluster\n",
      "\n",
      "\n",
      "Summary after Tuning: 'Performs cluster analysis for US health data.\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: calculates the covariance matrix  of the Swiss Bank  dataset and the variance of the counterfeit bank notes  after they were lineary transformed by a vector a = \n",
      "\n",
      "\n",
      "Summary before Tuning: This function calculates the covariance matrix and the variance of the counterfeit bank notes after theywere lineary transformed by a vector a.\n",
      "\n",
      "\n",
      "Summary after Tuning: 'Computes the covariance matrix for the Swiss bank notes and the variance of the counterfeit bank notes.\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: 'Simulates a normal sample with [mu1,mu2]=[1, 2] and [sigma1, sigma2, sigma3, sigma4]=[1, 0.5, 0.5, 2] and tests the hypothesis H0\n",
      "\n",
      "\n",
      "Summary before Tuning: clear all clcset\n",
      "\n",
      "\n",
      "Summary after Tuning: 'Tests the equality of 2 groups of the standard normal random variables with a standard normal and covariance matrix.'\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: 'computes the nonmetric MDS for the athletic data set  without four most outlying countries, e.g. Netherlands, West Samoa, Mauritius, and Cook Islands.'\n",
      "\n",
      "\n",
      "Summary before Tuning: clear variables and close windows\n",
      "\n",
      "\n",
      "Summary after Tuning: 'Computes a linear regression of the athletic records using nonmetric multidimensional scaling. n = 10 and the non-metric MDS = 0.5.\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: 'computes a two dimensional scatterplot of X4 vs. X5  of the Swiss bank notes data.'\n",
      "\n",
      "\n",
      "Summary before Tuning: SM Sscabank45 computes a two dimensional scatterplot of X4 vs. X5 with the Swiss bank notes data.\n",
      "\n",
      "\n",
      "Summary after Tuning: 'Computes a two dimensional scatterplot of X4 vs. X5  of the Swiss bank notes data, respectively.'\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Computes Fisher's linear discrimination function  either for the 20 bank notes from Exercise 12.6 or for a random set of banknotes. The discrimination function is then applied to the the entire bank data set. With the linear discrimination function based on the 20 bank notes from Ex. 12.6 only 6 bank notes out of the entire 200  are misclassified, which leads to an error rate of 3 percent.\n",
      "\n",
      "\n",
      "Summary before Tuning: load all clc and load data and take sample from bank2. dat\n",
      "\n",
      "\n",
      "Summary after Tuning: Computes Fisher's LDA with a training sample of the Swiss bank notes. The first step is to remove the 20 randomly chosen bank notes, the second step is to remove the 20 randomly chosen bank notes from the swiss bank notes.\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: 'Applies multidimensional scaling for Swiss bank notes. The Swiss bank note data is transformed into an Euclidean distance matrix of size . An attempt to achieve the original configuration of points is made employing metric MDS. Results are quite similar to configuration from PCA. The scatterplot of two-dimensional projections, however, gives better seperation. A reason could be that principal components are based only on the estimates of a covariance matrix which is wrong if the data set consists of more subgroups.'\n",
      "\n",
      "\n",
      "Summary before Tuning: clear variables and close windows\n",
      "\n",
      "\n",
      "Summary after Tuning: 'Computes a linear regression of the correlation of the variables on the Swiss bank notes data. The data is not provided due to NDA.'\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "Analysis finished\n",
      "CodeT5_domain_test_20231104\n",
      "CodeT5_domain_test_20231104\n",
      "cuda\n",
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset json (/home/RDC/zinovyee.hub/.cache/huggingface/datasets/json/default-6aaeab545afde721/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c216bc59a5f2406d8143e5c4cde2c105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset json (/home/RDC/zinovyee.hub/.cache/huggingface/datasets/json/default-7f5c8e4d974366b6/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87bc341d5a5345fc9e54676ae3171382",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/RDC/zinovyee.hub/.cache/huggingface/datasets/json/default-6aaeab545afde721/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-896542230648c889.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/320 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='160' max='80' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [80/80 02:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   eval_loss  eval_rouge1  eval_rouge2  eval_rougeL  eval_rougeLsum  \\\n",
      "0      9.415        0.135        0.024        0.116           0.119   \n",
      "\n",
      "   eval_bleu  eval_gen_len  \n",
      "0      0.002        13.778  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/RDC/zinovyee.hub/.local/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10320' max='10320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10320/10320 28:41, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Brevity Penalty</th>\n",
       "      <th>Length Ratio</th>\n",
       "      <th>Translation Length</th>\n",
       "      <th>Reference Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.621200</td>\n",
       "      <td>4.708849</td>\n",
       "      <td>0.288700</td>\n",
       "      <td>0.107800</td>\n",
       "      <td>0.250100</td>\n",
       "      <td>0.255800</td>\n",
       "      <td>18.071900</td>\n",
       "      <td>0.037800</td>\n",
       "      <td>0.376100</td>\n",
       "      <td>0.505600</td>\n",
       "      <td>4093</td>\n",
       "      <td>8096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.401000</td>\n",
       "      <td>4.350377</td>\n",
       "      <td>0.325600</td>\n",
       "      <td>0.143300</td>\n",
       "      <td>0.282300</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>18.362500</td>\n",
       "      <td>0.058700</td>\n",
       "      <td>0.417400</td>\n",
       "      <td>0.533700</td>\n",
       "      <td>4321</td>\n",
       "      <td>8096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.752500</td>\n",
       "      <td>4.168271</td>\n",
       "      <td>0.341000</td>\n",
       "      <td>0.166900</td>\n",
       "      <td>0.303200</td>\n",
       "      <td>0.309000</td>\n",
       "      <td>18.481200</td>\n",
       "      <td>0.077500</td>\n",
       "      <td>0.413100</td>\n",
       "      <td>0.530800</td>\n",
       "      <td>4297</td>\n",
       "      <td>8096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.279200</td>\n",
       "      <td>4.094425</td>\n",
       "      <td>0.351000</td>\n",
       "      <td>0.184200</td>\n",
       "      <td>0.317400</td>\n",
       "      <td>0.324300</td>\n",
       "      <td>18.090600</td>\n",
       "      <td>0.087500</td>\n",
       "      <td>0.407300</td>\n",
       "      <td>0.526800</td>\n",
       "      <td>4265</td>\n",
       "      <td>8096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.917400</td>\n",
       "      <td>4.122588</td>\n",
       "      <td>0.341800</td>\n",
       "      <td>0.188600</td>\n",
       "      <td>0.313400</td>\n",
       "      <td>0.318400</td>\n",
       "      <td>17.928100</td>\n",
       "      <td>0.086700</td>\n",
       "      <td>0.384200</td>\n",
       "      <td>0.511100</td>\n",
       "      <td>4138</td>\n",
       "      <td>8096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.636900</td>\n",
       "      <td>4.071011</td>\n",
       "      <td>0.363100</td>\n",
       "      <td>0.204300</td>\n",
       "      <td>0.329900</td>\n",
       "      <td>0.337000</td>\n",
       "      <td>18.006200</td>\n",
       "      <td>0.095300</td>\n",
       "      <td>0.373300</td>\n",
       "      <td>0.503700</td>\n",
       "      <td>4078</td>\n",
       "      <td>8096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.422800</td>\n",
       "      <td>4.043410</td>\n",
       "      <td>0.368600</td>\n",
       "      <td>0.207900</td>\n",
       "      <td>0.331400</td>\n",
       "      <td>0.337800</td>\n",
       "      <td>18.184400</td>\n",
       "      <td>0.101600</td>\n",
       "      <td>0.391000</td>\n",
       "      <td>0.515700</td>\n",
       "      <td>4175</td>\n",
       "      <td>8096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.247300</td>\n",
       "      <td>4.036574</td>\n",
       "      <td>0.368700</td>\n",
       "      <td>0.215700</td>\n",
       "      <td>0.340500</td>\n",
       "      <td>0.346600</td>\n",
       "      <td>18.381200</td>\n",
       "      <td>0.108400</td>\n",
       "      <td>0.406700</td>\n",
       "      <td>0.526400</td>\n",
       "      <td>4262</td>\n",
       "      <td>8096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.115900</td>\n",
       "      <td>4.050159</td>\n",
       "      <td>0.376100</td>\n",
       "      <td>0.221400</td>\n",
       "      <td>0.346800</td>\n",
       "      <td>0.353400</td>\n",
       "      <td>18.040600</td>\n",
       "      <td>0.109000</td>\n",
       "      <td>0.384800</td>\n",
       "      <td>0.511500</td>\n",
       "      <td>4141</td>\n",
       "      <td>8096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.001300</td>\n",
       "      <td>4.057422</td>\n",
       "      <td>0.384200</td>\n",
       "      <td>0.228500</td>\n",
       "      <td>0.354400</td>\n",
       "      <td>0.361000</td>\n",
       "      <td>18.306200</td>\n",
       "      <td>0.118400</td>\n",
       "      <td>0.408900</td>\n",
       "      <td>0.527900</td>\n",
       "      <td>4274</td>\n",
       "      <td>8096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.912300</td>\n",
       "      <td>4.054507</td>\n",
       "      <td>0.389200</td>\n",
       "      <td>0.232700</td>\n",
       "      <td>0.355000</td>\n",
       "      <td>0.360400</td>\n",
       "      <td>18.278100</td>\n",
       "      <td>0.117600</td>\n",
       "      <td>0.406600</td>\n",
       "      <td>0.526300</td>\n",
       "      <td>4261</td>\n",
       "      <td>8096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.841600</td>\n",
       "      <td>4.061653</td>\n",
       "      <td>0.387200</td>\n",
       "      <td>0.230400</td>\n",
       "      <td>0.354000</td>\n",
       "      <td>0.360900</td>\n",
       "      <td>18.193800</td>\n",
       "      <td>0.116200</td>\n",
       "      <td>0.386100</td>\n",
       "      <td>0.512400</td>\n",
       "      <td>4148</td>\n",
       "      <td>8096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.784100</td>\n",
       "      <td>4.073672</td>\n",
       "      <td>0.379700</td>\n",
       "      <td>0.226800</td>\n",
       "      <td>0.347500</td>\n",
       "      <td>0.353400</td>\n",
       "      <td>18.171900</td>\n",
       "      <td>0.113600</td>\n",
       "      <td>0.389100</td>\n",
       "      <td>0.514500</td>\n",
       "      <td>4165</td>\n",
       "      <td>8096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.736700</td>\n",
       "      <td>4.071831</td>\n",
       "      <td>0.389300</td>\n",
       "      <td>0.234000</td>\n",
       "      <td>0.356900</td>\n",
       "      <td>0.363800</td>\n",
       "      <td>18.031200</td>\n",
       "      <td>0.117100</td>\n",
       "      <td>0.387000</td>\n",
       "      <td>0.513000</td>\n",
       "      <td>4153</td>\n",
       "      <td>8096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.706000</td>\n",
       "      <td>4.071160</td>\n",
       "      <td>0.387200</td>\n",
       "      <td>0.230800</td>\n",
       "      <td>0.355500</td>\n",
       "      <td>0.361900</td>\n",
       "      <td>18.181200</td>\n",
       "      <td>0.116400</td>\n",
       "      <td>0.390200</td>\n",
       "      <td>0.515200</td>\n",
       "      <td>4171</td>\n",
       "      <td>8096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='80' max='80' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [80/80 00:31]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   eval_loss  eval_rouge1  eval_rouge2  eval_rougeL  eval_rougeLsum  \\\n",
      "0      4.071        0.387        0.231        0.355           0.362   \n",
      "\n",
      "   eval_bleu  eval_gen_len  \n",
      "0      0.116        18.181  \n",
      "__________\n",
      "Original: Generates graphs and tables to evaluate prediction performance of naive predictor, LASSO, and LSTM models.\n",
      "\n",
      "\n",
      "Summary before Tuning: load Error Measures for Applications in Michael K\n",
      "\n",
      "\n",
      "Summary after Tuning: The Quantlet is used to predict the return of the predictor model to obtain error measures and to compute the predictions of energy consumption values and predicted error measures.\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Generates plots of total over-/underestimation errors of naive, LASSO, and LSTM models for multiple energy consumer and prosumer data sets.\n",
      "\n",
      "\n",
      "Summary before Tuning: The Path, Id, and Return: Michael K\n",
      "\n",
      "\n",
      "Summary after Tuning: Plots exemplary the density function of one household's raw energy consumption values and the density function its log energy consumption values scaled between 0 and 1 for a LSTM neural network.\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Computes and plots descriptive statistics of datasets containing consumers/prosumers' energy readings.\n",
      "\n",
      "\n",
      "Summary before Tuning: The Path, Id, and Return: Michael K\n",
      "\n",
      "\n",
      "Summary after Tuning: Plots exemplary the density function of one household's raw energy consumption values and the density function its log energy consumption values scaled between 0 and 1 for a LSTM neural network.\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Conducts a simulation study of the proposed methodology and computes the MSE\n",
      "\n",
      "\n",
      "Summary before Tuning: A Function For Simulation of Data\n",
      "\n",
      "\n",
      "Summary after Tuning: Simulates the data, its variance and the A-squared minimization and computes the variables.\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Plots time series graphs of mortality and fertility of Japan and Taiwan based on historical data sets.\n",
      "\n",
      "\n",
      "Summary before Tuning: Dancing with Mortality and Fine-tight\n",
      "\n",
      "\n",
      "Summary after Tuning: Plots the Japan, the French and the Taiwan mortality data.\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Analyzes demographic trend  and forecasts mortality in Japan and Taiwan using Lee-Carter method.\n",
      "\n",
      "\n",
      "Summary before Tuning: Set windows image prediction\n",
      "\n",
      "\n",
      "Summary after Tuning: Compares and plots forecast accuracy using Lee-Carter and Hyndman-Ullah methods for mortality data of Japan, and computes Diebold Mariano statistics.\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Inspects the LDA output and posterior output and attempts to investigate structures and insights\n",
      "\n",
      "\n",
      "Summary before Tuning: Transforming LDA output into Topic models\n",
      "\n",
      "\n",
      "Summary after Tuning: The LDA model is used to create a topic distribution for the training of the LDA model. The optimal topic is used for the latent distributions of the latent distributions, such as a matrix of the latent distributions, topic-term distributions are applied on a\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Generates synthetic data in form of a partial linear model to apply simulations for causal inference estimation. The parameter of interest is the treatment or uplift effect for a binary treatment assignment.\n",
      "\n",
      "\n",
      "Summary before Tuning: Cluster Generation: A Multi-scale Series Data Model\n",
      "\n",
      "\n",
      "Summary after Tuning: Simulation of datasets for the estimation of the CATE. Different settings are possible, e.g random assignment, confounding, linear and non-linear dependencies.\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: 'Computes parallel coordinates plots for multivariate car data. It allows to capture the structure of multivariate data in a 2 dimensional space. Parallel lines indicate a positive relationship between the variables. The full black line marks U.S. cars, the dotted red line marks Japanese cars, and the dashed blue line marks European cars. Variables 7  to 11  imply that US cars are larger than European or Japanese cars. The strong intersection of Variable 1  and 2  indicates a strong negative relationship between both variables. As relationships between variables are only visible between neighbouring variables other data visualization techniques are recommended.'\n",
      "\n",
      "\n",
      "Summary before Tuning: Understanding and Improving Carc. rda\n",
      "\n",
      "\n",
      "Summary after Tuning: Computes parallel coordinates plot for car data.\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: 'Provides a profile analysis of citrate concentrations in plasma.'\n",
      "\n",
      "\n",
      "Summary before Tuning: Towards High Performance Multi-View Profiles\n",
      "\n",
      "\n",
      "Summary after Tuning: 'Tests the equality of parallel profiles by applying the cars for multiple profiles.'\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: 'computes the nonmetric MDS for the athletic data set. Performing the nonmetric MDS we observe a cloud containing most of the countries and at some distance Netherlands, Mauritius, West Samoa, Cook Islands.'\n",
      "\n",
      "\n",
      "Summary before Tuning: athletic.rda: Multi-scale Nonmetric Matrix\n",
      "\n",
      "\n",
      "Summary after Tuning: 'Computes a Descriptive statistics ofathletic records over time\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Performs a cluster analysis for the US cereal data from the R-package MASS. On the transformed data will be performed a principal component analysis and a cluster analysis employing Euclidean distance with the Ward linkage algorithm. Plots of principal components and the dendrogram are presented. After extraction of 3 clusters, the principal components with the 3 clusters  are shown\n",
      "\n",
      "\n",
      "Summary before Tuning: Using Ward Dendrogram for U Scereal Data\n",
      "\n",
      "\n",
      "Summary after Tuning: 'Performs cluster analysis for US cereal data.\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Employs the centroid linkage using squared Euclidean distance matrices to perform a cluster analysis on an 8 points example\n",
      "\n",
      "\n",
      "Summary before Tuning: Fast and Accurate Learning of Large-Scale Cluster\n",
      "\n",
      "\n",
      "Summary after Tuning: Employs the WTO-IMF algorithm for cluster analysis on an 8 points example\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Performs cluster analysis for US health 2005 data. On the transformed data we perform a principal component analysis and a cluster analysis employing Eulcidean distance and Ward linkage algorithm. Plots of principal components and the dendrogram are presented. After extraction of 4 clusters, the principal components with the four clusters  are shown. The graphs and clustering differs from R due to different algorithms.\n",
      "\n",
      "\n",
      "Summary before Tuning: Using Ward Dendrogram and Practice for US Health\n",
      "\n",
      "\n",
      "Summary after Tuning: Performs cluster analysis for US health data.\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: calculates the covariance matrix  of the Swiss Bank  dataset and the variance of the counterfeit bank notes  after they were lineary transformed by a vector a = \n",
      "\n",
      "\n",
      "Summary before Tuning: SMSQuantlet: SM Scovbank Description\n",
      "\n",
      "\n",
      "Summary after Tuning: 'Computes the covariance matrix and eigevalues of the Swiss bank notes data and the variance of the counterfeit bank notes. Additionally, it computes the first two of them. Additionally, the screeplot of the counterfeit bank notes. Additionally, the first two of them.\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: 'Simulates a normal sample with [mu1,mu2]=[1, 2] and [sigma1, sigma2, sigma3, sigma4]=[1, 0.5, 0.5, 2] and tests the hypothesis H0\n",
      "\n",
      "\n",
      "Summary before Tuning: Rand Stream: New Pseudo Random numbers\n",
      "\n",
      "\n",
      "Summary after Tuning: 'Tests covariance matrix for two pseudo random numbers'\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: 'computes the nonmetric MDS for the athletic data set  without four most outlying countries, e.g. Netherlands, West Samoa, Mauritius, and Cook Islands.'\n",
      "\n",
      "\n",
      "Summary before Tuning: athletic.rda: Multi-scale and Multi-\n",
      "\n",
      "\n",
      "Summary after Tuning: 'Computes a linear regression of the athletic records data.\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: 'computes a two dimensional scatterplot of X4 vs. X5  of the Swiss bank notes data.'\n",
      "\n",
      "\n",
      "Summary before Tuning: SM Sscabank45 Reconstruction of Two Dimensional\n",
      "\n",
      "\n",
      "Summary after Tuning: 'Computes a two dimensional scatterplot of two dimensional scatterplot of the Swiss bank notes data.\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Computes Fisher's linear discrimination function  either for the 20 bank notes from Exercise 12.6 or for a random set of banknotes. The discrimination function is then applied to the the entire bank data set. With the linear discrimination function based on the 20 bank notes from Ex. 12.6 only 6 bank notes out of the entire 200  are misclassified, which leads to an error rate of 3 percent.\n",
      "\n",
      "\n",
      "Summary before Tuning: Banks to the Data Set: Model Prediction, Model,\n",
      "\n",
      "\n",
      "Summary after Tuning: 'performs a linear regression for 20 randomly chosen bank notes from the swiss bank notes dataset. Centered principal components are chosen by LDA and LDA and LDA and LDA. However, the LDA is applied on their Swiss bank notes dataset. However, the LDA is applied on their real from the Swiss bank notes dataset is\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: 'Applies multidimensional scaling for Swiss bank notes. The Swiss bank note data is transformed into an Euclidean distance matrix of size . An attempt to achieve the original configuration of points is made employing metric MDS. Results are quite similar to configuration from PCA. The scatterplot of two-dimensional projections, however, gives better seperation. A reason could be that principal components are based only on the estimates of a covariance matrix which is wrong if the data set consists of more subgroups.'\n",
      "\n",
      "\n",
      "Summary before Tuning: Bank2.rda: Unsupervised Learning of Mult\n",
      "\n",
      "\n",
      "Summary after Tuning: 'performs a linear regression of the mds for the Swiss bank notes data.\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "Analysis finished\n"
     ]
    }
   ],
   "source": [
    "for MODE in ['no_context', \"domain\"]:#\n",
    "    analysis_config = {\n",
    "        \"DATE\": \"20231104\",\n",
    "        \"MODE\": MODE,\n",
    "        \"model_name\": \"CodeT5\",\n",
    "        \"encoder_max_length\": 512,\n",
    "        \"decoder_max_length\": 75,\n",
    "        \"random_state\": 42,\n",
    "        \"learning_rate\": 5e-4,\n",
    "        \"epochs\": 15,\n",
    "        \"train_batch\": 4,\n",
    "        \"eval_batch\": 4,\n",
    "        \"warmup_steps\": 100,\n",
    "        \"weight_decay\": 0.1,\n",
    "        \"logging_stes\": 100,\n",
    "        \"save_total_lim\": 1,\n",
    "        \"save_strategy\": \"steps\",\n",
    "        \"label_smooting\": 0.1,\n",
    "        \"predict_generate\": True,\n",
    "        \"load_best_model_at_end\": False,\n",
    "        \"evaluation_strategy\": \"epoch\",\n",
    "        \"freeze\": True,\n",
    "    }\n",
    "    if analysis_config[\"MODE\"] == \"domain\":\n",
    "        analysis_config[\"train_data_path\"] = f\"../../data/preprocessed/Quantlet/{analysis_config['DATE']}/no_context/\"\n",
    "    else:\n",
    "        analysis_config[\"train_data_path\"] = f\"../../data/preprocessed/Quantlet/{analysis_config['DATE']}/{analysis_config['MODE']}/\"\n",
    "\n",
    "    analysis_config[\"train_data_name\"] = f\"full_train_dataset_{analysis_config['DATE']}_sample0.json\"\n",
    "    \n",
    "    if analysis_config[\"MODE\"] == \"domain\":\n",
    "        analysis_config[\n",
    "            \"val_data_path\"\n",
    "        ] = f\"../../data/preprocessed/Quantlet/{analysis_config['DATE']}/no_context/\"\n",
    "    else:\n",
    "        analysis_config[\n",
    "            \"val_data_path\"\n",
    "        ] = f\"../../data/preprocessed/Quantlet/{analysis_config['DATE']}/{analysis_config['MODE']}/\"\n",
    "    analysis_config[\n",
    "        \"val_data_name\"\n",
    "    ] = f\"test_dataset_{analysis_config['DATE']}_sample0.json\"\n",
    "\n",
    "    analysis_config[\"analysis_name\"] = analysis_modules.create_name(analysis_config)\n",
    "    \n",
    "    print(analysis_config[\"analysis_name\"])\n",
    "\n",
    "    if analysis_config[\"MODE\"] == \"domain\":\n",
    "        if analysis_config[\"model_name\"] == \"CodeT5\":\n",
    "            analysis_config[\n",
    "                \"model_name\"\n",
    "            ] = \"../../data/pretrained/analysis_report_CodeT5-test-12-300-4-2023-09-26-v2/results/checkpoint-88488\"\n",
    "        if analysis_config[\"model_name\"] == \"CodeTrans\":\n",
    "            analysis_config[\n",
    "                \"model_name\"\n",
    "            ] = \"../../data/pretrained/CodeTrans/results/checkpoint-12290\"\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    trainer = analysis_modules.scs_analyze(**analysis_config)\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    logs = analysis_modules.parse_logs(trainer).drop_duplicates()\n",
    "\n",
    "    ANALYSIS_FOLDER=f'reports/analysis_report_{analysis_config[\"analysis_name\"]}'\n",
    "\n",
    "    logs.to_csv(f'{ANALYSIS_FOLDER}/logs.csv', index=False)\n",
    "\n",
    "    print('Analysis finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import scan_cache_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cache = scan_cache_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPZowVhs+ksZzaIPG80pu3+",
   "gpuType": "A100",
   "machine_shape": "hm",
   "mount_file_id": "119XCh3Q64Zw4MGlsZQuGrMLS78o5MeWS",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

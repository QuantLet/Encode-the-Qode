{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "JZal6ahJZQBU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "QPATH = \"Quantlet/4-qode2desc\"\n",
    "\n",
    "import sys\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "import os\n",
    "import gc\n",
    "\n",
    "if IN_COLAB:\n",
    "    os.chdir(\n",
    "        f\"/content/drive/MyDrive/ColabNotebooks/IRTG/Encode_the_Qode/Encode-the-Qode/{QPATH}\"\n",
    "    )\n",
    "\n",
    "#%%capture\n",
    "#%pip install protobuf==3.20.1\n",
    "if IN_COLAB:\n",
    "    %pip install transformers[torch]\n",
    "    %pip install -q sentencepiece\n",
    "    %pip install datasets==2.13.1\n",
    "    %pip install evaluate\n",
    "    %pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "iaAT_m3NaEzp",
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "oRmp1O7SZgaI",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "import torch\n",
    "import torch, gc\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "import importlib\n",
    "import analysis_modules\n",
    "\n",
    "importlib.reload(analysis_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeT5_no_context_test_20231119_normal\n",
      "CodeT5_no_context_test_20231119_normal\n",
      "cuda\n",
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/RDC/zinovyee.hub/.cache/huggingface/datasets/json/default-9befa6bc65e542c9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8439490a0a1b4944a52368c084205d56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /home/RDC/zinovyee.hub/.cache/huggingface/datasets/json/default-4cf165996c876d3c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f3d30f19c064c4287f0541b41fa7c92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9fd2a48b07244828dfa5b3585ac0af5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41782040f6604cc8b1ae8cdbf989ef18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/RDC/zinovyee.hub/.cache/huggingface/datasets/json/default-4cf165996c876d3c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0afaa6aa5854b97b72ffb23ad560e47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3e4f2de65aa488685cb7ba827c9f821",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3040 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da0cd16132914b21be20e07724406a60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/327 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='164' max='82' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [82/82 01:58]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   eval_loss  eval_rouge1  eval_rouge2  eval_rougeL  eval_rougeLsum  \\\n",
      "0      6.288        0.145        0.039        0.125           0.131   \n",
      "\n",
      "   eval_bleu  eval_gen_len  \n",
      "0      0.012        14.177  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/RDC/zinovyee.hub/.local/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11400' max='11400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11400/11400 26:46, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Brevity Penalty</th>\n",
       "      <th>Length Ratio</th>\n",
       "      <th>Translation Length</th>\n",
       "      <th>Reference Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.494900</td>\n",
       "      <td>4.169928</td>\n",
       "      <td>0.313900</td>\n",
       "      <td>0.137800</td>\n",
       "      <td>0.275400</td>\n",
       "      <td>0.285300</td>\n",
       "      <td>18.042800</td>\n",
       "      <td>0.047600</td>\n",
       "      <td>0.335600</td>\n",
       "      <td>0.478100</td>\n",
       "      <td>4219</td>\n",
       "      <td>8825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.499300</td>\n",
       "      <td>4.010826</td>\n",
       "      <td>0.333400</td>\n",
       "      <td>0.168900</td>\n",
       "      <td>0.292800</td>\n",
       "      <td>0.299300</td>\n",
       "      <td>17.816500</td>\n",
       "      <td>0.063600</td>\n",
       "      <td>0.316900</td>\n",
       "      <td>0.465300</td>\n",
       "      <td>4106</td>\n",
       "      <td>8825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.955100</td>\n",
       "      <td>3.921113</td>\n",
       "      <td>0.354100</td>\n",
       "      <td>0.196500</td>\n",
       "      <td>0.312900</td>\n",
       "      <td>0.320100</td>\n",
       "      <td>18.391400</td>\n",
       "      <td>0.087000</td>\n",
       "      <td>0.361300</td>\n",
       "      <td>0.495500</td>\n",
       "      <td>4373</td>\n",
       "      <td>8825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.583000</td>\n",
       "      <td>3.901985</td>\n",
       "      <td>0.377700</td>\n",
       "      <td>0.225500</td>\n",
       "      <td>0.340100</td>\n",
       "      <td>0.347600</td>\n",
       "      <td>18.250800</td>\n",
       "      <td>0.096200</td>\n",
       "      <td>0.337600</td>\n",
       "      <td>0.479400</td>\n",
       "      <td>4231</td>\n",
       "      <td>8825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.325600</td>\n",
       "      <td>3.909333</td>\n",
       "      <td>0.377400</td>\n",
       "      <td>0.222200</td>\n",
       "      <td>0.339900</td>\n",
       "      <td>0.346800</td>\n",
       "      <td>18.113100</td>\n",
       "      <td>0.096700</td>\n",
       "      <td>0.351300</td>\n",
       "      <td>0.488700</td>\n",
       "      <td>4313</td>\n",
       "      <td>8825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.142600</td>\n",
       "      <td>3.941986</td>\n",
       "      <td>0.371800</td>\n",
       "      <td>0.227300</td>\n",
       "      <td>0.338300</td>\n",
       "      <td>0.343900</td>\n",
       "      <td>18.385300</td>\n",
       "      <td>0.104900</td>\n",
       "      <td>0.356500</td>\n",
       "      <td>0.492200</td>\n",
       "      <td>4344</td>\n",
       "      <td>8825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.007200</td>\n",
       "      <td>3.958220</td>\n",
       "      <td>0.377300</td>\n",
       "      <td>0.227900</td>\n",
       "      <td>0.343800</td>\n",
       "      <td>0.350200</td>\n",
       "      <td>18.055000</td>\n",
       "      <td>0.099500</td>\n",
       "      <td>0.324800</td>\n",
       "      <td>0.470700</td>\n",
       "      <td>4154</td>\n",
       "      <td>8825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.901900</td>\n",
       "      <td>3.956727</td>\n",
       "      <td>0.383800</td>\n",
       "      <td>0.235700</td>\n",
       "      <td>0.348000</td>\n",
       "      <td>0.355100</td>\n",
       "      <td>18.275200</td>\n",
       "      <td>0.105800</td>\n",
       "      <td>0.347300</td>\n",
       "      <td>0.486000</td>\n",
       "      <td>4289</td>\n",
       "      <td>8825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.825900</td>\n",
       "      <td>3.938743</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.247700</td>\n",
       "      <td>0.366900</td>\n",
       "      <td>0.374100</td>\n",
       "      <td>18.149800</td>\n",
       "      <td>0.111600</td>\n",
       "      <td>0.352100</td>\n",
       "      <td>0.489300</td>\n",
       "      <td>4318</td>\n",
       "      <td>8825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.769700</td>\n",
       "      <td>3.935363</td>\n",
       "      <td>0.395200</td>\n",
       "      <td>0.245300</td>\n",
       "      <td>0.360200</td>\n",
       "      <td>0.367600</td>\n",
       "      <td>18.238500</td>\n",
       "      <td>0.111100</td>\n",
       "      <td>0.354000</td>\n",
       "      <td>0.490500</td>\n",
       "      <td>4329</td>\n",
       "      <td>8825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.720800</td>\n",
       "      <td>3.943485</td>\n",
       "      <td>0.398000</td>\n",
       "      <td>0.254600</td>\n",
       "      <td>0.366800</td>\n",
       "      <td>0.373200</td>\n",
       "      <td>18.290500</td>\n",
       "      <td>0.115200</td>\n",
       "      <td>0.351300</td>\n",
       "      <td>0.488700</td>\n",
       "      <td>4313</td>\n",
       "      <td>8825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.684600</td>\n",
       "      <td>3.918009</td>\n",
       "      <td>0.391600</td>\n",
       "      <td>0.247800</td>\n",
       "      <td>0.359700</td>\n",
       "      <td>0.365400</td>\n",
       "      <td>18.211000</td>\n",
       "      <td>0.113800</td>\n",
       "      <td>0.344800</td>\n",
       "      <td>0.484300</td>\n",
       "      <td>4274</td>\n",
       "      <td>8825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.655400</td>\n",
       "      <td>3.913865</td>\n",
       "      <td>0.397300</td>\n",
       "      <td>0.251900</td>\n",
       "      <td>0.365800</td>\n",
       "      <td>0.372100</td>\n",
       "      <td>18.415900</td>\n",
       "      <td>0.114400</td>\n",
       "      <td>0.350500</td>\n",
       "      <td>0.488200</td>\n",
       "      <td>4308</td>\n",
       "      <td>8825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.632500</td>\n",
       "      <td>3.907481</td>\n",
       "      <td>0.397300</td>\n",
       "      <td>0.251400</td>\n",
       "      <td>0.360700</td>\n",
       "      <td>0.366600</td>\n",
       "      <td>18.376100</td>\n",
       "      <td>0.114800</td>\n",
       "      <td>0.353600</td>\n",
       "      <td>0.490300</td>\n",
       "      <td>4327</td>\n",
       "      <td>8825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.616100</td>\n",
       "      <td>3.897423</td>\n",
       "      <td>0.401600</td>\n",
       "      <td>0.253700</td>\n",
       "      <td>0.365600</td>\n",
       "      <td>0.372100</td>\n",
       "      <td>18.385300</td>\n",
       "      <td>0.116000</td>\n",
       "      <td>0.355500</td>\n",
       "      <td>0.491600</td>\n",
       "      <td>4338</td>\n",
       "      <td>8825</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='82' max='82' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [82/82 00:25]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   eval_loss  eval_rouge1  eval_rouge2  eval_rougeL  eval_rougeLsum  \\\n",
      "0      3.897        0.402        0.254        0.366           0.372   \n",
      "\n",
      "   eval_bleu  eval_gen_len  \n",
      "0      0.116        18.385  \n",
      "__________\n",
      "Original: Generates plots of total over-/underestimation errors of naive, LASSO, and LSTM models for multiple energy consumer and prosumer data sets.\n",
      "\n",
      "\n",
      "Summary before Tuning: Plots over - and underestimation for each non - terminal node in a single - line system.\n",
      "\n",
      "\n",
      "Summary after Tuning: Generates plots over-andunderestimation for each dataset.\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: 'hfhd_marketimpact_diff code plots the static and time-varying\n",
      "\n",
      "\n",
      "Summary before Tuning: missing - block - length - non - zero - time - series - time - series - time - series - time - series - time - series - time - series - time - series - time - series - time - series - time - series - time - series - time - series - time - series - time - series - time - series - time - series -\n",
      "\n",
      "\n",
      "Summary after Tuning: 'hfhd_rob code plots the robust time-varying risk transmission channels\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Produces the Ensemble XGB model that creates artificial features during training. Model is tested using real data for Polish companies\n",
      "\n",
      "\n",
      "Summary before Tuning: import all the missing - missing - missing - missing - missing - missing - missing - missing - missing - missing - missing - missing - missing - missing - missing - missing - missing - missing - missing - missing - missing - missing - missing - missing - missing - missing - missing - missing - missing - missing - missing - missing - missing - missing - missing - missing\n",
      "\n",
      "\n",
      "Summary after Tuning: Use machines learning algorithms to predict the price of the CV1 year from Exgb 1.1, Exgb 2.2, Exgb 3.4, Exgb5.\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Plots time series graphs of mortality and fertility of Japan and Taiwan based on historical data sets.\n",
      "\n",
      "\n",
      "Summary before Tuning: plot windows windows missing national data\n",
      "\n",
      "\n",
      "Summary after Tuning: Plots the demographic log-returns of the DAX index for the DAX and FTSE mortality data from Japan, and Taiwan.\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Analyzes demographic trend  and forecasts mortality and fertility in Japan using Hyndman-Ullah method.\n",
      "\n",
      "\n",
      "Summary before Tuning: Plots windows and opens window with n - th window of n - th window of n - th window of n - th window of n - th window of n - th window of n - th window of n - th window of n - th window of n - th window of n - th window of n - th window of n - th window of n - th\n",
      "\n",
      "\n",
      "Summary after Tuning: Compares and plots forecasting with Japan and Mortality data.\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: 'performs a PCA and a cluster analysis for 20 randomly chosen bank notes from the swiss bank notes dataset.\n",
      "\n",
      "\n",
      "Summary before Tuning: Plots the non - random non - random non - random non - random non - random non - random non - random non - random non - random non - random non - random non - random non - random non - random non - random non - random non - random non - random non - random non - random non - random non - random non - random non - random\n",
      "\n",
      "\n",
      "Summary after Tuning: 'performs a PCA and a cluster analysis for 20 randomly chosen bank notes from the swiss bank notes dataset. Centered principal components as well as cluster analysis employing ward algorithm to perform cluster analysis on the whole bank data set. Centered principal components as well as cluster analysis employing ward algorithm with squared Euclidean distance matrices to perform a cluster analysis\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: 'performs a PCA and a cluster analysis for 20 randomly chosen bank notes from the swiss bank notes dataset.\n",
      "\n",
      "\n",
      "Summary before Tuning: Add a series of basic basic cluster analysis related to a specific cluster analysis. This series contains all the basic cluster analysis of a specific type and all the basic cluster analysis of a specific type.\n",
      "\n",
      "\n",
      "Summary after Tuning: 'performs cluster analysis for 20 randomly chosen bank notes from the swiss bank notes dataset. Centered principal components as well as cluster analysis employing ward algorithm to Lasso solution. However, the banknote 116 is misspecified by cluster analysis with squared Euclidean distance matrices perform similarly on seperating real from forged bank notes. However, the\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: 'Tests covariance matrices in Härdle & Simar  Exercise 7.18'\n",
      "\n",
      "\n",
      "Summary before Tuning: Clear variables and close windows of the n - th window.\n",
      "\n",
      "\n",
      "Summary after Tuning: Tests the hypothesis of the equality of the covariance matrices on two simulated 4-dimensional samples of sizes n1=30 and n2=20.\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: 'Tests covariance matrices in Härdle & Simar  Exercise 7.18'\n",
      "\n",
      "\n",
      "Summary before Tuning: clear variables and close windows\n",
      "\n",
      "\n",
      "Summary after Tuning: Tests the hypothesis of the equality of the covariance matrices on two simulated 4-dimensional samples of sizes n1=30 and n2=20.\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: 'Provides a profile analysis of citrate concentrations in plasma.'\n",
      "\n",
      "\n",
      "Summary before Tuning: find windows where there are no windows where there are no windows where there are no windows where there are no windows where there are no windows where there are no windows where there are no windows where there are no windows where there are no windows where there are no windows where there are no windows where there are no windows where there are no windows where there are no windows where there\n",
      "\n",
      "\n",
      "Summary after Tuning: 'employs the profiles of a network evolution. The profiles are then plotted as scatter and regression.'\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: 'Provides a profile analysis of citrate concentrations in plasma.'\n",
      "\n",
      "\n",
      "Summary before Tuning: Estimate all non - zero non - zero non - zero non - zero non - zero non - zero non - zero non - zero non - zero non - zero non - zero non - zero non - zero non - zero non - zero non - zero non - zero non - zero non - zero non - zero non - zero non - zero non - zero non - zero\n",
      "\n",
      "\n",
      "Summary after Tuning: 'Plots the equality of the profiles of the profiles. The profiles are taken from the profiles package. The profiles are taken from the profiles package, and the profiles are tested to see whether the profiles are equal.\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Performs a factor analysis on the variables 'Length','Height Left','Height Right','Inner Frame Lower', 'Inner Frame Upper' and 'Diagonal' in the bank data set using iterated principal factors method  with and without manual rotation by an angle of 7*pi/12 counterclockwise. Estimated factor loadings, communalities and specific variances are presented in a table, plots of original and rotated factor loadings are given\n",
      "\n",
      "\n",
      "Summary before Tuning: Plots all non - duplicate non - duplicate non - duplicate non - duplicate non - duplicate non - duplicate non - duplicate non - duplicate non - duplicate non - duplicate non - duplicate non - duplicate non - duplicate non - duplicate non - duplicate non - duplicate non - duplicate non - duplicate non - duplicate non - duplicate non - duplicate non - duplicate non - duplicate non - duplicate\n",
      "\n",
      "\n",
      "Summary after Tuning: 'performs a factor analysis on the Swiss bank notes. The first one\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Performs a factor analysis on the variables 'Length','Height Left','Height Right','Inner Frame Lower', 'Inner Frame Upper' and 'Diagonal' in the bank data set using iterated principal factors method  with and without manual rotation by an angle of 7*pi/12 counterclockwise. Estimated factor loadings, communalities and specific variances are presented in a table, plots of original and rotated factor loadings are given\n",
      "\n",
      "\n",
      "Summary before Tuning: This method is used to compute the commutative and unrotated loadings of a non - zero - valued block of data.\n",
      "\n",
      "\n",
      "Summary after Tuning: 'Computing the orthogonal factor model via PCM and PFM '\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: 'Compares a lasso shrinkage regression with ridge regression'\n",
      "\n",
      "\n",
      "Summary before Tuning: clear windows missing variable\n",
      "\n",
      "\n",
      "Summary after Tuning: Plots the parameters of a stochastic process with beta = 1.\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: 'Performes a normalized principal component analysis NPCA on the Geopol data set, which contains a comparison of 41 countries according to 10 political and economic parameters. The NPCA is computed without the first variable, size of population. The remaining variables are\n",
      "\n",
      "\n",
      "Summary before Tuning: region all - principal - components of a residue in a normalized principal components for geopol data.\n",
      "\n",
      "\n",
      "Summary after Tuning: Performs a PCA for the standardized principal components for the German data set and shows the first three principal components in two-dimensional scatterplots. Additionally, a screeplot of the eigenvalues is displayed.\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: 'Performes a normalized principal component analysis NPCA on the Geopol data set, which contains a comparison of 41 countries according to 10 political and economic parameters. The NPCA is computed without the first variable, size of population. The remaining variables are\n",
      "\n",
      "\n",
      "Summary before Tuning: plot all kernels with a warning on the first and last non - zero eigenvalues of the variance matrix\n",
      "\n",
      "\n",
      "Summary after Tuning: Computes a PCA for the standardized Swiss bank notes and shows the first three principal components in two-dimensional scatterplots. Additionally, a screeplot of the eigenvalues is displayed.\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: 'performs a correspondence analysis for the US crime data, shows the eigenvalues of the singular value decomposition of the chi-matrix,projections on the first 3 axes and absolute contributions. It also displays graphical the factorial decomposition, the regions are selected in 4 groups , Midwest, South and West).'\n",
      "\n",
      "\n",
      "Summary before Tuning: region uw_scala_all_windows uw_scala_all_windows uw_scala_all_windows uw_scala_all_windows uw_scala_all_windows uw_scala_all_windows uw_scala_all_windows uw_\n",
      "\n",
      "\n",
      "Summary after Tuning: 'performs a correspondence analysis for the US crime, shows the eigenvalues of the singular value decomposition of the chi-matrix,projections on the first 3 axes and absolute contributions. It also displays graphical the factorial decomposition, the regions are selected in 4 groups, Midwest, South and West).'\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: 'performs a correspondence analysis for the US crime data, shows the eigenvalues of the singular value decomposition of the chi-matrix,projections on the first 3 axes and absolute contributions. It also displays graphical the factorial decomposition, the regions are selected in 4 groups , Midwest, South and West).'\n",
      "\n",
      "\n",
      "Summary before Tuning: shows the series of windows where the user has no variable that can be found in the system\n",
      "\n",
      "\n",
      "Summary after Tuning: 'performs a correspondence analysis for the US crime, shows the eigenvalues of the singular value decomposition of the chi-matrix and displays graphical its factorial decomposition.'\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: ‘Performs a normalized principal component\n",
      "\n",
      "\n",
      "Summary before Tuning: -\n",
      " athletic - normalized principal component\n",
      " athletic records of 55 countries in eight meters and 10km and 10km and 10km and 10km and 10km and 10km and 10km and 10km and 10km and 10km and 10km and 10km and 10km and 10km and 10km and 10km and 10km\n",
      "\n",
      "\n",
      "Summary after Tuning: 'computes a normalized principal component analysis for the athletic data set. Evidently, for different length of tracks of 55 countries in eight countries and the original variables are shown as results.\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: ‘Performs a normalized principal component\n",
      "\n",
      "\n",
      "Summary before Tuning: Plots the non - zero non - zero non - zero non - zero non - zero non - zero non - zero non - zero non - zero non - zero non - zero non - zero non - zero non - zero non - zero non - zero non - zero non - zero non - zero non - zero non - zero non - zero non - zero non - zero\n",
      "\n",
      "\n",
      "Summary after Tuning: 'performs a PCA for the standardized Brownian motion with two principal components in two-dimensional scatterplots. The first three principal components are taken as a dependent variable and the second three principal components are taken as an example.'\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "Analysis finished\n"
     ]
    }
   ],
   "source": [
    "for MODEL in ['CodeT5']:   #'CodeTrans',\n",
    "    for SAMPLE_MODE in [ 'test']:#'val',\n",
    "        if SAMPLE_MODE=='test':\n",
    "            train_name = 'full_train'\n",
    "            test_name  = 'test'\n",
    "        elif SAMPLE_MODE=='val':\n",
    "            train_name = 'train'\n",
    "            test_name  = 'val'\n",
    "        else: \n",
    "            print('Only test and val are available. Please change the SAMPLE_MODE')\n",
    "        \n",
    "        analysis_config = {\n",
    "            \"DATE\": \"20231119_normal\",\n",
    "            \"MODE\": \"no_context\",\n",
    "            \"model_name\": MODEL,\n",
    "            \"encoder_max_length\": 512,\n",
    "            \"decoder_max_length\": 75,\n",
    "            \"random_state\": 42,\n",
    "            \"learning_rate\": 5e-4,\n",
    "            \"epochs\": 15,\n",
    "            \"train_batch\": 4,\n",
    "            \"eval_batch\": 4,\n",
    "            \"warmup_steps\": 100,\n",
    "            \"weight_decay\": 0.1,\n",
    "            \"logging_stes\": 100,\n",
    "            \"save_total_lim\": 1,\n",
    "            \"save_strategy\": \"epoch\",\n",
    "            \"label_smooting\": 0.1,\n",
    "            \"predict_generate\": True,\n",
    "            \"load_best_model_at_end\": False,\n",
    "            \"evaluation_strategy\": \"epoch\",\n",
    "            \"freeze\": True,\n",
    "        }\n",
    "        \n",
    "        analysis_config[\"train_data_path\"] = f\"../../data/preprocessed/Quantlet/{analysis_config['DATE']}/{analysis_config['MODE']}/\"\n",
    "        analysis_config[\"train_data_name\"] = f\"{train_name}_dataset_{analysis_config['DATE']}_sample0.json\"\n",
    "        \n",
    "        analysis_config[\"val_data_path\"] = f\"../../data/preprocessed/Quantlet/{analysis_config['DATE']}/{analysis_config['MODE']}/\"\n",
    "        analysis_config[\"val_data_name\"] = f\"{test_name}_dataset_{analysis_config['DATE']}_sample0.json\"\n",
    "\n",
    "        analysis_config[\"analysis_name\"] = analysis_modules.create_name(analysis_config)\n",
    "        \n",
    "        print(analysis_config[\"analysis_name\"])\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        trainer = analysis_modules.scs_analyze(**analysis_config)\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        logs = analysis_modules.parse_logs(trainer).drop_duplicates()\n",
    "\n",
    "        ANALYSIS_FOLDER=f'reports/analysis_report_{analysis_config[\"analysis_name\"]}'\n",
    "\n",
    "        logs.to_csv(f'{ANALYSIS_FOLDER}/logs.csv', index=False)\n",
    "\n",
    "        print('Analysis finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check\n"
     ]
    }
   ],
   "source": [
    "print('check')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPZowVhs+ksZzaIPG80pu3+",
   "gpuType": "A100",
   "machine_shape": "hm",
   "mount_file_id": "119XCh3Q64Zw4MGlsZQuGrMLS78o5MeWS",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

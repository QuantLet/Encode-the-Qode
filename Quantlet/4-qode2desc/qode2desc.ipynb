{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JZal6ahJZQBU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "#%pip install protobuf==3.20.1\n",
    "%pip install transformers[torch]\n",
    "%pip install -q sentencepiece\n",
    "%pip install datasets==2.13.1\n",
    "%pip install evaluate\n",
    "%pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "O5Mlzgdzaliu",
    "tags": []
   },
   "outputs": [],
   "source": [
    "QPATH = \"Quantlet/4-qode2desc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "iaAT_m3NaEzp",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "import os\n",
    "\n",
    "if IN_COLAB:\n",
    "    os.chdir(\n",
    "        f\"/content/drive/MyDrive/ColabNotebooks/IRTG/Encode_the_Qode/Encode-the-Qode/{QPATH}\"\n",
    "    )\n",
    "else:\n",
    "    %load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "oRmp1O7SZgaI",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/RDC/zinovyee.hub/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import json\n",
    "import re\n",
    "import sys\n",
    "from IPython.display import display\n",
    "import datetime\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead, SummarizationPipeline\n",
    "from transformers import AdamW\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "import evaluate\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "kM604UgeBuMl",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/RDC/zinovyee.hub/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/RDC/zinovyee.hub/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'analysis_modules' from '/usr/net/zinovyee.hub/IRTG/MLSC/Encode-the-Qode/Quantlet/4-qode2desc/analysis_modules.py'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import analysis_modules\n",
    "\n",
    "importlib.reload(analysis_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "CbZX3Y3_q1pw",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeT5_domain_4_19500-20231014_42\n"
     ]
    }
   ],
   "source": [
    "# model_name = '../5-domain-pre-training/analysis_report_CodeT5-test-12-300-4-2023-09-26-v3/results/checkpoint-78656'\n",
    "# model_name = '../5-domain-pre-training/results/checkpoint-12290'\n",
    "model_name = \"CodeTrans\"\n",
    "# model_name = '../4-qode2desc/no_bootstrap/analysis_report_CodeTrans_domain-checkpoint-12290-test-10-512-16-2023-10-07/results/checkpoint-2500'\n",
    "# model_name = '../5-domain-pre-training/arxiv_pretraining/results/checkpoint-12290'\n",
    "\n",
    "model_name = \"../5-domain-pre-training/analysis_report_CodeTrans-12-300-15-4-202310145_1/results/checkpoint-58500\"\n",
    "model_name = \"../5-domain-pre-training/analysis_report_CodeT5-4-300-15-4-202310145_1/results/checkpoint-19500\"\n",
    "\n",
    "\n",
    "DATE = \"20231014\"\n",
    "\n",
    "SAMPLE = \"test\"\n",
    "if SAMPLE == \"test\":\n",
    "    load_best_model_at_end = False\n",
    "else:\n",
    "    load_best_model_at_end = None\n",
    "\n",
    "# tokenization\n",
    "encoder_max_length = 512\n",
    "decoder_max_length = 150\n",
    "RS = 42\n",
    "LR = 5e-5\n",
    "\n",
    "EPOCHS = 10\n",
    "TRAIN_BATCH = 16\n",
    "EVAL_BATCH = 4\n",
    "\n",
    "WARMUP_STEPS = 500\n",
    "WEIGHT_DECAY = 0.1\n",
    "LOGGING_STEPS = 100\n",
    "SAVE_TOTAL_LIM = 1\n",
    "SAVE_STRATEGY = \"steps\"\n",
    "\n",
    "LABEL_SMOOTHING = 0.1\n",
    "PREDICT_GENERATE = True\n",
    "\n",
    "MODE = \"no_context\"\n",
    "\n",
    "EVAL_COLUMNS = [\n",
    "    \"eval_loss\",\n",
    "    \"eval_rouge1\",\n",
    "    \"eval_rouge2\",\n",
    "    \"eval_rougeL\",\n",
    "    \"eval_rougeLsum\",\n",
    "    \"eval_bleu\",\n",
    "    \"eval_gen_len\",\n",
    "]\n",
    "\n",
    "\n",
    "analysis_name = (\n",
    "    model_name\n",
    "    + \"-\"\n",
    "    + MODE\n",
    "    + \"-\"\n",
    "    + SAMPLE\n",
    "    + \"-\"\n",
    "    + str(EPOCHS)\n",
    "    + \"-\"\n",
    "    + str(encoder_max_length)\n",
    "    + \"-\"\n",
    "    + str(TRAIN_BATCH)\n",
    ")\n",
    "\n",
    "if len(model_name) > 15:\n",
    "    analysis_name = (\n",
    "        model_name.split(\"/\")[-1]\n",
    "        + \"-\"\n",
    "        + SAMPLE\n",
    "        + \"-\"\n",
    "        + str(EPOCHS)\n",
    "        + \"-\"\n",
    "        + str(encoder_max_length)\n",
    "        + \"-\"\n",
    "        + str(TRAIN_BATCH)\n",
    "    )\n",
    "\n",
    "analysis_name = \"CodeT5_domain_4_19500\"\n",
    "analysis_name = analysis_name + \"-\" + DATE + \"_\" + str(RS)\n",
    "\n",
    "print(analysis_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "FRHaw5S7X81D",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "9t6d_nrX_lyy",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeT5_domain_4_19500-20231014_42\n",
      "CodeT5_domain_4_19500-20231014_42\n",
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/RDC/zinovyee.hub/.local/lib/python3.9/site-packages/transformers/models/auto/modeling_auto.py:1322: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/RDC/zinovyee.hub/.cache/huggingface/datasets/json/default-495b925114e2be5e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af22a12ae5b1459d9b45f2756b22d0e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/RDC/zinovyee.hub/.cache/huggingface/datasets/json/default-85ef2c2e992ded74/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fd43bb8e5f34c18b9aadd14422ebe16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/RDC/zinovyee.hub/.cache/huggingface/datasets/json/default-495b925114e2be5e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-6151a835646b297b.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/430 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/RDC/zinovyee.hub/.local/lib/python3.9/site-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='108' max='54' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [54/54 22:38]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   eval_loss  eval_rouge1  eval_rouge2  eval_rougeL  eval_rougeLsum  \\\n",
      "0     10.001        0.176        0.058        0.148           0.151   \n",
      "\n",
      "   eval_bleu  eval_gen_len  \n",
      "0      0.001        13.102  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/RDC/zinovyee.hub/.local/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1380' max='1380' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1380/1380 21:44, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>7.965700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>4.910900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>4.537500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>4.108600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.774400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.454000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>3.260000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>3.035800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.916900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.868300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>2.745600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.684500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>2.649700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   eval_loss  eval_rouge1  eval_rouge2  eval_rougeL  eval_rougeLsum  \\\n",
      "0      3.584        0.368        0.217        0.333           0.338   \n",
      "\n",
      "   eval_bleu  eval_gen_len  \n",
      "0      0.066        18.037  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/RDC/zinovyee.hub/.local/lib/python3.9/site-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________\n",
      "Original: Experiment_Sampling\n",
      "Summary before Tuning: Data-clear-non-ull Data Analysis: Random Over S\n",
      "Summary after Tuning: Classification of users by using machine learning methods\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Experiment_Sampling\n",
      "Summary before Tuning: Missing Values Table: A Data Frame for Sklearn Model Selection\n",
      "Summary after Tuning: Modelling_Modelling_Risk\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Plots the network based on an adjacency matrix before and after thresholding, so that we can see the directional connection caused by spillover effects among 100 financial institutions.\n",
      "Summary before Tuning: Plotting the returns of 100 firms\n",
      "Summary after Tuning: Plots the directional connection between 100 financial institutions based on the estimated par\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Ranks the risk receivers and the risk emitters, the top ten firms of each group are identified as the systemically important financial insitutions (SIFIs)\n",
      "Summary before Tuning: Ranking and emitting the risk receivers in the total c-\n",
      "Summary after Tuning: Ranks the risk emitters and the risk receivers of 100 firms based on the\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Estimates and plots (yearly) empirical pricing kernels (EPK), risk neutral densities (RND) and physical densities (PD) of DAX 30 index return conditional on time to maturity 3 months and different levels of VDAX-NEW-Subindex 3 (20 equally spaced numbers from 5% to 95% quantile of VDAX-NEW-Subindex 3 in a given year). Local linear kernel regression is used for estimation of the conditional risk neutral density and local constant kernel regression is used for estimation of conditional physical density. EPK is obtained as a ratio of RND and PD. The panels on the right-hand side of figures depict EPK, RND, PD conditional on 20%, 50% and 80% quantiles of VDAX-NEW-Subindex 3 with 95% confidence intervals. Colors from red to blue correspond to increasing values of volatility within each interval. All results are shown on a continuously compounded 3-months period returns scale.\n",
      "Summary before Tuning: LinBWvdax3m: A Dataset and B\n",
      "Summary after Tuning: Estimates and plots (yearly) empirical pricing kernels (EPK\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Applying eXtreme Gradient Boosting to three different sets of variables, comparing performances on the validation set, and using Partial Least Squares Regression as a benchmark\n",
      "Summary before Tuning: Data-Clean: A New Dataset for Model Quantlet\n",
      "Summary after Tuning: This Quantlet is used to create the models in the Quantlet environment. We\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Applying eXtreme Gradient Boosting to three different sets of variables, comparing performances on the validation set, and using Partial Least Squares Regression as a benchmark\n",
      "Summary before Tuning: Exploring the Efficiency of eXreme Gradient Bo\n",
      "Summary after Tuning: 'Applying eXtreme Gradient Boosting to three different sets of variables\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Reads in test and train data from csv-file and calculates out-of-sample performance measures for the trained models. Creates plots of real against predicted data, showing performance visually.\n",
      "Summary before Tuning: Model Overfit: A New Model and Method for Training Data\n",
      "Summary after Tuning: Reads in pre-processed data and uses them to predict the log-sale price.\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Estimates the implied volatility for the Black-Scholes formula, using the Newton-Raphson method.\n",
      "Summary before Tuning: Buy, Sell: Finding Implied Volume\n",
      "Summary after Tuning: We provide results for \"Does non-linear factorization of CRIX return help build better\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Preprocessing the cryptocurrency data in such a way that it fits the requirements of LSTM\n",
      "Summary before Tuning: Split Sequence into Input and Output Data\n",
      "Summary after Tuning: An example of LSTM model for time series forecasting.\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Show that uncorrelatedness does not imply independence - the difference between results of PCA and ICA\n",
      "Summary before Tuning: Independent Signals Y (3-D): A Stud\n",
      "Summary after Tuning: 'How to show a counterexample? A function to generate 3 independent signals, why\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Produces the tree map of study decision of 117 alumnus of the Econ Boot Camp (EBC) from 2008 to 2016\n",
      "Summary before Tuning: Plot Treemap: A New Visual Study\n",
      "Summary after Tuning: Computes a plot of the study decision of participants of the CRC data set.\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Network map of of collaborating disciplines in CRC 649 according to JEL classification\n",
      "Summary before Tuning: Data Preprocessing for CNN\n",
      "Summary after Tuning: Computes the CRCDP data from a CSV file and performs some pre-defined corrections.\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Generates confusion matrix between test set predictions generated by Microsoft Emotion API and true test set values. 'results_API_test_set.pkl' contains predictions from API, reruns process from emotion score raw data.\n",
      "Summary before Tuning: Test results of y_true: Boolean series of True/False\n",
      "Summary after Tuning: This quantlet includes mainly the data processing part for the RCVJ_Forecasting\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Plots smoothed emo scores (anger, contempt, disgust, fear, surprise, happiness, sadness, neutral, surprise) and stock movement of Volkswagen stock for first 30 minutes of Volkswagen press conference on 05.05.2017.\n",
      "Summary before Tuning: Plot of Happy and Neural emotions in V\n",
      "Summary after Tuning: Plots the emotion score of the stock data of VW and the happy and neutr\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Estimates a linear discriminant analysis (LDA) in order to separate positive and negative returns of Eurostoxx50 on press conference days of the European Central Bank based on facial expression scores of the provided video material. Plots resulting histograms and distributions.\n",
      "Summary before Tuning: ECB-avg-emo: A CSV of Ema\n",
      "Summary after Tuning: Estimates the bayes-rule model and plots the predictions and confusion table for the\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Finetunes an Convolutional Neural Network that classifies facial expressions into seven basic emotions (anger, disgust, fear, happiness, neutral, sadness, surprise). The underlying deep convolutional net is trained on VGGface. Image based static facial expression recognition with multiple deep network learning. The original dataset can be obtained from kaggle's 'Facial Expression Recognition Challenge'. The subfolder 'model weights' contains model weights and model architecture in .json-Format. This allows to access the estimation results without rerunning the calculations. VGGFace implementation on keras by github.com/rcmalli/keras-vggface. \n",
      "Summary before Tuning: Keras-VggFace: Modeling to JSON with Application\n",
      "Summary after Tuning: Use VGG to visualise the effect of the effect of FVCload_dataset and\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Clustering & Classifying Ethereum Smart Contracts\n",
      "Summary before Tuning: Ethereum Contract Downloader: A Data Frame for Ethere\n",
      "Summary after Tuning: This Quantlet is dedicated to gathering data on the open source source codes of the\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Cryptocurrency and CRIX Time Series\n",
      "Summary before Tuning: Charts, BarCharts, and AxpM5o\n",
      "Summary after Tuning: Plots the time series of the ETH prices and the BTC prices for the time period\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Plots different kernel functions: uniform, triangle, epanechnikov, quartic, gaussian.\n",
      "Summary before Tuning: Kernel Functions and their Applications\n",
      "Summary after Tuning: Plots different kernel functions: Uniform, Triangle, Epanechnikov, Qu\n",
      "__________\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(analysis_name)\n",
    "analysis_modules.scs_analyze(\n",
    "    analysis_name=analysis_name,\n",
    "    model_name=model_name,\n",
    "    train_data_path=f\"../../data/preprocessed/Quantlet/{DATE}/{MODE}/\",\n",
    "    train_data_name=f\"full_train_dataset_{DATE}_sample0.json\",\n",
    "    val_data_path=f\"../../data/preprocessed/Quantlet/{DATE}/{MODE}/\",\n",
    "    val_data_name=f\"test_dataset_{DATE}_sample0.json\",\n",
    "    encoder_max_length=encoder_max_length,\n",
    "    decoder_max_length=decoder_max_length,\n",
    "    random_state=RS,\n",
    "    eval_columns_list=EVAL_COLUMNS,\n",
    "    learning_rate=LR,\n",
    "    epochs=EPOCHS,\n",
    "    train_batch=TRAIN_BATCH,\n",
    "    eval_batch=EVAL_BATCH,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    logging_stes=LOGGING_STEPS,\n",
    "    save_total_lim=SAVE_TOTAL_LIM,\n",
    "    save_strategy=SAVE_STRATEGY,\n",
    "    label_smooting=LABEL_SMOOTHING,\n",
    "    predict_generate=PREDICT_GENERATE,\n",
    "    load_best_model_at_end=load_best_model_at_end,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "smrdGzzNyo5n",
    "tags": []
   },
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPZowVhs+ksZzaIPG80pu3+",
   "gpuType": "A100",
   "machine_shape": "hm",
   "mount_file_id": "119XCh3Q64Zw4MGlsZQuGrMLS78o5MeWS",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "encode_code",
   "language": "python",
   "name": "encode_code"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

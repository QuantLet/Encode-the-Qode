{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JZal6ahJZQBU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "#%pip install protobuf==3.20.1\n",
    "%pip install transformers[torch]\n",
    "%pip install -q sentencepiece\n",
    "%pip install datasets==2.13.1\n",
    "%pip install evaluate\n",
    "%pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "O5Mlzgdzaliu",
    "tags": []
   },
   "outputs": [],
   "source": [
    "QPATH = \"Quantlet/4-qode2desc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "iaAT_m3NaEzp",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "import os\n",
    "\n",
    "if IN_COLAB:\n",
    "    os.chdir(\n",
    "        f\"/content/drive/MyDrive/ColabNotebooks/IRTG/Encode_the_Qode/Encode-the-Qode/{QPATH}\"\n",
    "    )\n",
    "else:\n",
    "    %load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "oRmp1O7SZgaI",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/RDC/zinovyee.hub/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import json\n",
    "import re\n",
    "import sys\n",
    "from IPython.display import display\n",
    "import datetime\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead, SummarizationPipeline\n",
    "from transformers import AdamW\n",
    "from datasets import load_dataset\n",
    "import torch, gc\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "import evaluate\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "kM604UgeBuMl",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/RDC/zinovyee.hub/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/RDC/zinovyee.hub/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'analysis_modules' from '/usr/net/zinovyee.hub/IRTG/MLSC/Encode-the-Qode/Quantlet/4-qode2desc/analysis_modules.py'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import analysis_modules\n",
    "\n",
    "importlib.reload(analysis_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_name(analysis_config):\n",
    "    name = analysis_config[\"model_name\"]\n",
    "    mode = analysis_config[\"MODE\"]\n",
    "    date = analysis_config[\"DATE\"]\n",
    "    if analysis_config[\"val_data_name\"].startswith(\"val\"):\n",
    "        sample = \"val\"\n",
    "    else:\n",
    "        sample = \"test\"\n",
    "    return f\"{name}_{mode}_{sample}_{date}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeT5_no_context_test_20231021\n"
     ]
    }
   ],
   "source": [
    "analysis_config = {\n",
    "    \"DATE\": \"20231021\",\n",
    "    \"MODE\": \"no_context\",\n",
    "    \"model_name\": \"CodeT5\",\n",
    "    \"encoder_max_length\": 512,\n",
    "    \"decoder_max_length\": 75,\n",
    "    \"random_state\": 42,\n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"epochs\": 4,\n",
    "    \"train_batch\": 16,\n",
    "    \"eval_batch\": 4,\n",
    "    \"warmup_steps\": 500,\n",
    "    \"weight_decay\": 0.1,\n",
    "    \"logging_stes\": 100,\n",
    "    \"save_total_lim\": 1,\n",
    "    \"save_strategy\": \"steps\",\n",
    "    \"label_smooting\": 0.1,\n",
    "    \"predict_generate\": True,\n",
    "    \"load_best_model_at_end\": False,\n",
    "    \"evaluation_strategy\": \"epoch\",\n",
    "}\n",
    "analysis_config[\n",
    "    \"train_data_path\"\n",
    "] = f\"../../data/preprocessed/Quantlet/{analysis_config['DATE']}/{analysis_config['MODE']}/\"\n",
    "analysis_config[\"train_data_name\"] = (\n",
    "    f\"full_train_dataset_{analysis_config['DATE']}_sample0.json\",\n",
    ")\n",
    "analysis_config[\n",
    "    \"val_data_path\"\n",
    "] = f\"../../data/preprocessed/Quantlet/{analysis_config['DATE']}/{analysis_config['MODE']}/\"\n",
    "analysis_config[\n",
    "    \"val_data_name\"\n",
    "] = f\"test_dataset_{analysis_config['DATE']}_sample0.json\"\n",
    "analysis_config[\"analysis_name\"] = create_name(analysis_config)\n",
    "print(analysis_config[\"analysis_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "FRHaw5S7X81D",
    "tags": []
   },
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeT5_no_context_test_20231021\n",
      "cuda\n",
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset json (/home/RDC/zinovyee.hub/.cache/huggingface/datasets/json/default-2a4283db175ca957/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02ded55bad4d489ca18f0af1511ed653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset json (/home/RDC/zinovyee.hub/.cache/huggingface/datasets/json/default-b2781735718bced0/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d6e2c9066a3480984ad64378389f68f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/RDC/zinovyee.hub/.cache/huggingface/datasets/json/default-2a4283db175ca957/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-c3d528dad76c347b.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/RDC/zinovyee.hub/.cache/huggingface/datasets/json/default-b2781735718bced0/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-22c620181ef2887f.arrow\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='246' max='123' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [123/123 04:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   eval_loss  eval_rouge1  eval_rouge2  eval_rougeL  eval_rougeLsum  \\\n",
      "0      6.119        0.147        0.041        0.125           0.128   \n",
      "\n",
      "   eval_bleu  eval_gen_len  \n",
      "0      0.007        12.472  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/RDC/zinovyee.hub/.local/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1084' max='1084' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1084/1084 15:44, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Brevity Penalty</th>\n",
       "      <th>Length Ratio</th>\n",
       "      <th>Translation Length</th>\n",
       "      <th>Reference Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.115200</td>\n",
       "      <td>4.642611</td>\n",
       "      <td>0.280300</td>\n",
       "      <td>0.092800</td>\n",
       "      <td>0.235200</td>\n",
       "      <td>0.242000</td>\n",
       "      <td>17.126000</td>\n",
       "      <td>0.028300</td>\n",
       "      <td>0.342200</td>\n",
       "      <td>0.482600</td>\n",
       "      <td>6198</td>\n",
       "      <td>12844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.199100</td>\n",
       "      <td>4.190638</td>\n",
       "      <td>0.305700</td>\n",
       "      <td>0.123100</td>\n",
       "      <td>0.260100</td>\n",
       "      <td>0.265600</td>\n",
       "      <td>17.672800</td>\n",
       "      <td>0.043100</td>\n",
       "      <td>0.349300</td>\n",
       "      <td>0.487400</td>\n",
       "      <td>6260</td>\n",
       "      <td>12844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.607300</td>\n",
       "      <td>4.015602</td>\n",
       "      <td>0.327700</td>\n",
       "      <td>0.149300</td>\n",
       "      <td>0.284400</td>\n",
       "      <td>0.290500</td>\n",
       "      <td>17.731700</td>\n",
       "      <td>0.061400</td>\n",
       "      <td>0.369700</td>\n",
       "      <td>0.501200</td>\n",
       "      <td>6438</td>\n",
       "      <td>12844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.344800</td>\n",
       "      <td>3.994790</td>\n",
       "      <td>0.331400</td>\n",
       "      <td>0.152500</td>\n",
       "      <td>0.284800</td>\n",
       "      <td>0.292100</td>\n",
       "      <td>17.863800</td>\n",
       "      <td>0.063800</td>\n",
       "      <td>0.367200</td>\n",
       "      <td>0.499500</td>\n",
       "      <td>6416</td>\n",
       "      <td>12844</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='123' max='123' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [123/123 00:49]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   eval_loss  eval_rouge1  eval_rouge2  eval_rougeL  eval_rougeLsum  \\\n",
      "0      3.995        0.331        0.152        0.285           0.292   \n",
      "\n",
      "   eval_bleu  eval_gen_len  \n",
      "0      0.064        17.864  \n",
      "__________\n",
      "Original: Algorithm that identify the S&P500 firm tickers from news text. The algorithm only identify those tickers in the three most commonly seen cases. Detailed description refer to the paper\n",
      "\n",
      "\n",
      "Summary before Tuning: Python 3. 8. 10\n",
      "\n",
      "\n",
      "Summary after Tuning: This Quantlet is dedicated to gathering data on the time-series of the tickers. The data is a table of the tickers, the name-segments of the tickers and the text cleaning regex. The data is a table of the tickers. The data is a table of the tickers.\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Inspect the derived variables from the News Network, Plots and Stats\n",
      "\n",
      "\n",
      "Summary before Tuning: Generate the attention variable and the number of evolving.\n",
      "\n",
      "\n",
      "Summary after Tuning: This Quantlet builds network-based attention variables for SPX leads from 1992-01-01 to 2006-12-29. The attention variables are computed using the network-based approach.\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Inspect the derived variables from the News Network, Plots and Stats\n",
      "\n",
      "\n",
      "Summary before Tuning: A class for plotting a series of KDE with a given bandwidth and kernel functions.\n",
      "\n",
      "\n",
      "Summary after Tuning: Plotting Kernel Density  for a series of data points.\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Estimates and plots  empirical pricing kernels , risk neutral densities  and physical densities  of DAX 30 index return conditional on 20% , 40%  and 60%  quantiles of volatility index VDAX-NEW , and time to maturity 1 month. Local linear kernel regression is used for estimation of the conditional risk neutral density and local constant kernel regression is used for estimation of conditional physical density. Moreover, 95% confidence intervals have been calculated for PK, RND and PD conditioned by 40% quantile of VDAX-NEW and time to maturity 1 month. All results are shown on a continuously compounded 1-month period returns scale. \n",
      "\n",
      "\n",
      "Summary before Tuning: install and load pilot - specific packages\n",
      "\n",
      "\n",
      "Summary after Tuning: Calculates and plots the price kernels and volatility of the physical density of the localising Burr claim amounts for the selected price kernels and the localising Burr claim amounts.\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Estimates and plots  empirical pricing kernels , risk neutral densities  and physical densities  of DAX 30 index return conditional on time to maturity 2 months and different levels of VDAX-NEW-Subindex 2 . Local linear kernel regression is used for estimation of the conditional risk neutral density and local constant kernel regression is used for estimation of conditional physical density. EPK is obtained as a ratio of RND and PD. The panels on the right-hand side of figures depict EPK, RND, PD conditional on 20%, 50% and 80% quantiles of VDAX-NEW-Subindex 2 with 95% confidence intervals. Colors from red to blue correspond to increasing values of volatility within each interval. All results are shown on a continuously compounded 2-months period returns scale.\n",
      "\n",
      "\n",
      "Summary before Tuning: install and load packages\n",
      "\n",
      "\n",
      "Summary after Tuning: Calculates and plots the price kernels and effective volatility for the master arbeit trading kernels  for the time period from 2000-01-01 to 2012-12-31.\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Conducts a statistical analysis, heteroskedasticity and normality checks on housing data provided by Sberbank\n",
      "\n",
      "\n",
      "Summary before Tuning: This function is used to plot a histogram of the data provided by Sberbank.\n",
      "\n",
      "\n",
      "Summary after Tuning: Conducting a statistical analysis, heteroskedasticity and normality checks onhousing data provided by Sberbank\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Descriptive statistical analysis of Moscow housing data provided by Sberbank\n",
      "\n",
      "\n",
      "Summary before Tuning: Includes functions to get an overview of summary statistics for the variables and analysis of correlations of variables\n",
      "\n",
      "\n",
      "Summary after Tuning: Includes functions to get an overview of summary statistics for the variables and analysis of correlations of variables\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Descriptive statistical analysis of Moscow housing data provided by Sberbank\n",
      "\n",
      "\n",
      "Summary before Tuning: This script is used to load the raw data from the training and macro data sets. It will merge the training and macro data sets by timestamp and delete the transaction ID variable.\n",
      "\n",
      "\n",
      "Summary after Tuning: We load the raw and messy datasets, which will then be used in the models quantlet. We load the raw and messy datasets, which will then be used in the models quantlet. We load the raw and messy datasets, which will then be used in the models quantlet. We load the raw and messy datasets, which will then be used\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Contains the VIX index and the VIX future intraday analysis on the tail-event day of February 5, 2018. The next day, Credit Suisse lost 500m USD because of a structured product on the VIX. Market disruption can already be observed the day before.\n",
      "\n",
      "\n",
      "Summary before Tuning: install and load packages\n",
      "\n",
      "\n",
      "Summary after Tuning: R - This Quantlet is dedicated to visualizing the price of the VIX index for the quote price data.\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Contains the VIX index and the VIX future intraday analysis on the tail-event day of February 5, 2018. The next day, Credit Suisse lost 500m USD because of a structured product on the VIX. Market disruption can already be observed the day before.\n",
      "\n",
      "\n",
      "Summary before Tuning: restrict_to_start_end_date Select a subset of the data based on a given date and time range\n",
      "\n",
      "\n",
      "Summary after Tuning: Restricts the data to the given date and time range.\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Implements the statistics tests for the Efficient Market Hypothesis: Bartell Test, Ljung-Box Test, Variance Ratio Test, Von-Newmann Test.\n",
      "\n",
      "\n",
      "Summary before Tuning: Get the stock prices and plot the VRT and the variance of the VRT\n",
      "\n",
      "\n",
      "Summary after Tuning: This Quantlet builds the VR test for the stock prices of Yahoo Fin.\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Implements the statistics tests for the Efficient Market Hypothesis: Bartell Test, Ljung-Box Test, Variance Ratio Test, Von-Newmann Test.\n",
      "\n",
      "\n",
      "Summary before Tuning: Get the quantmod library and plot the data\n",
      "\n",
      "\n",
      "Summary after Tuning: 'This Quandl package contains the code for the quantlet. It is used to detect if the price is a random price.\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Plots time series from cryptocurrency database\n",
      "\n",
      "\n",
      "Summary before Tuning: library - library for plotting\n",
      "\n",
      "\n",
      "Summary after Tuning: This Quantlet plots time series data for different currencies of the DSHusd and LTCusd and XMrusd.\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Plots time series from cryptocurrency database\n",
      "\n",
      "\n",
      "Summary before Tuning: library - library library for plotting\n",
      "\n",
      "\n",
      "Summary after Tuning: This Quantlet plots the time series of the crypto-currencies and the time series of the crypto-currencies. The time series are plotted in a time series of time series of time series of time series of time series of time series of time series of time series of time series of time series of time series of time series of time series of time series\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Split the entire dataset in two parts. the first one consists of all observations without the last 24 ones. It is used for the training process of LSTM with the optimal parameters. The second subset consists of the last 24 observations used for the prediction of the hourly returns in the next 24 hours. Once the predictions are generated for each of the coins, they are exported from python in order to be imported in MATLAB.\n",
      "\n",
      "\n",
      "Summary before Tuning: Generate predictions for LTC BTC and DASH.\n",
      "\n",
      "\n",
      "Summary after Tuning: This Quantlet builds predictions for the LSTM model for the daily returns of the Ethereum CRIX. The predictions are saved to a.csv file.\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Define a parameter grid for the LSTM and choose the optimal set of parameters that leads to a minimal root-mean-squared error.\n",
      "\n",
      "\n",
      "Summary before Tuning: Train and predict using LSTM.\n",
      "\n",
      "\n",
      "Summary after Tuning: LSTM model to predict the minimum RMSE achieved in the model.\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Produces the estimation results using GeoCopula approach and saves it as RDate file.\n",
      "\n",
      "\n",
      "Summary before Tuning: clear variables and close windows\n",
      "\n",
      "\n",
      "Summary after Tuning: Performs a PCA for the data from the European Tracking API and plots the results.\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: UNIT 3 and UNIT 4 of DEDA. Introduce webscraping and object-oriented programming\n",
      "\n",
      "\n",
      "Summary before Tuning: Web Scraping encompasses any method allowing us to retrieve data from a webpage.\n",
      "\n",
      "\n",
      "Summary after Tuning: Web Scraping encompasses any method allowing for extracting data from websites. Requests allows us to send an HTTP request to a webpage. BeautifulSoup allows us to retrieve the desired information. Project: We wish to scrape the first page of the South China Morning Postâ€™s news website. We acquire the data about the news\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: UNIT 3 and UNIT 4 of DEDA. Introduce webscraping and object-oriented programming\n",
      "\n",
      "\n",
      "Summary before Tuning: Unpacks data from the webpage and extracts the number of US passport applications each fiscal year\n",
      "\n",
      "\n",
      "Summary after Tuning: We use json module to unpack data from webpage. We use json module to unpack data from webpage. We use pandas module to retrieve the number of US passport applications each fiscal year.\n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n",
      "__________\n",
      "Original: ['Demonstrate decision tree', 'Demonstrate applied SVM on NLP sentiment classification', 'Plot some activation functions for Neural Networks']\n",
      "\n",
      "\n",
      "Summary before Tuning: Keras training and test models.\n",
      "\n",
      "\n",
      "Summary after Tuning: Keras Model for the Hougaard S&P500 \n",
      "\n",
      "\n",
      "__________\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer = analysis_modules.scs_analyze(**analysis_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "smrdGzzNyo5n",
    "tags": []
   },
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPZowVhs+ksZzaIPG80pu3+",
   "gpuType": "A100",
   "machine_shape": "hm",
   "mount_file_id": "119XCh3Q64Zw4MGlsZQuGrMLS78o5MeWS",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "encode_code",
   "language": "python",
   "name": "encode_code"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JZal6ahJZQBU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "#%pip install protobuf==3.20.1\n",
    "%pip install transformers[torch]\n",
    "%pip install -q sentencepiece\n",
    "%pip install datasets==2.13.1\n",
    "%pip install evaluate\n",
    "%pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "O5Mlzgdzaliu",
    "tags": []
   },
   "outputs": [],
   "source": [
    "QPATH = \"Quantlet/4-qode2desc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "iaAT_m3NaEzp",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "import os\n",
    "\n",
    "if IN_COLAB:\n",
    "    os.chdir(\n",
    "        f\"/content/drive/MyDrive/ColabNotebooks/IRTG/Encode_the_Qode/Encode-the-Qode/{QPATH}\"\n",
    "    )\n",
    "else:\n",
    "    %load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "oRmp1O7SZgaI",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/RDC/zinovyee.hub/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import json\n",
    "import re\n",
    "import sys\n",
    "from IPython.display import display\n",
    "import datetime\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelWithLMHead,\n",
    "    SummarizationPipeline,\n",
    "    AutoModelForSeq2SeqLM,\n",
    ")\n",
    "from transformers import AdamW\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "import evaluate\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "kM604UgeBuMl",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/RDC/zinovyee.hub/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/RDC/zinovyee.hub/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'analysis_modules' from '/usr/net/zinovyee.hub/IRTG/MLSC/Encode-the-Qode/Quantlet/4-qode2desc/analysis_modules.py'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import analysis_modules\n",
    "\n",
    "importlib.reload(analysis_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_tokenize_preprocess(batch, tokenizer, max_input_length, max_output_length):\n",
    "\n",
    "    source = batch[\"input_sequence\"]\n",
    "    target = batch[\"output_sequence\"]\n",
    "\n",
    "    source_tokenized = tokenizer(\n",
    "        source, padding=\"max_length\", truncation=True, max_length=max_input_length\n",
    "    )\n",
    "\n",
    "    target_tokenized = tokenizer(\n",
    "        target, padding=\"max_length\", truncation=True, max_length=max_output_length\n",
    "    )\n",
    "\n",
    "    batch = {k: v for k, v in source_tokenized.items()}\n",
    "\n",
    "    # Ignore padding in the loss\n",
    "\n",
    "    batch[\"labels\"] = [\n",
    "        [-100 if token == tokenizer.pad_token_id else token for token in l]\n",
    "        for l in target_tokenized[\"input_ids\"]\n",
    "    ]\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Salesforce/codet5-base-multi-sum\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaTokenizerFast(name_or_path='Salesforce/codet5-base-multi-sum', vocab_size=32100, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'sep_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'cls_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': [AddedToken(\"<extra_id_99>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_98>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_97>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_96>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_95>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_94>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_93>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_92>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_91>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_90>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_89>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_88>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_87>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_86>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_85>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_84>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_83>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_82>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_81>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_80>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_79>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_78>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_77>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_76>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_75>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_74>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_73>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_72>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_71>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_70>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_69>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_68>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_67>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_66>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_65>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_64>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_63>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_62>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_61>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_60>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_59>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_58>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_57>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_56>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_55>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_54>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_53>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_52>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_51>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_50>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_49>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_48>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_47>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_46>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_45>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_44>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_43>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_42>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_41>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_40>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_39>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_38>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_37>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_36>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_35>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_34>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_33>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_32>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_31>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_30>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_29>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_28>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_27>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_26>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_25>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_24>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_23>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_22>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_21>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_20>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_19>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_18>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_17>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_16>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_15>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_14>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_13>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_12>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_11>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_10>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_9>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_8>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_7>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_6>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_5>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_4>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_3>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_2>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_1>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_0>\", rstrip=False, lstrip=True, single_word=False, normalized=True)]}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/RDC/zinovyee.hub/.cache/huggingface/datasets/json/default-a9640bba95be2093/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc9dbc0f52e74355b7806b8fdff81cda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files=\"train_dataset_20231021_sample0.json\",\n",
    "    field=\"data\",\n",
    "    data_dir=\"/home/RDC/zinovyee.hub/H:/zinovyee.hub/IRTG/MLSC/Encode-the-Qode/data/preprocessed/Quantlet/20231021/no_context/\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fc5c12ca21b4163b2dbb4f78ee10519",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3945 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1111 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "tokenized = train_data_txt.map(\n",
    "    lambda batch: tokenizer(batch[\"input_sequence\"], max_length=None)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#',\n",
       " 'ĠClear',\n",
       " 'Ġvariables',\n",
       " 'Ġand',\n",
       " 'Ġclose',\n",
       " 'Ġwindows',\n",
       " 'Ċ',\n",
       " 'rm',\n",
       " '(',\n",
       " 'list',\n",
       " 'Ġ=',\n",
       " 'Ġls',\n",
       " '(',\n",
       " 'all',\n",
       " 'Ġ=',\n",
       " 'ĠTRUE',\n",
       " '))',\n",
       " 'Ċ',\n",
       " 'graphics',\n",
       " '.',\n",
       " 'off',\n",
       " '()',\n",
       " 'Ċ',\n",
       " '#',\n",
       " 'ĠLoad',\n",
       " 'Ġpackages',\n",
       " 'Ċ',\n",
       " 'require',\n",
       " '(',\n",
       " 'd',\n",
       " 'ply',\n",
       " 'r',\n",
       " ')',\n",
       " 'Ċ',\n",
       " 'require',\n",
       " '(',\n",
       " 'foreach',\n",
       " ')',\n",
       " 'Ċ',\n",
       " 'library',\n",
       " '(',\n",
       " 'parallel',\n",
       " ')',\n",
       " 'Ċ',\n",
       " 'require',\n",
       " '(',\n",
       " 'do',\n",
       " 'MC',\n",
       " ')',\n",
       " 'Ċ',\n",
       " 'register',\n",
       " 'Do',\n",
       " 'MC',\n",
       " '(',\n",
       " 'parallel',\n",
       " '::',\n",
       " 'detect',\n",
       " 'Cores',\n",
       " '())',\n",
       " 'Ġ#',\n",
       " 'ĠRegister',\n",
       " 'Ġall',\n",
       " 'Ġcores',\n",
       " 'Ġfor',\n",
       " 'Ġparallel',\n",
       " 'ized',\n",
       " 'Ġcomputation',\n",
       " 'Ċ',\n",
       " '#',\n",
       " 'Ġ-----------------------------------------------------------------',\n",
       " '----------------',\n",
       " '--------',\n",
       " 'Ċ',\n",
       " '#',\n",
       " 'ĠDefine',\n",
       " 'Ġsimulation',\n",
       " 'Ġfunction',\n",
       " 'Ċ',\n",
       " 'two',\n",
       " 'Stage',\n",
       " 'Sim',\n",
       " 'Ġ=',\n",
       " 'Ġfunction',\n",
       " '(',\n",
       " 'n',\n",
       " '_',\n",
       " 'obs',\n",
       " 'Ġ=',\n",
       " 'ĠNA',\n",
       " ',',\n",
       " 'Ġcov',\n",
       " '_',\n",
       " 'error',\n",
       " 'Ġ=',\n",
       " 'ĠNA',\n",
       " ',',\n",
       " 'Ġbeta',\n",
       " '_',\n",
       " 'z',\n",
       " '1',\n",
       " 'Ġ=',\n",
       " 'ĠNA',\n",
       " ',',\n",
       " 'Ġbeta',\n",
       " '_',\n",
       " 'z',\n",
       " '2',\n",
       " 'Ġ=',\n",
       " 'ĠNA',\n",
       " '){',\n",
       " 'Ċ',\n",
       " 'Ċ',\n",
       " '#',\n",
       " 'ĠSim',\n",
       " 'ulate',\n",
       " 'Ġthe',\n",
       " 'Ġjoint',\n",
       " 'Ġdistribution',\n",
       " 'Ġof',\n",
       " 'Ġx',\n",
       " '*',\n",
       " ',',\n",
       " 'Ġx',\n",
       " '2',\n",
       " 'Ġand',\n",
       " 'Ġc',\n",
       " 'Ċ',\n",
       " 'n',\n",
       " '_',\n",
       " 'obs',\n",
       " 'Ġ=',\n",
       " 'Ġn',\n",
       " '_',\n",
       " 'obs',\n",
       " 'Ġ#',\n",
       " 'Ġnumber',\n",
       " 'Ġof',\n",
       " 'Ġobservations',\n",
       " 'Ċ',\n",
       " 'mu',\n",
       " 'Ġ=',\n",
       " 'Ġc',\n",
       " '(',\n",
       " '20',\n",
       " ',',\n",
       " 'Ġ15',\n",
       " ',',\n",
       " 'Ġ10',\n",
       " ')',\n",
       " 'Ġ#',\n",
       " 'Ġvector',\n",
       " 'Ġof',\n",
       " 'Ġmeans',\n",
       " 'Ġfor',\n",
       " 'Ġx',\n",
       " '1',\n",
       " ',',\n",
       " 'Ġx',\n",
       " '2',\n",
       " 'Ġand',\n",
       " 'Ġc',\n",
       " 'Ċ',\n",
       " 'cov',\n",
       " '12',\n",
       " 'Ġ=',\n",
       " 'Ġ0',\n",
       " '.',\n",
       " '0',\n",
       " 'Ġ#',\n",
       " 'Ġcovariance',\n",
       " 'Ġbetween',\n",
       " 'Ġx',\n",
       " '1',\n",
       " 'Ġand',\n",
       " 'Ġx',\n",
       " '2',\n",
       " 'Ċ',\n",
       " 'cov',\n",
       " '13',\n",
       " 'Ġ=',\n",
       " 'Ġcov',\n",
       " '_',\n",
       " 'error',\n",
       " 'Ġ#',\n",
       " 'Ġcovariance',\n",
       " 'Ġbetween',\n",
       " 'Ġx',\n",
       " '1',\n",
       " 'Ġand',\n",
       " 'Ġerror',\n",
       " 'Ġ(',\n",
       " 'as',\n",
       " 'Ġc',\n",
       " 'Ġis',\n",
       " 'Ġun',\n",
       " 'observed',\n",
       " ',',\n",
       " 'Ġit',\n",
       " 'Ġmoves',\n",
       " 'Ġinto',\n",
       " 'Ġthe',\n",
       " 'Ġerror',\n",
       " 'Ġterm',\n",
       " ')',\n",
       " 'Ċ',\n",
       " 'cov',\n",
       " '23',\n",
       " 'Ġ=',\n",
       " 'Ġ0',\n",
       " '.',\n",
       " '0',\n",
       " 'Ġ#',\n",
       " 'Ġcovariance',\n",
       " 'Ġbetween',\n",
       " 'Ġx',\n",
       " '2',\n",
       " 'Ġand',\n",
       " 'Ġerror',\n",
       " 'Ġ(',\n",
       " 'x',\n",
       " '2',\n",
       " 'Ġis',\n",
       " 'Ġex',\n",
       " 'ogen',\n",
       " 'ous',\n",
       " ')',\n",
       " 'Ċ',\n",
       " 'sigma',\n",
       " 'Ġ=',\n",
       " 'Ġmatrix',\n",
       " '(',\n",
       " 'c',\n",
       " '(',\n",
       " '1',\n",
       " ',',\n",
       " 'Ġcov',\n",
       " '12',\n",
       " ',',\n",
       " 'Ċ',\n",
       " 'cov',\n",
       " '12',\n",
       " ',',\n",
       " 'Ġ1',\n",
       " ',',\n",
       " 'Ġcov',\n",
       " '23',\n",
       " ',',\n",
       " 'Ċ',\n",
       " 'data',\n",
       " 'Ġ=',\n",
       " 'Ġmv',\n",
       " 'r',\n",
       " 'norm',\n",
       " '(',\n",
       " 'n',\n",
       " 'Ġ=',\n",
       " 'Ġn',\n",
       " '_',\n",
       " 'obs',\n",
       " ',',\n",
       " 'Ġmu',\n",
       " 'Ġ=',\n",
       " 'Ġmu',\n",
       " ',',\n",
       " 'ĠS',\n",
       " 'igma',\n",
       " 'Ġ=',\n",
       " 'Ġsigma',\n",
       " ')',\n",
       " 'Ċ',\n",
       " '#',\n",
       " 'ĠSim',\n",
       " 'ulate',\n",
       " 'Ġthe',\n",
       " 'Ġinstruments',\n",
       " 'Ċ',\n",
       " 'x',\n",
       " '1',\n",
       " 'Star',\n",
       " 'Ġ=',\n",
       " 'Ġdata',\n",
       " '[',\n",
       " ',',\n",
       " 'Ġ1',\n",
       " ']',\n",
       " 'Ġ#',\n",
       " 'Ġtrue',\n",
       " 'Ġx',\n",
       " '1',\n",
       " 'Ċ',\n",
       " 'c',\n",
       " 'Ġ=',\n",
       " 'Ġdata',\n",
       " '[',\n",
       " ',',\n",
       " 'Ġ3',\n",
       " ']',\n",
       " 'Ġ#',\n",
       " 'Ġun',\n",
       " 'observed',\n",
       " 'Ġcovar',\n",
       " 'iate',\n",
       " 'Ġthat',\n",
       " \"'s\",\n",
       " 'Ġcor',\n",
       " 'related',\n",
       " 'Ġwith',\n",
       " 'Ġx',\n",
       " 'Star',\n",
       " 'Ċ',\n",
       " 'z',\n",
       " '1',\n",
       " 'Ġ=',\n",
       " 'Ġr',\n",
       " 'norm',\n",
       " '(',\n",
       " 'n',\n",
       " '_',\n",
       " 'obs',\n",
       " ')',\n",
       " 'Ċ',\n",
       " 'x',\n",
       " '1',\n",
       " 'Ġ=',\n",
       " 'Ġx',\n",
       " '1',\n",
       " 'Star',\n",
       " 'Ġ+',\n",
       " 'Ġbeta',\n",
       " '_',\n",
       " 'z',\n",
       " '1',\n",
       " 'Ġ*',\n",
       " 'Ġz',\n",
       " '1',\n",
       " 'Ġ+',\n",
       " 'Ġbeta',\n",
       " '_',\n",
       " 'z',\n",
       " '2',\n",
       " 'Ġ*',\n",
       " 'Ġz',\n",
       " '2',\n",
       " 'Ġ#',\n",
       " 'Ġobserved',\n",
       " '/',\n",
       " 'end',\n",
       " 'ogen',\n",
       " 'ous',\n",
       " 'Ġx',\n",
       " 'Ċ',\n",
       " '#',\n",
       " 'ĠSim',\n",
       " 'ulate',\n",
       " 'Ġoutcome',\n",
       " 'Ġy',\n",
       " 'Ċ',\n",
       " 'beta',\n",
       " '_',\n",
       " '0',\n",
       " 'Ġ=',\n",
       " 'Ġ1',\n",
       " '.',\n",
       " '0',\n",
       " 'Ġ#',\n",
       " 'ĠInter',\n",
       " 'cept',\n",
       " 'Ċ',\n",
       " 'beta',\n",
       " '_',\n",
       " 'x',\n",
       " '1',\n",
       " 'Ġ=',\n",
       " 'Ġ0',\n",
       " '.',\n",
       " '7',\n",
       " 'Ġ#',\n",
       " 'Ġweight',\n",
       " 'Ġfor',\n",
       " 'Ġx',\n",
       " '1',\n",
       " 'Ċ',\n",
       " 'eps',\n",
       " 'Ġ=',\n",
       " 'Ġr',\n",
       " 'norm',\n",
       " '(',\n",
       " 'n',\n",
       " 'Ġ=',\n",
       " 'Ġn',\n",
       " '_',\n",
       " 'obs',\n",
       " ',',\n",
       " 'Ġmean',\n",
       " 'Ġ=',\n",
       " 'Ġ0',\n",
       " ',',\n",
       " 'Ġsd',\n",
       " 'Ġ=',\n",
       " 'Ġ1',\n",
       " ')',\n",
       " 'Ġ#',\n",
       " 'ĠStandard',\n",
       " 'Ġnormal',\n",
       " 'Ġerror',\n",
       " 'Ġterm',\n",
       " 'Ċ',\n",
       " 'y',\n",
       " 'Ġ=',\n",
       " 'Ġbeta',\n",
       " '_',\n",
       " '0',\n",
       " 'Ġ*',\n",
       " 'Ġ1',\n",
       " 'Ġ+',\n",
       " 'Ġbeta',\n",
       " '_',\n",
       " 'x',\n",
       " '1',\n",
       " 'Ġ*',\n",
       " 'Ġx',\n",
       " '1',\n",
       " 'Ġ+',\n",
       " 'Ġbeta',\n",
       " '_',\n",
       " 'x',\n",
       " '2',\n",
       " 'Ġ*',\n",
       " 'Ġx',\n",
       " '2',\n",
       " 'Ġ+',\n",
       " 'Ġbeta',\n",
       " '_',\n",
       " 'c',\n",
       " 'Ġ*',\n",
       " 'Ġc',\n",
       " 'Ġ+',\n",
       " 'Ġeps',\n",
       " 'Ċ',\n",
       " '#',\n",
       " 'ĠEst',\n",
       " 'imate',\n",
       " 'Ġeffect',\n",
       " 'Ġof',\n",
       " 'Ġx',\n",
       " 'Ġon',\n",
       " 'Ġy',\n",
       " 'Ċ',\n",
       " 'm',\n",
       " '_',\n",
       " 'true',\n",
       " 'Ġ=',\n",
       " 'Ġlm',\n",
       " '(',\n",
       " 'y',\n",
       " 'Ġ~',\n",
       " 'Ġ1',\n",
       " 'Ġ+',\n",
       " 'Ġx',\n",
       " '1',\n",
       " 'Ġ+',\n",
       " 'Ġx',\n",
       " '2',\n",
       " 'Ġ+',\n",
       " 'Ġc',\n",
       " ')',\n",
       " 'Ġ#',\n",
       " 'Ġif',\n",
       " 'Ġc',\n",
       " 'Ġwere',\n",
       " 'Ġknown',\n",
       " 'Ċ',\n",
       " '#',\n",
       " 'ĠRe',\n",
       " 'cover',\n",
       " 'Ġtrue',\n",
       " 'Ġeffect',\n",
       " 'Ġof',\n",
       " 'Ġx',\n",
       " 'Ġon',\n",
       " 'Ġy',\n",
       " 'Ġusing',\n",
       " 'Ġ2',\n",
       " 'S',\n",
       " 'LS',\n",
       " 'Ġestimation',\n",
       " 'Ċ',\n",
       " 'x',\n",
       " '1',\n",
       " '_',\n",
       " '2',\n",
       " 'sl',\n",
       " 's',\n",
       " 'Ġ=',\n",
       " 'Ġfitted',\n",
       " '(',\n",
       " 'lm',\n",
       " '(',\n",
       " 'x',\n",
       " '1',\n",
       " 'Ġ~',\n",
       " 'Ġ1',\n",
       " 'Ġ+',\n",
       " 'Ġz',\n",
       " '1',\n",
       " 'Ġ+',\n",
       " 'Ġz',\n",
       " '2',\n",
       " 'Ġ+',\n",
       " 'Ġx',\n",
       " '2',\n",
       " '))',\n",
       " 'Ġ#',\n",
       " 'Ġx',\n",
       " '1',\n",
       " '_',\n",
       " '2',\n",
       " 'sl',\n",
       " 's',\n",
       " 'Ġis',\n",
       " 'Ġthe',\n",
       " 'Ġinstrument',\n",
       " 'Ġfor',\n",
       " 'Ġx',\n",
       " 'Ċ',\n",
       " 'm',\n",
       " '_',\n",
       " '2',\n",
       " 'S',\n",
       " 'LS',\n",
       " 'Ġ=',\n",
       " 'Ġlm',\n",
       " '(',\n",
       " 'y',\n",
       " 'Ġ~',\n",
       " 'Ġ1',\n",
       " 'Ġ+',\n",
       " 'Ġx',\n",
       " '1',\n",
       " '_',\n",
       " '2',\n",
       " 'sl',\n",
       " 's',\n",
       " 'Ġ+',\n",
       " 'Ġx',\n",
       " '2',\n",
       " ')',\n",
       " 'Ċ',\n",
       " '#',\n",
       " 'ĠCollect',\n",
       " 'Ġresults',\n",
       " 'Ċ',\n",
       " 'models',\n",
       " 'Ġ=',\n",
       " 'Ġr',\n",
       " 'bind',\n",
       " '(',\n",
       " 'tid',\n",
       " 'y',\n",
       " '(',\n",
       " 'm',\n",
       " '_',\n",
       " 'true',\n",
       " '),',\n",
       " 'Ġtidy',\n",
       " '(',\n",
       " 'm',\n",
       " '_',\n",
       " 'bi',\n",
       " 'ased',\n",
       " '),',\n",
       " 'Ġtidy',\n",
       " '(',\n",
       " 'm',\n",
       " '_',\n",
       " '2',\n",
       " 'S',\n",
       " 'LS',\n",
       " '))',\n",
       " 'Ċ',\n",
       " 'models',\n",
       " '$',\n",
       " 'cov',\n",
       " '_',\n",
       " 'error',\n",
       " 'Ġ=',\n",
       " 'Ġcov',\n",
       " '_',\n",
       " 'error',\n",
       " 'Ċ',\n",
       " 'models',\n",
       " '$',\n",
       " 'n',\n",
       " '_',\n",
       " 'obs',\n",
       " 'Ġ=',\n",
       " 'Ġn',\n",
       " '_',\n",
       " 'obs',\n",
       " 'Ċ',\n",
       " 'models',\n",
       " '$',\n",
       " 'beta',\n",
       " '_',\n",
       " 'z',\n",
       " '1',\n",
       " 'Ġ=',\n",
       " 'Ġbeta',\n",
       " '_',\n",
       " 'z',\n",
       " '1',\n",
       " 'Ċ',\n",
       " 'models',\n",
       " '$',\n",
       " 'model',\n",
       " 'Ġ=',\n",
       " 'Ġc',\n",
       " '(',\n",
       " 'rep',\n",
       " '(\"',\n",
       " 'True',\n",
       " 'Ġmodel',\n",
       " '\",',\n",
       " 'Ġ4',\n",
       " '),',\n",
       " 'Ġrep',\n",
       " '(\"',\n",
       " 'Bi',\n",
       " 'ased',\n",
       " 'Ġmodel',\n",
       " '\",',\n",
       " 'Ġ3',\n",
       " '),',\n",
       " 'Ġrep',\n",
       " '(\"',\n",
       " '2',\n",
       " 'S',\n",
       " 'LS',\n",
       " '\",',\n",
       " 'Ġ3',\n",
       " '))',\n",
       " 'Ċ',\n",
       " 'return',\n",
       " '(',\n",
       " 'models',\n",
       " ')',\n",
       " 'Ċ',\n",
       " '}',\n",
       " 'Ċ',\n",
       " '#',\n",
       " 'ĠPerform',\n",
       " 'ance',\n",
       " 'Ġcomparison',\n",
       " 'Ġbetween',\n",
       " 'Ġforeach',\n",
       " 'Ġand',\n",
       " 'Ġm',\n",
       " 'cl',\n",
       " 'apply',\n",
       " 'Ċ',\n",
       " '#',\n",
       " 'ĠFor',\n",
       " 'Ġusing',\n",
       " 'Ġforeach',\n",
       " 'Ġand',\n",
       " 'Ġm',\n",
       " 'cl',\n",
       " 'apply',\n",
       " ',',\n",
       " 'Ġonly',\n",
       " 'Ġone',\n",
       " 'Ġargument',\n",
       " 'Ġcan',\n",
       " 'Ġvary',\n",
       " '.',\n",
       " 'ĠTherefore',\n",
       " ',',\n",
       " 'ĠI',\n",
       " 'Ġchoose',\n",
       " 'Ġto',\n",
       " 'Ġfix',\n",
       " 'Ġeverything',\n",
       " 'Ġbut',\n",
       " 'Ġn',\n",
       " '_',\n",
       " 'obs',\n",
       " '.',\n",
       " 'Ċ',\n",
       " 'two',\n",
       " 'Stage',\n",
       " 'Sim',\n",
       " '2',\n",
       " 'Ġ<-',\n",
       " 'Ġfunction',\n",
       " '(',\n",
       " 'n',\n",
       " '){',\n",
       " 'return',\n",
       " '(',\n",
       " 'two',\n",
       " 'Stage',\n",
       " 'Sim',\n",
       " '(',\n",
       " 'n',\n",
       " '_',\n",
       " 'obs',\n",
       " 'Ġ=',\n",
       " 'Ġn',\n",
       " ',',\n",
       " 'Ġcov',\n",
       " '_',\n",
       " 'error',\n",
       " 'Ġ=',\n",
       " 'Ġ0',\n",
       " '.',\n",
       " '5',\n",
       " ',',\n",
       " 'Ġbeta',\n",
       " '_',\n",
       " 'z',\n",
       " '1',\n",
       " 'Ġ=',\n",
       " 'Ġ10',\n",
       " ',',\n",
       " 'Ġbeta',\n",
       " '_',\n",
       " 'z',\n",
       " '2',\n",
       " 'Ġ=',\n",
       " 'Ġ10',\n",
       " '))',\n",
       " '}',\n",
       " 'Ċ',\n",
       " '#',\n",
       " 'ĠOptions',\n",
       " 'Ġfor',\n",
       " 'Ġsim',\n",
       " 'ulations',\n",
       " 'Ċ',\n",
       " 'set',\n",
       " '.',\n",
       " 'seed',\n",
       " '(',\n",
       " '12',\n",
       " '35',\n",
       " ')',\n",
       " 'Ċ',\n",
       " 'n',\n",
       " '_',\n",
       " 'min',\n",
       " 'Ġ=',\n",
       " 'Ġ100',\n",
       " 'Ċ',\n",
       " 'n',\n",
       " '_',\n",
       " 'max',\n",
       " 'Ġ=',\n",
       " 'Ġ100000',\n",
       " 'Ċ',\n",
       " 'n',\n",
       " '_',\n",
       " 'by',\n",
       " 'Ġ=',\n",
       " 'Ġ100',\n",
       " 'Ċ',\n",
       " 'n',\n",
       " '_',\n",
       " 'obs',\n",
       " '_',\n",
       " 'vec',\n",
       " 'Ġ=',\n",
       " 'Ġseq',\n",
       " '(',\n",
       " 'n',\n",
       " '_',\n",
       " 'min',\n",
       " ',',\n",
       " 'Ġn',\n",
       " '_',\n",
       " 'max',\n",
       " ',',\n",
       " 'Ġn',\n",
       " '_',\n",
       " 'by',\n",
       " ')',\n",
       " 'Ċ',\n",
       " '#',\n",
       " 'Ġl',\n",
       " 'apply',\n",
       " 'Ġwith',\n",
       " 'Ġ1',\n",
       " 'Ġcore',\n",
       " 'Ċ',\n",
       " 'lap',\n",
       " 'ply',\n",
       " '_',\n",
       " '1',\n",
       " 'core',\n",
       " 'Ġ=',\n",
       " 'Ġsystem',\n",
       " '.',\n",
       " 'time',\n",
       " '({',\n",
       " 'Ċ',\n",
       " 'ply',\n",
       " 'r',\n",
       " '::',\n",
       " 'r',\n",
       " 'bind',\n",
       " '.',\n",
       " 'fill',\n",
       " '(',\n",
       " 'lap',\n",
       " 'ply',\n",
       " '(',\n",
       " 'n',\n",
       " '_',\n",
       " 'obs',\n",
       " '_',\n",
       " 'vec',\n",
       " ',',\n",
       " 'Ġtwo',\n",
       " 'Stage',\n",
       " 'Sim',\n",
       " '2',\n",
       " '))',\n",
       " 'Ċ',\n",
       " '})',\n",
       " 'Ċ',\n",
       " '#',\n",
       " 'Ġm',\n",
       " 'cl',\n",
       " 'apply',\n",
       " 'Ġwith',\n",
       " 'Ġ2',\n",
       " 'Ġcores',\n",
       " 'Ċ',\n",
       " 'm',\n",
       " 'cl',\n",
       " 'apply',\n",
       " '_',\n",
       " '2',\n",
       " 'cores',\n",
       " 'Ġ=',\n",
       " 'Ġsystem',\n",
       " '.',\n",
       " 'time',\n",
       " '({',\n",
       " 'Ċ',\n",
       " 'ply',\n",
       " 'r',\n",
       " '::',\n",
       " 'r',\n",
       " 'bind',\n",
       " '.',\n",
       " 'fill',\n",
       " '(',\n",
       " 'm',\n",
       " 'cl',\n",
       " 'apply',\n",
       " '(',\n",
       " 'n',\n",
       " '_',\n",
       " 'obs',\n",
       " '_',\n",
       " 'vec',\n",
       " ',',\n",
       " 'Ġtwo',\n",
       " 'Stage',\n",
       " 'Sim',\n",
       " '2',\n",
       " ',',\n",
       " 'Ġmc',\n",
       " '.',\n",
       " 'cores',\n",
       " 'Ġ=',\n",
       " 'Ġ2',\n",
       " '))',\n",
       " 'Ċ',\n",
       " '#',\n",
       " 'Ġm',\n",
       " 'cl',\n",
       " 'apply',\n",
       " 'Ġwith',\n",
       " 'Ġ4',\n",
       " 'Ġcores',\n",
       " 'Ċ',\n",
       " 'm',\n",
       " 'cl',\n",
       " 'apply',\n",
       " '_',\n",
       " '4',\n",
       " 'cores',\n",
       " 'Ġ=',\n",
       " 'Ġsystem',\n",
       " '.',\n",
       " 'time',\n",
       " '({',\n",
       " 'Ċ',\n",
       " 'ply',\n",
       " 'r',\n",
       " '::',\n",
       " 'r',\n",
       " 'bind',\n",
       " '.',\n",
       " 'fill',\n",
       " '(',\n",
       " 'm',\n",
       " 'cl',\n",
       " 'apply',\n",
       " '(',\n",
       " 'n',\n",
       " '_',\n",
       " 'obs',\n",
       " '_',\n",
       " 'vec',\n",
       " ',',\n",
       " 'Ġtwo',\n",
       " 'Stage',\n",
       " 'Sim',\n",
       " '2',\n",
       " ',',\n",
       " 'Ġmc',\n",
       " '.',\n",
       " 'cores',\n",
       " 'Ġ=',\n",
       " 'Ġ4',\n",
       " '))',\n",
       " 'Ċ',\n",
       " '#',\n",
       " 'Ġforeach',\n",
       " 'Ġwith',\n",
       " 'Ġ2',\n",
       " 'Ġcores',\n",
       " 'Ċ',\n",
       " 'register',\n",
       " 'Do',\n",
       " 'SEQ',\n",
       " '()',\n",
       " 'Ġ#',\n",
       " 'Ġfirst',\n",
       " ',',\n",
       " 'Ġgo',\n",
       " 'Ġback',\n",
       " 'Ġto',\n",
       " 'Ġsequential',\n",
       " 'Ġcomputation',\n",
       " 'Ċ',\n",
       " 'register',\n",
       " 'Do',\n",
       " 'MC',\n",
       " '(',\n",
       " '2',\n",
       " ')',\n",
       " 'Ġ#',\n",
       " 'Ġregister',\n",
       " 'Ġ2',\n",
       " 'Ġcores',\n",
       " 'Ġfor',\n",
       " 'Ġforeach',\n",
       " 'Ċ',\n",
       " '#',\n",
       " 'Ġrun',\n",
       " 'Ġsimulation',\n",
       " 'Ġusing',\n",
       " 'Ġparallel',\n",
       " 'ized',\n",
       " 'Ġfor',\n",
       " '-',\n",
       " 'loop',\n",
       " 'Ċ',\n",
       " 'foreach',\n",
       " '_',\n",
       " '2',\n",
       " 'cores',\n",
       " 'Ġ=',\n",
       " 'Ġsystem',\n",
       " '.',\n",
       " 'time',\n",
       " '({',\n",
       " 'Ċ',\n",
       " 'foreach',\n",
       " '(',\n",
       " 'i',\n",
       " 'Ġ=',\n",
       " 'Ġseq',\n",
       " '(',\n",
       " 'n',\n",
       " '_',\n",
       " 'obs',\n",
       " '_',\n",
       " 'vec',\n",
       " '),',\n",
       " 'Ġ.',\n",
       " 'combine',\n",
       " '=',\n",
       " 'r',\n",
       " 'bind',\n",
       " ')',\n",
       " 'Ġ%',\n",
       " ...]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(train_data_txt[\"input_sequence\"][100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"close all\\ngraph = 1;\\n% Open & read txt file with names of files to work with\\nfilelist = 'filelist.txt';\\nfilenames = textread(filelist,'%s');\\nnfiles = length(filenames);\\ndisp([filelist, ' has ', num2str(nfiles), ' files.']);\\nfigcount = 0;\\n% Processing txt files\\nfor i = 1:nfiles\\n% checking what kind of file (single or mapping) it is\\nfilename = char(filenames(i));\\nfprintf('%s: single file\\\\n', filename);\\n[x,y] = textread(filename,'%f%f','headerlines',1); % Reads in txt file\\ndata = [x y];\\nFS = process(data, graph, filename);\\nfigcount = figcount + 1;\\n% pausing program to look at figures\\nif graph == 1 && figcount > 50\\nfprintf('PAUSED after displaying %d figures\\\\n\\\\n', figcount);\\npause\\nfigcount = 0;\\nend\\nend\\n%------------------------------------------------------\\n% Pre-processing data file\\nfunction FS = process(data, graph, filename)\\ndata = sortrows(data,1); % sort rows in ascending order\\nrawdata = data;\\nmedfiltdata(:,2) = medfilt1(data(:,2),5); % applies median filter to data\\ndata(:,2) = wden(medfiltdata(:,2), 'sqtwolog', 's', 'sln', 5, 'sym8'); %denoises spectra\\nresidual = data(:,2) - rawdata(:,2);\\nmeanres = mean(residual);\\ncutoff = 3;\\nbadindex = find(abs(residual) > cutoff*stdres);\\nfigure(101);\\nplot(data(:,1),residual(:),'b',data(badindex,1),residual(badindex),'ro'); hold on\\nplot(data(:,1),-cutoff*stdres,'k',data(:,1),cutoff*stdres,'k')\\naxis tight\\nylabel('Residual')\\ntitle(['Cutoff of ',num2str(cutoff),' standard deviations']);\\nclose\\n[yfit_final, chosen_order, FS] = (data);\\nunique(chosen_order);\\norder = FS(2);\\nysub_final = data(:,2) - yfit_final;\\nysub_final(ysub_final < 0) = 0;\\ny_normalized = normalize(ysub_final);\\nBGS_data = [data(:,1) y_normalized];\\ndatastore(BGS_data, filename); % saves to txt file\\nysub_final_weighted = data(:,2) - yfit_final_weighted;\\nysub_final_weighted(ysub_final_weighted < 0) = 0;\\ny_normalized_weighted = normalize(ysub_final_weighted);\\nBGS_data_weighted = [data(:,1) y_normalized_weighted];\\nif graph\\nfigure; handle = gcf;\\nset(handle,'Position',[100 80 1100 650]) % maximizes window\\nh(1) = subplot(211);\\nplot(data(:,1),data(:,2),'linewidth',2.5); hold on % plots denoised spectra\\ntitle(filename);\\nplot(data(:,1),yfit_final,'k','linewidth',2);\\naxis tight;\\nh(2) = subplot(212);\\nplot(data(:,1),y_normalized,'k','linewidth',2); hold on\\ntitle('Subtracted Normalized Spectrum');\\n\\nlinkaxes(h,'x');\\n% process\\nfunction [yfit_final, chosen_order, FS] = _weighted(data)\\ninitial_order = 4;\\ninitial_constraint = 0;\\n[ysub_init yfit_init] = backsub_weighted(data,initial_order,initial_constraint); % modified polynomial fit\\nFSratio = (max(yfit_init)-min(yfit_init))/(max(ysub_init)-min(ysub_init));\\norder = findorder(FSratio);\\nFS = [FSratio order];\\nYFITS = zeros(size(yfit_init,1),4);\\ndelta = 1;\\nfor constraint = 0:1\\nfor z = order:delta:order+delta\\ncol = col + 1;\\nif z == initial_order && constraint == initial_constraint\\nYFITS(:,col) = yfit_init;\\nelse\\n[ysub1 yfit1] = backsub_weighted(data,z,constraint);\\nYFITS(:,col) = yfit1;\\nend\\n[yfit_final, chosen_fit] = max(YFITS,[],2);\\nchosen_order = order + delta - delta*rem(chosen_fit,size(YFITS,2)/2);\\n\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized[\"input_sequence\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07dfea3df2d046569bdf9c92f88ba5d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3945 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'encoder_max_length' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/RDC/zinovyee.hub/H:/zinovyee.hub/IRTG/MLSC/Encode-the-Qode/Quantlet/4-qode2desc/qode2desc-Copy1.ipynb Zelle 10\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsunflower.wiwi.hu-berlin.de/home/RDC/zinovyee.hub/H%3A/zinovyee.hub/IRTG/MLSC/Encode-the-Qode/Quantlet/4-qode2desc/qode2desc-Copy1.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m train_data_txt \u001b[39m=\u001b[39m train_dataset[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bsunflower.wiwi.hu-berlin.de/home/RDC/zinovyee.hub/H%3A/zinovyee.hub/IRTG/MLSC/Encode-the-Qode/Quantlet/4-qode2desc/qode2desc-Copy1.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m train_data \u001b[39m=\u001b[39m train_data_txt\u001b[39m.\u001b[39;49mmap(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsunflower.wiwi.hu-berlin.de/home/RDC/zinovyee.hub/H%3A/zinovyee.hub/IRTG/MLSC/Encode-the-Qode/Quantlet/4-qode2desc/qode2desc-Copy1.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mlambda\u001b[39;49;00m batch: batch_tokenize_preprocess(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsunflower.wiwi.hu-berlin.de/home/RDC/zinovyee.hub/H%3A/zinovyee.hub/IRTG/MLSC/Encode-the-Qode/Quantlet/4-qode2desc/qode2desc-Copy1.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m         batch,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsunflower.wiwi.hu-berlin.de/home/RDC/zinovyee.hub/H%3A/zinovyee.hub/IRTG/MLSC/Encode-the-Qode/Quantlet/4-qode2desc/qode2desc-Copy1.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m         tokenizer\u001b[39m=\u001b[39;49mtokenizer,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsunflower.wiwi.hu-berlin.de/home/RDC/zinovyee.hub/H%3A/zinovyee.hub/IRTG/MLSC/Encode-the-Qode/Quantlet/4-qode2desc/qode2desc-Copy1.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m         max_input_length\u001b[39m=\u001b[39;49mencoder_max_length,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsunflower.wiwi.hu-berlin.de/home/RDC/zinovyee.hub/H%3A/zinovyee.hub/IRTG/MLSC/Encode-the-Qode/Quantlet/4-qode2desc/qode2desc-Copy1.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m         max_output_length\u001b[39m=\u001b[39;49mdecoder_max_length,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsunflower.wiwi.hu-berlin.de/home/RDC/zinovyee.hub/H%3A/zinovyee.hub/IRTG/MLSC/Encode-the-Qode/Quantlet/4-qode2desc/qode2desc-Copy1.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     ),\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsunflower.wiwi.hu-berlin.de/home/RDC/zinovyee.hub/H%3A/zinovyee.hub/IRTG/MLSC/Encode-the-Qode/Quantlet/4-qode2desc/qode2desc-Copy1.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49m\u001b[39m8\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsunflower.wiwi.hu-berlin.de/home/RDC/zinovyee.hub/H%3A/zinovyee.hub/IRTG/MLSC/Encode-the-Qode/Quantlet/4-qode2desc/qode2desc-Copy1.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     batched\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsunflower.wiwi.hu-berlin.de/home/RDC/zinovyee.hub/H%3A/zinovyee.hub/IRTG/MLSC/Encode-the-Qode/Quantlet/4-qode2desc/qode2desc-Copy1.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     remove_columns\u001b[39m=\u001b[39;49mtrain_data_txt\u001b[39m.\u001b[39;49mcolumn_names,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsunflower.wiwi.hu-berlin.de/home/RDC/zinovyee.hub/H%3A/zinovyee.hub/IRTG/MLSC/Encode-the-Qode/Quantlet/4-qode2desc/qode2desc-Copy1.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/encode_code/lib/python3.9/site-packages/datasets/arrow_dataset.py:580\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    579\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 580\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    581\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    582\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[1;32m    583\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/encode_code/lib/python3.9/site-packages/datasets/arrow_dataset.py:545\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    538\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[1;32m    539\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[1;32m    540\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[1;32m    541\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[1;32m    542\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[1;32m    543\u001b[0m }\n\u001b[1;32m    544\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 545\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    546\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    547\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/encode_code/lib/python3.9/site-packages/datasets/arrow_dataset.py:3087\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3079\u001b[0m \u001b[39mif\u001b[39;00m transformed_dataset \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3080\u001b[0m     \u001b[39mwith\u001b[39;00m logging\u001b[39m.\u001b[39mtqdm(\n\u001b[1;32m   3081\u001b[0m         disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled(),\n\u001b[1;32m   3082\u001b[0m         unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m examples\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3085\u001b[0m         desc\u001b[39m=\u001b[39mdesc \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mMap\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   3086\u001b[0m     ) \u001b[39mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3087\u001b[0m         \u001b[39mfor\u001b[39;00m rank, done, content \u001b[39min\u001b[39;00m Dataset\u001b[39m.\u001b[39m_map_single(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3088\u001b[0m             \u001b[39mif\u001b[39;00m done:\n\u001b[1;32m   3089\u001b[0m                 shards_done \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/encode_code/lib/python3.9/site-packages/datasets/arrow_dataset.py:3463\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3459\u001b[0m indices \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\n\u001b[1;32m   3460\u001b[0m     \u001b[39mrange\u001b[39m(\u001b[39m*\u001b[39m(\u001b[39mslice\u001b[39m(i, i \u001b[39m+\u001b[39m batch_size)\u001b[39m.\u001b[39mindices(shard\u001b[39m.\u001b[39mnum_rows)))\n\u001b[1;32m   3461\u001b[0m )  \u001b[39m# Something simpler?\u001b[39;00m\n\u001b[1;32m   3462\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3463\u001b[0m     batch \u001b[39m=\u001b[39m apply_function_on_filtered_inputs(\n\u001b[1;32m   3464\u001b[0m         batch,\n\u001b[1;32m   3465\u001b[0m         indices,\n\u001b[1;32m   3466\u001b[0m         check_same_num_examples\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(shard\u001b[39m.\u001b[39;49mlist_indexes()) \u001b[39m>\u001b[39;49m \u001b[39m0\u001b[39;49m,\n\u001b[1;32m   3467\u001b[0m         offset\u001b[39m=\u001b[39;49moffset,\n\u001b[1;32m   3468\u001b[0m     )\n\u001b[1;32m   3469\u001b[0m \u001b[39mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[1;32m   3470\u001b[0m     \u001b[39mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[1;32m   3471\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3472\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/encode_code/lib/python3.9/site-packages/datasets/arrow_dataset.py:3344\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3342\u001b[0m \u001b[39mif\u001b[39;00m with_rank:\n\u001b[1;32m   3343\u001b[0m     additional_args \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (rank,)\n\u001b[0;32m-> 3344\u001b[0m processed_inputs \u001b[39m=\u001b[39m function(\u001b[39m*\u001b[39;49mfn_args, \u001b[39m*\u001b[39;49madditional_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfn_kwargs)\n\u001b[1;32m   3345\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3346\u001b[0m     processed_inputs \u001b[39m=\u001b[39m {\n\u001b[1;32m   3347\u001b[0m         k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m processed_inputs\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m processed_inputs\u001b[39m.\u001b[39mkeys_to_format\n\u001b[1;32m   3348\u001b[0m     }\n",
      "\u001b[1;32m/home/RDC/zinovyee.hub/H:/zinovyee.hub/IRTG/MLSC/Encode-the-Qode/Quantlet/4-qode2desc/qode2desc-Copy1.ipynb Zelle 10\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsunflower.wiwi.hu-berlin.de/home/RDC/zinovyee.hub/H%3A/zinovyee.hub/IRTG/MLSC/Encode-the-Qode/Quantlet/4-qode2desc/qode2desc-Copy1.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m train_data_txt \u001b[39m=\u001b[39m train_dataset[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsunflower.wiwi.hu-berlin.de/home/RDC/zinovyee.hub/H%3A/zinovyee.hub/IRTG/MLSC/Encode-the-Qode/Quantlet/4-qode2desc/qode2desc-Copy1.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m train_data \u001b[39m=\u001b[39m train_data_txt\u001b[39m.\u001b[39mmap(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsunflower.wiwi.hu-berlin.de/home/RDC/zinovyee.hub/H%3A/zinovyee.hub/IRTG/MLSC/Encode-the-Qode/Quantlet/4-qode2desc/qode2desc-Copy1.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mlambda\u001b[39;00m batch: batch_tokenize_preprocess(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsunflower.wiwi.hu-berlin.de/home/RDC/zinovyee.hub/H%3A/zinovyee.hub/IRTG/MLSC/Encode-the-Qode/Quantlet/4-qode2desc/qode2desc-Copy1.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m         batch,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsunflower.wiwi.hu-berlin.de/home/RDC/zinovyee.hub/H%3A/zinovyee.hub/IRTG/MLSC/Encode-the-Qode/Quantlet/4-qode2desc/qode2desc-Copy1.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m         tokenizer\u001b[39m=\u001b[39mtokenizer,\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bsunflower.wiwi.hu-berlin.de/home/RDC/zinovyee.hub/H%3A/zinovyee.hub/IRTG/MLSC/Encode-the-Qode/Quantlet/4-qode2desc/qode2desc-Copy1.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m         max_input_length\u001b[39m=\u001b[39mencoder_max_length,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsunflower.wiwi.hu-berlin.de/home/RDC/zinovyee.hub/H%3A/zinovyee.hub/IRTG/MLSC/Encode-the-Qode/Quantlet/4-qode2desc/qode2desc-Copy1.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m         max_output_length\u001b[39m=\u001b[39mdecoder_max_length,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsunflower.wiwi.hu-berlin.de/home/RDC/zinovyee.hub/H%3A/zinovyee.hub/IRTG/MLSC/Encode-the-Qode/Quantlet/4-qode2desc/qode2desc-Copy1.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     ),\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsunflower.wiwi.hu-berlin.de/home/RDC/zinovyee.hub/H%3A/zinovyee.hub/IRTG/MLSC/Encode-the-Qode/Quantlet/4-qode2desc/qode2desc-Copy1.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     batch_size\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsunflower.wiwi.hu-berlin.de/home/RDC/zinovyee.hub/H%3A/zinovyee.hub/IRTG/MLSC/Encode-the-Qode/Quantlet/4-qode2desc/qode2desc-Copy1.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     batched\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsunflower.wiwi.hu-berlin.de/home/RDC/zinovyee.hub/H%3A/zinovyee.hub/IRTG/MLSC/Encode-the-Qode/Quantlet/4-qode2desc/qode2desc-Copy1.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     remove_columns\u001b[39m=\u001b[39mtrain_data_txt\u001b[39m.\u001b[39mcolumn_names,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsunflower.wiwi.hu-berlin.de/home/RDC/zinovyee.hub/H%3A/zinovyee.hub/IRTG/MLSC/Encode-the-Qode/Quantlet/4-qode2desc/qode2desc-Copy1.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'encoder_max_length' is not defined"
     ]
    }
   ],
   "source": [
    "train_data_txt = train_dataset[\"train\"]\n",
    "train_data = train_data_txt.map(\n",
    "    lambda batch: batch_tokenize_preprocess(\n",
    "        batch,\n",
    "        tokenizer=tokenizer,\n",
    "        max_input_length=encoder_max_length,\n",
    "        max_output_length=decoder_max_length,\n",
    "    ),\n",
    "    batch_size=8,\n",
    "    batched=True,\n",
    "    remove_columns=train_data_txt.column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "                                \n",
    "   \n",
    "        \n",
    "    validation_data_txt = test_dataset['train']\n",
    "    \n",
    "   \n",
    "    \n",
    "    validation_data = validation_data_txt.map(\n",
    "        lambda batch: batch_tokenize_preprocess(\n",
    "            batch, \n",
    "            tokenizer=tokenizer,\n",
    "            max_input_length=encoder_max_length,\n",
    "            max_output_length=decoder_max_length\n",
    "        ),\n",
    "        batched=True,\n",
    "        remove_columns=validation_data_txt.column_names,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_analyze(analysis_name: str,\n",
    "                    model_name: str, \n",
    "                    train_data_path: str, \n",
    "                    val_data_path:   str,\n",
    "                    train_data_name: str,\n",
    "                    val_data_name: str,\n",
    "                    encoder_max_length: int, \n",
    "                    decoder_max_length: int,\n",
    "                    random_state: int, \n",
    "                    learning_rate: float=5e-5,\n",
    "                    epochs: int=4, \n",
    "                    train_batch: int=4, \n",
    "                    eval_batch: int=4,\n",
    "                    warmup_steps: int=500, \n",
    "                    weight_decay: float=0.1,\n",
    "                    logging_stes: int=100,\n",
    "                    save_total_lim: int=3,\n",
    "                    label_smooting: float = 0.1,\n",
    "                    predict_generate: bool=True,\n",
    "                    eval_columns_list: list=['eval_loss', 'eval_rouge1'],\n",
    "                    save_strategy='no',\n",
    "                    load_best_model_at_end=True,\n",
    "                    evaluate_only=False,\n",
    "                    report_to=None): \n",
    "                         \n",
    "    # CREATE ANALYSIS FOLDER\n",
    "    os.mkdir(f'analysis_report_{analysis_name}')\n",
    "    print(analysis_name)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "    \n",
    "    if model_name=='CodeT5':\n",
    "        model_name=\"Salesforce/codet5-base-multi-sum\"\n",
    "        \n",
    "    elif model_name=='CodeTrans':\n",
    "        model_name=\"SEBIS/code_trans_t5_base_source_code_summarization_python_multitask\"\n",
    "                         \n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, skip_special_tokens=False)\n",
    "    model.to(device)\n",
    "    print(device)\n",
    "    \n",
    "    train_dataset = load_dataset(\"json\",\n",
    "                             data_files=train_data_name,\n",
    "                             field=\"data\",\n",
    "                             data_dir=train_data_path)\n",
    "\n",
    "    test_dataset = load_dataset(\"json\",\n",
    "                                data_files=val_data_name,\n",
    "                                field=\"data\",\n",
    "                                data_dir=val_data_path)\n",
    "                                \n",
    "    train_data_txt = train_dataset['train']\n",
    "        \n",
    "    validation_data_txt = test_dataset['train']\n",
    "    \n",
    "    train_data = train_data_txt.map(\n",
    "        lambda batch: batch_tokenize_preprocess(\n",
    "            batch, \n",
    "            tokenizer=tokenizer,\n",
    "            max_input_length=encoder_max_length,\n",
    "            max_output_length=decoder_max_length\n",
    "        ),\n",
    "        batch_size=8,\n",
    "        batched=True,\n",
    "        remove_columns=train_data_txt.column_names,\n",
    "    )\n",
    "    \n",
    "    validation_data = validation_data_txt.map(\n",
    "        lambda batch: batch_tokenize_preprocess(\n",
    "            batch, \n",
    "            tokenizer=tokenizer,\n",
    "            max_input_length=encoder_max_length,\n",
    "            max_output_length=decoder_max_length\n",
    "        ),\n",
    "        batched=True,\n",
    "        remove_columns=validation_data_txt.column_names,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # SUBSAMPLE FOR GENERATION BEFORE TUNING\n",
    "    test_samples = validation_data_txt.select(range(20))\n",
    "    summaries_before_tuning = generate_summary(test_samples, \n",
    "                                                model, \n",
    "                                                tokenizer, \n",
    "                                                encoder_max_length)[1]\n",
    "\n",
    "    \n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=f\"analysis_report_{analysis_name}/results\",\n",
    "        num_train_epochs=epochs,\n",
    "        do_train=True,\n",
    "        do_eval=True,\n",
    "        per_device_train_batch_size=train_batch,\n",
    "        per_device_eval_batch_size=eval_batch,\n",
    "        learning_rate=learning_rate,\n",
    "        warmup_steps=warmup_steps,\n",
    "        weight_decay=weight_decay,\n",
    "        label_smoothing_factor=label_smooting,\n",
    "        predict_with_generate=predict_generate,\n",
    "        logging_dir=f\"analysis_report_{analysis_name}/logs\",\n",
    "        logging_steps=logging_stes,\n",
    "        save_total_limit=save_total_lim,\n",
    "        report_to=report_to,\n",
    "        save_strategy=save_strategy,\n",
    "        load_best_model_at_end=load_best_model_at_end\n",
    "    )\n",
    "    \n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "    \n",
    "    compute_metrics = compute_metric_with_params(tokenizer)\n",
    "    \n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=validation_data,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "    # ZERO - SHOT\n",
    "    results_zero_shot = trainer.evaluate()\n",
    "    results_zero_shot_df = pd.DataFrame(data=results_zero_shot, index=[0])[eval_columns_list]\n",
    "    results_zero_shot_df.loc[0, :] = results_zero_shot_df.loc[0, :].apply(lambda x: round(x, 3))\n",
    "    print(results_zero_shot_df)\n",
    "    \n",
    "    results_zero_shot_df.to_csv(f'analysis_report_{analysis_name}/results_zero_shot.csv', index=False)\n",
    "    \n",
    "    if evaluate_only:\n",
    "        with open(f'analysis_report_{analysis_name}/results.txt', \"w\") as results_file:\n",
    "        \n",
    "            for i, description in enumerate(test_samples[\"output_sequence\"]):\n",
    "                results_file.write('_'*10)\n",
    "                results_file.write(f'Original: {description}')\n",
    "                results_file.write(f'Summaries: {summaries_before_tuning[i]}')\n",
    "        return 'Finished'\n",
    "        \n",
    "    \n",
    "    # TRAINING\n",
    "    trainer.train()\n",
    "    \n",
    "    # FINE-TUNING\n",
    "    results_fine_tune = trainer.evaluate()\n",
    "    results_fine_tune_df = pd.DataFrame(data=results_fine_tune, index=[0])[eval_columns_list]\n",
    "    results_fine_tune_df.loc[0, :] = results_fine_tune_df.loc[0, :].apply(lambda x: round(x, 3))\n",
    "    print(results_fine_tune_df)\n",
    "    \n",
    "    results_fine_tune_df.to_csv(f'analysis_report_{analysis_name}/results_fine_tune.csv', index=False)\n",
    "    \n",
    "    summaries_after_tuning = generate_summary(test_samples, \n",
    "                                             model,\n",
    "                                             tokenizer,\n",
    "                                             encoder_max_length)[1]\n",
    "    \n",
    "    for i, description in enumerate(test_samples[\"output_sequence\"]):\n",
    "      print('_'*10)\n",
    "      print(f'Original: {description}')\n",
    "      print(f'Summary before Tuning: {summaries_before_tuning[i]}')\n",
    "      print(f'Summary after Tuning: {summaries_after_tuning[i]}')\n",
    "      print('_'*10)\n",
    "      print('\\n')\n",
    "      \n",
    "    # CREATE REPORT\n",
    "    with open(f'analysis_report_{analysis_name}/results.txt', \"w\") as results_file:\n",
    "        \n",
    "        for i, description in enumerate(test_samples[\"output_sequence\"]):\n",
    "            results_file.write('_'*10)\n",
    "            results_file.write(f'Original: {description}')\n",
    "            results_file.write(f'Summary before Tuning: {summaries_before_tuning[i]}')\n",
    "            results_file.write(f'Summary after Tuning: {summaries_after_tuning[i]}')\n",
    "            results_file.write('_'*10)\n",
    "            results_file.write('\\n')\n",
    "    \n",
    "    \n",
    "    # STORE PARAMS\n",
    "    with open(f'analysis_report_{analysis_name}/config.json', \"w\") as params_file:\n",
    "        config_params = {'analysis_name': analysis_name, \n",
    "                         'model_name': model_name, \n",
    "                         'train_data_path': train_data_path, \n",
    "                         'val_data_path':   val_data_path,\n",
    "                         'train_data_name': train_data_name,\n",
    "                         'val_data_name': val_data_name,\n",
    "                         'encoder_max_length': encoder_max_length, \n",
    "                         'decoder_max_length': decoder_max_length,\n",
    "                         'random_state': random_state, \n",
    "                         'learning_rate': learning_rate,\n",
    "                         'epochs': epochs, \n",
    "                         'train_batch': train_batch, \n",
    "                         'eval_batch': eval_batch,\n",
    "                         'warmup_steps': warmup_steps, \n",
    "                         'weight_decay': weight_decay,\n",
    "                         'logging_stes': logging_stes,\n",
    "                         'save_total_lim': save_total_lim,\n",
    "                         'label_smooting': label_smooting,\n",
    "                         'predict_generate': predict_generate,\n",
    "                         'eval_columns_list': eval_columns_list,\n",
    "                         'save_strategy' : save_strategy,\n",
    "                         }\n",
    "        json.dump(config_params, params_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'model_name': str, \n",
    "'train_data_path': str, \n",
    "'val_data_path':   str,\n",
    "'train_data_name': str,\n",
    "'val_data_name': str,\n",
    "'encoder_max_length': int, \n",
    "'decoder_max_length': int,\n",
    "'random_state': int, \n",
    "'learning_rate': float=5e-5,\n",
    "'epochs': int=4, \n",
    "'train_batch': int=4, \n",
    "'eval_batch': int=4,\n",
    "'save_strategy': ''\n",
    "'load_best_model_at_end' : \n",
    "'evaluate_only'=False,\n",
    "'report_to'=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "CbZX3Y3_q1pw",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeT5-no_context-test-3-512-16-20231021_42\n"
     ]
    }
   ],
   "source": [
    "# model_name = '../5-domain-pre-training/analysis_report_CodeT5-test-12-300-4-2023-09-26-v3/results/checkpoint-78656'\n",
    "# model_name = '../5-domain-pre-training/results/checkpoint-12290'\n",
    "model_name = \"CodeT5\"\n",
    "# model_name = '../4-qode2desc/no_bootstrap/analysis_report_CodeTrans_domain-checkpoint-12290-test-10-512-16-2023-10-07/results/checkpoint-2500'\n",
    "# model_name = '../5-domain-pre-training/arxiv_pretraining/results/checkpoint-12290'\n",
    "\n",
    "# model_name = \"../5-domain-pre-training/analysis_report_CodeTrans-12-300-15-4-202310145_1/results/checkpoint-58500\"\n",
    "# model_name = \"../5-domain-pre-training/analysis_report_CodeT5-4-300-15-4-202310145_1/results/checkpoint-19500\"\n",
    "# model_name = \"../5-domain-pre-training/analysis_report_CodeT5-4-300-100-4-20231017_1/results/checkpoint-39000\"\n",
    "\n",
    "\n",
    "DATE = \"20231021\"\n",
    "\n",
    "SAMPLE = \"test\"\n",
    "if SAMPLE == \"test\":\n",
    "    load_best_model_at_end = False\n",
    "else:\n",
    "    load_best_model_at_end = None\n",
    "\n",
    "# tokenization\n",
    "encoder_max_length = 512\n",
    "decoder_max_length = 150\n",
    "RS = 42\n",
    "LR = 5e-5\n",
    "\n",
    "EPOCHS = 3\n",
    "TRAIN_BATCH = 16\n",
    "EVAL_BATCH = 4\n",
    "\n",
    "WARMUP_STEPS = 500\n",
    "WEIGHT_DECAY = 0.1\n",
    "LOGGING_STEPS = 100\n",
    "SAVE_TOTAL_LIM = 1\n",
    "SAVE_STRATEGY = \"steps\"\n",
    "\n",
    "LABEL_SMOOTHING = 0.1\n",
    "PREDICT_GENERATE = True\n",
    "\n",
    "MODE = \"no_context\"\n",
    "\n",
    "EVAL_COLUMNS = [\n",
    "    \"eval_loss\",\n",
    "    \"eval_rouge1\",\n",
    "    \"eval_rouge2\",\n",
    "    \"eval_rougeL\",\n",
    "    \"eval_rougeLsum\",\n",
    "    \"eval_bleu\",\n",
    "    \"eval_gen_len\",\n",
    "]\n",
    "\n",
    "\n",
    "analysis_name = (\n",
    "    model_name\n",
    "    + \"-\"\n",
    "    + MODE\n",
    "    + \"-\"\n",
    "    + SAMPLE\n",
    "    + \"-\"\n",
    "    + str(EPOCHS)\n",
    "    + \"-\"\n",
    "    + str(encoder_max_length)\n",
    "    + \"-\"\n",
    "    + str(TRAIN_BATCH)\n",
    ")\n",
    "\n",
    "if len(model_name) > 15:\n",
    "    analysis_name = (\n",
    "        model_name.split(\"/\")[-1]\n",
    "        + \"-\"\n",
    "        + SAMPLE\n",
    "        + \"-\"\n",
    "        + str(EPOCHS)\n",
    "        + \"-\"\n",
    "        + str(encoder_max_length)\n",
    "        + \"-\"\n",
    "        + str(TRAIN_BATCH)\n",
    "    )\n",
    "\n",
    "# analysis_name = \"CodeT5_domain_4_19500\"\n",
    "analysis_name = analysis_name + \"-\" + DATE + \"_\" + str(RS)\n",
    "\n",
    "print(analysis_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "FRHaw5S7X81D",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "9t6d_nrX_lyy",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeT5-no_context-test-3-512-16-20231021_42\n",
      "CodeT5-no_context-test-3-512-16-20231021_42\n",
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/RDC/zinovyee.hub/.local/lib/python3.9/site-packages/transformers/models/auto/modeling_auto.py:1322: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/RDC/zinovyee.hub/.cache/huggingface/datasets/json/default-976d380dd9a2721a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65eb2dca7d72440ab56a58030e43344f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/RDC/zinovyee.hub/.cache/huggingface/datasets/json/default-53639e3945c15472/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34143f6aa322426da9ac6c6c9aa862f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/RDC/zinovyee.hub/.cache/huggingface/datasets/json/default-976d380dd9a2721a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-4f99d6327ccb76ee.arrow\n",
      "Loading cached processed dataset at /home/RDC/zinovyee.hub/.cache/huggingface/datasets/json/default-53639e3945c15472/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-eb2937518902f738.arrow\n",
      "/home/RDC/zinovyee.hub/.local/lib/python3.9/site-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='246' max='123' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [123/123 11:49]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   eval_loss  eval_rouge1  eval_rouge2  eval_rougeL  eval_rougeLsum  \\\n",
      "0      6.409        0.146        0.035        0.127           0.131   \n",
      "\n",
      "   eval_bleu  eval_gen_len  \n",
      "0      0.009        15.152  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/RDC/zinovyee.hub/.local/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='813' max='813' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [813/813 10:12, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>5.881600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>5.109300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>4.761600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>4.429900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.203100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.861700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>3.689300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>3.671500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   eval_loss  eval_rouge1  eval_rouge2  eval_rougeL  eval_rougeLsum  \\\n",
      "0      4.053        0.323        0.142        0.274           0.282   \n",
      "\n",
      "   eval_bleu  eval_gen_len  \n",
      "0      0.048        17.752  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/RDC/zinovyee.hub/.local/lib/python3.9/site-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________\n",
      "Original: Algorithm that identify the S&P500 firm tickers from news text. The algorithm only identify those tickers in the three most commonly seen cases. Detailed description refer to the paper\n",
      "Summary before Tuning: This function is used to create a single - leader version of a nltk object.\n",
      "Summary after Tuning: This Quantlet is dedicated to finding the articles in the Marco website. The\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Inspect the derived variables from the News Network, Plots and Stats\n",
      "Summary before Tuning: Generate a single - month\n",
      " from a CSV file.\n",
      "Summary after Tuning: This Quantlet simulates the attention variables for the SPX article data set.\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Inspect the derived variables from the News Network, Plots and Stats\n",
      "Summary before Tuning: A object for plotting a series of KDE with given bandwidths and kernel functions.\n",
      "Summary after Tuning: Plotting Kernel Density  and Kernel Kernel Density \n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Estimates and plots  empirical pricing kernels , risk neutral densities  and physical densities  of DAX 30 index return conditional on 20% , 40%  and 60%  quantiles of volatility index VDAX-NEW , and time to maturity 1 month. Local linear kernel regression is used for estimation of the conditional risk neutral density and local constant kernel regression is used for estimation of conditional physical density. Moreover, 95% confidence intervals have been calculated for PK, RND and PD conditioned by 40% quantile of VDAX-NEW and time to maturity 1 month. All results are shown on a continuously compounded 1-month period returns scale. \n",
      "Summary before Tuning: regionregion region region\n",
      "Summary after Tuning: Calculates and plots the price kernels of the localising BW model for the time period\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Estimates and plots  empirical pricing kernels , risk neutral densities  and physical densities  of DAX 30 index return conditional on time to maturity 2 months and different levels of VDAX-NEW-Subindex 2 . Local linear kernel regression is used for estimation of the conditional risk neutral density and local constant kernel regression is used for estimation of conditional physical density. EPK is obtained as a ratio of RND and PD. The panels on the right-hand side of figures depict EPK, RND, PD conditional on 20%, 50% and 80% quantiles of VDAX-NEW-Subindex 2 with 95% confidence intervals. Colors from red to blue correspond to increasing values of volatility within each interval. All results are shown on a continuously compounded 2-months period returns scale.\n",
      "Summary before Tuning: regionregion region region\n",
      "Summary after Tuning: Computes the price kernels and effective volatility for the DAX and VDAX option\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Conducts a statistical analysis, heteroskedasticity and normality checks on housing data provided by Sberbank\n",
      "Summary before Tuning: Summary of the target variable in a series of different types.\n",
      "Summary after Tuning: Conducting a statistical analysis, heteroskedasticity and normal\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Descriptive statistical analysis of Moscow housing data provided by Sberbank\n",
      "Summary before Tuning: Includes functions to get an overview of summary statistics for the variables and analysis of correlations of\n",
      "Summary after Tuning: Includes functions to get an overview of summary statistics for the variables and analysis of correlations of\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Descriptive statistical analysis of Moscow housing data provided by Sberbank\n",
      "Summary before Tuning: Load the raw and messy datasets which will then be used in the models quantlet\n",
      "\n",
      "Summary after Tuning: We load the raw and messy datasets, which will then be used in the models quant\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Contains the VIX index and the VIX future intraday analysis on the tail-event day of February 5, 2018. The next day, Credit Suisse lost 500m USD because of a structured product on the VIX. Market disruption can already be observed the day before.\n",
      "Summary before Tuning: requires that all packages have been installed and loaded\n",
      "Summary after Tuning: Analyse the VIX market on the 5th of February, 2018.\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Contains the VIX index and the VIX future intraday analysis on the tail-event day of February 5, 2018. The next day, Credit Suisse lost 500m USD because of a structured product on the VIX. Market disruption can already be observed the day before.\n",
      "Summary before Tuning: This function restricts the given data frame to a subset of the VIX records based on\n",
      "Summary after Tuning: Restricts the data to the given date and time range.\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Implements the statistics tests for the Efficient Market Hypothesis: Bartell Test, Ljung-Box Test, Variance Ratio Test, Von-Newmann Test.\n",
      "Summary before Tuning: Function to plot the VR test of a single residue of a sequence.\n",
      "Summary after Tuning: This Quantlet builds a plot of the VRT and the variance of the AAP\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Implements the statistics tests for the Efficient Market Hypothesis: Bartell Test, Ljung-Box Test, Variance Ratio Test, Von-Newmann Test.\n",
      "Summary before Tuning: Get the basic data of a single residue in the system.\n",
      "Summary after Tuning: A simple example of how to see if the stock data is a random price.\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Plots time series from cryptocurrency database\n",
      "Summary before Tuning: library - library for all n - ary objects.\n",
      "Summary after Tuning: This Quantlet plots time series of crypto_ts for the DSHusd,\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Plots time series from cryptocurrency database\n",
      "Summary before Tuning: library for the high frequency series and time series plot\n",
      "Summary after Tuning: Plotting the crypto_ts of the time series of the time series of the time series\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Split the entire dataset in two parts. the first one consists of all observations without the last 24 ones. It is used for the training process of LSTM with the optimal parameters. The second subset consists of the last 24 observations used for the prediction of the hourly returns in the next 24 hours. Once the predictions are generated for each of the coins, they are exported from python in order to be imported in MATLAB.\n",
      "Summary before Tuning: Generate predictions for all nanoseconds in the data.\n",
      "Summary after Tuning: This Quantlet builds predictions for the LSTM model with the given data.\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Define a parameter grid for the LSTM and choose the optimal set of parameters that leads to a minimal root-mean-squared error.\n",
      "Summary before Tuning: Train predict and evaluate on a set of data using the RMSE.\n",
      "Summary after Tuning: This Quantlet builds a model to predict the minimum RMSE achieved by using\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Produces the estimation results using GeoCopula approach and saves it as RDate file.\n",
      "Summary before Tuning: find windows with no window - related functions\n",
      "Summary after Tuning: Performs a PCA for the five countries of the German data set. The PCA\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: UNIT 3 and UNIT 4 of DEDA. Introduce webscraping and object-oriented programming\n",
      "Summary before Tuning: This function retrieves the data of a single node of the system.\n",
      "Summary after Tuning: Web Scraping encompasses any method allowing for extracting data from websites.\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: UNIT 3 and UNIT 4 of DEDA. Introduce webscraping and object-oriented programming\n",
      "Summary before Tuning: This module returns the number of US GOV applications each fiscal year and the number of\n",
      "Summary after Tuning: We use json module to unpack data from webpage.\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: ['Demonstrate decision tree', 'Demonstrate applied SVM on NLP sentiment classification', 'Plot some activation functions for Neural Networks']\n",
      "Summary before Tuning: Plots a single block of the n - th block of the model with a single block of\n",
      "Summary after Tuning: Keras model to predict and predict the model for the given epochs\n",
      "__________\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(analysis_name)\n",
    "analysis_modules.scs_analyze(\n",
    "    analysis_name=analysis_name,\n",
    "    model_name=model_name,\n",
    "    train_data_path=f\"../../data/preprocessed/Quantlet/{DATE}/{MODE}/\",\n",
    "    train_data_name=f\"full_train_dataset_{DATE}_sample0.json\",\n",
    "    val_data_path=f\"../../data/preprocessed/Quantlet/{DATE}/{MODE}/\",\n",
    "    val_data_name=f\"test_dataset_{DATE}_sample0.json\",\n",
    "    encoder_max_length=encoder_max_length,\n",
    "    decoder_max_length=decoder_max_length,\n",
    "    random_state=RS,\n",
    "    eval_columns_list=EVAL_COLUMNS,\n",
    "    learning_rate=LR,\n",
    "    epochs=EPOCHS,\n",
    "    train_batch=TRAIN_BATCH,\n",
    "    eval_batch=EVAL_BATCH,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    logging_stes=LOGGING_STEPS,\n",
    "    save_total_lim=SAVE_TOTAL_LIM,\n",
    "    save_strategy=SAVE_STRATEGY,\n",
    "    label_smooting=LABEL_SMOOTHING,\n",
    "    predict_generate=PREDICT_GENERATE,\n",
    "    load_best_model_at_end=load_best_model_at_end,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "smrdGzzNyo5n",
    "tags": []
   },
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPZowVhs+ksZzaIPG80pu3+",
   "gpuType": "A100",
   "machine_shape": "hm",
   "mount_file_id": "119XCh3Q64Zw4MGlsZQuGrMLS78o5MeWS",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "encode_code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

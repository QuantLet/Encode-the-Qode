{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"10R1AK7SDnSY0ry5TJ2X67Cnws5wkNKVI","authorship_tag":"ABX9TyP/FKktAFOilTDSTG5TexmY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"9ef17b20a2e041c1b6446f209fb89dc1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ed03221665c6483ebba40366f25b4133","IPY_MODEL_8de7a9c7809d4a1e992de12195132a3b","IPY_MODEL_72991249907749599c573e5b5a4c1fe7"],"layout":"IPY_MODEL_94322ec6da0e439f94957086e7aac037"}},"ed03221665c6483ebba40366f25b4133":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c0b29ad76d2a4b8c93f5969787c3fbe5","placeholder":"​","style":"IPY_MODEL_5637601d82404a22a14cdb022b106a2a","value":"100%"}},"8de7a9c7809d4a1e992de12195132a3b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_01e3489a86aa4dc6b363ced7413d3b78","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_204fe05551da417189817e8c0d300215","value":1}},"72991249907749599c573e5b5a4c1fe7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_952a72fb93bf423fbb1130c731b6e768","placeholder":"​","style":"IPY_MODEL_d017e015fcf54dea90ec745d33538531","value":" 1/1 [00:00&lt;00:00, 39.03it/s]"}},"94322ec6da0e439f94957086e7aac037":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c0b29ad76d2a4b8c93f5969787c3fbe5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5637601d82404a22a14cdb022b106a2a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"01e3489a86aa4dc6b363ced7413d3b78":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"204fe05551da417189817e8c0d300215":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"952a72fb93bf423fbb1130c731b6e768":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d017e015fcf54dea90ec745d33538531":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a3cf614f775046de99e7aad1df3b2816":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ed26d6fa495547e5b3b2adb8fb2980b8","IPY_MODEL_7e8f9bab675c432fb7f5283de1395fed","IPY_MODEL_3cda25a98dd44f53942a773fcd3a59ab"],"layout":"IPY_MODEL_eaa29e2a11d14832b9f5b79ad75437b2"}},"ed26d6fa495547e5b3b2adb8fb2980b8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c9243c214ca74e618b11258a432eae7a","placeholder":"​","style":"IPY_MODEL_c8252b7533ab418bb49c6b7f021d3db3","value":"Downloading data files: 100%"}},"7e8f9bab675c432fb7f5283de1395fed":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3aec4ef1affa4dc7836f5cc4772d02c8","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_88b6269f49054b4a9c8db480fc135062","value":1}},"3cda25a98dd44f53942a773fcd3a59ab":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fad008a73fa8488aa7964e6b7a20b385","placeholder":"​","style":"IPY_MODEL_964b80d02438401fb6dca67226d8e623","value":" 1/1 [00:00&lt;00:00, 56.32it/s]"}},"eaa29e2a11d14832b9f5b79ad75437b2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c9243c214ca74e618b11258a432eae7a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c8252b7533ab418bb49c6b7f021d3db3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3aec4ef1affa4dc7836f5cc4772d02c8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"88b6269f49054b4a9c8db480fc135062":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fad008a73fa8488aa7964e6b7a20b385":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"964b80d02438401fb6dca67226d8e623":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dccd902d528e4a5e89b27dfd183ed3a7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_86b9baf7fd7a4b4eace680379ca2ef51","IPY_MODEL_6e2cdfc2d12b4a5d8e8f39c3d0eba6df","IPY_MODEL_179a5b2158fe4ebe8aa65d22b8e16de9"],"layout":"IPY_MODEL_3e291d7858e743a6af92b6a5802f6636"}},"86b9baf7fd7a4b4eace680379ca2ef51":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b6cf7cff4ac2475fb2bfe943cb8c9731","placeholder":"​","style":"IPY_MODEL_d04fa3e4d4a84fda8695383fc10e29f9","value":"Extracting data files: 100%"}},"6e2cdfc2d12b4a5d8e8f39c3d0eba6df":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fffdd38e076a452fb0d6309fae7a4e54","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6ddd24ac3b264d089d221c1e2aeec23e","value":1}},"179a5b2158fe4ebe8aa65d22b8e16de9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_20c0334179df43b39b406f51987331f4","placeholder":"​","style":"IPY_MODEL_7705cad36f874d29b98d3c0dbeb9129e","value":" 1/1 [00:00&lt;00:00,  1.29it/s]"}},"3e291d7858e743a6af92b6a5802f6636":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b6cf7cff4ac2475fb2bfe943cb8c9731":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d04fa3e4d4a84fda8695383fc10e29f9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fffdd38e076a452fb0d6309fae7a4e54":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6ddd24ac3b264d089d221c1e2aeec23e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"20c0334179df43b39b406f51987331f4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7705cad36f874d29b98d3c0dbeb9129e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"547b3ab03dba4e8c963a2928a915cfdc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6246fb20495a4b4cb02b1a99273ff365","IPY_MODEL_0c113a9445ff4750a3c780bd272c0735","IPY_MODEL_bcca9cab8f704e29bbc53f6d0cbb7cdc"],"layout":"IPY_MODEL_ae2df1b401384ce3907510993b0cea61"}},"6246fb20495a4b4cb02b1a99273ff365":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0858bf986ac049138d79fc72b053089a","placeholder":"​","style":"IPY_MODEL_117ddc3fa6c445d88988e55e12e0aa60","value":"Generating train split: "}},"0c113a9445ff4750a3c780bd272c0735":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"info","description":"","description_tooltip":null,"layout":"IPY_MODEL_6951f8a784384a2a928ef17e2987d0cf","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_58fe8b5d622048c9b45e605fef0ef799","value":1}},"bcca9cab8f704e29bbc53f6d0cbb7cdc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c10064cc84d14d728bd19c21d0fe4485","placeholder":"​","style":"IPY_MODEL_a9cc97780c464f49aa563b827af0b454","value":" 667/0 [00:00&lt;00:00, 3053.07 examples/s]"}},"ae2df1b401384ce3907510993b0cea61":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"0858bf986ac049138d79fc72b053089a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"117ddc3fa6c445d88988e55e12e0aa60":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6951f8a784384a2a928ef17e2987d0cf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"58fe8b5d622048c9b45e605fef0ef799":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c10064cc84d14d728bd19c21d0fe4485":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a9cc97780c464f49aa563b827af0b454":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"96b6b5917c2c4a028875139154b5a226":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_033dcf49e0de40808d017a27dfd99fa4","IPY_MODEL_20d61b832575444bab5d15b51dd483a8","IPY_MODEL_0cd0a0b891ac448ba116fcea78a62c31"],"layout":"IPY_MODEL_57bb503bc3704eff9d0dccdd8181319a"}},"033dcf49e0de40808d017a27dfd99fa4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_271b9212533844e89ede3cfb31e9e1e7","placeholder":"​","style":"IPY_MODEL_2673fab41a354039812e73c730e19eb8","value":"100%"}},"20d61b832575444bab5d15b51dd483a8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4e11ad4a01e94c4da0efd5734e547756","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ce268cbd3d724cb1ab4f9a5f554c1b77","value":1}},"0cd0a0b891ac448ba116fcea78a62c31":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_76d2bcbca13d41e7bb36c7c9246f483e","placeholder":"​","style":"IPY_MODEL_4b9fa10dd89e4a06b11fe2b2a0d10b10","value":" 1/1 [00:00&lt;00:00, 37.05it/s]"}},"57bb503bc3704eff9d0dccdd8181319a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"271b9212533844e89ede3cfb31e9e1e7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2673fab41a354039812e73c730e19eb8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4e11ad4a01e94c4da0efd5734e547756":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ce268cbd3d724cb1ab4f9a5f554c1b77":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"76d2bcbca13d41e7bb36c7c9246f483e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4b9fa10dd89e4a06b11fe2b2a0d10b10":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b0ebe892442a48ddb9196db822b47ad8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_99f172049f974f4c8b9a9ae0c400abbe","IPY_MODEL_e32780821c7d41d8a6a7cead7e319fe6","IPY_MODEL_328fd643f55a4ba18841185e9042cc4b"],"layout":"IPY_MODEL_7603fcf52ec7444886467299fa4a8781"}},"99f172049f974f4c8b9a9ae0c400abbe":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ef65175a3b5e41458055f83a8e12458f","placeholder":"​","style":"IPY_MODEL_74005f94a58b43b4b2d0fd2f666bf615","value":"Downloading data files: 100%"}},"e32780821c7d41d8a6a7cead7e319fe6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d81499bcf10648d786e0db3351c087af","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4919c895fcaf4a06b2647dc1364d3b3b","value":1}},"328fd643f55a4ba18841185e9042cc4b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0ddd9485177d453b8a749c7deda216b9","placeholder":"​","style":"IPY_MODEL_5fff1dff16e7475aad6bd895bdf41a73","value":" 1/1 [00:00&lt;00:00, 58.13it/s]"}},"7603fcf52ec7444886467299fa4a8781":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef65175a3b5e41458055f83a8e12458f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"74005f94a58b43b4b2d0fd2f666bf615":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d81499bcf10648d786e0db3351c087af":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4919c895fcaf4a06b2647dc1364d3b3b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0ddd9485177d453b8a749c7deda216b9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5fff1dff16e7475aad6bd895bdf41a73":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"59a73f653d894d789f08af4eef5dda11":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_27d25aa10f474e06a9034edc3fc706b2","IPY_MODEL_196f03b83da649359d3fb1752e6fb65f","IPY_MODEL_70495d8391344bc1ab791e6a3dc1781a"],"layout":"IPY_MODEL_147cf0cfbce442458b4788b146ccc9b5"}},"27d25aa10f474e06a9034edc3fc706b2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_73fae14a54ae4885a1c0f866e726ebdd","placeholder":"​","style":"IPY_MODEL_51952380bb204e36bac5c7f3d92dd7a6","value":"Extracting data files: 100%"}},"196f03b83da649359d3fb1752e6fb65f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2c6a918b74654ad4a1d99b0c058145bc","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_440d01adad3c4d6790ebedfab64a78dd","value":1}},"70495d8391344bc1ab791e6a3dc1781a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e3d6b11ae7f54f24821a01baa59a2a0c","placeholder":"​","style":"IPY_MODEL_c405bef3196f43568c2a9f342191e296","value":" 1/1 [00:00&lt;00:00,  1.62it/s]"}},"147cf0cfbce442458b4788b146ccc9b5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"73fae14a54ae4885a1c0f866e726ebdd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"51952380bb204e36bac5c7f3d92dd7a6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2c6a918b74654ad4a1d99b0c058145bc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"440d01adad3c4d6790ebedfab64a78dd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e3d6b11ae7f54f24821a01baa59a2a0c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c405bef3196f43568c2a9f342191e296":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bec49b10102f4d768282323cd5b0a910":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cac12027f4e7410ea50194be71b2b8b5","IPY_MODEL_8c0763f28fa542429de6a12fba39d470","IPY_MODEL_5375d111f12b497a8f7c41fe25e3cc07"],"layout":"IPY_MODEL_22cfeb111ea849189c707501f0148d6e"}},"cac12027f4e7410ea50194be71b2b8b5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7c40e55749a6471d909ae335e3569aed","placeholder":"​","style":"IPY_MODEL_0064a60d100841fa81711d39cfc35629","value":"Generating train split: "}},"8c0763f28fa542429de6a12fba39d470":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"info","description":"","description_tooltip":null,"layout":"IPY_MODEL_46c3ebce1dd24d33be147902356b5c9d","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_26a8189bba6d436db8deeba2e7dafda6","value":1}},"5375d111f12b497a8f7c41fe25e3cc07":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c89489b064774a0cac032762ba97ffb7","placeholder":"​","style":"IPY_MODEL_bd1e5ca62f1c4306b302b920e8e5204d","value":" 527/0 [00:00&lt;00:00, 2416.64 examples/s]"}},"22cfeb111ea849189c707501f0148d6e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"7c40e55749a6471d909ae335e3569aed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0064a60d100841fa81711d39cfc35629":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"46c3ebce1dd24d33be147902356b5c9d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"26a8189bba6d436db8deeba2e7dafda6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c89489b064774a0cac032762ba97ffb7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bd1e5ca62f1c4306b302b920e8e5204d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8640e6d117fd48b9a618bd262fe7af55":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_325ca59eeb7c4e83b339d7d0ca47499d","IPY_MODEL_5d5abd8d950e45e193b49fcb3cc27da7","IPY_MODEL_cf6c31129d344ed781660d8867b7b73d"],"layout":"IPY_MODEL_5c1946022ec149bd972ee9519387719f"}},"325ca59eeb7c4e83b339d7d0ca47499d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2ab36cdc7c8d4982937d8f65eb6f01ad","placeholder":"​","style":"IPY_MODEL_1669145c9eef4d6396b0ea8b2aa45315","value":"100%"}},"5d5abd8d950e45e193b49fcb3cc27da7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_68c898a5ca2545ce8b3f0c7de1a37d6f","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6a7f74944d9d4c3badce8de4c94c5a5e","value":1}},"cf6c31129d344ed781660d8867b7b73d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d4934391e9244acfa49b8341f71337d9","placeholder":"​","style":"IPY_MODEL_ddf1ef7752a647d8a09b97109e593286","value":" 1/1 [00:00&lt;00:00, 55.75it/s]"}},"5c1946022ec149bd972ee9519387719f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ab36cdc7c8d4982937d8f65eb6f01ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1669145c9eef4d6396b0ea8b2aa45315":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"68c898a5ca2545ce8b3f0c7de1a37d6f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6a7f74944d9d4c3badce8de4c94c5a5e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d4934391e9244acfa49b8341f71337d9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ddf1ef7752a647d8a09b97109e593286":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YauRofAZ7b63","executionInfo":{"status":"ok","timestamp":1695110199899,"user_tz":-120,"elapsed":3249,"user":{"displayName":"Elizaveta Zinovyeva","userId":"01253993997636551956"}},"outputId":"3e6c245f-4b97-4efb-f6a4-6429e482a7b7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  /content/drive/MyDrive/ColabNotebooks/IRTG/Encode_the_Qode/Encode-the-Qode/data/raw/ArXiv/archive (1).zip\n","  inflating: arxiv_data.csv          \n","  inflating: arxiv_data_210930-054931.csv  \n"]}],"source":["!unzip '/content/drive/MyDrive/ColabNotebooks/IRTG/Encode_the_Qode/Encode-the-Qode/data/raw/ArXiv/archive (1).zip'"]},{"cell_type":"markdown","source":["## LOAD TOKENS"],"metadata":{"id":"12lln48ig_JT"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","import json\n","import ast\n","import pickle\n","\n","import nltk\n","nltk.download('stopwords')\n","\n","from nltk.stem import PorterStemmer\n","from nltk.corpus import stopwords"],"metadata":{"id":"KrpxUiLW7gSF","executionInfo":{"status":"ok","timestamp":1695111312498,"user_tz":-120,"elapsed":230,"user":{"displayName":"Elizaveta Zinovyeva","userId":"01253993997636551956"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"7a2768cf-c5bf-4ea9-97ba-80511c61fb67"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}]},{"cell_type":"code","source":["RS = 42\n","DATA_PATH = '/content/drive/MyDrive/ColabNotebooks/IRTG/Encode_the_Qode/Encode-the-Qode/data'"],"metadata":{"id":"sQoQpLgw7h_o","executionInfo":{"status":"ok","timestamp":1695110203535,"user_tz":-120,"elapsed":5,"user":{"displayName":"Elizaveta Zinovyeva","userId":"01253993997636551956"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["data = pd.read_csv('arxiv_data_210930-054931.csv')\n","q_data = pd.read_csv(DATA_PATH +'/preprocessed/Quantinar/course_chapters_202309061748.csv', delimiter=';')"],"metadata":{"id":"RA64r8O27qLO","executionInfo":{"status":"ok","timestamp":1695111205695,"user_tz":-120,"elapsed":1003,"user":{"displayName":"Elizaveta Zinovyeva","userId":"01253993997636551956"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["tokens_arxiv = []\n","\n","data['titles'].apply(lambda title: tokens_arxiv.extend(title.lower().split()))\n","data['abstracts'].apply(lambda title: tokens_arxiv.extend(title.lower().split()))"],"metadata":{"id":"ZPqrOfcY7u-y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokens_quantinar = []\n","q_data.loc[q_data[\"'word_cloud'\"].notna(),\"'word_cloud'\"].apply(lambda x: tokens_quantinar.extend(list(ast.literal_eval(x).keys())))\n","q_data.loc[(q_data[\"'chapter_name'\"].notna()) & (q_data[\"'chapter_name'\"].apply(lambda x: isinstance(x, dict))),\"'chapter_name'\"].apply(lambda x: tokens_quantinar.extend(list(ast.literal_eval(x).values())))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SGJVl2PI8p1W","executionInfo":{"status":"ok","timestamp":1695111207890,"user_tz":-120,"elapsed":894,"user":{"displayName":"Elizaveta Zinovyeva","userId":"01253993997636551956"}},"outputId":"92b9bd2a-1049-485c-abb7-e770b92f590c"},"execution_count":39,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Series([], Name: 'chapter_name', dtype: object)"]},"metadata":{},"execution_count":39}]},{"cell_type":"code","source":["ps = PorterStemmer()"],"metadata":{"id":"bTilHnJ0fSQf","executionInfo":{"status":"ok","timestamp":1695111207891,"user_tz":-120,"elapsed":5,"user":{"displayName":"Elizaveta Zinovyeva","userId":"01253993997636551956"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["tokens_arxiv = [ps.stem(token) for token in set(tokens_arxiv) if token not in stopwords.words('english')]\n","tokens_quantinar = [ps.stem(token.lower()) for token in set(tokens_quantinar) if token.lower() not in stopwords.words('english')]"],"metadata":{"id":"LVe2GjGffC8S","executionInfo":{"status":"ok","timestamp":1695111228931,"user_tz":-120,"elapsed":21044,"user":{"displayName":"Elizaveta Zinovyeva","userId":"01253993997636551956"}}},"execution_count":41,"outputs":[]},{"cell_type":"code","source":["with open(DATA_PATH +'/preprocessed/Quantinar/text_tokens.pkl', 'wb') as handle:\n","    pickle.dump(tokens_quantinar, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","with open(DATA_PATH +'/preprocessed/ArXiv/text_tokens.pkl', 'wb') as handle:\n","    pickle.dump(tokens_arxiv, handle, protocol=pickle.HIGHEST_PROTOCOL)"],"metadata":{"id":"YXU9AocUf7sj","executionInfo":{"status":"ok","timestamp":1695111365052,"user_tz":-120,"elapsed":240,"user":{"displayName":"Elizaveta Zinovyeva","userId":"01253993997636551956"}}},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":["## COMPUTE OVERLAP"],"metadata":{"id":"AeGAvLjHhCf6"}},{"cell_type":"code","source":["QPATH = \"Quantlet/7-corpus-token-ident\""],"metadata":{"id":"IqSy-2PIhn0O","executionInfo":{"status":"ok","timestamp":1695111639754,"user_tz":-120,"elapsed":254,"user":{"displayName":"Elizaveta Zinovyeva","userId":"01253993997636551956"}}},"execution_count":50,"outputs":[]},{"cell_type":"code","source":["import sys\n","IN_COLAB = 'google.colab' in sys.modules\n","\n","import os\n","if IN_COLAB:\n","  os.chdir(f'/content/drive/MyDrive/ColabNotebooks/IRTG/Encode_the_Qode/Encode-the-Qode/{QPATH}')\n","\n","sys.path.append('../src')\n","\n",""],"metadata":{"id":"meuiRWGUhmri","executionInfo":{"status":"ok","timestamp":1695111640748,"user_tz":-120,"elapsed":1,"user":{"displayName":"Elizaveta Zinovyeva","userId":"01253993997636551956"}}},"execution_count":51,"outputs":[]},{"cell_type":"code","source":["%pip install transformers[torch]\n","%pip install -q sentencepiece\n","%pip install datasets==2.13.1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4OwMKGK6h43A","executionInfo":{"status":"ok","timestamp":1695111703858,"user_tz":-120,"elapsed":16088,"user":{"displayName":"Elizaveta Zinovyeva","userId":"01253993997636551956"}},"outputId":"614984b6-b1b1-4e9b-c96a-3d7202822b9c"},"execution_count":54,"outputs":[{"output_type":"stream","name":"stdout","text":["Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers, accelerate\n","Successfully installed accelerate-0.23.0 huggingface-hub-0.17.2 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.33.2\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting datasets==2.13.1\n","  Downloading datasets-2.13.1-py3-none-any.whl (486 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.2/486.2 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets==2.13.1) (1.23.5)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.13.1) (9.0.0)\n","Collecting dill<0.3.7,>=0.3.0 (from datasets==2.13.1)\n","  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==2.13.1) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.13.1) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.13.1) (4.66.1)\n","Collecting xxhash (from datasets==2.13.1)\n","  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess (from datasets==2.13.1)\n","  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.13.1) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==2.13.1) (3.8.5)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.13.1) (0.17.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets==2.13.1) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.13.1) (6.0.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.13.1) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.13.1) (3.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.13.1) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.13.1) (4.0.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.13.1) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.13.1) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.13.1) (1.3.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets==2.13.1) (3.12.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets==2.13.1) (4.5.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.13.1) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.13.1) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.13.1) (2023.7.22)\n","INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n","  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.13.1) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.13.1) (2023.3.post1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets==2.13.1) (1.16.0)\n","Installing collected packages: xxhash, dill, multiprocess, datasets\n","Successfully installed datasets-2.13.1 dill-0.3.6 multiprocess-0.70.14 xxhash-3.3.0\n"]}]},{"cell_type":"code","source":["from datasets import load_dataset"],"metadata":{"id":"BrfRas8qh2xP","executionInfo":{"status":"ok","timestamp":1695111704637,"user_tz":-120,"elapsed":783,"user":{"displayName":"Elizaveta Zinovyeva","userId":"01253993997636551956"}}},"execution_count":55,"outputs":[]},{"cell_type":"code","source":["train_dataset = load_dataset(\"json\",\n","                             data_files=\"train_dataset_descr.json\",\n","                             field=\"data\",\n","                             data_dir=\"../../data/preprocessed/Quantlet/\")\n","test_dataset = load_dataset(\"json\",\n","                            data_files=\"test_dataset_descr.json\",\n","                            field=\"data\",\n","                            data_dir=\"../../data/preprocessed/Quantlet/\")\n","\n","test_dataset = load_dataset(\"json\",\n","                            data_files=\"val_dataset_json.json\",\n","                            field=\"data\",\n","                            data_dir=\"../../data/preprocessed/Quantlet/\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":348,"referenced_widgets":["9ef17b20a2e041c1b6446f209fb89dc1","ed03221665c6483ebba40366f25b4133","8de7a9c7809d4a1e992de12195132a3b","72991249907749599c573e5b5a4c1fe7","94322ec6da0e439f94957086e7aac037","c0b29ad76d2a4b8c93f5969787c3fbe5","5637601d82404a22a14cdb022b106a2a","01e3489a86aa4dc6b363ced7413d3b78","204fe05551da417189817e8c0d300215","952a72fb93bf423fbb1130c731b6e768","d017e015fcf54dea90ec745d33538531","a3cf614f775046de99e7aad1df3b2816","ed26d6fa495547e5b3b2adb8fb2980b8","7e8f9bab675c432fb7f5283de1395fed","3cda25a98dd44f53942a773fcd3a59ab","eaa29e2a11d14832b9f5b79ad75437b2","c9243c214ca74e618b11258a432eae7a","c8252b7533ab418bb49c6b7f021d3db3","3aec4ef1affa4dc7836f5cc4772d02c8","88b6269f49054b4a9c8db480fc135062","fad008a73fa8488aa7964e6b7a20b385","964b80d02438401fb6dca67226d8e623","dccd902d528e4a5e89b27dfd183ed3a7","86b9baf7fd7a4b4eace680379ca2ef51","6e2cdfc2d12b4a5d8e8f39c3d0eba6df","179a5b2158fe4ebe8aa65d22b8e16de9","3e291d7858e743a6af92b6a5802f6636","b6cf7cff4ac2475fb2bfe943cb8c9731","d04fa3e4d4a84fda8695383fc10e29f9","fffdd38e076a452fb0d6309fae7a4e54","6ddd24ac3b264d089d221c1e2aeec23e","20c0334179df43b39b406f51987331f4","7705cad36f874d29b98d3c0dbeb9129e","547b3ab03dba4e8c963a2928a915cfdc","6246fb20495a4b4cb02b1a99273ff365","0c113a9445ff4750a3c780bd272c0735","bcca9cab8f704e29bbc53f6d0cbb7cdc","ae2df1b401384ce3907510993b0cea61","0858bf986ac049138d79fc72b053089a","117ddc3fa6c445d88988e55e12e0aa60","6951f8a784384a2a928ef17e2987d0cf","58fe8b5d622048c9b45e605fef0ef799","c10064cc84d14d728bd19c21d0fe4485","a9cc97780c464f49aa563b827af0b454","96b6b5917c2c4a028875139154b5a226","033dcf49e0de40808d017a27dfd99fa4","20d61b832575444bab5d15b51dd483a8","0cd0a0b891ac448ba116fcea78a62c31","57bb503bc3704eff9d0dccdd8181319a","271b9212533844e89ede3cfb31e9e1e7","2673fab41a354039812e73c730e19eb8","4e11ad4a01e94c4da0efd5734e547756","ce268cbd3d724cb1ab4f9a5f554c1b77","76d2bcbca13d41e7bb36c7c9246f483e","4b9fa10dd89e4a06b11fe2b2a0d10b10","b0ebe892442a48ddb9196db822b47ad8","99f172049f974f4c8b9a9ae0c400abbe","e32780821c7d41d8a6a7cead7e319fe6","328fd643f55a4ba18841185e9042cc4b","7603fcf52ec7444886467299fa4a8781","ef65175a3b5e41458055f83a8e12458f","74005f94a58b43b4b2d0fd2f666bf615","d81499bcf10648d786e0db3351c087af","4919c895fcaf4a06b2647dc1364d3b3b","0ddd9485177d453b8a749c7deda216b9","5fff1dff16e7475aad6bd895bdf41a73","59a73f653d894d789f08af4eef5dda11","27d25aa10f474e06a9034edc3fc706b2","196f03b83da649359d3fb1752e6fb65f","70495d8391344bc1ab791e6a3dc1781a","147cf0cfbce442458b4788b146ccc9b5","73fae14a54ae4885a1c0f866e726ebdd","51952380bb204e36bac5c7f3d92dd7a6","2c6a918b74654ad4a1d99b0c058145bc","440d01adad3c4d6790ebedfab64a78dd","e3d6b11ae7f54f24821a01baa59a2a0c","c405bef3196f43568c2a9f342191e296","bec49b10102f4d768282323cd5b0a910","cac12027f4e7410ea50194be71b2b8b5","8c0763f28fa542429de6a12fba39d470","5375d111f12b497a8f7c41fe25e3cc07","22cfeb111ea849189c707501f0148d6e","7c40e55749a6471d909ae335e3569aed","0064a60d100841fa81711d39cfc35629","46c3ebce1dd24d33be147902356b5c9d","26a8189bba6d436db8deeba2e7dafda6","c89489b064774a0cac032762ba97ffb7","bd1e5ca62f1c4306b302b920e8e5204d","8640e6d117fd48b9a618bd262fe7af55","325ca59eeb7c4e83b339d7d0ca47499d","5d5abd8d950e45e193b49fcb3cc27da7","cf6c31129d344ed781660d8867b7b73d","5c1946022ec149bd972ee9519387719f","2ab36cdc7c8d4982937d8f65eb6f01ad","1669145c9eef4d6396b0ea8b2aa45315","68c898a5ca2545ce8b3f0c7de1a37d6f","6a7f74944d9d4c3badce8de4c94c5a5e","d4934391e9244acfa49b8341f71337d9","ddf1ef7752a647d8a09b97109e593286"]},"id":"MKcSnOVEgzBr","executionInfo":{"status":"ok","timestamp":1695112340225,"user_tz":-120,"elapsed":2689,"user":{"displayName":"Elizaveta Zinovyeva","userId":"01253993997636551956"}},"outputId":"c1fb4f8d-e132-43fa-cab6-0cc688fcb7d3"},"execution_count":57,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:datasets.builder:Found cached dataset json (/root/.cache/huggingface/datasets/json/default-8490eda138451f81/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ef17b20a2e041c1b6446f209fb89dc1"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-3f18bb67f140614e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3cf614f775046de99e7aad1df3b2816"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dccd902d528e4a5e89b27dfd183ed3a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"547b3ab03dba4e8c963a2928a915cfdc"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-3f18bb67f140614e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96b6b5917c2c4a028875139154b5a226"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-9fd014b5cbf2c454/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0ebe892442a48ddb9196db822b47ad8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59a73f653d894d789f08af4eef5dda11"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bec49b10102f4d768282323cd5b0a910"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-9fd014b5cbf2c454/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8640e6d117fd48b9a618bd262fe7af55"}},"metadata":{}}]},{"cell_type":"code","source":["train_dataset['train']['output_sequence']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RaeKkvpwka8U","executionInfo":{"status":"ok","timestamp":1695112936175,"user_tz":-120,"elapsed":461,"user":{"displayName":"Elizaveta Zinovyeva","userId":"01253993997636551956"}},"outputId":"1d4007bb-61f6-4776-d18f-0d0ec30565d8"},"execution_count":63,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Applies the adaptive algorithm on the model proposed by Barrow (2006) to capture the disaster of stock market and forecast stock price.',\n"," 'Scenario analysis for an ETF on the CRIX (Haerdle and Trimborn 2015). Requires Cryptocurrency Data from https://box.hu-berlin.de/d/588d5e7e5de84ae58a51/ ',\n"," 'Scenario analysis for an ETF on the CRIX (Haerdle and Trimborn 2015). Requires Cryptocurrency Data from https://box.hu-berlin.de/d/588d5e7e5de84ae58a51/ ',\n"," 'Scenario analysis for an ETF on the CRIX (Haerdle and Trimborn 2015). Requires Cryptocurrency Data from https://box.hu-berlin.de/d/588d5e7e5de84ae58a51/ ',\n"," 'Scenario analysis for an ETF on the CRIX (Haerdle and Trimborn 2015). Requires Cryptocurrency Data from https://box.hu-berlin.de/d/588d5e7e5de84ae58a51/ ',\n"," 'Scenario analysis for an ETF on the CRIX (Haerdle and Trimborn 2015). Requires Cryptocurrency Data from https://box.hu-berlin.de/d/588d5e7e5de84ae58a51/ ',\n"," 'Data visualisation for phacking talk',\n"," 'Data visualisation for phacking talk',\n"," 'Data visualisation for phacking talk',\n"," 'Data visualisation for phacking talk',\n"," 'Data visualisation for phacking talk',\n"," 'Data visualisation for phacking talk',\n"," 'Data visualisation for phacking talk',\n"," 'Data visualisation for phacking talk',\n"," 'Data visualisation for phacking talk',\n"," 'Data visualisation for phacking talk',\n"," 'Data visualisation for phacking talk',\n"," 'Examples of using DPCA with daily maximum precipitation data in August during the period of year 1997 to 2008.',\n"," 'Construct news network out of the identified tickers of S&P500 stocks',\n"," 'Portfolio constructed by sorting the network attention proxy, network degree. S&P500 constituents are sorted into quintile, and each quintile portfolio is rebalanced at the end of each month by the preceding monthly network degree. Portfolios can be value-weighted or equal-weighted.',\n"," 'Algorithm that identify the S&P500 firm tickers from news text. The algorithm only identify those tickers in the three most commonly seen cases. Detailed description refer to the paper',\n"," 'Inspect the derived variables from the News Network, Plots and Stats',\n"," 'Inspect the derived variables from the News Network, Plots and Stats',\n"," 'Inspect the derived variables from the News Network, Plots and Stats',\n"," 'use LSTM and GRU to optimize prediction',\n"," 'use RNN to do cryptocurrency prediction',\n"," 'use binance api to collect crypto currency data',\n"," 'Use machines learning algorithms to predict daily price move in Bitcoin and perform day trading',\n"," 'Use machines learning algorithms to predict daily price move in Bitcoin and perform day trading',\n"," 'Use machines learning algorithms to predict daily price move in Bitcoin and perform day trading',\n"," 'Use machines learning algorithms to predict daily price move in Bitcoin and perform day trading',\n"," 'Use machines learning algorithms to predict daily price move in Bitcoin and perform day trading',\n"," 'Use machines learning algorithms to predict daily price move in Bitcoin and perform day trading',\n"," 'Use machines learning algorithms to predict daily price move in Bitcoin and perform day trading',\n"," 'Use machines learning algorithms to predict daily price move in Bitcoin and perform day trading',\n"," 'MNIST Chinese Symbol with CNN',\n"," 'Project_Group11 file',\n"," 'Project_Group11 file',\n"," 'Project_Group11 file',\n"," 'Project_Group11 file',\n"," 'Project_Group11 file',\n"," 'Project_Group11 file',\n"," 'Project_Group11 file',\n"," 'Project_Group11 file',\n"," 'Retrain the snownlp model to calculate the sentiment index by Sina financial news',\n"," 'Retrain the snownlp model to calculate the sentiment index by Sina financial news',\n"," 'Retrain the snownlp model to calculate the sentiment index by Sina financial news',\n"," 'Retrain the snownlp model to calculate the sentiment index by Sina financial news',\n"," 'Retrain the snownlp model to calculate the sentiment index by Sina financial news',\n"," 'Retrain the snownlp model to calculate the sentiment index by Sina financial news',\n"," 'Retrain the snownlp model to calculate the sentiment index by Sina financial news',\n"," 'Retrain the snownlp model to calculate the sentiment index by Sina financial news',\n"," 'Retrain the snownlp model to calculate the sentiment index by Sina financial news',\n"," 'Project_Group8 file',\n"," 'Project_Group8 file',\n"," 'Project_Group8 file',\n"," 'Project_Group8 file',\n"," 'Project_Group8 file',\n"," 'Project_Group8 file',\n"," 'Project_Group8 file',\n"," 'Project_Group8 file',\n"," 'Project_Group8 file',\n"," 'Project_Group8 file',\n"," 'Project_Group8 file',\n"," 'Project_Group8 file',\n"," 'Project_Group8 file',\n"," 'Project_Group8 file',\n"," 'Project_Group8 file',\n"," 'Project_Group8 file',\n"," 'Project_Group9 file',\n"," 'Using TFT to do high frequency prediction of stock index futures',\n"," 'Use HRP method to get allocation weights for portfolio and create back test based on allocation',\n"," 'Use HRP method to get allocation weights for portfolio and create back test based on allocation',\n"," 'Use HRP method to get allocation weights for portfolio and create back test based on allocation',\n"," 'Use HRP method to get allocation weights for portfolio and create back test based on allocation',\n"," 'Use HRP method to get allocation weights for portfolio and create back test based on allocation',\n"," 'Uses python script to download NASDAQ and NYSE mega/large/mid/small cap stock daily price data from Yahoo!Finance',\n"," 'Uses python script to download NASDAQ and NYSE mega/large/mid/small cap stock daily price data from Yahoo!Finance',\n"," 'Uses python script to download NASDAQ and NYSE mega/large/mid/small cap stock daily price data from Yahoo!Finance',\n"," 'Identifies news articles as fake or real based on its contents',\n"," 'Collecting all the cryptopunks by webscraping and cluster them',\n"," 'Collecting all the cryptopunks by webscraping and cluster them',\n"," 'Collecting all the cryptopunks by webscraping and cluster them',\n"," 'Collecting all the cryptopunks by webscraping and cluster them',\n"," 'Collecting all the cryptopunks by webscraping and cluster them',\n"," 'Generate the extract using Markov',\n"," 'Construction, State analysis and back test of HMM model',\n"," 'Comparison of the constructed model with alternative methods',\n"," 'Calculation of the prediction error of the constructed model',\n"," 'Bitcoin price prediction using the constructed model',\n"," 'Modelling_Baseline',\n"," 'Modelling_Baseline',\n"," 'Modelling_Baseline',\n"," 'Modelling_Baseline',\n"," 'Modelling_Baseline',\n"," 'Modelling_Encoding',\n"," 'Modelling_Encoding',\n"," 'Modelling_Encoding',\n"," 'Modelling_Encoding',\n"," 'Modelling_Encoding',\n"," 'Feature_Engineering',\n"," 'Feature_Engineering',\n"," 'Feature_Engineering',\n"," 'Feature_Engineering',\n"," 'Feature_Engineering',\n"," 'Feature_Engineering',\n"," 'Feature_Engineering',\n"," 'Experiment_Sampling',\n"," 'Experiment_Sampling',\n"," 'Experiment_Sampling',\n"," 'Estimates the Value at Risks (VaRs) of 100 financial institutions using moving window estimation based on seven macro state variables. The plot shows the log returns of JP Morgan (black points) and VaR of JP Morgan (red line) as an example. The estimated CoVaR by using Single-Index Model with LASSO (blue line) and estimated CoVaR using linear quantile LASSO model(green line) are also plotted as comparison.',\n"," 'estimates Conditional Value at Risk (CoVaR) of 100 financial institutions by using linear quantile lasso algorithm. The 110 covariates include log returns of 99 firms (except for firm k) 7 macro state variables and 4 firm k’s characteristics. The data is not publicly published.',\n"," 'estimates Conditional Value at Risk (CoVaR) of 100 financial institutions by using linear quantile lasso algorithm. The 110 covariates include log returns of 99 firms (except for firm k) 7 macro state variables and 4 firm k’s characteristics. The data is not publicly published.',\n"," 'estimates Conditional Value at Risk (CoVaR) of 100 financial institutions by using linear quantile lasso algorithm. The 110 covariates include log returns of 99 firms (except for firm k) 7 macro state variables and 4 firm k’s characteristics. The data is not publicly published.',\n"," 'estimates Conditional Value at Risk (CoVaR) of 100 financial institutions by using linear quantile lasso algorithm. The 110 covariates include log returns of 99 firms (except for firm k) 7 macro state variables and 4 firm k’s characteristics. The data is not publicly published.',\n"," 'estimates Conditional Value at Risk (CoVaR) of 100 financial institutions by using linear quantile lasso algorithm. The 110 covariates include log returns of 99 firms (except for firm k) 7 macro state variables and 4 firm k’s characteristics. The data is not publicly published.',\n"," 'Ranks the total incoming and outgoing links for each individual firm, and lists the first three most influential firms with respect to incoming and outgoing links for each firm',\n"," 'Plots the total incoming and outgoing links of four financial industry groups: Depositories (red solid line), Insurances (blue dashed line), Broker-Dealers (green dotted line), Others (violet dash-dot line)',\n"," 'Plots the total connectedness and the averaged lambda of 100 financial institutions from 20071207 to 20130105.',\n"," 'Ranks the risk receivers and the risk emitters, the top ten firms of each group are identified as the systemically important financial insitutions (SIFIs)',\n"," 'Estimates and plots (yearly) empirical pricing kernels (EPK), risk neutral densities (RND) and physical densities (PD) of DAX 30 index return conditional on 20% (red curve), 40% (green curve) and 60% (blue curve) quantiles of volatility index VDAX-NEW (current year), and time to maturity 1 month. Local linear kernel regression is used for estimation of the conditional risk neutral density and local constant kernel regression is used for estimation of conditional physical density. Moreover, 95% confidence intervals have been calculated for PK, RND and PD conditioned by 40% quantile of VDAX-NEW and time to maturity 1 month. All results are shown on a continuously compounded 1-month period returns scale. ',\n"," 'Estimates and plots (yearly) empirical pricing kernels (EPK), risk neutral densities (RND) and physical densities (PD) of DAX 30 index return conditional on time to maturity 1 month and different levels of VDAX-NEW (20 equally spaced numbers from 5% to 95% quantile of VDAX-NEW in a given year). Local linear kernel regression is used for estimation of the conditional risk neutral density and local constant kernel regression is used for estimation of conditional physical density. EPK is obtained as a ratio of RND and PD. The panels on the right-hand side of figures depict EPK, RND, PD conditional on 20%, 50% and 80% quantiles of VDAX-NEW with 95% confidence intervals. Colors from red to blue correspond to increasing values of volatility within each interval. All results are shown on a continuously compounded 1-month period returns scale.',\n"," 'Estimates and plots (yearly) empirical pricing kernels (EPK), risk neutral densities (RND) and physical densities (PD) of DAX 30 index return conditional on 20% (red curve), 40% (green curve) and 60% (blue curve) quantiles of volatility index VDAX-NEW (current year), and time to maturity 1 month. Local linear kernel regression is used for estimation of the conditional risk neutral density and local linear kernel regression is used for estimation of conditional physical density. Moreover, 95% confidence intervals have been calculated for PK, RND and PD conditioned by 40% quantile of VDAX-NEW and time to maturity 1 month. All results are shown on a continuously compounded 1-month period returns scale.',\n"," 'Estimates and plots (yearly) empirical pricing kernels (EPK), risk neutral densities (RND) and physical densities (PD) of DAX 30 index return conditional on time to maturity 2 months and different levels of VDAX-NEW-Subindex 2 (20 equally spaced numbers from 5% to 95% quantile of VDAX-NEW-Subindex 2 in a given year). Local linear kernel regression is used for estimation of the conditional risk neutral density and local constant kernel regression is used for estimation of conditional physical density. EPK is obtained as a ratio of RND and PD. The panels on the right-hand side of figures depict EPK, RND, PD conditional on 20%, 50% and 80% quantiles of VDAX-NEW-Subindex 2 with 95% confidence intervals. Colors from red to blue correspond to increasing values of volatility within each interval. All results are shown on a continuously compounded 2-months period returns scale.',\n"," 'Selects bandwidth for multivariate local linear kernel regression using cross-validation',\n"," 'Estimates and plots (yearly) empirical pricing kernels (EPK), risk neutral densities (RND) and physical densities (PD) of DAX 30 index return conditional on time to maturity 1 month and different levels of VDAX-NEW-Subindex 1 (20 equally spaced numbers from 5% to 95% quantile of VDAX-NEW-Subindex 1 in a given year). Local linear kernel regression is used for estimation of the conditional risk neutral density and local constant kernel regression is used for estimation of conditional physical density. EPK is obtained as a ratio of RND and PD. The panels on the right-hand side of figures depict EPK, RND, PD conditional on 20%, 50% and 80% quantiles of VDAX-NEW-Subindex 1 with 95% confidence intervals. Colors from red to blue correspond to increasing values of volatility within each interval. All results are shown on a continuously compounded 1-month period returns scale.',\n"," 'Estimates and plots (yearly) empirical pricing kernels (EPK) of DAX 30 index return conditional on times to maturity 1, 2 and 3 months and different levels of VDAX-NEW (10 equally spaced numbers from 35% to 65% quantile of VDAX-NEW in a given year). VDAX-NEW (and VDAX-NEW-Subindex 1), VDAX-NEW-Subindex 2 and VDAX-NEW-Subindex 3 were used for estimation of pricing kernels condiotioned by time to maturity 1, 2 and 3 months respectively. Local linear kernel regression is used for estimation of the conditional risk neutral density and local constant kernel regression is used for estimation of conditional physical density. Colors from red to blue correspond to increasing values of volatility.',\n"," 'Fit market IV surface by double exponential jump diffusion model',\n"," 'Fit market IV surface by double exponential jump diffusion model',\n"," 'Fit market IV surface by double exponential jump diffusion model',\n"," 'Demonstrates the cleaning of the housing dataset provided by Sberbank',\n"," 'Demonstrates the cleaning of the housing dataset provided by Sberbank',\n"," 'Performs basic variable selction methods (LASSO, rf variable importance plot) on housing data provided by Sberbank',\n"," 'Introduces the simple instrumental variables estimator and explores its properties using simulated data and a practical example.',\n"," 'Conducts a statistical analysis, heteroskedasticity and normality checks on housing data provided by Sberbank',\n"," 'Analyses the consistency of the simple IV estimator as well as the effects of differing instrument quality using simulation.',\n"," 'Predicting periods of decline of Moscows housing market through relevant Google Trends, such as the search keyword real estate market or or the complete category real estate',\n"," 'Predicting periods of decline of Moscows housing market through relevant Google Trends, such as the search keyword real estate market or or the complete category real estate',\n"," 'Predicting periods of decline of Moscows housing market through relevant Google Trends, such as the search keyword real estate market or or the complete category real estate',\n"," 'Descriptive statistical analysis of Moscow housing data provided by Sberbank',\n"," 'Descriptive statistical analysis of Moscow housing data provided by Sberbank',\n"," 'Descriptive statistical analysis of Moscow housing data provided by Sberbank',\n"," 'This quantlet visualizes the distribution of different prices per square meter for the districts of Moscow.',\n"," 'This quantlet visualizes the distribution of different prices per square meter for the districts of Moscow.',\n"," 'This quantlet visualizes the distribution of different prices per square meter for the districts of Moscow.',\n"," 'Reads in test and train data from csv-file and calculates out-of-sample performance measures for the trained models. Creates plots of real against predicted data, showing performance visually.',\n"," 'Reads in preprocessed train data and, trains and tunes RF and GBM Models and saves both the models as well as tuning results in graphical and tabular form',\n"," 'Reads in train data from csv-file and does some exploratory data analysis on the distribution of the target variable and the available categoric and numeric input variables.',\n"," 'Performs four regression based methods to select appropriate variables in a machine lerning approach: Lasso regression, Ridge regression, stepwise backward selection and stepwise forward selection. Several graphs are produced: for Lasso, a plot of the crossvalidation procedure for the optimal choice of the hyperparameter lambda based on the mean squared error is plotted, as well as a trace plot. For the stepwise forward selection a graph displays AIC versus number of variables for the different models. A table that compares all models is exported in LaTex format. The fitted models are exported in RData format for further analysis.',\n"," 'Reads in train data from csv-file and does some exploratory data analysis on the linear relationship of the numeric vaiables in the data, including the target variable. Looks at distribution of target variable via boxplots conditional on the levels of the categoric variables in the dataset.',\n"," 'Analyses dataset on missings, performs imputation, handles outliers and empty factor levels. Performs PCA to reduce dimensionality. Three plots are created: a missingnessmap, a matrix of histograms and a screeplot for the PCA. The preprocessed dataset is split into training (80%) and test data (20%) and exported to .csv.',\n"," 'Contains the LSTM model used to predict the VIX index by using the SP500 option intraday data.',\n"," 'Contains the VIX index and the VIX future intraday analysis on the tail-event day of February 5, 2018. The next day, Credit Suisse lost 500m USD because of a structured product on the VIX. Market disruption can already be observed the day before.',\n"," 'Contains the VIX index and the VIX future intraday analysis on the tail-event day of February 5, 2018. The next day, Credit Suisse lost 500m USD because of a structured product on the VIX. Market disruption can already be observed the day before.',\n"," 'Contains the VIX index and the VIX future intraday analysis on the tail-event day of February 5, 2018. The next day, Credit Suisse lost 500m USD because of a structured product on the VIX. Market disruption can already be observed the day before.',\n"," 'This program fits Generalized Pareto Distributions to excess losses of stock/index.',\n"," 'Hypothesis: The bitcoin prices are likely to fall on Monday, i.e. the closing price of Monday is less than the bitcoin price of the previous Friday.',\n"," 'Implements the statistics tests for the Efficient Market Hypothesis: Bartell Test, Ljung-Box Test, Variance Ratio Test, Von-Newmann Test.',\n"," 'Implements the statistics tests for the Efficient Market Hypothesis: Bartell Test, Ljung-Box Test, Variance Ratio Test, Von-Newmann Test.',\n"," 'Implements the statistics tests for the Efficient Market Hypothesis: Bartell Test, Ljung-Box Test, Variance Ratio Test, Von-Newmann Test.',\n"," 'SFM_Sim_pareto simulated a Pareto distribution.',\n"," 'SFM_Sim_pareto simulates Y=-X ~ Pareto distribution.',\n"," 'This anomaly describes the increase in the prices of the Bitcoin in the last week of December and the first half month of January.',\n"," 'SFM_Gen_Pareto simulates Generalized Pareto Distributions: Pareto, Exponential, Pareto II.',\n"," 'This program tests the hypothesis that the DJIA log-returns are drawn from a Normal distribution and estimates the probability of extreme negative returns.',\n"," 'Analysis of the evolution of prices of shares of AAPL, AMZN, GOOG, FB, NFLX, MSFT, SPY between 2013 and 2019.',\n"," 'Simulates the following GEV Distributions: Weibull, Frechet, Gumbel.',\n"," 'Estimates the implied volatility for the Black-Scholes formula, using the Newton-Raphson method.',\n"," 'This program applies the Block Maxima method to annual losses of DJIA.',\n"," 'Simulates a bivariate Normal distribution.',\n"," 'This quantlet estimates Tail Entropy Expected Shortfall for a Pareto Distribution.',\n"," 'Evaluation of cluster coherence for a manual k-means algorithm by returning the 5 highest centroid values per cluster',\n"," 'A manual k-means algorithm as alternative to the one provided by sklearn package',\n"," 'Plotting cluster sizes for k-means',\n"," 'Plots time series from cryptocurrency database',\n"," 'Plots time series from cryptocurrency database',\n"," 'Plots time series from cryptocurrency database',\n"," 'Plotting modules to graph rolling-mean time series data taken from derived sentiment scores. These are plotted against BTC prices in USD for visualisation of the relationship. Basic correlation and Granger causation are calculated as well.',\n"," 'Plotting modules to graph rolling-mean time series data taken from derived sentiment scores. These are plotted against BTC prices in USD for visualisation of the relationship. Basic correlation and Granger causation are calculated as well.',\n"," 'Plotting modules to graph rolling-mean time series data taken from derived sentiment scores. These are plotted against BTC prices in USD for visualisation of the relationship. Basic correlation and Granger causation are calculated as well.',\n"," 'Plotting modules to graph rolling-mean time series data taken from derived sentiment scores. These are plotted against BTC prices in USD for visualisation of the relationship. Basic correlation and Granger causation are calculated as well.',\n"," 'Plotting modules to graph rolling-mean time series data taken from derived sentiment scores. These are plotted against BTC prices in USD for visualisation of the relationship. Basic correlation and Granger causation are calculated as well.',\n"," 'Plotting modules to graph rolling-mean time series data taken from derived sentiment scores. These are plotted against BTC prices in USD for visualisation of the relationship. Basic correlation and Granger causation are calculated as well.',\n"," 'Plotting modules to graph rolling-mean time series data taken from derived sentiment scores. These are plotted against BTC prices in USD for visualisation of the relationship. Basic correlation and Granger causation are calculated as well.',\n"," 'Plotting modules to graph rolling-mean time series data taken from derived sentiment scores. These are plotted against BTC prices in USD for visualisation of the relationship. Basic correlation and Granger causation are calculated as well.',\n"," 'Plotting modules to graph rolling-mean time series data taken from derived sentiment scores. These are plotted against BTC prices in USD for visualisation of the relationship. Basic correlation and Granger causation are calculated as well.',\n"," 'Plotting modules to graph rolling-mean time series data taken from derived sentiment scores. These are plotted against BTC prices in USD for visualisation of the relationship. Basic correlation and Granger causation are calculated as well.',\n"," 'Plotting modules to graph rolling-mean time series data taken from derived sentiment scores. These are plotted against BTC prices in USD for visualisation of the relationship. Basic correlation and Granger causation are calculated as well.',\n"," 'Plotting modules to graph rolling-mean time series data taken from derived sentiment scores. These are plotted against BTC prices in USD for visualisation of the relationship. Basic correlation and Granger causation are calculated as well.',\n"," 'Plotting modules to graph rolling-mean time series data taken from derived sentiment scores. These are plotted against BTC prices in USD for visualisation of the relationship. Basic correlation and Granger causation are calculated as well.',\n"," 'Sentiment scoring tool which takes the output of the news source scraping tool and applies to them the unsupervised VADER and TextBlob sentiment methods.',\n"," 'Sentiment scoring tool which takes the output of the news source scraping tool and applies to them the unsupervised VADER and TextBlob sentiment methods.',\n"," 'Sentiment scoring tool which takes the output of the news source scraping tool and applies to them the unsupervised VADER and TextBlob sentiment methods.',\n"," 'Sentiment scoring tool which takes the output of the news source scraping tool and applies to them the unsupervised VADER and TextBlob sentiment methods.',\n"," 'Sentiment scoring tool which takes the output of the news source scraping tool and applies to them the unsupervised VADER and TextBlob sentiment methods.',\n"," 'Interactive, terminal based scraping tool built to collect news story text from the New York Times, BBC, CNN, and Reuters. The user chooses the desired news source and enters 1-3 search terms. Resulting publications are saved to a .csv file for further processing needs of the user.',\n"," 'Interactive, terminal based scraping tool built to collect news story text from the New York Times, BBC, CNN, and Reuters. The user chooses the desired news source and enters 1-3 search terms. Resulting publications are saved to a .csv file for further processing needs of the user.',\n"," 'Interactive, terminal based scraping tool built to collect news story text from the New York Times, BBC, CNN, and Reuters. The user chooses the desired news source and enters 1-3 search terms. Resulting publications are saved to a .csv file for further processing needs of the user.',\n"," 'Interactive, terminal based scraping tool built to collect news story text from the New York Times, BBC, CNN, and Reuters. The user chooses the desired news source and enters 1-3 search terms. Resulting publications are saved to a .csv file for further processing needs of the user.',\n"," 'Interactive, terminal based scraping tool built to collect news story text from the New York Times, BBC, CNN, and Reuters. The user chooses the desired news source and enters 1-3 search terms. Resulting publications are saved to a .csv file for further processing needs of the user.',\n"," 'Interactive, terminal based scraping tool built to collect news story text from the New York Times, BBC, CNN, and Reuters. The user chooses the desired news source and enters 1-3 search terms. Resulting publications are saved to a .csv file for further processing needs of the user.',\n"," 'Interactive, terminal based scraping tool built to collect news story text from the New York Times, BBC, CNN, and Reuters. The user chooses the desired news source and enters 1-3 search terms. Resulting publications are saved to a .csv file for further processing needs of the user.',\n"," 'Interactive, terminal based scraping tool built to collect news story text from the New York Times, BBC, CNN, and Reuters. The user chooses the desired news source and enters 1-3 search terms. Resulting publications are saved to a .csv file for further processing needs of the user.',\n"," 'Interactive, terminal based scraping tool built to collect news story text from the New York Times, BBC, CNN, and Reuters. The user chooses the desired news source and enters 1-3 search terms. Resulting publications are saved to a .csv file for further processing needs of the user.',\n"," 'Interactive, terminal based scraping tool built to collect news story text from the New York Times, BBC, CNN, and Reuters. The user chooses the desired news source and enters 1-3 search terms. Resulting publications are saved to a .csv file for further processing needs of the user.',\n"," 'Interactive, terminal based scraping tool built to collect news story text from the New York Times, BBC, CNN, and Reuters. The user chooses the desired news source and enters 1-3 search terms. Resulting publications are saved to a .csv file for further processing needs of the user.',\n"," 'Interactive, terminal based scraping tool built to collect news story text from the New York Times, BBC, CNN, and Reuters. The user chooses the desired news source and enters 1-3 search terms. Resulting publications are saved to a .csv file for further processing needs of the user.',\n"," 'Interactive, terminal based scraping tool built to collect news story text from the New York Times, BBC, CNN, and Reuters. The user chooses the desired news source and enters 1-3 search terms. Resulting publications are saved to a .csv file for further processing needs of the user.',\n"," 'Analyse the hourly returns of the collected data of the four coins.',\n"," 'Plotting the CVaR for all sets of different initial weights and see it converging',\n"," 'Select the optimal window-size for the training of the LSTM',\n"," 'Preprocessing the cryptocurrency data in such a way that it fits the requirements of LSTM',\n"," 'Definition of the architecture of the LSTM that is used for the time-series prediction of the hourly returns.',\n"," 'Varying all combinations of initial weights to determine which one leads to a minimal CVaR',\n"," 'Plotting the CVaR for all sets of different initial weights',\n"," 'Determining which the optimal portfolio weights are and what CVaR they result into',\n"," 'Training the LSTM on the data of one of the Coins (ETH) and analysing the results on the test set without parameter tuning.',\n"," 'Finding all possible combinations of initial weights for a four-cryptocurrencies-portfolio',\n"," 'Tail event driven portfolio construction.',\n"," 'Tail event driven portfolio construction.',\n"," 'Tail event driven portfolio construction.',\n"," 'Example of bootstrap application to estimate performance measures of U-Statistics',\n"," 'Comparison of variances of U-Statistics and alternative unbiased estimators',\n"," 'Produces the estimation results using GeoCopula approach and saves it as RDate file.',\n"," 'Produces the plot for parametrically fitted variogram.',\n"," 'Produces the plot for parametrically fitted variogram.',\n"," 'Generate random sample – Observations & Hidden states of dice (with Faif and Loaded dice used interchangeably), sample size is set as N',\n"," 'Give a break down of the related algorithms in HMM – in Casino case of fair and loaded dice data',\n"," 'Use standard HMM package (i.e., hmmlearn) to analyze the data with continuous emissions (observations) – in heart rate bpm) case',\n"," 'Negentropy approximation and comparison',\n"," 'Analyze financial data (daily returns of 15 stocks in Asia market) - show the difference between results of PCA and ICA',\n"," 'Solve Cocktail Party Problem (Blind source separation) - show the difference between results of PCA, FA and ICA',\n"," \"['Preprocessing Japanese text from Yahoo', 'Create a wordcloud based on the text']\",\n"," 'UNIT 2 of DEDA. Introduce the conditional execution and how to iterate by using',\n"," 'UNIT 2 of DEDA. Introduce the conditional execution and how to iterate by using',\n"," 'UNIT 2 of DEDA. Introduce the conditional execution and how to iterate by using',\n"," \"['Demonstrate decision tree', 'Demonstrate applied SVM on NLP sentiment classification', 'Plot some activation functions for Neural Networks']\",\n"," \"['Demonstrate decision tree', 'Demonstrate applied SVM on NLP sentiment classification', 'Plot some activation functions for Neural Networks']\",\n"," \"['Demonstrate decision tree', 'Demonstrate applied SVM on NLP sentiment classification', 'Plot some activation functions for Neural Networks']\",\n"," \"['Demonstrate decision tree', 'Demonstrate applied SVM on NLP sentiment classification', 'Plot some activation functions for Neural Networks']\",\n"," \"['Demonstrate decision tree', 'Demonstrate applied SVM on NLP sentiment classification', 'Plot some activation functions for Neural Networks']\",\n"," \"['Demonstrate decision tree', 'Demonstrate applied SVM on NLP sentiment classification', 'Plot some activation functions for Neural Networks']\",\n"," \"['Demonstrate decision tree', 'Demonstrate applied SVM on NLP sentiment classification', 'Plot some activation functions for Neural Networks']\",\n"," \"['Demonstrate decision tree', 'Demonstrate applied SVM on NLP sentiment classification', 'Plot some activation functions for Neural Networks']\",\n"," \"['Demonstrate decision tree', 'Demonstrate applied SVM on NLP sentiment classification', 'Plot some activation functions for Neural Networks']\",\n"," 'Replication code for the simulation study. Comparison of the two-model approach and the doubly-robust estimator on different data generating processes',\n"," 'Replication code for the simulation study. Comparison of the two-model approach and the doubly-robust estimator on different data generating processes',\n"," 'Replication code for the simulation study. Comparison of the two-model approach and the doubly-robust estimator on different data generating processes',\n"," 'We provide code to generate a correlation plot for multiple variables. The plot contains the density and the pearson correlation coefficient. The color management makes it easy to read and interpret the direction and strength of the correlation.',\n"," 'Demonstrating getting data from various websites (web scraping) and analyzing the election rally speeches in Turkey (24.06.2018) by constructing a bridge between Turkish and English through tokenization and translation.',\n"," 'Demonstrating getting data from various websites (web scraping) and analyzing the election rally speeches in Turkey (24.06.2018) by constructing a bridge between Turkish and English through tokenization and translation.',\n"," 'Demonstrating getting data from various websites (web scraping) and analyzing the election rally speeches in Turkey (24.06.2018) by constructing a bridge between Turkish and English through tokenization and translation.',\n"," 'Demonstrating getting data from various websites (web scraping) and analyzing the election rally speeches in Turkey (24.06.2018) by constructing a bridge between Turkish and English through tokenization and translation.',\n"," 'Demonstrating getting data from various websites (web scraping) and analyzing the election rally speeches in Turkey (24.06.2018) by constructing a bridge between Turkish and English through tokenization and translation.',\n"," 'Demonstrating getting data from various websites (web scraping) and analyzing the election rally speeches in Turkey (24.06.2018) by constructing a bridge between Turkish and English through tokenization and translation.',\n"," 'Demonstrating getting data from various websites (web scraping) and analyzing the election rally speeches in Turkey (24.06.2018) by constructing a bridge between Turkish and English through tokenization and translation.',\n"," 'Demonstrating getting data from various websites (web scraping) and analyzing the election rally speeches in Turkey (24.06.2018) by constructing a bridge between Turkish and English through tokenization and translation.',\n"," 'Demonstrating getting data from various websites (web scraping) and analyzing the election rally speeches in Turkey (24.06.2018) by constructing a bridge between Turkish and English through tokenization and translation.',\n"," 'Demonstrating getting data from various websites (web scraping) and analyzing the election rally speeches in Turkey (24.06.2018) by constructing a bridge between Turkish and English through tokenization and translation.',\n"," 'Demonstrating getting data from various websites (web scraping) and analyzing the election rally speeches in Turkey (24.06.2018) by constructing a bridge between Turkish and English through tokenization and translation.',\n"," 'Demonstrating getting data from various websites (web scraping) and analyzing the election rally speeches in Turkey (24.06.2018) by constructing a bridge between Turkish and English through tokenization and translation.',\n"," 'Demonstrating getting data from various websites (web scraping) and analyzing the election rally speeches in Turkey (24.06.2018) by constructing a bridge between Turkish and English through tokenization and translation.',\n"," 'Demonstrating getting data from various websites (web scraping) and analyzing the election rally speeches in Turkey (24.06.2018) by constructing a bridge between Turkish and English through tokenization and translation.',\n"," 'Demonstrating getting data from various websites (web scraping) and analyzing the election rally speeches in Turkey (24.06.2018) by constructing a bridge between Turkish and English through tokenization and translation.',\n"," 'Use Folium Package for Site Location Selection: A Case for New-entry Retailers to the Xinyi District of Taipei.',\n"," 'Use Ant Colony Optimisation(ACO) for Logistic Route Planning: A Case for Logistic Distribution of 7-Eleven Stores in the Xinyi District of Taipei.',\n"," 'Use Ant Colony Optimisation(ACO) for Logistic Route Planning: A Case for Logistic Distribution of 7-Eleven Stores in the Xinyi District of Taipei.',\n"," 'Use Ant Colony Optimisation(ACO) for Logistic Route Planning: A Case for Logistic Distribution of 7-Eleven Stores in the Xinyi District of Taipei.',\n"," 'Use Ant Colony Optimisation(ACO) for Logistic Route Planning: A Case for Logistic Distribution of 7-Eleven Stores in the Xinyi District of Taipei.',\n"," 'Use Ant Colony Optimisation(ACO) for Logistic Route Planning: A Case for Logistic Distribution of 7-Eleven Stores in the Xinyi District of Taipei.',\n"," 'Use Ant Colony Optimisation(ACO) for Logistic Route Planning: A Case for Logistic Distribution of 7-Eleven Stores in the Xinyi District of Taipei.',\n"," 'Use Ant Colony Optimisation(ACO) for Logistic Route Planning: A Case for Logistic Distribution of 7-Eleven Stores in the Xinyi District of Taipei.',\n"," 'Predicting success of startups based on crunchbase database',\n"," 'Quantify the degree of Mining Pool concentration for different cryptocurrencies',\n"," 'Quantify the degree of Mining Pool concentration for different cryptocurrencies',\n"," 'Quantify the degree of Mining Pool concentration for different cryptocurrencies',\n"," 'Create Choropleth WorldMap to depict sum of all World Cup Goals by each team that ever participated in a WorldCup. Using plotly, a dropdown menu is created depicting for each World Cup the amount of goals shot by each team participating .',\n"," 'Create Choropleth WorldMap to depict sum of all World Cup Goals by each team that ever participated in a WorldCup. Using plotly, a dropdown menu is created depicting for each World Cup the amount of goals shot by each team participating .',\n"," 'Create Choropleth WorldMap to depict sum of all World Cup Goals by each team that ever participated in a WorldCup. Using plotly, a dropdown menu is created depicting for each World Cup the amount of goals shot by each team participating .',\n"," 'Use Multinomial Logistic Regression (LogReg) to predict the Outcome of the Games of the World Cup Matches 2018. Predictions are based on historical results since 1993. Predictions were adapted after the Group Stages with the teams that actually passed on to the Knockout Stages, including all the games played so far in the World Cup into the Training/Test data. The same was done after the Quarterfinals.',\n"," 'Use Multinomial Logistic Regression (LogReg) to predict the Outcome of the Games of the World Cup Matches 2018. Predictions are based on historical results since 1993. Predictions were adapted after the Group Stages with the teams that actually passed on to the Knockout Stages, including all the games played so far in the World Cup into the Training/Test data. The same was done after the Quarterfinals.',\n"," 'Use Multinomial Logistic Regression (LogReg) to predict the Outcome of the Games of the World Cup Matches 2018. Predictions are based on historical results since 1993. Predictions were adapted after the Group Stages with the teams that actually passed on to the Knockout Stages, including all the games played so far in the World Cup into the Training/Test data. The same was done after the Quarterfinals.',\n"," 'Use Multinomial Logistic Regression (LogReg) to predict the Outcome of the Games of the World Cup Matches 2018. Predictions are based on historical results since 1993. Predictions were adapted after the Group Stages with the teams that actually passed on to the Knockout Stages, including all the games played so far in the World Cup into the Training/Test data. The same was done after the Quarterfinals.',\n"," 'Use Multinomial Logistic Regression (LogReg) to predict the Outcome of the Games of the World Cup Matches 2018. Predictions are based on historical results since 1993. Predictions were adapted after the Group Stages with the teams that actually passed on to the Knockout Stages, including all the games played so far in the World Cup into the Training/Test data. The same was done after the Quarterfinals.',\n"," 'Analyzing textual of IRTG researchers then matching IRTG Researchers based on their interests',\n"," 'Analysing Sentiment of Subreddits',\n"," 'Analysing Sentiment of Subreddits',\n"," 'Analysing Sentiment of Subreddits',\n"," 'Analysing Sentiment of Subreddits',\n"," 'Analysing Sentiment of Subreddits',\n"," 'Regulation Risk for Cryptocurrencies: Lexicon based sentiment analysis of scraped regulation news',\n"," 'Regulation Risk for Cryptocurrencies: Lexicon based sentiment analysis of scraped regulation news',\n"," 'Regulation Risk for Cryptocurrencies: Lexicon based sentiment analysis of scraped regulation news',\n"," 'Regulation Risk for Cryptocurrencies: Lexicon based sentiment analysis of scraped regulation news',\n"," 'Regulation Risk for Cryptocurrencies: Lexicon based sentiment analysis of scraped regulation news',\n"," 'Regulation Risk for Cryptocurrencies: Lexicon based sentiment analysis of scraped regulation news',\n"," 'Regulation Risk for Cryptocurrencies: Lexicon based sentiment analysis of scraped regulation news',\n"," 'Regulation Risk for Cryptocurrencies: Lexicon based sentiment analysis of scraped regulation news',\n"," 'Regulation Risk for Cryptocurrencies: Lexicon based sentiment analysis of scraped regulation news',\n"," 'Scripts written in python to scrape details of room offers on wg-gesucht.de. It is done in two stages, first from the list of offers available and second the detail description of an offer.',\n"," 'Scripts written in python to scrape details of room offers on wg-gesucht.de. It is done in two stages, first from the list of offers available and second the detail description of an offer.',\n"," 'Scripts written in python to scrape details of room offers on wg-gesucht.de. It is done in two stages, first from the list of offers available and second the detail description of an offer.',\n"," 'Scripts written in python to scrape details of room offers on wg-gesucht.de. It is done in two stages, first from the list of offers available and second the detail description of an offer.',\n"," 'Scripts written in python to scrape details of room offers on wg-gesucht.de. It is done in two stages, first from the list of offers available and second the detail description of an offer.',\n"," 'An web application built in Python. From the interface written in HTML, user input of a Berlin housing offer in doubt is parsed and entered into the prediction model built from existing data. A probability of how similar the offer is to scams is returned on the submit page.',\n"," 'Calculates forecasts based on historical data and Sentiment of Stock News',\n"," 'Python Based Raspberry Pi Zero W OLED Setup live BTC ticker',\n"," 'Python Based Raspberry Pi Zero W OLED Setup live BTC ticker',\n"," 'Python Based Raspberry Pi Zero W OLED Setup live BTC ticker',\n"," 'Python Based Raspberry Pi Zero W OLED Setup live BTC ticker',\n"," 'Python Based Raspberry Pi Zero W OLED Setup live BTC ticker',\n"," 'Python Based Raspberry Pi Zero W OLED Setup live BTC ticker',\n"," 'Python Based Raspberry Pi Zero W OLED Setup live BTC ticker',\n"," 'Python Based Raspberry Pi Zero W OLED Setup live BTC ticker',\n"," 'Python Based Raspberry Pi Zero W OLED Setup live BTC ticker',\n"," 'Plotting the p2p lendings using LLE',\n"," 'Plotting the p2p lendings using LLE',\n"," 'Plotting the p2p lendings using LLE',\n"," 'Plotting the Swiss roll and reduction using LLE',\n"," 'Plotting the Swiss roll and reduction using LLE',\n"," 'Plotting the Swiss roll and reduction using LLE',\n"," 'Plotting the Moebius band',\n"," 'Plotting the Antisocial Online Behavior using LLE',\n"," 'Plotting the 2 company reports using LLE',\n"," 'Plotting the S and sphere and reduction using LLE',\n"," 'Plotting the S and sphere and reduction using LLE',\n"," 'Plotting the S and sphere and reduction using LLE',\n"," 'Quantlet data reduction with PCA and LLE',\n"," 'Produces the mosaic plot of current jobs (for June, 2016) of CRC members who defended their PhD from 2005 to 2016, June',\n"," 'Produces the tree map of study decision of 117 alumnus of the Econ Boot Camp (EBC) from 2008 to 2016',\n"," 'Produces a map with the locations of scientific events visited by CRC members from 2005 to 2016, June (paid from CRC)',\n"," 'Network map of of collaborating disciplines in CRC 649 according to JEL classification',\n"," 'Produces a map of university locations of guest researchers who visited the CRC from 2005 to 2016, June',\n"," \"Generates confusion matrix between test set predictions generated by Microsoft Emotion API and true test set values. 'results_API_test_set.pkl' contains predictions from API, reruns process from emotion score raw data.\",\n"," 'Generates confusion matrix between test set predictions of FVCConvNet and true test set values. Can be made by loading model and weights from Q-let FVCConvNet and data loader function for original data set FVCload_dataset, or by simply loading already generated predictions and true values.',\n"," 'Plots smoothed emo scores (anger, contempt, disgust, fear, surprise, happiness, sadness, neutral, surprise) and stock movement of Volkswagen stock for first 30 minutes of Volkswagen press conference on 05.05.2017.',\n"," 'Calls Emotion API of Microsoft Cognitive Service. Requires either a key from Microsoft Azure (www.azure.microsoft.com). Can be used to send images from hard drive to API, returns a vector of eight emotion-scores.',\n"," 'Estimates a linear discriminant analysis (LDA) in order to separate positive and negative returns of Eurostoxx50 on press conference days of the European Central Bank based on facial expression scores of the provided video material. Plots resulting histograms and distributions.',\n"," 'Generates confusion matrix between test set predictions of FVCfinetuning and true test set values. Can be made by loading model and weights from Q-let FVCfinetuning and data loader function for original data set FVCload_dataset, or by simply loading already generated predictions and true values.',\n"," 'Plots metrics based on confusion matrices generated by FVCconf_CNN, FVCconf_finetune, FVCconf_API to illustrate the accuracy and prediction power of the different models for an intuitive comparison.',\n"," \"Automizes the procedure of loading the data set and restructuring the images from one-dimensional array into two-dimensional images. Required dataset is from Kaggle's 2013 Facial Expression Recognition Competition and can be downloaded under 'https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge' \",\n"," 'Plots accuracy of FVCConvNet and FVCConvNetFinetune on identical training and validation set for each iteration of estimation',\n"," 'Plots the correlation of aggregated emotional scores for 70 ECB press conferences (Jan. 2011 - Sep. 2017) and daily returns of Eurostoxx50 data.',\n"," 'Plots the correlation circle of the partial least squares model from Q-let FVCEurostoxxPLS. Dependent variable Y are daily returns of Eurostoxx50 data (Jan. 2011 - Sep. 2017), independent are emotional scores on ECB press conference webcasts (70 observations).',\n"," \"Finetunes an Convolutional Neural Network that classifies facial expressions into seven basic emotions (anger, disgust, fear, happiness, neutral, sadness, surprise). The underlying deep convolutional net is trained on VGGface. Image based static facial expression recognition with multiple deep network learning. The original dataset can be obtained from kaggle's 'Facial Expression Recognition Challenge'. The subfolder 'model weights' contains model weights and model architecture in .json-Format. This allows to access the estimation results without rerunning the calculations. VGGFace implementation on keras by github.com/rcmalli/keras-vggface. \",\n"," 'Plots the volatility of DAX30 index on days where the European Central Bank is holding press conferences vs. all other days between January 2015 until September 2017. Volatilities are averaged for each group and estimated from the data, based on 5 minute data. Vertical lines indicate the press release by European central bank council at 13.45, the beginning of the press conference at 14.30, held by the president, and the approximate end at 15.30.',\n"," 'Performs bivariate kernel smoothed regression and bootstrapped confidence bands for gasoline demand on personal income and gasoline price.',\n"," 'This Quantlet is dedicated to classification of the Solidity source code using machine learning methods. We differentiate between traditional machine learning methods and deep learning. There are three modes for training these methods: on the source code, on the source code without comments and on only comments. Moreover, deep pre-trained transformer BERT - the state-of-the-art approach in the NLP were used on the comments mode. Furthermore, additional feature extraction of the most important block names of the Solidity code was made and the best performing ML algorithm was trained on these features.',\n"," 'This Quantlet is dedicated to classification of the Solidity source code using machine learning methods. We differentiate between traditional machine learning methods and deep learning. There are three modes for training these methods: on the source code, on the source code without comments and on only comments. Moreover, deep pre-trained transformer BERT - the state-of-the-art approach in the NLP were used on the comments mode. Furthermore, additional feature extraction of the most important block names of the Solidity code was made and the best performing ML algorithm was trained on these features.',\n"," 'This Quantlet is dedicated to classification of the Solidity source code using machine learning methods. We differentiate between traditional machine learning methods and deep learning. There are three modes for training these methods: on the source code, on the source code without comments and on only comments. Moreover, deep pre-trained transformer BERT - the state-of-the-art approach in the NLP were used on the comments mode. Furthermore, additional feature extraction of the most important block names of the Solidity code was made and the best performing ML algorithm was trained on these features.',\n"," 'This Quantlet is dedicated to classification of the Solidity source code using machine learning methods. We differentiate between traditional machine learning methods and deep learning. There are three modes for training these methods: on the source code, on the source code without comments and on only comments. Moreover, deep pre-trained transformer BERT - the state-of-the-art approach in the NLP were used on the comments mode. Furthermore, additional feature extraction of the most important block names of the Solidity code was made and the best performing ML algorithm was trained on these features.',\n"," 'This Quantlet is dedicated to classification of the Solidity source code using machine learning methods. We differentiate between traditional machine learning methods and deep learning. There are three modes for training these methods: on the source code, on the source code without comments and on only comments. Moreover, deep pre-trained transformer BERT - the state-of-the-art approach in the NLP were used on the comments mode. Furthermore, additional feature extraction of the most important block names of the Solidity code was made and the best performing ML algorithm was trained on these features.',\n"," 'This Quantlet is dedicated to classification of the Solidity source code using machine learning methods. We differentiate between traditional machine learning methods and deep learning. There are three modes for training these methods: on the source code, on the source code without comments and on only comments. Moreover, deep pre-trained transformer BERT - the state-of-the-art approach in the NLP were used on the comments mode. Furthermore, additional feature extraction of the most important block names of the Solidity code was made and the best performing ML algorithm was trained on these features.',\n"," 'This Quantlet is dedicated to classification of the Solidity source code using machine learning methods. We differentiate between traditional machine learning methods and deep learning. There are three modes for training these methods: on the source code, on the source code without comments and on only comments. Moreover, deep pre-trained transformer BERT - the state-of-the-art approach in the NLP were used on the comments mode. Furthermore, additional feature extraction of the most important block names of the Solidity code was made and the best performing ML algorithm was trained on these features.',\n"," 'This Quantlet is dedicated to classification of the Solidity source code using machine learning methods. We differentiate between traditional machine learning methods and deep learning. There are three modes for training these methods: on the source code, on the source code without comments and on only comments. Moreover, deep pre-trained transformer BERT - the state-of-the-art approach in the NLP were used on the comments mode. Furthermore, additional feature extraction of the most important block names of the Solidity code was made and the best performing ML algorithm was trained on these features.',\n"," 'This Quantlet is dedicated to classification of the Solidity source code using machine learning methods. We differentiate between traditional machine learning methods and deep learning. There are three modes for training these methods: on the source code, on the source code without comments and on only comments. Moreover, deep pre-trained transformer BERT - the state-of-the-art approach in the NLP were used on the comments mode. Furthermore, additional feature extraction of the most important block names of the Solidity code was made and the best performing ML algorithm was trained on these features.',\n"," 'This Quantlet is dedicated to classification of the Solidity source code using machine learning methods. We differentiate between traditional machine learning methods and deep learning. There are three modes for training these methods: on the source code, on the source code without comments and on only comments. Moreover, deep pre-trained transformer BERT - the state-of-the-art approach in the NLP were used on the comments mode. Furthermore, additional feature extraction of the most important block names of the Solidity code was made and the best performing ML algorithm was trained on these features.',\n"," 'This Quantlet is dedicated to classification of the Solidity source code using machine learning methods. We differentiate between traditional machine learning methods and deep learning. There are three modes for training these methods: on the source code, on the source code without comments and on only comments. Moreover, deep pre-trained transformer BERT - the state-of-the-art approach in the NLP were used on the comments mode. Furthermore, additional feature extraction of the most important block names of the Solidity code was made and the best performing ML algorithm was trained on these features.',\n"," 'This Quantlet is dedicated to classification of the Solidity source code using machine learning methods. We differentiate between traditional machine learning methods and deep learning. There are three modes for training these methods: on the source code, on the source code without comments and on only comments. Moreover, deep pre-trained transformer BERT - the state-of-the-art approach in the NLP were used on the comments mode. Furthermore, additional feature extraction of the most important block names of the Solidity code was made and the best performing ML algorithm was trained on these features.',\n"," 'This Quantlet is dedicated to classification of the Solidity source code using machine learning methods. We differentiate between traditional machine learning methods and deep learning. There are three modes for training these methods: on the source code, on the source code without comments and on only comments. Moreover, deep pre-trained transformer BERT - the state-of-the-art approach in the NLP were used on the comments mode. Furthermore, additional feature extraction of the most important block names of the Solidity code was made and the best performing ML algorithm was trained on these features.',\n"," 'This Quantlet is dedicated to visualization of time-series of energy consumption of Ethereum',\n"," 'Clustering & Classifying Ethereum Smart Contracts',\n"," 'Clustering & Classifying Ethereum Smart Contracts',\n"," 'Clustering & Classifying Ethereum Smart Contracts',\n"," 'Clustering & Classifying Ethereum Smart Contracts',\n"," 'Clustering & Classifying Ethereum Smart Contracts',\n"," 'Clustering & Classifying Ethereum Smart Contracts',\n"," 'Clustering & Classifying Ethereum Smart Contracts',\n"," 'Clustering & Classifying Ethereum Smart Contracts',\n"," 'Cryptocurrency and CRIX Time Series',\n"," 'This Quantlet is dedicated to clustering of the unlabelled open source Ethereum Smart Contracts. First, we encode our abstracts with Distilbert embeddings and using UMAP we reduce dimensionality reduction to perform K-Means, finally again using UMAP reduce dimensionality to 2, to be able to plot it.',\n"," 'This Quantlet is dedicated to topic modeling on the existing research on Ethereum Smart Contracts. First, we encode our abstracts with Distilbert embeddings and using UMAP we reduce dimensionality reduction to perform K-Means, finally again using UMAP reduce dimensionality to 2, to be able to plot it.',\n"," 'This Quantlet is dedicated to topic modeling on the existing research on Ethereum Smart Contracts. First, we encode our abstracts with Distilbert embeddings and using UMAP we reduce dimensionality reduction to perform K-Means, finally again using UMAP reduce dimensionality to 2, to be able to plot it.',\n"," 'This Quantlet is dedicated to topic modeling on the existing research on Ethereum Smart Contracts. First, we encode our abstracts with Distilbert embeddings and using UMAP we reduce dimensionality reduction to perform K-Means, finally again using UMAP reduce dimensionality to 2, to be able to plot it.',\n"," 'Smart Contracts Network Parameters Visualization',\n"," 'This Quantlet is dedicated to gathering data on the open source source codes of Solidity Smart Contracts using the API of https://etherscan.io/. Finally, the codes are parsed and preprocessed to extract comments and stored as a csv file.',\n"," 'This Quantlet is dedicated to gathering data on the open source source codes of Solidity Smart Contracts using the API of https://etherscan.io/. Finally, the codes are parsed and preprocessed to extract comments and stored as a csv file.',\n"," 'This Quantlet is dedicated to gathering data on the open source source codes of Solidity Smart Contracts using the API of https://etherscan.io/. Finally, the codes are parsed and preprocessed to extract comments and stored as a csv file.',\n"," 'This Quantlet is dedicated to test of the difference in performance of different training modes. The wilcoxon test is used.',\n"," 'Computes a derivative estimator for UK 1973 expenditure data.',\n"," 'Estimates the densities of netincome for all years (1969-83) from the UK family expenditure survey (FES).',\n"," 'Estimates a logit model for migration data from Mecklenburg-Vorpommern, GSOEP 1991.',\n"," 'Computes a kernel regression and confidence intervals for the regression of food expenditures on net-income for the UK 1973 expenditure data.',\n"," 'Plots different kernel functions: uniform, triangle, epanechnikov, quartic, gaussian.',\n"," 'Computes a semiparametric logit model with additive component functions for Age and Amount using mgcv.',\n"," 'Illustrates bias of a histogram.',\n"," 'Computes the an orthogonal series regression food expenditures on net-income for the UK 1973 expenditure data.',\n"," 'Visualizes bias^2, variance and mean squared error (MSE) for a kernel density estimate.',\n"," 'Estimates a bivariate kernel density for age and income of migration data from Mecklenburg-Vorpommern, GSOEP 1991. The density estimate is plotted via contour lines in 2D.',\n"," 'Computes the descriptive statistics of migration data from Mecklenburg-Vorpommern, GSOEP 1991.',\n"," 'Visualizes the construction of a kernel density estimate (with additional slider to choose bandwidth and sample size).',\n"," 'Illustrates a histogram of stock returns.',\n"," 'Computes the median smoother of food expenditures on netincome for the UK 1973 expenditure data.',\n"," 'Shows the averaged squared error for the simulated data set with Gaussian (label 1) and quartic kernel (label 2). The weighted function w(u)=I(|u-0.5|<=0.4) was used. The binwidths h with minimal ASE (min1, min2) are computed.',\n"," 'Shows the Gasser-Mueller estimator and its first derivative for simulated data set.',\n"," 'Computes the regression of food share on net-income for the UK 1976 expenditure data.',\n"," 'Illustrates an ordinary histogram for stock returns.',\n"," 'Visualizes the construction of a kernel density estimate for a Gaussian kernel.',\n"," 'Illustrates Buffalo snowfall data.',\n"," 'Visualizes bias effects for a kernel density estimate via simulation n = 10000.',\n"," 'Computes the regression of food on net-income for the UK 1976 expenditure data.',\n"," 'Illustrates a mean squared error (MSE) of histogram for Buffalo snowfall data.',\n"," 'Estimates a bivariate kernel density for age and income of migration data from Mecklenburg-Vorpommern, GSOEP 1991. The density estimate is plotted in the 3D space.',\n"," 'Computes an additive fit.',\n"," 'Computes the Nadaraya-Watson estimator for the UK 1973 expenditure data.',\n"," 'Illustrates parametric link functions: logit, probit and heteroskedastic.',\n"," 'Displays different penalizing function: S, AIC, FPE, GCV, T.',\n"," 'Computes the regression of food on net-income for the UK 1976 expenditure data.',\n"," 'Illustrates a histogram of an exponential distribution.',\n"," 'Computes the local polynomial estimator for the UK 1973 expenditure data employing a gaussian kernel.',\n"," 'Computes the regression of food on net-income for the UK 1976 expenditure data using a k-nearest neighbor classification.',\n"," 'Computes the kernel density estimate for simulated multivariate normal random numbers.',\n"," 'Computes a kernel estimation of epanechnikov and triangle kernel for net-income data from the U.K. Family Expenditure Survey.',\n"," 'Computes an additive fit with normal and correlated regressors.',\n"," 'Creates plots of EUR/USD rates and returns from 2002-01-01 to 2009-01-01.',\n"," 'Computes the regression of food expenditures on netincome with four different bandwidths for the UK 1973 expenditure data.',\n"," 'Visualizes bias^2, variance and mean averaged squared error (MASE) for the regression of simulated data.',\n"," 'We conduct a simulation study in order to show consistency of component-based and integration-based Shapley curves.',\n"," 'We estimate the coverage probability for component-based and integration-based Shapley curves by bootstrap.',\n"," 'Slice plots with bootstrapped CI.',\n"," 'ICplots gives 3 plots which are an example how different AIC and BIC perform.',\n"," 'Plots three probability density functions (top) and three cumulative density functions (bottom) of the GH, Hyperbolic and NIG distributions',\n"," 'R-Script with analysis of log returns used on CRIX Forecasting App',\n"," 'R-Script with ETS model training used on CRIX Forecasting App',\n"," 'R-Script with ETS model training used on CRIX Forecasting App',\n"," 'R-Script with analysis functions used on CRIX Forecasting App',\n"," 'R-Script to load CRIX Data from the web',\n"," 'R-Script with Machine Learning model training used on CRIX Forecasting App',\n"," 'R-Script with model functions used on CRIX Forecasting App',\n"," '‘Stochastic pricing and calibration of risk premium. Out-of-sample backtesting for price prediction using zero MPR, constant MPR, smooth MPR. Sensitivity test with cross-validation.’',\n"," '‘Stochastic pricing and calibration of risk premium. Out-of-sample backtesting for price prediction using zero MPR, constant MPR, smooth MPR. Sensitivity test with cross-validation.’',\n"," 'Normalisation of German wind power utilisation for stochastic pricing.',\n"," 'Normalisation of German wind power utilisation for stochastic pricing.',\n"," 'Normalisation of German wind power utilisation for stochastic pricing.',\n"," 'Normalisation of German wind power utilisation for stochastic pricing.',\n"," 'Normalisation of German wind power utilisation for stochastic pricing.',\n"," 'Derives the ecdfs according to groups of crypto-currencies and plots them for comparison.',\n"," 'Gives a plot visualizing the trading volume of the top 10 crypto-currencies by market capitalization.',\n"," 'Plots the mean log returns of the top 10 crypto-currencies by market capitalization on a rolling window.',\n"," 'Plots the cumulative explained variance by the principal components of a PCA of the top 10 crypto-currencies by market capitalization.',\n"," 'Gives 2 histograms for the distribution of market capitalization of crypto-currencies in 2 different time periods.',\n"," 'Plots the standard variation of the log returns of the top 10 crypto-currencies by market capitalization on a rolling window.',\n"," 'Derives the alpha of the power law and plots them to compare them over time.',\n"," 'Gives 2 histograms for the distribution of market capitalization of crypto-currencies in 2 different time periods with a high overall market capitalization.',\n"," 'Compares the densities of the top 10 crypto-currencies by market capitalization against a histogram of the normal distribution.',\n"," 'LDA analysis of the abstracts of a set of scientifique papers to understand main topics that are handled in those papers.',\n"," 'LDA analysis of the abstracts of a set of scientifique papers to understand main topics that are handled in those papers.',\n"," 'LDA analysis of the abstracts of a set of scientifique papers to understand main topics that are handled in those papers.',\n"," 'Calculate probability of the data that will lie outside of the upper cutoff point',\n"," 'Plot the ubike station that located in hsinchu city area',\n"," \"['Demonstrate getting data from webpage API and scraping', 'Maping the location of UBike station all over New Taipei City']\",\n"," 'PCA and a cluster analysis for 20 randomly chosen bank notes from the swiss bank notes dataset.',\n"," 'Kernel Density Estimation Using Scikit-Learn',\n"," \"['Preprocessing Japanese text from Yahoo', 'Create a wordcloud based on the text']\",\n"," 'Summarize K topics from 130 collected abstracts with LDA method',\n"," 'Summarize K topics from 130 collected abstracts with LDA method',\n"," 'Summarize K topics from 130 collected abstracts with LDA method',\n"," 'Create a function that can generate normal distribution from polar coordinate. The basic idea is from muller-box transformation.',\n"," 'Final Project of SDA I Class: Modelling The Effect of Confirmed Number COVID-19 Ratio to The Exchange Rate of Euro to Rupiah',\n"," 'Final Project of SDA I Class: Modelling The Effect of Confirmed Number COVID-19 Ratio to The Exchange Rate of Euro to Rupiah',\n"," 'Final Project of SDA I Class: Modelling The Effect of Confirmed Number COVID-19 Ratio to The Exchange Rate of Euro to Rupiah',\n"," 'Final Project of SDA I Class: Modelling The Effect of Confirmed Number COVID-19 Ratio to The Exchange Rate of Euro to Rupiah',\n"," 'Final Project of SDA I Class: Modelling The Effect of Confirmed Number COVID-19 Ratio to The Exchange Rate of Euro to Rupiah',\n"," 'Final Project of SDA I Class: Modelling The Effect of Confirmed Number COVID-19 Ratio to The Exchange Rate of Euro to Rupiah',\n"," 'Final Project of SDA I Class: Modelling The Effect of Confirmed Number COVID-19 Ratio to The Exchange Rate of Euro to Rupiah',\n"," 'Final Project of SDA I Class: Modelling The Effect of Confirmed Number COVID-19 Ratio to The Exchange Rate of Euro to Rupiah',\n"," 'Final Project of SDA I Class: Modelling The Effect of Confirmed Number COVID-19 Ratio to The Exchange Rate of Euro to Rupiah',\n"," 'Distance measure for continuous variables using Euclidian/Manhattan/Maximum methods',\n"," 'Collect >=130 abstracts from http://www.wiwi.hu-berlin.de/de/forschung/irtg/results/discussion-papers',\n"," 'Generate 2D Normal Distribution with Polar Coordinates',\n"," 'Distance measure for continuous variables using Euclidian/Mannheim/Maximum methods',\n"," 'Generating 2D Normal Distribution with Polar Coordinates',\n"," 'The First Progress of Final Project of SDA I Class',\n"," \"['Preprocessing Japanese text from Yahoo', 'Create a wordcloud based on the text']\",\n"," \"['Preprocessing Japanese text from Yahoo', 'Create a wordcloud based on the text']\",\n"," \"['Preprocessing Japanese text from Yahoo', 'Create a wordcloud based on the text']\",\n"," 'Calculating the probability upper cutoff point of normal standard distribution',\n"," 'Final Project of SDA I Class: Modelling The Effect of Confirmed Number COVID-19 Ratio to The Exchange Rate of Euro to Rupiah. (You can change the data in python code to get the result of other models)',\n"," 'Test for Granger causality among series',\n"," 'scraping on sciencedirect with scrapy, the data will be used to create topic modeling about covid19 based on the abstract',\n"," 'scraping on sciencedirect with scrapy, the data will be used to create topic modeling about covid19 based on the abstract',\n"," 'scraping on sciencedirect with scrapy, the data will be used to create topic modeling about covid19 based on the abstract',\n"," 'scraping on sciencedirect with scrapy, the data will be used to create topic modeling about covid19 based on the abstract',\n"," 'scraping on sciencedirect with scrapy, the data will be used to create topic modeling about covid19 based on the abstract',\n"," 'Preprocessing data which is text data. it contains case folding, tokenizing, lemmatizer, and stemming.',\n"," 'Create a wordcloud about covid19 using abstract that crawled from sciencedirect',\n"," 'Use complex numbers as parameters to fit an pregnant shark that wiggling. Creates an MP4 file of the wiggling pregnant shark',\n"," 'Use complex numbers as parameters to fit an pregnant shark that wiggling. Creates an MP4 file of the wiggling pregnant shark',\n"," 'Use complex numbers as parameters to fit an pregnant shark that wiggling. Creates an MP4 file of the wiggling pregnant shark',\n"," 'Calculate the upper-cut-off of normal distribution.',\n"," 'Calculating the probability of a “normal (distribution)” to be outside the upper cutoff point',\n"," 'Calculating the probability of a “normal (distribution)” to be outside the upper cutoff point',\n"," 'Calculating the probability of a “normal (distribution)” to be outside the upper cutoff point',\n"," '2nd Iteration - Data processing and encoder-decoder',\n"," '3rd Iteration - Full-code implementation',\n"," 'Final Iteration- Final implementation',\n"," 'Extract open data of Xinbei bike',\n"," 'Proximity measure for binary variables using Jaccard/Simple matching/Tanimoto methods',\n"," 'Calculating index for Crypto Currency',\n"," 'generate normal distribution using Box-Muller method',\n"," 'simple web-scraping on vietnam.net',\n"," 'Create a function that can generate exponential distribution from polar coordinate. The basic idea is from muller-box transformation.',\n"," 'A Web Crawler for UBike website, and visualizes dataset with scatter plot',\n"," 'perform LDA Analysis on given dataset',\n"," 'Use parameters to fit a swimming fish',\n"," 'Map geographic linestring representations of public transport lines in the city of Berlin (data from VBB)',\n"," 'computes the Jaccard, simple matching and Tanimoto proximity coefficients for binary car data',\n"," 'Calculates distance matrices for Maine, New Hampshire and New York from the US health 2005 data set. The distance measures are Euclidean, Manhattan and maximum distance.',\n"," 'Proximity measure for binary variables using Jaccard/Simple matching/Tanimoto methods',\n"," 'draw moving animal',\n"," 'This is the example quantlet. You see here the coding file named the same way as the \"Name of Quantlet\" field in this document, but with the .py extension (could be also .ipynb or .r etc.). This metainfo.txt file is needed to create the README automatically and index through the quantlet.com website. So your project is searchable there. All the images in the same folder as the metainfo.txt will be attached to the README file automatically (it does not matter how you call it).',\n"," 'Plot fair premium curves',\n"," 'Plot fair premium curves',\n"," 'Plot fair premium curves',\n"," 'Implementation of the Adaptive Weights Community Detection algorithm.',\n"," 'Time series classification using PCA + SVM. Examples in cryptocurrency market.',\n"," 'Simulates the risk bound of a CARE model according to theta1 parameter constellation at expectile level tau = 0.05',\n"," 'Simulates the risk bound of a CARE model according to theta1 parameter constellation at expectile level tau = 0.05',\n"," 'Simulates the risk bound of a CARE model according to theta1 parameter constellation at expectile level tau = 0.05',\n"," 'Simulates the risk bound of a CARE model according to theta1 parameter constellation at expectile level tau = 0.05',\n"," 'Simulates the risk bound of a CARE model according to theta1 parameter constellation at expectile level tau = 0.05',\n"," 'Simulates the risk bound of a CARE model according to theta1 parameter constellation at expectile level tau = 0.05',\n"," 'Simulates the risk bound of a CARE model according to theta1 parameter constellation at expectile level tau = 0.05',\n"," 'Estimate the adaptive interval length based on the estimated critical value at expectile level 0.01 for the select three stock markets.',\n"," 'Estimate the adaptive interval length based on the estimated critical value at expectile level 0.01 for the select three stock markets.',\n"," 'Estimate the adaptive interval length based on the estimated critical value at expectile level 0.01 for the select three stock markets.',\n"," 'Estimate the adaptive interval length based on the estimated critical value at expectile level 0.01 for the select three stock markets.',\n"," 'Estimate the adaptive interval length based on the estimated critical value at expectile level 0.01 for the select three stock markets.',\n"," 'Estimate the adaptive interval length based on the estimated critical value at expectile level 0.01 for the select three stock markets.',\n"," 'Estimate the adaptive interval length based on the estimated critical value at expectile level 0.01 for the select three stock markets.',\n"," 'Estimate the adaptive interval length based on the estimated critical value at expectile level 0.01 for the select three stock markets.',\n"," 'Estimate the adaptive interval length based on the estimated critical value at expectile level 0.01 for the select three stock markets.',\n"," 'Estimate the adaptive interval length based on the estimated critical value at expectile level 0.01 for the select three stock markets.',\n"," 'Estimate the adaptive interval length based on the estimated critical value at expectile level 0.01 for the select three stock markets.',\n"," 'Evaluates the likelihood ratio test statistics for parameter constellation theta1 and expectile leve 0.01',\n"," 'Evaluates the likelihood ratio test statistics for parameter constellation theta1 and expectile leve 0.01',\n"," 'Evaluates the likelihood ratio test statistics for parameter constellation theta1 and expectile leve 0.01',\n"," 'Evaluates the likelihood ratio test statistics for parameter constellation theta1 and expectile leve 0.01',\n"," 'Evaluates the likelihood ratio test statistics for parameter constellation theta1 and expectile leve 0.01',\n"," 'Evaluates the likelihood ratio test statistics for parameter constellation theta1 and expectile leve 0.01',\n"," 'Evaluates the likelihood ratio test statistics for parameter constellation theta1 and expectile leve 0.01',\n"," 'Evaluates the critical values based on the likelihood ration and risk bound at parameter constellation theta1 and expectile leve tau = 0.05',\n"," 'Evaluates the critical values based on the likelihood ration and risk bound at parameter constellation theta1 and expectile leve tau = 0.05',\n"," 'Evaluates the critical values based on the likelihood ration and risk bound at parameter constellation theta1 and expectile leve tau = 0.05',\n"," 'Evaluates the critical values based on the likelihood ration and risk bound at parameter constellation theta1 and expectile leve tau = 0.01',\n"," 'Evaluates the critical values based on the likelihood ration and risk bound at parameter constellation theta1 and expectile leve tau = 0.01',\n"," 'Evaluates the critical values based on the likelihood ration and risk bound at parameter constellation theta1 and expectile leve tau = 0.01',\n"," 'Estimates length of the interval of homogeneity in trading days across the selected three stock markets from 3 January 2006 to 31 December 2014 for the modest (upper panel, r = 0.5) and the conservative (lower panel, r = 1) risk cases, at expectile level 0.0025.',\n"," 'Estimates length of the interval of homogeneity in trading days across the selected three stock markets from 3 January 2006 to 31 December 2014 for the modest (upper panel, r = 0.5) and the conservative (lower panel, r = 1) risk cases, at expectile level 0.0025.',\n"," 'Estimates length of the interval of homogeneity in trading days across the selected three stock markets from 3 January 2006 to 31 December 2014 for the modest (upper panel, r = 0.5) and the conservative (lower panel, r = 1) risk cases, at expectile level 0.0025.',\n"," 'Evaluates the likelihood ratio test statistics for parameter constellation theta1 and expectile leve 0.05',\n"," 'Evaluates the likelihood ratio test statistics for parameter constellation theta1 and expectile leve 0.05',\n"," 'Evaluates the likelihood ratio test statistics for parameter constellation theta1 and expectile leve 0.05',\n"," 'Evaluates the likelihood ratio test statistics for parameter constellation theta1 and expectile leve 0.05',\n"," 'Evaluates the likelihood ratio test statistics for parameter constellation theta1 and expectile leve 0.05',\n"," 'Evaluates the likelihood ratio test statistics for parameter constellation theta1 and expectile leve 0.05',\n"," 'Evaluates the likelihood ratio test statistics for parameter constellation theta1 and expectile leve 0.05',\n"," 'Plots the simulated critical values across different parameter constellations for the modest (upper panel, r = 0.5) and conservative (lower panel, r = 1) risk cases, for two expectile levels: 0.05 (blue) and 0.01 (red).',\n"," 'Provides descriptive statistics for the selected index return time series from 3 January 2005 to 31 December 2014 (2608 trading days): mean, median, minimum (Min), maximum (Max), standard deviation (Std), skewness (Skew.) and kurtosis (Kurt.)',\n"," 'Estimates the adaptive interval length based on the estimated critical value at expectile level 0.01 for the select three stock markets.',\n"," 'Estimates the adaptive interval length based on the estimated critical value at expectile level 0.01 for the select three stock markets.',\n"," 'Estimates the adaptive interval length based on the estimated critical value at expectile level 0.01 for the select three stock markets.',\n"," 'Estimates the adaptive interval length based on the estimated critical value at expectile level 0.01 for the select three stock markets.',\n"," 'Estimates the adaptive interval length based on the estimated critical value at expectile level 0.01 for the select three stock markets.',\n"," 'Estimates the adaptive interval length based on the estimated critical value at expectile level 0.01 for the select three stock markets.',\n"," 'Estimates the adaptive interval length based on the estimated critical value at expectile level 0.01 for the select three stock markets.',\n"," 'Estimates the adaptive interval length based on the estimated critical value at expectile level 0.01 for the select three stock markets.',\n"," 'Estimates the adaptive interval length based on the estimated critical value at expectile level 0.01 for the select three stock markets.',\n"," 'Estimates the adaptive interval length based on the estimated critical value at expectile level 0.01 for the select three stock markets.',\n"," 'Estimates the adaptive interval length based on the estimated critical value at expectile level 0.01 for the select three stock markets.',\n"," 'Simulates the risk bound of a CARE model according to theta1 parameter constellation at expectile level tau = 0.01',\n"," 'Simulates the risk bound of a CARE model according to theta1 parameter constellation at expectile level tau = 0.01',\n"," 'Simulates the risk bound of a CARE model according to theta1 parameter constellation at expectile level tau = 0.01',\n"," 'Simulates the risk bound of a CARE model according to theta1 parameter constellation at expectile level tau = 0.01',\n"," 'Simulates the risk bound of a CARE model according to theta1 parameter constellation at expectile level tau = 0.01',\n"," 'Simulates the risk bound of a CARE model according to theta1 parameter constellation at expectile level tau = 0.01',\n"," 'Simulates the risk bound of a CARE model according to theta1 parameter constellation at expectile level tau = 0.01',\n"," 'Estimates the parameters of CARE model with fix rolling window for selected indices. Selected rolling window lengths: 1 month (20 days), 3 months (60 days), 6 months (125 days) and 12 months (250 days) for tau = 0.01.',\n"," 'Estimates the parameters of CARE model with fix rolling window for selected indices. Selected rolling window lengths: 1 month (20 days), 3 months (60 days), 6 months (125 days) and 12 months (250 days) for tau = 0.01.',\n"," 'Estimates the parameters of CARE model with fix rolling window for selected indices. Selected rolling window lengths: 1 month (20 days), 3 months (60 days), 6 months (125 days) and 12 months (250 days) for tau = 0.01.',\n"," 'Estimates the parameters of CARE model with fix rolling window for selected indices. Selected rolling window lengths: 1 month (20 days), 3 months (60 days), 6 months (125 days) and 12 months (250 days) for tau = 0.01.',\n"," 'Estimates the parameters of CARE model with fix rolling window for selected indices. Selected rolling window lengths: 1 month (20 days), 3 months (60 days), 6 months (125 days) and 12 months (250 days) for tau = 0.01.',\n"," 'Estimates the parameters of CARE model with fix rolling window for selected indices. Selected rolling window lengths: 1 month (20 days), 3 months (60 days), 6 months (125 days) and 12 months (250 days) for tau = 0.01.',\n"," 'Estimates the parameters of CARE model with fix rolling window for selected indices. Selected rolling window lengths: 1 month (20 days), 3 months (60 days), 6 months (125 days) and 12 months (250 days) for tau = 0.01.',\n"," 'Estimate the adaptive interval length based on the estimated critical value at expectile level 0.0025 for the select three stock markets.',\n"," 'Estimate the adaptive interval length based on the estimated critical value at expectile level 0.0025 for the select three stock markets.',\n"," 'Estimate the adaptive interval length based on the estimated critical value at expectile level 0.0025 for the select three stock markets.',\n"," 'Estimate the adaptive interval length based on the estimated critical value at expectile level 0.0025 for the select three stock markets.',\n"," 'Estimate the adaptive interval length based on the estimated critical value at expectile level 0.0025 for the select three stock markets.',\n"," 'Estimate the adaptive interval length based on the estimated critical value at expectile level 0.0025 for the select three stock markets.',\n"," 'Estimate the adaptive interval length based on the estimated critical value at expectile level 0.0025 for the select three stock markets.',\n"," 'Estimate the adaptive interval length based on the estimated critical value at expectile level 0.0025 for the select three stock markets.',\n"," 'Estimate the adaptive interval length based on the estimated critical value at expectile level 0.0025 for the select three stock markets.',\n"," 'Estimate the adaptive interval length based on the estimated critical value at expectile level 0.0025 for the select three stock markets.',\n"," 'Estimate the adaptive interval length based on the estimated critical value at expectile level 0.0025 for the select three stock markets.',\n"," 'Estimates length of the interval of homogeneity in trading days across the selected three stock markets from 3 January 2006 to 31 December 2014 for the modest (upper panel, r = 0.5) and the conservative (lower panel, r = 1) risk cases, at expectile level 0.05.',\n"," 'Estimates length of the interval of homogeneity in trading days across the selected three stock markets from 3 January 2006 to 31 December 2014 for the modest (upper panel, r = 0.5) and the conservative (lower panel, r = 1) risk cases, at expectile level 0.05.',\n"," 'Estimates length of the interval of homogeneity in trading days across the selected three stock markets from 3 January 2006 to 31 December 2014 for the modest (upper panel, r = 0.5) and the conservative (lower panel, r = 1) risk cases, at expectile level 0.05.',\n"," 'Evaluates the critical values based on the likelihood ration and risk bound at parameter constellation theta3 and expectile leve tau = 0.05',\n"," 'Evaluates the critical values based on the likelihood ration and risk bound at parameter constellation theta3 and expectile leve tau = 0.05',\n"," 'Evaluates the critical values based on the likelihood ration and risk bound at parameter constellation theta3 and expectile leve tau = 0.05',\n"," 'Evaluates the likelihood ratio test statistics for parameter constellation theta3 and expectile leve 0.01',\n"," 'Evaluates the likelihood ratio test statistics for parameter constellation theta3 and expectile leve 0.01',\n"," 'Evaluates the likelihood ratio test statistics for parameter constellation theta3 and expectile leve 0.01',\n"," 'Evaluates the likelihood ratio test statistics for parameter constellation theta3 and expectile leve 0.01',\n"," 'Evaluates the likelihood ratio test statistics for parameter constellation theta3 and expectile leve 0.01',\n"," 'Evaluates the likelihood ratio test statistics for parameter constellation theta3 and expectile leve 0.01',\n"," 'Evaluates the likelihood ratio test statistics for parameter constellation theta3 and expectile leve 0.01',\n"," 'Simulates the risk bound of a CARE model according to theta2 parameter constellation at expectile level tau = 0.01',\n"," 'Simulates the risk bound of a CARE model according to theta2 parameter constellation at expectile level tau = 0.01',\n"," 'Simulates the risk bound of a CARE model according to theta2 parameter constellation at expectile level tau = 0.01',\n"," 'Simulates the risk bound of a CARE model according to theta2 parameter constellation at expectile level tau = 0.01',\n"," 'Simulates the risk bound of a CARE model according to theta2 parameter constellation at expectile level tau = 0.01',\n"," 'Simulates the risk bound of a CARE model according to theta2 parameter constellation at expectile level tau = 0.01',\n"," 'Simulates the risk bound of a CARE model according to theta2 parameter constellation at expectile level tau = 0.01',\n"," 'Evaluates the critical values based on the likelihood ration and risk bound at parameter constellation theta2 and expectile leve tau = 0.01',\n"," 'Evaluates the critical values based on the likelihood ration and risk bound at parameter constellation theta2 and expectile leve tau = 0.01',\n"," 'Evaluates the critical values based on the likelihood ration and risk bound at parameter constellation theta2 and expectile leve tau = 0.01',\n"," 'Generate analytical intervals for LCARE model',\n"," 'Evaluates the likelihood ratio test statistics for parameter constellation theta2 and expectile leve 0.05',\n"," 'Evaluates the likelihood ratio test statistics for parameter constellation theta2 and expectile leve 0.05',\n"," 'Evaluates the likelihood ratio test statistics for parameter constellation theta2 and expectile leve 0.05',\n"," 'Evaluates the likelihood ratio test statistics for parameter constellation theta2 and expectile leve 0.05',\n"," 'Evaluates the likelihood ratio test statistics for parameter constellation theta2 and expectile leve 0.05',\n"," 'Evaluates the likelihood ratio test statistics for parameter constellation theta2 and expectile leve 0.05',\n"," 'Evaluates the likelihood ratio test statistics for parameter constellation theta2 and expectile leve 0.05',\n"," 'Simulates the risk bound of a CARE model according to theta3 parameter constellation at expectile level tau = 0.05.',\n"," 'Simulates the risk bound of a CARE model according to theta3 parameter constellation at expectile level tau = 0.05.',\n"," 'Simulates the risk bound of a CARE model according to theta3 parameter constellation at expectile level tau = 0.05.',\n"," 'Simulates the risk bound of a CARE model according to theta3 parameter constellation at expectile level tau = 0.05.',\n"," 'Simulates the risk bound of a CARE model according to theta3 parameter constellation at expectile level tau = 0.05.',\n"," 'Simulates the risk bound of a CARE model according to theta3 parameter constellation at expectile level tau = 0.05.',\n"," 'Simulates the risk bound of a CARE model according to theta3 parameter constellation at expectile level tau = 0.05.',\n"," 'Simulates the risk bound of a CARE model according to theta3 parameter constellation at expectile level tau = 0.01',\n"," 'Simulates the risk bound of a CARE model according to theta3 parameter constellation at expectile level tau = 0.01',\n"," 'Simulates the risk bound of a CARE model according to theta3 parameter constellation at expectile level tau = 0.01',\n"," 'Simulates the risk bound of a CARE model according to theta3 parameter constellation at expectile level tau = 0.01',\n"," 'Simulates the risk bound of a CARE model according to theta3 parameter constellation at expectile level tau = 0.01',\n"," 'Simulates the risk bound of a CARE model according to theta3 parameter constellation at expectile level tau = 0.01',\n"," 'Simulates the risk bound of a CARE model according to theta3 parameter constellation at expectile level tau = 0.01',\n"," 'Evaluates the likelihood ratio test statistics for parameter constellation theta2 and expectile leve 0.01',\n"," 'Evaluates the likelihood ratio test statistics for parameter constellation theta2 and expectile leve 0.01',\n"," 'Evaluates the likelihood ratio test statistics for parameter constellation theta2 and expectile leve 0.01',\n"," 'Evaluates the likelihood ratio test statistics for parameter constellation theta2 and expectile leve 0.01',\n"," 'Evaluates the likelihood ratio test statistics for parameter constellation theta2 and expectile leve 0.01',\n"," 'Evaluates the likelihood ratio test statistics for parameter constellation theta2 and expectile leve 0.01',\n"," 'Evaluates the likelihood ratio test statistics for parameter constellation theta2 and expectile leve 0.01',\n"," 'Evaluates the critical values based on the likelihood ration and risk bound at parameter constellation theta2 and expectile leve tau = 0.05',\n"," 'Evaluates the critical values based on the likelihood ration and risk bound at parameter constellation theta2 and expectile leve tau = 0.05',\n"," 'Evaluates the critical values based on the likelihood ration and risk bound at parameter constellation theta2 and expectile leve tau = 0.05',\n"," 'Simulates paths of a CARE model with fixed parameter  vector.',\n"," 'Simulates paths of a CARE model with fixed parameter  vector.',\n"," 'Simulates paths of a CARE model with fixed parameter  vector.',\n"," 'Simulates the risk bound of a CARE model according to theta2 parameter constellation at expectile level tau = 0.05',\n"," 'Simulates the risk bound of a CARE model according to theta2 parameter constellation at expectile level tau = 0.05',\n"," 'Simulates the risk bound of a CARE model according to theta2 parameter constellation at expectile level tau = 0.05',\n"," 'Simulates the risk bound of a CARE model according to theta2 parameter constellation at expectile level tau = 0.05',\n"," 'Simulates the risk bound of a CARE model according to theta2 parameter constellation at expectile level tau = 0.05',\n"," 'Simulates the risk bound of a CARE model according to theta2 parameter constellation at expectile level tau = 0.05',\n"," 'Simulates the risk bound of a CARE model according to theta2 parameter constellation at expectile level tau = 0.05',\n"," 'Evaluates the likelihood ratio test statistics for parameter constellation theta3 and expectile leve 0.05',\n"," 'Evaluates the likelihood ratio test statistics for parameter constellation theta3 and expectile leve 0.05',\n"," 'Evaluates the likelihood ratio test statistics for parameter constellation theta3 and expectile leve 0.05',\n"," 'Evaluates the likelihood ratio test statistics for parameter constellation theta3 and expectile leve 0.05',\n"," 'Evaluates the likelihood ratio test statistics for parameter constellation theta3 and expectile leve 0.05',\n"," 'Evaluates the likelihood ratio test statistics for parameter constellation theta3 and expectile leve 0.05',\n"," 'Evaluates the likelihood ratio test statistics for parameter constellation theta3 and expectile leve 0.05',\n"," 'Summarizes the risk bound for different parameter constellations and two expectile levels 0.01 and 0.05.',\n"," 'Estimates the parameters of CARE model with fix rolling window for selected indices. Selected rolling window lengths: 1 month (20 days), 3 months (60 days), 6 months (125 days) and 12 months (250 days) for tau = 0.05.',\n"," 'Estimates the parameters of CARE model with fix rolling window for selected indices. Selected rolling window lengths: 1 month (20 days), 3 months (60 days), 6 months (125 days) and 12 months (250 days) for tau = 0.05.',\n"," 'Estimates the parameters of CARE model with fix rolling window for selected indices. Selected rolling window lengths: 1 month (20 days), 3 months (60 days), 6 months (125 days) and 12 months (250 days) for tau = 0.05.',\n"," 'Estimates the parameters of CARE model with fix rolling window for selected indices. Selected rolling window lengths: 1 month (20 days), 3 months (60 days), 6 months (125 days) and 12 months (250 days) for tau = 0.05.',\n"," 'Estimates the parameters of CARE model with fix rolling window for selected indices. Selected rolling window lengths: 1 month (20 days), 3 months (60 days), 6 months (125 days) and 12 months (250 days) for tau = 0.05.',\n"," 'Estimates the parameters of CARE model with fix rolling window for selected indices. Selected rolling window lengths: 1 month (20 days), 3 months (60 days), 6 months (125 days) and 12 months (250 days) for tau = 0.05.',\n"," 'Estimates the parameters of CARE model with fix rolling window for selected indices. Selected rolling window lengths: 1 month (20 days), 3 months (60 days), 6 months (125 days) and 12 months (250 days) for tau = 0.05.',\n"," 'Evaluates the critical values based on the likelihood ration and risk bound at parameter constellation theta3 and expectile leve tau = 0.01',\n"," 'Evaluates the critical values based on the likelihood ration and risk bound at parameter constellation theta3 and expectile leve tau = 0.01',\n"," 'Evaluates the critical values based on the likelihood ration and risk bound at parameter constellation theta3 and expectile leve tau = 0.01',\n"," 'Estimates length of the interval of homogeneity in trading days across the selected three stock markets from 3 January 2006 to 31 December 2014 for the modest (upper panel, r = 0.8) and the conservative (lower panel, r = 1) risk cases, at expectile level 0.01.',\n"," 'Estimates length of the interval of homogeneity in trading days across the selected three stock markets from 3 January 2006 to 31 December 2014 for the modest (upper panel, r = 0.8) and the conservative (lower panel, r = 1) risk cases, at expectile level 0.01.',\n"," 'Estimates length of the interval of homogeneity in trading days across the selected three stock markets from 3 January 2006 to 31 December 2014 for the modest (upper panel, r = 0.8) and the conservative (lower panel, r = 1) risk cases, at expectile level 0.01.',\n"," 'We provide results for \"Quantinar: A Blockchain p2p Ecosystem for Honest Scientific Research\", Bag et all (2022). Please refer to the GitHub wiki for a detailed description on how to use the code!',\n"," 'Code infrastructure for paper Hedging Cryptocurrency Options',\n"," 'Code infrastructure for paper Hedging Cryptocurrency Options',\n"," 'Code infrastructure for paper Hedging Cryptocurrency Options',\n"," 'Code infrastructure for paper Hedging Cryptocurrency Options',\n"," 'Code infrastructure for paper Hedging Cryptocurrency Options',\n"," 'Code infrastructure for paper Hedging Cryptocurrency Options',\n"," 'Code infrastructure for paper Hedging Cryptocurrency Options',\n"," 'Code infrastructure for paper Hedging Cryptocurrency Options',\n"," 'Code infrastructure for paper Hedging Cryptocurrency Options',\n"," 'Code infrastructure for paper Hedging Cryptocurrency Options',\n"," 'Code infrastructure for paper Hedging Cryptocurrency Options',\n"," 'Code infrastructure for paper Hedging Cryptocurrency Options',\n"," 'Code infrastructure for paper Hedging Cryptocurrency Options',\n"," 'Code infrastructure for paper Hedging Cryptocurrency Options',\n"," 'Code infrastructure for paper Hedging Cryptocurrency Options',\n"," 'Code infrastructure for paper Hedging Cryptocurrency Options',\n"," 'Code infrastructure for paper Hedging Cryptocurrency Options',\n"," 'Code infrastructure for paper Hedging Cryptocurrency Options',\n"," 'Code infrastructure for paper Hedging Cryptocurrency Options',\n"," 'Code infrastructure for paper Hedging Cryptocurrency Options',\n"," 'Code infrastructure for paper Hedging Cryptocurrency Options',\n"," 'Code infrastructure for paper Hedging Cryptocurrency Options',\n"," 'Code infrastructure for paper Hedging Cryptocurrency Options',\n"," 'Code infrastructure for paper Hedging Cryptocurrency Options',\n"," 'Code infrastructure for paper Hedging Cryptocurrency Options',\n"," 'Code infrastructure for paper Hedging Cryptocurrency Options',\n"," 'Code infrastructure for paper Hedging Cryptocurrency Options',\n"," 'Code infrastructure for paper Hedging Cryptocurrency Options',\n"," 'Code infrastructure for paper Hedging Cryptocurrency Options',\n"," 'Code infrastructure for paper Hedging Cryptocurrency Options',\n"," 'Code infrastructure for paper Hedging Cryptocurrency Options',\n"," 'Code infrastructure for paper Hedging Cryptocurrency Options',\n"," 'Code infrastructure for paper Hedging Cryptocurrency Options',\n"," 'Simulates and estimates Markov-switching skew t - t factor copulas with two regimes.',\n"," 'Simulates and estimates Markov-switching skew t - t factor copulas with two regimes.',\n"," 'Simulates and estimates Markov-switching skew t - t factor copulas with two regimes.',\n"," 'Simulates and estimates Markov-switching skew t - t factor copulas with two regimes.',\n"," 'Simulates and estimates Markov-switching skew t - t factor copulas with two regimes.',\n"," 'Simulates and estimates Markov-switching skew t - t factor copulas with two regimes.',\n"," 'Simulates and estimates Markov-switching skew t - t factor copulas with two regimes.',\n"," 'Simulates and estimates Markov-switching skew t - t factor copulas with two regimes.',\n"," 'Simulates and estimates Markov-switching skew t - t factor copulas with two regimes.',\n"," 'Simulates and estimates Markov-switching skew t - t factor copulas with two regimes.',\n"," 'Simulates and estimates Markov-switching skew t - t factor copulas with two regimes.',\n"," 'Simulates and estimates Markov-switching skew t - t factor copulas with two regimes.',\n"," 'Simulates and estimates Markov-switching skew t - t factor copulas with two regimes.',\n"," 'Simulates and estimates Markov-switching skew t - t factor copulas with two regimes.',\n"," 'Simulates and estimates Markov-switching skew t - t factor copulas with two regimes.',\n"," 'Simulates and estimates Markov-switching skew t - t factor copulas with two regimes.',\n"," 'Simulates and estimates Markov-switching skew t - t factor copulas with two regimes.',\n"," 'Simulates and estimates Markov-switching skew t - t factor copulas with two regimes.',\n"," 'Simulates and estimates Markov-switching skew t - t factor copulas with two regimes.',\n"," 'Simulates and estimates Markov-switching skew t - t factor copulas with two regimes.',\n"," 'Simulates and estimates Markov-switching skew t - t factor copulas with two regimes.',\n"," 'Simulates and estimates Markov-switching skew t - t factor copulas with two regimes.',\n"," 'Simulates and estimates Markov-switching skew t - t factor copulas with two regimes.',\n"," 'Simulates and estimates Markov-switching skew t - t factor copulas with two regimes.',\n"," 'Simulates and estimates Markov-switching skew t - t factor copulas with two regimes.',\n"," 'Simulates and estimates Markov-switching skew t - t factor copulas with two regimes.',\n"," 'Simulates and estimates Markov-switching skew t - t factor copulas with two regimes.',\n"," 'All source codes for HAND.',\n"," 'All source codes for HAND.',\n"," 'All source codes for HAND.',\n"," 'All source codes for HAND.',\n"," 'All source codes for HAND.',\n"," 'All source codes for HAND.',\n"," 'All source codes for HAND.',\n"," \"‘Performs the Penalized Adaptive Method (PAM), a combination of propagation-separation approach and a SCAD penalty, to fit a model with possible structural changes on a given dataset. Compares the fit to models of Cochrane and Piazzesi (2005) and Ludvigson and Ng (2009). The input data are monthly observations of k-year excess bond risk premia, k = 2, 3, 4, 5, forward rates and other pre-defined macro variables. Computes RMSE, MAE, R^2 and adjusted R^2 for the fitted models. Plots the time series of the fitted vs. observed values of bond risk excess premia.'\",\n"," \"‘Performs the Penalized Adaptive Method (PAM), a combination of propagation-separation approach and a SCAD penalty, to fit a model with possible structural changes on a given dataset. Compares the fit to models of Cochrane and Piazzesi (2005) and Ludvigson and Ng (2009). The input data are monthly observations of k-year excess bond risk premia, k = 2, 3, 4, 5, forward rates and other pre-defined macro variables. Computes RMSE, MAE, R^2 and adjusted R^2 for the fitted models. Plots the time series of the fitted vs. observed values of bond risk excess premia.'\",\n"," \"‘Performs the Penalized Adaptive Method (PAM), a combination of propagation-separation approach and a SCAD penalty, to fit a model with possible structural changes on a given dataset. Compares the fit to models of Cochrane and Piazzesi (2005) and Ludvigson and Ng (2009). The input data are monthly observations of k-year excess bond risk premia, k = 2, 3, 4, 5, forward rates and other pre-defined macro variables. Computes RMSE, MAE, R^2 and adjusted R^2 for the fitted models. Plots the time series of the fitted vs. observed values of bond risk excess premia.'\",\n"," '‘Performs the Penalized Adaptive Method (PAM), a combination of propagation-separation approach and a SCAD penalty, to fit a model to a simulated data with a pre-defined change point. Computes the percentage of correctly identified change points over a specified number of scenarios.’',\n"," '‘Performs the Penalized Adaptive Method (PAM), a combination of propagation-separation approach and a SCAD penalty, to fit a model to a simulated data with a pre-defined change point. Computes the percentage of correctly identified change points over a specified number of scenarios.’',\n"," '‘Performs the Penalized Adaptive Method (PAM), a combination of propagation-separation approach and a SCAD penalty, to fit a model to a simulated data with a pre-defined change point. Computes the percentage of correctly identified change points over a specified number of scenarios.’',\n"," '‘Performs the Penalized Adaptive Method (PAM), a combination of propagation-separation approach and a SCAD penalty, to evaluate empirical coverage probability of bootstrapped confidence regions of penalized likelihood ratio.’',\n"," '‘Performs the Penalized Adaptive Method (PAM), a combination of propagation-separation approach and a SCAD penalty, to evaluate empirical coverage probability of bootstrapped confidence regions of penalized likelihood ratio.’',\n"," '‘Performs the Penalized Adaptive Method (PAM), a combination of propagation-separation approach and a SCAD penalty, to evaluate empirical coverage probability of bootstrapped confidence regions of penalized likelihood ratio.’',\n"," '‘Performs the Penalized Adaptive Method (PAM), a combination of propagation-separation approach and a SCAD penalty, to fit a model with possible structural changes on a given dataset. The model is then used for forecasting over a 1-year horizon. The input data are monthly observations of k-year excess bond risk premia, k = 2, 3, 4, 5, forward rates and other pre-defined macro variables. Computes RMSPE and MAPE for the forecasted values. Plots the time series of the predicted vs. observed values of bond risk excess premia over the prediction interval.’',\n"," '‘Performs the Penalized Adaptive Method (PAM), a combination of propagation-separation approach and a SCAD penalty, to fit a model with possible structural changes on a given dataset. The model is then used for forecasting over a 1-year horizon. The input data are monthly observations of k-year excess bond risk premia, k = 2, 3, 4, 5, forward rates and other pre-defined macro variables. Computes RMSPE and MAPE for the forecasted values. Plots the time series of the predicted vs. observed values of bond risk excess premia over the prediction interval.’',\n"," '‘Performs the Penalized Adaptive Method (PAM), a combination of propagation-separation approach and a SCAD penalty, to fit a model with possible structural changes on a given dataset. The model is then used for forecasting over a 1-year horizon. The input data are monthly observations of k-year excess bond risk premia, k = 2, 3, 4, 5, forward rates and other pre-defined macro variables. Computes RMSPE and MAPE for the forecasted values. Plots the time series of the predicted vs. observed values of bond risk excess premia over the prediction interval.’',\n"," '‘Performs the Penalized Adaptive Method (PAM), a combination of propagation-separation approach and a SCAD penalty, to fit a model with possible structural changes on a given dataset. The model is then used for forecasting over a 1-year horizon. The input data are monthly observations of k-year excess bond risk premia, k = 2, 3, 4, 5, forward rates and other pre-defined macro variables. Computes RMSPE and MAPE for the forecasted values. Plots the time series of the predicted vs. observed values of bond risk excess premia over the prediction interval.’',\n"," 'This Quantlet provides necessary functions for portfolio optimization. Single optimization routines are stored in the subfolder opt. The Quantlet wraps them and produces a set of estimated weights as well as the true returns of the portfolios.',\n"," 'This Quantlet processes and prepares the raw data used in the thesis. It creates returns and formats them into workable dataframes.',\n"," 'This Quantlet produces visual results of the optimization routines. It gives information about (cumulated) performance, density, success metrics and their reliability as well as results according to strategies and estimators.',\n"," 'This Quantlet produces visual results of the optimization routines. It gives information about (cumulated) performance, density, success metrics and their reliability as well as results according to strategies and estimators.',\n"," 'This Quantlet produces visual results of the optimization routines. It gives information about (cumulated) performance, density, success metrics and their reliability as well as results according to strategies and estimators.',\n"," 'Add Crypto currencies to Portugal stocks or DAX30 stocks to form portfolios using Markowitz(1952) method.',\n"," 'Provides the scatter plot of log returns of gold, US treasury bond 3 yr and CRIX.',\n"," 'Provides a boxplot comparing the standard deviation of cryptocurrencies against stock markets.',\n"," 'Add Crypto currencies to Portugal stocks or DAX30 stocks to form portfolios using Markowitz(1952) method with shortsell constraints.',\n"," 'Add Crypto currencies to portfolios including Portugal stocks or DAX30 stocks to check whether there are return increase',\n"," 'Provides a boxplot comparing the median trading volume of cryptocurrencies against US stock market.',\n"," 'Estimates a linear model (ANOVA). The plot presents results of the estimation of a linear model',\n"," 'Calculates and plots the simplicial depth of a simulated data set.',\n"," 'Illustrates the proof of increasing monotonicity of a probability function on positive real numbers for a volatility model with natural parameter theta.',\n"," 'Plots the Lorenz curve in order to check the goodness of the model.',\n"," 'Draws a plot of the Kullback-Leibler function for a canonical parameter.',\n"," 'Estimates the risk aversion parameter based on the data given in the table and using a logit model.',\n"," 'Performs a linear regression with data from The General Social Survey (GSS) using an instrumental variable.',\n"," 'Performs a Kolmogorov-Smirnov test on the DAX log return data from 2009-12-21 to 2011-12-22',\n"," 'Plots the time series of the DAX30 index from 2009-12-21 to 2011-12-22',\n"," 'Computes or plots the sample auto-correlation function (ACF) of a univariate, stochastic time series. When called with no output arguments, AUTOCORR displays the ACF sequence with confidence bounds.',\n"," 'Performs a Jarque-Bera test on the DAX log return data from 2009-12-21 to 2011-12-22 based on skewness and kurtosis',\n"," 'Generates n = 300 samples, then performs linear regressions in the error in design model.',\n"," 'Computes GLM coefficients, standard errors, z- and p-values and gives overall model fit.',\n"," 'Illustrates the proof of an alpha-level LR test. Output is a plot.',\n"," 'Constructs the confidence bands for an expectile curve with a confidence level of 0.95.',\n"," 'Draws n observations from a standard normal distribution and plots its empirical distribution function (edf) vs. the normal cumulative distribution function (cdf). Number of draws can be entered interactively.',\n"," 'Performes a Cramer von Mises normality test on DAX log return data from 2009-12-21 to 2011-12-22',\n"," 'Plots the critical region of the Neyman-Pearson test within a specific interval if p is a density of the standard Cauchy distribution.',\n"," 'Plots the deconvoluted kernel regression curve, the kernel regression curve from the sample without measurement errors (i.e. kernel regression based on x) and the kernel regression curve from the sample with measurement errors (i.e. kernel regression based on z).',\n"," 'Tests the hypothesis of the equality of the covariance matrices on two simulated 4-dimensional samples of sizes n1=30 and n2=20.',\n"," 'Plots quarterly S&P 500 index log-returns for data from Q2 1980 to Q2 2012.',\n"," 'Generates n=300 standard normal distributed random sample, and draws the kernel density curve of the estimated kernel density function using a Gaussian kernel. Since the kernel density function is biased in a finite sample, one can not compare it with the true density directly. One rather compare it with the expectation of the kernel density function under the true density. Then it draws the expectation of the kernel density function under the true density.',\n"," 'Draws n observations from standard normal distribution and plots its empirical distribution function (edf) vs. the normal cumulative distribution function (cdf). Number of draws can be entered interactively. The point where the edf and cdf differs most is also reported.',\n"," 'Plots the significance levels in context with n from n=1 to n=200.',\n"," 'Generates an n=300 t(3) distributed random sample, and draws the kernel density curve of the estimated kernel density function using a Gaussian kernel, since the kernel density function is biased.',\n"," 'Plots the scores with respect to probability of default and response variable.',\n"," 'Draws a plot of the Kullback-Leibler function for a natural parameter.',\n"," 'Visualisation of c and beta_min dependent on least fraction omega and minimum bound for probability that specific feature is used as analysis for the behaviour of central limit theorem (CLT) of GRF.',\n"," 'Computation and visualisation of aggregation of ICQ (individual conditional quantile) with and without treatment effect computed with a generalized Random Forest, for variable S3 (=Students self-reported expectation for success in the future) in the National Mindset Study data set used by Athey and Wager in \"Estimating Treatment Effects with Causal Forests: An Application\".',\n"," 'Computation and visualisation of (mean) ICE (individual conditional expectation) computed with a generalized Random Forest, for variable S3 (=Students self-reported expectation for success in the future) in the National Mindset Study data set used by Athey and Wager in \"Estimating Treatment Effects with Causal Forests: An Application\".',\n"," 'Visualisation of huberized loss function consisting of L1 and L2 loss, its scoring function and the derivative of the scoring function for symmetric and asymmetric loss function. The loss function can be used for approximation of functions, e.g. in Generalized Random Forests.',\n"," 'Computation and visualisation of (mean) ICE (individual conditional expectation) computed with a generalized Random Forest, for variable \"ratio001\"= (Total assets / Shareholder Funds) - 1 in the peer to peer lending dataset as was used in Giudici et al. (2019).',\n"," 'Computation and visualisation of (mean) ICE (individual conditional expectation) computed with a generalized Random Forest, for variable \"ratio001\"= (Total assets / Shareholder Funds) - 1 in the peer to peer lending dataset as was used in Giudici et al. (2019).',\n"," 'Estimation of effective weights alpha_i(x) for the infeasable observations theta_tilde. The effective weights are computed by a regression forest on n=500, 1000, 2000 observations of grid data x_i = -1 + 2*i/n and target variable Y_i=theta(x_i) + eps_i, for a given theta function, here triangle function theta(x) = max(0, 1 - |x|/0.2), with Gaussian noise eps_i with mean zero and standard deviation of 0 and 0.1.',\n"," 'Estimation of effective weights alpha_i(x_1, x_2) for the infeasable observations theta_tilde. The effective weights are computed by a regression forest on a grid of observations x_ij=(-0.5 + i/n1, 0 + 0.02 * j) for i=1,...,n1 and j=0,...,50, for n1=50, 100, 200 and target variable Y_i=theta(x_ij) + eps_ij, for a given theta function, here triangle function theta(x_ij) = max(0, 1 - |x_ij,1|/0.2), with Gaussian noise eps_ij with mean zero and standard deviation of 0 and 0.1.',\n"," 'Estimation of effective weights alpha_i(x_1, x_2) for the infeasable observations theta_tilde. The effective weights are computed by a regression forest on a grid of observations x_ij=(-0.5 + i/n1, 0 + 0.02 * j) for i=1,...,n1 and j=0,...,50, for n1=50, 100, 200 and target variable Y_i=theta(x_ij) + eps_ij, for a given theta function, here triangle function theta(x_ij) = max(0, 1 - |x_ij,1|/0.2), with Gaussian noise eps_ij with mean zero and standard deviation of 0 and 0.1.',\n"," 'Estimation of effective weights alpha_i(x_1, x_2) for the infeasable observations theta_tilde. The effective weights are computed by a regression forest on a grid of observations x_ij=(-0.5 + i/n1, 0 + 0.02 * j) for i=1,...,n1 and j=0,...,50, for n1=50, 100, 200 and target variable Y_i=theta(x_ij) + eps_ij, for a given theta function, here triangle function theta(x_ij) = max(0, 1 - |x_ij,1|/0.2), with Gaussian noise eps_ij with mean zero and standard deviation of 0 and 0.1.',\n"," 'Estimation of effective weights alpha_i(x) for the infeasable observations theta_tilde. The effective weights are computed by a quantile random forest on n=500, 1000, 2000 observations of uniform distributed data x_i ~U[-1,1] and target variable Y_i=theta(x_i) + eps_i, for a given theta function, here polynomial theta(x) = x + 4x^3, with Gaussian noise eps_i with mean zero and standard deviation of 1.',\n"," 'Displays the effect of the expiration date on the level of estimated loadings corresponding to the second component.',\n"," 'Estimates second partial derivative of the curves using local polynomial regression.',\n"," 'Estimates smooth second partial derivative for each curve using local polynomial regression using individual bandwidths.',\n"," 'Estimates smooth second partial derivative for each curve using local polynomial regression using individual bandwidths.',\n"," 'Employs a moving window to estimate an AR(1) model for the loadings and to estimate standard deviation of the VDAX, and plots the results.',\n"," 'Implements the Epanechnikov kernel function for the local polynomial regression.',\n"," 'Estimates state price densities (SPD) from a sample of discretely observed and noisy call price curves at different time to maturity and strike prices. Use FPCA decomposition of the dual covariance matrix and estimate functional components of the SPD and their corresponding loadings.',\n"," 'Starts a simulation for varying number of curves and observations per curve for repeated samples, and performs a comparison of the methods.',\n"," 'Simulates real and noisy call prices using a mixture of log-normal densities.',\n"," 'Simulates real and noisy call prices using a mixture of log-normal densities.',\n"," 'Calculates the scagnostic measures for the dataset and plots the SPLOM, the scagnostics SPLOM and the heat-map of the scagnostic measures',\n"," 'Plots of COVID19 data in Austria in the time interval 27.02. - 02.04. in R. Additionaly a wordcloud - code from Python. Corona.txt is needed to create the wordcloud.',\n"," 'Plots of COVID19 data in Austria in the time interval 27.02. - 02.04. in R. Additionaly a wordcloud - code from Python. Corona.txt is needed to create the wordcloud.',\n"," 'Plots of COVID19 data in Austria in the time interval 27.02. - 02.04. in R. Additionaly a wordcloud - code from Python. Corona.txt is needed to create the wordcloud.',\n"," 'Calculates the scagnostic measures for the dataset p2p',\n"," 'Three dimensional interactive scatterplot for unemployment regressed on the number of armed forces and the GNP. The data belongs to the longley dataset. A hyperplane for the regression results shows a negative realtionship between umemployment and the number of armed forces. The increase in GNP is associated wih higher unemployment this is odd.',\n"," 'The trivariate normal distribution is produced by random sampling. In order to specify the dependency between the variables, it is necessary to define the covariance matrix. The mean vector for all three variables is also needed. The bivariate marginal distributions are estimated by copulae.',\n"," 'A simple density estimation with the epanechnikov kernel for the DAX log-returns. The plot states the number of obervations N = 1859 and the auto-selected bandwidth, optimally chosen by the function density equal to 0.001645.',\n"," 'The binomial distribution is plotted for sample sizes 5, 10 and 100 and probabilities 0.1, 0.5 and 0.9. The plots show nicely the shape of the pdf for the different values and show when it can likely be approximated by the normal distribution.',\n"," 'Multiple LOESS regressions for DAX log-returns on FTSE log-returns. The parameter alpha is alternated to see the effect of this parameter for the regression results. The smootheness of the regression line increases with a higher alpha. This causes a smaller bias but a higher variance.',\n"," 'The HAC package is used to create a random sample. The distribution is created iteratively. First the bivariate marginal distribution is created for the variables, which have a higher correlation. In the second step the bivariate marginal distribution is stacked together with the remaining variable. The actual observations are obtained, by using the quantile function of the respective marginal distribution. The trivariate distribution is plotted in a three dimensional scatterplot with its bivariate marginal distributions.',\n"," 'Graphic of a normal distribution N(0,1), also called standard normal distribution, with a two-tailed (1-alpha)-confidence interval. It is sometimes called Gauss, after the mathematician, or Bell curve, after its shape.',\n"," 'Nine Plots are created for three different archimedean copulae with the same dependency parameter (theta=2). The figures in the left column are created based on normal marginal distributions. In the right column all copulae are based on a t-distribution with six degrees of freedom. The rows depict the Gumbel, Clayton and Frank copulae in descending order.',\n"," 'Perform normal principal component analysis (PCA) on the data \"banknotes\" from package \"ncomplete\". Plot that correlation of the NPCs and the originial variable including the unit circle in the plot for reference.',\n"," 'Graphic of a normal distribution N(0,1) with a left-tailed (1-alpha)-confidence interval',\n"," 'Plots the probability mass distribution of the Poisson distribution for different values of the shape parameter lambda. It can be seen that the poisson distribution approaches the normal distribution for higher lambda.',\n"," 'A 3d plot for the function z = (1/3) * x^3 * (1/4) * y^4, which is the integral of its first derivative with respect to both function arguments. Both arguments are in the interval [0, 1]. The function wireframe included in the lattice package is used.',\n"," 'Graphic of the Probability Mass Function of a binomial distribution vs. normal distribution. The binomial is given by bars, while the normal distr. is plotted as a red line. It can be seen that the binomial closely approaches the normal distribution for the specified values.',\n"," 'Three dimensional interactive surface plot with different front and back drawing for the multivariate function z = 0.1 * (x^2 - y^2). The front is drawn by a solid colour and the back with lines.',\n"," 'The plots for a conditional or grouped variable differ, even if you use the same variable. The left plot has three panels for the types of Species and contains four different combinations of iris characteristics. This plot corresponds to Species as a conditioning variable. Species as a grouping variable produces the right plot. Here four panels are created, which differ by combinations of variables and types of species.',\n"," 'Multidimensional Scaling is performed on a subset of the data \"cars93\" from package \"MASS\". First, the subset is selected, then the MDS model is fitted using Euclidean distance. The result is shown in a plot.',\n"," 'Box-Plot of the dataset nhtemp. The notation on the axes is calculated manually, giving a nice example of the calcuation of a five number summary, on which the box-plot is based. Note that R by default draws the whiskers to the highest/lowest value in the sample which is still in the upper-/lower fence.',\n"," 'A 3d plot for a nonlinear progamming problem. The black hyperplane depicts the function to be optimized. A red hyperplane depicts the constraint of the function.',\n"," 'The rpanel package employs different graphical user interface (GUI) controls to enable the immediate communication with the graphical output and provides dynamic graphics. In this example the rp.listbox function is presented. The function adds a list of items to the control panel. Selecting an item calls an action function and modifies the graphical output accordingly. In this case rp.listbox is used to switch between a histogram and a boxplot.',\n"," 'Time series datasets are easily plotted with lattice. Here the dataset Nile is used to illustrate the usage of the package for time series data. The plots show the annual flow of the river Nile from 1871 to 1970. The first plot depicts the whole series and the second plot splits the series into three intervals, where each interval is one panel.',\n"," 'This code shows how to create different shapes with random numbers in R. Six different objects are created with six different functions. Each function uses variables as coordinates for the shapes created.',\n"," 'The exponential distribution only works for positive random variables. The normal and logisitc distribution are special cases of the exponential distribution. Lambda defines the steepness of the pdfs.',\n"," 'Example of non-metric Multidimensional Scaling (MDS) on the data \"voting\" from package \"HSAUR2\". This approach is useful for a number of negative eigenvalues and ordinal data.',\n"," \"Stundent's t-distribution is the ratio of a normal distribution and a chisquare distribution. The t-distribution is used for several statisitcal tests. The degrees of freedom specify the moments of the distribution.\",\n"," 'The F-distribution, also called the Fisher-Snedecor distribution, is the ratio of two independent chisquared distributed random variables. The degrees of freedom of the numerator and denominatoer determine how the pdf and cdf behave. If n=1 the pdf is monotonically decreasing and the ordinate is the asymptote. For n=2 the pdf intersects the ordinate at 1 and monotically decreases. Only if n>=3 the pdf is assymetrically bell shaped.',\n"," 'A world map as an illustrative example how powerful R is. This plot is supposed to show new users an interesting application of R.',\n"," 'Multiple spline regressions for DAX log-returns on FTSE log-returns. The lambda is alternated to see the effect of lambda for the regression results. The smootheness of the regression line increases with a higher lambda. This causes a smaller bias but a higher variance.',\n"," 'Multiple unform kernel regressions for DAX log-returns on FTSE log-returns. The bandwidth is alternated to see the effect of the bandwidth for the regression results. The smootheness of the regression line increases with a higher bandwidth. This causes a smaller bias but a higher variance.',\n"," 'The Chi-squared distribution has two special pdfs. If the degrees of freedom are equal to one, the mean is undefined and the vertical axis is an asymptote. If the degrees of freedom are equal to two the pdf steadily decreases from 0.5.',\n"," 'Perform normal principal component analysis (PCA) on the data \"banknotes\" from package \"ncomplete\". Plot that correlation of the NPCs and the originial variable including the unit circle in the plot for reference.',\n"," 'The rpanel package employs different graphical user interface (GUI) controls to enable the immediate communication with the graphical output and provides dynamic graphics. In this example a control panel with a slider and a double button is created to control the bandwidth of a density plot.',\n"," 'This Quantlet produces plots to show the effect of the bin size on the smoothness of an histogram.',\n"," 'Three dimensional interactive plot for five spheres with coordiantes and radii obtained by random sampling. It is possible to interactively select different scenes of the plot.',\n"," 'A function is implemented to use all four different kernel weighting functions to compute the respective weight of a point x w.r.t. its distance to the point of evaluation x_0. Four plots are created to illustrate each weighting functions and their properties.',\n"," 'Multiple kNN regressions for DAX log-returns on FTSE log-returns. The selection of nearest neighbours k is alternated to see the effect of k for the regression curves. The smoothness of regression line is less smooth with a smaller k. This causes a smaller bias but a higher variance.',\n"," 'The chi-square distribution describes the sum of independent squared standard normal random variables. Most commonly used in tests regarding the sample variance. The pdf is bell shaped, moves to the right and becomes symmetric for higher degrees of freedom.',\n"," 'Boxplot of R example dataset Nile and manual calculation and notation of important values. The notation on the axes is calculated manually, giving a nice example of the calcuation of a five number summary, on which the box-plot is based. Note that R by default draws the whiskers to the highest/lowest value in the sample which is still in the upper-/lower fence.',\n"," 'Different lighted plots for three dimensional data. The data is obtained by univariate random normal sampling. Light sources are then alternated for the different plots to show the effect on the appearance of the plot.',\n"," 'Performs a factor analysis on a subset of the data \"decathlon\" from package \"FactoMineR\" for three factors. The result is depicted in a correlation plot.',\n"," 'Performs a factor analysis on a subset of the data \"decathlon\" from package \"FactoMineR\" for three factors. The result is depicted in a correlation plot.',\n"," 'This plot shows how to use R to create your own diagrams. As an illustration the different sample types are displayed in a diagram.',\n"," 'Plot of the Empirical Cumulative Distribution Function of the R example dataset Formaldehyde, specifically Formaldehyde$car.',\n"," 'Probability Mass Function of the binomial distribution with number of trials n = 10 and probability of success p = 0.2',\n"," 'A chi-squared distributed random variable is the sum of independent standard normally distributed rv. Histograms for continuous variables just estimate the pdf of the rv. As the degrees of freedom increase the empirical density (histogram) converges to the normal distribution.',\n"," 'The code below plots the pdfs and cdfs for three special cases of the stable distribution for which the pdf and cdf have a closed form expression. The special cases are the normal, Cauchy and Levy distribution. Normal distributions have a kurtosis of three and zero skewness. The Cauchy distribution is symmetric around its mean but has thicker tails than the normal distribution. Random variables with a Levy distribution have a skewness which tends to one. Only variables which are above the mean will be observed.',\n"," 'Analysis of principal component analysis (PCA) in order to identify the efficient number of principal components. The first three PCs are plotted against each other. Another plot shows the cumulative percentage variance against the number of components.',\n"," 'Histogram with default choice of grids based on R example data nhtemp. Histograms are commonly used to visualize data frequencies of continous variables.',\n"," 'The Cauchy distribution has no mean and no variance. Its location and form are defined by the parameters mu and sigma. Mu defines the position of the peak (mode) and the median.',\n"," 'The code creates two graphics. One graphic is a scatterplot of the bivariate normal distribution. Contours are plotted to illustrate for which combinations of the two variables the density function is the same. You can call them also isolines. The other plot is a 3 dimensional scatterplot, which shows the specific value of the joint pdf. It looks like a mountain. The highest point is the expectation.',\n"," 'Graphic of the binomial cumulative distribution function with observations n = 10 and two different probabilities for the event, p = 0.2 (black) and p = 0.6 (red).',\n"," 'A 3d plot for the Rosenbrock function: z = 100*(y - x^2)^2 + (1 - y)^2. The minimum (1, 1) is found by the Nelder Mead method.',\n"," \"Two random variables are created. One is normally distributed and one is uniformly distributed. Their correlations are calculated with different methods. Kendall and Spearman's methods are superior to Pearson's correlation measure. Both are less variant due to monotone increasing transformations and outliers.\",\n"," 'Plots two pseudo random variables with (1.) uniform distribution and (2.) N(0,1) distribution with clayton (theta=0.79) dependence structure.',\n"," 'The normal distribution is very often applied in statisitics. It is a stable distribution, which is symmetric around its mean. The standard normal distribution has an expectaion of zero and variance equals one. Higher variance leads to flatter pdfs and cdfs.',\n"," 'A bivariate normal distribution is created by the function normalCopula.The normalCopula function creates copula objects and also computes joint densities and probabilities. Both rv have a correlation of 0.7. The first plot is a standard scatterplot of the two variables. A contour and three dimensional scatterplot illustrate the cdf of the two variables. The last two plots depict the density functions for the rvs. One can observe the elliptical shape of the contour lines.',\n"," 'Interactive scatterplot for a univariate regression of unemployment on GNP. The data comes from the longley dataset and is a time series. It is possible to include and exclude the regression line and residuals.',\n"," 'A 3d plot for the function: z = 0.03*sin(x)*sin(y) - 0.05*sin(2*x)*sin(y) + 0.01*sin(x)*sin(2*y) + 0.09*sin(2*x)*sin(2*xy). All maxima are depicted by blue points and all minima by red points. The function is used to illstrate algotithms in R to find roots of multivariate functions. Here the BFGS method is used to find the optima.',\n"," 'This Q installs all necessary packages in the book Basic Computational Statistics.',\n"," 'Pie Chart of chickwts$feed depicting the shares of different feed types. It is a common technique for qualitative or discreet variables. Since the human brain does not distinguish as easily between angles and shapes as it does between lengths, bar diagrams and bar plots are preferred.',\n"," 'Bar diagram and plot of relative frequencies of the R data subset \"chickwts$feed\". It is an efficient way of plotting the frequencies of discreet variables, particularly useful to illustrate the behaviour (variation) over time.',\n"," 'Implementation of the Lagged Fibonacci Generator (LFG) for random numbers. The seed is a sequence of j integers, of which one integer should be odd. The value of the modulus is 2^l and j and k denote the number of lags. Resulting from the use of the Fibonacci Sequence, the generated sequences do not have satisfactory randomness properties.',\n"," 'Implementation of the RANDU random number generator developed by IBM in the 1960s. It is a Linear Congruential Generator procedure. This generator has some cearly non-random characteristics, due to badly chosen starting values.',\n"," 'Perform stepwise regression on all subsets to compare the values of R squared for all combinations of explanatory variables. All combinations with R squared > 0.7 are shown in a plot of R squared against subsample size.',\n"," 'Performs a cluster anlalysis on the data \"agriculture\" from package \"cluster\" using the Ward algorithm. The result is depicted in a dendrogram with the optimal clusters highlighted by red boxes.',\n"," 'The distribution of solar radiation and average ozone concentration depending on wind and temperature is depicted in the plot. The two continous conditioning variables are transformed into factors. Wind has now just four categories and Temeperature three. There are 12 possible combination of both factors coinciding with the number of panels.',\n"," 'Multiple nonparametric regressions for DAX log-returns on FTSE log-returns. They show how different nonparametric regressions predict the same simulated data.',\n"," 'Plot of two variables from the R example dataset longley (GNP, employed) including a regression line (OLS) and grid.',\n"," 'The bias, variance and mean squared error of an estimator for a histogram with origin x0 = 0 evaluated at x = 0.001 and X ~ N(0,1) is depicted in the plot. The binwidth minimizing  the MSE h MSE is depicted by a black point.',\n"," 'Two plots for the empirical cumulative distribution function standardised and not standardised log-returns for the DAX and FTSE indeces. The edf for both is close enough for the Kolmogorov-Smirnov test to not reject the null hypothesis of different distributions. This is not  true for non standardised log-returns.',\n"," 'The code produces an inteactive plot for the illustration of the Newton-Raphson method. In the example the univariate function f(x) = x^2 - 4 is optimized. An optimum is found, if the first derivative of the function at this point is equal to zero. Therefore the method looks for the root of the first derivative.',\n"," 'The code creates two graphics. One graphic is a scatterplot of the bivariate normal cumulative distribution. Contours are plotted to illustrate for which combinations of the two variables the probability is the same. You can call them alo isolines. The other plot is a 3 dimensional scatterplot, which shows the specific value of the joint pdf.',\n"," 'The plots for the density of a conditional or grouped variable are for the used data the same. In this case it is preferable to use the same variable as a group variable. R produces one panel with the densities for the different groups, which are drawn with different lines. Therefore a direct comparison of the different groups is possible. In this example the dependency of weight and the food of chickens is illustrated.',\n"," 'A stable distribution can be linearly transformed and stays a stable distribtuion. The location and scale parameter for the plots are identical. Only the skewness and kurtosis varies among the graphs. If beta, the skewness parameter, is smaller than zero, the distribuion is skewed to the left. Therefore smaller values of the rv are more likely. If beta is greater than zero, the opposite is true. Alpha determines the kurtosis of the distribution. Higher values lead to thicker tails and therefore values far from the mean are more likely.',\n"," 'Perform a cluster analysis using single, complete and average linkage algorithms on the data set \"agriculture\" from the package \"cluster\". The three resulting dendrograms are plotted to show the divergence in the results.',\n"," 'The datset barley contains yield data from Minnesota. Average yield is plotted depnding on where it is planted (site) and which variety is used. The third conditioning variable is time creating two panels. Higher average yield is associated with greater and lighter rectangles. Therefore four variables are illustrated in one graph (yield, time, variety, site).',\n"," 'Multiple Gaussian kernel regressions for DAX log-returns on FTSE log-returns. The bandwidth is alternated to see the effect of the bandwidth for the regression results. The smootheness of the regression line increases with a higher bandwidth. This causes a smaller bias but a higher variance.',\n"," 'Histogram for the height of trees from the trees package. It is possible to display density estimates for the height based on a assumed normal distribuion and estimated density.',\n"," 'Generates synthetic data in form of a partial linear model to apply simulations for causal inference estimation. The parameter of interest is the treatment or uplift effect for a binary treatment assignment.',\n"," 'Deviations in Risk Neutral Density (RND) and Historical Density (HD) can be used to develop trading strategies. The Risk Neutral Density is estimated via Rookley’s Method, which uses Local Polynomial Regression to smooth the implied volatility surface. In order to estimate the Historical Density, a GARCH(1,1) model is used to fit the bahavior of the underlying. In the scope of this thesis two python packages were developed and deployed on pypi.org - localpoly and spd_trading.',\n"," 'Deviations in Risk Neutral Density (RND) and Historical Density (HD) can be used to develop trading strategies. The Risk Neutral Density is estimated via Rookley’s Method, which uses Local Polynomial Regression to smooth the implied volatility surface. In order to estimate the Historical Density, a GARCH(1,1) model is used to fit the bahavior of the underlying. In the scope of this thesis two python packages were developed and deployed on pypi.org - localpoly and spd_trading.',\n"," 'Deviations in Risk Neutral Density (RND) and Historical Density (HD) can be used to develop trading strategies. The Risk Neutral Density is estimated via Rookley’s Method, which uses Local Polynomial Regression to smooth the implied volatility surface. In order to estimate the Historical Density, a GARCH(1,1) model is used to fit the bahavior of the underlying. In the scope of this thesis two python packages were developed and deployed on pypi.org - localpoly and spd_trading.',\n"," 'Deviations in Risk Neutral Density (RND) and Historical Density (HD) can be used to develop trading strategies. The Risk Neutral Density is estimated via Rookley’s Method, which uses Local Polynomial Regression to smooth the implied volatility surface. In order to estimate the Historical Density, a GARCH(1,1) model is used to fit the bahavior of the underlying. In the scope of this thesis two python packages were developed and deployed on pypi.org - localpoly and spd_trading.',\n"," 'Deviations in Risk Neutral Density (RND) and Historical Density (HD) can be used to develop trading strategies. The Risk Neutral Density is estimated via Rookley’s Method, which uses Local Polynomial Regression to smooth the implied volatility surface. In order to estimate the Historical Density, a GARCH(1,1) model is used to fit the bahavior of the underlying. In the scope of this thesis two python packages were developed and deployed on pypi.org - localpoly and spd_trading.',\n"," 'Deviations in Risk Neutral Density (RND) and Historical Density (HD) can be used to develop trading strategies. The Risk Neutral Density is estimated via Rookley’s Method, which uses Local Polynomial Regression to smooth the implied volatility surface. In order to estimate the Historical Density, a GARCH(1,1) model is used to fit the bahavior of the underlying. In the scope of this thesis two python packages were developed and deployed on pypi.org - localpoly and spd_trading.',\n"," 'Deviations in Risk Neutral Density (RND) and Historical Density (HD) can be used to develop trading strategies. The Risk Neutral Density is estimated via Rookley’s Method, which uses Local Polynomial Regression to smooth the implied volatility surface. In order to estimate the Historical Density, a GARCH(1,1) model is used to fit the bahavior of the underlying. In the scope of this thesis two python packages were developed and deployed on pypi.org - localpoly and spd_trading.',\n"," 'Deviations in Risk Neutral Density (RND) and Historical Density (HD) can be used to develop trading strategies. The Risk Neutral Density is estimated via Rookley’s Method, which uses Local Polynomial Regression to smooth the implied volatility surface. In order to estimate the Historical Density, a GARCH(1,1) model is used to fit the bahavior of the underlying. In the scope of this thesis two python packages were developed and deployed on pypi.org - localpoly and spd_trading.',\n"," 'Deviations in Risk Neutral Density (RND) and Historical Density (HD) can be used to develop trading strategies. The Risk Neutral Density is estimated via Rookley’s Method, which uses Local Polynomial Regression to smooth the implied volatility surface. In order to estimate the Historical Density, a GARCH(1,1) model is used to fit the bahavior of the underlying. In the scope of this thesis two python packages were developed and deployed on pypi.org - localpoly and spd_trading.',\n"," 'Deviations in Risk Neutral Density (RND) and Historical Density (HD) can be used to develop trading strategies. The Risk Neutral Density is estimated via Rookley’s Method, which uses Local Polynomial Regression to smooth the implied volatility surface. In order to estimate the Historical Density, a GARCH(1,1) model is used to fit the bahavior of the underlying. In the scope of this thesis two python packages were developed and deployed on pypi.org - localpoly and spd_trading.',\n"," 'Deviations in Risk Neutral Density (RND) and Historical Density (HD) can be used to develop trading strategies. The Risk Neutral Density is estimated via Rookley’s Method, which uses Local Polynomial Regression to smooth the implied volatility surface. In order to estimate the Historical Density, a GARCH(1,1) model is used to fit the bahavior of the underlying. In the scope of this thesis two python packages were developed and deployed on pypi.org - localpoly and spd_trading.',\n"," 'Deviations in Risk Neutral Density (RND) and Historical Density (HD) can be used to develop trading strategies. The Risk Neutral Density is estimated via Rookley’s Method, which uses Local Polynomial Regression to smooth the implied volatility surface. In order to estimate the Historical Density, a GARCH(1,1) model is used to fit the bahavior of the underlying. In the scope of this thesis two python packages were developed and deployed on pypi.org - localpoly and spd_trading.',\n"," 'Deviations in Risk Neutral Density (RND) and Historical Density (HD) can be used to develop trading strategies. The Risk Neutral Density is estimated via Rookley’s Method, which uses Local Polynomial Regression to smooth the implied volatility surface. In order to estimate the Historical Density, a GARCH(1,1) model is used to fit the bahavior of the underlying. In the scope of this thesis two python packages were developed and deployed on pypi.org - localpoly and spd_trading.',\n"," 'R code to replicate the figures in Quantile Cross-Spectral Measures of Dependence between Economic Variables. Introduce quantile cross-spectral analysis of multiple time series which is designed to detect general dependence structures emerging in quantiles of the joint distribution in the frequency domain',\n"," 'R code to replicate the figures in Quantile Cross-Spectral Measures of Dependence between Economic Variables. Introduce quantile cross-spectral analysis of multiple time series which is designed to detect general dependence structures emerging in quantiles of the joint distribution in the frequency domain',\n"," 'R code to replicate the figures in Quantile Cross-Spectral Measures of Dependence between Economic Variables. Introduce quantile cross-spectral analysis of multiple time series which is designed to detect general dependence structures emerging in quantiles of the joint distribution in the frequency domain',\n"," 'R code to replicate the figures in Quantile Cross-Spectral Measures of Dependence between Economic Variables. Introduce quantile cross-spectral analysis of multiple time series which is designed to detect general dependence structures emerging in quantiles of the joint distribution in the frequency domain',\n"," 'R code to replicate the figures in Quantile Cross-Spectral Measures of Dependence between Economic Variables. Introduce quantile cross-spectral analysis of multiple time series which is designed to detect general dependence structures emerging in quantiles of the joint distribution in the frequency domain',\n"," 'R code to replicate the figures in Quantile Cross-Spectral Measures of Dependence between Economic Variables. Introduce quantile cross-spectral analysis of multiple time series which is designed to detect general dependence structures emerging in quantiles of the joint distribution in the frequency domain',\n"," 'R code to replicate the figures in Quantile Cross-Spectral Measures of Dependence between Economic Variables. Introduce quantile cross-spectral analysis of multiple time series which is designed to detect general dependence structures emerging in quantiles of the joint distribution in the frequency domain',\n"," 'R code to replicate the figures in Quantile Cross-Spectral Measures of Dependence between Economic Variables. Introduce quantile cross-spectral analysis of multiple time series which is designed to detect general dependence structures emerging in quantiles of the joint distribution in the frequency domain',\n"," 'R code to replicate the figures in Quantile Cross-Spectral Measures of Dependence between Economic Variables. Introduce quantile cross-spectral analysis of multiple time series which is designed to detect general dependence structures emerging in quantiles of the joint distribution in the frequency domain',\n"," 'R code to replicate the figures in Quantile Cross-Spectral Measures of Dependence between Economic Variables. Introduce quantile cross-spectral analysis of multiple time series which is designed to detect general dependence structures emerging in quantiles of the joint distribution in the frequency domain',\n"," 'R code to replicate the figures in Quantile Cross-Spectral Measures of Dependence between Economic Variables. Introduce quantile cross-spectral analysis of multiple time series which is designed to detect general dependence structures emerging in quantiles of the joint distribution in the frequency domain',\n"," 'R code to replicate the figures in Quantile Cross-Spectral Measures of Dependence between Economic Variables. Introduce quantile cross-spectral analysis of multiple time series which is designed to detect general dependence structures emerging in quantiles of the joint distribution in the frequency domain',\n"," 'R code to replicate the figures in Quantile Cross-Spectral Measures of Dependence between Economic Variables. Introduce quantile cross-spectral analysis of multiple time series which is designed to detect general dependence structures emerging in quantiles of the joint distribution in the frequency domain',\n"," 'R code to replicate the figures in Quantile Cross-Spectral Measures of Dependence between Economic Variables. Introduce quantile cross-spectral analysis of multiple time series which is designed to detect general dependence structures emerging in quantiles of the joint distribution in the frequency domain',\n"," 'R code to replicate the figures in Quantile Cross-Spectral Measures of Dependence between Economic Variables. Introduce quantile cross-spectral analysis of multiple time series which is designed to detect general dependence structures emerging in quantiles of the joint distribution in the frequency domain',\n"," 'R code to replicate the figures in Quantile Cross-Spectral Measures of Dependence between Economic Variables. Introduce quantile cross-spectral analysis of multiple time series which is designed to detect general dependence structures emerging in quantiles of the joint distribution in the frequency domain',\n"," 'R code to replicate the figures in Quantile Cross-Spectral Measures of Dependence between Economic Variables. Introduce quantile cross-spectral analysis of multiple time series which is designed to detect general dependence structures emerging in quantiles of the joint distribution in the frequency domain',\n"," 'R code to replicate the figures in Quantile Cross-Spectral Measures of Dependence between Economic Variables. Introduce quantile cross-spectral analysis of multiple time series which is designed to detect general dependence structures emerging in quantiles of the joint distribution in the frequency domain',\n"," 'R code to replicate the figures in Quantile Cross-Spectral Measures of Dependence between Economic Variables. Introduce quantile cross-spectral analysis of multiple time series which is designed to detect general dependence structures emerging in quantiles of the joint distribution in the frequency domain',\n"," 'R code to replicate the figures in Quantile Cross-Spectral Measures of Dependence between Economic Variables. Introduce quantile cross-spectral analysis of multiple time series which is designed to detect general dependence structures emerging in quantiles of the joint distribution in the frequency domain',\n"," 'R code to replicate the figures in Quantile Cross-Spectral Measures of Dependence between Economic Variables. Introduce quantile cross-spectral analysis of multiple time series which is designed to detect general dependence structures emerging in quantiles of the joint distribution in the frequency domain',\n"," 'R code to replicate the figures in Quantile Cross-Spectral Measures of Dependence between Economic Variables. Introduce quantile cross-spectral analysis of multiple time series which is designed to detect general dependence structures emerging in quantiles of the joint distribution in the frequency domain',\n"," 'R code to replicate the figures in Quantile Cross-Spectral Measures of Dependence between Economic Variables. Introduce quantile cross-spectral analysis of multiple time series which is designed to detect general dependence structures emerging in quantiles of the joint distribution in the frequency domain',\n"," 'This project aims to predict the death of patients suffering from heart disease. In this way, it might be possible to adapt the treatments and maybe avoid heart failures in some cases.',\n"," 'This project aims to predict the death of patients suffering from heart disease. In this way, it might be possible to adapt the treatments and maybe avoid heart failures in some cases.',\n"," 'This project aims to predict the death of patients suffering from heart disease. In this way, it might be possible to adapt the treatments and maybe avoid heart failures in some cases.',\n"," 'Spatial analysis of Berlin rent prices and landlord premiums',\n"," 'Spatial analysis of Berlin rent prices and landlord premiums',\n"," 'Spatial analysis of Berlin rent prices and landlord premiums',\n"," 'Spatial analysis of Berlin rent prices and landlord premiums',\n"," 'Spatial analysis of Berlin rent prices and landlord premiums',\n"," 'Spatial analysis of Berlin rent prices and landlord premiums',\n"," 'Spatial analysis of Berlin rent prices and landlord premiums',\n"," 'Spatial analysis of Berlin rent prices and landlord premiums',\n"," 'Spatial analysis of Berlin rent prices and landlord premiums',\n"," 'Creates a relational database, fills it with bitcoin transaction and does an analysis on them.',\n"," 'Creates a relational database, fills it with bitcoin transaction and does an analysis on them.',\n"," 'Creates a relational database, fills it with bitcoin transaction and does an analysis on them.',\n"," 'Creates a relational database, fills it with bitcoin transaction and does an analysis on them.',\n"," 'Creates a relational database, fills it with bitcoin transaction and does an analysis on them.',\n"," 'Creates a relational database, fills it with bitcoin transaction and does an analysis on them.',\n"," 'Creates a relational database, fills it with bitcoin transaction and does an analysis on them.',\n"," 'Creates a relational database, fills it with bitcoin transaction and does an analysis on them.',\n"," 'Creates a relational database, fills it with bitcoin transaction and does an analysis on them.',\n"," 'Creates a relational database, fills it with bitcoin transaction and does an analysis on them.',\n"," 'Creates a relational database, fills it with bitcoin transaction and does an analysis on them.',\n"," 'Creates a relational database, fills it with bitcoin transaction and does an analysis on them.',\n"," 'Creates a relational database, fills it with bitcoin transaction and does an analysis on them.',\n"," 'Creates a relational database, fills it with bitcoin transaction and does an analysis on them.',\n"," 'Creates a relational database, fills it with bitcoin transaction and does an analysis on them.',\n"," 'Creates a relational database, fills it with bitcoin transaction and does an analysis on them.',\n"," 'Creates a relational database, fills it with bitcoin transaction and does an analysis on them.',\n"," 'Creates a relational database, fills it with bitcoin transaction and does an analysis on them.',\n"," 'Creates a relational database, fills it with bitcoin transaction and does an analysis on them.',\n"," 'Creates a relational database, fills it with bitcoin transaction and does an analysis on them.',\n"," 'Creates a relational database, fills it with bitcoin transaction and does an analysis on them.',\n"," 'The main file which performs the Monte Carlo simulation for the coverage ratio of the bootstrap multivariate confidence band for nonparametric kernel expectile regression. The data generating model is a homogeneous model.',\n"," 'The main file which performs the Monte Carlo simulation for the coverage ratio of the bootstrap multivariate confidence band for nonparametric kernel expectile regression. The data generating model is a homogeneous model.',\n"," 'The main file which performs the Monte Carlo simulation for the coverage ratio of the bootstrap multivariate confidence band for nonparametric kernel expectile regression. The data generating model is a homogeneous model.',\n"," 'The main file which performs the Monte Carlo simulation for the coverage ratio of the bootstrap multivariate confidence band for nonparametric kernel expectile regression. The data generating model is a homogeneous model.',\n"," 'The main file which performs the Monte Carlo simulation for the coverage ratio of the bootstrap multivariate confidence band for nonparametric kernel expectile regression. The data generating model is a homogeneous model.',\n"," 'The main file which performs the Monte Carlo simulation for the coverage ratio of the bootstrap multivariate confidence band for nonparametric kernel expectile regression. The data generating model is a heterogeneous model.',\n"," 'The main file which performs the Monte Carlo simulation for the coverage ratio of the bootstrap multivariate confidence band for nonparametric kernel expectile regression. The data generating model is a heterogeneous model.',\n"," 'The main file which performs the Monte Carlo simulation for the coverage ratio of the bootstrap multivariate confidence band for nonparametric kernel expectile regression. The data generating model is a heterogeneous model.',\n"," 'The main file which performs the Monte Carlo simulation for the coverage ratio of the bootstrap multivariate confidence band for nonparametric kernel expectile regression. The data generating model is a heterogeneous model.',\n"," 'The main file which performs the Monte Carlo simulation for the coverage ratio of the bootstrap multivariate confidence band for nonparametric kernel expectile regression. The data generating model is a heterogeneous model.',\n"," 'The main file which performs the Monte Carlo simulation for the coverage ratio of the theoretical multivariate confidence band for nonparametric kernel expectile regression. The data generating model is a heterogeneous model.',\n"," 'The main file which performs the Monte Carlo simulation for the coverage ratio of the theoretical multivariate confidence band for nonparametric kernel expectile regression. The data generating model is a heterogeneous model.',\n"," 'The main file which performs the Monte Carlo simulation for the coverage ratio of the theoretical multivariate confidence band for nonparametric kernel expectile regression. The data generating model is a heterogeneous model.',\n"," 'The main file which performs the Monte Carlo simulation for the coverage ratio of the theoretical multivariate confidence band for nonparametric kernel expectile regression. The data generating model is a heterogeneous model.',\n"," 'The main file which performs the Monte Carlo simulation for the coverage ratio of the theoretical multivariate confidence band for nonparametric kernel expectile regression. The data generating model is a heterogeneous model.',\n"," 'The main file which performs the Monte Carlo simulation for the coverage ratio of the theoretical multivariate confidence band for nonparametric kernel expectile regression. The data generating model is a homogeneous model.',\n"," 'The main file which performs the Monte Carlo simulation for the coverage ratio of the theoretical multivariate confidence band for nonparametric kernel expectile regression. The data generating model is a homogeneous model.',\n"," 'The main file which performs the Monte Carlo simulation for the coverage ratio of the theoretical multivariate confidence band for nonparametric kernel expectile regression. The data generating model is a homogeneous model.',\n"," 'The main file which performs the Monte Carlo simulation for the coverage ratio of the theoretical multivariate confidence band for nonparametric kernel expectile regression. The data generating model is a homogeneous model.',\n"," 'The main file which performs the Monte Carlo simulation for the coverage ratio of the theoretical multivariate confidence band for nonparametric kernel expectile regression. The data generating model is a homogeneous model.',\n"," 'The main file which performs the Monte Carlo simulation for the coverage ratio of the bootstrap multivariate confidence band for nonparametric kernel quantile regression. The data generating model is a heterogeneous model.',\n"," 'The main file which performs the Monte Carlo simulation for the coverage ratio of the bootstrap multivariate confidence band for nonparametric kernel quantile regression. The data generating model is a heterogeneous model.',\n"," 'The main file which performs the Monte Carlo simulation for the coverage ratio of the bootstrap multivariate confidence band for nonparametric kernel quantile regression. The data generating model is a heterogeneous model.',\n"," 'The main file which performs the Monte Carlo simulation for the coverage ratio of the bootstrap multivariate confidence band for nonparametric kernel quantile regression. The data generating model is a heterogeneous model.',\n"," 'The main file which performs the Monte Carlo simulation for the coverage ratio of the bootstrap multivariate confidence band for nonparametric kernel quantile regression. The data generating model is a heterogeneous model.',\n"," 'The main file which generates the 2 dimensional (age, years of schooling) 3D figures',\n"," 'The main file which generates the 2 dimensional (age, years of schooling) 3D figures',\n"," ...]"]},"metadata":{},"execution_count":63}]},{"cell_type":"code","source":[],"metadata":{"id":"pP1ACWL9mpAj"},"execution_count":null,"outputs":[]}]}
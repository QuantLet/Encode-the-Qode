{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers==4.28.1\n",
    "%pip install datasets\n",
    "%pip install sentencepiece\n",
    "%pip install rouge_score\n",
    "%pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import json \n",
    "import ast\n",
    "import pickle\n",
    "\n",
    "sys.path.append('../../Quantlet/Create_description/')\n",
    "\n",
    "import importlib\n",
    "import preprocessing_utils\n",
    "importlib.reload(preprocessing_utils)\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import datasets\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "\n",
    "import nltk\n",
    "from datetime import datetime\n",
    "\n",
    "import evaluate\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "\n",
    "\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "import evaluate\n",
    "metric = evaluate.load(\"rouge\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0, 1\"\n",
    "\n",
    "\n",
    "RS = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD created descriptions and the parsed Quantlets\n",
    "parsed_Qs_file = '../../data/preprocessed/Quantlet/Parsed_Qs_19032023.pkl'\n",
    "dataset = preprocessing_utils.QuantletDataset(parsed_Qs_file)\n",
    "\n",
    "with open('../../data/preprocessed/Quantlet/Descriptions_Qs_19032023.pkl', 'rb') as f:\n",
    "    descriptions = pickle.load(f)\n",
    "\n",
    "# Identify the most common language in each folder containing metainfo file\n",
    "dataset = dataset.parsed_Qs_file\n",
    "dataset.type_script = dataset.type_script.str.replace('ipynb', 'py')\n",
    "dataset['most_commen_lang'] = dataset.folder_name.map(dataset.groupby(['folder_name'])['type_script'].agg(pd.Series.mode))\n",
    "dataset['most_commen_lang'] = dataset['most_commen_lang'].astype(str)\n",
    "\n",
    "# create the additional index to merge discription back to the dataset\n",
    "dataset['desc_idx'] = dataset.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_metainfo_files = dataset.metainfo_file.astype(str).unique()\n",
    "\n",
    "tocheck_data = dataset.copy()\n",
    "tocheck_data.metainfo_file = tocheck_data.metainfo_file.astype(str)\n",
    "tocheck_data['empty'] = (tocheck_data.metainfo_file=='empty').astype(int)\n",
    "tocheck_data['empty_ratio'] = tocheck_data.folder_name.map(tocheck_data.groupby('folder_name').empty.sum()) / tocheck_data.folder_name.map(tocheck_data.groupby('folder_name').empty.count())\n",
    "\n",
    "# identify the repos that do not have metainfo files at all\n",
    "q_no_meta = tocheck_data[tocheck_data.empty_ratio==1].folder_name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates\n",
    "Q_lang = dataset[['folder_name', 'most_commen_lang']].drop_duplicates().reset_index(drop=True)\n",
    "Q_lang = Q_lang[~Q_lang.folder_name.isin(q_no_meta)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2376, 2) (595, 2)\n"
     ]
    }
   ],
   "source": [
    "# create the train and test set\n",
    "labelled, test = train_test_split(Q_lang, test_size=0.2, random_state=RS, stratify=Q_lang.most_commen_lang)\n",
    "print(labelled.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metainfo_field(row, field='keywords'):\n",
    "    try :\n",
    "        row = ast.literal_eval(row)\n",
    "    except:\n",
    "        return 'empty'\n",
    "    row = {k.lower():v for (k,v) in row.items()}\n",
    "    if field in row.keys():\n",
    "        return row[field]\n",
    "    else:\n",
    "        return 'empty'\n",
    "    \n",
    "def set_keywords_to_list(row):\n",
    "    if row=='empty':\n",
    "        return []\n",
    "    elif isinstance(row, list):\n",
    "        return row\n",
    "    else:\n",
    "        return [x.strip() for x in row.split(',')]\n",
    "    \n",
    "def get_keywords_len(row):\n",
    "    if row=='empty':\n",
    "        return 0\n",
    "    else:\n",
    "        return len(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract keywords from metainfo file\n",
    "dataset['keywords'] = dataset.metainfo_file.apply(get_metainfo_field)\n",
    "\n",
    "# remove with empty keywords\n",
    "dataset = dataset[~dataset.keywords.isna()]\n",
    "\n",
    "# keywords to list\n",
    "dataset['keywords'] = dataset.keywords.apply(set_keywords_to_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {'cryptocurrency' : ['crypto', 'crpytocurrencies', 'cryptocurrencies'], \n",
    "           'visualization' : ['data visualization', 'plot', 'plotting', 'graphical representation', 'data-visualization', 'visualisation', 'data visualisation'],\n",
    "           'machine learning' : ['ml', 'statistical learning'],\n",
    "           'deep learning' : ['dl', 'ai', 'artificial intelligence', 'neural network', 'neural networks', 'neural-network', 'neural-networks'],\n",
    "           'nlp' : ['natural language processing', 'nlp', 'textual analysis', 'text'],\n",
    "           'web scraping': ['scraping', 'crawler', 'crawling', 'web crawler', 'web crawling'],\n",
    "           'hacking' : ['hack'],\n",
    "           'principal component analysis' : ['principal component', 'pca'],\n",
    "            'time series' : ['ts', 'time-series', 'timeseries'],\n",
    "            'random forest' : ['rf', 'random forests'], }\n",
    "mapping_back = {v:k for k, v_list in mapping.items() for v in v_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_keywords = pd.read_csv('../../data/preprocessed/Quantlet/keywords_higher30.csv')\n",
    "freq_keywords = freq_keywords.keyword.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['keywords'] = dataset['keywords'].apply(lambda x: [mapping_back.get(keyword.lower(), keyword.lower()) for keyword in x])\n",
    "dataset['keywords'] = dataset['keywords'].apply(lambda x: list(set(x)))\n",
    "dataset['keywords'] = dataset['keywords'].apply(lambda x: [keyword for keyword in x if keyword in freq_keywords])\n",
    "dataset['keywords'] = dataset['keywords'].apply(lambda x: sorted(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess keywords\n",
    "dataset['keywords_n'] = dataset['keywords'].apply(get_keywords_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3863, 9) (968, 9)\n",
      "0.7996274063340923 0.20037259366590768\n"
     ]
    }
   ],
   "source": [
    "no_meta_ds = dataset.query('folder_name in @q_no_meta')\n",
    "labelled_ds = dataset.query('folder_name in @labelled.folder_name')\n",
    "test_ds = dataset.query('folder_name in @test.folder_name')\n",
    "\n",
    "print(labelled_ds.shape, test_ds.shape)\n",
    "full_shape = labelled_ds.shape[0] + test_ds.shape[0]\n",
    "print(labelled_ds.shape[0] / full_shape, test_ds.shape[0]/full_shape)\n",
    "\n",
    "\n",
    "no_meta_ds = no_meta_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_ds = labelled_ds[labelled_ds.keywords.apply(len)>0]\n",
    "test_ds = test_ds[test_ds.keywords.apply(len)>0]\n",
    "\n",
    "labelled_ds.keywords = labelled_ds.keywords.apply(lambda x : '; '.join(x))\n",
    "test_ds.keywords = test_ds.keywords.apply(lambda x : '; '.join(x))\n",
    "\n",
    "#labelled_ds = labelled_ds[labelled_ds.description.apply(len)>0]\n",
    "#test_ds = test_ds[test_ds.description.apply(len)>0]\n",
    "\n",
    "#labelled_ds.description = labelled_ds.description.apply(lambda x : '; '.join(x))\n",
    "#test_ds.description = test_ds.description.apply(lambda x : '; '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract descriptions for train, test, no meta datasets\n",
    "descriptions_labelled = [descriptions[i] for i in labelled_ds.desc_idx.values]\n",
    "descriptions_test = [descriptions[i] for i in test_ds.desc_idx.values]\n",
    "descriptions_no_meta = [descriptions[i] for i in no_meta_ds.desc_idx.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract description lists\n",
    "descriptions_labelled = [list(descr_dict.values()) for descr_dict in descriptions_labelled]\n",
    "descriptions_test = [list(descr_dict.values()) for descr_dict in descriptions_test]\n",
    "descriptions_no_meta = [list(descr_dict.values()) for descr_dict in descriptions_no_meta]\n",
    "\n",
    "descriptions_labelled_list = [' \\n '.join(descr[0]) for descr in descriptions_labelled]\n",
    "descriptions_test_list = [' \\n '.join(descr[0]) for descr in descriptions_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_json = {'version' : '0.1.0',\n",
    "                     'data' : [{'description': descriptions_labelled_list[i], \n",
    "                                'keywords' : labelled_ds.iloc[i]['keywords']} for i in range(len(descriptions_labelled_list))]}\n",
    "\n",
    "test_dataset_json = {'version' : '0.1.0',\n",
    "                     'data' : [{'description': descriptions_test_list[i], \n",
    "                                'keywords' : test_ds.iloc[i]['keywords']} for i in range(len(descriptions_test_list))]}\n",
    "\n",
    "\n",
    "with open('../../data/preprocessed/Quantlet/labelled_dataset.json', 'w') as f:\n",
    "    json.dump(train_dataset_json, f)\n",
    "\n",
    "with open('../../data/preprocessed/Quantlet/test_dataset.json', 'w') as f:\n",
    "    json.dump(test_dataset_json, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /home/RDC/zinovyee.hub/.cache/huggingface/datasets/json/default-c68c24d2b856788f/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2b2d501bf9c4f0db835a45a522efe9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d147ab472a7413c8f85c8416a7354e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdd5a02d76a54c18823b71d06c1952d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/RDC/zinovyee.hub/.cache/huggingface/datasets/json/default-c68c24d2b856788f/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13bb8dd6500c4548b76f4739b8b2ef65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /home/RDC/zinovyee.hub/.cache/huggingface/datasets/json/default-5eff949ce8d8743d/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66152df970b3401bb586414bd94402ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93982e26181746d3b8d18b13745ad229",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79d786d297154142b68e3812abb6f1c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/RDC/zinovyee.hub/.cache/huggingface/datasets/json/default-5eff949ce8d8743d/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b87b9d54b0f340729621c59fc8ffe58b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = load_dataset(\"json\", data_files=\"../../data/preprocessed/Quantlet/labelled_dataset.json\", field=\"data\")\n",
    "test_dataset = load_dataset(\"json\", data_files=\"../../data/preprocessed/Quantlet/test_dataset.json\", field=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"sshleifer/distilbart-xsum-12-3\"\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# tokenization\n",
    "encoder_max_length = 512  # demo\n",
    "decoder_max_length = 26\n",
    "\n",
    "train_data_txt, validation_data_txt = train_dataset['train'].train_test_split(test_size=0.1).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [len(desc) for desc in train_data_txt['description']]\n",
    "empty_descr_idx = np.where(np.array(a) == 0)\n",
    "\n",
    "\n",
    "b = [len(desc) for desc in validation_data_txt['description']]\n",
    "empty_descr_val_idx = np.where(np.array(b) == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'indices'=<generator object <genexpr> at 0x7f4b29656f20> of the transform datasets.arrow_dataset.Dataset.select couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    }
   ],
   "source": [
    "# create new dataset exluding those idx\n",
    "train_data_txt = train_data_txt.select(\n",
    "    (\n",
    "        i for i in range(len(train_data_txt)) \n",
    "        if i not in set(empty_descr_idx[0])\n",
    "    )\n",
    ")\n",
    "\n",
    "# create new dataset exluding those idx\n",
    "validation_data_txt = validation_data_txt.select(\n",
    "    (\n",
    "        i for i in range(len(validation_data_txt)) \n",
    "        if i not in set(empty_descr_val_idx[0])\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7243dbea6db24acea77e9115fb15bd42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3009 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dab70986fee42f9990c34e02f437d46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/335 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def batch_tokenize_preprocess(batch, tokenizer, max_source_length, max_target_length):\n",
    "    source, target = batch[\"description\"], batch[\"keywords\"]\n",
    "    source_tokenized = tokenizer(\n",
    "        source, padding=\"max_length\", truncation=True, max_length=max_source_length\n",
    "    )\n",
    "    target_tokenized = tokenizer(\n",
    "        target, padding=\"max_length\", truncation=True, max_length=max_target_length\n",
    "    )\n",
    "\n",
    "    batch = {k: v for k, v in source_tokenized.items()}\n",
    "    # Ignore padding in the loss\n",
    "    batch[\"labels\"] = [\n",
    "        [-100 if token == tokenizer.pad_token_id else token for token in l]\n",
    "        for l in target_tokenized[\"input_ids\"]\n",
    "    ]\n",
    "    return batch\n",
    "\n",
    "\n",
    "train_data = train_data_txt.map(\n",
    "    lambda batch: batch_tokenize_preprocess(\n",
    "        batch, tokenizer, encoder_max_length, decoder_max_length\n",
    "    ),\n",
    "    batched=True,\n",
    "    remove_columns=train_data_txt.column_names,\n",
    ")\n",
    "\n",
    "validation_data = validation_data_txt.map(\n",
    "    lambda batch: batch_tokenize_preprocess(\n",
    "        batch, tokenizer, encoder_max_length, decoder_max_length\n",
    "    ),\n",
    "    batched=True,\n",
    "    remove_columns=validation_data_txt.column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def postprocess_text(preds, labels):\n",
    "\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    \n",
    "    preds, labels = eval_preds\n",
    "\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    \n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "    \n",
    "    result = metric.compute(\n",
    "        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
    "    )\n",
    "\n",
    "    # Extract a few results from ROUGE\n",
    "    result = {key: value * 100 for key, value in result.items()}\n",
    "\n",
    "    prediction_lens = [\n",
    "        np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds\n",
    "    ]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"results\",\n",
    "    num_train_epochs=5,  # demo\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=16,  # demo\n",
    "    per_device_eval_batch_size=16,\n",
    "    # learning_rate=3e-05,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.1,\n",
    "    label_smoothing_factor=0.1,\n",
    "    predict_with_generate=True,\n",
    "    logging_dir=\"logs\",\n",
    "    logging_steps=100,\n",
    "    save_total_limit=3,\n",
    "    report_to=None\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=validation_data,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11' max='11' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11/11 00:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 9.648341178894043,\n",
       " 'eval_rouge1': 0.2321,\n",
       " 'eval_rouge2': 0.0,\n",
       " 'eval_rougeL': 0.2052,\n",
       " 'eval_rougeLsum': 0.2057,\n",
       " 'eval_gen_len': 18.2866,\n",
       " 'eval_runtime': 26.5313,\n",
       " 'eval_samples_per_second': 12.627,\n",
       " 'eval_steps_per_second': 0.415}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/RDC/zinovyee.hub/.local/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='475' max='475' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [475/475 06:18, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>6.041100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.153700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.548200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.271900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=475, training_loss=3.2890943989000823, metrics={'train_runtime': 378.7514, 'train_samples_per_second': 39.723, 'train_steps_per_second': 1.254, 'total_flos': 9315230810112000.0, 'train_loss': 3.2890943989000823, 'epoch': 5.0})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/RDC/zinovyee.hub/.local/lib/python3.9/site-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (62) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def generate_summary(test_samples, model):\n",
    "    inputs = tokenizer(\n",
    "        test_samples[\"description\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=encoder_max_length,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    input_ids = inputs.input_ids.to(model.device)\n",
    "    attention_mask = inputs.attention_mask.to(model.device)\n",
    "    outputs = model.generate(input_ids, attention_mask=attention_mask)\n",
    "    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return outputs, output_str\n",
    "\n",
    "\n",
    "model_before_tuning = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "test_samples = validation_data_txt.select(range(20))\n",
    "\n",
    "summaries_before_tuning = generate_summary(test_samples, model_before_tuning)[1]\n",
    "summaries_after_tuning = generate_summary(test_samples, model)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 'loss function; pareto; pdf; visualization', \" A look at some of the key questions in this week's English language language.\"), (1, 'machine learning; model; prediction; simulation; visualization', ' A look at some of the key issues faced by researchers in Python.'), (2, '3d; poisson process; price; visualization', ' Researchers at Abertawe Bro Morgan University in Cardiff have published a guide to a new type of computer programming language called NHPPALP.'), (3, 'boxplot; mean; parameter; scatterplot; visualization', ' Here is a guide to the key points on the follow-up to a paper on the USCRIME data set.'), (4, '3d; poisson process; price; visualization', ' The following is a guide to some of the key questions in this article.'), (5, 'crix; cryptocurrency; deep learning; lstm', ' The following is a guide to some of the key questions being asked by researchers at the University of Cambridge.'), (6, 'crix; cryptocurrency; deep learning; time series', ' Here is a guide to some of the key questions in this article.'), (7, 'copula; density; distribution; normal; visualization', ' A look back at some of the key stories of the week in politics.'), (8, 'normal; normal-distribution; simulation; visualization', ' Here is a look at some of the key stories of the week.'), (9, 'crix; cryptocurrency; deep learning; prediction; simulation', ' Here is a guide to some of the key parts of the research.'), (10, 'distribution; eigenvalues; multivariate; visualization', ' Here is a look at some of the key parts of the project.'), (11, 'loss function; poisson process; simulation; visualization', ' A look at some of the key topics used in the European Union this year.'), (12, 'bandwidth; implied-volatility; kernel; option; uniform', ' A look at some of the key questions in the results of Matlab.'), (13, 'visualization; scatterplot; visualization; volatility', ' Here is a look at some of the key stories of the week.'), (14, 'density; gaussian; pdf; probability; visualization', ' A look at some of the most popular forms of mathematics.'), (15, '3d; poisson process; price; visualization', ' Here is a guide to the key topics of the French language.'), (16, 'deep learning; empirical; estimation; multivariate; visualization', ' A look at some of the key stories of the week in politics.'), (17, 'estimation; quantile; regression; visualization', ' Here is a look at some of the key information about C.'), (18, 'random; normal-distribution; random; visualization', ' Here is a look at some of the key technologies used by the European Union.'), (19, 'black-scholes; european-option; implied-volatility', ' A look at some of the key topics that caught the attention of the Cambridge-based software.')]\n",
      "\n",
      "Target summaries:\n",
      "\n",
      "[(0, 'empirical; loss function; model; pareto; visualization'), (1, 'pacf'), (2, '3d; poisson process; price'), (3, 'empirical; histogram; parameter; visualization'), (4, 'poisson process; price; visualization'), (5, 'crix; cryptocurrency'), (6, 'bitcoin; cryptocurrency'), (7, 'copula; gumbel; visualization'), (8, 'kernel; regression; visualization'), (9, 'classification; machine learning; prediction; visualization'), (10, 'cryptocurrency'), (11, 'cdf; normal; random'), (12, 'bandwidth; black-scholes; implied-volatility; option-price'), (13, 'visualization'), (14, 'density; gaussian; pdf; visualization'), (15, 'poisson process; price; visualization'), (16, 'python'), (17, 'text mining; time series; visualization'), (18, 'cdf; distribution; gumbel; random; visualization'), (19, 'lasso; portfolio; quantile regression; var')]\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \n",
    "        list(zip(\n",
    "            range(len(summaries_after_tuning)),\n",
    "            summaries_after_tuning,\n",
    "            summaries_before_tuning,\n",
    "        ))\n",
    "    \n",
    ")\n",
    "print(\"\\nTarget summaries:\\n\")\n",
    "print(\n",
    "    list(enumerate(test_samples[\"keywords\"])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['empirical; loss function; model; pareto; visualization',\n",
       " 'pacf',\n",
       " '3d; poisson process; price',\n",
       " 'empirical; histogram; parameter; visualization',\n",
       " 'poisson process; price; visualization',\n",
       " 'crix; cryptocurrency',\n",
       " 'bitcoin; cryptocurrency',\n",
       " 'copula; gumbel; visualization',\n",
       " 'kernel; regression; visualization',\n",
       " 'classification; machine learning; prediction; visualization',\n",
       " 'cryptocurrency',\n",
       " 'cdf; normal; random',\n",
       " 'bandwidth; black-scholes; implied-volatility; option-price',\n",
       " 'visualization',\n",
       " 'density; gaussian; pdf; visualization',\n",
       " 'poisson process; price; visualization',\n",
       " 'python',\n",
       " 'text mining; time series; visualization',\n",
       " 'cdf; distribution; gumbel; random; visualization',\n",
       " 'lasso; portfolio; quantile regression; var']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_samples[\"keywords\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "encode_code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

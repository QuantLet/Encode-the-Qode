{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.28.1 in /home/RDC/zinovyee.hub/.local/lib/python3.9/site-packages (4.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from transformers==4.28.1) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from transformers==4.28.1) (1.24.1)\n",
      "Requirement already satisfied: requests in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from transformers==4.28.1) (2.28.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from transformers==4.28.1) (23.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from transformers==4.28.1) (2022.10.31)\n",
      "Requirement already satisfied: filelock in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from transformers==4.28.1) (3.9.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/RDC/zinovyee.hub/.local/lib/python3.9/site-packages (from transformers==4.28.1) (0.13.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /home/RDC/zinovyee.hub/.local/lib/python3.9/site-packages (from transformers==4.28.1) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from transformers==4.28.1) (4.64.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/RDC/zinovyee.hub/.local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.1) (4.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from requests->transformers==4.28.1) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/RDC/zinovyee.hub/.local/lib/python3.9/site-packages (from requests->transformers==4.28.1) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from requests->transformers==4.28.1) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from requests->transformers==4.28.1) (3.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: datasets in /home/RDC/zinovyee.hub/.local/lib/python3.9/site-packages (2.10.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from datasets) (4.64.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from datasets) (2023.1.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /home/RDC/zinovyee.hub/.local/lib/python3.9/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: xxhash in /home/RDC/zinovyee.hub/.local/lib/python3.9/site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: responses<0.19 in /home/RDC/zinovyee.hub/.local/lib/python3.9/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from datasets) (2.28.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /home/RDC/zinovyee.hub/.local/lib/python3.9/site-packages (from datasets) (0.13.3)\n",
      "Requirement already satisfied: aiohttp in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from datasets) (3.8.3)\n",
      "Requirement already satisfied: multiprocess in /home/RDC/zinovyee.hub/.local/lib/python3.9/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /home/RDC/zinovyee.hub/.local/lib/python3.9/site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: packaging in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from datasets) (23.0)\n",
      "Requirement already satisfied: pandas in /home/RDC/zinovyee.hub/.local/lib/python3.9/site-packages (from datasets) (1.3.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from datasets) (1.24.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from aiohttp->datasets) (2.1.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/RDC/zinovyee.hub/.local/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.5.0)\n",
      "Requirement already satisfied: filelock in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.9.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/RDC/zinovyee.hub/.local/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from pandas->datasets) (2022.7.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: sentencepiece in /home/RDC/zinovyee.hub/.local/lib/python3.9/site-packages (0.1.97)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: rouge_score in /home/RDC/zinovyee.hub/.local/lib/python3.9/site-packages (0.1.2)\n",
      "Requirement already satisfied: absl-py in /home/RDC/zinovyee.hub/.local/lib/python3.9/site-packages (from rouge_score) (1.4.0)\n",
      "Requirement already satisfied: numpy in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from rouge_score) (1.24.1)\n",
      "Requirement already satisfied: nltk in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from rouge_score) (3.8.1)\n",
      "Requirement already satisfied: six>=1.14.0 in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: joblib in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from nltk->rouge_score) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from nltk->rouge_score) (2022.10.31)\n",
      "Requirement already satisfied: tqdm in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from nltk->rouge_score) (4.64.1)\n",
      "Requirement already satisfied: click in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from nltk->rouge_score) (8.1.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting tabulate\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Installing collected packages: tabulate\n",
      "Successfully installed tabulate-0.9.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers==4.28.1\n",
    "%pip install datasets\n",
    "%pip install sentencepiece\n",
    "%pip install rouge_score\n",
    "%pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import json \n",
    "import ast\n",
    "\n",
    "sys.path.append('../../Quantlet/Create_description/')\n",
    "\n",
    "import importlib\n",
    "import preprocessing_utils\n",
    "importlib.reload(preprocessing_utils)\n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RS = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD created descriptions and the parsed Quantlets\n",
    "parsed_Qs_file = '../../data/preprocessed/Quantlet/Parsed_Qs_19032023.pkl'\n",
    "dataset = preprocessing_utils.QuantletDataset(parsed_Qs_file)\n",
    "\n",
    "with open('../../data/preprocessed/Quantlet/Descriptions_Qs_19032023.pkl', 'rb') as f:\n",
    "    descriptions = pickle.load(f)\n",
    "\n",
    "# Identify the most common language in each folder containing metainfo file\n",
    "dataset = dataset.parsed_Qs_file\n",
    "dataset.type_script = dataset.type_script.str.replace('ipynb', 'py')\n",
    "dataset['most_commen_lang'] = dataset.folder_name.map(dataset.groupby(['folder_name'])['type_script'].agg(pd.Series.mode))\n",
    "dataset['most_commen_lang'] = dataset['most_commen_lang'].astype(str)\n",
    "\n",
    "# create the additional index to merge discription back to the dataset\n",
    "dataset['desc_idx'] = dataset.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_metainfo_files = dataset.metainfo_file.astype(str).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tocheck_data = dataset.copy()\n",
    "tocheck_data.metainfo_file = tocheck_data.metainfo_file.astype(str)\n",
    "tocheck_data['empty'] = (tocheck_data.metainfo_file=='empty').astype(int)\n",
    "tocheck_data['empty_ratio'] = tocheck_data.folder_name.map(tocheck_data.groupby('folder_name').empty.sum()) / tocheck_data.folder_name.map(tocheck_data.groupby('folder_name').empty.count())\n",
    "\n",
    "# identify the repos that do not have metainfo files at all\n",
    "q_no_meta = tocheck_data[tocheck_data.empty_ratio==1].folder_name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates\n",
    "Q_lang = dataset[['folder_name', 'most_commen_lang']].drop_duplicates().reset_index(drop=True)\n",
    "Q_lang = Q_lang[~Q_lang.folder_name.isin(q_no_meta)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2376, 2) (595, 2)\n"
     ]
    }
   ],
   "source": [
    "# create the train and test set\n",
    "labelled, test = train_test_split(Q_lang, test_size=0.2, random_state=RS, stratify=Q_lang.most_commen_lang)\n",
    "print(labelled.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\'Name of Quantlet\\': \\'DataExpo2013_QuachSymanzikForsgren\\', \\'Published in\\': \\'Computational Statistics, Data Expo 2013 Special Issue, Volume 31, Issue 3, 2016\\', \\'Description\\': \\'This repository contains supplementary material for the paper \"Soul of the Community: An Attempt to Assess Attachment to a Community\" to enable the reader to reproduce the analysis.\\', \\'Keywords\\': \\'Machine Learning, Statistical Learning, Data Mining, Predictive Analytics, Random Forests, Archetypes, Knight Foundation\\', \\'See also\\': \\'Other articles in Data Expo 2013 Special Issue, accessible at https://github.com/COSTDataExpo2013\\', \\'Author\\': \\'Anna Quach, Juergen Symanzik, Nicole Forsgren\\', \\'Submitted\\': \\'August 27, 2015\\', \\'Datafile\\': \\'knightfoundation2008sotcdata.por, knightfoundation2009sotcdata.por, knightfoundation2010sotcdata.por\\', \\'Main function\\': \\'Read_Me.R\\', \\'Example\\': [{1: \\'archetype_convex_hull_2008.png -- Fig. 7: Graphical representation of the three archetype solution for the year 2008. The three points labeled 1, 2, and 3 are the archetypes. Communities are colored by the dominating group according to the three attachment status levels.\\'}, {2: \\'percentile_profile_archetypes_2008.png -- Fig. 8: Graphical representation to aid with the interpretation of the three archetype solution for the year 2008. The points represent the percentile for each archetype for each variable as compared to the average response to each community. See Table 4 in Appendix B for the meaning of each variable.\\'}]}'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.metainfo_file.iloc[8374]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metainfo_field(row, field='keywords'):\n",
    "    try :\n",
    "        row = ast.literal_eval(row)\n",
    "    except:\n",
    "        return 'empty'\n",
    "    row = {k.lower():v for (k,v) in row.items()}\n",
    "    if field in row.keys():\n",
    "        return row[field]\n",
    "    else:\n",
    "        return 'empty'\n",
    "    \n",
    "def set_keywords_to_list(row):\n",
    "    if row=='empty':\n",
    "        return []\n",
    "    elif isinstance(row, list):\n",
    "        return row\n",
    "    else:\n",
    "        return [x.strip() for x in row.split(',')]\n",
    "    \n",
    "def get_keywords_len(row):\n",
    "    if row=='empty':\n",
    "        return 0\n",
    "    else:\n",
    "        return len(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract keywords from metainfo file\n",
    "dataset['keywords'] = dataset.metainfo_file.apply(get_metainfo_field)\n",
    "\n",
    "# remove with empty keywords\n",
    "dataset = dataset[~dataset.keywords.isna()]\n",
    "\n",
    "# keywords to list\n",
    "dataset['keywords'] = dataset.keywords.apply(set_keywords_to_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess keywords\n",
    "dataset['keywords_n'] = dataset['keywords'].apply(get_keywords_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3863, 9) (968, 9)\n",
      "0.7996274063340923 0.20037259366590768\n"
     ]
    }
   ],
   "source": [
    "no_meta_ds = dataset.query('folder_name in @q_no_meta')\n",
    "labelled_ds = dataset.query('folder_name in @labelled.folder_name')\n",
    "test_ds = dataset.query('folder_name in @test.folder_name')\n",
    "\n",
    "print(labelled_ds.shape, test_ds.shape)\n",
    "full_shape = labelled_ds.shape[0] + test_ds.shape[0]\n",
    "print(labelled_ds.shape[0] / full_shape, test_ds.shape[0]/full_shape)\n",
    "\n",
    "\n",
    "no_meta_ds = no_meta_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_ds = labelled_ds[labelled_ds.keywords.apply(len)>0]\n",
    "test_ds = test_ds[test_ds.keywords.apply(len)>0]\n",
    "\n",
    "labelled_ds.keywords = labelled_ds.keywords.apply(lambda x : '; '.join(x))\n",
    "test_ds.keywords = test_ds.keywords.apply(lambda x : '; '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract descriptions for train, test, no meta datasets\n",
    "descriptions_labelled = [descriptions[i] for i in labelled_ds.desc_idx.values]\n",
    "descriptions_test = [descriptions[i] for i in test_ds.desc_idx.values]\n",
    "descriptions_no_meta = [descriptions[i] for i in no_meta_ds.desc_idx.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract description lists\n",
    "descriptions_labelled = [list(descr_dict.values()) for descr_dict in descriptions_labelled]\n",
    "descriptions_test = [list(descr_dict.values()) for descr_dict in descriptions_test]\n",
    "descriptions_no_meta = [list(descr_dict.values()) for descr_dict in descriptions_no_meta]\n",
    "\n",
    "descriptions_labelled_list = [' \\n '.join(descr[0]) for descr in descriptions_labelled]\n",
    "descriptions_test_list = [' \\n '.join(descr[0]) for descr in descriptions_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_json = {'version' : '0.1.0',\n",
    "                     'data' : [{'description': descriptions_labelled_list[i], \n",
    "                                'keywords' : labelled_ds.iloc[i]['keywords']} for i in range(len(descriptions_labelled_list))]}\n",
    "\n",
    "test_dataset_json = {'version' : '0.1.0',\n",
    "                     'data' : [{'description': descriptions_test_list[i], \n",
    "                                'keywords' : test_ds.iloc[i]['keywords']} for i in range(len(descriptions_test_list))]}\n",
    "\n",
    "\n",
    "with open('../../data/preprocessed/Quantlet/labelled_dataset.json', 'w') as f:\n",
    "    json.dump(train_dataset_json, f)\n",
    "\n",
    "with open('../../data/preprocessed/Quantlet/test_dataset.json', 'w') as f:\n",
    "    json.dump(test_dataset_json, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /home/RDC/zinovyee.hub/.cache/huggingface/datasets/json/default-be30c325a25e2512/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b310f99f3c84d2fb8a7a0faae0f0b51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aab19e0c7a840128b4d0dd3a03c2dcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ecc051e37844c718777b095635eccf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/RDC/zinovyee.hub/.cache/huggingface/datasets/json/default-be30c325a25e2512/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4ffc21f15134543b5a9ee99364554fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /home/RDC/zinovyee.hub/.cache/huggingface/datasets/json/default-8177b78c8cb2c947/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fdb668979fb4f72b7bbdf5fbb9ddc1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d09be809b7744739b7e688eb931cd056",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24507126bcf34244859593bf162b57e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/RDC/zinovyee.hub/.cache/huggingface/datasets/json/default-8177b78c8cb2c947/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbe1c559c7174810a78bb949375e42a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = load_dataset(\"json\", data_files=\"../../data/preprocessed/Quantlet/labelled_dataset.json\", field=\"data\")\n",
    "test_dataset = load_dataset(\"json\", data_files=\"../../data/preprocessed/Quantlet/test_dataset.json\", field=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import datasets\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "\n",
    "import nltk\n",
    "from datetime import datetime\n",
    "\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached split indices for dataset at /home/RDC/zinovyee.hub/.cache/huggingface/datasets/json/default-be30c325a25e2512/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-4c1200390fa53600.arrow and /home/RDC/zinovyee.hub/.cache/huggingface/datasets/json/default-be30c325a25e2512/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-a206d34e9cfe25d5.arrow\n"
     ]
    }
   ],
   "source": [
    "model_name = \"sshleifer/distilbart-xsum-12-3\"\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# tokenization\n",
    "encoder_max_length = 512  # demo\n",
    "decoder_max_length = 6\n",
    "\n",
    "train_data_txt, validation_data_txt = train_dataset['train'].train_test_split(test_size=0.1).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [len(desc) for desc in train_data_txt['description']]\n",
    "empty_descr_idx = np.where(np.array(a) == 0)\n",
    "\n",
    "\n",
    "b = [len(desc) for desc in validation_data_txt['description']]\n",
    "empty_descr_val_idx = np.where(np.array(b) == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new dataset exluding those idx\n",
    "train_data_txt = train_data_txt.select(\n",
    "    (\n",
    "        i for i in range(len(train_data_txt)) \n",
    "        if i not in set(empty_descr_idx[0])\n",
    "    )\n",
    ")\n",
    "\n",
    "# create new dataset exluding those idx\n",
    "validation_data_txt = validation_data_txt.select(\n",
    "    (\n",
    "        i for i in range(len(validation_data_txt)) \n",
    "        if i not in set(empty_descr_val_idx[0])\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b9457ad5d87446c9efa0f91316bd045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3458 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f86b59c5c2874472b8571ce28d579440",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/386 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def batch_tokenize_preprocess(batch, tokenizer, max_source_length, max_target_length):\n",
    "    source, target = batch[\"description\"], batch[\"keywords\"]\n",
    "    source_tokenized = tokenizer(\n",
    "        source, padding=\"max_length\", truncation=True, max_length=max_source_length\n",
    "    )\n",
    "    target_tokenized = tokenizer(\n",
    "        target, padding=\"max_length\", truncation=True, max_length=max_target_length\n",
    "    )\n",
    "\n",
    "    batch = {k: v for k, v in source_tokenized.items()}\n",
    "    # Ignore padding in the loss\n",
    "    batch[\"labels\"] = [\n",
    "        [-100 if token == tokenizer.pad_token_id else token for token in l]\n",
    "        for l in target_tokenized[\"input_ids\"]\n",
    "    ]\n",
    "    return batch\n",
    "\n",
    "\n",
    "train_data = train_data_txt.map(\n",
    "    lambda batch: batch_tokenize_preprocess(\n",
    "        batch, tokenizer, encoder_max_length, decoder_max_length\n",
    "    ),\n",
    "    batched=True,\n",
    "    remove_columns=train_data_txt.column_names,\n",
    ")\n",
    "\n",
    "validation_data = validation_data_txt.map(\n",
    "    lambda batch: batch_tokenize_preprocess(\n",
    "        batch, tokenizer, encoder_max_length, decoder_max_length\n",
    "    ),\n",
    "    batched=True,\n",
    "    remove_columns=validation_data_txt.column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"punkt\", quiet=True)\n",
    "\n",
    "metric = evaluate.load(\"rouge\")\n",
    "\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    \n",
    "    preds, labels = eval_preds\n",
    "\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    \n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "    \n",
    "    result = metric.compute(\n",
    "        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
    "    )\n",
    "\n",
    "    # Extract a few results from ROUGE\n",
    "    result = {key: value * 100 for key, value in result.items()}\n",
    "\n",
    "    prediction_lens = [\n",
    "        np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds\n",
    "    ]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"results\",\n",
    "    num_train_epochs=5,  # demo\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=16,  # demo\n",
    "    per_device_eval_batch_size=16,\n",
    "    # learning_rate=3e-05,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.1,\n",
    "    label_smoothing_factor=0.1,\n",
    "    predict_with_generate=True,\n",
    "    logging_dir=\"logs\",\n",
    "    logging_steps=100,\n",
    "    save_total_limit=3,\n",
    "    report_to=None\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=validation_data,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 00:18]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 9.085010528564453,\n",
       " 'eval_rouge1': 0.2277,\n",
       " 'eval_rouge2': 0.0,\n",
       " 'eval_rougeL': 0.2255,\n",
       " 'eval_rougeLsum': 0.221,\n",
       " 'eval_gen_len': 17.9508,\n",
       " 'eval_runtime': 19.9092,\n",
       " 'eval_samples_per_second': 19.388,\n",
       " 'eval_steps_per_second': 1.256}"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/RDC/zinovyee.hub/.local/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1030' max='1085' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1030/1085 12:25 < 00:39, 1.38 it/s, Epoch 4.74/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>6.371400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>4.345500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.614600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.183100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.850800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.661800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.411400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.266000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.152600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.984000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1085, training_loss=3.0879715176771314, metrics={'train_runtime': 786.0218, 'train_samples_per_second': 21.997, 'train_steps_per_second': 1.38, 'total_flos': 1.0705240326144e+16, 'train_loss': 3.0879715176771314, 'epoch': 5.0})"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/RDC/zinovyee.hub/.local/lib/python3.9/site-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (62) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def generate_summary(test_samples, model):\n",
    "    inputs = tokenizer(\n",
    "        test_samples[\"description\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=encoder_max_length,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    input_ids = inputs.input_ids.to(model.device)\n",
    "    attention_mask = inputs.attention_mask.to(model.device)\n",
    "    outputs = model.generate(input_ids, attention_mask=attention_mask)\n",
    "    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return outputs, output_str\n",
    "\n",
    "\n",
    "model_before_tuning = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "test_samples = validation_data_txt.select(range(16))\n",
    "\n",
    "summaries_before_tuning = generate_summary(test_samples, model_before_tuning)[1]\n",
    "summaries_after_tuning = generate_summary(test_samples, model)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 'cluster-analysis--analysis;', ' A look at some of the key snippets from the news in mathematics.'), (1, 'correlation; volatility; distribution;', ' Here is a guide to some of the key snippets from the Irish language.'), (2, 'Silverman; bandwidth-integral', ' In our series of letters from African journalists, film-maker and columnist Farai Sevenzo looks at some of the things you might have missed.'), (3, 'Silverman; bandwidth..', ' A look at some of the key elements of the study.'), (4, 'correlation; ga Dirich distribution;', ' Here is a look at some of the key stories of the week in Cyprus.'), (5, 'asset; black-.', ' A look at some of the key snippets from the past week in C.'), (6, 'loss function; empirical;. parasci', ' A look at some of the key points on the logfile.'), (7, 'Covariance; millennials', ' Here is a look at some of the key information about computer programming.'), (8, 'smart contract; e (ML;', ' A look at some of the key stories of the week.'), (9, 'power law; energy-;.', ' Here is a look at some of the key information about computer science.'), (10, 'cluster-analysis-analysis;', ' Here is a guide to some of the key questions in computer science.'), (11, 'cluster-analysis-', ' A look at some of the key questions about the tools used by the G plots.'), (12, 'CATE; ML; C CATE', ' The following is a guide to some of the key questions in this article.'), (13, 'VaR; back..', ' A look at some of the key stories of the week in politics and economics.'), (14, 'cyber-bullbull-expect', ' A look at some of the key information about how to train pd.'), (15, 'outlier analysis;.', ' Here is a guide to some of the key questions in mathematics.')]\n",
      "\n",
      "Target summaries:\n",
      "\n",
      "[(0, 'FFT; convolution; kernel smoothing; image processing; filter'), (1, 'plot; scatterplot; VWAP; residual load; renewable energy; correlation; descriptive-statistics'), (2, 'RF; GRF; infeasable function; estimation; effective weights; bandwidth'), (3, '2D; normal; polar; exponential; box-muller'), (4, 'F-test; F-statistic; critical-value; confidence-interval; test; hypothesis-testing; financial'), (5, 'financial; implied-volatility; interpolation; linear; approximation; option; volatility; risk; graphical representation; plot'), (6, 'loss function; pareto; pdf; lognormal; graphical representation; visualization'), (7, '3D; data visualization; graphical representation; scatterplot; plot; financial'), (8, 'Machine Learning (ML); NLP; Textual Analysis; Word Embeddings; SIF Embedding; FastText; Dummy Model Performance Benchmark'), (9, 'blind double auction; market simulation; energy trading; zero-intelligence traders; smart grid; blockchain; smart contract; market mechanism; market outcome; equilibrium price; line graphs; time series; energy prediction; forecast; prediction error; energy cost; energy supply; energy demand'), (10, 'nonparametric; quantile; plot; graphical representation; data visualization'), (11, 'tenure; PhD holders; time series'), (12, 'CATE; ML; simulation; causal-inference; treatment'), (13, 'POT; VaR; pareto; portfolio; historical moving window; backtest; backtesting'), (14, 'cyber-bullying; hate speech; reduction techniques; attention architecture; antisocial online behavior; Wikipedia Talk Pages; neural networks; random Forest; Lightgbm; Ridge Regression; Twitter; Formspring; Facebook'), (15, 'random number generation; upper cutoff point; outlier; normal distribution; probability')]\n",
      "\n",
      "Source documents:\n",
      "\n",
      "[(0, 'FFT; convolution; kernel smoothing; image processing; filter'), (1, 'plot; scatterplot; VWAP; residual load; renewable energy; correlation; descriptive-statistics'), (2, 'RF; GRF; infeasable function; estimation; effective weights; bandwidth'), (3, '2D; normal; polar; exponential; box-muller'), (4, 'F-test; F-statistic; critical-value; confidence-interval; test; hypothesis-testing; financial'), (5, 'financial; implied-volatility; interpolation; linear; approximation; option; volatility; risk; graphical representation; plot'), (6, 'loss function; pareto; pdf; lognormal; graphical representation; visualization'), (7, '3D; data visualization; graphical representation; scatterplot; plot; financial'), (8, 'Machine Learning (ML); NLP; Textual Analysis; Word Embeddings; SIF Embedding; FastText; Dummy Model Performance Benchmark'), (9, 'blind double auction; market simulation; energy trading; zero-intelligence traders; smart grid; blockchain; smart contract; market mechanism; market outcome; equilibrium price; line graphs; time series; energy prediction; forecast; prediction error; energy cost; energy supply; energy demand'), (10, 'nonparametric; quantile; plot; graphical representation; data visualization'), (11, 'tenure; PhD holders; time series'), (12, 'CATE; ML; simulation; causal-inference; treatment'), (13, 'POT; VaR; pareto; portfolio; historical moving window; backtest; backtesting'), (14, 'cyber-bullying; hate speech; reduction techniques; attention architecture; antisocial online behavior; Wikipedia Talk Pages; neural networks; random Forest; Lightgbm; Ridge Regression; Twitter; Formspring; Facebook'), (15, 'random number generation; upper cutoff point; outlier; normal distribution; probability')]\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \n",
    "        list(zip(\n",
    "            range(len(summaries_after_tuning)),\n",
    "            summaries_after_tuning,\n",
    "            summaries_before_tuning,\n",
    "        ))\n",
    "    \n",
    ")\n",
    "print(\"\\nTarget summaries:\\n\")\n",
    "print(\n",
    "    list(enumerate(test_samples[\"keywords\"])))\n",
    "\n",
    "print(\"\\nSource documents:\\n\")\n",
    "print(list(enumerate(test_samples[\"keywords\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "encode_code",
   "language": "python",
   "name": "encode_code"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.28.1 in /home/RDC/zinovyee.hub/.local/lib/python3.9/site-packages (4.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from transformers==4.28.1) (6.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/RDC/zinovyee.hub/.local/lib/python3.9/site-packages (from transformers==4.28.1) (0.13.3)\n",
      "Requirement already satisfied: requests in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from transformers==4.28.1) (2.28.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /home/RDC/zinovyee.hub/.local/lib/python3.9/site-packages (from transformers==4.28.1) (0.13.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from transformers==4.28.1) (2022.10.31)\n",
      "Requirement already satisfied: filelock in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from transformers==4.28.1) (3.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from transformers==4.28.1) (1.24.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from transformers==4.28.1) (4.64.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from transformers==4.28.1) (23.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/RDC/zinovyee.hub/.local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.1) (4.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from requests->transformers==4.28.1) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/RDC/zinovyee.hub/.local/lib/python3.9/site-packages (from requests->transformers==4.28.1) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from requests->transformers==4.28.1) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from requests->transformers==4.28.1) (2.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers==4.28.1\n",
    "%pip install datasets\n",
    "%pip install sentencepiece\n",
    "%pip install rouge_score\n",
    "%pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import json \n",
    "import ast\n",
    "\n",
    "sys.path.append('../../Quantlet/Create_description/')\n",
    "\n",
    "import importlib\n",
    "import preprocessing_utils\n",
    "importlib.reload(preprocessing_utils)\n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RS = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD created descriptions and the parsed Quantlets\n",
    "parsed_Qs_file = '../../data/preprocessed/Quantlet/Parsed_Qs_19032023.pkl'\n",
    "dataset = preprocessing_utils.QuantletDataset(parsed_Qs_file)\n",
    "\n",
    "with open('../../data/preprocessed/Quantlet/Descriptions_Qs_19032023.pkl', 'rb') as f:\n",
    "    descriptions = pickle.load(f)\n",
    "\n",
    "# Identify the most common language in each folder containing metainfo file\n",
    "dataset = dataset.parsed_Qs_file\n",
    "dataset.type_script = dataset.type_script.str.replace('ipynb', 'py')\n",
    "dataset['most_commen_lang'] = dataset.folder_name.map(dataset.groupby(['folder_name'])['type_script'].agg(pd.Series.mode))\n",
    "dataset['most_commen_lang'] = dataset['most_commen_lang'].astype(str)\n",
    "\n",
    "# create the additional index to merge discription back to the dataset\n",
    "dataset['desc_idx'] = dataset.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_metainfo_files = dataset.metainfo_file.astype(str).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tocheck_data = dataset.copy()\n",
    "tocheck_data.metainfo_file = tocheck_data.metainfo_file.astype(str)\n",
    "tocheck_data['empty'] = (tocheck_data.metainfo_file=='empty').astype(int)\n",
    "tocheck_data['empty_ratio'] = tocheck_data.folder_name.map(tocheck_data.groupby('folder_name').empty.sum()) / tocheck_data.folder_name.map(tocheck_data.groupby('folder_name').empty.count())\n",
    "\n",
    "# identify the repos that do not have metainfo files at all\n",
    "q_no_meta = tocheck_data[tocheck_data.empty_ratio==1].folder_name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates\n",
    "Q_lang = dataset[['folder_name', 'most_commen_lang']].drop_duplicates().reset_index(drop=True)\n",
    "Q_lang = Q_lang[~Q_lang.folder_name.isin(q_no_meta)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2376, 2) (595, 2)\n"
     ]
    }
   ],
   "source": [
    "# create the train and test set\n",
    "labelled, test = train_test_split(Q_lang, test_size=0.2, random_state=RS, stratify=Q_lang.most_commen_lang)\n",
    "print(labelled.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\'Name of Quantlet\\': \\'DataExpo2013_QuachSymanzikForsgren\\', \\'Published in\\': \\'Computational Statistics, Data Expo 2013 Special Issue, Volume 31, Issue 3, 2016\\', \\'Description\\': \\'This repository contains supplementary material for the paper \"Soul of the Community: An Attempt to Assess Attachment to a Community\" to enable the reader to reproduce the analysis.\\', \\'Keywords\\': \\'Machine Learning, Statistical Learning, Data Mining, Predictive Analytics, Random Forests, Archetypes, Knight Foundation\\', \\'See also\\': \\'Other articles in Data Expo 2013 Special Issue, accessible at https://github.com/COSTDataExpo2013\\', \\'Author\\': \\'Anna Quach, Juergen Symanzik, Nicole Forsgren\\', \\'Submitted\\': \\'August 27, 2015\\', \\'Datafile\\': \\'knightfoundation2008sotcdata.por, knightfoundation2009sotcdata.por, knightfoundation2010sotcdata.por\\', \\'Main function\\': \\'Read_Me.R\\', \\'Example\\': [{1: \\'archetype_convex_hull_2008.png -- Fig. 7: Graphical representation of the three archetype solution for the year 2008. The three points labeled 1, 2, and 3 are the archetypes. Communities are colored by the dominating group according to the three attachment status levels.\\'}, {2: \\'percentile_profile_archetypes_2008.png -- Fig. 8: Graphical representation to aid with the interpretation of the three archetype solution for the year 2008. The points represent the percentile for each archetype for each variable as compared to the average response to each community. See Table 4 in Appendix B for the meaning of each variable.\\'}]}'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.metainfo_file.iloc[8374]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metainfo_field(row, field='keywords'):\n",
    "    try :\n",
    "        row = ast.literal_eval(row)\n",
    "    except:\n",
    "        return 'empty'\n",
    "    row = {k.lower():v for (k,v) in row.items()}\n",
    "    if field in row.keys():\n",
    "        return row[field]\n",
    "    else:\n",
    "        return 'empty'\n",
    "    \n",
    "def set_keywords_to_list(row):\n",
    "    if row=='empty':\n",
    "        return []\n",
    "    elif isinstance(row, list):\n",
    "        return row\n",
    "    else:\n",
    "        return [x.strip() for x in row.split(',')]\n",
    "    \n",
    "def get_keywords_len(row):\n",
    "    if row=='empty':\n",
    "        return 0\n",
    "    else:\n",
    "        return len(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract keywords from metainfo file\n",
    "dataset['keywords'] = dataset.metainfo_file.apply(get_metainfo_field)\n",
    "\n",
    "# remove with empty keywords\n",
    "dataset = dataset[~dataset.keywords.isna()]\n",
    "\n",
    "# keywords to list\n",
    "dataset['keywords'] = dataset.keywords.apply(set_keywords_to_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess keywords\n",
    "dataset['keywords_n'] = dataset['keywords'].apply(get_keywords_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3863, 9) (968, 9)\n",
      "0.7996274063340923 0.20037259366590768\n"
     ]
    }
   ],
   "source": [
    "no_meta_ds = dataset.query('folder_name in @q_no_meta')\n",
    "labelled_ds = dataset.query('folder_name in @labelled.folder_name')\n",
    "test_ds = dataset.query('folder_name in @test.folder_name')\n",
    "\n",
    "print(labelled_ds.shape, test_ds.shape)\n",
    "full_shape = labelled_ds.shape[0] + test_ds.shape[0]\n",
    "print(labelled_ds.shape[0] / full_shape, test_ds.shape[0]/full_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract descriptions for train, test, no meta datasets\n",
    "descriptions_labelled = [descriptions[i] for i in labelled_ds.desc_idx.values]\n",
    "descriptions_test = [descriptions[i] for i in labelled_ds.desc_idx.values]\n",
    "descriptions_no_meta = [descriptions[i] for i in no_meta_ds.desc_idx.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract description lists\n",
    "descriptions_labelled = [list(descr_dict.values()) for descr_dict in descriptions_labelled]\n",
    "descriptions_test = [list(descr_dict.values()) for descr_dict in descriptions_test]\n",
    "descriptions_no_meta = [list(descr_dict.values()) for descr_dict in descriptions_no_meta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import datasets\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "\n",
    "import nltk\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"sshleifer/distilbart-xsum-12-3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization\n",
    "encoder_max_length = 256  # demo\n",
    "decoder_max_length = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(idx, description_ds, keywords_ds):\n",
    "    return {\n",
    "        \"code_description\": description_ds[idx],\n",
    "        \"keywords\": keywords_ds.iloc[idx],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list2samples(idx, description_ds, keywords_ds):\n",
    "    documents = []\n",
    "    summaries = []\n",
    "    for sample in zip(description_ds[idx], keywords_ds.iloc[idx]):\n",
    "        if len(sample[0]) > 0:\n",
    "            documents += sample[0]\n",
    "            summaries += sample[1]\n",
    "    return {\"document\": documents, \"summary\": summaries}\n",
    "\n",
    "\n",
    "dataset = data.map(flatten, remove_columns=[\"article\", \"url\"])\n",
    "dataset = dataset.map(list2samples, batched=True)\n",
    "\n",
    "train_data_txt, validation_data_txt = dataset.train_test_split(test_size=0.1).values()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "encode_code",
   "language": "python",
   "name": "encode_code"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

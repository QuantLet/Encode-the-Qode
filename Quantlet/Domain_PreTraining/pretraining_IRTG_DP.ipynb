{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"FT4ah19dzgvl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["QPATH = \"Quantlet/Domain_PreTraining\"\n","MODE = 'test'\n","\n","import sys\n","IN_COLAB = 'google.colab' in sys.modules\n","\n","import os\n","if IN_COLAB:\n","  os.chdir(f'/content/drive/MyDrive/ColabNotebooks/IRTG/Encode_the_Qode/Encode-the-Qode/{QPATH}')\n","\n","sys.path.append('../src')"],"metadata":{"id":"AqIKOMpbzugg"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZDVRAGWj_F-6"},"outputs":[],"source":["#%pip install protobuf==3.20.1\n","%pip install transformers[torch]\n","%pip install -q sentencepiece\n","%pip install datasets==2.13.1\n","%pip install evaluate\n","%pip install rouge_score\n","#%pip install wandb\n","#%pip install git+https://github.com/huggingface/nlp.git@fix-bad-type-in-overflow-check"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yfphDSXv_F-8"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import re\n","import os\n","import sys\n","import json\n","import ast\n","import pickle\n","import random\n","\n","sys.path.append('../../Quantlet/Domain_PreTraining/')\n","from abstracts import abstracts\n","\n","import importlib\n","\n","\n","#import preprocessing_utils\n","#importlib.reload(preprocessing_utils)\n","\n","import torch\n","import numpy as np\n","import datasets\n","\n","from transformers import (\n","    AutoModelForSeq2SeqLM,\n","    AutoTokenizer,\n","    Seq2SeqTrainingArguments,\n","    Seq2SeqTrainer,\n","    DataCollatorForSeq2Seq\n",")\n","\n","import nltk\n","from datetime import datetime\n","\n","import evaluate\n","nltk.download(\"punkt\", quiet=True)\n","\n","\n","from datasets import Dataset\n","from datasets import load_dataset\n","\n","\n","import evaluate\n","metric = evaluate.load(\"rouge\")\n","\n","from sklearn.model_selection import train_test_split, KFold\n","\n","os.environ[\"WANDB_DISABLED\"] = \"true\"\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","\n","\n","RS = 42"]},{"cell_type":"code","source":["torch.manual_seed(RS)\n","random.seed(RS)\n","np.random.seed(RS)\n","torch.use_deterministic_algorithms(True)\n","os.environ[\"CUBLAS_WORKSPACE_CONFIG\"]=':16:8'"],"metadata":{"id":"awsNw0KZFmyZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["abstract_pattern = re.compile(r'(?<=Abstract)(.+)(?=Keywords)', re.DOTALL)\n","keywords_pattern = re.compile(r'(?<=Keywords)(.+)', re.DOTALL)"],"metadata":{"id":"LWY6IbDK3E1X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["papers = []\n","for paper in abstracts.split('IRTG1792DP'):\n","    paper_dict = {}\n","    keywords = re.findall(keywords_pattern, paper)\n","    if len(keywords) > 0:\n","       keywords = keywords[0]\n","       keywords = keywords.split('JEL Classification')[0]\n","       paper_dict['keywords'] = keywords\n","\n","    abstract = re.findall(abstract_pattern, paper)\n","    if len(abstract) > 0:\n","       abstract = abstract[0]\n","       paper_dict['abstract'] = abstract\n","\n","    if ('abstract' in list(paper_dict.keys())) & ('keywords' in list(paper_dict.keys())):\n","      papers.append(paper_dict)"],"metadata":{"id":"rtQVp0fZ36mw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["EVAL_COLUMNS = ['eval_loss',\n","                'eval_rouge1',\n","                'eval_rouge2',\n","                'eval_rougeL',\n","                'eval_rougeLsum',\n","                'eval_bleu',\n","                'eval_gen_len']"],"metadata":{"id":"-GlL0ND06eqL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_idx  = random.sample(range(len(papers)), k=int(0.3*len(papers)))\n","train_idx = list(set(range(len(papers))).difference(test_idx))"],"metadata":{"id":"De1ZEYxk3hu2"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IFdga3AZ_F-_"},"outputs":[],"source":["train_dataset_json = {'version' : '0.1.0',\n","                     'data' : [{'description': papers[i]['abstract'],\n","                                'summary' : papers[i]['keywords']} for i in train_idx]}\n","\n","test_dataset_json = {'version' : '0.1.0',\n","                     'data' : [{'description': papers[i]['abstract'],\n","                                'summary' : papers[i]['keywords']} for i in test_idx]}\n","\n","\n","\n","with open('../../data/preprocessed/Quantlet/domain_pretrain_train.json', 'w') as f:\n","    json.dump(train_dataset_json, f)\n","\n","with open('../../data/preprocessed/Quantlet/domain_pretrain_test.json', 'w') as f:\n","    json.dump(test_dataset_json, f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"igNnZO0X_F-_"},"outputs":[],"source":["train_dataset = load_dataset(\"json\", data_files=\"../../data/preprocessed/Quantlet/domain_pretrain_train.json\", field=\"data\")['train']\n","test_dataset = load_dataset(\"json\", data_files=\"../../data/preprocessed/Quantlet/domain_pretrain_test.json\", field=\"data\")['train']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tg39PoJn_F-_"},"outputs":[],"source":["model_name = \"sshleifer/distilbart-xsum-12-3\"\n","\n","model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","# tokenization\n","encoder_max_length = 512  # demo\n","decoder_max_length = 10"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"htGlnUcJ_F_A"},"outputs":[],"source":["def batch_tokenize_preprocess(batch, tokenizer, max_source_length, max_target_length):\n","    source, target = batch[\"description\"], batch[\"summary\"]\n","    source_tokenized = tokenizer(\n","        source, padding=\"max_length\", truncation=True, max_length=max_source_length\n","    )\n","    target_tokenized = tokenizer(\n","        target, padding=\"max_length\", truncation=True, max_length=max_target_length\n","    )\n","\n","    batch = {k: v for k, v in source_tokenized.items()}\n","    # Ignore padding in the loss\n","    batch[\"labels\"] = [\n","        [-100 if token == tokenizer.pad_token_id else token for token in l]\n","        for l in target_tokenized[\"input_ids\"]\n","    ]\n","    return batch\n","\n","\n","train_data = train_dataset.map(\n","    lambda batch: batch_tokenize_preprocess(\n","        batch, tokenizer, encoder_max_length, decoder_max_length\n","    ),\n","    batched=True,\n","    remove_columns=train_dataset.column_names,\n",")\n","\n","test_data = test_dataset.map(\n","    lambda batch: batch_tokenize_preprocess(\n","        batch, tokenizer, encoder_max_length, decoder_max_length\n","    ),\n","    batched=True,\n","    remove_columns=test_dataset.column_names,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nvTcdEg6_F_A"},"outputs":[],"source":["def postprocess_text(preds, labels):\n","\n","    preds = [pred.strip() for pred in preds]\n","    labels = [label.strip() for label in labels]\n","\n","    # rougeLSum expects newline after each sentence\n","    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n","    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n","\n","    return preds, labels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mvmcxmTI_F_A"},"outputs":[],"source":["def compute_metrics(eval_preds, metrics_list=['rouge', 'bleu']):\n","\n","    preds, labels = eval_preds\n","\n","    if isinstance(preds, tuple):\n","        preds = preds[0]\n","\n","    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n","\n","    # Replace -100 in the labels as we can't decode them.\n","    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","\n","    # POST PROCESSING\n","    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n","\n","    results_dict = {}\n","    for m in metrics_list:\n","        metric = evaluate.load(m)\n","\n","        if m=='bleu':\n","            result = metric.compute(\n","              predictions=decoded_preds, references=decoded_labels\n","           )\n","        elif m=='rouge':\n","            result = metric.compute(\n","                predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n","            )\n","        result = {key: value for key, value in result.items() if key!='precisions'}\n","\n","        prediction_lens = [\n","            np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds\n","        ]\n","        result[\"gen_len\"] = np.mean(prediction_lens)\n","        result = {k: round(v, 4) for k, v in result.items()}\n","        results_dict.update(result)\n","    return results_dict"]},{"cell_type":"code","source":["if MODE=='test':\n","  eval_data = test_data"],"metadata":{"id":"lpIuZ5JMAMGB"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F1sS4AlU_F_A"},"outputs":[],"source":["training_args = Seq2SeqTrainingArguments(\n","    output_dir=\"results\",\n","    num_train_epochs=5,  # demo\n","    do_train=True,\n","    do_eval=True,\n","    per_device_train_batch_size=16,  # demo\n","    per_device_eval_batch_size=16,\n","    # learning_rate=3e-05,\n","    warmup_steps=500,\n","    weight_decay=0.1,\n","    label_smoothing_factor=0.1,\n","    predict_with_generate=True,\n","    logging_dir=\"logs\",\n","    logging_steps=100,\n","    report_to=None,\n","    seed=RS,\n","    save_total_limit = 1,\n","    load_best_model_at_end= True,\n","    evaluation_strategy='epoch',\n","    save_strategy='epoch',\n",")\n","\n","data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n","\n","trainer = Seq2SeqTrainer(\n","    model=model,\n","    args=training_args,\n","    data_collator=data_collator,\n","    train_dataset=train_data,\n","    eval_dataset=eval_data,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gUOcVWSp_F_A"},"outputs":[],"source":["results_zero_shot = trainer.evaluate()\n","\n","results_zero_shot_df = pd.DataFrame(data=results_zero_shot, index=[0])[EVAL_COLUMNS]\n","results_zero_shot_df.loc[0, :] = results_zero_shot_df.loc[0, :].apply(lambda x: round(x, 3))\n","display(results_zero_shot_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I6yJFARb_F_A"},"outputs":[],"source":["trainer.train()"]},{"cell_type":"code","source":["results_fine_tune = trainer.evaluate()\n","\n","results_fine_tune_df = pd.DataFrame(data=results_fine_tune, index=[0])[EVAL_COLUMNS]\n","\n","results_fine_tune_df.loc[0, :] = results_fine_tune_df.loc[0, :].apply(lambda x: round(x, 3))\n","\n","\n","display(results_fine_tune_df)"],"metadata":{"id":"_f3aCMn36EJ5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["best_ckpt_path = trainer.state.best_model_checkpoint"],"metadata":{"id":"wyk6Vkpd68sD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["best_ckpt_path"],"metadata":{"id":"5akstbO569eo"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LbqAx2BL_F_A"},"outputs":[],"source":["def generate_summary(test_samples, model):\n","    inputs = tokenizer(\n","        test_samples[\"description\"],\n","        padding=\"max_length\",\n","        truncation=True,\n","        max_length=encoder_max_length,\n","        return_tensors=\"pt\",\n","    )\n","    input_ids = inputs.input_ids.to(model.device)\n","    attention_mask = inputs.attention_mask.to(model.device)\n","    outputs = model.generate(input_ids, attention_mask=attention_mask)\n","    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n","    return outputs, output_str\n","\n","\n","model_before_tuning = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n","\n","test_samples = test_dataset.select(range(20))\n","\n","summaries_before_tuning = generate_summary(test_samples, model_before_tuning)[1]\n","summaries_after_tuning = generate_summary(test_samples, model)[1]"]},{"cell_type":"code","source":["for i, description in enumerate(test_samples[\"summary\"]):\n","  print('_'*10)\n","  print(f'Original: {description}')\n","  print(f'Summary before Tuning: {summaries_before_tuning[i]}')\n","  print(f'Summary after Tuning: {summaries_after_tuning[i]}')\n","  print('_'*10)\n","  print('\\n')"],"metadata":{"id":"bNYWWTgJ6IxC"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"orig_nbformat":4,"colab":{"provenance":[],"private_outputs":true,"machine_shape":"hm","gpuType":"A100"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}
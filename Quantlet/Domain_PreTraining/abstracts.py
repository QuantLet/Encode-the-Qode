
abstracts =  """
Abstracts
IRTG 1792 DP Abstracts
IRTG1792DP2018 001
IRTG 1792 Discussion Paper 2018-001

Data Driven Value-at-Risk Forecasting using a SVR-GARCH-KDE Hybrid

Marius Lux
Wolfgang Karl Härdle
Stefan Lessmann


Abstract:
Appropriate risk management is crucial to ensure the competitiveness of financial institutions
and the stability of the economy. One widely used financial risk measure is Value-at-Risk
(VaR). VaR estimates based on linear and parametric models can lead to biased results or
even underestimation of risk due to time varying volatility, skewness and leptokurtosis of
nancial return series. The paper proposes a nonlinear and nonparametric framework to
forecast VaR. Mean and volatility are modeled via support vector regression (SVR) where
the volatility model is motivated by the standard generalized autoregressive conditional
heteroscedasticity (GARCH) formulation. Based on this, VaR is derived by applying kernel
density estimation (KDE). This approach allows for exible tail shapes of the profit and loss
distribution and adapts for a wide class of tail events.
The SVR-GARCH-KDE hybrid is compared to standard, exponential and threshold
GARCH models coupled with different error distributions. To examine the performance in
different markets, one-day-ahead forecasts are produced for different financial indices. Model
evaluation using a likelihood ratio based test framework for interval forecasts indicates that
the SVR-GARCH-KDE hybrid performs competitive to benchmark models. Especially models
that are coupled with a normal distribution are systematically outperformed.


Keywords: Value-at-Risk, Support Vector Regression, Kernel Density Estimation, GARCH

IRTG1792DP2018 002
Nonparametric Variable Selection and Its Application to Additive Models


Zheng-Hui Feng
Lu Lin
Ruo-Qing Zhu
Li-Xing Zhu


Abstract:
For multivariate nonparametric regression models, existing variable selection
methods with penalization require high-dimensional nonparametric approximations
in objective functions. When the dimension is high, none of methods with penalization
in the literature are readily available. Also, ranking and screening approaches
cannot have selection consistency when iterative algorithms cannot be used due to
inefficient nonparametric approximation. In this paper, a novel and easily implemented
approach is proposed to make existing methods feasible for selection with
no need of nonparametric approximation. Selection consistency can be achieved.
As an application to additive regression models, we then suggest a two-stage procedure
that separates selection and estimation steps. An adaptive estimation to
the smoothness of underlying components can be constructed such that the consistency
can be even at parametric rate if the underlying model is really parametric.
Simulations are carried out to examine the performance of our method, and a real
data example is analyzed for illustration.


Keywords:
Adaptive estimation; non-parametric additive model; purely nonparametric
regression; variable selection

IRTG1792DP2018 003
Systemic Risk in Global Volatility Spillover Networks: Evidence from Option-implied Volatility Indices


Zihui Yang
Yinggang Zhou


Abstract
With option-implied volatility indices, we provide a new tool for event studies in a network setting and document systemic risk in the spillover networks across global financial markets. Network linkages are sufficiently asymmetric because the US stock and bond markets play as dominant volatility suppliers to other countries and markets. Shocks from the US generate systemic risk through intensifying volatility spillovers across countries and asset classes. The findings offer new evidence that asymmetric network linkages can lead to sizable aggregate fluctuations and thus potential systemic risk.


Keywords:
Network; Option-implied Volatility; Spillover; Asymmetric linkage; Systemic risk

IRTG1792DP2018 004
Pricing Cryptocurrency options: the case of CRIX and Bitcoin


Cathy YH Chen
Wolfgang Karl Härdle
Ai Jun Hou
Weining Wang


Abstract
The CRIX (CRyptocurrency IndeX) has been constructed based on a number of cryptos
and provides a high coverage of market liquidity, hu.berlin/crix. The crypto currency
market is a new asset market and attracts a lot of investors recently. Surprisingly a market
for contingent claims hat not been built up yet. A reason is certainly the lack of pricing
tools that are based on solid financial econometric tools. Here a first step towards pricing of
derivatives of this new asset class is presented. After a careful econometric pre-analysis we
motivate an affine jump diffusion model, i.e., the SVCJ (Stochastic Volatility with Correlated
Jumps) model. We calibrate SVCJ by MCMC and obtain interpretable jump processes
and then via simulation price options. The jumps present in the cryptocurrency fluctutations
are an essential component. Concrete examples are given to establish an OCRIX exchange
platform trading options on CRIX.


Keywords:
CRyptocurrency IndeX, CRIX, Bitcoin,Cryptocurrency, SVCJ, Option pricing,OCRIX

IRTG1792DP2018 005
Testing for bubbles in cryptocurrencies with time-varying volatility Christian M. Hafner Abstract The recent evolution of cryptocurrencies has been characterized by bubble-like behavior and extreme volatility. While it is difficult to assess an intrinsic value to a specific cryptocurrency, one can employ recently proposed bubble tests that rely on recursive applications of classical unit root tests. This paper extends this approach to the case where volatility is time varying, assuming a deterministic longrun component that may take into account a decrease of unconditional volatility when the cryptocurrency matures with a higher market dissemination. Volatility also includes a stochastic short-run component to capture volatility clustering. The wild bootstrap is shown to correctly adjust the size properties of the bubble test, which retains good power properties. In an empirical application using eleven of the largest cryptocurrencies and the CRIX index, the general evidence in favor of bubbles is confirmed, but much less pronounced than under constant volatility. Keywords: cryptocurrencies, speculative bubbles, wild bootstrap, volatility JEL classification: C14, C43, Z11

IRTG1792DP2018 006
A Note on Cryptocurrencies and Currency Competition

Anna Almosova


Abstract
The recent development of private cryptocurrencies has created a need to
extend existing models of private currency provision and currency competi-
tion. The outcome of cryptocurrency competition should be analyzed in a
model which incorporates important features of the modern cryptocurren-
cies. In this paper I focus on two such features. First, cryptocurrencies
operate according to a protocol - a blockchain - and are, therefore, free from
the time-inconsistency problem. Second, the operation of the blockchain
costs real resources. I use the Lagos-Wright search theoretic monetary model
augmented with privately issued currencies as in Fernandez-Villaverde and
Sanches (2016) and extend it by linear costs of private currency circulation. I
show that in contrast to Fernandez-Villaverde and Sanches (2016) cryptocur-
rency competition 1) does not deliver price stability and 2) puts downward
pressure on the in ation in the public currency only when the costs private
currency circulation (mining costs) are suciently low.


Keywords:
Currency competition, Cryptocurrency, In ation, Blockchain

JEL classication:
E40, E42, E50, E58

IRTG1792DP2018 007
Knowing me, knowing you: inventor mobility and the formation of technology-oriented alliances

Stefan Wagner
Martin C. Goossen



Abstract
We link the hiring of R&D scientists from industry competitors to the subsequent formation of collaborative agreements, namely technology-oriented alliances. By transferring technological knowledge as well as cognitive elements to the hiring firm, mobile inventors foster the alignment of decision frames applied by potential alliance partners in the process of alliance formation thereby making collaboration more likely. Using data on inventor mobility and alliance formation amongst 42 global pharmaceutical firms over 16 years, we show that inventor mobility is positively associated with the likelihood of alliance formation in periods following inventor movements. This relationship becomes more pronounced if mobile employees bring additional knowledge about their prior firms technological capabilities and for alliances aimed at technology development rather than for agreements related to technology transfer. It is weakened, however, if the focal firm is already familiar with the competitors technological capabilities. By revealing these relationships, our study contributes to research on alliance formation, employee mobility, and organizational frames.


Keywords:


JEL classication:

IRTG1792DP2018 008
A Monetary Model of Blockchain

Anna Almosova



Abstract
The recent emergence of blockchain-based cryptocurrencies has received a
considerable attention. The growing acceptance of cryptocurrencies has led
many to speculate that the blockchain technology can surpass a traditional
centralized monetary system. However, no monetary model has yet been de-
veloped to study the economics of the blockchain. This paper builds a model
of the economy with a single generally acepted blockchain-based currency. In
the spirit of the search and matching literature I use a matching function to
model the operation of the blockchain. The formulation of the money demand
is taken from a workhorse of monetary economics - Lagos and Wright (2005).
I show that in a blockchain-based monetary system money demand features
a precautionary motive which is absent in the standard Lagos-Wright model.
Due to this precautionary money demand the monetary equilibrium can be
stable for some calibrations. I also used the developed model to study how
the equilibrium return on money is

Keywords:
Blockchain, Miners, Cryptocurrency, Matching function

JEL classification:
E40, E41, E42

IRTG1792DP2018 009
Deregulated day-ahead electricity markets in Southeast Europe: Price forecasting and comparative structural analysis

Antanina Hryshchuk
Stefan Lessmann



Abstract
Many Southeast European countries are currently undergoing a process of liberalization of electric power markets. The paper analyses day-ahead price dynamics on some of these new markets and in Germany as a benchmark of a completely decentralized Western European market. To that end, several price forecasting methods including autoregressive approaches, multiple linear regression, and neural networks are considered. These methods are tested on hourly day-ahead price data during four two-week periods corresponding to different seasons and varying levels of volatility in all selected markets. The most influential fundamental factors are determined and performance of forecasting techniques is analysed with respect to the age of the market, its degree of liberalization, and the level of volatility. A comparison of Southeast European electricity markets of different age with the older German market is made and clusters of similar Southeast European markets are identified.


Keywords:
ARIMA models, energy forecasting, time series models, neural networks

JEL classification:

IRTG1792DP2018 010
How Sensitive are Tail-related Risk Measures in a Contamination Neighbourhood?

Wolfgang Karl Härdle
Chengxiu Ling



Abstract
Estimation or mis-specification errors in the portfolio loss distribution can have a considerable impact
on risk measures. This paper investigates the sensitivity of tail-related risk measures including
the Value-at-Risk, expected shortfall and the expectile-quantile transformation level in an epsiloncontamination
neighbourhood. The findings give the different approximations via the tail heaviness of
the contamination models and its contamination levels. Illustrating examples and an empirical study
on the dynamic CRIX capturing and displaying the market movements are given. The codes used to
obtain the results in this paper are available via https://github.com/QuantLet/SRMC


Keywords:
Sensitivity, expected shortfall, expectile, Value-at-Risk, risk management, influence function, CRIX

JEL classification:
C13, G10, G31

IRTG1792DP2018 011
How to Measure a Performance of a Collaborative Research Centre

Alona Zharova
Janine Tellinger-Rice
Wolfgang Karl Härdle



Abstract
New Public Management helps universities and research institutions to perform in a highly competitive
research environment. Evaluating publicly financed research results improves transparency, helps in reflection
and self-assessment, and provides information for strategic decision making. In this paper we provide
empirical evidence using data from a Collaborative Research Centre (CRC) on financial inputs and research
output from 2005 to 2016. After selecting performance indicators suitable for a CRC, we describe main
properties of the data using visualization techniques. To study the relationship between the dimensions of
research performance, we use a time fixed effects panel data model and fixed effects Poisson model. With
the help of year dummy variables, we show how the pattern of research productivity changed over time after
controlling for staff and travel costs. The joint depiction of the time fixed effects and the research projects
life cycle allows a better understanding of the development of the number of discussion papers over time.


Keywords:
Research Performance, Time Fixed Effects Panel Data Model, Fixed Effects Poisson Model, Network, Collaborative Research Centre

JEL classification:
C00

IRTG1792DP2018 012
Targeting customers for profit: An ensemble learning framework to support marketing decision making

Stefan Lessmann
Kristof Coussement
Koen W. De Bock
Johannes Haupt



Abstract
Marketing messages are most effective if they reach the right customers. Deciding which customers
to contact is thus an important task in campaign planning. The paper focuses on empirical targeting
models. We argue that common practices to develop such models do not account sufficiently for
business goals. To remedy this, we propose profit-conscious ensemble selection, a modeling framework
that integrates statistical learning principles and business objectives in the form of campaign profit
maximization. The results of a comprehensive empirical study confirm the business value of the
proposed approach in that it recommends substantially more profitable target groups than several
benchmarks.


Keywords:
Marketing Decision Support, Business Value, Profit-Analytics, Machine Learning

JEL classification:
C00

IRTG1792DP2018 013
Improving Crime Count Forecasts Using Twitter and Taxi Data

Lara Vomfell
Wolfgang Karl Härdle
Stefan Lessmann



Abstract
Data from social media has created opportunities to understand how and why
people move through their urban environment and how this relates to criminal
activity. To aid resource allocation decisions in the scope of predictive
policing, the paper proposes an approach to predict weekly crime counts. The
novel approach captures spatial dependency of criminal activity through approximating
human dynamics. It integrates point of interest data in the form
of Foursquare venues with Twitter activity and taxi trip data, and introduces a
set of approaches to create features from these data sources. Empirical results
demonstrate the explanatory and predictive power of the novel features. Analysis
of a six-month period of real-world crime data for the city of New York
evidences that both temporal and static features are necessary to eectively account
for human dynamics and predict crime counts accurately. Furthermore,
results provide new evidence into the underlying mechanisms of crime and give
implications for crime analysis and intervention.


Keywords:
Predictive Policing, Crime Forecasting, Social Media Data, Spatial Econometrics

JEL classification:
C00

IRTG1792DP2018 014
Price Discovery on Bitcoin Markets

Paolo Pagnottoni
Dirk G. Baur
Thomas Dimpfl



Abstract
Trading of Bitcoin is spread about multiple venues where buying and selling is offered
in various currencies. However, all markets trade one common good and by the law of
one price, the different prices should not deviate in the long run. In this context we are
interested in which platform is the most important one in terms of price discovery. To this
end, we use a pairwise approach accounting for a potential impact of exchange rates. The
contribution to price discovery is measured by Hasbrouck's and Gonzalo and Granger's
information share. We then derive an ordering with respect to the importance of each
market which reveals that the Chinese OKCoin platform is the leader in price discovery
of Bitcoin, followed by BTC China.


Keywords:
price discovery; Bitcoin; Hasbrouck information shares;

JEL classification:
C58, C32, G23

IRTG1792DP2018 015
Bitcoin is not the New Gold - A Comparison of Volatility, Correlation, and Portfolio Performance

Tony Klein
Hien Pham Thu
Thomas Walther


Abstract
Cryptocurrencies such as Bitcoin are establishing themselves as an investment asset and
are often named the New Gold. This study, however, shows that the two assets could
barely be more dierent. Firstly, we analyze and compare conditional variance properties
of Bitcoin and Gold as well as other assets and nd dierences in their structure.
Secondly, we implement a BEKK-GARCH model to estimate time-varying conditional
correlations. Gold plays an important role in nancial markets with ight-to-quality in
times of market distress. Our results show that Bitcoin behaves as the exact opposite
and it positively correlates with downward markets. Lastly, we analyze the properties of
Bitcoin as portfolio component and nd no evidence for hedging capabilities. We conclude
that Bitcoin and Gold feature fundamentally dierent properties as assets and linkages to
equity markets. Our results hold for the broad cryptocurrency index CRIX. As of now,
Bitcoin does not re ect any distinctive properties of Gold other than asymmetric response
in variance.


Keywords:
BEKK, Bitcoin, CRIX, Cryptocurrency, Gold, GARCH, Conditional Correlation, Asymmetry, Long memory

JEL classification:
C10; C58; G11

IRTG1792DP2018 016
Time-varying Limit Order Book Networks

Wolfgang Karl Härdle
Shi Chen
Chong Liang
Melanie Schienle


Abstract
This paper analyzes the market impact of limit order books (LOB) taking crossstock
effects into account. Based on penalized vector autoregressive approach, we
aim to identify significance and magnitude of the directed network channels within
and between LOBs by bootstrapped impulse response functions. Moreover, information
on asymmetries and imbalances within the LOB over time would be derived. For
the sample of a NASDAQ blue-chip portfolio during 06-07/2016 we find that LOB
network effects crucially determine prices and bid-ask asymmetries are prevalent.


Keywords:
limit order book, high dimension, generalized impulse response, high frequency, market risk, market impact, network, bootstrap

JEL classification:
C02, C13, C22, C45, G12

IRTG1792DP2018 017
Regularization Approach for Network Modeling of German Energy Market

Shi Chen
Wolfgang Karl Härdle
Brenda López Cabrera


Abstract
We investigate the concept of connectedness, which is important for risk
measurement and management inGerman energy market. Understanding and
learning from these mechanisms are essential to avoid future systemic disasters.
To deal with large portfolio selection, we propose regularization approach
to capture the spillover and contagion effects acrossGerman power derivatives.
This paper shows how network analysis can facilitate the monitoring of futures
price movements. Our methodology combines high-dimensional variable selection
techniques with network analysis, the results show that contracts like
Phelix Base Year Options and Phelix Peak Year Futures are in the core of the
Energy futures market.


Keywords:
regularization, energy risk transmission, network, German energy market

JEL classification:
C1, Q41, Q47

IRTG1792DP2018 018
Adaptive Nonparametric Clustering

Kirill Efimov
Larisa Adamyan
Vladimir Spokoiny


Abstract
This paper presents a new approach to non-parametric cluster analysis
called Adaptive Weights Clustering (AWC). The idea is to identify the
clustering structure by checking at different points and for dierent scales
on departure from local homogeneity. The proposed procedure describes
the clustering structure in terms of weights wij each of them measures
the degree of local inhomogeneity for two neighbor local clusters using
statistical tests of \no gap" between them. The procedure starts from
very local scale, then the parameter of locality grows by some factor
at each step. The method is fully adaptive and does not require to
specify the number of clusters or their structure. The clustering results
are not sensitive to noise and outliers, the procedure is able to recover
dierent clusters with sharp edges or manifold structure. The method
is scalable and computationally feasible. An intensive numerical study
shows a state-of-the-art performance of the method in various articial
examples and applications to text data. Our theoretical study states
optimal sensitivity of AWC to local inhomogeneity.


Keywords:
adaptive weights, clustering, gap coecient, manifold clustering

JEL classification:

AMS 2000 Subject Classication:
Primary 62H30. Secondary 62G10

IRTG1792DP2018 019
Lasso, knockoff and Gaussian covariates: a comparison


Laurie Davies


Abstract
Given data y and k covariates xj one problem in linear regression
is to decide which if any of the covariates to include when regressing
the dependent variable y on the covariates xj . In this paper three
such methods, lasso, knockoff and Gaussian covariates are compared
using simulations and real data. The Gaussian covariate method is
based on exact probabilities which are valid for all y and xj making
it model free. Moreover the probabilities agree with those based on
the F-distribution for the standard linear model with i.i.d. Gaussian
errors. It is conceptually, mathematically and algorithmically very
simple, it is very fast and makes no use of simulations. It outperforms
lasso and knockoff in all respects by a considerable margin.


Keywords:


JEL classification:

IRTG1792DP2018 020
A Regime Shift Model with Nonparametric Switching Mechanism


Haiqiang Chen
Yingxing Li
Ming Lin
Yanli Zhu


Abstract
In this paper, we propose a new class of regime shift models with exible switching
mechanism that relies on a nonparametric probability function of the observed thresh-
old variables. The proposed models generally embrace traditional threshold models
with contaminated threshold variables or heterogeneous threshold values, thus gaining
more power in handling complicated data structure. We solve the identification issue by
imposing either global shape restriction or boundary condition on the nonparametric
probability function. We utilize the natural connection between penalized splines and
hierarchical Bayes to conduct smoothing. By adopting dierent priors, our procedure
could work well for estimations of smooth curve as well as discontinuous curves with
occasionally structural breaks. Bayesian tests for the existence of threshold eects are
also conducted based on the posterior samples from Markov chain Monte Carlo (M-
CMC) methods. Both simulation studies and an empirical application in predicting
the U.S. stock market returns demonstrate the validity of our methods.


Keywords:
Threshold Model, Nonparametric, Markov Chain Monte Carlo, Bayesian Inference, Spline.

JEL classification:


IRTG1792DP2018 021
LASSO-Driven Inference in Time and Space


Victor Chernozhukov
Wolfgang K. Härdle
Chen Huang
Weining Wang


Abstract
We consider the estimation and inference in a system of high-dimensional regression equations
allowing for temporal and cross-sectional dependency in covariates and error processes, covering
rather general forms of weak dependence. A sequence of large-scale regressions with LASSO is
applied to reduce the dimensionality, and an overall penalty level is carefully chosen by a block
multiplier bootstrap procedure to account for multiplicity of the equations and dependencies in the
data. Correspondingly, oracle properties with a jointly selected tuning parameter are derived. We
further provide high-quality de-biased simultaneous inference on the many target parameters of
the system. We provide bootstrap consistency results of the test procedure, which are based on a
general Bahadur representation for the Z-estimators with dependent data. Simulations demonstrate
good performance of the proposed inference procedure. Finally, we apply the method to quantify
spillover effects of textual sentiment indices in a financial market and to test the connectedness
among sectors.


Keywords:
LASSO, time series, simultaneous inference, system of equations, Z-estimation, Bahadur representation, martingale decomposition

JEL classification:
C12, C22, C51, C53

IRTG1792DP2018 022
Learning from Errors: The case of monetary and fiscal policy regimes

Andreas Tryphonides


Abstract
The New Keynesian theory of inflation determination has been under scrutiny
due to identification issues, which rather have to do with the mechanism of inflation determination
at its core (i.e. Cochrane (2011)). Moreover, similar identification problems
arise in the case of fiscal inflation (see for example Leeper and Leith (2016), Leeper and
Li (2017) and Leeper and Walker (2012)). This paper makes a positive contribution.
We argue that statements about observational equivalence stem from referring to the
equilibrium path, while this should not be our primary source of identifying restrictions.
Moreover, policy identification (or lack thereof) relies on assumptions on the underlying
shock structure, which is unobservable. We instead extract shocks using heterogeneous
uncertain restrictions and external datasets, that is, we learn from errors. We are then
able to recover deep and policy parameters irrespective of the prevailing equilibrium. We
provide time varying evidence on the efficacy of policy in stabilizing the US economy and
on the time varying plausibility of Ricardian versus non-Ricardian price determination.
Results are work in progress.


Keywords:
Monetary and fiscal policy, Price Determination, Identification, Learning from errors

JEL classification:
C11, C13, E62, E63

IRTG1792DP2018 023
Textual Sentiment, Option Characteristics, and Stock Return Predictability

Cathy Yi-Hsuan Chen
Matthias R. Fengler
Wolfgang Karl Härdle
Yanchu Liu


Abstract
We distill sentiment from a huge assortment of NASDAQ news articles by means of machine
learning methods and examine its predictive power in single-stock option markets and equity
markets. We provide evidence that single-stock options react to contemporaneous sentiment.
Next, examining return predictability, we discover that while option variables indeed predict
stock returns, sentiment variables add further informational content. In fact, both in a
regression and a trading context, option variables orthogonalized to public and sentimental
news are even more informative predictors of stock returns. Distinguishing further between
overnight and trading-time news, we find the first to be more informative. From a statistical
topic model, we uncover that this is attributable to the differing thematic coverage of the
alternate archives. Finally, we show that sentiment disagreement commands a strong positive
risk premium above and beyond market volatility and that lagged returns predict future
returns in concentrated sentiment environments.


Keywords:
investor disagreement; option markets; overnight information; stock return
predictability; textual sentiment; topic model; trading-time information;

JEL classification:
C58, G12, G14, G41

IRTG1792DP2018 024
Bootstrap Confidence Sets for Spectral Projectors of Sample Covariance

A. Naumov
V. Spokoiny
V. Ulyanovk


Abstract
Let X1, . . . ,Xn be i.i.d. sample in Rp with zero mean and the
covariance matrix . The problem of recovering the projector onto
an eigenspace of from these observations naturally arises in many
applications. Recent technique from [9] helps to study the asymp-
totic distribution of the distance in the Frobenius norm kPr - bP
rk2
between the true projector Pr on the subspace of the rth eigenvalue
and its empirical counterpart bP
r in terms of the effective rank of .
This paper offers a bootstrap procedure for building sharp confidence
sets for the true projector Pr from the given data. This procedure
does not rely on the asymptotic distribution of kPr - bP
rk2 and its
moments. It could be applied for small or moderate sample size n and
large dimension p. The main result states the validity of the proposed
procedure for finite samples with an explicit error bound for the er-
ror of bootstrap approximation. This bound involves some new sharp
results on Gaussian comparison and Gaussian anti-concentration in
high-dimensional spaces. Numeric results confirm a good performance
of the method in realistic examples.


Keywords:


JEL classification:

IRTG1792DP2018 025
Construction of Non-asymptotic Confidence Sets in 2 -Wasserstein Space

Johannes Ebert
Vladimir Spokoiny
Alexandra Suvorikova


Abstract
In this paper, we consider a probabilistic setting where the probability measures
are considered to be random objects. We propose a procedure of construction
non-asymptotic confidence sets for empirical barycenters in 2 -Wasserstein space and
develop the idea further to construction of a non-parametric two-sample test that is
then applied to the detection of structural breaks in data with complex geometry. Both
procedures mainly rely on the idea of multiplier bootstrap (Spokoiny and Zhilova [29],
Chernozhukov, Chetverikov and Kato [13]). The main focus lies on probability measures
that have commuting covariance matrices and belong to the same scatter-location
family: we proof the validity of a bootstrap procedure that allows to compute confidence
sets and critical values for a Wasserstein-based two-sample test.


Keywords:
Wasserstein barycenters, hypothesis testing, multiplier bootstrap,
change point detection, confidence sets.

JEL classification:

IRTG1792DP2018 027
Bayesian inference for spectral projectors of covariance matrix

Igor Silin
Vladimir Spokoiny


Abstract
Let X1; : : : ;Xn be i.i.d. sample in Rp with zero mean and
the covariance matrix . The classic principal component analysis esti-
mates the projector P
J onto the direct sum of some eigenspaces of
by its empirical counterpart bPJ . Recent papers [20, 23] investigate the
asymptotic distribution of the Frobenius distance between the projectors
k bPJ ??P
J k2 . The problem arises when one tries to build a condence set
for the true projector eectively. We consider the problem from Bayesian
perspective and derive an approximation for the posterior distribution of
the Frobenius distance between projectors. The derived theorems hold true
for non-Gaussian data: the only assumption that we impose is the con-
centration of the sample covariance b
in a vicinity of . The obtained
results are applied to construction of sharp condence sets for the true pro-
jector. Numerical simulations illustrate good performance of the proposed
procedure even on non-Gaussian data in quite challenging regime.


Keywords:
covariance matrix, spectral projector, principal
component analysis, Bernstein { von Mises theorem.

JEL classification:

IRTG1792DP2018 028
Toolbox: Gaussian comparison on Eucledian balls

Andzhey Koziuk
Vladimir Spokoiny


Abstract
In the work a characterization of difference of multivariate Gaussian measures is found on the
family of centered Eucledian balls. In particular, it helps to derive (xx see paper).


Keywords:
multivariate Gaussian measure, Kolmogorov distance, Gaussian comparison

JEL classification:

IRTG1792DP2018 029
Pointwise adaptation via stagewise aggregation of local estimates for multiclass classification

Nikita Puchkin
Vladimir Spokoiny


Abstract
We consider a problem of multiclass classification, where the
training sample Sn = {(Xi, Yi)}n
i=1 is generated from the model P(Y =
m|X = x) = m(x), 1 6 m 6 M, and 1(x), . . . , M(x) are unknown Lip-
schitz functions. Given a test point X, our goal is to estimate 1(X), . . . ,
M(X). An approach based on nonparametric smoothing uses a localization
technique, i.e. the weight of observation (Xi, Yi) depends on the distance
between Xi and X. However, local estimates strongly depend on localiz-
ing scheme. In our solution we fix several schemes W1, . . . ,WK, compute
corresponding local estimates e(1), . . . , e(K) for each of them and apply an
aggregation procedure. We propose an algorithm, which constructs a con-
vex combination of the estimates e(1), . . . , e(K) such that the aggregated
estimate behaves approximately as well as the best one from the collection
e(1), . . . , e(K). We also study theoretical properties of the procedure, prove
oracle results and establish rates of convergence under mild assumptions.


Keywords:


JEL classification:

IRTG1792DP2018 030
Gaussian Process Forecast with multidimensional distributional entries

Francois Bachoc
Alexandra Suvorikova
Jean-Michel Loubes
Vladimir Spokoiny



Abstract
In this work, we propose to define Gaussian Processes indexed by multidimensional distributions.
In the framework where the distributions can be modeled as i.i.d realizations of a measure on
the set of distributions, we prove that the kernel defined as the quadratic distance between the
transportation maps, that transport each distribution to the barycenter of the distributions, provides
a valid covariance function. In this framework, we study the asymptotic properties of this process,
proving micro ergodicity of the parameters.


Keywords:
Gaussian Process, Kernel methods, Wasserstein Distance

JEL classification:

IRTG1792DP2018 031
Gaussian Process Forecast with multidimensional distributional entries

Andzhey Koziuk
Vladimir Spokoiny



Abstract
IV regression in the context of a re-sampling is considered in the work. Comparatively, the contribution
in the development is a structural identication in the IV model. The work also contains a
multiplier-bootstrap justication.


Keywords:
Gaussian Process, Kernel methods, Wasserstein Distance
JEL classification:

IRTG1792DP2018 032
Understanding Latent Group Structure of Cryptocurrencies Market: A Dynamic Network Perspective

Li Guo
Yubo Tao
Wolfgang Karl Hardle



Abstract
In this paper, we study the latent group structure in cryptocurrencies market
by forming a dynamic return inferred network with coin attributions. We develop
a dynamic covariate-assisted spectral clustering method to detect the communities
in dynamic network framework and prove its uniform consistency along the horizons.
Applying our new method, we show the return inferred network structure and
coin attributions, including algorithm and proof types, jointly determine the market
segmentation. Based on the network model, we propose a novel \hard-to-value"
measure using the centrality scores. Further analysis reveals that the group with a
lower centrality score exhibits stronger short-term return reversals. Cross-sectional
return predictability further conrms the economic meanings of our grouping results
and reveal important portfolio management implications.


Keywords:
Community Detection, Dynamic Network, Return Predictability, Behavioural
Bias, Market Segmentation, Bitcoin

IRTG1792DP2018 033
Optimal contracts under competition when uncertainty from adverse selection and moral hazard are present

Natalie Packham



Abstract
In a continuous-time setting where a risk-averse agent controls the drift of an output
process driven by a Brownian motion, optimal contracts are linear in the terminal output;
this result is well-known in a setting with moral hazard and  under stronger assumptions
 adverse selection. We show that this result continues to hold when in addition reser-
vation utilities are type-dependent. This type of problem occurs in the study of optimal
compensation problems involving competing principals.


Keywords:
Principal-agent modelling; contract design; stochastic process; stochastic control

IRTG1792DP2018 034
A factor-model approach for correlation scenarios and correlation stress-testing

Natalie Packham
Fabian Woebbeking



Abstract
In 2012, JPMorgan accumulated a USD 6.2 billion loss on a credit derivatives portfolio,
the so-called \London Whale", partly as a consequence of de-correlations of non-perfectly
correlated positions that were supposed to hedge each other. Motivated by this case, we
devise a factor model for correlations that allows for scenario-based stress-testing of correlations.
We derive a number of analytical results related to a portfolio of homogeneous
assets. Using the concept of Mahalanobis distance, we show how to identify adverse scenarios
of correlation risk. As an example, we apply the factor-model approach to the \London
Whale" portfolio and determine the value-at-risk impact from correlation changes. Since our
ndings are particularly relevant for large portfolios, where even small correlation changes
can have a large impact, a further application would be to stress-test portfolios of central
counterparties, which are of systemically relevant size.


Keywords:
Correlation stress testing, scenario selection, market risk, "London Whale"

JEL Classication:
C58, G15, G17, G18

IRTG1792DP2018 035
Correlation Under Stress In Normal Variance Mixture Models

Michael Kalkbrener
Natalie Packham


Abstract
We investigate correlations of asset returns in stress scenarios where a common risk
factor is truncated. Our analysis is performed in the class of normal variance mixture
(NVM) models, which encompasses many distributions commonly used in nancial
modelling. For the special cases of jointly normally and t-distributed asset returns
we derive closed formulas for the correlation under stress. For the NVM distribution,
we calculate the asymptotic limit of the correlation under stress, which depends on
whether the variables are in the maximum domain of attraction of the Frechet or
Gumbel distribution. It turns out that correlations in heavy-tailed NVM models are
less sensitive to stress than in medium- or light-tailed models. Our analysis sheds light
on the suitability of this model class to serve as a quantitative framework for stress
testing, and as such provides valuable information for risk and capital management
in nancial institutions, where NVM models are frequently used for assessing capital
adequacy. We also demonstrate how our results can be applied for more prudent stress
testing.


Keywords:
Stress testing, risk management, correlation, normal variance mixture distribution, multivariate normal distribution, multivariate t-distribution.

IRTG1792DP2018 036
Model risk of contingent claims

Nils Detering
Natalie Packham


Abstract
Paralleling regulatory developments, we devise value-at-risk and expected shortfall type
risk measures for the potential losses arising from using misspecied models when pricing
and hedging contingent claims. Essentially, losses from model risk correspond to losses realized
on a perfectly hedged position. Model uncertainty is expressed by a set of pricing
models, relative to which potential losses are determined. Using market data, a unied
loss distribution is attained by weighing models according to a relative likelihood criterion.
Examples demonstrate the magnitude of model risk and corresponding capital buers necessary
to suciently protect trading book positions against unexpected losses from model
risk.


Keywords:
Model risk, parameter uncertainty, hedge error, value-at-risk, expected shortfall

JEL Clasification:
G32, G13

IRTG1792DP2018 037
Default probabilities and default correlations under stress

Natalie Packham
Michael Kalkbrener
Ludger Overbeck


Abstract
We investigate default probabilities and default correlations of Merton-type credit portfolio
models in stress scenarios where a common risk factor is truncated. The analysis is
performed in the class of elliptical distributions, a family of light-tailed to heavy-tailed distributions
encompassing many distributions commonly found in nancial modelling. It turns
out that the asymptotic limit of default probabilities and default correlations depend on the
max-domain of the elliptical distribution's mixing variable. In case the mixing variable is
regularly varying, default probabilities are strictly smaller than 1 and default correlations
are in (0; 1). Both can be expressed in terms of the Student t-distribution function. In the
rapidly varying case, default probabilities are 1 and default correlations are 0. We compare
our results to the tail dependence function and discuss implications for credit portfolio
modelling.


Keywords:
financial risk management, credit portfolio modelling, stress testing, elliptic distribution, max-domain

MSC classification:
60G70, 91G40

IRTG1792DP2018 038
Tail-Risk Protection Trading Strategies

Natalie Packham
Jochen Papenbrock
Peter Schwendner
Fabian Woebbeking


Abstract
Starting from well-known empirical stylised facts of nancial time series, we develop
dynamic portfolio protection trading strategies based on econometric methods. As a criterion
for riskiness we consider the evolution of the value-at-risk spread from a GARCH
model with normal innovations relative to a GARCH model with generalised innovations.
These generalised innovations may for example follow a Student t, a generalised
hyperbolic (GH), an alpha-stable or a Generalised Pareto (GPD) distribution. Our
results indicate that the GPD distribution provides the strongest signals for avoiding
tail risks. This is not surprising as the GPD distribution arises as a limit of tail behaviour
in extreme value theory and therefore is especially suited to deal with tail risks.
Out-of-sample backtests on 11 years of DAX futures data, indicate that the dynamic
tail-risk protection strategy eectively reduces the tail risk while outperforming traditional
portfolio protection strategies. The results are further validated by calculating
the statistical signicance of the results obtained using bootstrap methods. A number of
robustness tests including application to other assets further underline the eectiveness
of the strategy. Finally, by empirically testing for second order stochastic dominance,
we nd that risk averse investors would be willing to pay a positive premium to move
from a static buy-and-hold investment in the DAX future to the tail-risk protection
strategy.

Keywords:
tail-risk protection, portfolio protection, extreme events, tail distributions

JEL Classification:
C15, G11, G17.

IRTG1792DP2018 039
Penalized Adaptive Forecasting with Large Information Sets and Structural Changes

Lenka Zbonakova
Xinjue Li
Wolfgang Karl Härdle


Abstract
In the present paper we propose a new method, the Penalized Adaptive
Method (PAM), for a data driven detection of structural changes in sparse linear
models. The method is able to allocate the longest homogeneous intervals over
the data sample and simultaneously choose the most proper variables with the
help of penalized regression models. The method is simple yet exible and can
be safely applied in high-dimensional cases with dierent sources of parameter
changes. Comparing with the adaptive method in linear models, its combination
with dimension reduction yields a method which properly selects signicant
variables and detects structural breaks while steadily reduces the forecast error
in high-dimensional data.

Keywords:
SCAD penalty, propagation-separation, adaptive window choice, multiplier bootstrap

JEL Classification:
C12, C13, C50, E47, G12

IRTG1792DP2018 041
On complete convergence in Marcinkiewicz-Zygmund type SLLN for random variables

Anna Kuczmaszewska
Ji Gao YAN


Abstract
We consider a generalization of Baum-Katz theorem for random vari-
ables satisfying some cover conditions. Consequently, we get the result for many
dependent structure, such as END, -mixing, -mixing and -mixing, etc.

Keywords:
Complete convergence; Marcinkiewicz-Zygmund type SLLN; Extended negatively dependent; Mixing dependency; Weakly mean bounded.

JEL Classification:
C00

MSC(2010) Subject Classification:
60F15

IRTG1792DP2018 040
Complete Convergence and Complete Moment Convergence for Maximal Weighted Sums of Extended Negatively Dependent Random Variables

Ji Gao YAN


Abstract
In this paper, the complete convergence and complete moment convergence for maximal
weighted sums of extended negatively dependent random variables are investigated. Some su±cient
conditions for the convergence are provided. In addition, the Marcinkiewicz{Zygmund type strong law
of large numbers for weighted sums of extended negatively dependent random variables is obtained.
The results obtained in the article extend the corresponding ones for independent random variables
and some dependent random variables.

Keywords:
Extended negatively dependent, complete convergence, complete moment convergence, maximal weighted sums, strong law of large numbers

JEL Classification:
C00

IRTG1792DP2018 042
On Complete Convergence in Marcinkiewicz-Zygmund Type SLLN for END Random Variables and its Applications

Ji Gao YAN


Abstract
In this paper, the complete convergence for maximal weighted sums of extended negatively
dependent (END, for short) random variables is investigated. Some sucient conditions
for the complete convergence and some applications to a nonparametric model are provided. The
results obtained in the paper generalise and improve the corresponding ones of Wang el al. (2014b)
and Shen, Xue, and Wang (2017).

Keywords:
Complete convergence; Maximal weighted sums; Extended negatively dependent.

JEL Classification:
C00

MSC(2010) Subject Classification:
60F15

IRTG1792DP2018 043
Textual Sentiment and Sector specific reaction

Elisabeth Bommes
Cathy Yi-Hsuan Chen
Wolfgang Karl Härdle


Abstract
News move markets and contains incremental information about stock
reactions. Future trading volumes, volatility and returns are a
ected by sentiments of texts and opinions expressed in articles. Earlier
work of sentiment distillation of stock news suggests that risk prole reactions
might differ across sectors.
Conventional asset pricing theory recognizes the role of a sector and its
risk uniqueness that differs from market or rm specic risk.
Our research assesses whether incorporating the sentiment distilled from
sector specic news carries information about risk proles. Textual analytics applied to about 600K
articles leads us with lexical projection and machine learning to classication of sentiment polarities. The
texts are scraped from offcial NASDAQ web pages and with Natural Language Processing (NLP)
techniques, such as tokenization, lemmatization, a sector specic sentiment is extracted using a lexical
approach and a nancial phrase bank. Predicted sentence-level polarities are aggregated into a bullishness
measure on a daily basis and fed into a panel regression analysis with sector indicators. Supervised
learning with hinge or logistic loss and regularization yields good prediction results of polarity. Compared with
standard lexical projections, the supervised learning approach yields superior predictions of sentiment,
leading to highly sector specic sentiment reactions. The Consumer Staples, Health Care and Materials
sectors show strong risk prole reactions to negative polarity.

Keywords:
Investor Sentiment, Attention Analysis, Sector-specic Reactions, Volatility, Text Mining, Polarity

JEL Classification:
C81, G14, G17

IRTG1792DP2018 044
Understanding Cryptocurrencies

Wolfgang Karl Härdle
Campbell R. Harvey
Raphael C. G. Reule


Abstract
Cryptocurrencies refer to a type of digital cash that use distributed ledger - or
blockchain technology - to provide secure transactions. These currencies are generally
misunderstood. While initially dismissed as fads or bubbles, many large central
banks are considering launching their own version of national cryptocurrencies. In
contrast to most data in nancial economics, there is a plethora of detailed (free)
data on the history of every transaction for the cryptocurrency complex. Further,
there is little empirically-oriented research on this new asset class. This is an extraordinary
research opportunity for academia. We provide a starting point by
giving an insight into cryptocurrency mechanisms and detailing summary statistics
and focusing on potential future research avenues in nancial economics.

Keywords:
Cryptocurrency, Blockchain, Bitcoin, Economic bubbles, Peer-to-Peer, Cryptographic
hashing, Consensus, Proof-of-Work, Proof-of-stake, Volatility

JEL Classification:
C01, C58, E42, E51, G10, K24, K42, L86, O31

IRTG1792DP2018 045
Predicative Ability of Similarity-based Futures Trading Strategies

Hsin-Yu Chiu
Mi-Hsiu Chiang
Wei-Yu Kuo


Abstract
A trading rule that draws on the empirical similarity concept is proposed to simulate the
technical trading mentality|one that selectively perceives structural resemblances between
market scenarios of the present and the past. In more than half of the nineteen futures
markets that we test against for protability of this similarity-based trading rule, we nd
evidence of predictive ability that is robust to data-snooping and transaction-cost adjust-
ments. When aided by an exit strategy that liquidates the trader's positions across some
evenly-spaced time points, this rule generates the most robust returns.

Keywords:
empirical similarity; technical trading; futures markets; analogical reasoning

JEL Classification:
G11, G12

IRTG1792DP2018 046
Forecasting the Term Structure of Option Implied Volatility: The Power of an Adaptive Method

Ying Chen
Qian Han
Linlin Niu

Abstract
We model the term structure of implied volatility (TSIV) with an adaptive approach
to improve predictability, which treats dynamic time series models of globally time-
varying but locally constant parameters and uses a data-driven procedure to ?nd the
local optimal interval. We choose two speci?cations of the adaptive models: a simple
local AR (LAR) model for a univariate implied volatility series and an adaptive dynamic
Nelson-Siegel (ADNS) model of three factors, each based on an LAR, to model the cross-
section of the TSIV simultaneously with parsimony. Both LAR and ADNS models
uniformly outperform more than a dozen alternative models with signi?cance across
maturities for 1-20 day forecast horizons. Measured by RMSE and MAE, the forecast
errors of the random walk model can be reduced by between 20% and 60% for the 5 to
20 days ahead forecast. In terms of prediction accuracy of future directional changes,
the adaptive models achieve an accuracy range of 60%-90%, which strictly dominates
the range of 30%-59% of the alternative models.

Keywords:
Term structure of implied volatility, local parametric models, forecasting

JEL Classification:
C32, C53

IRTG1792DP2018-053.txt
The impact of temperature on gaming productivity: evidence from online games

Xiaojia Bao
Qingliang Fan

Abstract
This paper studies the short-run impacts of temperature on human performance in the
computer-mediated environment using server logs of a popular online game in China.
Taking advantage of the quasi-experiment of winter central heating policy inChina, we
distinguish the impacts of outdoor and indoor temperature and find that low temperatures
below 5 ?C decrease game performance significantly. Non-experienced players
suffered larger performance drop than experienced ones. Access to central heating
attenuates negative impacts of low outdoor temperatures on gamers performance.
High temperatures above 21 ?C also lead to drops in game performance.We conclude
that expanding the current central heating zone will bring an increase in human performance
by approximately 4% in Shanghai and surrounding provinces in the winter.
While often perceived as a leisure activity, online gaming requires intense engagement
and the deployment of cognitive, social, and motor skills, which are also key skills
for productive activities. Our results draw attention to potential damages of extreme
temperature on human performance in the modern computer-mediated environment.

Keywords:
Temperature, Human performance, Online game, Heating

JEL Classification:
Q54, J22, J24, D03

IRTG1792DP2018-047.txt
Inferences for a Partially Varying Coefficient Model With Endogenous Regressors

Zongwu Cai
Ying Fang
Ming Lin
Jia Su

Abstract
In this article, we propose a new class of semiparametric instrumental variable models with partially varying
coefficients, in which the structural function has a partially linear form and the impact of endogenous
structural variables can vary over different levels of some exogenous variables. We propose a three-step
estimation procedure to estimate both functional and constant coefficients. The consistency and asymptotic
normality of these proposed estimators are established. Moreover, a generalized F-test is developed to
test whether the functional coefficients are of particular parametric forms with some underlying economic
intuitions, and furthermore, the limiting distribution of the proposed generalized F-test statistic under the
null hypothesis is established. Finally, we illustrate the finite sample performance of our approach with
simulations and two real data examples in economics.

Keywords:
Endogeneity; Functional coefficients; Generalized F-test; Instrumental variables models;
Nonparametric test; Profile least squares

JEL Classification:
C00

IRTG1792DP2018-048.txt
A Regime Shift Model with Nonparametric Switching Mechanism

Haiqiang Chen
Yingxing Li
Ming Lin
Yanli Zhu

Abstract
In this paper, we propose a new class of regime shift models with flexible switching
mechanism that relies on a nonparametric probability function of the observed threshold
variables. The proposed models generally embrace traditional threshold models
with contaminated threshold variables or heterogeneous threshold values, thus gaining
more power in handling complicated data structure. We solve the identification issue by
imposing either global shape restriction or boundary condition on the nonparametric
probability function. We utilize the natural connection between penalized splines and
hierarchical Bayes to conduct smoothing. By adopting different priors, our procedure
could work well for estimations of smooth curve as well as discontinuous curves with
occasionally structural breaks. Bayesian tests for the existence of threshold effects are
also conducted based on the posterior samples from Markov chain Monte Carlo (MCMC)
methods. Both simulation studies and an empirical application in predicting
the U.S. stock market returns demonstrate the validity of our methods.

Keywords:
Threshold Model, Nonparametric, Markov Chain Monte Carlo, Bayesian
Inference, Spline.

JEL Classification:
C00

IRTG1792DP2018-049.txt
Strict Stationarity Testing and GLAD Estimation of Double Autoregressive Models

Shaojun Guo
Dong Li
Muyi Li

Abstract
In this article we develop a tractable procedure for testing strict stationarity in a
double autoregressive model and formulate the problem as testing if the top Lyapunov
exponent is negative. Without strict stationarity assumption, we construct a consistent
estimator of the associated top Lyapunov exponent and employ a random weighting
approach for its variance estimation, which in turn are used in a t-type test. We also
propose a GLAD estimation for parameters of interest, relaxing key assumptions on
the commonly used QMLE. All estimators, except for the intercept, are shown to be
consistent and asymptotically normal in both stationary and explosive situations. The
nite-sample performance of the proposed procedures is evaluated via Monte Carlo
simulation studies and a real dataset of interest rates is analyzed.

Keywords:
DAR model, GLAD estimation, Nonstationarity, Random weighting, Strict
stationarity testing.

JEL Classification:
C15, C22

IRTG1792DP2018-050.txt
Variable selection and direction estimation for single-index models via DC-TGDR method

Wei Zhong
Xi Liu
Shuangge Ma

Abstract
This paper is concerned with selecting important covariates
and estimating the index direction simultaneously for
high dimensional single-index models. We develop an efficient
Threshold Gradient Directed Regularization method
via maximizing Distance Covariance (DC-TGDR) between
the single index and response variable. Due to the appealing
property of distance covariance which can measure nonlinear
dependence between random variables, the proposed
method avoids estimating the unknown link function of the
single index and dramatically reduces computational complexity
compared to other methods that use smoothing techniques.
It keeps the model-free advantage from the view of
sufficient dimension reduction and requires neither predictors
nor response variable to be continuous. In addition, the
DC-TGDR method encourages a grouping effect. That is,
it is capable of choosing highly correlated covariates in or
out of the model together. We examine finite-sample performance
of the proposed method by Monte Carlo simulations.
In a real data analysis, we identify important copy number
alterations (CNAs) for gene expression.

Keywords:
Distance covariance, Highdimensional
data, Threshold gradient directed regularization,
Single-index models, Variable selection.

JEL Classification:
C00

IRTG1792DP2018-051.txt
Variable selection and direction estimation for single-index models via DC-TGDR method

Honglin Wang
Fan Yu
Yinggang Zhou

Abstract
The conventional wisdom that housing prices are the present value of future
rents ignores the fact that unlike dividends on stocks, rent is not discretionary.
Housing price uncertainty can affect household property investments, which
in turn affect rent. By extending the theory of investment under uncertainty, we
model the renters decision to buy a house and the landlords decision to sell
as the exercising of real options of waiting and examine real options effects on
rent. Using data from Hong Kong and mainland China, we find a significant
effect of housing price on rent and draw important policy implications.

Keywords:


JEL Classification:
C00

IRTG1792DP2018-052.txt
Nonparametric Additive Instrumental Variable Estimator: A Group Shrinkage Estimation Perspective

Qingliang Fan
Wei Zhong

Abstract
In this article, we study a nonparametric approach regarding a general nonlinear reduced form equation
to achieve a better approximation of the optimal instrument. Accordingly, we propose the nonparametric
additive instrumental variable estimator (NAIVE) with the adaptive group Lasso.We theoretically demonstrate
that the proposed estimator is root-n consistent and asymptotically normal. The adaptive group
Lasso helps us select the valid instruments while the dimensionality of potential instrumental variables is
allowed to be greater than the sample size. In practice, the degree and knots of B-spline series are selected
by minimizing the BIC or EBIC criteria for each nonparametric additive component in the reduced form
equation. In Monte Carlo simulations, we show that the NAIVE has the same performance as the linear
instrumental variable (IV) estimator for the truly linear reduced form equation. On the other hand, the
NAIVE performs much better in terms of bias and mean squared errors compared to other alternative
estimators under the high-dimensional nonlinear reduced form equation. We further illustrate our method
in an empirical study of international trade and growth. Our findings provide

Keywords:
Adaptive group Lasso; Instrumental variables; Nonparametric additive model; Optimal
estimator; Variable selection.

JEL Classification:
C00

IRTG1792DP2018-054.txt
Topic Modeling for Analyzing Open-Ended Survey Responses

Andra-Selina Pietsch
Stefan Lessmann

Abstract
Open-ended responses are widely used in market research studies. Processing of such
responses requires labor-intensive human coding. This paper focuses on unsupervised topic
models and tests their ability to automate the analysis of open-ended responses. Since state-ofthe-
art topic models struggle with the shortness of open-ended responses, the paper considers
three novel short text topic models: Latent Feature Latent Dirichlet Allocation, Biterm Topic
Model and Word Network Topic Model. The models are fitted and evaluated on a set of realworld
open-ended responses provided by a market research company. Multiple components
such as topic coherence and document classification are quantitatively and qualitatively
evaluated to appraise whether topic models can replace human coding. The results suggest that
topic models are a viable alternative for open-ended response coding. However, their
usefulness is limited when a correct one-to-one mapping of responses and topics or the exact
topic distribution is needed.

Keywords:
Market research, open-ended responses, text analytics, short text topic models

JEL Classification:

IRTG1792DP2018 055
Estimation of the discontinuous leverage effect: Evidence from the NASDAQ order book

Markus Bibinger
Christopher Neely
Lars Winkelmann

Abstract
An extensive empirical literature documents a generally negative relation, named the leverage
effect, between asset returns and changes of volatility. It is more challenging to establish
such a return-volatility relationship for jumps in high-frequency data. We propose new nonparametric
methods to assess and test for a discontinuous leverage effect  i.e. a covariation
between contemporaneous jumps in prices and volatility. The methods are robust to market
microstructure noise and build on a newly developed price-jump localization and estimation
procedure. Our empirical investigation of six years of transaction data from 320 NASDAQ
firms displays no unconditional negative covariation between price and volatility cojumps.
We show, however, that there is a strong and significant discontinuous leverage effect if
one conditions on the sign of price jumps and whether the price jumps are market-wide or
idiosyncratic.

Keywords:
High-frequency data, market microstructure, news impact, market-wide jumps, price jump, volatility jump

JEL Classification:
C13, C58

IRTG1792DP2018 056
Estimation of the discontinuous leverage effect: Evidence from the NASDAQ order book

Daniel Traian Pele
Miruna Mazurencu-Marinescu-Pele

Abstract
In this paper we investigate the statistical properties of cryptocurrencies by using alpha-stable distributions. We also study the benefits of the Metcalfe's law (the value of a network is proportional to the square of the number of connected users of the system) for the evaluation of cryptocurrencies. As the results showed a potential for herding behaviour, we used LPPL models to capture the behaviour of cryptocurrencies exchange rates during an endogenous bubble and to predict the most probable time of the regime switching.

Keywords:
cryptocurrency, Bitcoin, CRIX, Log-Periodic Power Law, Metcalfes law, stable distribution

JEL Classification:
C22, C32, C51, C53, C58, E41, E42, E47, E51, G1, G17

IRTG1792DP2018 057
Trending Mixture Copula Models with Copula Selection

Bingduo Yang
Zongwu Caib
Christian M. Hafner
Guannan Liu

Abstract
Modeling the joint tails of multiple nancial time series has important im-
plications for risk management. Classical models for dependence often encounter a lack
of t in the joint tails, calling for additional exibility. In this paper we introduce a new
nonparametric time-varying mixture copula model, in which both weights and depen-
dence parameters are deterministic functions of time. We propose penalized trending
mixture copula models with group smoothly clipped absolute deviation (SCAD) penal-
ty functions to do the estimation and copula selection simultaneously. Monte Carlo
simulation results suggest that the shrinkage estimation procedure performs well in s-
electing and estimating both constant and trending mixture copula models. Using the
proposed model and method, we analyze the evolution of the dependence among four
international stock markets, and nd substantial changes in the levels and patterns of
the dependence, in particular around crisis periods.

Keywords:
Copula, Time-Varying Copula, Mixture Copula, Copula Selection

JEL Classification:

IRTG1792DP2018 058
Investing with cryptocurrencies - evaluating the potential of portfolio allocation strategies

Alla Petukhina
Simon Trimborn
Wolfgang Karl Härdle
Hermann Elendner

Abstract
The market capitalization of cryptocurrencies has risen rapidly during the
last few years. Despite their high volatility, this fact has spurred growing
interest in cryptocurrencies as an alternative investment asset for portfolio and
risk management. We characterise the effects of adding cryptocurrencies in addition
to traditional assets to the set of eligible assets in portfolio management.
Out-of-sample performance and diversification benefits are studied for the most
popular portfolio-construction rules, including mean-variance optimization,
risk-parity, and maximum-diversification strategies, as well as combined strategies.
To account for the frequently low liquidity of cryptocurrency markets
we incorporate the LIBRO method, which gives suitable liquidity constraints.
Our results show that cryptocurrencies can improve the risk-return profile of
portfolios. In particular, cryptocurrencies are more useful for portfolio strategies
with higher target returns; they do not play a role in minimum-variance
portfolios. However, a maximum-diversification strategy (maximising the Portfolio
Diversification Index, PDI) draws appreciably on cryptocurrencies, and
spanning tests clearly indicate that cryptocurrency returns are non-redundant
additions to the investment universe.

Keywords:
cryptocurrency, CRIX, investments, portfolio management, asset
classes, blockchain, Bitcoin, altcoins, DLT

JEL Classification:
C01, C58, G11

IRTG1792DP2018 059
Towards the interpretation of time-varying regularization parameters in streaming penalized regression models

Lenka Zbonakova
Ricardo Pio Monti
Wolfgang Karl Härdle

Abstract
High-dimensional, streaming datasets are ubiquitous in modern applications.
Examples range from nance and e-commerce to the study of biomedical and
neuroimaging data. As a result, many novel algorithms have been proposed to
address challenges posed by such datasets. In this work, we focus on the use of L1-
regularized linear models in the context of (possibly non-stationary) streaming
data. Recently, it has been noted that the choice of the regularization parameter
is fundamental in such models and several methods have been proposed which
iteratively tune such a parameter in a time-varying manner, thereby allowing
the underlying sparsity of estimated models to vary. Moreover, in many applications,
inference on the regularization parameter may itself be of interest, as
such a parameter is related to the underlying sparsity of the model. However, in
this work, we highlight and provide extensive empirical evidence regarding how
various (often unrelated) statistical properties in the data can lead to changes
in the regularization parameter. In particular, through various synthetic experiments,
we demonstrate that changes in the regularization parameter may be
driven by changes in the true underlying sparsity, signal-to-noise ratio or even
model misspecication. The purpose of this letter is, therefore, to highlight and
catalog various statistical properties which induce changes in the associated regularization
parameter. We conclude by presenting two applications: one relating
to nancial data and another to neuroimaging data, where the aforementioned
discussion is relevant.

Keywords:
Lasso, penalty parameter, stock prices, neuroimaging

JEL Classification:
C13, C15, C63

IRTG1792DP2018 061
PLUG-IN L2-UPPER ERROR BOUNDS IN DECONVOLUTION, FOR A MIXING DENSITY ESTIMATE IN Rd AND FOR ITS DERIVATIVES

Yannis G. Yatracos

Abstract
In deconvolution in Rd; d 1; with mixing density p(2 P) and kernel h; the mixture
density fp(2 Fp) can always be estimated with f^pn; ^pn 2 P; via Minimum Distance
Estimation approaches proposed herein, with calculation of f^pn's upper L1-error rate, an;
in probability or in risk; h is either known or unknown, an decreases to zero with n: In
applications, an is obtained when P consists either of products of d densities dened on
a compact, or L1 separable densities in R with their dierences changing sign at most J
times; J is either known or unknown. When h is known and p is ~q-smooth, vanishing
outside a compact in Rd; plug-in upper bounds are then also provided for the L2-error
rate of ^pn and its derivatives, respectively, in probability or in risk; ~q 2 R+; d 1: These
L2-upper bounds depend on h's Fourier transform, ~h(6= 0); and have rates (log a??1
n )??N1 and aN2 n , respectively, for h super-smooth and smooth; N1 > 0; N2 > 0: For the typical
an (log n) n??; the former (logarithmic) rate bound is optimal for any > 0 and
the latter misses the optimal rate by the factor (log n) when = :5; > 0; > 0: The
exponents N1 and N2 appear also in optimal rates and lower error and risk bounds in the
deconvolution literature.

Keywords:


JEL Classification:

IRTG1792DP2018 060
RESIDUAL'S INFLUENCE INDEX (RINFIN), BAD LEVERAGE AND UNMASKING IN HIGH DIMENSIONAL L2-REGRESSION

Yannis G. Yatracos

Abstract
In linear regression of Y on X(2 Rp) with parameters (2 Rp+1);
statistical inference is unreliable when observations are obtained
from gross-error model, F;G = (1??)F +G; instead of the assumed
probability F;G is gross-error probability, 0 < < 1: When G is unit
mass at (x; y); Residual's In uence Index, RINFIN(x; y; ; ), measures
the dierence in small x-perturbations of L2-residual, r(x; y);
for model F and for F;G via r's x-partial derivatives. Asymptotic
properties are presented for sample RINFIN that is successful in
extracting indications for in uential and bad leverage cases in microarray
data and simulated, high dimensional data. Its performance
improves as p increases and can also be used in multiple response
linear regression. RINFIN's advantage is that, whereas in in uence
functions of L2-regression coecients each x-coordinate and r(x; y)
appear in a sum as product with moderate size when (x; y) is bad
leverage case and masking makes r(x; y) nearly vanish, RINFIN's
x-partial derivatives convert the product in sum allowing for unmasking.

Keywords:
Big Data, Data Science, In fluence Function, Leverage, Masking, Residual's In fluence Index (RINFIN)

JEL Classification:

AMS 2010 subject classications:
62-07, 62-09, 62J05, 62F35, 62G35

IRTG1792DP2018 062
Conversion uplift in e-commerce: A systematic benchmark of modeling strategies

Robin Gubela
Artem Bequé
Fabian Gebert
Stefan Lessmann

Abstract
Uplift modeling combines machine learning and experimental strategies to estimate the differential
effect of a treatment on individuals behavior. The paper considers uplift models in the scope of
marketing campaign targeting. Literature on uplift modeling strategies is fragmented across academic
disciplines and lacks an overarching empirical comparison. Using data from online retailers,
we fill this gap and contribute to literature through consolidating prior work on uplift modeling
and systematically comparing the predictive performance and utility of available uplift modeling
strategies. Our empirical study includes three experiments in which we examine the interaction
between an uplift modeling strategy and the underlying machine learning algorithm to implement
the strategy, quantify model performance in terms of business value and demonstrate the advantages
of uplift models over response models, which are widely used in marketing. The results
facilitate making specific recommendations how to deploy uplift models in e-commerce applications.

Keywords:
e-commerce analytics, machine learning, uplift modeling, real-time targeting

JEL Classification:

IRTG1792DP2018 063
Causal Inference using Machine Learning. An Evaluation of recent Methods through Simulations

Daniel Jacob
Stefan Lessmann
Wolfgang Karl Härdle

Abstract
The estimation of a causal parameter in a high-dimensional setting where the
functions are potentially complex is a challenging task. Parametric and linear
modelling is not sufficient to generate unbiased and consistent estimators. Modern
approaches, therefore, use machine learning (ML) algorithms to learn these nuisance
functions. However, this leads to new problems like the regularization bias or
overfitting that are common when using ML models.
This paper considers different novel methods that overcome these problems or at
least address them. These methods differ in terms of the target parameter, namely
the average treatment effect of the population, group heterogeneity or the conditional
average treatment effect for each individual. Each method is first investigated and
tested separately and second, they are compared among each other. To do this in a
disciplined manner, simulations with synthetic data are used. This ensures that all
distributions of the generated treatment effect parameters are known. The findings
are that each method has its limits in terms of unbiased estimation, the detection
of heterogeneity and also the determination of which covariates are responsible for
different causal effects.

Keywords:
causal inference, machine learning, simulation study, sample-splitting
double machine learning, sorted group ATE (GATES), causal tree

JEL Classification:
C01, C14, C31, C63

IRTG1792DP2018 064
Semiparametric Estimation and Variable Selection for Single-index Copula Models



Bingduo Yang

Christian M. Hafner

Guannan Liu

Wei Long



Abstract:

A copula model with flexibly specified dependence structure can be useful to capture the complexity and heterogeneity in economic and financial time series. However, there exists little methodological guidance for the specification process using copulas. This paper contributes to fill this gap by considering the recently proposed single-index copulas, for which we propose a simultaneous estimation and variable selection procedure. The proposed method allows to choose the most relevant state variables from a comprehensive set using a penalized estimation, and we derive its large sample properties. Simulation results demonstrate the good performance of the proposed method in selecting the appropriate state variables and estimating the unknown index coefficients and dependence parameters. An application of the new procedure identifies six macroeconomic driving factors for the dependence among U.S. housing markets.



Keywords:
Semiparametric Copula, Single-Index Copula, Variable Selection, SCAD



JEL Classification:
C14, C22

IRTG1792DP2018 065
Price Management in the Used-Car Market: An Evaluation of Survival Analysis

Alexander Born
Nikoleta Kovachka
Stefan Lessmann
Hsin-Vonn Seow

Abstract:
Second-hand car markets contribute to billions of Euro turnover each year but
hardly generate profit for used car dealers. The paper examines the potential of
sophisticated data-driven pricing systems to enhance supplier-side decision-
making and escape the zero-profit-trap. Profit maximization requires an accurate
understanding of demand. The paper identifies factors that characterize consumer
demand and proposes a framework to estimate demand functions using survival
analysis. Empirical analysis of a large data set of daily used car sales between
2008 to 2012 confirm the merit of the new factors. Observed results also show
the value of survival analysis to explain and predict demand. Random survival
forest emerges as the most suitable vehicle to develop price response functions
as input for a dynamic pricing system.

Keywords:
Automotive Industry, Price Optimization, Survival Analysis, Dynamic Pricing

JEL Classification:
""

IRTG1792DP2018 066
Deep learning-based cryptocurrency sentiment construction

Sergey Nasekin
Cathy Yi-Hsuan Chen

Abstract:
We study investor sentiment on a non-classical asset, cryptocurrencies using a
“cryptospecificlexicon” recently proposed in Chen et al. (2018) and statistical
learning methods.We account for context-specific information and word similarity
by learning word embeddingsvia neural network-based Word2Vec model. On top of
pre-trained word vectors, weapply popular machine learning methods such as
recursive neural networks for sentencelevelclassification and sentiment index
construction. We perform this analysis on a noveldataset of 1220K messages
related to 425 cryptocurrencies posted on a microblogging platformStockTwits
during the period between March 2013 and May 2018. The constructed sentiment
indices are value-relevant in terms of its return and volatility predictability
for thecryptocurrency market index.

Keywords:
sentiment analysis, lexicon, social media, word embedding, deep learning

JEL Classification:
G41, G4, G12

IRTG1792DP2018 067
COOLING MEASURES AND HOUSING WEALTH: EVIDENCE FROM SINGAPORE Wolfgang Karl Härdle Rainer Schulz Taojun Xie Abstract: Excessive house price growth was at the heart of the financial crisis in 2007/08. Since then, many countries have added cooling measures to their regulatory frameworks. It has been found that these measures can indeed control price growth, but no one has examined whether this has adverse consequences for the housing wealth distribution. We examine this for Singapore, which started in 2009 to target price growth over ten rounds in total. We find that welfare from housing wealth in the last round might not be higher than before 2009. This depends on the deflator used to convert nominal into real prices. Irrespective of the deflator, we can reject that welfare increased monotonically over the different rounds. Keywords: house price distribution, stochastic dominance tests JEL Classification: R31, C31, C55

IRTG1792DP2019 001
Cooling Measures and Housing Wealth: Evidence from Singapore

Wolfgang Karl Härdle
Rainer Schulz
Taojun Xie

Abstract:
Excessive house price growth was at the heart of the financial crisis in
2007/08. Since then, many countries have added cooling measures to their
regulatory frameworks. It has been found that these measures can indeed control
price growth, but no one has examined whether this has adverse consequences for
the housing wealth distribution. We examine this for Singapore, which started in
2009 to target price growth over ten rounds in total. We find that welfare from
housing wealth in the last round might not be higher than before 2009. This
depends on the deflator used to convert nominal into real prices. Irrespective
of the deflator, we can reject that welfare increased monotonically over the
different rounds.

Keywords:
house price distribution, stochastic dominance tests

JEL Classification:
R31, C31, C55

IRTG1792DP2019 002
Information Arrival, News Sentiment, Volatilities and Jumps of Intraday Returns

Ya Qian
Jun Tu
Wolfgang Karl Härdle

Abstract:
This work aims to investigate the (inter)relations of information arrival, news
sentiment, volatilities and jump dynamics of intraday returns. Two parametric
GARCH-type jump models which explicitly incorporate both news arrival and news
sentiment variables are proposed, among which one assumes news affecting
financial markets through the jump component while the other postulating the
GARCH component channel. In order to give the most-likely format of the
interactions between news arrival and stock market behaviors, these two models
are compared with several other easier versions of GARCH-type models based on
the calibration results on DJIA 30 stocks. The necessity to include news
processes in intraday stock volatility modeling is justified in our specific
calibration samples (2008 and 2013, respectively). While it is not as profitable
to model jump process separately as using simpler GARCH process with error
distribution capable to capture fat tail behaviors of financial time series. In
conclusion, our calibration results suggest GARCH-news model with skew-t
innovation distribution as the best candidate for intraday returns of large
stocks in US market, which means one can probably avoid the complicatedness of
modelling jump behavior by using a simplier skew-t error distribution assumption
instead, but it’s necessary to incorporate news variables.

Keywords:
information arrival, volatility modeling, jump, sentiment, GARCH

JEL Classification:
C52, C55, C58, G14

IRTG1792DP2019 003
Estimating low sampling frequency risk measure by high-frequency data

Niels Wesselhöfft
Wolfgang K. Härdle

Abstract:
Weekly, quarterly and yearly risk measures are crucial for risk reporting
according to Basel III and Solvency II. For the respective data frequencies, the
authors show in a simulation and backtest study that available data series are
not sufficient in order to estimate Value at Risk and Expected Shortfall
sufficiently, given confidence levels of 99.9% and 99.99%. Accordingly, this
paper presents a semi-parametric estimation method, rescaling data from high- to
low-frequency which allows to obtain significantly more data points for the
estimation of the respective risk measures. The presented methodology in the
α-stable framework, which is able to mimic multifractal behavior in asset
returns, provides tail events which never occurred in the original low-frequency
dataset.

Keywords:
high-frequency, multifractal, stable distribution, rescaling, risk management,
Value at Risk, quantile distribution

JEL Classification:
C14, C22, C46, C53, G32

IRTG1792DP2019 004
Constrained Kelly portfolios under alpha-stable laws

Niels Wesselhöfft
Wolfgang K. Härdle

Abstract:
This paper provides a detailed framework for modeling portfolios, achieving the
highest growth rate under subjective risk constraints such as Value at Risk
(VaR) in the presence of stable laws. Although the maximization of the expected
logarithm of wealth induces outperforming any other significantly different
strategy, the Kelly Criterion implies larger bets than a risk-averse investor
would accept. Restricting the Kelly optimization by spectral risk measures, the
authors provide a generalized mapping for different measures of growth and
security. Analyzing over 30 years of S&P 500 returns for different sampling
frequencies, the authors find evidence for leptokurtic behavior for all
respective sampling frequencies. Given that lower sampling frequencies imply a
smaller number of data points, this paper argues in favor of α-stable laws and
its scaling behavior to model financial market returns for a given horizon in an
i.i.d. world. Instead of simulating from the class of elliptically stable
distributions, a nonparametric scaling approximation, based on the data-set
itself, is proposed. Our paper also uncovers that including long put options
into the portfolio optimization, improves the growth criterion for a given
security level, leading to a new Kelly portfolio providing the highest geometric
mean.

Keywords:
growth-optimal, Kelly criterion, protective put, portfolio optimization, stable
distribution, Value at Risk

JEL Classification:
C13, C46, C61, C73, G11

IRTG1792DP2019 005
Usage Continuance in Software-as-a-Service

Elias Baumann
Jana Kern
Stefan Lessmann

Abstract:
Software-as-a-service applications are experiencing immense growth as their
comparatively low cost makes them an important alternative to traditional
software. Following the initial adoption phase, vendors are now concerned with
the continued usage of their software. To analyze the influence of different
measures to improve continued usage over time, a longitudinal study design using
data from a SaaS vendor was implemented. By employing a linear mixed model, the
study finds several measures to have a positive effect on a software’s usage
penetration. In addition to these activation measures performed by the SaaS
vendor, software as well as client characteristics were likewise examined but
did not display significant estimates. In summary the study contributes novel
insights into the scarcely researched field of influencing factors on SaaS usage
continuance.

Keywords:
Linear Mixed Models Software-as-a-Service Usage Continuance

JEL Classification:
C00

IRTG1792DP2019 006
Adaptive Nonparametric Community Detection

Larisa Adamyan
Kirill Efimov
Vladimir Spokoiny

Abstract:
Understanding the topological structure of real world networks is of huge
interest in a variety of fields. One of the way to investigate this structure is
to find the groups of densely connected nodes called communities. This paper
presents a new non-parametric method of community detection in networks called
Adaptive Weights Community Detection. The idea of the algorithm is to associate
a local community for each node. On every iteration the algorithm tests a
hypothesis that two nodes are in the same community by comparing their local
communities. The test rejects the hypothesis if the density of edges between
these two local communities is lower than the density inside each one. A
detailed performance analysis of the method shows its dominance over state-of-
the-art methods on well known artificial and real world benchmarks.

Keywords:
Adaptive weights, Gap coefficient, Graph clustering, Nonparametric, Overlapping
communities

JEL Classification:
C00

IRTG1792DP2019 008
Forex Exchange Rate Forecasting Using Deep Recurrent Neural Networks

Alexander J. Dautel
Wolfgang K. Härdle
Stefan Lessmann
Hsin-Vonn Seow

Abstract:
Deep learning has substantially advanced the state-of-the-art in computer
vision, natural language processing and other elds. The paper examines the
potential of contemporary recurrent deep learning architectures for nancial
time series forecasting. Considering the foreign exchange market as testbed, we
systematically compare long short-term memory networks and gated recurrent units
to traditional recurrent architectures as well as feedforward networks in terms
of their directional forecasting accuracy and the profitability of trading model
predictions. Empirical results indicate the suitability of deep networks for
exchange rate forecasting in general but also evidence the diculty of
implementing and tuning corresponding architectures. Especially with regard to
trading pro t, a simpler neural network may perform as well as if not better
than a more complex deep neural network.

Keywords:
Deep learning, Financial time series forecasting, Recurrent neural networks,
Foreign exchange rates

JEL Classification:
C00

IRTG1792DP2019 009
Dynamic Network Perspective of Cryptocurrencies

Li Guo
Yubo Tao
Wolfgang K. Härdle

Abstract:
Cryptocurrencies are becoming an attractive asset class and are the focus of
recent quantitative research. The joint dynamics of the cryptocurrency market
yields information on network risk. Utilizing the adaptive LASSO approach, we
build a dynamic network of cryptocurrencies and model the latent communities
with a dynamic stochastic blockmodel. We develop a dynamic covariate-assisted
spectral clustering method to uniformly estimate the latent group membership of
cryptocurrencies consistently. We show that return inter-predictability and
crypto characteristics, including hashing algorithms and proof types, jointly
determine the crypto market segmentation. Based on this classification result,
it is natural to employ eigenvector centrality to identify a cryptocurrency’s
idiosyncratic risk. An asset pricing analysis finds that a cross-sectional
portfolio with a higher centrality earns a higher risk premium. Further tests
confirm that centrality serves as a risk factor well and delivers valuable
information content on cryptocurrency markets.

Keywords:
Community Detection, Dynamic Stochastic Blockmodel, Spectral Clustering, Node
Covariate, Return Predictability, Portfolio Management

JEL Classification:
C00

IRTG1792DP2019 010
Understanding the Role of Housing in Inequality and Social Mobility

Yang Tang
Xinwen Ni

Abstract:
Housing typically takes up a major proportion of households' expenditure, and
thus it certainly plays a critical role in shaping the pattern of income in-
equality and social mobility. Whether high housing price-to-rent ratio will am-
plify inequality and inhibit social class upgrading is still a controversial
issue in the existing literature. In this paper, we develop a partial
equilibrium life- cycle framework to address these issues. Agents in our economy
are divided into two social classes according to the initial human capital level
inherited from their parents. Those who belong to upper class will draw their
innate abilities from a distribution that rst order stochastically dominates
those from lower class. Throughout the entire lifecycle, agents make endogenous
human capital investment and housing tenure decisions. We calibrate the model to
mimic some stylized facts in the the real world counter part. Our simulation
results indicate an inverse-U pattern between housing price-to-rent ratio and
measures of income inequality, and as well as a U-shape pattern between price-
to-rent ratio and social mobility measured by Shorrocks Index. The implication
is that housing tends to amplify the inequality and slow down the social
mobility when houses can only be purchased by a small group of agents in the
economy. Moreover, our results also suggest that better quality of education as
a result of a higher return to human capital investment tends to dampen the role
of housing.

Keywords:
Income Inequality, Social Mobility, Price-to-rent ratio

JEL Classification:
C00

IRTG1792DP2019 011
The role of medical expenses in the saving decision of elderly: a life cycle
model

Xinwen Ni

Abstract:
In this paper, we develop an multi period overlapping generation framework to
investigate agents' consumption and saving decisions, inequality and welfare
among elderly. We assume that agents are heterogeneous in the non-asset income
and the medical expenditure. In order to explicitly analyze the e ects of
medical expenditure, we conduct three counterfactual exercises. We successively
shut down the heterogeneity in labor income, in the level and in the dispersion
of medical expenses respectively. By comparing the benchmark with the
counterfactual results, we nd that in general wealth inequality decreases with
age, and income uncertainty contributes the most to wealth inequality. Both
average consumption and consumption inequality increase with age. Consumption
inequality largely tracks income inequality. Though uncertainty in medical
expenditures has little e ect on consumption inequality, a higher level of
medical expenditures may exacerbate consumption inequality. Meanwhile, the
average saving of elderly exhibits an inverse-U shape with age. The impacts on
average saving are similar both in benchmark and in counterfactual exercises.
Welfare increases with age.

Keywords:
Income Inequality, Social Mobility, Price-to-rent ratio

JEL Classification:
C00

IRTG1792DP2019 012
Voting for Health Insurance Policy: the U.S. versus Europe

Xinwen Ni

Abstract:
In this paper, we build an overlapping generation model to examine the reason
why developed countries with similar background have implemented different
social health insurance systems. We propose two hypotheses to explain this
phenomenon: (i) the different participation rates of the poor in the voting;
(ii) the distinct attitudes towards the size of the government and the existence
of a compulsory social health insurance system. Agents need to vote for one of
two policies: Policy I without Social Health Insurance (SHI) but with the
subsidy for the poor, and Policy II with fully covered SHI. By comparing either
their current utility or the expected life time utility, households will choose
one policy. We find that under Policy I, the derivative of the changes of
expected utility with respect to income is not monotonic. This means that both
the poorest and the richest dislike the social health insurance system. With the
calibrated parameters, we solve the benchmark and find that the public’s
attitude towards the size of the government and the lower representation of the
poor affect the election result. The changes in the minimum consumption level
under Policy I affect the voting results most, followed by the attitude. Voting
Participant rate plays the most insignificant role in the voting outcome. The
sensitivity analysis shows that our main findings are robust to the input
parameters.

Keywords:
Social Health Insurance, Voting

JEL Classification:
C00

IRTG1792DP2019 013
Inference of Break-Points in High-Dimensional Time Series

Likai Chen
Weining Wang
Wei Biao Wu

Abstract:
We consider a new procedure for detecting structural breaks in mean for high-
dimensional time series. We target breaks happening at unknown time points and
locations. In particular, at a fixed time point our method is concerned with
either the biggest break in one location or aggregating simultaneous breaks over
multiple locations. We allow for both big or small sized breaks, so that we can
1), stamp the dates and the locations of the breaks, 2), estimate the break
sizes and 3), make inference on the break sizes as well as the break dates. Our
theoretical setup incorporates both temporal and crosssectional dependence, and
is suitable for heavy-tailed innovations. We derive the asymptotic distribution
for the sizes of the breaks by extending the existing powerful theory on local
linear kernel estimation and high dimensional Gaussian approximation to allow
for trend stationary time series with jumps. A robust long-run covariance matrix
estimation is proposed, which can be of independent interest. An application on
detecting structural changes of the US unemployment rate is considered to
illustrate the usefulness of our method.

Keywords:
high-dimensional time series, multiple change-points, Gaussian approximation,
nonparametric estimation, heavy tailed, long-run covariance matrix

JEL Classification:
C00

IRTG1792DP2019 014
Forecasting in Blockchain-based Local Energy Markets

Michael Kostmann
Wolfgang K. Härdle

Abstract:
Increasingly volatile and distributed energy production challenge traditional
mechanisms to manage grid loads and price energy. Local energy markets (LEMs)
may be a response to those challenges as they can balance energy production and
consumption locally and may lower energy costs for consumers. Blockchain-based
LEMs provide a decentralized market to local energy consumer and prosumers. They
implement a market mechanism in the form of a smart contract without the need
for a central authority coordinating the market. Recently proposed blockchain-
based LEMs use auction designs to match future demand and supply. Thus, such
blockchain-based LEMs rely on accurate short-term forecasts of individual
households’ energy consumption and production. Often, such accurate forecasts
are simply assumed to be given. The present research tests this assumption.
First, by evaluating the forecast accuracy achievable with state-of-the-art
energy forecasting techniques for individual households and, second, by
assessing the effect of prediction errors on market outcomes in three different
supply scenarios. The evaluation shows that, although a LASSO regression model
is capable of achieving reasonably low forecasting errors, the costly settlement
of prediction errors can offset and even surpass the savings brought to
consumers by a blockchain-based LEM. This shows, that due to prediction errors,
participation in LEMs may be uneconomical for consumers, and thus, has to be
taken into consideration for pricing mechanisms in blockchain-based LEMs.

Keywords:
Blockchain; Local Energy Market; Smart Contract; Machine Learning; Household;
Energy Prediction; Prediction Errors; Market Mechanism

JEL Classification:
Q47; D44; D47; C53

IRTG1792DP2019 015
Media-expressed tone, Option Characteristics, and Stock Return Predictability

Cathy Yi-Hsuan Chen
Matthias R. Fengler
Wolfgang K. Härdle
Yanchu Liu

Abstract:
We distill tone from a huge assortment of NASDAQ articles to examine the
predictive power of media-expressed tone in single-stock option markets and
equity markets. We find that (1) option markets are impacted by media tone; (2)
option variables predict stock returns along with tone; (3) option variables
orthogonalized to public information and tone are more effective predictors of
stock returns; (4) overnight tone appears to be more informative than trading-
time tone, possibly due to a different thematic coverage of the trading versus
the overnight archive; (5) tone disagreement commands a strong positive risk
premium above and beyond market volatility.

Keywords:
option markets, equity markets, stock return predictability, media tone, topic
model

JEL Classification:
G12, G14, G41

IRTG1792DP2019 016
What makes cryptocurrencies special? Investor sentiment and return
predictability during the bubble

Cathy Yi-Hsuan Chen
Roméo Després
Li Guo
Thomas Renault

Abstract:
The 2017 bubble on the cryptocurrency market recalls our memory in the dot-com
bubble, during which hard-to-measure fundamentals and investors’ illusion for
brand new technologies led to overvalued prices. Benefiting from the massive
increase in the volume of messages published on social media and message boards,
we examine the impact of investor sentiment, conditional on bubble regimes, on
cryptocurrencies aggregate return prediction. Constructing a crypto-specific
lexicon and using a local-momentum autoregression model, we find that the
sentiment effect is prolonged and sustained during the bubble while it turns out
a reversal effect once the bubble collapsed. The out-of-sample analysis along
with portfolio analysis is conducted in this study. When measuring investor
sentiment for a new type of asset such as cryptocurrencies, we highlight that
the impact of investor sentiment on cryptocurrency returns is conditional on
bubble regimes.

Keywords:
Cryptocurrency; Sentiment; Bubble; Return Predictability

JEL Classification:
G02; G10; G12

IRTG1792DP2019 017
Portmanteau Test and Simultaneous Inference for Serial Covariances

Han Xiao
Wei Biao Wu

Abstract:
The paper presents a systematic theory for asymptotic inferences based on
autocovariances of stationary processes. We consider nonparametric tests for se
rial correlations using the maximum and the quadratic deviations of sample
autocovariances. For these cases, with proper centering and rescaling, the
asymptotic distributions of the deviations are Gumbel and Gaussian, respec
tively. To establish such an asymptotic theory, as byproducts, we develop a
normal comparison principle and propose a sufficient condition for summability
of joint cumulants of stationary processes. We adapt a blocks of blocks
bootstrapping procedure proposed by Kuensch (1989) and Liu and Singh (1992) to
the maximum deviation based tests to improve the finite-sample performance.

Keywords:
Autocovariance, blocks of blocks bootstrapping, Box-Pierce test, extreme value
distribution, moderate deviation, normal comparison, physical dependence
measure, short range dependence, stationary process, summability of cumulants

JEL Classification:
C00

IRTG1792DP2019 018
Phenotypic convergence of cryptocurrencies

Daniel Traian Pele
Niels Wesselhöfft
Wolfgang K. Härdle
Michalis Kolossiatis
Yannis Yatracos

Abstract:
The aim of this paper is to prove the phenotypic convergence of
cryptocurrencies, in the sense that individual cryptocurrencies respond to
similar selection pressures by developing similar characteristics. In order to
retrieve the cryptocurrencies phenotype, we treat cryptocurrencies as financial
instruments (genus proximum) and find their specific difference (differentia
specifica) by using the daily time series of log-returns. In this sense, a daily time
series of asset returns (either cryptocurrencies or classical assets) can be
characterized by a multidimensional vector with statistical components like
volatility, skewness, kurtosis, tail probability, quantiles, conditional tail
expectation or fractal dimension. By using dimension reduction techniques
(Factor Analysis) and classification models (Binary Logistic Regression,
Discriminant Analysis, Support Vector Machines, K-means clustering, Variance
Components Split methods) for a representative sample of cryptocurrencies,
stocks, exchange rates and commodities, we are able to classify cryptocurrencies
as a new asset class with unique features in the tails of the log-returns
distribution. The main result of our paper is the complete separation of the
cryptocurrencies from the other type of assets, by using the Maximum Variance
Components Split method. More, we observe a divergent evolution of the
cryptocurrencies species, compared to the classical assets, mainly due to the
tails behaviour of the log-returns distribution. The codes used here are
available via www.quantlet.de.

Keywords:
cryptocurrency, genus proximum, differentia specifica, classification,
multivariate analysis, factor models, phenotypic convergence, divergent
evolution

JEL Classification:
C14, C22, C46, C53, G32

IRTG1792DP2019 019
Modelling Systemic Risk Using Neural Network Quantile Regression

Georg Keilbar
Weining Wang

Abstract:
We propose an approach to calibrate the conditional value-at-risk (CoVaR) of
financial institutions based on neural network quantile regression. Building on
the estimation results we model systemic risk spillover effects across banks by
considering the marginal effects of the quantile regression procedure. We adopt a
dropout regularization procedure to remedy the well-known issue of overfitting
for neural networks, and we provide empirical evidence for the favorable out-of-
sample performance of a regularized neural network. We then propose three
measures for systemic risk from our fitted results. We find that systemic risk
increases sharply during the height of the financial crisis in 2008 and again
after a short period of easing in 2011 and 2015. Our approach also allows
identifying systemically relevant firms during the financial crisis.

Keywords:
Systemic risk, CoVaR, Quantile regression, Neural networks

JEL Classification:
C00

IRTG1792DP2019 020
Rise of the Machines? Intraday High-Frequency Trading Patterns of
Cryptocurrencies

Alla A. Petukhina
Raphael C. G. Reule
Wolfgang Karl Härdle

Abstract:
This research analyses high-frequency data of the cryptocurrency market in
regards to intraday trading patterns. We study trading quantitatives such as
returns, traded volumes, volatility periodicity, and provide summary statistics
of return correlations to CRIX (CRyptocurrency IndeX), as well as respective
overall high-frequency based market statistics. Our results provide mandatory
insight into a market, where the grand scale employment of automated trading
algorithms and the extremely rapid execution of trades might seem to be a
standard based on media reports. Our findings on intraday momentum of trading
patterns lead to a new view on approaching the predictability of economic value
in this new digital market.

Keywords:
Cryptocurrency, High-Frequency Trading, Algorithmic Trading, Liquidity,
Volatility, Price Impact, CRIX

JEL Classification:
G02, G11, G12, G14, G15, G23

IRTG1792DP2019 021
FRM Financial Risk Meter

Andrija Mihoci
Michael Althof
Cathy Yi-Hsuan Chen
Wolfgang Karl Hardle

Abstract:
A daily systemic risk measure is proposed accounting for links and mutual
dependencies between financial institutions utilising tail event information. FRM
(Financial Risk Meter) is based on Lasso quantile regression designed to capture
tail event co-movements. The FRM focus lies on understanding active set data
characteristics and the presentation of interdependencies in a network topology.
Two FRM indices are presented, namely, FRM@Americas and FRM@Europe. The FRM
indices detect systemic risk at selected areas and identifies risk factors. In
practice, FRM is applied to the return time series of selected financial
institutions and macroeconomic risk factors. Using FRM on a daily basis, we
identify companies exhibiting extreme "co-stress", as well as "activators" of
stress. With the SRM@EuroArea, we extend to the government bond asset class. FRM
is a good predictor for recession probabilities, constituting the FRM-implied
recession probabilities. Thereby, FRM indicates tail event behaviour in a
network of financial risk factors.

Keywords:
Systemic Risk, Quantile Regression, Financial Markets, Risk Management, Network
Dynamics, Recession

JEL Classification:
C00

IRTG1792DP2019-021-1
FRM Financial Risk Meter

Andrija Mihoci
Michael Althof
Cathy Yi-Hsuan Chen
Wolfgang Karl Härdle

Abstract:
A daily systemic risk measure is proposed accounting for links and mutual
dependencies between financial institutions utilising tail event information. FRM
(Financial Risk Meter) is based on Lasso quantile regression designed to capture
tail event co-movements. The FRM focus lies on understanding active set data
characteristics and the presentation of interdependencies in a network topology.
Two FRM indices are presented, namely, FRM@Americas and FRM@Europe. The FRM
indices detect systemic risk at selected areas and identifies risk factors. In
practice, FRM is applied to the return time series of selected financial
institutions and macroeconomic risk factors. Using FRM on a daily basis, we
identify companies exhibiting extreme "co-stress", as well as "activators" of
stress. With the SRM@EuroArea, we extend to the government bond asset class. FRM
is a good predictor for recession probabilities, constituting the FRM-implied
recession probabilities. Thereby, FRM indicates tail event behaviour in a
network of financial risk factors.

Keywords:
Systemic Risk, Quantile Regression, Financial Markets, Risk Management, Network
Dynamics, Recession

JEL Classification:
C00

IRTG1792DP2019 022
A Machine Learning Approach Towards Startup Success Prediction

Cemre Ünal
Ioana Ceasu

Abstract:
The importance of startups for a dynamic, innovative and competitive economy has
already been acknowledged in the scientific and business literature. The highly
uncertain and volatile nature of the startup ecosystem makes the evaluation of
startup success through analysis and interpretation of information very time
consuming and computationally intensive. This prediction problem brings forward
the need for a quantitative model, which should enable an objective and fact-
based approach to startup success prediction. This paper presents a series of
reproducible models for startup success prediction, using machine learning
methods. The data used for this purpose was received from the online investor
platform, crunchbase.com. The data has been pre-processed for sampling bias and
imbalance by using the oversampling approach, ADASYN. A total of six different
models are implemented to predict startup success. Using goodness-of-fit
measures, applicable to each model case, the best models selected are the
ensemble methods, random forest and extreme gradient boosting with a test set
prediction accuracy of 94.1% and 94.5% and AUC of 92.22% and 92.91%
respectively. Top variables in these models are last funding to date, first
funding lag and company age. The models presented in this study can be used to
predict success rate for future new firms/ventures in a repeatable way.

Keywords:
Machine learning

JEL Classification:
C00

IRTG1792DP2019 023
Can Deep Learning Predict Risky Retail Investors? A Case Study in Financial Risk
Behavior Forecasting

A. Kolesnikova
Y. Yang
S. Lessmann
T. Ma
M.-C. Sung
J.E.V. Johnson

Abstract:
The paper examines the potential of deep learning to produce decision support
models from structured, tabular data. Considering the context of financial risk
management, we develop a deep learning model for predicting whether individual
spread traders are likely to secure profits from future trades. This embodies
typical modeling challenges faced in risk and behavior forecasting. Conventional
machine learning requires data that is representative of the feature-target
relationship and relies on the often costly development, maintenance, and
revision of handcrafted features. Consequently, modeling highly variable,
heterogeneous patterns such as the behavior of traders is challenging. Deep
learning promises a remedy. Learning hierarchical distributed representations of
the raw data in an automatic manner (e.g. risk taking behavior), it uncovers
generative features that determine the target (e.g., trader’s profitability),
avoids manual feature engineering, and is more robust toward change (e.g.
dynamic market conditions). The results of employing a deep network for
operational risk forecasting confirm the feature learning capability of deep
learning, provide guidance on designing a suitable network architecture and
demonstrate the superiority of deep learning over machine learning and rule-
based benchmarks.

Keywords:
risk management, retail finance, forecasting, deep learning

JEL Classification:
C00

IRTG1792DP2019 024
Risk of Bitcoin Market: Volatility, Jumps, and Forecasts

Junjie Hu
Weiyu Kuo
Wolfgang Karl Härdle

Abstract:
Among all the emerging markets, the cryptocurrency market is considered the most
controversial and simultaneously the most interesting one. The visibly
significant market capitalization of cryptos motivates modern financial
instruments such as futures and options. Those will depend on the dynamics,
volatility, or even the jumps of cryptos. In this paper, the risk
characteristics for Bitcoin are analyzed from a realized volatility dynamics
view. The realized variance RV is estimated with (threshold-)jump components
(T)J, semivariance RSV+(−) , and signed jumps (T)J+(−) . Our empirical results
show that the Bitcoin market is far riskier than any other developed financial
market. Up to 68% of the sample days are identified to entangle jumps. However,
the discontinuities do not contribute to the variance significantly. By
employing a 90-day rolling-window method, the in-sample evidence suggests that
the impacts of predictors change over time systematically under HAR-type models.
The out-of-sample forecasting results reveal that the forecasting horizon
plays an important role in choosing forecasting models. For long-horizon risk
forecast, a finer model calibrated with jumps gives extra utility up to 20 basis
points annually, while an approach based on the roughest estimators suits the
short-horizon risk forecast best. Last but not least, a simple equal-weighted
portfolio not only significantly reduces the size and quantity of jumps but also
gives investors higher utility in short-horizon risk forecast case.

Keywords:
Cryptocurrency, Bitcoin, Realized Variance, Thresholded Jump, Signed Jumps,
Realized Utility

JEL Classification:
C53, E47, G11, G17

IRTG1792DP2019 025
SONIC: SOcial Network with Influencers and Communities

Cathy Yi-Hsuan Chen
Wolfgang Karl Härdle
Yegor Klochkov

Abstract:
The integration of social media characteristics into an econometric framework
requires modeling a high dimensional dynamic network with dimensions of
parameter Θ typically much larger than the number of observations. To cope with
this problem, we introduce a new structural model — SONIC which assumes that
(1) a few influencers drive the network dynamics; (2) the community structure of
the network is characterized as the homogeneity of response to the specific
influencer, implying their underlying similarity. An estimation procedure is
proposed based on a greedy algorithm and LASSO regularization. Through
theoretical study and simulations, we show that the matrix parameter can be
estimated even when the observed time interval is smaller than the size of the
network. Using a novel dataset retrieved from a leading social media platform–
StockTwits and quantifying their opinions via natural language processing, we
model the opinions network dynamics among a select group of users and further
detect the latent communities. With a sparsity regularization, we can identify
important nodes in the network.

Keywords:
social media, network, community, opinion mining, natural language processing

JEL Classification:
C1, C22, C51, G41

IRTG1792DP2019 026
Affordable Uplift: Supervised Randomization in Controlled Exprtiments

Johannes Haupt
Daniel Jacob
Robin M. Gubela
Stefan Lessmann

Abstract:
Customer scoring models are the core of scalable direct marketing. Uplift models
provide an estimate of the incremental benefit from a treatment that is used for
operational decision-making. Training and monitoring of uplift models require
experimental data. However, the collection of data under randomized treatment
assignment is costly, since random targeting deviates from an established
targeting policy. To increase the cost-efficiency of experimentation and
facilitate frequent data collection and model training, we introduce supervised
randomization. It is a novel approach that integrates existing scoring models
into randomized trials to target relevant customers, while ensuring consistent
estimates of treatment effects through correction for active sample selection.
An empirical Monte Carlo study shows that data collection under supervised
randomization is cost-efficient, while downstream uplift models perform
competitively.

Keywords:
Uplift Modeling, Causal Inference, Experimental Design, Selection Bias

JEL Classification:
C00

IRTG1792DP2019 027
VCRIX - a volatility index for crypto-currencies

Alisa Kim
Simon Trimborn
Wolfgang Karl Härdle

Abstract:
Public interest, explosive returns, and diversification opportunities gave
stimulus to the adoption of traditional financial tools to crypto-currencies.
While the CRIX index offered the first scientifically-backed proxy to the
crypto- market (analogous to S&P 500), the introduction of Bitcoin futures by
Cboe became the milestone in the creation of the derivatives market for crypto-
currencies. Following the intuition of the "fear index" VIX for the American
stock market, the VCRIX volatility index was created to capture the investor
expectations about the crypto-currency ecosystem. VCRIX is built based on CRIX
and offers a forecast for the mean annualized volatility of the next 30 days,
re-estimated daily. The model was back-tested for its forecasting power,
resulting in low MSE performance and further examined by the simulation of VIX
(resulting in a correlation of 78% between the actual VIX and VIX estimated with
the VCRIX model). VCRIX provides forecasting functionality and serves as a proxy
for the investors’ expectations in the absence of the de- veloped derivatives
market. These features provide enhanced decision making capacities for market
monitoring, trading strategies, and potentially option pricing.

Keywords:
index construction, volatility, crypto-currency, VCRIX

JEL Classification:
C51, C52, C53, G10

IRTG1792DP2019 028-1
Group Average Treatment Effects for Observational Studies

 

Daniel Jacob
Wolfgang Karl Härdle
Stefan Lessmann

 

Abstract:
The paper proposes an estimator to make inference of heterogeneous treatment effects sorted by impact groups (GATES) for non-randomised experiments. Observational studies are standard in policy evaluation from labour markets, educational surveys and other empirical studies. To control for a potential selection-bias we implement a doubly-robust estimator in the first stage. Keeping the flexibility, we can use any machine learning method to learn the conditional mean functions as well as the propensity score. We also use machine learning methods to learn a function for the conditional average treatment effect. The group average treatment effect, is then estimated via a parametric linear model to provide p-values and confidence intervals. To control for confounding in the linear model we use Neyman-orthogonal moments to partial out the effect that covariates have on both, the treatment assignment and the outcome. The result is a best linear predictor for effect heterogeneity based on impact groups. We introduce inclusion-probability weighting as a form of cross-splitting and averaging for each observation to avoid biases through sample splitting. The advantage of the proposed method is a robust linear estimation of heterogeneous group treatment effects in observational studies.

Keywords: causal inference, machine learning, simulation study, confidence intervals, multiple splitting, sorted group ATE (GATES), doubly-robust estimator

JEL Classification: C01, C14, C31, C63

IRTG1792DP2019 029
Antisocial Online Behavior Detection Using Deep Learning

Elizaveta Zinovyeva
Wolfgang Karl Härdle
Stefan Lessmann

Abstract:
The shift of human communication to online platforms brings many benefits to
society due to the ease of publication of opinions, sharing experience, getting
immediate feedback and the opportunity to discuss the hottest topics. Besides
that, it builds up a space for antisocial behavior such as harassment, insult
and hate speech. This research is dedicated to detection of antisocial online
behavior detection (AOB) - an umbrella term for cyberbullying, hate speech,
cyberaggression and use of any hateful textual content. First, we provide a
benchmark of deep learning models found in the literature on AOB detection. Deep
learning has already proved to be efficient in different types of decision
support: decision support from financial disclosures, predicting process
behavior, text-based emoticon recognition. We compare methods of traditional
machine learning with deep learning, while applying important advancements of
natural language processing: we examine bidirectional encoding, compare
attention mechanisms with simpler reduction techniques, and investigate whether
the hierarchical representation of the data and application of attention on
different layers might improve the predictive performance. As a partial
contribution of the final hierarchical part, we introduce pseudo-sentence
hierarchical attention network, an extension of hierarchical attention network –
a recent advancement in document classification.

Keywords:
Deep Learning, Cyberbullying, Antisocial Online Behavior, Attention Mechanism,
Text Classification

JEL Classification:
C00

IRTG1792DP2019 030
Combining Penalization and Adaption in High Dimension with Application in Bond
Risk Premia Forecasting

Xinjue Li
Lenka Zboňáková
Weining Wang
Wolfgang Karl Härdle

Abstract:
The predictability of a high-dimensional time series model in forecasting with
large information sets depends not only on the stability of parameters but also
depends heavily on the active covariates in the model. Since the true empirical
environment can change as time goes by, the variables that function well at the
present may become useless in the future. Combined with the instable parameters,
finding the most active covariates in the parameter time-varying situations
becomes difficult. In this paper, we aim to propose a new method, the Penalized
Adaptive Method (PAM), which can adaptively detect the parameter homogeneous
intervals and simultaneously select the active variables in sparse models. The
newly developed method is able to identify the parameters stability at one hand
and meanwhile, at the other hand, can manage of selecting the active forecasting
covariates at every different time point. Comparing with the classical models,
the method can be applied to high-dimensional cases with different sources of
parameter changes while it steadily reduces the forecast error in high-
dimensional data. In the out-of-sample bond risk premia forecasting, the
Penalized Adaptive Method can reduce the forecasting error(RMSPE and MAPE)
around 24% to 50% comparing with the other forecasting methods.

Keywords:
SCAD penalty, propagation-separation, adaptive window choice, multiplier
bootstrap, bond risk premia

JEL Classification:
C4, C5, E4, G1

IRTG1792DP2020 001
Estimation and Determinants of Chinese Banks’ Total Factor Efficiency: A New
Vision Based on Unbalanced Development of Chinese Banks and Their Overall Risk

Shiyi Chen
Wolfgang K. Härdle
Li Wang

Abstract:
The paper estimates banks’ total factor efficiency (TFE) as well as TFE of each
production factor by incorporating banks’ overall risk endogenously into bank’s
production process as undesirable by-product in a Global-SMB Model. Our results
show that, compared with a model incorporated with banks’ overall risk, a model
considering only on-balance-sheet risk may over-estimate the integrated TFE
(TFIE) and under-estimate TFE volatility. Significant heterogeneities of bank
TFIE and TFE of each production factor exist among banks of different types and
regions, as a result of still prominent unbalanced development of Chinese
commercial banks. Based on the estimated TFIE, the paper further investigates
the determinants of bank efficiency, and finds that shadow banking, bank size,
NPL ratio, loan to deposit ratio, fiscal surplus to GDP ratio and banking sector
concentration are significant determinants of bank efficiency. Besides, a model
with risk-weighted assets as undesirable outputs can better capture the impact
of shadow banking involvement.

Keywords:
Nonparametric Methods, Commercial Banks, Shadow Bank, Financial Risk

JEL Classification:
C00

IRTG1792DP2020 002
Service Data Analytics and Business Intelligence

Desheng Dang Wu
Wolfgang Karl Härdle

Abstract:
With growing economic globalization, the modern service sector is in great need
of business intelligence for data analytics and computational statistics. The
joint application of big data analytics, computational statistics and business
intelligence has great potential to make the engineering of advanced service
systems more efficient. The purpose of this COST issue is to publish high-
quality research papers (including reviews) that address the challenges of
service data analytics with business intelligence in the face of uncertainty and
risk. High quality contributions that are not yet published or that are not
under review by other journals or peer-reviewed conferences have been collected.
The resulting topic oriented special issue includes research on business
intelligence and computational statistics, data-driven financial engineering,
service data analytics and algorithms for optimizing the business engineering.
It also covers implementation issues of managing the service process,
computational statistics for risk analysis and novel theoretical and
computational models, data mining algorithms for risk management related
business applications.

Keywords:
Data Analytics, Business Intelligence Systems

JEL Classification:
C00

IRTG1792DP2020 003
Structured climate financing: valuation of CDOs on inhomogeneous asset pools

Natalie Packham

Abstract:
Recently, a number of structured funds have emerged as public-private
partnerships with the intent of promoting investment in renewable energy in
emerging markets. These funds seek to attract institutional investors by
tranching the asset pool and issuing senior notes with a high credit quality.
Financing of renewable energy (RE) projects is achieved via two channels: small
RE projects are financed indirectly through local banks that draw loans from the
fund’s assets, whereas large RE projects are directly financed from the fund. In
a bottom-up Gaussian copula framework, we examine the diversification properties
and RE exposure of the senior tranche. To this end, we introduce the LH++ model,
which combines a homogeneous infinitely granular loan portfolio with a finite
number of large loans. Using expected tranche percentage notional (which takes a
similar role as the default probability of a loan), tranche prices and tranche
sensitivities in RE loans, we analyse the risk profile of the senior tranche. We
show how the mix of indirect and direct RE investments in the asset pool affects
the sensitivity of the senior tranche to RE investments and how to balance a
desired sensitivity with a target credit quality and target tranche size.

Keywords:
Renewable energy financing, structured finance, CDO pricing, LH++ model

JEL Classification:
C61, G13, G32

IRTG1792DP2020 004
Factorisable Multitask Quantile Regression

Shih-Kang Chao
Wolfgang K. Härdle
Ming Yuan

Abstract:
A multivariate quantile regression model with a factor structure is proposed to
study data with many responses of interest. The factor structure is allowed to
vary with the quantile levels, which makes our framework more flexible than the
classical factor models. The model is estimated with the nuclear norm
regularization in order to accommodate the high dimensionality of data, but the
incurred optimization problem can only be efficiently solved in an approximate
manner by off-the-shelf optimization methods. Such a scenario is often seen when
the empirical risk is non-smooth or the numerical procedure involves expensive
subroutines such as singular value decompo- sition. To ensure that the
approximate estimator accurately estimates the model, non-asymptotic bounds on
error of the the approximate estimator is established. For implementation, a
numerical procedure that provably marginalizes the approximate error is
proposed. The merits of our model and the proposed numerical procedures are
demonstrated through Monte Carlo experiments and an application to finance
involving a large pool of asset returns.

Keywords:
Factor model, quantile regression, non-asymptotic analysis, multivariate
regression, nuclear norm regularization

JEL Classification:
C13, C38, C61, G17

IRTG1792DP2020 005
Targeting Cutsomers Under Response-Dependent Costs

Johannes Haupt
Stefan Lessmann

Abstract:
This study provides a formal analysis of the customer targeting decision problem
in settings where the cost for marketing action is stochastic and proposes a
framework to efficiently estimate the decision variables for campaign profit
optimization. Targeting a customer is profitable if the positive impact of the
marketing treatment on the customer and the associated profit to the company is
higher than the cost of the treatment. While there is a growing literature on
developing causal or uplift models to identify the customers who are impacted
most strongly by the marketing action, no research has investigated optimal
targeting when the costs of the action are uncertain at the time of the
targeting decision. Because marketing incentives are routinely conditioned on a
positive response by the customer, e.g. a purchase or contract renewal,
stochastic costs are ubiquitous in direct marketing and customer retention
campaigns. This study makes two contributions to the literature, which are
evaluated on a coupon targeting campaign in an e-commerce setting. First, the
authors formally analyze the targeting decision problem under response-dependent
costs. Profit-optimal targeting requires an estimate of the treatment effect on
the customer and an estimate of the customer response probability under
treatment. The empirical results demonstrate that the consideration of treatment
cost substantially increases campaign profit when used for customer targeting in
combination with the estimation of the average or customer- level treatment
effect. Second, the authors propose a framework to jointly estimate the
treatment effect and the response probability combining methods for causal
inference with a hurdle mixture model. The proposed causal hurdle model achieves
competitive campaign profit while streamlining model building. The code for the
empirical analysis is available on Github.

Keywords:
Heterogeneous Treatment Effect, Uplift Modeling, Coupon Targeting,
Churn/Retention, Campaign Profit

JEL Classification:
C00

IRTG1792DP2020 006
Forex exchange rate forecasting using deep recurrent neural networks

Alexander Jakob Dautel
Wolfgang Karl Härdle
Stefan Lessmann
Hsin‐Vonn Seow

Abstract:
Deep learning has substantially advanced the state of the art in computer
vision, natural language processing, and other fields. The paper examines the
potential of deep learning for exchange rate forecasting. We systematically
compare long short- term memory networks and gated recurrent units to
traditional recurrent network architectures as well as feedforward networks in
terms of their directional forecasting accuracy and the profitability of trading
model predictions. Empirical results indicate the suitability of deep networks
for exchange rate forecasting in general but also evidence the difficulty of
implementing and tuning corresponding architectures. Especially with regard to
trading profit, a simpler neural network may perform as well as if not better
than a more complex deep neural network.

Keywords:
Deep learning, Financial time series forecasting, Recurrent neural networks,
Foreign exchange rates

JEL Classification:
C14, C22, C45

IRTG1792DP2020 007
Deep Learning application for fraud detection in financial statements

Patricia Craja
Alisa Kim
Stefan Lessmann

Abstract:
Financial statement fraud is an area of significant consternation for potential
investors, auditing companies, and state regulators. Intelligent systems
facilitate detecting financial statement fraud and assist the decision-making of
relevant stakeholders. Previous research detected instances in which financial
statements have been fraudulently misrepresented in managerial comments. The
paper aims to investigate whether it is possible to develop an enhanced system
for detecting financial fraud through the combination of information sourced
from financial ratios and managerial comments within corporate annual reports.
We employ a hierarchical attention network (HAN) with a long short-term memory
(LSTM) encoder to extract text features from the Management Discussion and
Analysis (MD&A) section of annual reports. The model is designed to offer two
distinct features. First, it reflects the structured hierarchy of documents,
which previous models were unable to capture. Second, the model embodies two
different attention mechanisms at the word and sentence level, which allows
content to be differentiated in terms of its importance in the process of
constructing the document representation. As a result of its architecture, the
model captures both content and context of managerial comments, which serve as
supplementary predictors to financial ratios in the detection of fraudulent
reporting. Additionally, the model provides interpretable indicators denoted as
“red-flag” sentences, which assist stakeholders in their process of determining
whether further investigation of a specific annual report is required. Empirical
results demonstrate that textual features of MD&A sections extracted by HAN
yield promising classification results and substantially reinforce financial
ratios.

Keywords:
fraud detection, financial statements, deep learning, text analytics

JEL Classification:
C00

IRTG1792DP2020 008
Simultaneous Inference of the Partially Linear Model with a Multivariate Unknown
Function

Kun Ho Kim
Shih-Kang Chao
Wolfgang K. Härdle

Abstract:
In this paper, we conduct simultaneous inference of the non-parametric part of a
partially linear model when the non-parametric component is a multivariate
unknown function. Based on semi-parametric estimates of the model, we construct
a simultaneous confidence region of the multivariate function for simultaneous
inference. The developed methodology is applied to perform simultaneous
inference for the U.S. gasoline demand where the income and price variables are
contaminated by Berkson errors. The empirical results strongly suggest that the
linearity of the U.S. gasoline demand is rejected. The results are also used to
propose an alternative form for the demand.

Keywords:
Simultaneous inference, Multivariate function, Simultaneous confidence region,
Berkson error, Regression calibration

JEL Classification:
C12, C13, C14

IRTG1792DP2020 009
CRIX an Index for cryptocurrencies

Simon Trimborn
Wolfgang Karl Härdle

Abstract:
The cryptocurrency market is unique on many levels: Very volatile, frequently
changing market structure, emerging and vanishing of cryptocurrencies on a daily
level. Following its development became a difficult task with the success of
cryptocurrencies (CCs) other than Bitcoin. For fiat currency markets, the IMF
offers the index SDR and, prior to the EUR, the ECU existed, which was an index
representing the development of European currencies. Index providers decide on a
fixed number of index constituents which will represent the market segment. It
is a challenge to fix a number and develop rules for the constituents in view of
the market changes. In the frequently changing CC market, this challenge is even
more severe. A method relying on the AIC is proposed to quickly react to market
changes and therefore enable us to create an index, referred to as CRIX, for the
cryptocurrency market. CRIX is chosen by model selection such that it represents
the market well to enable each interested party studying economic questions in
this market and to invest into the market. The diversified nature of the CC
market makes the inclusion of altcoins in the index product critical to improve
tracking performance. We have shown that assigning optimal weights to altcoins
helps to reduce the tracking errors of a CC portfolio, despite the fact that
their market cap is much smaller relative to Bitcoin. The codes used here are
available via www.quantlet.de.

Keywords:
Index construction, Model selection, Bitcoin, Cryptocurrency, CRIX, Altcoin

JEL Classification:
C51, C52, G10

IRTG1792DP2020 010
Kernel Estimation: the Equivalent Spline Smoothing Method

Wolfgang K. Härdle
Michael Nussbaum

Abstract:
Among nonparametric smoothers, there is a well-known correspondence between
kernel and Fourier series methods, pivoted by the Fourier transform of the
kernel. This suggests a similar relationship between kernel and spline
estimators. A known special case is the result of Silverman (1984) on the
effective kernel for the classical Reinsch-Schoenberg smoothing spline in the
nonparametric regression model. We present an extension by showing that a large
class of kernel estimators have a spline equivalent, in the sense of identical
asymptotic local behaviour of the weighting coefficients. This general class of
spline smoothers includes also the minimax linear estimator over Sobolev
ellipsoids. The analysis is carried out for piecewise linear splines and
equidistant design.

Keywords:
Kernel estimator, spline smoothing, filtering coefficients, differential
operator, Green's function approximation, asymptotic minimax spline

JEL Classification:
C00

IRTG1792DP2020 011
The Effect of Control Measures on COVID-19 Transmission and Work Resumption:
International Evidence

Lina Meng
Yinggang Zhou
Ruige Zhang
Zhen Ye
Senmao Xia
Giovanni Cerulli
Carter Casady
Wolfgang K. Härdle

Abstract:
Many countries have taken non-pharmaceutical interventions (NPIs) to contain the
spread of the coronavirus (COVID-19) and push the recovery of national
economies. This paper investigates the effect of these control measures by
comparing five selected countries, China, Italy, Germany, the United Kingdom,
and the United States. There is evidence that the degree of early intervention
and efficacy of control measures are essential to contain the pandemic. China
stands out because its early and strictly enforced interventions are effective
to contain the virus spread. Furthermore, we quantify the causal effect of
different control measures on COVID-19 transmission and work resumption in
China. Surprisingly, digital contact tracing and delegating clear responsibility
to the local community appear to be the two most effective policy measures for
disease containment and work resumption. Public information campaigns and social
distancing also help to flatten the peak significantly. Moreover, material
logistics that prevent medical supply shortages provide an additional
conditioning factor for disease containment and work resumption. Fiscal policy,
however, is less effective at the early to middle stage of the pandemic.

Keywords:
COVID-19, coronavirus

JEL Classification:
C00

IRTG1792DP2020 012
On Cointegration and Cryptocurrency Dynamics

Georg Keilbar
Yanfen Zhang

Abstract:
This paper aims to model the joint dynamics of cryptocurrencies in a
nonstationary setting. In particular, we analyze the role of cointegration
relationships within a large system of cryptocurrencies in a vector error
correction model (VECM) framework. To enable analysis in a dynamic setting, we
propose the COINtensity VECM, a nonlinear VECM specification accounting for a
varying systemwide cointegration exposure. Our results show that
cryptocurrencies are indeed cointegrated with a cointegration rank of four. We
also find that all currencies are affected by these long term equilibrium
relations. A simple statistical arbitrage trading strategy is proposed showing a
great in-sample performance.

Keywords:
Cointegration, VECM, Nonstationarity, Cryptocurrencies

JEL Classification:
C00

IRTG1792DP2020 013
A Machine Learning Based Regulatory Risk Index for Cryptocurrencies

Xinwen Ni
Wolfgang Karl Härdle
Taojun Xie

Abstract:
Cryptocurrencies’ values often respond aggressively to major policy changes, but
none of the existing indices informs on the market risks associated with
regulatory changes. In this paper, we quantify the risks originating from new
regulations on FinTech and cryptocurrencies (CCs), and analyse their impact on
market dynamics. Specifically, a Cryptocurrency Regulatory Risk IndeX (CRRIX) is
constructed based on policy-related news coverage frequency. The unlabeled news
data are collected from the top online CC news platforms and further classified
using a Latent Dirichlet Allocation model and Hellinger distance. Our results
show that the machine-learning-based CRRIX successfully captures major policy-
changing moments. The movements for both the VCRIX, a market volatility index,
and the CRRIX are synchronous, meaning that the CRRIX could be helpful for all
participants in the cryptocurrency market. The algorithms and Python code are
available for research purposes on www.quantlet.de.

Keywords:
Cryptocurrency, Regulatory Risk, Index, LDA, News Classification

JEL Classification:
C45, G11, G18

IRTG1792DP2020 014
Cross-Fitting and Averaging for Machine Learning Estimation of Heterogeneous
Treatment Effects

Daniel Jacob

Abstract:
We investigate the finite sample performance of sample splitting, cross-fitting
and averaging for the estimation of the conditional average treatment effect.
Recently proposed methods, so-called meta- learners, make use of machine
learning to estimate different nuisance functions and hence allow for fewer
restrictions on the underlying structure of the data. To limit a potential
overfitting bias that may result when using machine learning methods, cross-
fitting estimators have been proposed. This includes the splitting of the data
in different folds to reduce bias and averaging over folds to restore
efficiency. To the best of our knowledge, it is not yet clear how exactly the
data should be split and averaged. We employ a Monte Carlo study with different
data generation processes and consider twelve different estimators that vary in
sample-splitting, cross-fitting and averaging procedures. We investigate the
performance of each estimator independently on four different meta-learners: the
doubly-robust-learner, R-learner, T-learner and X-learner. We find that the
performance of all meta-learners heavily depends on the procedure of splitting
and averaging. The best performance in terms of mean squared error (MSE) among
the sample split estimators can be achieved when applying cross-fitting plus
taking the median over multiple different sample-splitting iterations. Some
meta-learners exhibit a high variance when the lasso is included in the ML
methods. Excluding the lasso decreases the variance and leads to robust and at
least competitive results.

Keywords:
causal inference, sample splitting, cross-fitting, sample averaging, machine
learning, simulation study

JEL Classification:
C01, C14, C31, C63

SFB 649
SFB 649 Abstracts
Dieser Ordner hat zur Zeit keinen Inhalt.

IRTG1792DP2020 015
Tail-risk protection: Machine Learning meets modern Econometrics

Bruno Spilak
Wolfgang Karl Härdle

Abstract:
Tail risk protection is in the focus of the financial industry and requires
solid mathematical and statistical tools, especially when a trading strategy is
derived. Recent hype driven by machine learning (ML) mechanisms has raised the
necessity to display and understand the functionality of ML tools. In this
paper, we present a dynamic tail risk protection strategy that targets a maximum
predefined level of risk measured by Value-At-Risk while controlling for
participation in bull market regimes. We propose different weak classifiers,
parametric and non-parametric, that estimate the exceedance probability of the
risk level from which we derive trading signals in order to hedge tail events.
We then compare the different approaches both with statistical and trading
strategy performance, finally we propose an ensemble classifier that produces a
meta tail risk protection strategy improving both generalization and trading
performance.

Keywords:
-

JEL Classification:
C00

IRTG1792DP2020 025
Non-Parametric Estimation of Spot Covariance Matrix with High-Frequency Data

Konul Mustafayeva
Weining Wang

Abstract:
Estimating spot covariance is an important issue to study, especially with the
increasing availability of high-frequency nancial data. We study the estimation
of spot covariance using a kernel method for high-frequency data. In particular,
we consider rst the kernel weighted version of realized covariance estimator
for the price process governed by a continuous multivariate semimartingale.
Next, we extend it to the threshold kernel estimator of the spot covariances
when the underlying price process is a discontinuous multivariate semimartingale
with nite activity jumps. We derive the asymptotic distribution of the
estimators for both xed and shrinking bandwidth. The estimator in a setting
with jumps has the same rate of convergence as the estimator for di usion
processes without jumps. A simulation study examines the nite sample properties
of the estimators. In addition, we study an application of the estimator in the
context of covariance forecasting. We discover that the forecasting model with
our estimator outperforms a benchmark model in the literature.

Keywords:
high-frequency data; kernel estimation; jump; forecasting covariance matrix

JEL Classification:
C00

IRTG1792DP2020 024
Dynamic Spatial Network Quantile Autoregression

Xiu Xu
Weining Wang
Yongcheol Shin

Abstract:
This paper proposes a dynamic spatial autoregressive quantile model. Using
predetermined network information, we study dynamic tail event driven risk using
a system of conditional quantile equations. Extending Zhu, Wang, Wang and Härdle
(2019), we allow the contemporaneous dependency of nodal responses by
incorporating a spatial lag in our model. For example, this is to allow a firm’s
tail behavior to be connected with a weighted aggregation of the simultaneous
returns of the other firms. In addition, we control for the common factor
effects. The instrumental variable quantile regressive method is used for our
model estimation, and the associated asymptotic theory for estimation is also
provided. Simulation results show that our model performs well at various
quantile levels with different network structures, especially when the node size
increases. Finally, we illustrate our method with an empirical study. We uncover
significant network effects in the spatial lag among financial institutions.

Keywords:
Network, Quantile autoregression, Instrumental variables, Dynamic models

JEL Classification:
C32, C51, G17

IRTG1792DP2020 023
The common and speci fic components of infl ation expectation across European
countries

Shi Chen
Wolfgang Karl Härdle
Weining Wang

Abstract:
Inflation expectation (IE) is often considered to be an important determinant of
actual inflation in modern economic theory, we are interested in investigating
the main risk factors that determine its dynamics. We fiirst apply a joint
arbitrage-free term structure model across different European countries to
obtain estimate for country-specific IE. Then we use the two-component and
three-component models to capture the main risk factors. We discover that the
extracted common trend for IE is an important driver for each country of
interest. Moreover a spatial-temporal copula model is tted to account for the
non-Gaussian dependency across countries. This paper aims to extract informative
estimates for IE and provide good implications for monetary policies.

Keywords:
in ation expectation; joint yield-curve modeling; factor model; common trend;
spatial-temporal copulas

JEL Classification:
C02, C13, C38, E31, E43

IRTG1792DP2020 022
Tail Event Driven Factor Augmented Dynamic Model

Weining Wang
Lining Yu
Bingling Wang

Abstract:
A factor augmented dynamic model for analysing tail behaviour of high
dimensional time series is proposed. As a first step, the tail event driven
latent factors are extracted. In the second step, a VAR (Vectorautoregression
model) is carried out to analyse the interaction between these factors and the
macroeconomic variables. Furthermore, this methodology also provides the
possibility for central banks to examine the sensitivity between macroeconomic
variables and financial shocks via impulse response analysis. Then the
predictability of our estimator is illustrated. Finally, forecast error variance
decomposition is carried out to investigate the network effect of these
variables. The interesting findings are: firstly, GDP and Unemployment rate are
very much sensitive to the shock of financial tail event driven factors, while
these factors are more affected by inflation and short term interest rate.
Secondly, financial tail event driven factors play important roles in the
network constructed by the extracted factors and the macroeconomic variables.
Thirdly, there is more connectedness during financial crisis than in the stable
periods. Compared with median case, the network is more dense in lower quantile
level.

Keywords:
Quantile Regression, Expectile Regression, Dynamic Factor Model, Dynamic Network

JEL Classification:
C21, C51, G01, G18, G32, G38

IRTG1792DP2020 021
Improved Estimation of Dynamic Models of Conditional Means and Variances

Weining Wang
Jeffrey M. Wooldridge
Mengshan Xu

Abstract:
Modelling dynamic conditional heteroscedasticity is the daily routine in time
series econometrics. We propose a weighted conditional moment estimation to
potentially improve the eciency of the QMLE (quasi maximum likelihood
estimation). The weights of conditional moments are selected based on the
analytical form of optimal instruments, and we nominally decide the optimal
instrument based on the third and fourth moments of the underlying error term.
This approach is motivated by the idea of general estimation equations (GEE). We
also provide an analysis of the eciency of QMLE for the location and variance
parameters. Simulations and applications are conducted to show the better
performance of our estimators.

Keywords:
-

JEL Classification:
C00

IRTG1792DP2020 020
Long- and Short-Run Components of Factor Betas: Implications for Stock Pricing

Hossein Asgharian
Charlotte Christiansen
Ai Jun Hou
Weining Wang

Abstract:
We propose a bivariate component GARCH-MIDAS model to estimate the long- and
short-run components of the variances and covariances. The advantage of our
model to the existing DCC-based models is that it uses the same form for both
the variances and covariances and that it estimates these moments
simultaneously. We apply this model to obtain long- and short-run factor betas
for industry test portfolios, where the risk factors are the market, SMB, and
HML portfolios. We use these betas in cross-sectional analysis of the risk
premia. Among other things, we find that the risk premium related to the short-
run market beta is significantly positive, irrespective of the choice of test
portfolio. Further, the risk premia for the short-run betas of all the risk
factors are significant outside recessions.

Keywords:
long-run betas; short-run betas; risk premia; business cycles; component GARCH
model; MIDAS

JEL Classification:
G12; C58; C51

IRTG1792DP2020 019
Inference of breakpoints in high-dimensional time series

Likai Chen
Weining Wang
Wei Biao Wu

Abstract:
For multiple change-points detection of high-dimensional time series, we provide
asymptotic theory concerning the consistency and the asymptotic distribution of
the breakpoint statistics and estimated break sizes. The theory backs up a
simple two- step procedure for detecting and estimating multiple change-points.
The proposed two-step procedure involves the maximum of a MOSUM (moving sum)
type statistics in the rst step and a CUSUM (cumulative sum) re nement step on
an aggregated time series in the second step. Thus, for a xed time-point, we
can capture both the biggest break across di erent coordinates and aggregating
simultaneous breaks over multiple coordinates. Extending the existing high-
dimensional Gaussian approxima- tion theorem to dependent data with jumps, the
theory allows us to characterize the size and power of our multiple change-point
test asymptotically. Moreover, we can make inferences on the breakpoints
estimates when the break sizes are small. Our theoretical setup incorporates
both weak temporal and strong or weak cross-sectional dependence and is suitable
for heavy-tailed innovations. A robust long-run covariance matrix estimation is
proposed, which can be of independent interest. An application on detecting
structural changes of the U.S. unemployment rate is considered to illus- trate
the usefulness of our method.

Keywords:
multiple change points detection; temporal and cross-sectional dependence;
Gaussian approximation; inference of break locations

JEL Classification:
C00

IRTG1792DP2020 018
A supreme test for periodic explosive GARCH

Stefan Richter
Weining Wang
Wei Biao Wu

Abstract:
We develop a uniform test for detecting and dating explosive behavior of a
strictly stationary GARCH(r, s) (generalized autoregressive conditional
heteroskedasticity) process. Namely, we test the null hypothesis of a globally
stable GARCH process with constant parameters against an alternative where there
is an ’abnormal’ period with changed parameter values. During this period, the
change may lead to an explosive behavior of the volatility process. It is
assumed that both the magnitude and the timing of the breaks are unknown. We
develop a double supreme test for the existence of a break, and then provide an
algorithm to identify the period of change. Our theoretical results hold under
mild moment assumptions on the innovations of the GARCH process. Technically,
the existing properties for the QMLE in the GARCH model need to be
reinvestigated to hold uniformly over all possible periods of change. The key
results involve a uniform weak Bahadur representation for the estimated
parameters, which leads to weak convergence of the test statistic to the supreme
of a Gaussian Process. In simulations we show that the test has good size and
power for reasonably large time series lengths. We apply the test to Apple asset
returns and Bitcoin returns.

Keywords:
GARCH, IGARCH, Change-point Analysis, Concentration Inequalities, Uniform Test

JEL Classification:
C00

IRTG1792DP2020 017
Using generalized estimating equations to estimate nonlinear models with
spatial data

Cuicui Lu
Weining Wang
Jeffrey M. Wooldridge

Abstract:
In this paper, we study estimation of nonlinear models with cross sectional data
using two-step generalized estimating equations (GEE) in the quasi-maximum
likelihood estimation (QMLE) framework. In the interest of improving efficiency,
we propose a grouping estimator to account for the potential spatial correlation
in the underlying innovations. We use a Poisson model and a Negative Binomial II
model for count data and a Probit model for binary response data to demonstrate
the GEE procedure. Under mild weak dependency assumptions, results on estimation
consistency and asymptotic normality are provided. Monte Carlo simulations show
efficiency gain of our approach in comparison of different estimation methods
for count data and binary response data. Finally we apply the GEE approach to
study the determinants of the inflow foreign direct investment (FDI) to China.

Keywords:
quasi-maximum likelihood estimation; generalized estimating equations; nonlinear
models; spatial dependence; count data; binary response data; FDI equation

JEL Classification:
C13, C21, C35, C51

IRTG1792DP2020 026
Data Analytics Driven Controlling: bridging statistical modeling and managerial
intuition

Kainat Khowaja
Danial Saef
Sergej Sizov
Wolfgang Karl Härdle

Abstract:
Strategic planning in a corporate environment is often based on experience and
intuition, although internal data is usually available and can be a valuable
source of information. Predicting merger & acquisition (M&A) events is at the
heart of strategic management, yet not sufficiently motivated by data
analytics driven controlling. One of the main obstacles in using e.g. count data
time series for M&A seems to be the fact that the intensity of M&A is time
varying at least in certain business sectors, e.g. communications. We propose a
new automatic procedure to bridge this obstacle using novel statistical methods.
The proposed approach allows for a selection of adaptive windows in count data
sets by detecting significant changes in the intensity of events. We test the
efficacy of the proposed method on a simulated count data set and put it into
action on various M&A data sets. It is robust to aberrant behaviour and
generates accurate forecasts for the evaluated business sectors. It also
provides guidance for an a-priori selection of fixed windows for forecasting.
Furthermore, it can be generalized to other business lines, e.g. for managing
supply chains, sales forecasts, or call center arrivals, thus giving managers
new ways for incorporating statistical modeling in strategic planning decisions.

Keywords:
-

JEL Classification:
C00

IRTG1792DP2020 028
Tail Risk Network Effects in the Cryptocurrency Market during the COVID-19
Crisis

Rui Ren
Michael Althof
Wolfgang Karl Härdle

Abstract:
Cryptocurrencies are gaining momentum in investor attention, are about to become
a new asset class, and may provide a hedging alternative against the risk of
devaluation of fiat currencies following the COVID-19 crisis. In order to
provide a thorough understanding of this new asset class, risk indicators need
to consider tail risk behaviour and the interdependencies between the
cryptocurrencies not only for risk management but also for portfolio
optimization. The tail risk network analysis framework proposed in the paper is
able to identify individual risk characteristics and capture spillover effect in
a network topology. Finally we construct tail event sensitive portfolios and
consequently test the performance during an unforeseen COVID-19 pandemic.

Keywords:
Cryptocurrencies, Network Dynamics, Portfolio Optimization, Quantile Regression,
Systemic Risk, Financial Risk Meter

JEL Classification:
C00

IRTG1792DP2021 001
Surrogate Models for Optimization of Dynamical Systems Kainat Khowaja Mykhaylo Shcherbatyy Wolfgang Karl Härdle Abstract: Surrogate models using a suitable orthogonal decomposition and radial basis functions have been proposed by many researchers to reduce the computational complexity of numerical solutions to optimization problems. However, these reduced-order models result in low accuracy, sometimes due to inappropriate initial sampling or the occurrence of optima at vertices. This paper provides an improved intelligent data-driven mechanism for constructing low-dimensional surrogate models using alternative memory-based sampling strategies in an iterative algorithm. Furthermore, the application of surrogate models to optimal control problems is extended.
It is shown that surrogate models with Latin hypercube sampling dominate variable-order methods in optimization computation time while maintaining accuracy. They are also shown to be robust to nonlinearities in the model. Therefore, these computationally efficient predictive surrogate models are applicable in various fields, especially for solving inverse problems and optimal control problems, some examples of which are shown in this paper. Keywords: Proper Orthogonal Decomposition, SVD, Radial Basis Functions, Optimization, Surrogate Models, Smart Data Analytics, Parameter Estimation JEL Classification: C00

IRTG1792DP2021 002
FRM Financial Risk Meter for Emerging Markets

Souhir Ben Amor
Michael Althof
Wolfgang Karl Härdle

Abstract:
The fast-growing Emerging Market (EM) economies and their improved transparency
and liquidity have attracted international investors. However, the external
price shocks can result in a higher level of volatility as well as domestic
policy instability. Therefore, an efficient risk measure and hedging strategies
are needed to help investors protect their investments against this risk. In
this paper, a daily systemic risk measure, called FRM (Financial Risk Meter) is
proposed. The FRM@ EM is applied to capture systemic risk behavior embedded in
the returns of the 25 largest EMs’ FIs, covering the BRIMST (Brazil, Russia,
India, Mexico, South Africa, and Turkey), and thereby reflects the financial
linkages between these economies. Concerning the Macro factors, in addition to
the Adrian & Brunnermeier (2016) Macro, we include the EM sovereign yield spread
over respective US Treasuries and the above-mentioned countries’ currencies. The
results indicated that the FRM of EMs’ FIs reached its maximum during the US
financial crisis following by COVID 19 crisis and the Macro factors explain the
BRIMST’ FIs with various degrees of sensibility. We then study the relationship
between those factors and the tail event network behavior to build our policy
recommendations to help the investors to choose the suitable market for
investment and tail-event optimized portfolios. For that purpose, an overlapping
region between portfolio optimization strategies and FRM network centrality is
developed. We propose a robust and well-diversified tail-event and cluster risk-
sensitive portfolio allocation model and compare it to more classical
approaches.

Keywords:
FRM (Financial Risk Meter), Lasso Quantile Regression, Network Dynamics,
Emerging Markets, Hierarchical Risk Parity

JEL Classification:
C30, C58, G11, G15, G21

IRTG1792DP2021-003
K-expectiles clustering

Bingling Wang
Yingxing Li
Wolfgang Karl Härdle

Abstract:
K-means clustering is one of the most widely-used partitioning algorithm in
cluster analysis due to its simplicity and computational efficiency, but it may
not provide ideal clustering results when applying to data with non-spherically
shaped clusters. By considering the asymmetrically weighted distance, We propose
the K-expectile clustering and search the clusters via a greedy algorithm that
minimizes the within cluster τ-variance. We provide algorithms based on two
schemes: the fixed τ clustering, and the adaptive τ clustering. Validated by
simulation results, our method has enhanced performance on data with asymmetric
shaped clusters or clusters with a complicated structure. Applications of our
method show that the fixed τ clustering can bring some flexibility on
segmentation with a decent accuracy, while the adaptive τ clustering may yield
better performance. All calculation can be redone via quantlet.com.

Keywords:
clustering, expectiles, asymmetric quadratic loss, image segmentation

JEL Classification:
C00

IRTG1792DP2021 004
Understanding Smart Contracts: Hype or Hope?

Elizaveta Zinovyev
Raphael C. G. Reule
Wolfgang Karl Härdle

Abstract:
Smart Contracts are commonly considered to be an important component or even a
key to many business solutions in an immense variety of sectors and promises to
securely increase their individual efficiency in an ever more digitized
environment. Introduced in the early 1990’s, the technology has gained a lot of
attention with its application to blockchain technology to an extent, that can
be considered a veritable hype. Reflecting the growing institutional interest,
this intertwined exploratory study between statistics, information technology,
and law contrasts these idealistic stories with the data reality and provides a
mandatory step of understanding the matter, before any further relevant
applications are discussed as being “factually” able to replace traditional
constructions. Besides fundamental flaws and application difficulties of
currently employed Smart Contracts, the technological drive and enthusiasm
backing it may however serve as a jump-off board for future developments
thrusting well in the presently unshakeable traditional structures.

Keywords:
Cryptocurrency, Smart Contract, Ethereum, CRIX

JEL Classification:
G02, G11, G12, G14, G15, G23

IRTG1792DP2021 005
CATE Meets ML: Conditional Average Treatment Effect and Machine Learning

Daniel Jacob

Abstract:
For treatment effects - one of the core issues in modern econometric analysis -
prediction and estimation are flip-sides of the same coin. As it turns out,
machine learning methods are the tool for generalized prediction models.
Combined with econometric theory allows us to estimate not only the average but
a personalized treatment effect - the conditional average treatment effect
(CATE). In this tutorial, we give an overview of novel methods, explain them in
detail, and apply them via Quantlets in real data applications. We study the
effect that microcredit availability has on the amount of money borrowed and if
the 401(k) pension plan eligibility has an impact on net financial assets, as
two empirical examples. The presented toolbox of methods contains meta-
learners, like the Doubly-Robust, the R-, T- and X-learner, and methods that are
specially designed to estimate the CATE like the causal BART and the generalized
random forest. In both, the microcredit and the 401(k) example, we find a
positive treatment effect for all observations but diverse evidence of treatment
effect heterogeneity. An additional simulation study, where the true treatment
effect is known, allows us to compare the different methods and to observe
patterns and similarities.

Keywords:
Causal Inference, CATE, Machine Learning, Tutorial

JEL Classification:
C00

IRTG1792DP2021 006
Coins with benefits: on existence, pricing kernel and risk premium of
cryptocurrencies

Cathy Yi-Hsuan Chen
Dmitri Vinogradov

Abstract:
Cryptocurrencies come with benefits, such as anonymity of payments and positive
network effects of user adoption, and transaction risks including unconfirmed
transactions, hacks, and frauds. They compete with central-bank-regulated money
but consumers may prefer one currency over the other. In our arbitrage-free
world utility from consumption depends on benefits, which are governed by
distinct stochastic processes, implying incomplete markets and distinct pricing
kernels. We characterize the cryptocurrency kernels, evaluate the otherwise
unobservable benefits, and show their contribution to pricing. The model
explains both the co-existence of the two currencies and the high volatility of
the cryptocurrency price.

Keywords:
Bitcoin, cryptocurrency, pricing kernel, currency competition

JEL Classification:
A1, D0, E21, G12

IRTG1792DP2021 007
Rodeo or Ascot: which hat to wear at the crypto race?

Konstantin Häusler
Wolfgang Karl Härdle

Abstract:
This paper sheds light on the dynamics of the cryptocurrency (CC) sector. By
modeling its dynamics via a stochastic volatility with correlated jumps (SVCJ)
model in combination with several rolling windows, it is possible to capture the
extreme ups and downs of the CC market and to understand its dynamics. Through
this approach, we obtain time series for each parameter of the model. Even
though parameter estimates change over time and depend on the window size,
several recurring patterns are observable which are robust to changes of the
window size and supported by clustering of parameter estimates: during bullish
periods, volatility stabilizes at low levels and the size and volatility of
jumps in mean decreases. In bearish periods though, volatility increases and
takes longer to return to its long-run trend. Furthermore, jumps in mean and
jumps in volatility are independent. With the rise of the CC market in 2017, a
level shift of the volatility of volatility occurred.

Keywords:
Cryptocurrency, SVCJ, Market Dynamics, Stochastic Volatility

JEL Classification:
C51, C58, G15

IRTG1792DP2021 008
Financial Risk Meter based on Expectiles

Rui Ren
Meng-Jou Lu
Yingxing Li
Wolfgang Karl Härdle

Abstract:
The Financial Risk Meter (FRM) is an established mechanism that, based on
conditional Value at Risk (VaR) ideas, yields insight into the dynamics of
network risk. Originally, the FRM has been composed via Lasso based quantile
regression, but we here extend it by incorporating the idea of expectiles, thus
indicating not only the tail probability but rather the actual tail loss given a
stress situation in the network. The expectile variant of the FRM enjoys several
advantages: Firstly, the coherent and multivariate tail risk indicator
conditional expectile-based VaR (CoEVaR) can be derived, which is sensitive to
the magnitude of extreme losses. Next, FRM index is not restricted to an index
compared to the quantile based FRM mechanisms, but can be expanded to a set of
systemic tail risk indicators, which provide investors with numerous tools in
terms of diverse risk preferences. The power of FRM also lies in displaying FRM
distribution across various entities every day. Two distinct patterns can be
discovered under high stress and during stable periods from the empirical
results in the United States stock market. Furthermore, the framework is able to
identify individual risk characteristics and capture spillover effects in a
network.

Keywords:
expectiles, EVaR, CoEVaR, expectile lasso regression, network analysis, systemic
risk, Financial Risk Meter

JEL Classification:
C00

IRTG1792DP2021 009
Von den Mühen der Ebenen und der Berge in den Wissenschaften

Annette Vogt

Abstract:
Der Mathematiker und Statistiker E. J. Gumbel führte eine Doppelexistenz – als
Mathematiker und Statistiker von 1923 bis zu seiner Vertreibung 1932 an der
Universität Heidelberg und als politischer Autor. Auch im Exil in Frankreich
behielt er diese Doppeltätigkeit bei, verfasste mathematische Arbeiten und
publizierte Artikel gegen das NS-Regime in Exil-Zeitschriften. Sein Hauptwerk
„Statistics of Extremes“ erschien 1958 in New York (eine Reprint-Ausgabe 2013).
Die „Wiederentdeckung“ des „politischen Gumbel“ begann 2012 und fast zeitgleich
die „Wiederentdeckung“ des „mathematischen Gumbel“. Die Anwendungen der „Gumbel
Distribution“ und der Gumbel-Copula zur Modellierung stochastischer
Abhängigkeiten weckten das Interesse an der Person Gumbel und seinen Leistungen.
Im Artikel werden neue Forschungsergebnisse zu E. J. Gumbel vorgestellt.

Keywords:
Emil J. Gumbel – Mathematiker, Pazifist und politischer Autor

JEL Classification:
C00

IRTG1792DP2021 010
A Data-driven Explainable Case-based Reasoning Approach for Financial Risk
Detection

Wei Li
Florentina Paraschiv
Georgios Sermpinis

Abstract:
The rapid development of artificial intelligence methods contributes to their
wide applications for forecasting various financial risks in recent years. This
study introduces a novel explainable case-based reasoning (CBR) approach without
a requirement of rich expertise in financial risk. Compared with other black-box
algorithms, the explainable CBR system allows a natural economic interpretation
of results. Indeed, the empirical results emphasize the interpretability of the
CBR system in predicting financial risk, which is essential for both financial
companies and their customers. In addition, results show that the proposed
automatic design CBR system has a good prediction performance compared to other
artificial intelligence methods, overcoming the main drawback of a standard CBR
system of highly depending on prior domain knowledge about the corresponding
field.

Keywords:
Case-based reasoning, Financial risk detection, Multiple-criteria decision-
making, Feature scoring, Particle swarm optimization, Parallel computing

JEL Classification:
C51, C52, C53, C61, C63, D81, G21, G32

IRTG1792DP2021 011
Valuing cryptocurrencies: Three easy pieces

Michael C. Burda

Abstract:
This paper surveys the capacity of simple macroeconomic models — ”three easy
pieces” — to account for persistent and positive valuations of privately issued
assets based on the blockchain. Each of these three models — transactions demand
for a means of payment, consumption-based capital asset pricing, and search and
matching — highlights important aspects of digital payments. The mutual
interference of these jointly produced features may impede widespread use of
cryptocurrencies until technological innovations have been developed to separate
them.

JEL Classification:
C00

IRTG1792DP2021 012
Correlation scenarios and correlation stress testing

Natalie Packham
Fabian Woebbeking

Abstract:
We develop a general approach for stress testing correlations of financial asset
portfolios. The correlation matrix of asset returns is specified in a parametric
form, where correlations are represented as a function of risk factors, such as
country and industry factors. A sparse factor structure linking assets and risk
factors is built using Bayesian variable selection methods. Regular calibration
yields a joint distribution of economically meaningful stress scenarios of the
factors. As such, the method also lends itself as a reverse stress testing
framework: using the Mahalanobis distance or highest density regions (HDR) on
the joint risk factor distribution allows to infer worst-case correlation
scenarios. We give examples of stress tests on a large portfolio of European and
North American stocks.

Keywords:
Correlation stress testing, reverse stress testing, factor selection, scenario
selection, Bayesian variable selection, market risk management

JEL Classification:
G11, G32

IRTG1792DP2021 013
Penalized Weigted Competing Risks Models Based on Quantile Regression

Erqian Li
Wolfgang Karl Härdle
Xiaowen Dai
Maozai Tian

Abstract:
The proportional subdistribution hazards (PSH) model is popularly used to deal
with competing risks data. Censored quantile regression provides an important
supplement as well as variable selection methods, due to large numbers of
irrelevant covariates in practice. In this paper, we study variable selection
procedures based on penalized weighted quantile regression for competing risks
models, which is conveniently applied by researchers. Asymptotic properties of
the proposed estimators including consistency and asymptotic normality of non-
penalized estimator and consistency of variable selection are established. Monte
Carlo simulation studies are conducted, showing that the proposed methods are
considerably stable and efficient. A real data about bone marrow transplant
(BMT) is also analyzed to illustrate the application of proposed procedure.

Keywords:
Competing risks, Cumulative incidence function, Kaplan-Meier estimator,
Redistribution method

JEL Classification:
C00

IRTG1792DP2021 014
Indices on Cryptocurrencies: an Evaluation

Konstantin Häusler
Hongyu Xia

Abstract:
Several cryptocurrency (CC) indices track the dynamics of the rising CC sector,
and soon ETFs will be issued on them. We conduct a qualitative and quantitative
evaluation of the currently existing CC indices. As the CC sector is not yet
consolidated, index issuers face the challenge of tracking the dynamics of a
fast-growing sector that is under continuous transformation. We propose several
criteria and various measures to compare the indices under review. Major
differences between the indices lie in their weighting schemes, their coverage
of CCs and the number of constituents, the level of transparency, and thus their
accuracy in mapping the dynamics of the CC sector. Our analysis reveals that
indices that adapt dynamically to this rising sector outperform their
competitors. Interestingly, increasing the number of constituents does not
automatically lead to a better fit of the CC sector. All codes are available on
Quantlet.com

Keywords:
Cryptocurrency, Index, Market Dynamics, Bitcoin

JEL Classification:
C00

IRTG1792DP2019 007
Localizing Multivariate CAViaR

Yegor Klochkov
Wolfgang Karl Härdle
Xiu Xu

Abstract:
The risk transmission among financial markets is time-evolving, especially for
the extreme risk scenarios. The possibly sudden time variations of these risk
structures ask for quantitative technology that is able to cope with such
situations. Here we present a novel localized multivariate CAViaR-type model to
respond to the challenge of time-varying risk contagion. For this purpose a
local adaptive approach determines homogeneous intervals at each time point.
Critical values for this technique are calculated via multiplier bootstrap, and
the statistical properties of this ”localized multivariate CAViaR” are derived.
A comprehensive simulation study supports the effectiveness of our approach in
detecting structural change in multivariate CAViaR. Finally, when applying for
the US and German financial markets, we can trace out the dynamic tail risk
spillovers and find that the US market appears to play dominate role in risk
transmissions, especially in volatile market periods.

Keywords:
conditional quantile autoregression, local parametric approach, change point
detection, multiplier bootstrap

JEL Classification:
C32, C51, G17

IRTG1792DP2021 015
High-dimensional Statistical Learning Techniques for Time-varying Limit Order
Book Networks

Shi Chen
Wolfgang Karl Härdle
Melanie Schienle

Abstract:
This paper provides statistical learning techniques for determining the full
own-price market impact and the relevance and effect of cross-price and cross-
asset spillover channels from intraday transactions data. The novel tools allow
extracting comprehensive information contained in the limit order books (LOB)
and quantify their impacts on the size and structure of price interdependencies
across stocks. For correct empirical network determination of such dynamic
liquidity price e ects even in small portfolios, we require high-dimensional
statistical learning methods with an integrated general bootstrap procedure. We
document the importance of LOB liquidity network spillovers even for a small
blue-chip NASDAQ portfolio.

Keywords:
limit order book, high-dimensional statistical learning, liquidity networks,
high frequency dynamics, market impact, bootstrap, network

JEL Classification:
C02, C13, C22, C45, G12

IRTG1792DP2021 016
A Time-Varying Network for Cryptocurrencies

Li Guo
Wolfgang Karl Härdle
Yubo Tao

Abstract:
Cryptocurrencies return cross-predictability and technological similarity yield
information on risk propagation and market segmentation. To investigate these
effects, we build a time-varying network for cryptocurrencies, based on the
evolution of return cross-predictability and technological similarities. We
develop a dynamic covariate-assisted spectral clustering method to consistently
estimate the latent community structure of cryptocurrencies network that
accounts for both sets of information. We demonstrate that investors can achieve
better risk diversification by investing in cryptocurrencies from different
communities. A cross-sectional portfolio that implements an inter-crypto
momentum trading strategy earns a 1.08% daily return. By dissecting the
portfolio returns on behavioral factors, we con rm that our results are not
driven by behavioral mechanisms.

Keywords:
Community detection, Dynamic stochastic blockmodel, Covariates, Co-clustering,
Network risk, Momentum

JEL Classification:
C00

IRTG1792DP2020 016
A data-driven P-spline smoother and the P-Spline-GARCH models

Yuanhua Feng
Wolfgang Karl Härdle

Abstract:
Penalized spline smoothing of time series and its asymptotic properties are
studied. A data-driven algorithm for selecting the smoothing parameter is
developed. The proposal is applied to define a semiparametric extension of the
well-known Spline- GARCH, called a P-Spline-GARCH, based on the log-data
transformation of the squared returns. It is shown that now the errors process
is exponentially strong mixing with finite moments of all orders. Asymptotic
normality of the P-spline smoother in this context is proved. Practical
relevance of the proposal is illustrated by data examples and simulation. The
proposal is further applied to value at risk and expected shortfall.

Keywords:
P-spline smoother, smoothing parameter selection, P-Spline-GARCH, strong mixing,
value at risk, expected shortfall

JEL Classification:
C14, C51

IRTG1792DP2020 027
Blockchain mechanism and distributional characteristics of cryptos

Min-Bin Lin
Kainat Khowaja
Cathy Yi-Hsuan Chen
Wolfgang Karl Härdle

Abstract:
We investigate the relationship between underlying blockchain mechanism of
cryptocurrencies and its distributional characteristics. In addition to price,
we emphasise on using actual block size and block time as the operational
features of cryptos. We use distributional characteristics such as fourier power
spectrum, moments, quantiles, global we optimums, as well as the measures for
long term dependencies, risk and noise to summarise the information from crypto
time series. With the hypothesis that the blockchain structure explains the
distributional characteristics of cryptos, we use characteristic based spectral
clustering to cluster the selected cryptos into  five groups. We scrutinise
these clusters and  find that indeed, the clusters of cryptos share similar
mechanism such as origin of fork, difficulty adjustment frequency, and the
nature of block size. This paper provides crypto creators and users with a
better understanding toward the connection between the blockchain protocol
design and distributional characteristics of cryptos.

Keywords:
Cryptocurrency, price, blockchain mechanism, distributional characteristics,
clustering

JEL Classification:
C00

IRTG1792DP 018
Robustifying Markowitz

 

Wolfgang Karl Härdle

Yegor Klochkov 

Alla Petukhina 

Nikita Zhivotovskiy

 

Abstract

Markowitz mean-variance portfolios with sample mean and covariance as input parameters feature numerous issues in practice. They perform poorly out of sample due to estimation error, they experience extreme weights together with high sen- sitivity to change in input parameters. The heavy-tail characteristics of financial time series are in fact the cause for these erratic fluctuations of weights that conse- quently create substantial transaction costs. In robustifying the weights we present a toolbox for stabilizing costs and weights for global minimum Markowitz portfolios. Utilizing a projected gradient descent (PGD) technique, we avoid the estimation and inversion of the covariance operator as a whole and concentrate on robust estimation of the gradient descent increment. Using modern tools of robust statistics we con- struct a computationally efficient estimator with almost Gaussian properties based on median-of-means uniformly over weights. This robustified Markowitz approach is confirmed by empirical studies on equity markets. We demonstrate that robustified portfolios reach higher risk-adjusted performance and the lowest turnover compared to shrinkage based and constrained portfolios.

IRTG1792DP2021 017
Green financial development improving energy efficiency and economic growth: a
study of CPEC area in COVID-19 era

Linyun Zhang
Feiming Huang
Lu Lu
Xinwen Ni

Abstract:
This study seeks to evaluate the effect of green financial development,
improving energy efficiency and economic growth on Covid-19 tenure. For this,
the CPEC area is recommended to look into. Present study revealed the energy
economic negative repercussions of Covid-19 impacts. It is assumed that, in
China and Pakistan, economic expansion, trade openness, financial development,
and urbanization coexist. To verify the postulated impacts of economic activity
on the environment, we do Johansen cointegration, error correction, and Granger
causality tests. We discovered that economic growth, energy consumption, trade
openness, financial development, and urbanization had a long-term relationship
to CO2 emissions in Pakistan. Urbanization is the only macroeconomic factor with
a detrimental effect on carbon emissions. As with China, no cointegration is
found across variables, but unidirectional causality from energy consumption and
economic growth to economic growth is established. Economic growth, energy
consumption, and trade openness also each have bidirectional causal effect on
financial development. According to statistical data, along with significant
projected economic development in CPEC countries, policymakers and regulators
are urged to strengthen environmental protection laws in China and Pakistan.

Keywords:
Green financial development, Energy Financing, Energy Efficiency, Economic
growth, Covid-19 crises, Capital formation

JEL Classification:
C00

IRTG1792DP2021 018
Robustifying Markowitz

Wolfgang Karl Härdle
Yegor Klochkov
Alla Petukhina
Nikita Zhivotovskiy

Abstract:
Markowitz mean-variance portfolios with sample mean and covariance as input
parameters feature numerous issues in practice. They perform poorly out of
sample due to estimation error, they experience extreme weights together with
high sensitivity to change in input parameters. The heavy-tail characteristics
of  financial time series are in fact the cause for these erratic fluctuations
of weights that consequently create substantial transaction costs. In
robustifying the weights we present a toolbox for stabilizing costs and weights
for global minimum Markowitz portfolios. Utilizing a projected gradient descent
(PGD) technique, we avoid the estimation and inversion of the covariance
operator as a whole and concentrate on robust estimation of the gradient descent
increment. Using modern tools of robust statistics we construct a
computationally efficient estimator with almost Gaussian properties based on
median-of-means uniformly over weights. This robustified Markowitz approach is
confirmed by empirical studies on equity markets. We demonstrate that
robustified portfolios reach higher risk-adjusted performance and the lowest
turnover compared to shrinkage based and constrained portfolios.

Keywords:
.

JEL Classification:
C00

IRTG1792DP2021 019
Understanding jumps in high frequency digital asset markets

Danial Saef
Odett Nagy
Sergej Sizov
Wolfgang Karl Härdle

Abstract:
While attention is a predictor for digital asset prices, and jumps in Bitcoin
prices are well-known, we know little about its alternatives. Studying high
frequency crypto data gives us the unique possibility to confirm that cross
market digital asset returns are driven by high frequency jumps clustered around
black swan events, resembling volatility and trading volume seasonalities.
Regressions show that intra-day jumps significantly influence end of day returns
in size and direction. This provides fundamental research for crypto option
pricing models. However, we need better econometric methods for capturing the
specific market microstructure of cryptos. All calculations are reproducible via
the quantlet.com technology.

Keywords:
jumps, market microstructure noise, high frequency data, cryptocurrencies, CRIX,
option pricing

JEL Classification:
C00

IRTG1792DP2021 020
Advanced Statistical Learning on Short Term Load Process Forecasting

Junjie Hu
Brenda López Cabrera
Awdesch Melzer

Abstract:
Short Term Load Forecast (STLF) is necessary for effective scheduling, operation
optimization trading, and decision-making for electricity consumers. Modern and
efficient machine learning methods are recalled nowadays to manage complicated
structural big datasets, which are characterized by having a nonlinear temporal
dependence structure. We propose different statistical nonlinear models to
manage these challenges of hard type datasets and forecast 15-min frequency
electricity load up to 2-days ahead. We show that the Long-short Term Memory
(LSTM) and the Gated Recurrent Unit (GRU) models applied to the production line
of a chemical production facility outperform several other predictive models in
terms of out-of-sample forecasting accuracy by the Diebold-Mariano (DM) test
with several metrics. The predictive information is fundamental for the risk
and production management of electricity consumers.

Keywords:
Short Term Load Forecast, Deep Neural Network, Hard Structure Load Process

JEL Classification:
C51, C52, C53, Q31, Q41

IRTG1792DP2021 021
Hedging Cryptocurrency Options

Jovanka Matic
Natalie Packham
Wolfgang Karl Härdle

Abstract:
The cryptocurrency (CC) market is volatile, non-stationary and noncontinuous.
Together with liquid derivatives markets, this poses a unique opportunity to
study risk management, especially the hedging of options, in a turbulent market.
We study the hedge behaviour and effectiveness for the class of a ne jump
diffusion models and infinite activity Lévy processes. First, market data is
calibrated to SVI-implied volatility surfaces to price options. To cover a wide
range of market dynamics, we generate Monte Carlo price paths using an SVCJ
model (stochastic volatility with correlated jumps) assumption and a close-to-
actual-market GARCH- filtered kernel density estimation. In these two markets,
options are dynamically hedged with Delta, Delta-Gamma, Delta-Vega and Minimum
Variance strategies. Including a wide range of market models allows to
understand the trade-off in the hedge performance between complete, but overly
parsimonious models, and more complex, but incomplete models. The calibration
results reveal a strong indication for stochastic volatility, low jump frequency
and evidence of infinite activity. Short-dated options are less sensitive to
volatility or Gamma hedges. For longer-date options, good tail risk reduction is
consistently achieved with multiple-instrument hedges. This is persistently
accomplished with complete market models with stochastic volatility.

 

JEL Classification:
C00

IRTG1792DP2021 022
A Financial Risk Meter for China

Ruting Wang
Michael Althof
Wolfgang Karl Härdle

Abstract:
This paper develops a new risk meter specifically for China – FRM@China – to
detect systemic financial risk as well as tail-event (TE) dependencies among
major financial institutions (FIs). Compared with the CBOE FIX VIX, which is
currently the most popular financial risk measure, FRM@China has less noise. It
also emitted a risk signature much earlier than the CBOE FIX VIX index in the
2020 COVID pandemic. In addition, FRM@China uses a single quantile-lasso
regression model to allow both the assessment of risk transfer between different
sectors in which FIs operate and the prediction of systemic risk. Because the
risk indicator in FRM@China is based on penalization terms, its relationship
with macro variables are unknown and non-linear. This paper further expands the
existing FRM approach by using Shapley values to identify the dynamic
contribution of different macro features in this type of "black box" situation.
The results show that short-term interest rates and forward guidance are
significant risk drivers. This paper considers the interaction among FIs from
mainland China, Hong Kong and Taiwan to provide an enhanced regional tool set
for regulators to evaluate financial policy responses. All quantlets are
available on quantlet.com.

Keywords:
FRM (Financial Risk Meter), Lasso Quantile Regression, Financial Network, China,
Shapley value

JEL Classification:
C30, C58, G11, G15, G21

IRTG1792DP2021 023
Networks of News and Cross-Sectional Returns

Junjie Hu
Wolfgang Karl Härdle

Abstract:
We uncover networks from news articles to study cross-sectional stock returns.
By analyzing a huge dataset of more than 1 million news articles collected from
the internet, we construct time-varying directed networks of the S&P500 stocks.
The well-defined directed news networks are formed based on a modest assumption
about firm-specific news structure, and we propose an algorithm to tackle type-I
errors in identifying the stock tickers. We find strong evidence for the
comovement effect between the news-linked stocks returns and reversal effect
from the lead stock return on the 1-day ahead follower stock return, after
controlling for many known effects. Furthermore, a series of portfolio tests
reveal that the news network attention proxy, network degree, provides a robust
and significant cross-sectional predictability of the monthly stock returns.
Among different types of news linkages, the linkages of within-sector stocks,
large size lead firms, and lead firms with lower stock liquidity are crucial for
cross-sectional predictability.

Keywords:
Networks, Textual News, Cross-Sectional Returns, Comovement, Network Degree

JEL Classification:
G11, G41, C21

IRTG1792DP2022 001
Hedging Cryptos with Bitcoin Futures

Francis Liu
Natalie Packham
Meng-Jou Lu
Wolfgang Karl Härdle

Abstract:
The introduction of derivatives on Bitcoin enables investors to hedge risk
exposures in cryptocurrencies. Because of volatility swings and jumps in
cryptocurrency prices, the traditional variance-based approach to obtain hedge
ratios is infeasible. As a consequence, we consider two extensions of the
traditional approach: first, different dependence structures are modelled by
different copulae, such as the Gaussian, Student-t, Normal Inverse Gaussian and
Archimedean copulae; second, different risk measures, such as value-at-risk,
expected shortfall and spectral risk measures are employed to and the optimal
hedge ratio. Extensive out-of-sample tests give insights in the practice of
hedging various cryptos and crypto indices, including Bitcoin, Ethereum,
Cardano, the CRIX index and a number of crypto-portfolios in the time period
December 2017 until May 2021. Evidences show that BTC futures can effectively
hedge BTC and BTC-involved indices. This promising result is consistent across
different risk measures and copulae except for Frank. On the other hand, we
observe complex and diverse dependence structures between BTC-not-involved
assets and the futures. As a consequence, results of hedging other assets and
indices are diverse and, in some occasions, not ideal.

Keywords:
Cryptocurrencies, risk management, hedging, copulas

JEL Classification:
G11, G13
"""
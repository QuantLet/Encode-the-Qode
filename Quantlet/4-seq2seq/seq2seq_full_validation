{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"machine_shape":"hm","gpuType":"T4","mount_file_id":"1wlGOOqN6HqI_4DtMDWwVViv1NPV1i_dd","authorship_tag":"ABX9TyMIKR0i0Gs+8dzS7BMcFWQ2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"fLZ-fhNrroEm"},"outputs":[],"source":["%%capture\n","%pip install torchtext==0.6.0\n","%pip install transformers\n","%pip install evaluate\n","%pip install rouge_score"]},{"cell_type":"code","source":["QPATH = 'Quantlet/4-seq2seq'\n","\n","import sys\n","IN_COLAB = 'google.colab' in sys.modules\n","\n","import os\n","if IN_COLAB:\n","  os.chdir(f'/content/drive/MyDrive/ColabNotebooks/IRTG/Encode_the_Qode/Encode-the-Qode/{QPATH}')"],"metadata":{"id":"PyN2kYYirokB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torchtext\n","torchtext.__version__"],"metadata":{"id":"CiqTfZxrA5oD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import ast\n","import torch\n","import pandas as pd\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","\n","from tqdm import tqdm\n","\n","import re\n","import pickle\n","\n","#from torchtext import data, datasets\n","#import torchdata.datapipes as dp\n","#import torchtext.transforms as T\n","from torchtext.vocab import build_vocab_from_iterator\n","from torch.utils.data import DataLoader\n","from torchtext.datasets import TranslationDataset, Multi30k\n","from torchtext.data import Field, BucketIterator, TabularDataset\n","\n","import spacy\n","\n","import random\n","import math\n","import time\n","\n","import importlib\n","from seq2seq_modeling import *\n","from seq2seq_modeling import Seq2Seq,  epoch_time # train,\n","#from seq2seq_modeling import  evaluate as ev\n","from transformers import AutoTokenizer"],"metadata":{"id":"u8WwMU8xA7D8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n","os.environ[\"WANDB_DISABLED\"] = \"true\"\n","#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""],"metadata":{"id":"yWc5RB1VA8no"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DATE = '20231104'"],"metadata":{"id":"34hZSDQjA-4p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def tokenize_summary(text):\n","    \"\"\"\n","    Tokenizes question from a string into a list of strings (tokens) and reverses it\n","    \"\"\"\n","    return list(filter(lambda x: len(x) < 15, re.findall(r\"[\\w']+\", text)))#[::-1]\n","\n","def tokenize_snippet(text):\n","    \"\"\"\n","    Tokenizes code snippet into a list of operands\n","    \"\"\"\n","    return list(filter(lambda x: (len(x) < 15) and (len(x)>=2), re.findall(r\"[\\w']+|[.,!?;:@~(){}\\[\\]+-/=\\\\\\'\\\"\\`]\", ' '.join(text.split()[:500]))))"],"metadata":{"id":"gcq89WX2BAJG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["SRC = Field(\n","    tokenize = tokenize_snippet,\n","    init_token = '<sos>',\n","    eos_token = '<eos>',\n","    lower = True,\n","    include_lengths = True\n",")\n","\n","TRG = Field(\n","    tokenize = tokenize_summary,\n","    init_token = '<sos>',\n","    eos_token = '<eos>',\n","    lower = True\n",")\n","\n","fields = {\n","    'code_script': ('src', SRC),\n","    'Description': ('trg', TRG)\n","}\n","\n","train_data, valid_data, test_data = TabularDataset.splits(\n","                            path = f'../../data/preprocessed/Quantlet/{DATE}/',\n","                            train = f\"train_df_{DATE}_sample0.csv\",\n","                            validation = f\"val_df_{DATE}_sample0.csv\",\n","                            test = f\"test_df_{DATE}_sample0.csv\",\n","                            format = 'csv',\n","                            fields = fields\n",")"],"metadata":{"id":"9CHkc-LJBB-C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["SRC.build_vocab([train_data.src], max_size=30000, min_freq=4)\n","print(SRC.vocab.freqs.most_common(20))\n","\n","\n","TRG.build_vocab([train_data.trg], max_size=4000, min_freq=2)\n","print(TRG.vocab.freqs.most_common(20))\n","\n","print(f\"Unique tokens in code: {len(SRC.vocab)}\")\n","print(f\"Unique tokens in descriptions: {len(TRG.vocab)}\")"],"metadata":{"id":"6OGRkGdbBEZ8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","#device = torch.device('cpu')"],"metadata":{"id":"YaeFchIFBG0J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["BATCH_SIZE = 4\n","\n","train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n","    (train_data, valid_data, test_data),\n","        batch_size = BATCH_SIZE,\n","        sort_within_batch = True,\n","        sort_key = lambda x : len(x.src),\n","        device = device)"],"metadata":{"id":"RAVohJaGBIEn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["INPUT_DIM = len(SRC.vocab)\n","OUTPUT_DIM = len(TRG.vocab)\n","ENC_EMB_DIM = 128\n","DEC_EMB_DIM = 128\n","ENC_HID_DIM = 100\n","DEC_HID_DIM = 50\n","ENC_DROPOUT = 0.8\n","DEC_DROPOUT = 0.3\n","PAD_IDX = SRC.vocab.stoi['<pad>']\n","SOS_IDX = TRG.vocab.stoi['<sos>']\n","EOS_IDX = TRG.vocab.stoi['<eos>']\n","\n","attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n","enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n","dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n","\n","model = Seq2Seq(enc, dec, PAD_IDX, SOS_IDX, EOS_IDX, device).to(device)"],"metadata":{"id":"9WF1KKMkBJdM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def init_weights(m):\n","    for name, param in m.named_parameters():\n","        if 'weight' in name:\n","            nn.init.normal_(param.data, mean=0, std=0.01)\n","        else:\n","            nn.init.constant_(param.data, 0)\n","\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(f'N Parameters {count_parameters(model):,}')"],"metadata":{"id":"xIwJk6XIBLDw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.apply(init_weights)\n","\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)"],"metadata":{"id":"iJCIOXDWQAm0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import evaluate\n","def compute_metrics(decoded_preds, decoded_labels, remove_special_tokens=False):\n","\n","        if remove_special_tokens:\n","          decoded_preds = [pred.replace('<eos>', '').replace('<sos>', '').replace('<unk>', '').replace('<pad>', '') for pred in decoded_preds]\n","          decoded_labels = [pred.replace('<eos>', '').replace('<sos>', '').replace('<unk>', '').replace('<pad>', '') for pred in decoded_labels]\n","\n","        results_dict = {}\n","        for m in ['rouge', 'bleu']:\n","            metric = evaluate.load(m)\n","\n","            if m=='bleu':\n","                result = metric.compute(\n","                    predictions=decoded_preds, references=decoded_labels\n","                )\n","            elif m=='rouge':\n","                result = metric.compute(\n","                    predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n","                )\n","            result = {key: value for key, value in result.items() if key!='precisions'}\n","\n","            result = {k: round(v, 4) for k, v in result.items()}\n","            results_dict.update(result)\n","        return results_dict"],"metadata":{"id":"a5lRE-t9BM5Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_local(model, iterator, optimizer, criterion, clip, store=False):\n","\n","    rouge_list = []\n","    loss_list = []\n","    trans_list = []\n","    targets_list = []\n","\n","    model.train()\n","\n","    epoch_loss = 0\n","\n","    for i, batch in tqdm(enumerate(iterator)):\n","          src, src_len = batch.src\n","          trg = batch.trg\n","\n","          optimizer.zero_grad()\n","\n","          output, attention = model(src, src_len, trg, 0.4)\n","\n","          #trg = [trg sent len, batch size]\n","          #output = [trg sent len, batch size, output dim]\n","\n","          output = output[1:].view(-1, output.shape[-1])\n","          trg = trg[1:].view(-1)\n","\n","\n","          #trg = [(trg sent len - 1) * batch size]\n","          #output = [(trg sent len - 1) * batch size, output dim]\n","\n","          loss = criterion(output, trg)\n","\n","          loss.backward()\n","\n","          torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n","\n","          optimizer.step()\n","\n","          epoch_loss += loss.item()\n","\n","    return epoch_loss / len(iterator)"],"metadata":{"id":"tWAUTn8zBOnA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def ev(model, iterator, criterion, store=False):\n","\n","    #rouge_list = []\n","    #loss_list = []\n","    trans_list = []\n","    targets_list = []\n","\n","    model.eval()\n","\n","    epoch_loss = 0\n","\n","    with torch.no_grad():\n","\n","        for i, batch in enumerate(iterator):\n","            src, src_len = batch.src\n","            trg = batch.trg\n","\n","            output, attention = model(src, src_len, trg, 0) #turn off teacher forcing\n","\n","            output = output[1:].view(-1, output.shape[-1])\n","            trg = trg[1:].view(-1)\n","\n","            #trg = [trg sent len, batch size]\n","            #output = [trg sent len, batch size, output dim]\n","            loss = criterion(output, trg)\n","\n","            #trg = [(trg sent len - 1) * batch size]\n","            #output = [(trg sent len - 1) * batch size, output dim]\n","\n","            epoch_loss += loss.item()\n","\n","    return epoch_loss / len(iterator)"],"metadata":{"id":"4e8QlCkEBQMb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def translate_sentence(model, sentence):\n","    global SRC\n","    global TRG\n","    model.eval()\n","    tokenized = tokenize_snippet(sentence)\n","    tokenized = ['<sos>'] + [t.lower() for t in tokenized] + ['<eos>']\n","    numericalized = [SRC.vocab.stoi[t] for t in tokenized]\n","    sentence_length = torch.LongTensor([len(numericalized)]).to(device)\n","    tensor = torch.LongTensor(numericalized).unsqueeze(1).to(device)\n","    translation_tensor_logits, attention = model(tensor, sentence_length, None, 0)\n","    translation_tensor = torch.argmax(translation_tensor_logits.squeeze(1), 1)\n","    translation = [TRG.vocab.itos[t] for t in translation_tensor]\n","    translation, attention = translation[1:], attention[1:]\n","    return translation, attention"],"metadata":{"id":"AOwiVMy2l0mS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["d_path = f'../../data/preprocessed/Quantlet/{DATE}/'\n","train_df = pd.read_csv(f\"{d_path}train_df_{DATE}_sample0.csv\")\n","validation_df = pd.read_csv(f\"{d_path}val_df_{DATE}_sample0.csv\")\n","test_df = pd.read_csv(f\"{d_path}test_df_{DATE}_sample0.csv\")"],"metadata":{"id":"ABabVVsSZ5dE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tqdm import tqdm\n","tqdm.pandas()"],"metadata":{"id":"2GHNzhync_sH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open(f'../../data/preprocessed/Quantlet/{DATE}/results_dict_epochs_loss_only.pickle', 'rb') as handle:\n","    results_dict = pickle.load(handle)"],"metadata":{"id":"4eIEcQylqkuM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results_dict = {}\n","STORE = True\n","READ = False\n","\n","results_dict_table = {}\n","\n","for SEED in range(1):\n","  print(SEED)\n","\n","  random.seed(SEED)\n","  torch.manual_seed(SEED)\n","  torch.backends.cudnn.deterministic = True\n","\n","  model = Seq2Seq(enc, dec, PAD_IDX, SOS_IDX, EOS_IDX, device).to(device)\n","  model.apply(init_weights)\n","\n","  optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","  criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)\n","\n","  N_EPOCHS = 50\n","  CLIP = 1\n","\n","  best_valid_loss = float('inf')\n","\n","  training_rouge = []\n","  training_loss = []\n","\n","  evaluation_rouge = []\n","  evaluation_loss = []\n","\n","  for epoch in range(0, N_EPOCHS):\n","      print(epoch)\n","\n","      start_time = time.time()\n","\n","      if READ:\n","        print('READING EXISTING MODEL')\n","        model.load_state_dict(torch.load(f'conala_model_attention_test_{SEED}_{epoch}.pt'))\n","        train_loss = ev(model, train_iterator, criterion)\n","        valid_loss = ev(model, valid_iterator, criterion)\n","      else:\n","        print('TRAINING')\n","        train_loss = train_local(model, train_iterator, optimizer, criterion, CLIP)\n","        valid_loss = ev(model, valid_iterator, criterion)\n","\n","      # FULL VALIDATION\n","      print('VALIDATION TRAIN')\n","      train_df[f'pred_{epoch}'] = train_df.code_script.progress_apply(lambda x: translate_sentence(model, x)[0])\n","      print('VALIDATION VALIDATION')\n","      validation_df[f'pred_{epoch}'] = validation_df.code_script.progress_apply(lambda x: translate_sentence(model, x)[0])\n","      print('VALIDATION TEST')\n","      test_df[f'pred_{epoch}'] = test_df.code_script.progress_apply(lambda x: translate_sentence(model, x)[0])\n","\n","      end_time = time.time()\n","\n","      epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","\n","      #if valid_loss < best_valid_loss:\n","      #    best_valid_loss = valid_loss\n","      if not READ:\n","        torch.save(model.state_dict(), f'conala_model_attention_test_{SEED}_{epoch}.pt')\n","\n","      print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n","      #print(f'Perplexity (training): {math.exp(train_loss):7.3f}')\n","      #print(f'Perplexity (validation): {math.exp(valid_loss):7.3f}')\n","\n","      print(f'Loss (training): {train_loss:7.3f}')\n","      print(f'Loss (validation): {valid_loss:7.3f}')\n","\n","      #print(f'ROUGE (training): {train_rouge}')\n","      #print(f'ROUGE (validation): {eval_rouge}')\n","      training_loss.append(train_loss)\n","      evaluation_loss.append(valid_loss)\n","\n","\n","      results_dict[SEED] = {'train_loss' : training_loss,\n","                        'valid_loss' : evaluation_loss}\n","\n","      #with open(f'../../data/preprocessed/Quantlet/{DATE}/results_dict_epochs_loss_only.pickle', 'wb') as handle:\n","      #    pickle.dump(results_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","      #train_df.to_csv(f'../../data/preprocessed/Quantlet/{DATE}/train_with_epoch_preds_{SEED}.csv', index=False)\n","      #validation_df.to_csv(f'../../data/preprocessed/Quantlet/{DATE}/val_with_epoch_preds_{SEED}.csv', index=False)\n","      #test_df.to_csv(f'../../data/preprocessed/Quantlet/{DATE}/test_with_epoch_preds_{SEED}.csv', index=False)\n","\n","  results_dict[SEED] = {'train_loss' : training_loss,\n","                        'valid_loss' : evaluation_loss}\n","\n","  #with open(f'../../data/preprocessed/Quantlet/{DATE}/results_dict_epochs_loss_only.pickle', 'wb') as handle:\n","  #    pickle.dump(results_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","  #train_df.to_csv(f'../../data/preprocessed/Quantlet/{DATE}/train_with_epoch_preds_{SEED}.csv', index=False)\n","  #validation_df.to_csv(f'../../data/preprocessed/Quantlet/{DATE}/val_with_epoch_preds_{SEED}.csv', index=False)\n","  #test_df.to_csv(f'../../data/preprocessed/Quantlet/{DATE}/test_with_epoch_preds_{SEED}.csv', index=False)"],"metadata":{"id":"-LYUCLjGBQz_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#with open(f'../../data/preprocessed/Quantlet/{DATE}/results_dict_epochs_loss_only.pickle', 'wb') as handle:\n","#    pickle.dump(results_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"],"metadata":{"id":"8uCASITXfrQn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open(f'../../data/preprocessed/Quantlet/{DATE}/results_dict_epochs_loss_only2.pickle', 'rb') as handle:\n","    results_dict = pickle.load(handle)"],"metadata":{"id":"BRDzjkKDXf7o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def join_pred(row):\n","  if isinstance(row, str):\n","    row = ast.literal_eval(row)\n","  return ' '.join(row)"],"metadata":{"id":"jHGJ4Vp6dA6J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for SEED in range(5):\n","  print(SEED)\n","  N_EPOCHS = 10\n","\n","  train_df = pd.read_csv(f'../../data/preprocessed/Quantlet/{DATE}/train_with_epoch_preds_{SEED}.csv')\n","  validation_df = pd.read_csv(f'../../data/preprocessed/Quantlet/{DATE}/val_with_epoch_preds_{SEED}.csv')\n","  test_df = pd.read_csv(f'../../data/preprocessed/Quantlet/{DATE}/test_with_epoch_preds_{SEED}.csv')\n","\n","  train_rouge = []\n","  val_rouge = []\n","  test_rouge = []\n","\n","  for epoch in range(0, N_EPOCHS):\n","      print(epoch)\n","      train_df[f'pred_{epoch}'] = train_df[f'pred_{epoch}'].apply(join_pred)\n","      validation_df[f'pred_{epoch}'] = validation_df[f'pred_{epoch}'].apply(join_pred)\n","      test_df[f'pred_{epoch}'] = test_df[f'pred_{epoch}'].apply(join_pred)\n","\n","      train_rouge.append(compute_metrics(train_df[f'pred_{epoch}'].head(1000).values, train_df['code_script'].head(1000).values, False)['rouge1'])\n","      val_rouge.append(compute_metrics(validation_df[f'pred_{epoch}'].values, validation_df['code_script'].values, False)['rouge1'])\n","      test_rouge.append(compute_metrics(test_df[f'pred_{epoch}'].values, test_df['code_script'].values, False)['rouge1'])\n","\n","  results_dict[SEED]['train_rouge'] = train_rouge\n","  results_dict[SEED]['val_rouge'] = val_rouge\n","  results_dict[SEED]['test_rouge'] = test_rouge"],"metadata":{"id":"VneMkO4rbQGZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["val_res = pd.DataFrame()\n","test_res = pd.DataFrame()\n","\n","for SEED in range(5):\n","  print(SEED)\n","  N_EPOCHS = 10\n","\n","  validation_df = pd.read_csv(f'../../data/preprocessed/Quantlet/{DATE}/val_with_epoch_preds_{SEED}.csv')\n","  test_df = pd.read_csv(f'../../data/preprocessed/Quantlet/{DATE}/test_with_epoch_preds_{SEED}.csv')\n","\n","  for epoch in [N_EPOCHS-1]:\n","\n","      print(epoch)\n","      validation_df[f'pred_{epoch}'] = validation_df[f'pred_{epoch}'].apply(join_pred)\n","      test_df[f'pred_{epoch}'] = test_df[f'pred_{epoch}'].apply(join_pred)\n","\n","      val_res = pd.concat([val_res, pd.DataFrame(compute_metrics(validation_df[f'pred_{epoch}'].values, validation_df['code_script'].values, False), index=[SEED])], axis=0)\n","      test_res = pd.concat([test_res, pd.DataFrame(compute_metrics(test_df[f'pred_{epoch}'].values, test_df['code_script'].values, False), index=[SEED])], axis=0)"],"metadata":{"id":"tjhj7d9t2PaB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["val_res.std()"],"metadata":{"id":"_BD5tUFQ4Izm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_res.std()"],"metadata":{"id":"ZKGSMY714PSK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open(f'../../data/preprocessed/Quantlet/{DATE}/results_dict.pickle', 'rb') as handle:\n","    results_dict = pickle.load(handle)"],"metadata":{"id":"IoXI_-hL72aQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vl, tl =  [], []\n","for SEED in range(5):\n","  print(SEED)\n","  model.load_state_dict(torch.load(f'conala_model_attention_test_{SEED}_9.pt'))\n","  test_loss = ev(model, test_iterator, criterion)\n","  vl.append(results_dict[SEED]['valid_loss'][-1])\n","  tl.append(test_loss)"],"metadata":{"id":"0FT_6Vhs75ce"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.array(vl).mean()"],"metadata":{"id":"16esqP3_8wm_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#with open(f'../../data/preprocessed/Quantlet/{DATE}/results_dict.pickle', 'wb') as handle:\n","#    pickle.dump(results_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"],"metadata":{"id":"efWjsWN4CfIe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#with open(f'results_dict.pickle', 'wb') as handle:\n","#    pickle.dump(results_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"],"metadata":{"id":"nMqYfFxKXDFK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#with open(f'../../data/preprocessed/Quantlet/{DATE}/results_dict.pickle', 'rb') as handle:\n","#    results_dict = pickle.load(handle)"],"metadata":{"id":"SEqJFlLD17J6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["example_idx = 2\n","\n","src = ' '.join(vars(train_data.examples[example_idx])['src'])\n","trg = ' '.join(vars(train_data.examples[example_idx])['trg'])\n","\n","print(f'src = {src}')\n","print(f'trg = {trg}')\n","\n","translation, attention = translate_sentence(model, src)\n","\n","print('predicted trg = ', ' '.join(translation))\n","\n","#display_attention(src, translation, attention)"],"metadata":{"id":"coUolij1CKdd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["example_idx = 2\n","\n","src = ' '.join(vars(test_data.examples[example_idx])['src'])\n","trg = ' '.join(vars(test_data.examples[example_idx])['trg'])\n","\n","print(f'src = {src}')\n","print(f'trg = {trg}')\n","\n","translation, attention = translate_sentence(model, src)\n","\n","print('predicted trg = ', ' '.join(translation))\n","\n","#display_attention(src, translation, attention)"],"metadata":{"id":"imTkjcipJMuQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"a9vYzxBKREhO"},"execution_count":null,"outputs":[]}]}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "########### FUNCTIONS #####################\n",
    "###########################################\n",
    "\n",
    "\n",
    "\n",
    "def generate_summary(test_samples, model, tokenizer, encoder_max_length, decoder_max_length):\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        test_samples,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=encoder_max_length,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    input_ids = inputs.input_ids.to(model.device)\n",
    "    attention_mask = inputs.attention_mask.to(model.device)\n",
    "    outputs = model.generate(input_ids, attention_mask=attention_mask, max_new_tokens=decoder_max_length)\n",
    "    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return outputs, output_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\"\"\"lapply(libraries, function(x) if (!(x %in% installed.packages())) {\n",
    "    install.packages(x)\n",
    "})\n",
    "lapply(libraries, library, quietly = TRUE, character.only = TRUE)\n",
    "\n",
    "DAX = read.csv(\"data_DAX091222-111222.csv\")\n",
    "dax1 = DAX[2:(length(DAX[, 7]) - 1), 7]\n",
    "dax2 = DAX[3:length(DAX[, 7]), 7]\n",
    "z = log(dax1) - log(dax2)  # log returns\n",
    "\n",
    "ks.test(z, \"pnorm\", list(mean = 0, sd = 1), H = NA, sim = 500, tol = 1e-05, estfun = NA) \n",
    "\"\"\", \n",
    "\"\"\"close all\n",
    "clear all\n",
    "clc\n",
    "\n",
    "disp('Please input number of draws n as') ;\n",
    "disp(' ') ;\n",
    "n  = input('[n]=');\n",
    "y  = normrnd(0,1,n,1);      % Generate standard normal random numbers\n",
    "y  = sort(y);\n",
    "cdfplot(y)                  % Plot the empirical distribution function\n",
    "hold on\n",
    "f  = cdf('Normal',y,0,1);   % Generate normal cumulative distribution function\n",
    "\n",
    "plot(y,f,'r','LineWidth',2.5)\n",
    "legend('Empirical','Theoretical','Location','NW')\n",
    "title('EDF and CFD')\n",
    "xlabel('X')\n",
    "ylabel('EDF(X), CDF(X)')\n",
    "grid off\n",
    "\n",
    "[g,y] = ecdf(y);\n",
    "g     = g(2:(n+1));\n",
    "[C,I] = max(abs(f-g))\n",
    "hold off\n",
    "]\"\"\", \"\"\"\n",
    "df = pd.read_csv('data/EECI_TWh - TWh per Year-2020-12-09.csv', na_values='#DIV/0!')\n",
    "df.fillna(method='ffill', inplace=True)\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df.index = df['Date']\n",
    "df.drop(['Date'], axis=1, inplace=True)\n",
    "fig, ax1 = plt.subplots(figsize=(20,10))\n",
    "\n",
    "color = '#bdb2ff'\n",
    "ax1.plot(df['Estimated TWh per Year'], color=color)\n",
    "color = '#023047'\n",
    "ax1.plot(df['Minimum TWh per Year'], color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "\n",
    "fig.tight_layout() \n",
    "plt.savefig('ethereum_energy_consumption.png', transparent=True)\"\"\",\n",
    "\n",
    "\"\"\"\n",
    "from SFM_Hurst_Exponent.AR import ar\n",
    "import os\n",
    "os.chdir('SFM_Hurst_Exponent/')\n",
    "\n",
    "# Execute Download\n",
    "# from SFM_Hurst_Exponent import binance_download\n",
    "\n",
    "# Read and select Close price (Header: Timestamp, OHLCV, ...)\n",
    "p = pd.read_json('Binance_BTCUSDT_1m_1577836800000-1580342400000.json')\n",
    "df = p[[4]]\n",
    "df['changes'] = df.pct_change()\n",
    "changevec = df['changes'].dropna() + 1\n",
    "changevec = changevec.dropna()\n",
    "\n",
    "random_changes = np.array(changevec)\n",
    "series = np.cumprod(random_changes)\n",
    "\n",
    "H_l = []\n",
    "c_l = []\n",
    "data_l = []\n",
    "mse_l = []\n",
    "series_splits = np.array_split(series, 100)\n",
    "\n",
    "for currseries in series_splits:\n",
    "    H, c, data = compute_Hc(currseries, kind='price', simplified=True)\n",
    "    H_l.append(H)\n",
    "    c_l.append(c)\n",
    "    data_l.append(data)\n",
    "    mse_l.append(ar(currseries - 1))  # Get back to simple returns around 0\n",
    "\n",
    "# Evaluate Hurst equation for complete data set\n",
    "H, c, data = compute_Hc(series, kind='price', simplified=True)\n",
    "\n",
    "# Use Autoregressive Model to predict price for periods of differing H.\n",
    "# Would expect high H periods (e.g. > 0.5) to have a smaller Mean Squared Error than other periods\n",
    "# Then use some hypothesis test on the mean difference.\n",
    "\n",
    "# Use t-test for mse_l small when H_l large\n",
    "cutoff_int = 0.5\n",
    "idx = np.where(np.array(H_l) > cutoff_int)\n",
    "mse_supposedly_low = np.array(mse_l)[idx]\"\"\",\n",
    "\"\"\"\n",
    "gen_physical_density = function(tau, interest_rate, initial_price, days_to_maturity, curr_date, simmethod = 'SVCJ', n_simulations = 100, f = 'BTC_USD_QUANDL', cached_svcj_parameters = TRUE){\n",
    "    \n",
    "    days_to_maturity = max(days_to_maturity, 1)\n",
    "\n",
    "    # Read historical Bitcoin Data\n",
    "    if(f == 'BTCUSDT'){\n",
    "        # Older, shorter data\n",
    "        hist_dat = read.csv('data/BTCUSDT.csv', stringsAsFactors = FALSE)\n",
    "        hist_dat$date = as.Date(hist_dat$Date)\n",
    "        hist_dat = hist_dat[hist_dat$date <= curr_date,]\n",
    "        hist_dat$Date = NULL\n",
    "        tdat = ts(hist_dat$Adj.Close)\n",
    "        rets = diff(log(tdat))\n",
    "    }else if(f == 'BTC_USD_QUANDL'){\n",
    "        \n",
    "        hist_dat = read.csv('data/BTC_USD_Quandl.csv', stringsAsFactors = FALSE)\n",
    "        hist_dat$date = as.Date(hist_dat$Date)\n",
    "        hist_dat = hist_dat[hist_dat$date <= curr_date & hist_dat$date >= '2017-01-01',]\n",
    "        hist_dat$Date = NULL\n",
    "        hist_dat = hist_dat[rev(seq_len(nrow(hist_dat))), , drop = FALSE]\n",
    "        tdat = ts(hist_dat$Adj.Close)\n",
    "        rets = diff(log(tdat))\n",
    "    }else{\n",
    "        stop('no time series')\n",
    "    }\n",
    "\n",
    "\n",
    "    if(simmethod != 'SVCJ'){\n",
    "\n",
    "        if(simmethod == 'garch'){\n",
    "            print('using garch, delete this later!')\n",
    "\n",
    "            # Estimate sigma with GARCH model\n",
    "            # Parameters for calculation/simulation\n",
    "            numbapprox  \t= 2000\t\t\t# fineness of the grid\n",
    "            N\t\t= n_simulations\t# only run once, because the loop is happening in physicaldensity.R\n",
    "            # Check return series for ARMA effects, e.g. with the following function\n",
    "            # auto.arima(dax.retts, max.p=10, max.q=10, max.P=5, max.Q=5, \n",
    "            # start.p=1, start.q=1,start.P=1, start.Q=1, stationary=T, seasonal=F)\n",
    "            p\t\t= 0\n",
    "            q\t\t= 0\n",
    "            arma= c(p,q)\n",
    "            # specify garch order (need to be checked)\n",
    "            m\t\t= 1\n",
    "            s\t\t= 1\n",
    "            garch\t\t= c(m,s)\n",
    "            garchmodel\t= \"eGARCH\"\n",
    "            submodel\t= \"GARCH\"\n",
    "            # underlying distribution (default: \"sstd\" - skewed stundent t's)\n",
    "            # (alternatives: \"norm\" - normal, \"ghyp\"- generalized hyperbolic)\n",
    "            udist\t\t= \"sstd\"\n",
    "            # set archm=T for ARCH in mean model (archpow specifies the power)\n",
    "            archm\t\t= F\n",
    "            archpow\t\t= 1\n",
    "            # set include.mean = F if you don't want to include a mean in the mean model\n",
    "            include.mean \t= T  \n",
    "            spec\t\t\t= ugarchspec(variance.model = list(model = garchmodel, \n",
    "                                garchOrder = garch, submodel = submodel), mean.model = \n",
    "                                list(armaOrder = arma, archm=archm,archpow=archpow,\n",
    "                                include.mean=include.mean), distribution.model = udist)\n",
    "            #garchfit = rugarch::ugarchfit(data = rets, spec = spec, solver = \"hybrid\")\n",
    "            garchsim = ugarchsim(garchfit, n.sim = days_to_maturity, \n",
    "                                n.start = 0, m.sim=N, startMethod=(\"sample\"), \n",
    "                                mexsimdata=TRUE)\n",
    "            est_sigma = garchsim@simulation$sigmaSim\"\"\", \n",
    "            \n",
    "            \"\"\"rm(list=ls(all=TRUE))\n",
    "graphics.off()\n",
    "  \n",
    "# install and load packages\n",
    "libraries = c(\"kernlab\",\"tseries\",\"quadprog\",\"zoo\")\n",
    "lapply(libraries, function(x) if (!(x %in% installed.packages())) {\n",
    "    install.packages(x)})\n",
    "lapply(libraries, library, quietly = TRUE, character.only = TRUE)  \n",
    "  \n",
    "  # generation of ideal data set: Xp with covariance (4,0,0,4) and Xn with covariance (0.25,0,0,0.25) for groups x(1) and x(-1)\n",
    "p       = 4   # number of plots\n",
    "n       = 200 # number of observations\n",
    "set.seed(2)\n",
    "# generating 2-variate data, member of group x(1)\n",
    "  \n",
    "sigma.p = matrix(c(4,0,0,4),2,2)\n",
    "  \n",
    "Mp      = t(chol(sigma.p))  \t\t\t\t# Cholesky square root\n",
    "Zp      = matrix(rnorm(n),2,100)  \t\t\t# 2 row, 50 columns\n",
    "Xp      = t(Mp %*% Zp)\n",
    "Xp1     = Xp[,1]\n",
    "Xp2     = Xp[,2]\n",
    "  \n",
    "  # generating 2-variate data, member of group x(-1)\n",
    "  \n",
    "sigma.n = matrix(c(0.25,0,0,0.25),2,2)\n",
    "  \n",
    "Mn      = t(chol(sigma.n))  \t\t\t\t# Cholesky square root\n",
    "Zn      = matrix(rnorm(n),2,100)  \t\t\t# 2 row, 50 columns\n",
    "Xn      = t(Mn %*% Zn)\n",
    "Xn1     = Xn[,1]\n",
    "Xn2     = Xn[,2]\n",
    "  \n",
    "  # Aggregating data\n",
    "  \n",
    "  X1      = c(Xp1,Xn1)\n",
    "  X2      = c(Xp2,Xn2)\n",
    "  \n",
    "  # generating indicator variable\n",
    "  \n",
    "yp      = rep(1,n/2)\n",
    "yn      = rep(-1,n/2)\n",
    "  \n",
    "Y       = c(yp,yn)\n",
    "OP      = cbind(X2, X1)\n",
    "  \n",
    "## Main program of SVM classification plot\n",
    "  \n",
    "sgm = c(0.2,5,0.2,5)\t# parameter r in anisotropic gaussian kernel\n",
    "C = c(0.1,0.1,8,8)\n",
    "\n",
    "for (i in 1:p){ \n",
    "  OrangePeelModel = ksvm(OP, Y, type=\"C-svc\", kernel=\"rbfdot\", kpar=list(sigma=sgm[i]), C=C[i], prob.model=TRUE, cross=4)\n",
    "  str = paste(\"s=\",sprintf(\"%0.1f\",sgm[i]),\", c=\",sprintf(\"%0.1f\",C[i]),\"                          \",sep=\"\")\n",
    "  plot(OrangePeelModel, data=OP,cex=0.7,cex.main=1.5)\n",
    "  title(sub=str)\n",
    "  print(OrangePeelModel)\n",
    "  }\"\"\", \"\"\"% SMSsvmorange MATLAB Code\n",
    "%\n",
    "% translated from R by: Bey, Patrik (beypatri@gmail.com)\n",
    "%\n",
    "\n",
    "clear all\n",
    "\n",
    "cd('./')                              %adjust working directory\n",
    "\n",
    "%%\n",
    "% generation of ideal data set: Xp with covariance (4,0,0,4) and Xn with covariance (0.25,0,0,0.25) for groups x(1) and x(-1)\n",
    "%%\n",
    "\n",
    "n = 200;                              % number of observations\n",
    "\n",
    "%%\n",
    "% generating 2-variate data, member of group x(1)\n",
    "%%  \n",
    "sigma_p = [4 0;0 4];\n",
    "  \n",
    "Mp      = chol(sigma_p);               % Cholesky square root\n",
    "Zp      = randn(2,n/2);                % 2 row, 100 columns /in orig R-code 50 columns\n",
    "Xp      = (Mp*Zp)';\n",
    "Xp1     = Xp(:,1);\n",
    "Xp2     = Xp(:,2);\n",
    "\n",
    "%%\n",
    "% generating 2-variate data, member of group x(-1)\n",
    "%%\n",
    "  \n",
    "sigma_n = [0.25 0; 0 0.25];\n",
    "  \n",
    "Mn      = chol(sigma_n);               % Cholesky square root\n",
    "Zn      = randn(2,n/2);                % 2 row, 100 columns /in orig R-code 50 columns\n",
    "Xn      = (Mn*Zn)';\n",
    "Xn1     = Xn(:,1);\n",
    "Xn2     = Xn(:,2);\n",
    "\n",
    "%%  \n",
    "% Aggregating data\n",
    "%%  \n",
    "\n",
    "X1      = [Xp1;Xn1];\n",
    "X2      = [Xp2;Xn2];\n",
    "  \n",
    "%%\n",
    "%generating indicator variable\n",
    "%%\n",
    "\n",
    "yp      = ones(n/2,1);\n",
    "yn      = -ones(n/2,1);\n",
    "  \n",
    "Y       = [yp;yn];                     % group labels used in SVM\n",
    "OP      = [X2, X1]                     % training data used in SVM\n",
    "\n",
    "%%%%%%\n",
    "%% Main program of SVM classification plot\n",
    "%%%%%%\n",
    "\n",
    "%%\n",
    "%define kernel parameter for SVM\n",
    "%%\n",
    "sgm = [0.2,5,0.2,5]'; %radial basis function kernel parameter sigma\n",
    "C = [0.1,0.1,8,8]; %SVM cost function parameter C\n",
    "  \n",
    "for i = 1:length(sgm)\n",
    "    figure(i)\n",
    "    svmStruct = svmtrain(OP,Y,'ShowPlot',true,'kernel_function','rbf','rbf_sigma',sgm(i),'boxconstraint',C(i));\n",
    "    title( ['SVM with sigma ' num2str(sgm(i)) ' and C ' num2str(C(i))])\n",
    "end\"\"\", \n",
    "\"\"\"import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "import ast\n",
    "import time\n",
    "from datetime import date\n",
    "today = date.today()\n",
    "We need to get the list of all hashes first\n",
    "with open('address.txt', 'r') as file:\n",
    "    address = file.read()\n",
    "    \n",
    "address_to_get_all = address + '?platform=ethereum&limit=100&page='\n",
    "\n",
    "data = pd.DataFrame(columns=['category', \n",
    "                             'categories', \n",
    "                             'created', \n",
    "                             'name', \n",
    "                             'rank', \n",
    "                             'slug', \n",
    "                             'teaser'])\n",
    "# we have the maximum of 20 pages, if the limit set to 100 apps per page\n",
    "for page in range(1, 21):\n",
    "    test = requests.get(f'{address_to_get_all}{page}')\n",
    "    for i, item in enumerate(test.json()['items']):\n",
    "        category = test.json()['items'][i]['categories']\n",
    "        if len(category)>1:\n",
    "            categories = ' '.join(category)\n",
    "            category = category[0]\n",
    "        else:\n",
    "            categories = 0\n",
    "            category = category[0]\n",
    "        created = test.json()['items'][i]['created']\n",
    "        name = test.json()['items'][i]['name']\n",
    "        rank = test.json()['items'][i]['rank']\n",
    "        slug = test.json()['items'][i]['slug']\n",
    "        teaser = test.json()['items'][i]['teaser']\n",
    "        df_temp = pd.DataFrame(data={'category': category, \n",
    "                                    'categories': categories,\n",
    "                                    'created': created,\n",
    "                                    'name': name,\n",
    "                                    'rank': rank,\n",
    "                                    'slug': slug,\n",
    "                                    'teaser': teaser}, index=[0])\n",
    "        data = pd.concat([data, df_temp])\n",
    "    time.sleep(0.25)\n",
    "data = data.drop('categories', axis=1)\n",
    "data.to_csv(f'data/dapps_names_{today}.csv', index=False)\n",
    "For each dapp, here or each slug, retrieve a list of hashes\n",
    "data = pd.read_csv(f'data/dapps_names_{today}.csv')\n",
    "address_for_dapp = f'{address}/'\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "hashes = pd.DataFrame(columns=['slug', 'hash', 'description', 'license'])\"\"\", \n",
    ",\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import kurtosis\n",
    "import os\n",
    "\n",
    "os.getcwd()\n",
    "os.chdir(\"/Users/jane_hsieh/Library/CloudStorage/OneDrive-國立陽明交通大學/Data Science Analysis Templates/Machine Learning/Part 9 - Dimensionality Reduction/SectionExtra 3 - Independent Component Analysis (ICA)/FRM Data Analysis_Porfolio Construction_with ICA-PCA\")  \n",
    "\n",
    "\n",
    "data_dir = './Data'\n",
    "output_dir = './Output'\n",
    "\n",
    "# ====================================  0. Input data: FRM prices / returns ====================================================\n",
    "# returns\n",
    "df_return = pd.read_csv(data_dir+'/FRM_CHHKTW_Time_Series_Returns_20201030.csv', parse_dates=['Date'], index_col = 'Date')\n",
    "\n",
    "\n",
    "\n",
    "col_chosen = [28, 159, 172, 182, 153, 108, 41, 154, 107, 173, 27, 122, 64, 163, 105] #[28, 159, 172, 182, 153] #\n",
    "col_chosen = [i-1 for i in col_chosen]\n",
    "\n",
    "\n",
    "stocks = df_return.columns[col_chosen];print(stocks)\n",
    "\n",
    "#df_return = df_return.iloc[:,col_chosen]\n",
    "#df_return.columns\n",
    "\n",
    "\n",
    "\n",
    "df = df_return.iloc[:,col_chosen]\n",
    "del df_return\n",
    "\n",
    "\n",
    "df.columns = [i.replace('.EQUITY', '') for i in df.columns]\n",
    "df.columns\n",
    "\n",
    "# prices\n",
    "df_price = pd.read_csv(data_dir+'/FRM_SHSZ300HSITWSE_Stock_Prices_update_20201030.csv', parse_dates=['Date'], index_col = 'Date')\n",
    "\n",
    "stocks2 = df_price.columns[col_chosen];print(stocks2) #check if stocks2== stocks\n",
    "\n",
    "df_price = df_price.iloc[:,col_chosen]\n",
    "df_price.columns = df.columns\n",
    "\n",
    "\n",
    "## 0.1 Missing data imputation ---------------------------------------------------------------\n",
    "print(\"Numnber of missing data for returns:: \\n\", df.isnull().sum())\n",
    "print(\"Numnber of missing data for prices: \\n\", df_price.isnull().sum())\n",
    "'''\n",
    "#Only sporatic missing points, hence we simply perform linear interpolation method to those missing values\n",
    "\n",
    "df_return.fillna(0) #since supposed stock price has no change for missing value; i.e., df_price.fillna(method='ffill', inplace=True)\n",
    "'''\n",
    "df_price.fillna(method='ffill', inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 0.2 Visualization: Multidimensional Time Series Data Plot (FRM) ---------------------------------------------------------------\n",
    "start = '2019-01-02'\n",
    "end =  '2020-10-30'\n",
    "\n",
    "df[start: end].plot(figsize=(15,6))\n",
    "#plt.legend(fancybox=True, framealpha=0.0, loc = 'upper right', prop={'size': 8})\n",
    "plt.legend(bbox_to_anchor=(1.02, 1),loc='upper left', borderaxespad=0., \n",
    "           fancybox=True, framealpha=0.0, prop={'size': 8})\n",
    "plt.title(f'Daily returns of each stock (FRM@Asia) from {start} to {end}')\n",
    "\n",
    "plt.show()\n",
    "plt.savefig(data_dir+f'/Multidimensional Daily Returns (FRM) from {start} to {end} ({len(col_chosen)}D).png', transparent = True)\n",
    "plt.close()\"\"\"]\n",
    "\n",
    "\n",
    "example_URI = [\"https://github.com/QuantLet/MSE/tree/master/MSEKolmogorov-Smirnovtest\",\n",
    "                \"https://github.com/QuantLet/MSE/tree/master/MSEGCthmnorm\",\n",
    "                \"https://github.com/QuantLet/USC/tree/master/SC-energy-consumption\", \n",
    "                \"https://github.com/QuantLet/SFM_Class_2019WS/tree/master/SFM_Hurst_Exponent\",\n",
    "                \"https://github.com/QuantLet/BitcoinPricingKernels/tree/master/BitcoinPricingKernels\", \n",
    "                \"https://github.com/QuantLet/SMS2/tree/master/SMSsvmorange\",\n",
    "                \"https://github.com/QuantLet/SMS2/tree/master/SMSsvmorange\", \n",
    "                \"https://github.com/QuantLet/USC/tree/master/SC-Dapp-scraping\", \n",
    "                \"https://github.com/QuantLet/DigitalEconomyDecisionAnalytics/tree/master/DEDA%20-%20Independent%20Component%20Analysis%20(ICA)/FRM%20Data%20Analysis_Porfolio%20Construction_with%20ICA-PCA\"]\n",
    " \n",
    "example_language = [\"R\", \"M\", \"Py\", \"Py\", \"R\", \"R\", \"M\", \"Py\", \"Py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_path = {#\"FlanT5 ZS\" : \"google/flan-t5-base\",\n",
    "\"FlanT5 FT\": \"/home/RDC/zinovyee.hub/H:/zinovyee.hub/IRTG/MLSC/Encode-the-Qode/Quantlet/4-qode2desc/reports/analysis_report_google/flan-t5-base_no_context_val_20231119_normal/results/checkpoint-10260\",\n",
    "\"CodeT5 ZS\" : \"Salesforce/codet5-base-multi-sum\",\n",
    "\"CodeT5 FS\" : \"/home/RDC/zinovyee.hub/H:/zinovyee.hub/IRTG/MLSC/Encode-the-Qode/Quantlet/4-qode2desc/reports/analysis_report_few_shot_CodeT5_no_context_test_20231104/results/checkpoint-1500\",\n",
    "\"CodeT5 FT\" : \"/home/RDC/zinovyee.hub/H:/zinovyee.hub/IRTG/MLSC/Encode-the-Qode/Quantlet/4-qode2desc/reports/analysis_report_CodeT5_no_context_test_20231104/results/checkpoint-11000\",\n",
    "\"CodeTrans ZS\": \"SEBIS/code_trans_t5_base_source_code_summarization_python_multitask\",\n",
    "\"CodeTrans FS\": \"/home/RDC/zinovyee.hub/H:/zinovyee.hub/IRTG/MLSC/Encode-the-Qode/Quantlet/4-qode2desc/reports/analysis_report_few_shot_CodeTrans_no_context_test_20231104/results/checkpoint-1500\",\n",
    "\"CodeTrans FT\": \"/home/RDC/zinovyee.hub/H:/zinovyee.hub/IRTG/MLSC/Encode-the-Qode/Quantlet/4-qode2desc/reports/analysis_report_CodeTrans_no_context_val_20231104/results/checkpoint-1000\",}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading  FlanT5 FT\n",
      "Tokenizing with  FlanT5 FT\n",
      "Generating with  FlanT5 FT\n",
      "Summmary : ['Performs a Kolmogorov-Smirnov test on the DAX log return data from 2009-12-21 to 2011-12-22', 'Draws n observations from a standard normal distribution and plots its empirical distribution function vs. the normal cumulative distribution function. Number of draws can be entered interactively.', 'Estimates the optimal portfolio weights with a minimum spanning tree, and plots the optimal portfolio weights with a minimum spanning tree.', 'Use Autoregressive Model to predict price for periods of differing H.', \"Reads historical Bitcoin Data, if (f == 'BTCUSDT') # Older, shorter data, hist_dat = read.csv\", \"'Computes the 2D map of 2 groups of the SVM classification using anipotropic Gaussian kernel for two different groups of the dataset.'\", 'Generates a two-dimensional scatterplot of two-dimensional observations and two-dimensional scatterplots for the two-dimensional SVM data.', 'This Quantlet generates two different types of data frames: one for the master thesis Supervised Machine Learning Sentiment Measures', 'This script generates a python3 file from the python3 directory. The python3 file is located in the directory of the python3.', 'Calculates the FRM returns for the portfolio of Bayer, BMW, Siemens and Volkswagen.']\n",
      "Release cache\n",
      "Loading  CodeT5 ZS\n",
      "Tokenizing with  CodeT5 ZS\n",
      "Generating with  CodeT5 ZS\n",
      "Summmary : ['lapply libraries libraries library and check if there are any missing packages in the library.', 'close all\\nclear all\\nclc\\ndisp all\\nhold all\\nclc\\ndisp all\\nclc\\ndisp all\\nclc\\nprocs', 'Plots the estimated minimum and estimated TWh per Year and Energy Consumption in a pretty way.', 'Retrieve a single high - term sequence from the BTCUSDT_1m_1577836800000 - 1580342400000.', 'function to generate physical density of a sequence of n_simulations', 'This function is a wrapper around the basic problem of the problem that is not available in the system. It is also used to generate the basic problem of the problem that is not available in the system.', '% SMSsvmorange MATLAB %', 'This function import pandas as a Pandas dataframe', 'Python 3. 6 doesn t support UTF - 8.', 'Function to plot a single\\n object in the FRM data.']\n",
      "Release cache\n",
      "Loading  CodeT5 FS\n",
      "Tokenizing with  CodeT5 FS\n",
      "Generating with  CodeT5 FS\n",
      "Summmary : ['Plots the difference between zero-coupon CAT bond prices in the DAX091222-111222.', 'Draws n observations from standard normal distribution and plots its empirical distribution function  vs. the normal cumulative distribution function. Number of draws can be entered interactively. The point where the edf and cdf differs most is also reported.', 'Outputs the estimated and minimum TWh per Year as a plot.', 'Computes EUR/USD and European hedging modes for a given price of 1 January 2002 and the Hurst model.', 'Simulates the time series of a SVCJ model for the time series of BTCUSD and BTCUSD_QUANDL with log-returns of the difference between 0.001 and 0.01.', 'Computes the optimal likelihood discrimination rule  for the EUR/USD data.', 'Plots the empirical results of a SVM classifier with different kernel parameter. The code is triggered with the following functions: Xp, Xn, and then performs various diagnostic tests.', 'obtaining the list of hashes of all publications from the webpage API and scraping the data from the API.', \"'Performs a test of independence for UTF-8 and UTF-8 in the case of TD, subset of the data.\", 'This Quantlet builds two cryptocurrencies from FRM-Coloured FRM and FRM-Coloured FRM stocks by Chao, FRM-Coloured FRM']\n",
      "Release cache\n",
      "Loading  CodeT5 FT\n",
      "Tokenizing with  CodeT5 FT\n",
      "Generating with  CodeT5 FT\n",
      "Summmary : ['Performs a Kolmogorov-Smirnov test on the DAX log return data from 2009-12-21 to 2011-12-22', 'Draws n observations from standard normal distribution and plots its empirical distribution function  vs. the normal cumulative distribution function. Number of draws can be entered interactively. The point where the edf and cdf differs most is also reported.', 'Draws the data of the EECI_TWh-TWh per Year to reflect the effect of a ETH, in the time period from 2000-01-01 to 2012-12-09.', 'Downloads BTC/USDT transaction data from Binance API aggregated per minute from 2017-01-01 until 2020-01-30. Selects close prices and divides them into 1000 subsets. For each subset, Hurst equation is calculated and an AR Model is fitted.', 'Simulates parameters of calculating a simple energy of Bitcoin returns from 1998 to 2007 using a simulated method.', \"'generates the ideal data set with mixture of 2-variate data, member of group x with covariance  and Xn with covariance.\", \"'plots the area of two different groups via svm classification using anipotropic Gaussian kernel for artificial data'\", 'Scraping all hashes from the webpage API and analyzing the difference of the api with the API and the api with the API.', 'This chapter first introduces the fatal errors of the visualization. It presents the compliance of the visualization in a later step.', 'Computes European Call-on-a-Call option prices for different stock prices and saves the data as Risk Analysis of the CRIX.']\n",
      "Release cache\n",
      "Loading  CodeTrans ZS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing with  CodeTrans ZS\n",
      "Generating with  CodeTrans ZS\n",
      "Summmary : ['# install.packages(library) return\"', \"disp(' '); = n; % Quit loop }; %\", '# plt.show() # plt.show() # plt.show()', '# import pandas as pd import numpy as np # Execute Download # # # # # # Compute the mean difference # # Compute the mean difference # # Compute the', ',, max.Q=10, max.R=5) # return', ', #tseries_) # # # #) # # generating a model for each plot) # generating a model for each plot #,) # Generating a model for each plot) #', \"%% % %% %','rbf_sigma','rbf_sigma'); %%\", 'import os import json import json import json import json import json import json import json', 'coding: utf', '# # # # missing data for prices:: # missing data for prices:: #, df.isnull().sum())']\n",
      "Release cache\n",
      "Loading  CodeTrans FS\n",
      "Tokenizing with  CodeTrans FS\n",
      "Generating with  CodeTrans FS\n",
      "Summmary : ['Computes the p-norm of the DAX in DAX order for the DAX of DAX DAX data from DAX091222-091222.', 'Draws n observations from standard normal distribution and plots its empirical distribution function vs. the normal cumulative distribution function. Number of draws can be entered interactively. The point where the edf and cdf differs most is also reported.', 'This Quantlet builds the empirical energy consumption of the empirical data.', 'This Quantlet runs the Hurst Exponent model and calculates the mean difference between the two models. The model is different with respect to the mean difference for the models. The model has a higher correlation between the models.', 'Estimates the physical density with a fixed period of time.', 'Generates and plots of the SVM-SVM model for the given parameters.', \"'Performs a SVM classification on an SVM classifier.\", 'This script loads the list of all the hashes of each dapp, this script loads the full set of hashes from the dapp, this script loads the full set of hashes from the dapp, this script loads the full set of hashes from the dapp, this script loads the data from the dapp, this script loads the data from the dapp, this script loads the data from the dapp, this script loads the data from the dapp, this script loads the data from the dapp, this script loads the data from the dapp,', 'This coding is used to run the program and save the result as utf-8', 'This Quantlet builds the data from the FRM data, calculating the Equity-Parity Ratio of returns from the SFE data.']\n",
      "Release cache\n",
      "Loading  CodeTrans FT\n",
      "Tokenizing with  CodeTrans FT\n",
      "Generating with  CodeTrans FT\n",
      "Summmary : ['Plots the probability density function of DAX log returns for the DAX log returns of the log returns of the log returns of the log returns of the log returns of the log returns of the log returns of the log returns of the log returns of the log returns of the log returns of the log returns of the log returns of the log returns of the log returns of the log returns of the log returns of the log returns of the log returns of the log returns of the log returns of the log returns of the log returns of the log returns of the log returns of the', 'Plots n observations of a normal distribution function and plots the empirical and cumulative distribution function.', 'Plots the estimated and minimum TWh per Year - TWh per Year - ', 'Estimate the close price for different H periods.', 'Estimates the estimated sigma of the Bitcoin data.', 'Plots the SVM classification of the data set.', 'Plots the classification of the SVM and its classification parameters.', 'Queries the Quantlet API for the addresses of the Quantlet Quantlet Quantlet and retrieves the hashes for each of them.', 'coding: utf-8 -*- coding: utf-8 -*-', 'This Quantlet runs the FRM data and calculates the number of missing data for returns and prices.']\n",
      "Release cache\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for model_name, model_path in model_name_path.items():\n",
    "    print(\"Loading \", model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "    print(\"Tokenizing with \", model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, skip_special_tokens=False)\n",
    "    \n",
    "    print(\"Generating with \", model_name)\n",
    "\n",
    "    if model_name.startswith(\"Flan\"):\n",
    "        examples_inst = [f\"Summarize the code: {example}\" for example in examples]\n",
    "        summaries = generate_summary(examples_inst, model, tokenizer, 512, 300)\n",
    "    else: \n",
    "        summaries = generate_summary(examples, model, tokenizer, 512, 300)\n",
    "\n",
    "    results[model_name] = summaries[1]\n",
    "    print(\"Summmary :\", summaries[1])\n",
    "    \n",
    "    print(\"Release cache\")\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "FlanT5_ZS = [\"# Close windows and clear variables rm(list = ls(all = TR\", \"[g,y] = ecdf(y); g = g\", \"import numpy as np import pandas as pd import matplot\", \"import pd, scipy, scipy, np import\", \"# # # # # # #\", \"\"\"# install and load packages libraries = c(\"kernlab\",\"tseries\"\"\",\n",
    "             \"MATLAB Code % % translated from R by\", \"# import re import numpy as np import ast import time from date\", \n",
    "             \"#plt.legend(fancybox=True, framealpha\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame.from_dict(results)\n",
    "results_df[\"FlanT5_ZS\"] = FlanT5_ZS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(\"results_20240526.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/net/zinovyee.hub/IRTG/MLSC/Encode-the-Qode/Quantlet/4-qode2desc\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "encode_code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

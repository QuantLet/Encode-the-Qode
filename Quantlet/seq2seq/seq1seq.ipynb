{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASED ON PYTORCH NLP TUTORIAL https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (3.7.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from spacy) (2.4.6)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from spacy) (23.0)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from spacy) (8.1.10)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from spacy) (4.64.1)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from spacy) (0.3.3)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from spacy) (1.24.1)\n",
      "Requirement already satisfied: setuptools in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from spacy) (67.1.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from spacy) (2.28.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from spacy) (6.3.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/RDC/zinovyee.hub/.local/lib/python3.9/site-packages (from spacy) (1.10.7)\n",
      "Requirement already satisfied: jinja2 in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/RDC/zinovyee.hub/.local/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/RDC/zinovyee.hub/.local/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.1.1)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.9)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages (from jinja2->spacy) (2.1.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "\n",
    "from torchtext import data, datasets\n",
    "import torchdata.datapipes as dp\n",
    "import torchtext.transforms as T\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import spacy\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "import importlib\n",
    "from seq2seq_modeling import *\n",
    "from seq2seq_modeling import Seq2Seq, train, evaluate, epoch_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codet5-base-multi-sum\", skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tokenize_summary(text):\n",
    "    \"\"\"\n",
    "    Tokenizes question from a string into a list of strings (tokens) and reverses it\n",
    "    \"\"\"\n",
    "    return list(filter(lambda x: len(x) < 15, re.findall(r\"[\\w']+\", text)[::-1]))\n",
    "\n",
    "def tokenize_snippet(text):\n",
    "    \"\"\"\n",
    "    Tokenizes code snippet into a list of operands\n",
    "    \"\"\"\n",
    "    return list(filter(lambda x: len(x) < 15, re.findall(r\"[\\w']+|[.,!?;:@~(){}\\[\\]+-/=\\\\\\'\\\"\\`]\", text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_descr(text):\n",
    "    return tokenizer.tokenize(text, max_length=75)\n",
    "\n",
    "def tokenize_code(code):\n",
    "    return tokenizer.tokenize(code, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATE = '20231104'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = f'../../data/preprocessed/Quantlet/{DATE}/train_df_{DATE}_sample0.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pipe = dp.iter.IterableWrapper([FILE_PATH])\n",
    "data_pipe = dp.iter.FileOpener(data_pipe, mode='rb')\n",
    "data_pipe = data_pipe.parse_csv(skip_lines=1, delimiter=',', as_tuple=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeAttribution(row):\n",
    "    \"\"\"\n",
    "    Function to keep the first two elements in a tuple\n",
    "    \"\"\"\n",
    "    return (row[1], row[5])\n",
    "\n",
    "data_pipe = data_pipe.map(removeAttribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[   1,  123,    5,  ...,    0,    0,    0],\n",
      "        [   1, 2122,   44,  ...,    0,    0,    0],\n",
      "        [   1, 1210,  124,  ...,    0,    0,    0],\n",
      "        [   1, 1210,  124,  ..., 2793,   83,    2]]), tensor([[   1,   44,   29,    9,  598,  579,   10,    4, 1993,    6, 2665, 1557,\n",
      "            5, 3588, 2089,  292, 3352,   21,  447,    9,  579,   10,    4, 1993,\n",
      "            6, 2665, 1557,   15,  317, 1720,   80,    5,  374, 3792,    7, 1307,\n",
      "           76,    5,  261,   28,  289,    2],\n",
      "        [   1,   27,    9, 1091,    6, 1792, 1031, 1845, 2250, 1124,  339,  820,\n",
      "          518,  383,   33,    2,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0],\n",
      "        [   1,   44,  118,   35,    6,   55,  782,   10,  133,  131,  927,  290,\n",
      "          239, 1554,  109,   21,  741,   12, 2886,   87,   44,    2,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0],\n",
      "        [   1,   44,  118,   35,    6,   55,  782,   10,  133,  131,  927,  290,\n",
      "          239, 1554,  109,   21,  741,   12, 2886,   87,   44,    2,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0]]))\n",
      "(tensor([[   1,  218,    5,  ..., 1225, 3500,    2],\n",
      "        [   1, 1377, 2256,  ...,   23,  295,    2],\n",
      "        [   1,  103, 1362,  ...,    4, 2474,    2],\n",
      "        [   1,  218,    5,  ...,   15, 4311,    2]]), tensor([[   1,  726,   16,   34, 2148,   11, 3317,    7,    3,  422,   16,    9,\n",
      "         1479,    5,  739,  720, 2383,    7, 2221, 1423,    4, 3691,   16,    4,\n",
      "         1479,    5,  592,  318, 1071,   39,    3,  144, 3460, 2521, 2346,   71,\n",
      "          272,    2,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   1,  249,    3,   14,   16, 1000,    6,    9, 1637,  116,   25,   12,\n",
      "         1752,  523,  416,   10, 1206,  316,   60, 3753,  234,    5,   18,   91,\n",
      "            6, 1632,   17,    4, 1092,  227,    3,  769, 3359,  381,   10,    9,\n",
      "         1470, 1092,    3,    5,    2,    0,    0,    0,    0,    0],\n",
      "        [   1,  830,    7,   32,  271, 2204,   30,   56, 1861,   11,  311, 3279,\n",
      "            7,   90,    3,  372,  565,   11,  761,  251, 1335,  394,   10,  176,\n",
      "         2133, 1101,   14,    6,  229, 1171,    8,    7,  447,   61, 1433, 2746,\n",
      "           66,  326,  932,  287,  344,    5,    2,    0,    0,    0],\n",
      "        [   1, 1677, 2383,    7, 2221, 1423,    4, 1464,   21,   19, 1730, 2481,\n",
      "          470,    5, 1191,  435, 2936,    3,    3,    3,    6,   40,    3,    8,\n",
      "          864, 2544, 1709, 1462, 3227,    8, 2521,    8, 2521, 2346,   71,    8,\n",
      "           50,   11, 3452,  155, 2521, 2346,   71,    3,    5,    2]]))\n"
     ]
    }
   ],
   "source": [
    "for i, sample in enumerate(data_pipe):\n",
    "    print(sample)\n",
    "    if i ==1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTokens(data_iter, place):\n",
    "    \"\"\"\n",
    "    Function to yield tokens from an iterator. Since, our iterator contains\n",
    "    tuple of sentences (source and target), `place` parameters defines for which\n",
    "    index to return the tokens for. `place=0` for source and `place=1` for target\n",
    "    \"\"\"\n",
    "    for code, summary in data_iter:\n",
    "        if place == 0:\n",
    "            #yield tokenize_snippet(code)\n",
    "            yield tokenize_code(code)\n",
    "        else:\n",
    "            #yield tokenize_summary(summary)\n",
    "            yield tokenize_descr(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/RDC/zinovyee.hub/H:/zinovyee.hub/IRTG/MLSC/Encode-the-Qode/Quantlet/seq2seq/seq1seq.ipynb Zelle 16\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bsunflower.wiwi.hu-berlin.de/home/RDC/zinovyee.hub/H%3A/zinovyee.hub/IRTG/MLSC/Encode-the-Qode/Quantlet/seq2seq/seq1seq.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m source_vocab \u001b[39m=\u001b[39m build_vocab_from_iterator(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsunflower.wiwi.hu-berlin.de/home/RDC/zinovyee.hub/H%3A/zinovyee.hub/IRTG/MLSC/Encode-the-Qode/Quantlet/seq2seq/seq1seq.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     getTokens(data_pipe, \u001b[39m0\u001b[39;49m),\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsunflower.wiwi.hu-berlin.de/home/RDC/zinovyee.hub/H%3A/zinovyee.hub/IRTG/MLSC/Encode-the-Qode/Quantlet/seq2seq/seq1seq.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     min_freq\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsunflower.wiwi.hu-berlin.de/home/RDC/zinovyee.hub/H%3A/zinovyee.hub/IRTG/MLSC/Encode-the-Qode/Quantlet/seq2seq/seq1seq.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     specials\u001b[39m=\u001b[39;49m [\u001b[39m'\u001b[39;49m\u001b[39m<pad>\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m<sos>\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m<eos>\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m<unk>\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsunflower.wiwi.hu-berlin.de/home/RDC/zinovyee.hub/H%3A/zinovyee.hub/IRTG/MLSC/Encode-the-Qode/Quantlet/seq2seq/seq1seq.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     special_first\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsunflower.wiwi.hu-berlin.de/home/RDC/zinovyee.hub/H%3A/zinovyee.hub/IRTG/MLSC/Encode-the-Qode/Quantlet/seq2seq/seq1seq.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m )\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsunflower.wiwi.hu-berlin.de/home/RDC/zinovyee.hub/H%3A/zinovyee.hub/IRTG/MLSC/Encode-the-Qode/Quantlet/seq2seq/seq1seq.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m source_vocab\u001b[39m.\u001b[39mset_default_index(source_vocab[\u001b[39m'\u001b[39m\u001b[39m<unk>\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[0;32m~/.conda/envs/encode_code/lib/python3.9/site-packages/torchtext/vocab/vocab_factory.py:98\u001b[0m, in \u001b[0;36mbuild_vocab_from_iterator\u001b[0;34m(iterator, min_freq, specials, special_first, max_tokens)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[39mBuild a Vocab from an iterator.\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39m    >>> vocab = build_vocab_from_iterator(yield_tokens(file_path), specials=[\"<unk>\"])\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     97\u001b[0m counter \u001b[39m=\u001b[39m Counter()\n\u001b[0;32m---> 98\u001b[0m \u001b[39mfor\u001b[39;00m tokens \u001b[39min\u001b[39;00m iterator:\n\u001b[1;32m     99\u001b[0m     counter\u001b[39m.\u001b[39mupdate(tokens)\n\u001b[1;32m    101\u001b[0m specials \u001b[39m=\u001b[39m specials \u001b[39mor\u001b[39;00m []\n",
      "\u001b[1;32m/home/RDC/zinovyee.hub/H:/zinovyee.hub/IRTG/MLSC/Encode-the-Qode/Quantlet/seq2seq/seq1seq.ipynb Zelle 16\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsunflower.wiwi.hu-berlin.de/home/RDC/zinovyee.hub/H%3A/zinovyee.hub/IRTG/MLSC/Encode-the-Qode/Quantlet/seq2seq/seq1seq.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m code, summary \u001b[39min\u001b[39;00m data_iter:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsunflower.wiwi.hu-berlin.de/home/RDC/zinovyee.hub/H%3A/zinovyee.hub/IRTG/MLSC/Encode-the-Qode/Quantlet/seq2seq/seq1seq.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mif\u001b[39;00m place \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsunflower.wiwi.hu-berlin.de/home/RDC/zinovyee.hub/H%3A/zinovyee.hub/IRTG/MLSC/Encode-the-Qode/Quantlet/seq2seq/seq1seq.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m         \u001b[39m#yield tokenize_snippet(code)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bsunflower.wiwi.hu-berlin.de/home/RDC/zinovyee.hub/H%3A/zinovyee.hub/IRTG/MLSC/Encode-the-Qode/Quantlet/seq2seq/seq1seq.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m         \u001b[39myield\u001b[39;00m tokenize_code(code)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsunflower.wiwi.hu-berlin.de/home/RDC/zinovyee.hub/H%3A/zinovyee.hub/IRTG/MLSC/Encode-the-Qode/Quantlet/seq2seq/seq1seq.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsunflower.wiwi.hu-berlin.de/home/RDC/zinovyee.hub/H%3A/zinovyee.hub/IRTG/MLSC/Encode-the-Qode/Quantlet/seq2seq/seq1seq.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m         \u001b[39m#yield tokenize_summary(summary)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsunflower.wiwi.hu-berlin.de/home/RDC/zinovyee.hub/H%3A/zinovyee.hub/IRTG/MLSC/Encode-the-Qode/Quantlet/seq2seq/seq1seq.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m         \u001b[39myield\u001b[39;00m tokenize_descr(summary)\n",
      "\u001b[1;32m/home/RDC/zinovyee.hub/H:/zinovyee.hub/IRTG/MLSC/Encode-the-Qode/Quantlet/seq2seq/seq1seq.ipynb Zelle 16\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsunflower.wiwi.hu-berlin.de/home/RDC/zinovyee.hub/H%3A/zinovyee.hub/IRTG/MLSC/Encode-the-Qode/Quantlet/seq2seq/seq1seq.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenize_code\u001b[39m(code):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bsunflower.wiwi.hu-berlin.de/home/RDC/zinovyee.hub/H%3A/zinovyee.hub/IRTG/MLSC/Encode-the-Qode/Quantlet/seq2seq/seq1seq.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39;49mtokenize(code, max_length\u001b[39m=\u001b[39;49m\u001b[39m512\u001b[39;49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py:320\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.tokenize\u001b[0;34m(self, text, pair, add_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenize\u001b[39m(\u001b[39mself\u001b[39m, text: \u001b[39mstr\u001b[39m, pair: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, add_special_tokens: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mstr\u001b[39m]:\n\u001b[0;32m--> 320\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencode_plus(text\u001b[39m=\u001b[39;49mtext, text_pair\u001b[39m=\u001b[39;49mpair, add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\u001b[39m.\u001b[39mtokens()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2717\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2707\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   2708\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   2709\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[1;32m   2710\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2714\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   2715\u001b[0m )\n\u001b[0;32m-> 2717\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_encode_plus(\n\u001b[1;32m   2718\u001b[0m     text\u001b[39m=\u001b[39;49mtext,\n\u001b[1;32m   2719\u001b[0m     text_pair\u001b[39m=\u001b[39;49mtext_pair,\n\u001b[1;32m   2720\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   2721\u001b[0m     padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[1;32m   2722\u001b[0m     truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[1;32m   2723\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   2724\u001b[0m     stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   2725\u001b[0m     is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   2726\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   2727\u001b[0m     return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   2728\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   2729\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   2730\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   2731\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   2732\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   2733\u001b[0m     return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   2734\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   2735\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2736\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/roberta/tokenization_roberta_fast.py:275\u001b[0m, in \u001b[0;36mRobertaTokenizerFast._encode_plus\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m is_split_into_words \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mis_split_into_words\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    270\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_prefix_space \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m is_split_into_words, (\n\u001b[1;32m    271\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mYou need to instantiate \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m with add_prefix_space=True \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    272\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mto use it with pretokenized inputs.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    273\u001b[0m )\n\u001b[0;32m--> 275\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_encode_plus(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py:500\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_encode_plus\u001b[39m(\n\u001b[1;32m    479\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    480\u001b[0m     text: Union[TextInput, PreTokenizedInput],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    498\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BatchEncoding:\n\u001b[1;32m    499\u001b[0m     batched_input \u001b[39m=\u001b[39m [(text, text_pair)] \u001b[39mif\u001b[39;00m text_pair \u001b[39melse\u001b[39;00m [text]\n\u001b[0;32m--> 500\u001b[0m     batched_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_encode_plus(\n\u001b[1;32m    501\u001b[0m         batched_input,\n\u001b[1;32m    502\u001b[0m         is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m    503\u001b[0m         add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m    504\u001b[0m         padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[1;32m    505\u001b[0m         truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[1;32m    506\u001b[0m         max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m    507\u001b[0m         stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m    508\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m    509\u001b[0m         return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m    510\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m    511\u001b[0m         return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m    512\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m    513\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m    514\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m    515\u001b[0m         return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m    516\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    517\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    518\u001b[0m     )\n\u001b[1;32m    520\u001b[0m     \u001b[39m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[1;32m    521\u001b[0m     \u001b[39m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[1;32m    522\u001b[0m     \u001b[39mif\u001b[39;00m return_tensors \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m return_overflowing_tokens:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/roberta/tokenization_roberta_fast.py:265\u001b[0m, in \u001b[0;36mRobertaTokenizerFast._batch_encode_plus\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    259\u001b[0m is_split_into_words \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mis_split_into_words\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    260\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_prefix_space \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m is_split_into_words, (\n\u001b[1;32m    261\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mYou need to instantiate \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m with add_prefix_space=True \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    262\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mto use it with pretokenized inputs.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    263\u001b[0m )\n\u001b[0;32m--> 265\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_batch_encode_plus(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py:428\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[39m# Set the truncation and padding strategy and restore the initial configuration\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_truncation_and_padding(\n\u001b[1;32m    421\u001b[0m     padding_strategy\u001b[39m=\u001b[39mpadding_strategy,\n\u001b[1;32m    422\u001b[0m     truncation_strategy\u001b[39m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    425\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m    426\u001b[0m )\n\u001b[0;32m--> 428\u001b[0m encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tokenizer\u001b[39m.\u001b[39;49mencode_batch(\n\u001b[1;32m    429\u001b[0m     batch_text_or_text_pairs,\n\u001b[1;32m    430\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m    431\u001b[0m     is_pretokenized\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m    432\u001b[0m )\n\u001b[1;32m    434\u001b[0m \u001b[39m# Convert encoding to dict\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[39m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \u001b[39m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[39m#                       List[EncodingFast]\u001b[39;00m\n\u001b[1;32m    438\u001b[0m \u001b[39m#                    ]\u001b[39;00m\n\u001b[1;32m    439\u001b[0m \u001b[39m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[1;32m    440\u001b[0m tokens_and_encodings \u001b[39m=\u001b[39m [\n\u001b[1;32m    441\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_convert_encoding(\n\u001b[1;32m    442\u001b[0m         encoding\u001b[39m=\u001b[39mencoding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[39mfor\u001b[39;00m encoding \u001b[39min\u001b[39;00m encodings\n\u001b[1;32m    452\u001b[0m ]\n",
      "\u001b[0;31mTypeError\u001b[0m: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]"
     ]
    }
   ],
   "source": [
    "source_vocab = build_vocab_from_iterator(\n",
    "    getTokens(data_pipe, 0),\n",
    "    min_freq=2,\n",
    "    specials= ['<pad>', '<sos>', '<eos>', '<unk>'],\n",
    "    special_first=True\n",
    ")\n",
    "source_vocab.set_default_index(source_vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_vocab = build_vocab_from_iterator(\n",
    "    getTokens(data_pipe,1),\n",
    "    min_freq=2,\n",
    "    specials= ['<pad>', '<sos>', '<eos>', '<unk>'],\n",
    "    special_first=True\n",
    ")\n",
    "target_vocab.set_default_index(target_vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTransform(vocab):\n",
    "    \"\"\"\n",
    "    Create transforms based on given vocabulary. The returned transform is applied to sequence\n",
    "    of tokens.\n",
    "    \"\"\"\n",
    "    text_tranform = T.Sequential(\n",
    "        ## converts the sentences to indices based on given vocabulary\n",
    "        T.VocabTransform(vocab=vocab),\n",
    "        ## Add <sos> at beginning of each sentence. 1 because the index for <sos> in vocabulary is\n",
    "        # 1 as seen in previous section\n",
    "        T.AddToken(1, begin=True),\n",
    "        ## Add <eos> at beginning of each sentence. 2 because the index for <eos> in vocabulary is\n",
    "        # 2 as seen in previous section\n",
    "        T.AddToken(2, begin=False)\n",
    "    )\n",
    "    return text_tranform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_list = list(data_pipe)\n",
    "some_sentence = temp_list[798][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_sentence = getTransform(source_vocab)(tokenize_code(some_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos> function Ġy Ġ= ĠComp ĠCor r ĠG auss ĠModel ĠC DO ( a , ĠR , Ġdef ĠProb , ĠU AP , ĠD F , ĠDay ĠCount , Ġtrue ĠSp read ) Ċ C Ġ= Ġnorm inv ( def ĠProb , Ġ0 , Ġ1 ); Ċ N inv ĠK Ġ= Ġnorm inv ( ĠU AP Ġ/ Ġ( 1 Ġ- ĠR ), Ġ0 , Ġ1 ); Ċ A Ġ= Ġ( ĠC Ġ- Ġsqrt ( 1 Ġ- Ġa ^ 2 ) Ġ* ĠN inv ĠK ) Ġ/ Ġa ; Ċ Sigma Ġ= Ġ[ 1 Ġ- a ; Ġ1 ]; Ċ Mu Ġ= Ġ[ 0 Ġ0 ]; Ċ EL 1 Ġ= Ġmv nc df ([ ĠC , Ġ- ĠA ], ĠM u , ĠS igma ); Ċ EL 2 Ġ= Ġnorm cdf ( ĠA ); if ĠL AP Ġ== Ġ0 Ċ EL Ġ= ĠEL 1 Ġ/ ĠU AP Ġ* Ġ( 1 - ĠR ) Ġ+ ĠEL 2 ; Ċ else Ċ N inv ĠL Ġ= Ġnorm inv ( ĠL AP Ġ/ Ġ( 1 Ġ- ĠR ), Ġ0 , Ġ1 ); Ċ B Ġ= Ġ( ĠC Ġ- Ġsqrt ( 1 Ġ- Ġa ^ 2 ) Ġ* ĠN inv ĠL ) Ġ/ Ġa ; Ċ EL 3 Ġ= Ġmv nc df ([ ĠC , Ġ- ĠB ], ĠM u , ĠS igma ); Ċ EL 4 Ġ= Ġnorm cdf ( ĠB ); Ċ Upper ĠET L Ġ= ĠEL 1 Ġ+ ĠEL 2 Ġ* ĠU AP Ġ/ Ġ( 1 Ġ- ĠR ); Ċ EL Ġ= Ġ( ĠUpper ĠET L Ġ- ĠLower ĠET L ) Ġ/ Ġ( ĠU AP Ġ- ĠL AP ) Ġ* Ġ( 1 Ġ- ĠR ); Ċ end Ċ Protect ĠLeg Ġ= Ġsum ( diff ([ 0 ; ĠEL ]) Ġ. * ĠD F ); Ċ Prem ium ĠLeg Ġ= Ġsum (( 1 Ġ- ĠEL ) Ġ. * ĠD F Ġ. * ĠDay ĠCount ); Ċ spread Ġ= ĠPro t ect ĠLeg Ġ/ ĠP rem ium ĠLeg Ġ* Ġ10000 ; Ċ if ĠL AP Ġ== Ġ0 Ċ spread Ġ= Ġ( ĠPro t ect ĠLeg Ġ- Ġ0 . 05 Ġ* ĠP rem ium ĠLeg ) Ġ* Ġ100 ; Ċ end <eos> "
     ]
    }
   ],
   "source": [
    "index_to_string = source_vocab.get_itos()\n",
    "for index in transformed_sentence:\n",
    "    print(index_to_string[index], end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([1, 401, 2901, 5563, 29, 135, 2422, 2502, 296, 674, 55, 2842, 1130, 868, 16, 110, 2631, 79, 377, 132, 708, 60, 1971, 599, 205, 4, 122, 6, 28, 31, 47, 1047, 46, 17, 4, 65, 4575, 1914, 35, 4, 128, 2642, 5, 151, 129, 5, 122, 7, 261, 7, 271, 8, 257, 6, 62, 21, 4384, 1445, 20, 3945, 1143, 4, 1829, 6, 28, 31, 116, 1255, 608, 3734, 8, 90, 17, 4, 65, 116, 1255, 1548, 2653, 1192, 8, 90, 17, 4, 65, 116, 1255, 10006, 164, 4775, 8, 90, 35, 4, 128, 2642, 5, 151, 129, 5, 1829, 7, 1478, 21, 1769, 69, 4464, 1065, 3368, 237, 4, 3546, 651, 4693, 6, 72, 5, 12, 7, 64, 9, 59, 575, 5, 12, 7, 64, 7, 478, 6, 732, 1847, 3932, 2970, 78, 7498, 1763, 3873, 42, 2502, 3331, 2040, 4, 263, 28, 6, 15, 1447, 51, 32, 2801, 1501, 4, 1113, 119, 6, 4679, 5, 97, 8, 1113, 5, 263, 119, 7, 2669, 6, 15, 4058, 146, 65, 23, 4, 10, 7, 1332, 453, 47, 5, 763, 7, 1107, 7, 813, 7, 7989, 7, 5741, 7, 8787, 389, 599, 1871, 5563, 50, 799, 716, 20, 5983, 234, 1679, 4, 3965, 6, 134, 5, 543, 7, 76, 133, 6, 7992, 1006, 7, 76, 144, 6, 186, 5, 1113, 119, 21, 4, 30, 6, 22, 4, 92, 5, 556, 55, 1238, 119, 9, 59, 4, 30, 6, 130, 109, 10, 4, 3965, 13, 7, 130, 25, 6, 608, 3734, 5, 263, 119, 7, 4, 556, 7, 4, 180, 6, 15, 7915, 17, 4, 219, 6, 15, 1767, 20, 149, 20, 176, 2645, 16, 754, 17, 4, 118, 6664, 29, 45, 20, 210, 769, 4, 3820, 6, 4679, 5, 1113, 119, 7, 716, 7, 1332, 9, 5606, 174, 78, 1049, 990, 69, 3037, 704, 4, 46, 302, 40, 6, 174, 53, 5385, 1341, 2181, 9107, 3, 4, 92, 5, 30, 55, 14, 16, 172, 5, 1113, 119, 21, 292, 70, 946, 6, 2175, 2123, 53, 48, 4, 2123, 634, 31, 1726, 3331, 1179, 651, 94, 3820, 13, 30, 63, 4, 304, 6, 192, 7, 4, 1412, 3152, 6, 15, 1937, 35, 4, 54, 6, 50, 8, 342, 31, 288, 65, 6, 303, 8, 2722, 1505, 82, 54, 5, 220, 8, 2722, 1505, 82, 1007, 31, 1767, 20, 149, 20, 176, 2645, 16, 344, 65, 23, 4, 220, 8, 2722, 1505, 82, 1007, 31, 2505, 20, 176, 20, 176, 2645, 16, 344, 65, 23, 4, 740, 6, 760, 344, 453, 10, 16, 34, 133, 5, 3965, 9, 63, 4, 65, 562, 65, 6, 5093, 13, 7, 130, 83, 4, 70, 6, 2175, 46, 5, 54, 7, 3027, 5, 12, 6, 234, 7, 64, 6, 388, 21, 48, 4, 2571, 613, 53, 48, 4, 1229, 64, 3108, 5, 7274, 6, 28, 5, 18, 7, 255, 5, 3965, 273, 48, 4, 43, 67, 31, 75, 5784, 32, 420, 7018, 35, 48, 4, 12, 67, 31, 3587, 35, 48, 4, 2713, 6111, 5, 1907, 295, 6, 391, 9, 4, 46, 302, 40, 361, 30, 482, 6, 232, 2], [1, 262, 252, 4, 300, 2279, 106, 159, 2054, 1787, 24, 66, 3327, 250, 280, 1848, 1073, 515, 5, 13, 16, 2658, 1968, 20, 9, 3, 57, 1578, 1050, 1059, 2750, 8, 15, 303, 951, 7, 2299, 14, 16, 97, 40, 2666, 3856, 13, 7, 15, 1025, 7, 1391, 813, 951, 98, 5, 2])\n"
     ]
    }
   ],
   "source": [
    "def applyTransform(sequence_pair):\n",
    "    \"\"\"\n",
    "    Apply transforms to sequence of tokens in a sequence pair\n",
    "    \"\"\"\n",
    "\n",
    "    return (\n",
    "        getTransform(source_vocab)(tokenize_code(sequence_pair[0])),\n",
    "        getTransform(target_vocab)(tokenize_descr(sequence_pair[1]))\n",
    "    )\n",
    "data_pipe = data_pipe.map(applyTransform) ## Apply the function to each element in the iterator\n",
    "temp_list = list(data_pipe)\n",
    "print(temp_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortBucket(bucket):\n",
    "    \"\"\"\n",
    "    Function to sort a given bucket. Here, we want to sort based on the length of\n",
    "    source and target sequence.\n",
    "    \"\"\"\n",
    "    return sorted(bucket, key=lambda x: (len(x[0]), len(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pipe = data_pipe.bucketbatch(\n",
    "    batch_size = 4, batch_num=5,  bucket_num=1,\n",
    "    use_in_batch_shuffle=False, sort_key=sortBucket\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(([1, 123, 5, 97, 19, 445, 5, 112, 19, 62, 21, 4, 150, 8, 131, 53, 12, 6, 28, 5, 10, 8, 2079, 7, 10, 8, 3306, 7, 10, 8, 1817, 7, 10, 8, 1535, 7, 10, 8, 1461, 7, 10, 8, 2025, 7, 10, 8, 3305, 7, 10, 8, 940, 7, 10, 8, 1534, 7, 10, 8, 2795, 7, 10, 8, 1520, 7, 10, 8, 1593, 7, 10, 8, 2185, 9, 2], [1, 2122, 44, 121, 500, 1265, 32, 116, 3581, 89, 250, 5440, 84, 206, 662, 89, 32, 2494, 2518, 32, 398, 1595, 231, 178, 42, 7394, 8033, 160, 4, 215, 47, 4, 103, 4, 171, 44, 2098, 66, 5391, 8, 275, 73, 4, 457, 10, 6, 1898, 44, 3509, 2412, 33, 49, 79, 71, 7873, 160, 4, 795, 10, 6, 850, 44, 6899, 33, 1898, 10, 7, 676, 4, 86, 506, 1136, 1285, 33, 1121, 252, 33, 49, 2469, 33, 49, 39, 424, 86, 1120, 676, 4, 86, 190, 1136, 33, 113, 18, 5482, 356, 242, 192, 1041, 63, 676, 4, 86, 82, 1136, 1285, 33, 1121, 344, 16, 344, 74, 625, 16, 344, 74, 242, 16, 344, 74, 784, 16, 344, 74, 800, 16, 344, 86, 1120, 676, 4, 86, 82, 1136, 33, 113, 18, 1167, 733, 3262, 253, 63, 676, 4, 86, 217, 339, 33, 588, 41, 4, 3846, 5, 795, 10, 7, 1987, 1253, 8, 52, 813, 739, 4, 302, 5, 795, 10, 7, 49, 246, 160, 4, 2527, 80, 5, 2098, 275, 7, 49, 6899, 33, 850, 10, 7, 49, 5263, 620, 33, 49, 1942, 160, 4, 444, 44, 596, 29, 1015, 33, 49, 217, 339, 33, 409, 41, 2], [1, 1210, 124, 42, 196, 203, 4, 150, 8, 131, 53, 4, 123, 5, 97, 19, 445, 5, 112, 19, 62, 21, 4, 98, 114, 31, 121, 500, 883, 9, 332, 1050, 508, 432, 7, 1388, 3774, 93, 9152, 49, 51, 86, 69, 26, 2040, 4, 171, 31, 1831, 531, 8, 61, 486, 35, 4, 3056, 5, 1831, 531, 9, 4, 609, 6, 1005, 5, 6898, 2768, 419, 1472, 280, 9, 983, 1680, 4, 2693, 6, 1005, 5, 1231, 376, 419, 1472, 280, 9, 4, 244, 5, 528, 133, 19, 47, 5, 10, 7, 11, 21, 232, 29, 26, 736, 4, 46, 5, 6898, 2768, 419, 1472, 280, 7, 144, 140, 247, 17, 334, 140, 6898, 2768, 711, 8, 1472, 280, 17, 517, 19, 47, 139, 18, 8, 763, 7, 18, 8, 763, 23, 638, 19, 47, 139, 18, 8, 2793, 7, 18, 8, 2793, 21, 4, 360, 470, 5, 609, 9, 4, 46, 5, 1231, 376, 419, 1472, 280, 7, 144, 140, 247, 17, 334, 140, 1231, 376, 711, 8, 1472, 280, 17, 517, 19, 47, 139, 18, 8, 763, 7, 18, 8, 763, 23, 638, 19, 47, 139, 18, 8, 2793, 7, 18, 8, 2793, 21, 4, 360, 470, 5, 2693, 9, 2], [1, 1210, 124, 42, 196, 203, 4, 103, 135, 4, 215, 47, 4, 171, 44, 1831, 531, 8, 161, 73, 5903, 280, 6898, 2768, 2449, 179, 766, 246, 201, 3, 66, 173, 8493, 1231, 376, 4, 609, 6, 2286, 734, 8, 282, 5, 1831, 531, 101, 7, 10, 23, 1831, 531, 101, 7, 11, 268, 1483, 1680, 4, 3677, 6, 852, 8, 869, 125, 2695, 8, 963, 3157, 36, 4, 54, 6, 27, 18, 8, 11, 16, 18, 8, 204, 16, 18, 8, 11, 36, 4, 2693, 6, 2286, 734, 8, 282, 5, 1831, 531, 101, 7, 198, 23, 1831, 531, 101, 7, 11, 268, 4, 2799, 6, 5813, 8, 869, 125, 2695, 8, 963, 3157, 36, 4, 418, 5, 10, 7, 11, 7, 10, 9, 46, 29, 26, 736, 4, 46, 5, 1831, 531, 101, 7, 10, 23, 1831, 531, 101, 7, 11, 23, 86, 2800, 73, 4, 199, 44, 6898, 2768, 711, 8, 1472, 280, 74, 217, 339, 33, 314, 137, 217, 425, 74, 91, 437, 73, 4, 631, 188, 4, 98, 5, 99, 387, 137, 328, 373, 33, 10, 8, 145, 137, 217, 339, 33, 314, 137, 217, 425, 74, 91, 437, 73, 4, 517, 228, 20, 18, 8, 763, 7, 18, 8, 763, 83, 4, 470, 5, 54, 7, 3677, 5, 10, 473, 3677, 5, 11, 262, 54, 137, 620, 74, 75, 33, 49, 328, 373, 33, 10, 8, 52, 9, 4, 418, 5, 10, 7, 11, 7, 11, 9, 4, 46, 5, 1831, 531, 101, 7, 10, 23, 1831, 531, 101, 7, 198, 23, 86, 2800, 73, 4, 199, 44, 1231, 376, 711, 8, 1472, 280, 74, 217, 339, 33, 314, 137, 217, 425, 74, 91, 437, 73, 4, 638, 228, 20, 18, 8, 2793, 7, 18, 8, 2793, 83, 2]), ([1, 44, 29, 9, 598, 579, 10, 4, 1993, 6, 2665, 1557, 5, 3588, 2089, 292, 3352, 21, 447, 9, 579, 10, 4, 1993, 6, 2665, 1557, 15, 317, 1720, 80, 5, 374, 3792, 7, 1307, 76, 5, 261, 28, 289, 2], [1, 27, 9, 1091, 6, 1792, 1031, 1845, 2250, 1124, 339, 820, 518, 383, 33, 2], [1, 44, 118, 35, 6, 55, 782, 10, 133, 131, 927, 290, 239, 1554, 109, 21, 741, 12, 2886, 87, 44, 2], [1, 44, 118, 35, 6, 55, 782, 10, 133, 131, 927, 290, 239, 1554, 109, 21, 741, 12, 2886, 87, 44, 2]))\n"
     ]
    }
   ],
   "source": [
    "def separateSourceTarget(sequence_pairs):\n",
    "    \"\"\"\n",
    "    input of form: `[(X_1,y_1), (X_2,y_2), (X_3,y_3), (X_4,y_4)]`\n",
    "    output of form: `((X_1,X_2,X_3,X_4), (y_1,y_2,y_3,y_4))`\n",
    "    \"\"\"\n",
    "    sources,targets = zip(*sequence_pairs)\n",
    "    return sources,targets\n",
    "\n",
    "## Apply the function to each element in the iterator\n",
    "data_pipe = data_pipe.map(separateSourceTarget)\n",
    "print(list(data_pipe)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyPadding(pair_of_sequences):\n",
    "    \"\"\"\n",
    "    Convert sequences to tensors and apply padding\n",
    "    \"\"\"\n",
    "    return (T.ToTensor(0)(list(pair_of_sequences[0])), T.ToTensor(0)(list(pair_of_sequences[1])))\n",
    "## `T.ToTensor(0)` returns a transform that converts the sequence to `torch.tensor` and also applies\n",
    "# padding. Here, `0` is passed to the constructor to specify the index of the `<pad>` token in the\n",
    "# vocabulary.\n",
    "data_pipe = data_pipe.map(applyPadding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source:  <sos> rm ( list = ls ( all = ĠTRUE )) Ċ graphics . off () x Ġ= Ġc ( 1 . 72 , 1 . 83 , 1 . 74 , 1 . 79 , 1 . 94 , 1 . 71 , 1 . 66 , 1 . 60 , 1 . 78 , 1 . 77 , 1 . 85 , 1 . 70 , 1 . 76 ) <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Traget:  <sos> ' Computes Ġa Ġsimple Ġhistogram Ġfor Ġthe Ġheight Ġof Ġstud ents . ĠSM Sh is height s Ġcomputes Ġa Ġhistogram Ġfor Ġthe Ġheight Ġof Ġstud ents Ġwith Ġbin width Ġ0 . 05 Ġmeters Ġand Ġorigin Ġ1 . 6 m .' <eos>\n",
      "Source:  <sos> cd (' ĠC :/ ĠUsers / ĠF ran z is ka ĠS ch ul z / ĠDes ktop / ĠJ AS A ĠData Ġand ĠCommand ĠFiles '); Ċ cl c Ċ clear Ċ load (' ĠLoad ĠT SO . mat ') Ċ figure 1 Ġ= Ġfigure (' ĠPaper ĠType ', Ġ' a 4 letter '); Ċ axes 1 Ġ= Ġaxes (' ĠParent ', Ġfigure 1 , ... Ċ ' ĠZ ĠTick ĠLabel ', Ġ{' 100 ', Ġ' 250 ', Ġ' 3 50 ' }, ... Ċ ' ĠY ĠTick ', Ġ[ 0 Ġ365 Ġ7 12 Ġ10 95 ], ... Ċ ' ĠX ĠTick ĠLabel ', Ġ{' 00 : 00 ',' 06 : 00 ',' 12 : 00 ',' 18 : 00 ',' 24 : 00 ' }, ... Ċ ' ĠX ĠTick ', Ġ[ 0 Ġ25 Ġ50 Ġ75 Ġ100 ], ... Ċ ' ĠFont ĠSize ', Ġ14 ); Ċ view ( axes 1 , Ġ[- 37 . 5 Ġ30 ]); Ċ grid ( axes 1 , Ġ' on '); Ċ sur f ( ĠLoad mat , Ġ' ĠParent ', Ġaxes 1 , Ġ' ĠEdge ĠColor ', Ġ' none '); Ċ xlabel (' ĠTime Ġof Ġday ', Ġ' ĠFont ĠSize ', Ġ20 ); <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Traget:  <sos> Plots Ġa Ġsurface Ġof Ġintr ad aily Ġelect ric ity Ġload Ġcurves Ġover Ġtime <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Source:  <sos> Clear Ġvariables Ġand Ġclose Ġwindows Ċ graphics . off () Ċ rm ( list = ls ( all = ĠTRUE )) Ċ set wd (\" ĠC :/ ...\" ) Ġset Ġyour Ġworking Ġdirectory , Ġcreate Ġthere Ġa Ġsubdirectory Ġ' data ' Ġfor Ġthe Ġdatasets Ċ load (\" ny se . r da \") Ċ attach ( ny se ) Ċ pi Ġ= Ġlm ( ĠPan ĠAm ~ ĠIB M ) Ġlinear Ġmodels Ċ di Ġ= Ġlm ( ĠDE C ~ ĠIB M ) Ċ par ( mf row = c ( 1 , 2 )) Ġplot Ġof Ġthe Ġresults Ċ plot ( ĠPan ĠAm ~ ĠIB M , col =\" blue \", main =\" ĠPan ĠAm Ġvs . ĠIB M \", xlim = c (- 0 . 19 , 0 . 19 ), ylim = c (- 0 . 41 , 0 . 41 )) Ċ ab line ( pi ) Ċ plot ( ĠDE C ~ ĠIB M , col =\" blue \", main =\" ĠDE C Ġvs . ĠIB M \", xlim = c (- 0 . 19 , 0 . 19 ), ylim = c (- 0 . 41 , 0 . 41 )) Ċ ab line ( di ) <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Traget:  <sos> ' Produces Ġplot Ġof Ġregression Ġlines Ġfor Ġ3 ĠN Y SE Ġtra ded Ġstock s Ġaccording Ġto ĠCAP M ' <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Source:  <sos> Clear Ġvariables Ġand Ġclose Ġwindows Ċ clear Ġall Ċ cl c Ċ load (' ny se . dat ') IB M ĠPan ĠAm ĠDelta ĠE dis on ĠG <unk> ĠT ex aco ĠDE C Ċ pi Ġ= ĠLinear ĠModel . fit ( ny se (: , 1 ), ny se (: , 2 )); linear Ġmodels Ċ bp Ġ= Ġpi . ĠCo e fficients . ĠEst imate ; Ċ t Ġ= Ġ- 0 . 2 : 0 . 05 : 0 . 2 ; Ċ di Ġ= ĠLinear ĠModel . fit ( ny se (: , 7 ), ny se (: , 2 )); Ċ bd Ġ= Ġdi . ĠCo e fficients . ĠEst imate ; Ċ subplot ( 1 , 2 , 1 ) plot Ġof Ġthe Ġresults Ċ plot ( ny se (: , 1 ), ny se (: , 2 ), ' bo ') Ċ title (' ĠPan ĠAm Ġvs . ĠIB M ',' ĠFont ĠSize ', 16 ,' ĠFont ĠWeight ',' ĠB old ') Ċ box Ġon Ċ set ( g ca ,' ĠLine ĠWidth ', 1 . 6 ,' ĠFont ĠSize ', 16 ,' ĠFont ĠWeight ',' ĠB old ') Ċ xlim ([ - 0 . 19 , 0 . 19 ]) Ċ line ( t , bp ( 1 )+ bp ( 2 )* t ,' ĠColor ',' k ', Ġ' ĠLine ĠWidth ', 1 . 5 ) Ċ subplot ( 1 , 2 , 2 ) Ċ plot ( ny se (: , 1 ), ny se (: , 7 ), ' bo ') Ċ title (' ĠDE C Ġvs . ĠIB M ',' ĠFont ĠSize ', 16 ,' ĠFont ĠWeight ',' ĠB old ') Ċ ylim ([ - 0 . 41 , 0 . 41 ]) <eos>\n",
      "Traget:  <sos> ' Produces Ġplot Ġof Ġregression Ġlines Ġfor Ġ3 ĠN Y SE Ġtra ded Ġstock s Ġaccording Ġto ĠCAP M ' <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "source_index_to_string = source_vocab.get_itos()\n",
    "target_index_to_string = target_vocab.get_itos()\n",
    "\n",
    "def showSomeTransformedSentences(data_pipe):\n",
    "    \"\"\"\n",
    "    Function to show how the sentences look like after applying all transforms.\n",
    "    Here we try to print actual words instead of corresponding index\n",
    "    \"\"\"\n",
    "    for sources,targets in data_pipe:\n",
    "        if sources[0][-1] != 0:\n",
    "            continue # Just to visualize padding of shorter sentences\n",
    "        for i in range(4):\n",
    "            source = \"\"\n",
    "            for token in sources[i]:\n",
    "                source += \" \" + source_index_to_string[token]\n",
    "            target = \"\"\n",
    "            for token in targets[i]:\n",
    "                target += \" \" + target_index_to_string[token]\n",
    "            print(f\"Source: {source}\")\n",
    "            print(f\"Traget: {target}\")\n",
    "        break\n",
    "\n",
    "showSomeTransformedSentences(data_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dl = DataLoader(dataset=data_pipe, batch_size=5, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 42, in fetch\n    return self.collate_fn(data)\n  File \"/home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 265, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 142, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 142, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 119, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n  File \"/home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 161, in collate_tensor_fn\n    out = elem.new(storage).resize_(len(batch), *list(elem.size()))\nRuntimeError: Trying to resize storage that is not resizable\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/RDC/zinovyee.hub/H:/zinovyee.hub/IRTG/MLSC/Encode-the-Qode/Quantlet/seq2seq/seq1seq.ipynb Zelle 29\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bsunflower.wiwi.hu-berlin.de/home/RDC/zinovyee.hub/H%3A/zinovyee.hub/IRTG/MLSC/Encode-the-Qode/Quantlet/seq2seq/seq1seq.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m first \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(\u001b[39miter\u001b[39;49m(dl))\n",
      "File \u001b[0;32m~/.conda/envs/encode_code/lib/python3.9/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.conda/envs/encode_code/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1345\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1343\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1344\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1345\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_data(data)\n",
      "File \u001b[0;32m~/.conda/envs/encode_code/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1371\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_put_index()\n\u001b[1;32m   1370\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1371\u001b[0m     data\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m   1372\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.conda/envs/encode_code/lib/python3.9/site-packages/torch/_utils.py:694\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    691\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    692\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    693\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m--> 694\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 42, in fetch\n    return self.collate_fn(data)\n  File \"/home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 265, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 142, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 142, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 119, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n  File \"/home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 161, in collate_tensor_fn\n    out = elem.new(storage).resize_(len(batch), *list(elem.size()))\nRuntimeError: Trying to resize storage that is not resizable\n"
     ]
    }
   ],
   "source": [
    "first = next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "SRC = Field(\n",
    "    tokenize = tokenize_code, \n",
    "    init_token = '<sos>', \n",
    "    eos_token = '<eos>', \n",
    "    lower = True,\n",
    "    include_lengths = True\n",
    ")\n",
    "\n",
    "TRG = Field(\n",
    "    tokenize = tokenize_summary, \n",
    "    init_token = '<sos>', \n",
    "    eos_token = '<eos>', \n",
    "    lower = True\n",
    ")\n",
    "\n",
    "fields = {\n",
    "    'code_script': ('src', SRC),\n",
    "    'Description': ('trg', TRG)\n",
    "}\n",
    "\n",
    "train_data, valid_data, test_data = TabularDataset.splits(\n",
    "                            path = f'../../data/preprocessed/Quantlet/{DATE}/',\n",
    "                            train = f\"train_df_{DATE}_sample0.csv\",\n",
    "                            validation = f\"val_df_{DATE}_sample0.csv\",\n",
    "                            test = f\"test_df_{DATE}_sample0.csv\",\n",
    "                            format = 'csv',\n",
    "                            fields = fields\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 160783), ('(', 155363), (')', 153066), ('=', 132957), ('.', 94494), ('\"', 69028), ('[', 41935), (']', 41715), ('1', 41517), ('-', 33500), (':', 26800), ('0', 22978), ('2', 21118), (\"'\", 19160), (';', 18083), ('x', 17087), ('/', 13954), ('+', 13534), ('i', 12632), ('data', 12200)]\n",
      "[('the', 6010), ('of', 3291), ('and', 2544), ('a', 1716), ('for', 1585), ('to', 1078), ('data', 843), ('with', 750), ('in', 693), ('is', 671), ('from', 596), ('on', 512), ('are', 424), ('by', 391), ('m', 334), ('time', 329), ('plot', 315), ('as', 284), ('an', 245), ('this', 239)]\n",
      "Unique tokens in code: 5429\n",
      "Unique tokens in descriptions: 478\n"
     ]
    }
   ],
   "source": [
    "SRC.build_vocab([train_data.src], max_size=25000, min_freq=3)\n",
    "print(SRC.vocab.freqs.most_common(20))\n",
    "\n",
    "\n",
    "TRG.build_vocab([train_data.trg], min_freq=5)\n",
    "print(TRG.vocab.freqs.most_common(20))\n",
    "\n",
    "print(f\"Unique tokens in code: {len(SRC.vocab)}\")\n",
    "print(f\"Unique tokens in descriptions: {len(TRG.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SRC' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/RDC/zinovyee.hub/H:/zinovyee.hub/IRTG/MLSC/Encode-the-Qode/Quantlet/seq2seq/seq1seq.ipynb Zelle 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bsunflower.wiwi.hu-berlin.de/home/RDC/zinovyee.hub/H%3A/zinovyee.hub/IRTG/MLSC/Encode-the-Qode/Quantlet/seq2seq/seq1seq.ipynb#Y120sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m SRC\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SRC' is not defined"
     ]
    }
   ],
   "source": [
    "SRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "        batch_size = BATCH_SIZE,\n",
    "        sort_within_batch = True,\n",
    "        sort_key = lambda x : len(x.src),\n",
    "        device = device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "ENC_EMB_DIM = 128\n",
    "DEC_EMB_DIM = 128\n",
    "ENC_HID_DIM = 100\n",
    "DEC_HID_DIM = 100\n",
    "ENC_DROPOUT = 0.8\n",
    "DEC_DROPOUT = 0.8\n",
    "PAD_IDX = SRC.vocab.stoi['<pad>']\n",
    "SOS_IDX = TRG.vocab.stoi['<sos>']\n",
    "EOS_IDX = TRG.vocab.stoi['<eos>']\n",
    "\n",
    "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
    "\n",
    "model = Seq2Seq(enc, dec, PAD_IDX, SOS_IDX, EOS_IDX, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель содержит 1,278,458 параметров\n"
     ]
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "            \n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'Модель содержит {count_parameters(model):,} параметров')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.apply(init_weights)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sm_37', 'sm_50', 'sm_60', 'sm_70']\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_arch_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    print(epoch)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'conala_model_attention_test.pt')\n",
    "    \n",
    "    print(f'Эпоха: {epoch+1:02} | Время: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'Перплексия (обучение): {math.exp(train_loss):7.3f}')\n",
    "    print(f'Перплексия (валидация): {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('conala_model_attention_test.pt'))\n",
    "\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Перплексия (валидация): {math.exp(test_loss):7.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "encode_code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JZal6ahJZQBU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "#%pip install protobuf==3.20.1\n",
    "%pip install transformers[torch]\n",
    "%pip install -q sentencepiece\n",
    "%pip install datasets==2.13.1\n",
    "%pip install evaluate\n",
    "%pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "O5Mlzgdzaliu",
    "tags": []
   },
   "outputs": [],
   "source": [
    "QPATH = \"Quantlet/5-domain-pre-training\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "iaAT_m3NaEzp",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../4-qode2desc\")\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "import os\n",
    "\n",
    "if IN_COLAB:\n",
    "    os.chdir(\n",
    "        f\"/content/drive/MyDrive/ColabNotebooks/IRTG/Encode_the_Qode/Encode-the-Qode/{QPATH}\"\n",
    "    )\n",
    "else:\n",
    "    %load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "oRmp1O7SZgaI",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/RDC/zinovyee.hub/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import json\n",
    "import re\n",
    "import sys\n",
    "from IPython.display import display\n",
    "import datetime\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead, SummarizationPipeline\n",
    "from transformers import AdamW\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "import evaluate\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "kM604UgeBuMl",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/RDC/zinovyee.hub/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/RDC/zinovyee.hub/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'analysis_modules' from '/usr/net/zinovyee.hub/IRTG/MLSC/Encode-the-Qode/Quantlet/5-domain-pre-training/../4-qode2desc/analysis_modules.py'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import analysis_modules\n",
    "\n",
    "importlib.reload(analysis_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "CbZX3Y3_q1pw",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeT5-4-300-15-4-202310145_1\n"
     ]
    }
   ],
   "source": [
    "model_name = \"CodeT5\"\n",
    "\n",
    "DATE = \"202310145\"\n",
    "\n",
    "SAMPLE = \"test\"\n",
    "if SAMPLE == \"test\":\n",
    "    load_best_model_at_end = False\n",
    "else:\n",
    "    load_best_model_at_end = None\n",
    "\n",
    "# tokenization\n",
    "encoder_max_length = 300\n",
    "decoder_max_length = 30\n",
    "RS = 1\n",
    "LR = 5e-5\n",
    "\n",
    "EPOCHS = 4\n",
    "TRAIN_BATCH = 4\n",
    "EVAL_BATCH = 4\n",
    "\n",
    "WARMUP_STEPS = 500\n",
    "WEIGHT_DECAY = 0.1\n",
    "LOGGING_STEPS = 100\n",
    "SAVE_TOTAL_LIM = 4\n",
    "SAVE_STRATEGY = \"steps\"\n",
    "\n",
    "LABEL_SMOOTHING = 0.1\n",
    "PREDICT_GENERATE = True\n",
    "\n",
    "EVAL_COLUMNS = [\n",
    "    \"eval_loss\",\n",
    "    \"eval_rouge1\",\n",
    "    \"eval_rouge2\",\n",
    "    \"eval_rougeL\",\n",
    "    \"eval_rougeLsum\",\n",
    "    \"eval_bleu\",\n",
    "    \"eval_gen_len\",\n",
    "]\n",
    "\n",
    "\n",
    "analysis_name = (\n",
    "    model_name\n",
    "    + \"-\"\n",
    "    + str(EPOCHS)\n",
    "    + \"-\"\n",
    "    + str(encoder_max_length)\n",
    "    + \"-\"\n",
    "    + str(decoder_max_length)\n",
    "    + \"-\"\n",
    "    + str(TRAIN_BATCH)\n",
    ")\n",
    "\n",
    "analysis_name = analysis_name + \"-\" + DATE + \"_\" + str(RS)\n",
    "\n",
    "print(analysis_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "FRHaw5S7X81D",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "9t6d_nrX_lyy",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeT5-4-300-15-4-202310145_1\n",
      "CodeT5-4-300-15-4-202310145_1\n",
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/RDC/zinovyee.hub/.local/lib/python3.9/site-packages/transformers/models/auto/modeling_auto.py:1322: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/RDC/zinovyee.hub/.cache/huggingface/datasets/json/default-a416704e79b803c8/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "050c8f8dfe8a4aa482b06f935d8078d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/RDC/zinovyee.hub/.cache/huggingface/datasets/json/default-0766eb954d4fd3d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ef9f2c536534e92a7c7d5457562b61c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/RDC/zinovyee.hub/.cache/huggingface/datasets/json/default-a416704e79b803c8/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-04291db9508c5eaa.arrow\n",
      "Loading cached processed dataset at /home/RDC/zinovyee.hub/.cache/huggingface/datasets/json/default-0766eb954d4fd3d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-07c1f89849915b84.arrow\n",
      "/home/RDC/zinovyee.hub/.local/lib/python3.9/site-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4214' max='2107' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2107/2107 2:08:38]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   eval_loss  eval_rouge1  eval_rouge2  eval_rougeL  eval_rougeLsum  \\\n",
      "0      5.805         0.15        0.044        0.133           0.133   \n",
      "\n",
      "   eval_bleu  eval_gen_len  \n",
      "0      0.003        18.456  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/RDC/zinovyee.hub/.local/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19664' max='19664' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19664/19664 1:42:45, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>5.304500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>4.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>4.124100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.826600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.798500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.613500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>3.610500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>3.522400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>3.507200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.456200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>3.464600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>3.479900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>3.409900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>3.343600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.349600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>3.347900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>3.328400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>3.320500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>3.397600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>3.268000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>3.241500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>3.237000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>3.270500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>3.161700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>3.170200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>3.228000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>3.156900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>3.178100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>3.140600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>3.222400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>3.187900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>3.127800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>3.133200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>3.163500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>3.128500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>3.111900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>3.071400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>3.140200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>3.116500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>3.100200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>3.040500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>3.116500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>3.149000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>3.099200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>3.056600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>3.068900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>3.016700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>3.096600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>3.067000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.919900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>2.872000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>2.867900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>2.882100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>2.855800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>2.900200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>2.854400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>2.891400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>2.847800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>2.884000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>2.846300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>2.856500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>2.818200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>2.883700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>2.815900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>2.806700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>2.848400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>2.848800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>2.905500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>2.773000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>2.823700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>2.932100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>2.803400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>2.798500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>2.826100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>2.830400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>2.823200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>2.786400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>2.792500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>2.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>2.892700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>2.776700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>2.793200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>2.784900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>2.785400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>2.813400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>2.838300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>2.805300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>2.759000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8900</td>\n",
       "      <td>2.795400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>2.798300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9100</td>\n",
       "      <td>2.801500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>2.787100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9300</td>\n",
       "      <td>2.810900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>2.729500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>2.842000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>2.739900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9700</td>\n",
       "      <td>2.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>2.775200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9900</td>\n",
       "      <td>2.643800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.611700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10100</td>\n",
       "      <td>2.631600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>2.580300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10300</td>\n",
       "      <td>2.583700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>2.603100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>2.614300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>2.563000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10700</td>\n",
       "      <td>2.580200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>2.604100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10900</td>\n",
       "      <td>2.626300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.607200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11100</td>\n",
       "      <td>2.609500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>2.623800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11300</td>\n",
       "      <td>2.597200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11400</td>\n",
       "      <td>2.582100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>2.612800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11600</td>\n",
       "      <td>2.590300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11700</td>\n",
       "      <td>2.606200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11800</td>\n",
       "      <td>2.527000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11900</td>\n",
       "      <td>2.655100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>2.614000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12100</td>\n",
       "      <td>2.609400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12200</td>\n",
       "      <td>2.593300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12300</td>\n",
       "      <td>2.569400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12400</td>\n",
       "      <td>2.595400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>2.591400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12600</td>\n",
       "      <td>2.549000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12700</td>\n",
       "      <td>2.570900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12800</td>\n",
       "      <td>2.517900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12900</td>\n",
       "      <td>2.613600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>2.615300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13100</td>\n",
       "      <td>2.552900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13200</td>\n",
       "      <td>2.603000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13300</td>\n",
       "      <td>2.582700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13400</td>\n",
       "      <td>2.568600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>2.573400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13600</td>\n",
       "      <td>2.561700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13700</td>\n",
       "      <td>2.565000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13800</td>\n",
       "      <td>2.554800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13900</td>\n",
       "      <td>2.547600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>2.531500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14100</td>\n",
       "      <td>2.572400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14200</td>\n",
       "      <td>2.570000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14300</td>\n",
       "      <td>2.537100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14400</td>\n",
       "      <td>2.555400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>2.544300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14600</td>\n",
       "      <td>2.548400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14700</td>\n",
       "      <td>2.547800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14800</td>\n",
       "      <td>2.476100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14900</td>\n",
       "      <td>2.425200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>2.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15100</td>\n",
       "      <td>2.425600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15200</td>\n",
       "      <td>2.412000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15300</td>\n",
       "      <td>2.402400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15400</td>\n",
       "      <td>2.418300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>2.429800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15600</td>\n",
       "      <td>2.397300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15700</td>\n",
       "      <td>2.481700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15800</td>\n",
       "      <td>2.395400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15900</td>\n",
       "      <td>2.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>2.429600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16100</td>\n",
       "      <td>2.411400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16200</td>\n",
       "      <td>2.447300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16300</td>\n",
       "      <td>2.431600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16400</td>\n",
       "      <td>2.457100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>2.426900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16600</td>\n",
       "      <td>2.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16700</td>\n",
       "      <td>2.396800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16800</td>\n",
       "      <td>2.446700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16900</td>\n",
       "      <td>2.434700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>2.416200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17100</td>\n",
       "      <td>2.462000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17200</td>\n",
       "      <td>2.423100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17300</td>\n",
       "      <td>2.438700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17400</td>\n",
       "      <td>2.393100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>2.399800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17600</td>\n",
       "      <td>2.424500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17700</td>\n",
       "      <td>2.397000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17800</td>\n",
       "      <td>2.453700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17900</td>\n",
       "      <td>2.426700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>2.433900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18100</td>\n",
       "      <td>2.387100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18200</td>\n",
       "      <td>2.444200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18300</td>\n",
       "      <td>2.406800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18400</td>\n",
       "      <td>2.432800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>2.396700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18600</td>\n",
       "      <td>2.408600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18700</td>\n",
       "      <td>2.432600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18800</td>\n",
       "      <td>2.409100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18900</td>\n",
       "      <td>2.438700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>2.448300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19100</td>\n",
       "      <td>2.414800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19200</td>\n",
       "      <td>2.422900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19300</td>\n",
       "      <td>2.440900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19400</td>\n",
       "      <td>2.378100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>2.454800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19600</td>\n",
       "      <td>2.433900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/RDC/zinovyee.hub/.conda/envs/encode_code/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   eval_loss  eval_rouge1  eval_rouge2  eval_rougeL  eval_rougeLsum  \\\n",
      "0      2.697        0.483        0.305         0.45            0.45   \n",
      "\n",
      "   eval_bleu  eval_gen_len  \n",
      "0      0.192        13.868  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/RDC/zinovyee.hub/.local/lib/python3.9/site-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________\n",
      "Original: Comparing Different Deep Learning Architectures for Classification of Chest Radiographs\n",
      "Summary before Tuning: This function is used to classify chest radiographs.\n",
      "Summary after Tuning: Deep Learning on Chest Radiographs: A Compar\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: TGHop: An Explainable, Efficient and Lightweight Method for Texture Generation\n",
      "Summary before Tuning: This method is a lightweight method that is used to generate a high quality texture for a\n",
      "Summary after Tuning: TGHop: An Explainable Texture Generation PixelHop++\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Unidentified Floating Object detection in maritime environment using dictionary learning\n",
      "Summary before Tuning: Maritime domain is one of the most challenging scenarios for the Mar\n",
      "Summary after Tuning: Unidentified Floating Objects in Maritime using K\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Understanding Graph Neural Networks from Graph Signal Denoising Perspectives\n",
      "Summary before Tuning: Graph neural networks have attracted much attention on tasks such as node classification and a\n",
      "Summary after Tuning: Understanding Graph Neural Networks from Graph Signal Denoising P\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Self-Supervised Regional and Temporal Auxiliary Tasks for Facial Action Unit Recognition\n",
      "Summary before Tuning: A simple simple auto - facial action recognition for a single - image AUS.\n",
      "Summary after Tuning: Self-supervised Regional and Temporal Auxiliary Tasks for\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: SE(3)-Transformers: 3D Roto-Translation Equivariant Attention Networks\n",
      "Summary before Tuning: This variant is a variant of the SE module that is a self - attention module that is\n",
      "Summary after Tuning: SE(3)-Transformers: 3D Roto-Translation\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Articulated Shape Matching Using Laplacian Eigenfunctions and Unsupervised Point Registration\n",
      "Summary before Tuning: This is a utility method that can be used to map a set of non - identical shapes\n",
      "Summary after Tuning: Spectral Graph Isomorphism for Shape Matching\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Entropic Risk Constrained Soft-Robust Policy Optimization\n",
      "Summary before Tuning: This is a very basic method that is used to compute the optimal policy for a given unc\n",
      "Summary after Tuning: Entropic Risk Constrained Policy Gradient and Actor-C\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Long-Short Transformer: Efficient Transformers for Language and Vision\n",
      "Summary before Tuning: Transformer - LS is applied to all tasks in both language and vision domains.\n",
      "Summary after Tuning: Long-Short Transformer: Efficient Transformer for Language and Vision\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: DROCC: Deep Robust One-Class Classification\n",
      "Summary before Tuning: Classical approaches for one - class problems that require careful feature engineering when applied\n",
      "Summary after Tuning: DROCC: Deep Robust One-Class Classification\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: A hybrid approach of interpolations and CNN to obtain super-resolution\n",
      "Summary before Tuning: This is a novel architecture that learns a novel architecture that learns a no\n",
      "Summary after Tuning: Deep Neural Networks for Natural Image Super-Resolution\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Explainable cardiac pathology classification on cine MRI with motion characterization by semi-supervised learning of apparent flow\n",
      "Summary before Tuning: This method is a simple method to classify a sequence of images based on a novel approach\n",
      "Summary after Tuning: Explainable cardiac pathology classification using a semi-\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: An End-to-end Framework For Low-Resolution Remote Sensing Semantic Segmentation\n",
      "Summary before Tuning: This method is intended to be used by remote - sensing applications to provide a high -\n",
      "Summary after Tuning: Semantic Segmentation for Remote Sensing Images from Satell\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Multiple Exemplars-based Hallucinationfor Face Super-resolution and Editing\n",
      "Summary before Tuning: This method is used to reconstruct a high - frequency version of a image that is not yet\n",
      "Summary after Tuning: Face Super-Resolution Using Pixel-wise Weight Generation\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: The Location of Optimal Object Colors with More Than Two Transitions (Preprint)\n",
      "Summary before Tuning: The CIE 1931 diagram is shown to be slightly non - convex.\n",
      "Summary after Tuning: A peer-reviewed version of the color-matching functions for\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Actions Generation from Captions\n",
      "Summary before Tuning: This method is intended to be used by the sequence transduction models.\n",
      "Summary after Tuning: Action Generation Data Set for Captions\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Deep Preset: Blending and Retouching Photos with Color Style Transfer\n",
      "Summary before Tuning: This is a no - op for all end - users with knowledge in photography.\n",
      "Summary after Tuning: Deep Preset: Learning Image Style Transfer for End-User\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: CommPOOL: An Interpretable Graph Pooling Framework for Hierarchical Graph Representation Learning\n",
      "Summary before Tuning: This method is used to propose a hierarchical\n",
      "representation pooling framework that can be used\n",
      "Summary after Tuning: CommPOOL: An Interpretable Pooling Framework for Hierarchical Graph\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Helicopter Track Identification with Autoencoder\n",
      "Summary before Tuning: This is a helper function for the basic case of a sequence of has - tokens. It\n",
      "Summary after Tuning: Autoencoder for Unsupervised Learning of Helicopt\n",
      "__________\n",
      "\n",
      "\n",
      "__________\n",
      "Original: Deeply learning molecular structure-property relationships using attention- and gate-augmented graph convolutional network\n",
      "Summary before Tuning: This method is used to predict molecular properties with a sequence - property relationship.\n",
      "Summary after Tuning: Learning to Predict Molecular Structure-Property with Attention and\n",
      "__________\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(analysis_name)\n",
    "analysis_modules.scs_analyze(\n",
    "    analysis_name=analysis_name,\n",
    "    model_name=model_name,\n",
    "    train_data_path=f\"../../data/preprocessed/ArXiv/\",\n",
    "    train_data_name=f\"arxiv_pretrain_train.json\",\n",
    "    val_data_path=f\"../../data/preprocessed/ArXiv/\",\n",
    "    val_data_name=f\"arxiv_pretrain_test.json\",\n",
    "    encoder_max_length=encoder_max_length,\n",
    "    decoder_max_length=decoder_max_length,\n",
    "    random_state=RS,\n",
    "    eval_columns_list=EVAL_COLUMNS,\n",
    "    learning_rate=LR,\n",
    "    epochs=EPOCHS,\n",
    "    train_batch=TRAIN_BATCH,\n",
    "    eval_batch=EVAL_BATCH,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    logging_stes=LOGGING_STEPS,\n",
    "    save_total_lim=SAVE_TOTAL_LIM,\n",
    "    save_strategy=SAVE_STRATEGY,\n",
    "    label_smooting=LABEL_SMOOTHING,\n",
    "    predict_generate=PREDICT_GENERATE,\n",
    "    load_best_model_at_end=load_best_model_at_end,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "smrdGzzNyo5n",
    "tags": []
   },
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPZowVhs+ksZzaIPG80pu3+",
   "gpuType": "A100",
   "machine_shape": "hm",
   "mount_file_id": "119XCh3Q64Zw4MGlsZQuGrMLS78o5MeWS",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "encode_code",
   "language": "python",
   "name": "encode_code"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

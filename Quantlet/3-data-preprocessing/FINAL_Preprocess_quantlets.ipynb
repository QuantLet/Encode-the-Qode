{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jd7S7ZIIS6Fn"
   },
   "source": [
    "################################################################################\n",
    "# 1. Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "QjyD6T4D1XM4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "g4K0YWK11RjR",
    "tags": []
   },
   "outputs": [],
   "source": [
    "QPATH = \"Quantlet/3-data-preprocessing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Xz1ksIsd1SYO",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PREPARE WORKING DIRECTORY\n",
    "\n",
    "import sys\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "import os\n",
    "\n",
    "if IN_COLAB:\n",
    "    os.chdir(\n",
    "        f\"/content/drive/MyDrive/ColabNotebooks/IRTG/Encode_the_Qode/Encode-the-Qode/{QPATH}\"\n",
    "    )\n",
    "else:\n",
    "    os.chdir(\"./\")\n",
    "\n",
    "# sys.path.append('../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "TNkVUfU21UsR",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/RDC/zinovyee.hub/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# PACKAGES\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "import re\n",
    "import sys\n",
    "from IPython.display import display\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "import importlib\n",
    "import preprocessing_utils\n",
    "\n",
    "importlib.reload(preprocessing_utils)\n",
    "from preprocessing_utils import *\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from Levenshtein import distance\n",
    "\n",
    "# SETTINGS\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46Fkzpboq6sy"
   },
   "source": [
    "# 2. Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATE = \"20231021\"\n",
    "RS = 111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\n",
    "    f\"../../data/preprocessed/Quantlet/20231014/Quantlets_20231014.pkl\", \"rb\"\n",
    ") as file:\n",
    "    df = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4837, 6)\n",
      "(4836, 12)\n"
     ]
    }
   ],
   "source": [
    "df_long = df_metainfo_parse(df=df, prepare_script=True, remove_other=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4836/4836 [00:00<00:00, 503391.43it/s]\n",
      "100%|██████████| 4836/4836 [01:12<00:00, 66.80it/s]  \n",
      "100%|██████████| 4836/4836 [00:00<00:00, 817427.83it/s]\n",
      "100%|██████████| 4836/4836 [00:00<00:00, 10170.17it/s]\n",
      "100%|██████████| 4836/4836 [00:00<00:00, 831945.13it/s]\n",
      "100%|██████████| 4836/4836 [00:01<00:00, 3014.17it/s]\n",
      "100%|██████████| 4836/4836 [00:00<00:00, 673226.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4828, 15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_long = clean_up(df_long)\n",
    "print(df_long.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       close all\\ngraph = 1;\\n% Open & read txt file with names of files to work with\\nfilelist = 'filelist.txt';\\nfilenames = textread(filelist,'%s');\\nnfiles = length(filenames);\\ndisp([filelist, ' has ', num2str(nfiles), ' files.']);\\nfigcount = 0;\\n% Processing txt files\\nfor i = 1:nfiles\\n% checking what kind of file (single or mapping) it is\\nfilename = char(filenames(i));\\nfprintf('%s: single file\\n', filename);\\n[x,y] = textread(filename,'%f%f','headerlines',1); % Reads in txt file\\ndata = ...\n",
       "1       #########################################\\n## visualization of simulation results ##\\n#########################################\\n\\npar(bg=\"transparent\")\\n#-------------------------------------------------------------------------------\\n# stacked bar plot of spreads and fees\\nplot_spreads_fees <- function(){\\npar(bg=\"transparent\")\\n#aggregate spreads and fees\\n#take 95% quantile\\nfees <- aggregate(step3$costs, by=list(step3$date),\\nfunction(x)quantile(x,probs = 0.95))\\n\\nspreads <- aggregate(...\n",
       "2       #################################\\n### Understanding Crypto ETFs ###\\n#################################\\n#set directory\\n#setwd(\"path/directory\")\\n#load packages\\nlibraries = c( \"plyr\",\"dplyr\", \"ggplot2\", \"viridis\")\\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\\ninstall.packages(x)\\n})\\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\\n#############\\n#load data##\\n# crix time series\\ncrix.ts <- read.csv(\"new_crix.csv\")\\ncrix.ts$date <- as.Date(crix.ts$...\n",
       "3       # problem: the tickers/names of the cryptocurrencies of the two data sets\\n#(crix.weights and CCs prices) do not match. Needed to be revalued manually. :(\\n\\n#structure: \"CRIX_constituent_ticker\" = \"name of constituent in CC data set\"\\n# e.g.\"ada\"=\"cardano\"\\n#CRIX weights\\n#crix.weights <- read.csv(\".csv\") #read CRIX consituents file\\n#crix.weights$date <- as.Date(crix.weights$date, format=\"%Y-%m-%d\")\\n#crix.weights$index_name <- NULL\\n#names(crix.weights) <- c(\"date\", \"index_members\", \"weig...\n",
       "4       library(\"evd\")\\nm = 10000\\nn_large = 10000\\nx_c=c( rep(0, m) )\\nfor (i in 1:m){\\nx=max(rnorm(n))\\nx_large=max(rnorm(n_large))\\nx_c[i]=x\\nx_c_large[i]=x_large\\n}\\nmu_n=qnorm(1-(1/n))\\nsigma_n=qnorm(1-(1/n)*exp(-1))-mu_n\\nmu_n_l=qnorm(1-(1/n_large))\\nsigma_n_l=qnorm(1-(1/n_large)*exp(-1))-mu_n_l\\nx=sort(x_c)\\ny=sort(rgumbel(m,mu_n, sigma_n))\\nx_large=sort(x_c_large)\\ny_large=sort(rgumbel(m,mu_n_l, sigma_n_l))\\nplot(x, y, pch=16, col=\"steelblue\", xlim=c(1,7),\\npoints(x_large, y_large, pch=16, c...\n",
       "                                                                                                                                                                                                                                                               ...                                                                                                                                                                                                                                                         \n",
       "4823    # This script calculates the column percentages of the cross tabulation between\\n# the communities and the categorical varible for each year and combines it\\n# into one dotchart. Statistical test are performed to determine whether there\\n# are significant changes in the respondents across the three years.\\nlibrary(\"ggplot2\")\\n# The GetCrossTab function calculates the column percentages of the\\n# crosstabulation between the communites and the categorical variable\\nGetCrossTab <- function(vars...\n",
       "4824    # This script creates heatmaps of the missing values and the cases and\\n# variables that are removed in all three data sets.\\nlibrary(foreign)\\n# Replace values where respondent didn't give an informative answer\\nSubNA1 <- function(x){\\n# Args:\\n# x: variable\\n\\nx <- gsub(\"8\", NA, x)\\nx\\n}\\nSubNA2 <- function(x){\\nx <- gsub(\"7\", NA, x)\\nyear08 <- read.spss(\"knightfoundation2008sotcdata.por\",\\nuse.value.labels = FALSE,\\nyear09 <- read.spss(\"knightfoundation2009sotcdata.por\",\\ncolnames(year08)...\n",
       "4825    # Soul of the Community: An Attempt to Assess Attachment to a Community\\n# Anna Quach, Juergen Symanzik, and Nicole Forsgren\\n# 3/19/2015\\n#\\n# To produce a pdf file of the manuscript follow these steps:\\n# 1) Set your working directory to where all the R scripts are saved.\\n# 1) Run 01_data_cleaning.R first to produce the final data set which most of\\n# the scripts will depend on.\\n# 2) Run any of these scripts in any order to produce the figures in the\\n# manuscript:\\n#\\n# 02_remove_cases_...\n",
       "4826    # This script runs random forests only on Gary, Indiana. It then plots a\\n# dotchart of the variable importance from random forests then creates\\n# a parallel coordinate plot of the top 12 raw important variables for each\\n# of the three years.\\nlibrary(RColorBrewer)\\nlibrary(GGally)\\nlibrary(randomForest)\\nlibrary(plyr)\\n# GetPCP takes the raw data and the variables interested in to be plotted in\\n# the parallel coordinate plot\\nGetPCP <- function(data, myvars, mylevels, mycolors, mytitle){...\n",
       "4827    # This script (only needs to be ran once) goes through the data cleaning steps\\n# and outputs our final data sets.\\nlibrary(foreign)\\n# Replace values where respondent didn't give an informative answer\\nSubNA1 <- function(x){\\n# Args:\\n# x: variable\\n\\nx <- gsub(\"8\", NA, x)\\nx\\n}\\nSubNA2 <- function(x){\\nx <- gsub(\"7\", NA, x)\\nyear08 <- read.spss(\"knightfoundation2008sotcdata.por\",\\nuse.value.labels = FALSE,\\nyear09 <- read.spss(\"knightfoundation2009sotcdata.por\",\\ncolnames(year08) <- tolowe...\n",
       "Name: code_script, Length: 4828, dtype: object"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_long.code_script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if DATE == \"20231021\":\n",
    "    # ADDITIONAL PREPROCESSING OF DESCRIPTIONS\n",
    "\n",
    "    # remove parentheses\n",
    "    df_long.Description = df_long.Description.str.replace(r\"\\(.+?\\)\", \"\", regex=True)\n",
    "\n",
    "    # remove URL\n",
    "    df_long.Description = df_long.Description.str.replace(\n",
    "        r\"\"\"(?i)\\b((?:https?:(?:/{1,3}|[a-z0-9%])|[a-z0-9.\\-]+[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)/)(?:[^\\s()<>{}\\[\\]]+|\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\))+(?:\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’])|(?:(?<!@)[a-z0-9]+(?:[.\\-][a-z0-9]+)*[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)\\b/?(?!@)))\"\"\",\n",
    "        \"\",\n",
    "        regex=True,\n",
    "    )\n",
    "\n",
    "    # ADDITIONAL PREPROCESSING OF CODE\n",
    "    df_long.code_script = df_long.code_script.str.replace(\n",
    "        r\"#\", \"\", regex=True\n",
    "    ).str.replace(r\"\\n\", \" \", regex=True)\n",
    "    df_long.loc[df_long.type_script == \"m\", \"code_script\"] = df_long.loc[\n",
    "        df_long.type_script == \"m\", \"code_script\"\n",
    "    ].str.replace(r\"\\%\", \" \", regex=True)\n",
    "\n",
    "    # remove the same sign repeated more than 4 times\n",
    "    df_long.code_script = df_long.code_script.str.replace(\n",
    "        r\"(.)\\1{4,}\", r\"\\1\", regex=True\n",
    "    )\n",
    "    df_long.code_script = df_long.code_script.str.replace(\n",
    "        \"\\s{2,}\", \"\", regex=True\n",
    "    ).str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "CmQLCYodJUqs"
   },
   "outputs": [],
   "source": [
    "df_long[\"Q_ID\"] = df_long.index\n",
    "\n",
    "df_long.to_csv(f\"../../data/preprocessed/Quantlet/{DATE}/full_{DATE}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "-ClyiPTu_tjr"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4828/4828 [00:00<00:00, 40030.01it/s]\n"
     ]
    }
   ],
   "source": [
    "df_long[\"url\"] = df_long.progress_apply(combine_url, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "ITt9WLZNA-Bw"
   },
   "outputs": [],
   "source": [
    "df_long[[\"Description\", \"url\"]].to_csv(\n",
    "    f\"../../data/preprocessed/Quantlet/{DATE}/Description_annotation_{DATE}.csv\",\n",
    "    index=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>folder_name</th>\n",
       "      <th>code_script</th>\n",
       "      <th>type_script</th>\n",
       "      <th>script_name</th>\n",
       "      <th>Quantlet</th>\n",
       "      <th>Description</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Authors</th>\n",
       "      <th>scr_n</th>\n",
       "      <th>description_len</th>\n",
       "      <th>description_n_words</th>\n",
       "      <th>repo</th>\n",
       "      <th>code_len</th>\n",
       "      <th>new_len</th>\n",
       "      <th>new_len2</th>\n",
       "      <th>Q_ID</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [folder_name, code_script, type_script, script_name, Quantlet, Description, Keywords, Authors, scr_n, description_len, description_n_words, repo, code_len, new_len, new_len2, Q_ID, url]\n",
       "Index: []"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_long[df_long.Description == \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "wTr5ZFiOKEND"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3945, 17)\n",
      "r     0.462357\n",
      "m     0.283397\n",
      "py    0.254246\n",
      "Name: type_script, dtype: float64\n",
      "(391, 17)\n",
      "r     0.511509\n",
      "m     0.255754\n",
      "py    0.232737\n",
      "Name: type_script, dtype: float64\n",
      "(492, 17)\n",
      "r     0.436992\n",
      "m     0.323171\n",
      "py    0.239837\n",
      "Name: type_script, dtype: float64\n",
      "(3945, 17)\n",
      "r     1824\n",
      "m     1118\n",
      "py    1003\n",
      "Name: type_script, dtype: int64\n",
      "(391, 17)\n",
      "r     200\n",
      "m     100\n",
      "py     91\n",
      "Name: type_script, dtype: int64\n",
      "(492, 17)\n",
      "r     215\n",
      "m     159\n",
      "py    118\n",
      "Name: type_script, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# SPLIT THE DATA GROUP QUANTLET\n",
    "\n",
    "labelled_qs, test_qs = train_test_split(\n",
    "    list(df_long.Quantlet.unique()), test_size=0.1, random_state=RS\n",
    ")\n",
    "train_qs, val_qs = train_test_split(labelled_qs, test_size=0.1, random_state=RS)\n",
    "\n",
    "\n",
    "train = df_long[df_long[\"Quantlet\"].isin(set(train_qs))].reset_index(drop=True)\n",
    "val = df_long[df_long[\"Quantlet\"].isin(set(val_qs))].reset_index(drop=True)\n",
    "test = df_long[df_long[\"Quantlet\"].isin(set(test_qs))].reset_index(drop=True)\n",
    "\n",
    "save_datasets(train=train, val=val, test=test, DATE=DATE, RS=RS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN1WASBG9SoZypHtCRaCZX/",
   "mount_file_id": "19c37YEU8LH5C0d1bxiNpE23xesKn01ad",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "encode_code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jd7S7ZIIS6Fn"
   },
   "source": [
    "################################################################################\n",
    "# 1. Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "QjyD6T4D1XM4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "g4K0YWK11RjR",
    "tags": []
   },
   "outputs": [],
   "source": [
    "QPATH = \"Quantlet/3-data-preprocessing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Xz1ksIsd1SYO",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PREPARE WORKING DIRECTORY\n",
    "\n",
    "import sys\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "import os\n",
    "\n",
    "if IN_COLAB:\n",
    "    os.chdir(\n",
    "        f\"/content/drive/MyDrive/ColabNotebooks/IRTG/Encode_the_Qode/Encode-the-Qode/{QPATH}\"\n",
    "    )\n",
    "else:\n",
    "    os.chdir(\"./\")\n",
    "\n",
    "# sys.path.append('../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "TNkVUfU21UsR",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# PACKAGES\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "import re\n",
    "import sys\n",
    "from IPython.display import display\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "import importlib\n",
    "import preprocessing_utils\n",
    "\n",
    "importlib.reload(preprocessing_utils)\n",
    "from preprocessing_utils import *\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from Levenshtein import distance\n",
    "\n",
    "# SETTINGS\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46Fkzpboq6sy"
   },
   "source": [
    "# 2. Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATE = \"20231027\"\n",
    "RS = 111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\n",
    "    f\"../../data/preprocessed/Quantlet/20231027/Quantlets_20231027.pkl\", \"rb\"\n",
    ") as file:\n",
    "    df = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5018, 6)\n",
      "(5017, 12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5017/5017 [00:16<00:00, 305.65it/s]\n",
      "100%|██████████| 5017/5017 [00:00<00:00, 257442.35it/s]\n",
      "100%|██████████| 5017/5017 [00:00<00:00, 251729.49it/s]\n",
      "100%|██████████| 5017/5017 [00:03<00:00, 1549.24it/s]\n",
      "100%|██████████| 5017/5017 [00:00<00:00, 632201.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5009, 13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df = df_metainfo_parse(df=df,\n",
    "                    prepare_script=True,\n",
    "                    remove_other=True,\n",
    "                    remove_empty=False)\n",
    "\n",
    "df = clean_up(df)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [(0, ), (1, ## Function: getData\\n## Author: Michael Kostmann\\n## Args: path - path of the directory that contains the datasets of\\n## consumers or prosumers that should be read in\\n## read. Must be of the form \"consumer-00000001\" or\\n## \"producer-00000001\".\\n## return - which values to return in the data frame. Arguments are\\n## \"consumption\" (first-order difference of energy),\\n## \"energy\", and \"production\" (first-order\\n## difference of energyOut)\\n## min_test_time - start date and time o...\n",
       "1       [(0, ), (1, ## Function: getData\\n## Author: Michael Kostmann\\n## Args: path - path of the directory that contains the datasets of\\n## consumers or prosumers that should be read in\\n## data - Set to \"all\" to load all datasets in directory or to\\n## \"single\" to load only single dataset specified in \"id\".\\n## If \"single\" is set, argument \"id\" must be given.\\n## id - specifies consumer or prosumer dataset that should be\\n## read in if \"single\" is specified in \"data\". Must be of\\n## the form \"co...\n",
       "2       [(0, ), (1, ## Function: generatePrices (zero-intelligence prices; random in interval)\\n## Author: Michael Kostmann\\n## Args: amounts - matrix of amounts demanded or supplied per market\\n## participant (cols) and trading period (rows)\\n## min_price - lower price bound (outside option for sellers)\\n## seed - single integer that can be supplied to\\n## base::set.seed()\\n##\\n## Returns: p - matrix with prices for each market agent and\\n## trading period\\ngeneratePrices = function(amounts,\\nmin_p...\n",
       "3       [(0, ), (1, ## Plot energy production of all relevant prosumers in testing period\\n## Author: Michael Kostmann\\n# Load packages\\npackages = c(\"cowplot\",\\n\"purrr\")\\ninvisible(lapply(packages, library, character.only = TRUE))\\n# Source user-defined functions\\nfunctions = c(\"FUN_getTargets.R\",\\n\"FUN_generatePrices.R\",\\ninvisible(lapply(functions, source))\\n# Function for easy string pasting\\n\"%&%\" = function(x, y) {paste(x, y, sep = \"\")}\\n# Specify paths to directories containing consumer and p...\n",
       "4       [(0, ), (1, ## Function: blindAuction\\n## Author: Michael Kostmann\\n## Args: cons_amount - numeric vector containing electricity consumption\\n## of current trading period for each electricity\\n## consuming market participant\\n## prod_amount - numeric vector containing electricity production\\n## producing market participant\\n## min_price - lower price boundary for possible bids and asks\\n## (outside option for electricity selling)\\n## max_price - upper price boundary for possible bids and ask...\n",
       "                                                                                                                                                                                                                                                               ...                                                                                                                                                                                                                                                         \n",
       "5004    [(0, ), (1, from LabeledLDA import *\\nfrom sklearn.metrics import auc\\nfrom optparse import OptionParser\\nimport pickle\\nimport numpy as np\\ndef one_roc(prob, real_binary):\\nresorted = np.argsort(prob)[::-1]\\nreals = real_binary[resorted]\\nprobs = prob[resorted]\\nthresholds = np.sort(list(set(probs)))[::-1]\\ntp = []\\nfor c in thresholds:\\npreds = [1 if x >= c else 0 for x in probs]\\nzipped = list(zip(preds, reals))\\ntp_pre = sum([x == y for (x, y) in zipped if x == 1])\\ntp.append(tp_pre)\\nre...\n",
       "5005    [(0, ), (1, import gensim.parsing.preprocessing as gensimm\\nfrom gensim.corpora import dictionary\\nimport numpy as np\\nfrom numpy.random import multinomial as multinom_draw\\ndef load_corpus(filename, d):\\nimport csv, sys, re\\n# Increase max line length for csv.reader:\\nmax_int = sys.maxsize\\ndecrement = True\\nwhile decrement:\\ndecrement = False\\ntry:\\ncsv.field_size_limit(max_int)\\nexcept OverflowError:\\nmax_int = int(max_int/10)\\ndecrement = True\\ndocs = []\\nlabelmap = dict()\\npat = re.comp...\n",
       "5006    [(0, ), (1, import gensim.parsing.preprocessing as gensimm\\nimport numpy as np\\nfrom scipy.stats import truncnorm\\nimport scipy\\nimport scipy.special\\nmultinom_draw = np.random.multinomial\\nrvs = truncnorm.rvs\\ndef partition_label(lab, d):\\nreturn [lab[:i+1] for i in range(d)]\\ndef phi(x):\\nreturn 1/2 * (1 + scipy.special.erf(x / np.sqrt(2)))\\ndef vect_multinom(prob_matrix):\\ns = prob_matrix.cumsum(axis=0)\\nr = np.random.rand(prob_matrix.shape[1])\\nk = (s < r).sum(axis=0)\\nreturn k\\ndef get_...\n",
       "5007    [(0, ), (1, from CascadeLDA import *\\nfrom sklearn.metrics import auc\\nfrom optparse import OptionParser\\nimport pickle\\ndef one_roc(prob, real_binary):\\nresorted = np.argsort(prob)[::-1]\\nreals = real_binary[resorted]\\nprobs = prob[resorted]\\nthresholds = np.sort(list(set(probs)))[::-1]\\ntp = []\\nfor c in thresholds:\\npreds = [1 if x >= c else 0 for x in probs]\\nzipped = list(zip(preds, reals))\\ntp_pre = sum([x == y for (x, y) in zipped if x == 1])\\ntp.append(tp_pre)\\nreturn tp, fn\\ndef fpr...\n",
       "5008    [(0, ), (1, import gensim.parsing.preprocessing as gensimm\\nfrom gensim.corpora import dictionary\\nimport numpy as np\\nimport re\\nmultinom_draw = np.random.multinomial\\ndef load_corpus(filename, d=3):\\nimport csv, sys\\n# Increase max line length for csv.reader:\\nmax_int = sys.maxsize\\ndecrement = True\\nwhile decrement:\\ndecrement = False\\ntry:\\ncsv.field_size_limit(max_int)\\nexcept OverflowError:\\nmax_int = int(max_int/10)\\ndecrement = True\\ndocs = []\\nlabelmap = dict()\\npat = re.compile(\"[A...\n",
       "Length: 5009, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.apply(lambda x: chunk_code(x['code_script'], chunk_size=250), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/RDC/zinovyee.hub/H:/zinovyee.hub/IRTG/MLSC/Encode-the-Qode/Quantlet/3-data-preprocessing/Quantlets_EDA.ipynb Zelle 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bsunflower.wiwi.hu-berlin.de/home/RDC/zinovyee.hub/H%3A/zinovyee.hub/IRTG/MLSC/Encode-the-Qode/Quantlet/3-data-preprocessing/Quantlets_EDA.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mchunk_ids\u001b[39m\u001b[39m'\u001b[39m], df[\u001b[39m'\u001b[39m\u001b[39mchunks\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: chunk_code(x[\u001b[39m'\u001b[39m\u001b[39mcode_script\u001b[39m\u001b[39m'\u001b[39m], chunk_size), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "df['chunk_ids'], df['chunks'] = df.apply(lambda x: chunk_code(x['code_script'], chunk_size), axis=1)\n",
    "#df = df.explode('chunk_id').reset_index(drop=True)\n",
    "\n",
    "# Assigning chunk IDs to the chunks\n",
    "#df['chunk_id'] = df.groupby(level=0).cumcount() + 1\n",
    "\n",
    "#print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3446, 16)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_long['chunk_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4828/4828 [00:00<00:00, 356083.27it/s]\n"
     ]
    }
   ],
   "source": [
    "n_sentences = df_long[\"Description\"].progress_apply(lambda descr: len(descr.split(\".\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ADDITIONAL PREPROCESSING OF DESCRIPTIONS\n",
    "\n",
    "# remove parentheses\n",
    "df.Description = df.Description.str.replace(r\"\\(.+?\\)\", \"\", regex=True)\n",
    "\n",
    "# remove URL\n",
    "df.Description = df.Description.str.replace(\n",
    "r\"\"\"(?i)\\b((?:https?:(?:/{1,3}|[a-z0-9%])|[a-z0-9.\\-]+[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)/)(?:[^\\s()<>{}\\[\\]]+|\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\))+(?:\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’])|(?:(?<!@)[a-z0-9]+(?:[.\\-][a-z0-9]+)*[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)\\b/?(?!@)))\"\"\",\n",
    "\"\",\n",
    "regex=True)\n",
    "\n",
    "# ADDITIONAL PREPROCESSING OF CODE\n",
    "df.code_script = df.code_script.str.replace(r\"#\", \"\", regex=True).str.replace(r\"\\n\", \" \", regex=True)\n",
    "df.loc[df.type_script == \"m\", \"code_script\"] = df.loc[df.type_script == \"m\", \"code_script\"].str.replace(r\"\\%\", \" \", regex=True)\n",
    "\n",
    "df.loc[df.type_script == \"r\", \"code_script\"] = df.loc[df.type_script == \"r\", \"code_script\"].str.replace(r\"\\$\", \" \", regex=True)\n",
    "\n",
    "# remove the same sign repeated more than 4 times\n",
    "df.code_script = df.code_script.str.replace(r\"(.)\\1{4,}\", r\"\\1\", regex=True)\n",
    "df.code_script = df.code_script.str.replace(\"\\s{2,}\", \"\", regex=True).str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample DataFrame\n",
    "data = {\n",
    "    'text_column': [\n",
    "        \"This is an example sentence for chunking.\",\n",
    "        \"Chunking text for better processing.\",\n",
    "        \"Breaking down text into smaller chunks is helpful.\"\n",
    "    ],\n",
    "    # Additional columns for illustration\n",
    "    'other_column_1': [10, 20, 30],\n",
    "    'other_column_2': ['A', 'B', 'C']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "\n",
    "df\n",
    "\n",
    "\n",
    "# Function to chunk text\n",
    "def chunk_text(text, chunk_size=20):\n",
    "    chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking the text_column and adding chunk_id to the DataFrame\n",
    "chunk_size = 20  # Change this value based on your preferred chunk size\n",
    "\n",
    "df['chunk_id'] = df['text_column'].apply(lambda x: chunk_text(x, chunk_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_column</th>\n",
       "      <th>other_column_1</th>\n",
       "      <th>other_column_2</th>\n",
       "      <th>chunk_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is an example sentence for chunking.</td>\n",
       "      <td>10</td>\n",
       "      <td>A</td>\n",
       "      <td>[This is an example s, entence for chunking, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chunking text for better processing.</td>\n",
       "      <td>20</td>\n",
       "      <td>B</td>\n",
       "      <td>[Chunking text for be, tter processing.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Breaking down text into smaller chunks is help...</td>\n",
       "      <td>30</td>\n",
       "      <td>C</td>\n",
       "      <td>[Breaking down text i, nto smaller chunks i, s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         text_column  other_column_1  \\\n",
       "0          This is an example sentence for chunking.              10   \n",
       "1               Chunking text for better processing.              20   \n",
       "2  Breaking down text into smaller chunks is help...              30   \n",
       "\n",
       "  other_column_2                                           chunk_id  \n",
       "0              A    [This is an example s, entence for chunking, .]  \n",
       "1              B           [Chunking text for be, tter processing.]  \n",
       "2              C  [Breaking down text i, nto smaller chunks i, s...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking the text_column and adding chunk_id to the DataFrame\n",
    "chunk_size = 20  # Change this value based on your preferred chunk size\n",
    "\n",
    "df['chunk_id'] = df['text_column'].apply(lambda x: chunk_text(x, chunk_size))\n",
    "df = df.explode('chunk_id').reset_index(drop=True)\n",
    "\n",
    "# Assigning chunk IDs to the chunks\n",
    "df['chunk_id'] = df.groupby(level=0).cumcount() + 1\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "CmQLCYodJUqs"
   },
   "outputs": [],
   "source": [
    "df_long[\"Q_ID\"] = df_long.index\n",
    "\n",
    "df_long.to_csv(f\"../../data/preprocessed/Quantlet/{DATE}/full_{DATE}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "wTr5ZFiOKEND"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3910, 17)\n",
      "r     0.460614\n",
      "m     0.287724\n",
      "py    0.251662\n",
      "Name: type_script, dtype: float64\n",
      "(435, 17)\n",
      "r     0.491954\n",
      "m     0.268966\n",
      "py    0.239080\n",
      "Name: type_script, dtype: float64\n",
      "(483, 17)\n",
      "r     0.463768\n",
      "m     0.279503\n",
      "py    0.256729\n",
      "Name: type_script, dtype: float64\n",
      "(3910, 17)\n",
      "r     1801\n",
      "m     1125\n",
      "py     984\n",
      "Name: type_script, dtype: int64\n",
      "(435, 17)\n",
      "r     214\n",
      "m     117\n",
      "py    104\n",
      "Name: type_script, dtype: int64\n",
      "(483, 17)\n",
      "r     224\n",
      "m     135\n",
      "py    124\n",
      "Name: type_script, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# SPLIT THE DATA GROUP QUANTLET\n",
    "\n",
    "labelled_qs, test = train_test_split(df_long, test_size=0.1, random_state=RS)\n",
    "train, val = train_test_split(labelled_qs, test_size=0.1, random_state=RS)\n",
    "\n",
    "save_datasets(train=train, val=val, test=test, DATE=DATE, RS=RS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN1WASBG9SoZypHtCRaCZX/",
   "mount_file_id": "19c37YEU8LH5C0d1bxiNpE23xesKn01ad",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "encode_code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

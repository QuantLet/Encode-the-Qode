{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jd7S7ZIIS6Fn"
   },
   "source": [
    "################################################################################\n",
    "# 1. Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "QjyD6T4D1XM4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "g4K0YWK11RjR",
    "tags": []
   },
   "outputs": [],
   "source": [
    "QPATH = \"Quantlet/3-data-preprocessing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Xz1ksIsd1SYO",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PREPARE WORKING DIRECTORY\n",
    "\n",
    "import sys\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "import os\n",
    "\n",
    "if IN_COLAB:\n",
    "    os.chdir(\n",
    "        f\"/content/drive/MyDrive/ColabNotebooks/IRTG/Encode_the_Qode/Encode-the-Qode/{QPATH}\"\n",
    "    )\n",
    "else:\n",
    "    os.chdir(\"./\")\n",
    "\n",
    "# sys.path.append('../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "TNkVUfU21UsR",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# PACKAGES\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "import re\n",
    "import sys\n",
    "from IPython.display import display\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "import importlib\n",
    "import preprocessing_utils\n",
    "\n",
    "importlib.reload(preprocessing_utils)\n",
    "from preprocessing_utils import *\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from Levenshtein import distance\n",
    "\n",
    "# SETTINGS\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46Fkzpboq6sy"
   },
   "source": [
    "# 2. Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATE = \"20231104\"\n",
    "RS = 111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\n",
    "    f\"../../data/preprocessed/Quantlet/{DATE}/Quantlets_{DATE}.pkl\", \"rb\"\n",
    ") as file:\n",
    "    df = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5018, 6)\n",
      "(5017, 12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5017/5017 [00:39<00:00, 125.48it/s]\n",
      "100%|██████████| 5017/5017 [01:04<00:00, 77.71it/s]  \n",
      "100%|██████████| 5017/5017 [00:00<00:00, 9213.52it/s] \n",
      "100%|██████████| 5017/5017 [00:01<00:00, 2572.92it/s]\n",
      "100%|██████████| 5017/5017 [00:00<00:00, 399559.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5009, 13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df = df_metainfo_parse(df=df,\n",
    "                    prepare_script=True,\n",
    "                    remove_other=True,\n",
    "                    remove_empty=False)\n",
    "\n",
    "df = clean_up(df)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['script_name_no_ext'] = df.script_name.str.split('.', expand=True)[0]\n",
    "df['main_script'] = df['script_name_no_ext']==df['Quantlet']\n",
    "df = df.loc[df['main_script']==True, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADDITIONAL PREPROCESSING OF DESCRIPTIONS\n",
    "\n",
    "# remove parentheses\n",
    "df.Description = df.Description.str.replace(r\"\\(.+?\\)\", \"\", regex=True)\n",
    "\n",
    "# remove URL\n",
    "df.Description = df.Description.str.replace(\n",
    "r\"\"\"(?i)\\b((?:https?:(?:/{1,3}|[a-z0-9%])|[a-z0-9.\\-]+[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)/)(?:[^\\s()<>{}\\[\\]]+|\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\))+(?:\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’])|(?:(?<!@)[a-z0-9]+(?:[.\\-][a-z0-9]+)*[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)\\b/?(?!@)))\"\"\",\n",
    "\"\",\n",
    "regex=True)\n",
    "\n",
    "# ADDITIONAL PREPROCESSING OF CODE\n",
    "df.code_script = df.code_script.str.replace(r\"#\", \"\", regex=True)\n",
    "df.loc[df.type_script == \"m\", \"code_script\"] = df.loc[df.type_script == \"m\", \"code_script\"].str.replace(r\"\\%\", \" \", regex=True)\n",
    "\n",
    "df.loc[df.type_script == \"r\", \"code_script\"] = df.loc[df.type_script == \"r\", \"code_script\"].str.replace(r\"\\$\", \" \", regex=True)\n",
    "\n",
    "# remove the same sign repeated more than 4 times\n",
    "df.code_script = df.code_script.str.replace(r\"(.)\\1{4,}\", r\"\\1\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Description_ID'] = df.groupby('Description').ngroup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from transformers import pipeline\\nclassifier = pipeline(\"zero-shot-classification\",\\n                        model=\"facebook/bart-large-mnli\")\\ncandidate_labels = [\\'project-level\\', \\'codesnippet-level\\']\\ndescriptions = df.Description.unique()\\ndescriptions[0]\\nclassifier(\"Plots the power curves by 2SQR(1), 2SQR(2) and IVX-QR in simulation.\\'\", candidate_labels)'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from transformers import pipeline\n",
    "classifier = pipeline(\"zero-shot-classification\",\n",
    "                        model=\"facebook/bart-large-mnli\")\n",
    "candidate_labels = ['project-level', 'codesnippet-level']\n",
    "descriptions = df.Description.unique()\n",
    "descriptions[0]\n",
    "classifier(\"Plots the power curves by 2SQR(1), 2SQR(2) and IVX-QR in simulation.'\", candidate_labels)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# CHUNKING\\ndf[['chunk_ids', 'chunks']] = df.apply(lambda x: chunk_code(x['code_script'], chunk_size=250), axis=1, result_type='expand')\\nchunks_df = df.apply(lambda x: chunk_code(x['code_script'], chunk_size=250), axis=1, result_type='expand')\\nchunks_df.columns = ['chunk_ids', 'chunks']\\ndf[['chunk_ids', 'chunks']]  = chunks_df[['chunk_ids', 'chunks']] \\ndf = df.explode('chunk_ids').reset_index(drop=True)\\ndf['chunks'] = df.apply(lambda x: x['chunks'][x['chunk_ids']], axis=1)\\ndf.shape\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# CHUNKING\n",
    "df[['chunk_ids', 'chunks']] = df.apply(lambda x: chunk_code(x['code_script'], chunk_size=250), axis=1, result_type='expand')\n",
    "chunks_df = df.apply(lambda x: chunk_code(x['code_script'], chunk_size=250), axis=1, result_type='expand')\n",
    "chunks_df.columns = ['chunk_ids', 'chunks']\n",
    "df[['chunk_ids', 'chunks']]  = chunks_df[['chunk_ids', 'chunks']] \n",
    "df = df.explode('chunk_ids').reset_index(drop=True)\n",
    "df['chunks'] = df.apply(lambda x: x['chunks'][x['chunk_ids']], axis=1)\n",
    "df.shape'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "CmQLCYodJUqs"
   },
   "outputs": [],
   "source": [
    "df[\"Q_ID\"] = df.index\n",
    "\n",
    "folder_to_save = f\"../../data/preprocessed/Quantlet/{DATE}/\"\n",
    "if not os.path.exists(folder_to_save):\n",
    "    os.mkdir(folder_to_save)\n",
    "\n",
    "#df.to_csv(f'{folder_to_save}full_{DATE}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"code_list = df.groupby(['folder_name'])['type_script'].apply(list)\\ndf['type_script'] = df['folder_name'].map(code_list)\\ndf['type_script'] = df['type_script'].apply(lambda x: ' '.join(x))\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df = pd.read_csv(f'{folder_to_save}full_{DATE}.csv')\n",
    "\n",
    "'''code_list = df.groupby(['folder_name'])['code_script'].apply(list)\n",
    "df['code_script'] = df['folder_name'].map(code_list)\n",
    "df['code_script'] = df['code_script'].apply(lambda x: '\\n\\n'.join(x))'''\n",
    "\n",
    "'''code_list = df.groupby(['folder_name'])['type_script'].apply(list)\n",
    "df['type_script'] = df['folder_name'].map(code_list)\n",
    "df['type_script'] = df['type_script'].apply(lambda x: ' '.join(x))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"print(df[['code_script', 'Quantlet', 'Description', 'Description_ID', 'Q_ID', 'folder_name', 'repo', 'Authors']].shape)\\n\\ndf_reduced = df[['code_script', 'Quantlet', 'Description', 'Description_ID', 'type_script', 'folder_name', 'repo', 'Authors']].drop_duplicates(['Quantlet', 'Description', 'Description_ID', 'folder_name', 'repo', 'Authors'])\\ndf_reduced.shape\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''print(df[['code_script', 'Quantlet', 'Description', 'Description_ID', 'Q_ID', 'folder_name', 'repo', 'Authors']].shape)\n",
    "\n",
    "df_reduced = df[['code_script', 'Quantlet', 'Description', 'Description_ID', 'type_script', 'folder_name', 'repo', 'Authors']].drop_duplicates(['Quantlet', 'Description', 'Description_ID', 'folder_name', 'repo', 'Authors'])\n",
    "df_reduced.shape'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLIT THE DATA GROUP QUANTLET\n",
    "labelled_descr_id, test_descr_id = train_test_split(list(df.Description_ID.unique()),\n",
    "                test_size=0.1,\n",
    "                random_state=RS)\n",
    "train_descr_id, val_descr_id = train_test_split(labelled_descr_id,\n",
    "                test_size=0.1,\n",
    "                random_state=RS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "wTr5ZFiOKEND"
   },
   "outputs": [],
   "source": [
    "full_train = df.loc[df.Description_ID.isin(labelled_descr_id)]\n",
    "train = df.loc[df.Description_ID.isin(train_descr_id)]\n",
    "val = df.loc[df.Description_ID.isin(val_descr_id)]\n",
    "test = df.loc[df.Description_ID.isin(test_descr_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "r     191\n",
       "m      81\n",
       "py     55\n",
       "Name: type_script, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.type_script.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2735.000000\n",
       "mean       25.179159\n",
       "std        20.542373\n",
       "min         1.000000\n",
       "25%        13.000000\n",
       "50%        19.000000\n",
       "75%        29.000000\n",
       "max       175.000000\n",
       "Name: Description, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_len_t = train.Description.apply(lambda x: len(x.split()))\n",
    "c_len_t.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_datasets(full_train, train, val, test, DATE, RS, 'code_script', False, True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:02<00:00, 13.67it/s]\n"
     ]
    }
   ],
   "source": [
    "# create bootstrap\n",
    "SIZE = test.shape[0]\n",
    "indices = range(SIZE)\n",
    "N_SAMPLES = 35\n",
    "\n",
    "for sample in tqdm(range(1, N_SAMPLES)):\n",
    "    np.random.seed(sample)\n",
    "    sample_idx = np.random.choice(indices, size=SIZE, replace=True)\n",
    "    sample_df = test.iloc[sample_idx, : ].reset_index(drop=True)\n",
    "    sample_df.to_csv(f'../../data/preprocessed/Quantlet/{DATE}/test_df_sample_{sample}.csv', index=False)\n",
    "\n",
    "    # PROGRAMMING LANGUAGE\n",
    "    for type_script in sample_df.type_script.unique():\n",
    "      group_test = sample_df.loc[sample_df.type_script == type_script, : ]\n",
    "      test_dataset_json = {'version' : type_script,\n",
    "                          'data' : [{'input_sequence'  : group_test['code_script'].iloc[i],\n",
    "                                    'output_sequence'  : group_test['Description'].iloc[i]} for i in range(group_test.shape[0])]}\n",
    "      with open(f'../../data/preprocessed/Quantlet/{DATE}/test_dataset_{type_script}_sample_{sample}.json', 'w') as f:\n",
    "        json.dump(test_dataset_json, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:02<00:00, 14.01it/s]\n"
     ]
    }
   ],
   "source": [
    "# create bootstrap\n",
    "SIZE = val.shape[0]\n",
    "indices = range(SIZE)\n",
    "N_SAMPLES = 35\n",
    "\n",
    "for sample in tqdm(range(1, N_SAMPLES)):\n",
    "    np.random.seed(sample)\n",
    "    sample_idx = np.random.choice(indices, size=SIZE, replace=True)\n",
    "    sample_df = val.iloc[sample_idx, : ].reset_index(drop=True)\n",
    "    sample_df.to_csv(f'../../data/preprocessed/Quantlet/{DATE}/val_df_sample_{sample}.csv', index=False)\n",
    "\n",
    "    # PROGRAMMING LANGUAGE\n",
    "    for type_script in sample_df.type_script.unique():\n",
    "      group_val = sample_df.loc[sample_df.type_script == type_script, : ]\n",
    "      val_dataset_json = {'version' : type_script,\n",
    "                          'data' : [{'input_sequence'  : group_val['code_script'].iloc[i],\n",
    "                                    'output_sequence'  : group_val['Description'].iloc[i]} for i in range(group_val.shape[0])]}\n",
    "      with open(f'../../data/preprocessed/Quantlet/{DATE}/val_dataset_{type_script}_sample_{sample}.json', 'w') as f:\n",
    "        json.dump(val_dataset_json, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEW SHOT RANDOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot = train.groupby('type_script').sample(n=35, random_state=RS)\n",
    "few_shot_random_ids = few_shot.Description_ID\n",
    "not_few_shot_random_ids = set(train.Description_ID.values).difference(set(few_shot.Description_ID.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(195, 17)\n",
      "(500, 17)\n"
     ]
    }
   ],
   "source": [
    "few_shot_train_df = train.copy(deep=True)\n",
    "few_shot_train_df = few_shot_train_df.loc[~few_shot_train_df.Description_ID.isin(not_few_shot_random_ids)]\n",
    "print(few_shot_train_df.shape)\n",
    "\n",
    "few_shot_full_train_df = full_train.copy(deep=True)\n",
    "few_shot_full_train_df = few_shot_full_train_df.loc[~few_shot_full_train_df.Description_ID.isin(not_few_shot_random_ids)]\n",
    "print(few_shot_full_train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_train_dataset_json = {'version' : '0',\n",
    "                        'data' : [{'input_sequence'  : few_shot_train_df['code_script'].iloc[i],\n",
    "                                    'output_sequence'  : few_shot_train_df['Description'].iloc[i]} for i in range(few_shot_train_df.shape[0])]}\n",
    "with open(f'../../data/preprocessed/Quantlet/{DATE}/fs_train_dataset_sample_0.json', 'w') as f:\n",
    "    json.dump(fs_train_dataset_json, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_full_train_dataset_json = {'version' : '0',\n",
    "                        'data' : [{'input_sequence'  : few_shot_full_train_df['code_script'].iloc[i],\n",
    "                                    'output_sequence'  : few_shot_full_train_df['Description'].iloc[i]} for i in range(few_shot_full_train_df.shape[0])]}\n",
    "with open(f'../../data/preprocessed/Quantlet/{DATE}/fs_full_train_dataset_sample_0.json', 'w') as f:\n",
    "    json.dump(fs_full_train_dataset_json, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN1WASBG9SoZypHtCRaCZX/",
   "mount_file_id": "19c37YEU8LH5C0d1bxiNpE23xesKn01ad",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

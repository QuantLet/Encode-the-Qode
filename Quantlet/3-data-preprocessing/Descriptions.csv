,Description
0,Applies the adaptive algorithm on the model proposed by Barrow (2006) to capture the disaster of stock market and forecast stock price.
1,Scenario analysis for an ETF on the CRIX (Haerdle and Trimborn 2015). Requires Cryptocurrency Data from https://box.hu-berlin.de/d/588d5e7e5de84ae58a51/ 
4,Data visualisation for phacking talk
10,Function comp_Pseudo which returns loadings of DPCA and contribution of each PCs
11,Examples of using DPCA with daily maximum precipitation data in August during the period of year 1997 to 2008.
12,Construct news network out of the identified tickers of S&P500 stocks
13,"Portfolio constructed by sorting the network attention proxy, network degree. S&P500 constituents are sorted into quintile, and each quintile portfolio is rebalanced at the end of each month by the preceding monthly network degree. Portfolios can be value-weighted or equal-weighted."
14,Algorithm that identify the S&P500 firm tickers from news text. The algorithm only identify those tickers in the three most commonly seen cases. Detailed description refer to the paper
15,"Inspect the derived variables from the News Network, Plots and Stats"
17,use LSTM and GRU to optimize prediction
19,use RNN to do cryptocurrency prediction
20,use binance api to collect crypto currency data
21,Use machines learning algorithms to predict daily price move in Bitcoin and perform day trading
29,MNIST Chinese Symbol with CNN
30,Project_Group11 file
35,Retrain the snownlp model to calculate the sentiment index by Sina financial news
40,Project_Group8 file
50,Project_Group9 file
51,Using TFT to do high frequency prediction of stock index futures
52,Using Xgboost and MLP to do high frequency prediction of stock index futures
53,Use HRP method to get allocation weights for portfolio and create back test based on allocation
56,Uses python script to download NASDAQ and NYSE mega/large/mid/small cap stock daily price data from Yahoo!Finance
58,Uses hierarchical clustering method to analyse the correlation of stock returns along time
60,Identifies news articles as fake or real based on its contents
61,Collecting all the cryptopunks by webscraping and cluster them
64,Generate the extract using Markov
65,"Construction, State analysis and back test of HMM model"
66,Comparison of the constructed model with alternative methods
67,Calculation of the prediction error of the constructed model
68,Bitcoin price prediction using the constructed model
69,Modelling_Baseline
72,Modelling_Encoding
75,Feature_Engineering
79,Experiment_Sampling
81,"Plots the network based on an adjacency matrix before and after thresholding, so that we can see the directional connection caused by spillover effects among 100 financial institutions."
82,Estimates the Value at Risks (VaRs) of 100 financial institutions using moving window estimation based on seven macro state variables. The plot shows the log returns of JP Morgan (black points) and VaR of JP Morgan (red line) as an example. The estimated CoVaR by using Single-Index Model with LASSO (blue line) and estimated CoVaR using linear quantile LASSO model(green line) are also plotted as comparison.
83,estimates Conditional Value at Risk (CoVaR) of 100 financial institutions by using Single-Index Model with variable selection. The 110 covariates include log returns of 99 firms (except for firm k) 7 macro state variables and 4 firm k’s characteristics. Then generates the necessory files for other TENET quantlets. The data is not publicly published.
86,estimates Conditional Value at Risk (CoVaR) of 100 financial institutions by using linear quantile lasso algorithm. The 110 covariates include log returns of 99 firms (except for firm k) 7 macro state variables and 4 firm k’s characteristics. The data is not publicly published.
89,"Ranks the total incoming and outgoing links for each individual firm, and lists the first three most influential firms with respect to incoming and outgoing links for each firm"
90,"Plots the total incoming and outgoing links of four financial industry groups: Depositories (red solid line), Insurances (blue dashed line), Broker-Dealers (green dotted line), Others (violet dash-dot line)"
91,Plots the total connectedness and the averaged lambda of 100 financial institutions from 20071207 to 20130105.
92,"Ranks the risk receivers and the risk emitters, the top ten firms of each group are identified as the systemically important financial insitutions (SIFIs)"
93,"Estimates and plots (yearly) empirical pricing kernels (EPK), risk neutral densities (RND) and physical densities (PD) of DAX 30 index return conditional on 20% (red curve), 40% (green curve) and 60% (blue curve) quantiles of volatility index VDAX-NEW (current year), and time to maturity 1 month. Local linear kernel regression is used for estimation of the conditional risk neutral density and local constant kernel regression is used for estimation of conditional physical density. Moreover, 95% confidence intervals have been calculated for PK, RND and PD conditioned by 40% quantile of VDAX-NEW and time to maturity 1 month. All results are shown on a continuously compounded 1-month period returns scale. "
94,"Estimates and plots (yearly) empirical pricing kernels (EPK), risk neutral densities (RND) and physical densities (PD) of DAX 30 index return conditional on time to maturity 1 month and different levels of VDAX-NEW (20 equally spaced numbers from 5% to 95% quantile of VDAX-NEW in a given year). Local linear kernel regression is used for estimation of the conditional risk neutral density and local constant kernel regression is used for estimation of conditional physical density. EPK is obtained as a ratio of RND and PD. The panels on the right-hand side of figures depict EPK, RND, PD conditional on 20%, 50% and 80% quantiles of VDAX-NEW with 95% confidence intervals. Colors from red to blue correspond to increasing values of volatility within each interval. All results are shown on a continuously compounded 1-month period returns scale."
95,"Estimates and plots (yearly) empirical pricing kernels (EPK), risk neutral densities (RND) and physical densities (PD) of DAX 30 index return conditional on 20% (red curve), 40% (green curve) and 60% (blue curve) quantiles of volatility index VDAX-NEW (current year), and time to maturity 1 month. Local linear kernel regression is used for estimation of the conditional risk neutral density and local linear kernel regression is used for estimation of conditional physical density. Moreover, 95% confidence intervals have been calculated for PK, RND and PD conditioned by 40% quantile of VDAX-NEW and time to maturity 1 month. All results are shown on a continuously compounded 1-month period returns scale."
96,"Estimates and plots (yearly) empirical pricing kernels (EPK), risk neutral densities (RND) and physical densities (PD) of DAX 30 index return conditional on time to maturity 2 months and different levels of VDAX-NEW-Subindex 2 (20 equally spaced numbers from 5% to 95% quantile of VDAX-NEW-Subindex 2 in a given year). Local linear kernel regression is used for estimation of the conditional risk neutral density and local constant kernel regression is used for estimation of conditional physical density. EPK is obtained as a ratio of RND and PD. The panels on the right-hand side of figures depict EPK, RND, PD conditional on 20%, 50% and 80% quantiles of VDAX-NEW-Subindex 2 with 95% confidence intervals. Colors from red to blue correspond to increasing values of volatility within each interval. All results are shown on a continuously compounded 2-months period returns scale."
97,"Estimates and plots (yearly) empirical pricing kernels (EPK), risk neutral densities (RND) and physical densities (PD) of DAX 30 index return conditional on time to maturity 3 months and different levels of VDAX-NEW-Subindex 3 (20 equally spaced numbers from 5% to 95% quantile of VDAX-NEW-Subindex 3 in a given year). Local linear kernel regression is used for estimation of the conditional risk neutral density and local constant kernel regression is used for estimation of conditional physical density. EPK is obtained as a ratio of RND and PD. The panels on the right-hand side of figures depict EPK, RND, PD conditional on 20%, 50% and 80% quantiles of VDAX-NEW-Subindex 3 with 95% confidence intervals. Colors from red to blue correspond to increasing values of volatility within each interval. All results are shown on a continuously compounded 3-months period returns scale."
98,Selects bandwidth for multivariate local linear kernel regression using cross-validation
99,"Estimates and plots (yearly) empirical pricing kernels (EPK), risk neutral densities (RND) and physical densities (PD) of DAX 30 index return conditional on time to maturity 1 month and different levels of VDAX-NEW-Subindex 1 (20 equally spaced numbers from 5% to 95% quantile of VDAX-NEW-Subindex 1 in a given year). Local linear kernel regression is used for estimation of the conditional risk neutral density and local constant kernel regression is used for estimation of conditional physical density. EPK is obtained as a ratio of RND and PD. The panels on the right-hand side of figures depict EPK, RND, PD conditional on 20%, 50% and 80% quantiles of VDAX-NEW-Subindex 1 with 95% confidence intervals. Colors from red to blue correspond to increasing values of volatility within each interval. All results are shown on a continuously compounded 1-month period returns scale."
100,"Estimates and plots (yearly) empirical pricing kernels (EPK) of DAX 30 index return conditional on times to maturity 1, 2 and 3 months and different levels of VDAX-NEW (10 equally spaced numbers from 35% to 65% quantile of VDAX-NEW in a given year). VDAX-NEW (and VDAX-NEW-Subindex 1), VDAX-NEW-Subindex 2 and VDAX-NEW-Subindex 3 were used for estimation of pricing kernels condiotioned by time to maturity 1, 2 and 3 months respectively. Local linear kernel regression is used for estimation of the conditional risk neutral density and local constant kernel regression is used for estimation of conditional physical density. Colors from red to blue correspond to increasing values of volatility."
101,Fit market IV surface by double exponential jump diffusion model
103,Demonstrates the cleaning of the housing dataset provided by Sberbank
105,"Performs basic variable selction methods (LASSO, rf variable importance plot) on housing data provided by Sberbank"
106,Introduces the simple instrumental variables estimator and explores its properties using simulated data and a practical example.
107,"Conducts a statistical analysis, heteroskedasticity and normality checks on housing data provided by Sberbank"
108,Analyses the consistency of the simple IV estimator as well as the effects of differing instrument quality using simulation.
109,Illustrates various tests applied in instrument variable estimation by simulation
110,"Predicting periods of decline of Moscows housing market through relevant Google Trends, such as the search keyword real estate market or or the complete category real estate"
112,Descriptive statistical analysis of Moscow housing data provided by Sberbank
114,"Applying eXtreme Gradient Boosting to three different sets of variables, comparing performances on the validation set, and using Partial Least Squares Regression as a benchmark"
116,"Illustrates the 2SLS estimator by simulating the effect of endogeneity, consistency, and the effect of instrument strength"
118,This quantlet visualizes the distribution of different prices per square meter for the districts of Moscow.
120,"Reads in test and train data from csv-file and calculates out-of-sample performance measures for the trained models. Creates plots of real against predicted data, showing performance visually."
121,"Reads in preprocessed train data and, trains and tunes RF and GBM Models and saves both the models as well as tuning results in graphical and tabular form"
122,Reads in train data from csv-file and does some exploratory data analysis on the distribution of the target variable and the available categoric and numeric input variables.
123,"Performs four regression based methods to select appropriate variables in a machine lerning approach: Lasso regression, Ridge regression, stepwise backward selection and stepwise forward selection. Several graphs are produced: for Lasso, a plot of the crossvalidation procedure for the optimal choice of the hyperparameter lambda based on the mean squared error is plotted, as well as a trace plot. For the stepwise forward selection a graph displays AIC versus number of variables for the different models. A table that compares all models is exported in LaTex format. The fitted models are exported in RData format for further analysis."
124,Illustrates the 2SLS estimator using an empirical example
125,"Reads in train data from csv-file and does some exploratory data analysis on the linear relationship of the numeric vaiables in the data, including the target variable. Looks at distribution of target variable via boxplots conditional on the levels of the categoric variables in the dataset."
126,"Analyses dataset on missings, performs imputation, handles outliers and empty factor levels. Performs PCA to reduce dimensionality. Three plots are created: a missingnessmap, a matrix of histograms and a screeplot for the PCA. The preprocessed dataset is split into training (80%) and test data (20%) and exported to .csv."
127,Contains the LSTM model used to predict the VIX index by using the SP500 option intraday data.
128,"Contains the VIX index and the VIX future intraday analysis on the tail-event day of February 5, 2018. The next day, Credit Suisse lost 500m USD because of a structured product on the VIX. Market disruption can already be observed the day before."
130,This program fits Generalized Pareto Distributions to excess losses of stock/index.
131,"Hypothesis: The bitcoin prices are likely to fall on Monday, i.e. the closing price of Monday is less than the bitcoin price of the previous Friday."
132,"Implements the statistics tests for the Efficient Market Hypothesis: Bartell Test, Ljung-Box Test, Variance Ratio Test, Von-Newmann Test."
134,SFM_Sim_pareto simulated a Pareto distribution.
135,SFM_Sim_pareto simulates Y=-X ~ Pareto distribution.
136,This anomaly describes the increase in the prices of the Bitcoin in the last week of December and the first half month of January.
137,"SFM_Gen_Pareto simulates Generalized Pareto Distributions: Pareto, Exponential, Pareto II."
138,This program tests the hypothesis that the DJIA log-returns are drawn from a Normal distribution and estimates the probability of extreme negative returns.
139,Estimates the parameters of Pareto-like distribution for the left tail of log-returns.
140,"Analysis of the evolution of prices of shares of AAPL, AMZN, GOOG, FB, NFLX, MSFT, SPY between 2013 and 2019."
141,"Simulates the following GEV Distributions: Weibull, Frechet, Gumbel."
142,"Estimates the implied volatility for the Black-Scholes formula, using the Newton-Raphson method."
143,This program applies the Block Maxima method to annual losses of DJIA.
144,Simulates a bivariate Normal distribution.
145,This quantlet estimates Tail Entropy Expected Shortfall for a Pareto Distribution.
146,Generating a hierarchical clustering as well as a dendrogram and a plot indicating cluster sizes
147,Computing and plotting the Silhouette Score as well as the Calinski Harabaz Score for a k-means and several hierarchical clusterings (different distance measures)
148,Evaluation of cluster coherence for a manual k-means algorithm by returning the 5 highest centroid values per cluster
149,A manual k-means algorithm as alternative to the one provided by sklearn package
150,Plotting the sum of squared distances for each k of a k-means clustering
151,Plotting cluster sizes for k-means
152,Plots time series from cryptocurrency database
154,Plotting modules to graph rolling-mean time series data taken from derived sentiment scores. These are plotted against BTC prices in USD for visualisation of the relationship. Basic correlation and Granger causation are calculated as well.
161,Sentiment scoring tool which takes the output of the news source scraping tool and applies to them the unsupervised VADER and TextBlob sentiment methods.
164,"Interactive, terminal based scraping tool built to collect news story text from the New York Times, BBC, CNN, and Reuters. The user chooses the desired news source and enters 1-3 search terms. Resulting publications are saved to a .csv file for further processing needs of the user."
171,"Split the entire dataset in two parts. the first one consists of all observations without the last 24 ones. It is used for the training process of LSTM with the optimal parameters. The second subset consists of the last 24 observations used for the prediction of the hourly returns in the next 24 hours. Once the predictions are generated for each of the coins, they are exported from python in order to be imported in MATLAB."
172,Analyse the hourly returns of the collected data of the four coins.
173,Plotting the CVaR for all sets of different initial weights and see it converging
174,Select the optimal window-size for the training of the LSTM
175,Preprocessing the cryptocurrency data in such a way that it fits the requirements of LSTM
176,Define a parameter grid for the LSTM and choose the optimal set of parameters that leads to a minimal root-mean-squared error.
177,Definition of the architecture of the LSTM that is used for the time-series prediction of the hourly returns.
178,Retrieval of cryptocurrncy data via API call.
179,Varying all combinations of initial weights to determine which one leads to a minimal CVaR
180,Plotting the CVaR for all sets of different initial weights
181,Determining which the optimal portfolio weights are and what CVaR they result into
182,Training the LSTM on the data of one of the Coins (ETH) and analysing the results on the test set without parameter tuning.
183,Finding all possible combinations of initial weights for a four-cryptocurrencies-portfolio
184,Tail event driven portfolio construction.
186,"Extraction, grading and clustering of the Quantlets in the GitHub Organization Quantlet with the use of the classes modules/QUANTLET.py and modules/METAFILE.py. With this program you can extract, update and save the data, model topics with Latent Semantic Analysis, compute different clusterings and visualize the clustering with t-Stochastic Neighbour embedding."
187,Example of bootstrap application to estimate performance measures of U-Statistics
188,Comparison of variances of U-Statistics and alternative unbiased estimators
189,Produces the estimation results using GeoCopula approach and saves it as RDate file.
190,Produces the plot for parametrically fitted variogram.
192,"Generate random sample – Observations & Hidden states of dice (with Faif and Loaded dice used interchangeably), sample size is set as N"
193,"Use standard HMM package (i.e., hmmlearn) to analyze the data with multinomial (discrete) emissions – in Casino case of fair and loaded dice data"
195,Give a break down of the related algorithms in HMM – in Casino case of fair and loaded dice data
196,"Use standard HMM package (i.e., hmmlearn) to analyze the data with continuous emissions (observations) – in heart rate bpm) case"
197,Negentropy approximation and comparison
198,Analyze financial data (daily returns of 15 stocks in Asia market) - show the difference between results of PCA and ICA
199,"Solve Cocktail Party Problem (Blind source separation) - show the difference between results of PCA, FA and ICA"
200,Show that uncorrelatedness does not imply independence - the difference between results of PCA and ICA
202,"UNIT 1 of DEDA. Introduce basic syntax, such as numeric and string, and basic data structure, like list, tuple, set and dict in Python"
204,"['Preprocessing Japanese text from Yahoo', 'Create a wordcloud based on the text']"
205,UNIT 2 of DEDA. Introduce the conditional execution and how to iterate by using
207,UNIT 3 and UNIT 4 of DEDA. Introduce webscraping and object-oriented programming
210,"['Demonstrate decision tree', 'Demonstrate applied SVM on NLP sentiment classification', 'Plot some activation functions for Neural Networks']"
216,Replication code for the simulation study. Comparison of the two-model approach and the doubly-robust estimator on different data generating processes
218,We provide code to generate a correlation plot for multiple variables. The plot contains the density and the pearson correlation coefficient. The color management makes it easy to read and interpret the direction and strength of the correlation.
219,Demonstrating getting data from various websites (web scraping) and analyzing the election rally speeches in Turkey (24.06.2018) by constructing a bridge between Turkish and English through tokenization and translation.
227,Use Folium Package for Site Location Selection: A Case for New-entry Retailers to the Xinyi District of Taipei.
228,Use Ant Colony Optimisation(ACO) for Logistic Route Planning: A Case for Logistic Distribution of 7-Eleven Stores in the Xinyi District of Taipei.
232,Predicting success of startups based on crunchbase database
233,Quantify the degree of Mining Pool concentration for different cryptocurrencies
235,"Create Choropleth WorldMap to depict sum of all World Cup Goals by each team that ever participated in a WorldCup. Using plotly, a dropdown menu is created depicting for each World Cup the amount of goals shot by each team participating ."
237,"Use Multinomial Logistic Regression (LogReg) to predict the Outcome of the Games of the World Cup Matches 2018. Predictions are based on historical results since 1993. Predictions were adapted after the Group Stages with the teams that actually passed on to the Knockout Stages, including all the games played so far in the World Cup into the Training/Test data. The same was done after the Quarterfinals."
240,Analyzing textual of IRTG researchers then matching IRTG Researchers based on their interests
241,Scarping from IRTG webpage to obtain researchers information for further analysis
242,Analysing Sentiment of Subreddits
245,Regulation Risk for Cryptocurrencies: Lexicon based sentiment analysis of scraped regulation news
250,"Scripts written in python to scrape details of room offers on wg-gesucht.de. It is done in two stages, first from the list of offers available and second the detail description of an offer."
253,"An web application built in Python. From the interface written in HTML, user input of a Berlin housing offer in doubt is parsed and entered into the prediction model built from existing data. A probability of how similar the offer is to scams is returned on the submit page."
254,Calculates forecasts based on historical data and Sentiment of Stock News
255,Python Based Raspberry Pi Zero W OLED Setup live BTC ticker
260,Plotting the p2p lendings using LLE
262,Plotting the Swiss roll and reduction using LLE
264,Plotting the Moebius band
265,Plotting the Antisocial Online Behavior using LLE
266,Plotting the 2 company reports using LLE
267,Plotting the S and sphere and reduction using LLE
269,Quantlet data reduction with PCA and LLE
270,"Produces the mosaic plot of current jobs (for June, 2016) of CRC members who defended their PhD from 2005 to 2016, June"
271,Produces the tree map of study decision of 117 alumnus of the Econ Boot Camp (EBC) from 2008 to 2016
272,"Produces a map with the locations of scientific events visited by CRC members from 2005 to 2016, June (paid from CRC)"
273,Network map of of collaborating disciplines in CRC 649 according to JEL classification
274,"Produces a map of university locations of guest researchers who visited the CRC from 2005 to 2016, June"
275,"Computes European/American option prices using a binomial tree for the CocaCola (KO) stock with fixed amount dividends. This quantlet contains a dataset ""KO_Bloomberg"" and the full code. Make sure you download the dataset, specify a path and then run the Quantlet."
276,"Generates confusion matrix between test set predictions generated by Microsoft Emotion API and true test set values. 'results_API_test_set.pkl' contains predictions from API, reruns process from emotion score raw data."
277,"Generates confusion matrix between test set predictions of FVCConvNet and true test set values. Can be made by loading model and weights from Q-let FVCConvNet and data loader function for original data set FVCload_dataset, or by simply loading already generated predictions and true values."
278,"Plots smoothed emo scores (anger, contempt, disgust, fear, surprise, happiness, sadness, neutral, surprise) and stock movement of Volkswagen stock for first 30 minutes of Volkswagen press conference on 05.05.2017."
279,"Calls Emotion API of Microsoft Cognitive Service. Requires either a key from Microsoft Azure (www.azure.microsoft.com). Can be used to send images from hard drive to API, returns a vector of eight emotion-scores."
280,"Plots scores for seven emotions (anger, contempt, disgust, fear, happiness, sadness, surprise) generated by Microsoft cognitive API for 70 press conferences held by the European Central Bank (Jan. 2011 - Sep 2017)."
281,Estimates a linear discriminant analysis (LDA) in order to separate positive and negative returns of Eurostoxx50 on press conference days of the European Central Bank based on facial expression scores of the provided video material. Plots resulting histograms and distributions.
282,"Generates confusion matrix between test set predictions of FVCfinetuning and true test set values. Can be made by loading model and weights from Q-let FVCfinetuning and data loader function for original data set FVCload_dataset, or by simply loading already generated predictions and true values."
283,"Plots metrics based on confusion matrices generated by FVCconf_CNN, FVCconf_finetune, FVCconf_API to illustrate the accuracy and prediction power of the different models for an intuitive comparison."
284,Estimates a ordinary least squares models in order to explain returns of Eurostoxx50 on press conference days of the European Central Bank based on facial expression scores of the provided video material.
285,Automizes the procedure of loading the data set and restructuring the images from one-dimensional array into two-dimensional images. Required dataset is from Kaggle's 2013 Facial Expression Recognition Competition and can be downloaded under 'https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge' 
286,"Fits an Convolutional Neural Network that classifies facial expressions into seven basic emotions (anger, disgust, fear, happiness, neutral, sadness, surprise) using the Keras library for python. The training and testing dataset can be obtained from kaggle's 'Facial Expression Recognition Challenge'. The subfolder 'model weights' contains model weights and model architecture in .json-Format. Allows to access the estimation results without rerunning the calculations."
287,"Estimates a partial least squares (PLS) model. Independent variables are averaged emotion scores (anger, disgust, fear, contempt, happiness, neutrality, sadness, surprise) for all webcasts of press conferences held by the European central bank between Jan. 2011 and Sep. 2017. The dependent variable is daily return data of Eurostoxx50."
288,Plots accuracy of FVCConvNet and FVCConvNetFinetune on identical training and validation set for each iteration of estimation
289,Plots the correlation of aggregated emotional scores for 70 ECB press conferences (Jan. 2011 - Sep. 2017) and daily returns of Eurostoxx50 data.
290,"Plots the correlation circle of the partial least squares model from Q-let FVCEurostoxxPLS. Dependent variable Y are daily returns of Eurostoxx50 data (Jan. 2011 - Sep. 2017), independent are emotional scores on ECB press conference webcasts (70 observations)."
291,"Finetunes an Convolutional Neural Network that classifies facial expressions into seven basic emotions (anger, disgust, fear, happiness, neutral, sadness, surprise). The underlying deep convolutional net is trained on VGGface. Image based static facial expression recognition with multiple deep network learning. The original dataset can be obtained from kaggle's 'Facial Expression Recognition Challenge'. The subfolder 'model weights' contains model weights and model architecture in .json-Format. This allows to access the estimation results without rerunning the calculations. VGGFace implementation on keras by github.com/rcmalli/keras-vggface. "
292,"Plots the volatility of DAX30 index on days where the European Central Bank is holding press conferences vs. all other days between January 2015 until September 2017. Volatilities are averaged for each group and estimated from the data, based on 5 minute data. Vertical lines indicate the press release by European central bank council at 13.45, the beginning of the press conference at 14.30, held by the president, and the approximate end at 15.30."
293,Performs bivariate kernel smoothed regression and bootstrapped confidence bands for gasoline demand on personal income and gasoline price.
294,"This Quantlet is dedicated to classification of the Solidity source code using machine learning methods. We differentiate between traditional machine learning methods and deep learning. There are three modes for training these methods: on the source code, on the source code without comments and on only comments. Moreover, deep pre-trained transformer BERT - the state-of-the-art approach in the NLP were used on the comments mode. Furthermore, additional feature extraction of the most important block names of the Solidity code was made and the best performing ML algorithm was trained on these features."
301,This Quantlet is dedicated to visualization of time-series of the amount of Smart contracts created since the invention of Ethereum
303,This Quantlet is dedicated to visualization of time-series of energy consumption of Ethereum
304,Clustering & Classifying Ethereum Smart Contracts
309,Cryptocurrency and CRIX Time Series
310,"This Quantlet is dedicated to clustering of the unlabelled open source Ethereum Smart Contracts. First, we encode our abstracts with Distilbert embeddings and using UMAP we reduce dimensionality reduction to perform K-Means, finally again using UMAP reduce dimensionality to 2, to be able to plot it."
311,"This Quantlet is dedicated to topic modeling on the existing research on Ethereum Smart Contracts. First, we encode our abstracts with Distilbert embeddings and using UMAP we reduce dimensionality reduction to perform K-Means, finally again using UMAP reduce dimensionality to 2, to be able to plot it."
313,Smart Contracts Network Parameters Visualization
314,"This Quantlet is dedicated to gathering data on the open source source codes of Solidity Smart Contracts. First, the Ethereum Dapps names are obtained using the API https://www.stateofthedapps.com/. For each DApp the list of smart contract hashes were obtained if available (Also through the API of state of the Dapps). And followingly using the API of https://etherscan.io/, the source codes (if verified) are obtained for the open source Dapps. Finally, the codes are parsed and preprocessed to extract comments and merged with the category from the State of the Dapps and stored as a csv file."
318,"This Quantlet is dedicated to gathering data on the open source source codes of Solidity Smart Contracts using the API of https://etherscan.io/. Finally, the codes are parsed and preprocessed to extract comments and stored as a csv file."
320,This Quantlet is dedicated to test of the difference in performance of different training modes. The wilcoxon test is used.
321,Textmining and visualization for tweets in Japanese
322,Computes a derivative estimator for UK 1973 expenditure data.
323,Estimates the densities of netincome for all years (1969-83) from the UK family expenditure survey (FES).
324,Illustrates Buffalo snowfall data with bin grid.
325,"Estimates a logit model for migration data from Mecklenburg-Vorpommern, GSOEP 1991."
326,Illustrates a construction of histogram.
327,Computes a kernel regression and confidence intervals for the regression of food expenditures on net-income for the UK 1973 expenditure data.
328,"Plots different kernel functions: uniform, triangle, epanechnikov, quartic, gaussian."
329,Computes a semiparametric logit model with additive component functions for Age and Amount using mgcv.
330,Computes different regressions of food on net-income or the UK 1976 expenditure data with Nadaraya Watson and with local linear regression.
331,Illustrates bias of a histogram.
332,Computes an additive fit for the Boston housing prices.
333,Illustrates the difference between a bivariate product and a bivariate radial-symmetric Epanechnikov kernel with equal bandwidths.
334,Computes the an orthogonal series regression food expenditures on net-income for the UK 1973 expenditure data.
335,"Visualizes bias^2, variance and mean squared error (MSE) for a kernel density estimate."
336,"Estimates a bivariate kernel density for age and income of migration data from Mecklenburg-Vorpommern, GSOEP 1991. The density estimate is plotted via contour lines in 2D."
337,"Computes the descriptive statistics of migration data from Mecklenburg-Vorpommern, GSOEP 1991."
338,Visualizes the construction of a kernel density estimate (with additional slider to choose bandwidth and sample size).
339,Illustrates a histogram of stock returns.
340,Computes a histogram for stock returns data with binwidth h = 0.02 and origin x0 = 0.
341,Computes the median smoother of food expenditures on netincome for the UK 1973 expenditure data.
342,"Shows the averaged squared error for the simulated data set with Gaussian (label 1) and quartic kernel (label 2). The weighted function w(u)=I(|u-0.5|<=0.4) was used. The binwidths h with minimal ASE (min1, min2) are computed."
343,Computes the regression of food on net-income for the UK 1973 expenditure data.
344,Shows the Gasser-Mueller estimator and its first derivative for simulated data set.
345,Computes the regression of food share on net-income for the UK 1976 expenditure data.
346,Illustrates an ordinary histogram for stock returns.
347,Visualizes the construction of a kernel density estimate for a Gaussian kernel.
348,Illustrates Buffalo snowfall data.
349,Visualizes bias effects for a kernel density estimate via simulation n = 10000.
350,Computes the regression of food on net-income for the UK 1976 expenditure data.
351,Illustrates an averaged shifted histogram for stock returns.
352,Illustrates a mean squared error (MSE) of histogram for Buffalo snowfall data.
353,"Estimates a bivariate kernel density for age and income of migration data from Mecklenburg-Vorpommern, GSOEP 1991. The density estimate is plotted in the 3D space."
354,Computes an additive fit.
355,Computes the Nadaraya-Watson estimator for the UK 1973 expenditure data.
356,"Illustrates parametric link functions: logit, probit and heteroskedastic."
357,"Displays different penalizing function: S, AIC, FPE, GCV, T."
359,Computes the wavelet regression using Daubechies basis for a simulated data set.
360,Illustrates a histogram of an exponential distribution.
362,Computes the local polynomial estimator for the UK 1973 expenditure data employing a gaussian kernel.
363,Visualizes bias effects for a kernel density estimate.
364,Computes the regression of food on net-income for the UK 1976 expenditure data using a k-nearest neighbor classification.
365,Computes the kernel density estimate for simulated multivariate normal random numbers.
366,Computes a kernel estimation of epanechnikov and triangle kernel for net-income data from the U.K. Family Expenditure Survey.
367,Computes an additive fit with normal and correlated regressors.
368,Creates plots of EUR/USD rates and returns from 2002-01-01 to 2009-01-01.
369,Computes the derivative of the food on net-income regression for the UK 1976 expenditure data using local polynomials.
370,Computes the regression of food expenditures on netincome with four different bandwidths for the UK 1973 expenditure data.
371,"Visualizes bias^2, variance and mean averaged squared error (MASE) for the regression of simulated data."
372,We conduct a simulation study in order to show consistency of component-based and integration-based Shapley curves.
373,Estimated and population Shapley curves.
374,"Application plots: Boxplots, slide plots with CI, density plots, 3d plots"
375,We estimate the Shapley curves and provide bootstrapped confidence intervals for the U.S. vehicle price data.
376,Sequential Shapley curves as described in the paper.
377,We estimate the coverage probability for component-based and integration-based Shapley curves by bootstrap.
378,Slice plots with bootstrapped CI.
379,ICplots gives 3 plots which are an example how different AIC and BIC perform.
380,"Plots three probability density functions (top) and three cumulative density functions (bottom) of the GH, Hyperbolic and NIG distributions"
381,R-Shiny app which does a one day forecast of the CRIX (thecrix.de) using an LSTM Model
382,R-Script with analysis of log returns used on CRIX Forecasting App
383,R-Script with ETS model training used on CRIX Forecasting App
385,R-Script with analysis functions used on CRIX Forecasting App
386,R-Script to load CRIX Data from the web
387,R-Script with Machine Learning model training used on CRIX Forecasting App
388,R-Script with model functions used on CRIX Forecasting App
389,"‘Stochastic pricing and calibration of risk premium. Out-of-sample backtesting for price prediction using zero MPR, constant MPR, smooth MPR. Sensitivity test with cross-validation.’"
391,Normalisation of German wind power utilisation for stochastic pricing.
394,Derives the ecdfs according to groups of crypto-currencies and plots them for comparison.
395,Gives a plot visualizing the trading volume of the top 10 crypto-currencies by market capitalization.
396,Plots the mean log returns of the top 10 crypto-currencies by market capitalization on a rolling window.
397,Plots the cumulative explained variance by the principal components of a PCA of the top 10 crypto-currencies by market capitalization.
398,Gives 2 histograms for the distribution of market capitalization of crypto-currencies in 2 different time periods.
399,Plots the standard variation of the log returns of the top 10 crypto-currencies by market capitalization on a rolling window.
400,Derives the alpha of the power law and plots them to compare them over time.
401,Gives 2 histograms for the distribution of market capitalization of crypto-currencies in 2 different time periods with a high overall market capitalization.
402,Compares the densities of the top 10 crypto-currencies by market capitalization against a histogram of the normal distribution.
403,Plots the aggregate market capitalization of all crypto-currencies while the thickness of the line gives the corresponding trading volume.
404,Boxplot of distance between MSFT and AMZN of 167 rolling wondows. Each window size is 500 days.
407,LDA analysis of the abstracts of a set of scientifique papers to understand main topics that are handled in those papers.
409,Calculate probability of the data that will lie outside of the upper cutoff point
410,Performs a PCA and a cluster analysis for 20 randomly chosen bank notes from the swiss bank notes dataset.
411,Plot the ubike station that located in hsinchu city area
412,"['Demonstrate getting data from webpage API and scraping', 'Maping the location of UBike station all over New Taipei City']"
413,PCA and a cluster analysis for 20 randomly chosen bank notes from the swiss bank notes dataset.
414,Kernel Density Estimation Using Scikit-Learn
416,Summarize K topics from 130 collected abstracts with LDA method
418,Create a function that can generate normal distribution from polar coordinate. The basic idea is from muller-box transformation.
419,Final Project of SDA I Class: Modelling The Effect of Confirmed Number COVID-19 Ratio to The Exchange Rate of Euro to Rupiah
424,Distance measure for continuous variables using Euclidian/Manhattan/Maximum methods
425,Collect >=130 abstracts from http://www.wiwi.hu-berlin.de/de/forschung/irtg/results/discussion-papers
426,Generate 2D Normal Distribution with Polar Coordinates
427,Distance measure for continuous variables using Euclidian/Mannheim/Maximum methods
428,Generating 2D Normal Distribution with Polar Coordinates
430,The First Progress of Final Project of SDA I Class
433,Calculating the probability upper cutoff point of normal standard distribution
434,Final Project of SDA I Class: Modelling The Effect of Confirmed Number COVID-19 Ratio to The Exchange Rate of Euro to Rupiah. (You can change the data in python code to get the result of other models)
435,Test for Granger causality among series
436,"scraping on sciencedirect with scrapy, the data will be used to create topic modeling about covid19 based on the abstract"
439,"Preprocessing data which is text data. it contains case folding, tokenizing, lemmatizer, and stemming."
440,Create a wordcloud about covid19 using abstract that crawled from sciencedirect
441,Use complex numbers as parameters to fit an pregnant shark that wiggling. Creates an MP4 file of the wiggling pregnant shark
443,Calculate the upper-cut-off of normal distribution.
444,Calculating the probability of a “normal (distribution)” to be outside the upper cutoff point
446,2nd Iteration - Data processing and encoder-decoder
447,3rd Iteration - Full-code implementation
448,Final Iteration- Final implementation
449,Extract open data of Xinbei bike
450,Proximity measure for binary variables using Jaccard/Simple matching/Tanimoto methods
451,Calculating index for Crypto Currency
452,generate normal distribution using Box-Muller method
453,simple web-scraping on vietnam.net
454,Create a function that can generate exponential distribution from polar coordinate. The basic idea is from muller-box transformation.
455,"A Web Crawler for UBike website, and visualizes dataset with scatter plot"
456,Mapping the u-bike station in Taipei with the number of u-bike left at that station.
458,perform LDA Analysis on given dataset
459,"Crawling cryptocurrencies data from coinmarketcap.com using beautifulsoup library from python language programming, and store it into csv file"
460,Use parameters to fit a swimming fish
461,Map geographic linestring representations of public transport lines in the city of Berlin (data from VBB)
462,"computes the Jaccard, simple matching and Tanimoto proximity coefficients for binary car data"
463,"Calculates distance matrices for Maine, New Hampshire and New York from the US health 2005 data set. The distance measures are Euclidean, Manhattan and maximum distance."
465,draw moving animal
466,"This is the example quantlet. You see here the coding file named the same way as the ""Name of Quantlet"" field in this document, but with the .py extension (could be also .ipynb or .r etc.). This metainfo.txt file is needed to create the README automatically and index through the quantlet.com website. So your project is searchable there. All the images in the same folder as the metainfo.txt will be attached to the README file automatically (it does not matter how you call it)."
467,Plot fair premium curves
469,Implementation of the Adaptive Weights Community Detection algorithm.
471,Time series classification using PCA + SVM. Examples in cryptocurrency market.
472,Simulates the risk bound of a CARE model according to theta1 parameter constellation at expectile level tau = 0.05
476,Estimate the adaptive interval length based on the estimated critical value at expectile level 0.01 for the select three stock markets.
482,Evaluates the likelihood ratio test statistics for parameter constellation theta1 and expectile leve 0.01
486,Evaluates the critical values based on the likelihood ration and risk bound at parameter constellation theta1 and expectile leve tau = 0.05
488,Evaluates the critical values based on the likelihood ration and risk bound at parameter constellation theta1 and expectile leve tau = 0.01
490,"Estimates length of the interval of homogeneity in trading days across the selected three stock markets from 3 January 2006 to 31 December 2014 for the modest (upper panel, r = 0.5) and the conservative (lower panel, r = 1) risk cases, at expectile level 0.0025."
492,Evaluates the likelihood ratio test statistics for parameter constellation theta1 and expectile leve 0.05
496,"Plots the simulated critical values across different parameter constellations for the modest (upper panel, r = 0.5) and conservative (lower panel, r = 1) risk cases, for two expectile levels: 0.05 (blue) and 0.01 (red)."
497,"Provides descriptive statistics for the selected index return time series from 3 January 2005 to 31 December 2014 (2608 trading days): mean, median, minimum (Min), maximum (Max), standard deviation (Std), skewness (Skew.) and kurtosis (Kurt.)"
498,Estimates the adaptive interval length based on the estimated critical value at expectile level 0.01 for the select three stock markets.
504,Simulates the risk bound of a CARE model according to theta1 parameter constellation at expectile level tau = 0.01
508,"Estimates the parameters of CARE model with fix rolling window for selected indices. Selected rolling window lengths: 1 month (20 days), 3 months (60 days), 6 months (125 days) and 12 months (250 days) for tau = 0.01."
512,Estimate the adaptive interval length based on the estimated critical value at expectile level 0.0025 for the select three stock markets.
518,"Estimates length of the interval of homogeneity in trading days across the selected three stock markets from 3 January 2006 to 31 December 2014 for the modest (upper panel, r = 0.5) and the conservative (lower panel, r = 1) risk cases, at expectile level 0.05."
520,Evaluates the critical values based on the likelihood ration and risk bound at parameter constellation theta3 and expectile leve tau = 0.05
522,Evaluates the likelihood ratio test statistics for parameter constellation theta3 and expectile leve 0.01
526,Simulates the risk bound of a CARE model according to theta2 parameter constellation at expectile level tau = 0.01
530,"Simulates the time series data following asymmetric normal distributions (AND), the simulated datasets are saved as txt and dat files."
531,Evaluates the critical values based on the likelihood ration and risk bound at parameter constellation theta2 and expectile leve tau = 0.01
533,Generate analytical intervals for LCARE model
534,Evaluates the likelihood ratio test statistics for parameter constellation theta2 and expectile leve 0.05
538,LCARE_Index_Returns plots selected index return time series from 3 January 2005 to 31 December 2014 (2608 trading days)
539,Simulates the risk bound of a CARE model according to theta3 parameter constellation at expectile level tau = 0.05.
543,"Pools the estimated parameters together based on CARE model, and show the descriptive statistics  first quartile (25% quantile), mean value, third quartile (75% quantile))"
544,Simulates the risk bound of a CARE model according to theta3 parameter constellation at expectile level tau = 0.01
548,Evaluates the likelihood ratio test statistics for parameter constellation theta2 and expectile leve 0.01
552,Evaluates the critical values based on the likelihood ration and risk bound at parameter constellation theta2 and expectile leve tau = 0.05
554,Simulates paths of a CARE model with fixed parameter  vector.
556,Simulates the risk bound of a CARE model according to theta2 parameter constellation at expectile level tau = 0.05
560,Evaluates the likelihood ratio test statistics for parameter constellation theta3 and expectile leve 0.05
564,Summarizes the risk bound for different parameter constellations and two expectile levels 0.01 and 0.05.
565,"Estimates the parameters of CARE model with fix rolling window for selected indices. Selected rolling window lengths: 1 month (20 days), 3 months (60 days), 6 months (125 days) and 12 months (250 days) for tau = 0.05."
569,Evaluates the critical values based on the likelihood ration and risk bound at parameter constellation theta3 and expectile leve tau = 0.01
571,"Estimates length of the interval of homogeneity in trading days across the selected three stock markets from 3 January 2006 to 31 December 2014 for the modest (upper panel, r = 0.8) and the conservative (lower panel, r = 1) risk cases, at expectile level 0.01."
573,"We provide results for ""Quantinar: A Blockchain p2p Ecosystem for Honest Scientific Research"", Bag et all (2022). Please refer to the GitHub wiki for a detailed description on how to use the code!"
574,Code infrastructure for paper Hedging Cryptocurrency Options
591,Simulates and estimates Markov-switching skew t - t factor copulas with two regimes.
605,All source codes for HAND.
609,"‘Performs the Penalized Adaptive Method (PAM), a combination of propagation-separation approach and a SCAD penalty, to fit a model with possible structural changes on a given dataset. Compares the fit to models of Cochrane and Piazzesi (2005) and Ludvigson and Ng (2009). The input data are monthly observations of k-year excess bond risk premia, k = 2, 3, 4, 5, forward rates and other pre-defined macro variables. Computes RMSE, MAE, R^2 and adjusted R^2 for the fitted models. Plots the time series of the fitted vs. observed values of bond risk excess premia.'"
611,"‘Performs the Penalized Adaptive Method (PAM), a combination of propagation-separation approach and a SCAD penalty, to fit a model to a simulated data with a pre-defined change point. Computes the percentage of correctly identified change points over a specified number of scenarios.’"
613,"‘Performs the Penalized Adaptive Method (PAM), a combination of propagation-separation approach and a SCAD penalty, to evaluate empirical coverage probability of bootstrapped confidence regions of penalized likelihood ratio.’"
615,"‘Performs the Penalized Adaptive Method (PAM), a combination of propagation-separation approach and a SCAD penalty, to fit a model with possible structural changes on a given dataset. The model is then used for forecasting over a 1-year horizon. The input data are monthly observations of k-year excess bond risk premia, k = 2, 3, 4, 5, forward rates and other pre-defined macro variables. Computes RMSPE and MAPE for the forecasted values. Plots the time series of the predicted vs. observed values of bond risk excess premia over the prediction interval.’"
618,This Quantlet provides necessary functions for portfolio optimization. Single optimization routines are stored in the subfolder opt. The Quantlet wraps them and produces a set of estimated weights as well as the true returns of the portfolios.
619,This Quantlet processes and prepares the raw data used in the thesis. It creates returns and formats them into workable dataframes.
620,"This Quantlet produces visual results of the optimization routines. It gives information about (cumulated) performance, density, success metrics and their reliability as well as results according to strategies and estimators."
622,This Quantlet calculates the necessary parameters for the optimization routines based on the sample data.
624,Add Crypto currencies to Portugal stocks or DAX30 stocks to form portfolios using Markowitz(1952) method.
625,"Provides the scatter plot of log returns of gold, US treasury bond 3 yr and CRIX."
626,Add Crypto currencies to Portugal stocks or DAX30 stocks to form portfolios using Markowitz(1952) method with shortsell constraints.
627,Provides a boxplot comparing the standard deviation of cryptocurrencies against stock markets.
629,Add Crypto currencies to portfolios including Portugal stocks or DAX30 stocks to check whether there are return increase
630,Provides a boxplot comparing the median trading volume of cryptocurrencies against US stock market.
631,Estimates a linear model (ANOVA). The plot presents results of the estimation of a linear model
632,Calculates and plots the simplicial depth of a simulated data set.
633,Illustrates the proof of increasing monotonicity of a probability function on positive real numbers for a volatility model with natural parameter theta.
634,Plots the Lorenz curve in order to check the goodness of the model.
635,Draws a plot of the Kullback-Leibler function for a canonical parameter.
636,Estimates the risk aversion parameter based on the data given in the table and using a logit model.
637,Performs a linear regression with data from The General Social Survey (GSS) using an instrumental variable.
638,Performs a Kolmogorov-Smirnov test on the DAX log return data from 2009-12-21 to 2011-12-22
639,Plots the time series of the DAX30 index from 2009-12-21 to 2011-12-22
640,"Computes or plots the sample auto-correlation function (ACF) of a univariate, stochastic time series. When called with no output arguments, AUTOCORR displays the ACF sequence with confidence bounds."
641,Performs a Jarque-Bera test on the DAX log return data from 2009-12-21 to 2011-12-22 based on skewness and kurtosis
642,Plots quarterly S&P log-returns for data from Q2 1980 to Q2 2012 to check the normality assumption for log-returns.
643,"Generates n = 300 samples, then performs linear regressions in the error in design model."
644,"Computes GLM coefficients, standard errors, z- and p-values and gives overall model fit."
645,Illustrates the proof of an alpha-level LR test. Output is a plot.
646,Constructs the confidence bands for an expectile curve with a confidence level of 0.95.
647,Draws n observations from a standard normal distribution and plots its empirical distribution function (edf) vs. the normal cumulative distribution function (cdf). Number of draws can be entered interactively.
648,Performes a Cramer von Mises normality test on DAX log return data from 2009-12-21 to 2011-12-22
649,"Plots the DAX returns from 2000 to 2011 and performs an F-test for variance comparison of specific time periods. From the plot a period of higher volatility is followed by a period of lower volatility and again followed by a period of higher volatility. To proof this finding, the time series is split into 3 intervals, each of which is tested against the other for variance comparison. The variances of the first and the third period differ significantly from the second period."
650,Plots the critical region of the Neyman-Pearson test within a specific interval if p is a density of the standard Cauchy distribution.
651,Creates plots and scatterplots of densities and LLR for two different parameters theta.
652,"Plots the deconvoluted kernel regression curve, the kernel regression curve from the sample without measurement errors (i.e. kernel regression based on x) and the kernel regression curve from the sample with measurement errors (i.e. kernel regression based on z)."
653,Tests the hypothesis of the equality of the covariance matrices on two simulated 4-dimensional samples of sizes n1=30 and n2=20.
654,Plots quarterly S&P 500 index log-returns for data from Q2 1980 to Q2 2012.
655,"Generates n=300 standard normal distributed random sample, and draws the kernel density curve of the estimated kernel density function using a Gaussian kernel. Since the kernel density function is biased in a finite sample, one can not compare it with the true density directly. One rather compare it with the expectation of the kernel density function under the true density. Then it draws the expectation of the kernel density function under the true density."
656,Computes a linear regression in a model that explains the vocabulary score in a grade by using the vocabulary scores of the two previous grades (vocabulary.rda).
657,Plots and illustrates the spectral model and the linear model in this exercise.
658,Draws n observations from standard normal distribution and plots its empirical distribution function (edf) vs. the normal cumulative distribution function (cdf). Number of draws can be entered interactively. The point where the edf and cdf differs most is also reported.
659,Plots the significance levels in context with n from n=1 to n=200.
660,"Generates an n=300 t(3) distributed random sample, and draws the kernel density curve of the estimated kernel density function using a Gaussian kernel, since the kernel density function is biased."
661,Plots the scores with respect to probability of default and response variable.
662,Draws a plot of the Kullback-Leibler function for a natural parameter.
663,Draws n observations from exponential(1) distribution and plots its empirical distribution function (edf) vs. the exponential cumulative distribution function (cdf). Number of draws can be entered interactively. The point where the edf and cdf differs most is also reported.
664,Visualisation of c and beta_min dependent on least fraction omega and minimum bound for probability that specific feature is used as analysis for the behaviour of central limit theorem (CLT) of GRF.
665,"Computation and visualisation of (mean) ICE (individual conditional expectation) with and without treatment effect computed with a generalized Random Forest, for variable S3 (=Students self-reported expectation for success in the future) in the National Mindset Study data set used by Athey and Wager in ""Estimating Treatment Effects with Causal Forests: An Application""."
666,"Computation and visualisation of aggregation of ICQ (individual conditional quantile) with and without treatment effect computed with a generalized Random Forest, for variable S3 (=Students self-reported expectation for success in the future) in the National Mindset Study data set used by Athey and Wager in ""Estimating Treatment Effects with Causal Forests: An Application""."
667,"Computation and visualisation of (mean) ICE (individual conditional expectation) computed with a generalized Random Forest, for variable S3 (=Students self-reported expectation for success in the future) in the National Mindset Study data set used by Athey and Wager in ""Estimating Treatment Effects with Causal Forests: An Application""."
668,"Visualisation of huberized loss function consisting of L1 and L2 loss, its scoring function and the derivative of the scoring function for symmetric and asymmetric loss function. The loss function can be used for approximation of functions, e.g. in Generalized Random Forests."
669,"Computation and visualisation of (mean) ICE (individual conditional expectation) computed with a generalized Random Forest, for variable ""ratio001""= (Total assets / Shareholder Funds) - 1 in the peer to peer lending dataset as was used in Giudici et al. (2019)."
671,"Estimation of effective weights alpha_i(x_1, x_2) for the infeasable observations theta_tilde. The effective weights are computed by a regression forest on a grid of observations x_ij=(-0.5 + i/n1, 0 + 0.02 * j) for i=1,...,n1 and j=0,...,50, for n1=50, 100, 200 and target variable Y_i=theta(x_ij) + eps_ij, for a given theta function, here triangle function theta(x_ij) = max(0, 1 - |x_ij,1|/0.2), with Gaussian noise eps_ij with mean zero and standard deviation of 0 and 0.1."
672,"Estimation of infeasable observations of theta_tilde, based on given theta (generally unknown) and effective weights (alpha) of (generalized) random forest."
675,"Estimation of effective weights alpha_i(x) for the infeasable observations theta_tilde. The effective weights are computed by a regression forest on n=500, 1000, 2000 observations of grid data x_i = -1 + 2*i/n and target variable Y_i=theta(x_i) + eps_i, for a given theta function, here triangle function theta(x) = max(0, 1 - |x|/0.2), with Gaussian noise eps_i with mean zero and standard deviation of 0 and 0.1."
678,"Estimation of effective weights alpha_i(x) for the infeasable observations theta_tilde. The effective weights are computed by a quantile random forest on n=500, 1000, 2000 observations of uniform distributed data x_i ~U[-1,1] and target variable Y_i=theta(x_i) + eps_i, for a given theta function, here polynomial theta(x) = x + 4x^3, with Gaussian noise eps_i with mean zero and standard deviation of 1."
679,Displays the effect of the expiration date on the level of estimated loadings corresponding to the second component.
680,Estimates second partial derivative of the curves using local polynomial regression.
681,Estimates smooth second partial derivative for each curve using local polynomial regression using individual bandwidths.
683,"Employs a moving window to estimate an AR(1) model for the loadings and to estimate standard deviation of the VDAX, and plots the results."
684,"It interpolates the estimated functional components at a common grid and plots them, together with the estimates loadings time series."
685,Implements the Epanechnikov kernel function for the local polynomial regression.
686,Estimates state price densities (SPD) from a sample of discretely observed and noisy call price curves at different time to maturity and strike prices. Use FPCA decomposition of the dual covariance matrix and estimate functional components of the SPD and their corresponding loadings.
687,"Calculates two-dimensional variance estimator, based on Hall and Marron (1990)."
688,"Starts a simulation for varying number of curves and observations per curve for repeated samples, and performs a comparison of the methods."
689,Simulates real and noisy call prices using a mixture of log-normal densities.
691,"Calculates the scagnostic measures for the dataset and plots the SPLOM, the scagnostics SPLOM and the heat-map of the scagnostic measures"
692,Plots of COVID19 data in Austria in the time interval 27.02. - 02.04. in R. Additionaly a wordcloud - code from Python. Corona.txt is needed to create the wordcloud.
696,Calculates the scagnostic measures for the dataset p2p
697,Scheduling algorithm for UAV teams based on a genetic algorithm.
700,Three dimensional interactive scatterplot for unemployment regressed on the number of armed forces and the GNP. The data belongs to the longley dataset. A hyperplane for the regression results shows a negative realtionship between umemployment and the number of armed forces. The increase in GNP is associated wih higher unemployment this is odd.
701,"The trivariate normal distribution is produced by random sampling. In order to specify the dependency between the variables, it is necessary to define the covariance matrix. The mean vector for all three variables is also needed. The bivariate marginal distributions are estimated by copulae."
702,"Graphic of a normal distribution N(0,1) with a right-tailed (1-alpha)-confidence interval"
703,"A simple density estimation with the epanechnikov kernel for the DAX log-returns. The plot states the number of obervations N = 1859 and the auto-selected bandwidth, optimally chosen by the function density equal to 0.001645."
704,"Graphic of the probability mass function of the poission distribution vs. binomial distribution. The binomial is given by bars, while the binomial distr. is plotted as dots. It can be seen that the poisson as well as the binomial distribution closely approache the binomial distribution for the specified values."
705,"The binomial distribution is plotted for sample sizes 5, 10 and 100 and probabilities 0.1, 0.5 and 0.9. The plots show nicely the shape of the pdf for the different values and show when it can likely be approximated by the normal distribution."
706,Multiple LOESS regressions for DAX log-returns on FTSE log-returns. The parameter alpha is alternated to see the effect of this parameter for the regression results. The smootheness of the regression line increases with a higher alpha. This causes a smaller bias but a higher variance.
707,"The HAC package is used to create a random sample. The distribution is created iteratively. First the bivariate marginal distribution is created for the variables, which have a higher correlation. In the second step the bivariate marginal distribution is stacked together with the remaining variable. The actual observations are obtained, by using the quantile function of the respective marginal distribution. The trivariate distribution is plotted in a three dimensional scatterplot with its bivariate marginal distributions."
708,"Graphic of a normal distribution N(0,1), also called standard normal distribution, with a two-tailed (1-alpha)-confidence interval. It is sometimes called Gauss, after the mathematician, or Bell curve, after its shape."
709,"Nine Plots are created for three different archimedean copulae with the same dependency parameter (theta=2). The figures in the left column are created based on normal marginal distributions. In the right column all copulae are based on a t-distribution with six degrees of freedom. The rows depict the Gumbel, Clayton and Frank copulae in descending order."
710,"Perform normal principal component analysis (PCA) on the data ""banknotes"" from package ""ncomplete"". Plot that correlation of the NPCs and the originial variable including the unit circle in the plot for reference."
711,"Graphic of a normal distribution N(0,1) with a left-tailed (1-alpha)-confidence interval"
712,Plots the probability mass distribution of the Poisson distribution for different values of the shape parameter lambda. It can be seen that the poisson distribution approaches the normal distribution for higher lambda.
713,"A 3d plot for the function z = (1/3) * x^3 * (1/4) * y^4, which is the integral of its first derivative with respect to both function arguments. Both arguments are in the interval [0, 1]. The function wireframe included in the lattice package is used."
714,"Graphic of the Probability Mass Function of a binomial distribution vs. normal distribution. The binomial is given by bars, while the normal distr. is plotted as a red line. It can be seen that the binomial closely approaches the normal distribution for the specified values."
715,Three dimensional interactive surface plot with different front and back drawing for the multivariate function z = 0.1 * (x^2 - y^2). The front is drawn by a solid colour and the back with lines.
716,"The plots for a conditional or grouped variable differ, even if you use the same variable. The left plot has three panels for the types of Species and contains four different combinations of iris characteristics. This plot corresponds to Species as a conditioning variable. Species as a grouping variable produces the right plot. Here four panels are created, which differ by combinations of variables and types of species."
717,"Multidimensional Scaling is performed on a subset of the data ""cars93"" from package ""MASS"". First, the subset is selected, then the MDS model is fitted using Euclidean distance. The result is shown in a plot."
718,"Box-Plot of the dataset nhtemp. The notation on the axes is calculated manually, giving a nice example of the calcuation of a five number summary, on which the box-plot is based. Note that R by default draws the whiskers to the highest/lowest value in the sample which is still in the upper-/lower fence."
719,A 3d plot for a nonlinear progamming problem. The black hyperplane depicts the function to be optimized. A red hyperplane depicts the constraint of the function.
720,"QQ plots compare empirical quantiles of a distribution with theoretical quantiles of the standard normal distribution. If the degrees of freedom for the t-distribution tend to infinity, the QQ plot is a 45 degree line. Quantiles of the t-distribution and normal distribution are identical. The t-distribution converges to the normal distribution in infinite samples."
721,The rpanel package employs different graphical user interface (GUI) controls to enable the immediate communication with the graphical output and provides dynamic graphics. In this example the rp.listbox function is presented. The function adds a list of items to the control panel. Selecting an item calls an action function and modifies the graphical output accordingly. In this case rp.listbox is used to switch between a histogram and a boxplot.
722,"The membership grade of a fuzzy set with elements x, y, z is plotted. The number of elements is plotted in a bar diagramm. "
723,"Time series datasets are easily plotted with lattice. Here the dataset Nile is used to illustrate the usage of the package for time series data. The plots show the annual flow of the river Nile from 1871 to 1970. The first plot depicts the whole series and the second plot splits the series into three intervals, where each interval is one panel."
724,This code shows how to create different shapes with random numbers in R. Six different objects are created with six different functions. Each function uses variables as coordinates for the shapes created.
725,The exponential distribution only works for positive random variables. The normal and logisitc distribution are special cases of the exponential distribution. Lambda defines the steepness of the pdfs.
726,"Example of non-metric Multidimensional Scaling (MDS) on the data ""voting"" from package ""HSAUR2"". This approach is useful for a number of negative eigenvalues and ordinal data."
727,Stundent's t-distribution is the ratio of a normal distribution and a chisquare distribution. The t-distribution is used for several statisitcal tests. The degrees of freedom specify the moments of the distribution.
728,"The F-distribution, also called the Fisher-Snedecor distribution, is the ratio of two independent chisquared distributed random variables. The degrees of freedom of the numerator and denominatoer determine how the pdf and cdf behave. If n=1 the pdf is monotonically decreasing and the ordinate is the asymptote. For n=2 the pdf intersects the ordinate at 1 and monotically decreases. Only if n>=3 the pdf is assymetrically bell shaped."
729,A world map as an illustrative example how powerful R is. This plot is supposed to show new users an interesting application of R.
730,"Perform multiple linear regression (MLR) on the dataset ""UScereal"" from package ""MASS"" and construct four diagnostic plots. These show first the residual errors plotted against their fitted values to check for a distinct trend. The second plot shows spread-location to detect skewness. The third plot is a QQ plot to check the deviation from the theoretical distribution of the errors. The fourth plot shows each points leverage."
731,The rpanel package employs different graphical user interface (GUI) controls to enable the immediate communication with the graphical output and provides dynamic graphics. The rp.tkrplot function uses the tkrplot package and enables to place a plot and its control panel in a single window. In this example a density plot with a control for altering the bandwidth is drawn.
732,Multiple spline regressions for DAX log-returns on FTSE log-returns. The lambda is alternated to see the effect of lambda for the regression results. The smootheness of the regression line increases with a higher lambda. This causes a smaller bias but a higher variance.
733,Multiple unform kernel regressions for DAX log-returns on FTSE log-returns. The bandwidth is alternated to see the effect of the bandwidth for the regression results. The smootheness of the regression line increases with a higher bandwidth. This causes a smaller bias but a higher variance.
734,"The Chi-squared distribution has two special pdfs. If the degrees of freedom are equal to one, the mean is undefined and the vertical axis is an asymptote. If the degrees of freedom are equal to two the pdf steadily decreases from 0.5."
736,The rpanel package employs different graphical user interface (GUI) controls to enable the immediate communication with the graphical output and provides dynamic graphics. In this example a control panel with a slider and a double button is created to control the bandwidth of a density plot.
737,This Quantlet produces plots to show the effect of the bin size on the smoothness of an histogram.
738,Three dimensional interactive plot for five spheres with coordiantes and radii obtained by random sampling. It is possible to interactively select different scenes of the plot.
739,A function is implemented to use all four different kernel weighting functions to compute the respective weight of a point x w.r.t. its distance to the point of evaluation x_0. Four plots are created to illustrate each weighting functions and their properties.
740,Multiple kNN regressions for DAX log-returns on FTSE log-returns. The selection of nearest neighbours k is alternated to see the effect of k for the regression curves. The smoothness of regression line is less smooth with a smaller k. This causes a smaller bias but a higher variance.
741,"The chi-square distribution describes the sum of independent squared standard normal random variables. Most commonly used in tests regarding the sample variance. The pdf is bell shaped, moves to the right and becomes symmetric for higher degrees of freedom."
742,"Boxplot of R example dataset Nile and manual calculation and notation of important values. The notation on the axes is calculated manually, giving a nice example of the calcuation of a five number summary, on which the box-plot is based. Note that R by default draws the whiskers to the highest/lowest value in the sample which is still in the upper-/lower fence."
743,Different lighted plots for three dimensional data. The data is obtained by univariate random normal sampling. Light sources are then alternated for the different plots to show the effect on the appearance of the plot.
744,A 3d plot for a simple linear progamming problem. The black hyperplane depicts the function to be optimized. A red hyperplane depicts the constraints of the function.
745,"Performs a factor analysis on a subset of the data ""decathlon"" from package ""FactoMineR"" for three factors. The result is depicted in a correlation plot."
747,This plot shows how to use R to create your own diagrams. As an illustration the different sample types are displayed in a diagram.
748,"Plot of the Empirical Cumulative Distribution Function of the R example dataset Formaldehyde, specifically Formaldehyde$car."
749,Probability Mass Function of the binomial distribution with number of trials n = 10 and probability of success p = 0.2
750,A chi-squared distributed random variable is the sum of independent standard normally distributed rv. Histograms for continuous variables just estimate the pdf of the rv. As the degrees of freedom increase the empirical density (histogram) converges to the normal distribution.
751,"The code below plots the pdfs and cdfs for three special cases of the stable distribution for which the pdf and cdf have a closed form expression. The special cases are the normal, Cauchy and Levy distribution. Normal distributions have a kurtosis of three and zero skewness. The Cauchy distribution is symmetric around its mean but has thicker tails than the normal distribution. Random variables with a Levy distribution have a skewness which tends to one. Only variables which are above the mean will be observed."
752,Analysis of principal component analysis (PCA) in order to identify the efficient number of principal components. The first three PCs are plotted against each other. Another plot shows the cumulative percentage variance against the number of components.
753,Histogram with default choice of grids based on R example data nhtemp. Histograms are commonly used to visualize data frequencies of continous variables.
754,The Cauchy distribution has no mean and no variance. Its location and form are defined by the parameters mu and sigma. Mu defines the position of the peak (mode) and the median.
755,"The code creates two graphics. One graphic is a scatterplot of the bivariate normal distribution. Contours are plotted to illustrate for which combinations of the two variables the density function is the same. You can call them also isolines. The other plot is a 3 dimensional scatterplot, which shows the specific value of the joint pdf. It looks like a mountain. The highest point is the expectation."
756,"Example of Linear Discriminant analysis using the data ""spanish"" and ""spanishMeta"" from package ""LanguageR"". The data is randomly split into a training and test set beforehand. The observations and discrimiation borders are shown in a plot and the prediction error is calculated."
757,"Graphic of the binomial cumulative distribution function with observations n = 10 and two different probabilities for the event, p = 0.2 (black) and p = 0.6 (red)."
758,"A 3d plot for the Rosenbrock function: z = 100*(y - x^2)^2 + (1 - y)^2. The minimum (1, 1) is found by the Nelder Mead method."
759,Two random variables are created. One is normally distributed and one is uniformly distributed. Their correlations are calculated with different methods. Kendall and Spearman's methods are superior to Pearson's correlation measure. Both are less variant due to monotone increasing transformations and outliers.
760,"Plots two pseudo random variables with (1.) uniform distribution and (2.) N(0,1) distribution with clayton (theta=0.79) dependence structure."
761,"The normal distribution is very often applied in statisitics. It is a stable distribution, which is symmetric around its mean. The standard normal distribution has an expectaion of zero and variance equals one. Higher variance leads to flatter pdfs and cdfs."
762,A bivariate normal distribution is created by the function normalCopula.The normalCopula function creates copula objects and also computes joint densities and probabilities. Both rv have a correlation of 0.7. The first plot is a standard scatterplot of the two variables. A contour and three dimensional scatterplot illustrate the cdf of the two variables. The last two plots depict the density functions for the rvs. One can observe the elliptical shape of the contour lines.
763,Interactive scatterplot for a univariate regression of unemployment on GNP. The data comes from the longley dataset and is a time series. It is possible to include and exclude the regression line and residuals.
764,A 3d plot for the function: z = 0.03*sin(x)*sin(y) - 0.05*sin(2*x)*sin(y) + 0.01*sin(x)*sin(2*y) + 0.09*sin(2*x)*sin(2*xy). All maxima are depicted by blue points and all minima by red points. The function is used to illstrate algotithms in R to find roots of multivariate functions. Here the BFGS method is used to find the optima.
765,This Q installs all necessary packages in the book Basic Computational Statistics.
766,"Pie Chart of chickwts$feed depicting the shares of different feed types. It is a common technique for qualitative or discreet variables. Since the human brain does not distinguish as easily between angles and shapes as it does between lengths, bar diagrams and bar plots are preferred."
767,"Bar diagram and plot of relative frequencies of the R data subset ""chickwts$feed"". It is an efficient way of plotting the frequencies of discreet variables, particularly useful to illustrate the behaviour (variation) over time."
768,"Implementation of the Lagged Fibonacci Generator (LFG) for random numbers. The seed is a sequence of j integers, of which one integer should be odd. The value of the modulus is 2^l and j and k denote the number of lags. Resulting from the use of the Fibonacci Sequence, the generated sequences do not have satisfactory randomness properties."
769,"Implementation of the RANDU random number generator developed by IBM in the 1960s. It is a Linear Congruential Generator procedure. This generator has some cearly non-random characteristics, due to badly chosen starting values."
770,Perform stepwise regression on all subsets to compare the values of R squared for all combinations of explanatory variables. All combinations with R squared > 0.7 are shown in a plot of R squared against subsample size.
771,A unform kernel regression for DAX log-returns on FTSE log-returns. Higher log-retunrs of the FTSE predict higher log-returns of the DAX. This comovement seems reasonable considering the close relationship between both indices.
772,"Performs a cluster anlalysis on the data ""agriculture"" from package ""cluster"" using the Ward algorithm. The result is depicted in a dendrogram with the optimal clusters highlighted by red boxes."
773,The distribution of solar radiation and average ozone concentration depending on wind and temperature is depicted in the plot. The two continous conditioning variables are transformed into factors. Wind has now just four categories and Temeperature three. There are 12 possible combination of both factors coinciding with the number of panels.
774,Multiple nonparametric regressions for DAX log-returns on FTSE log-returns. They show how different nonparametric regressions predict the same simulated data.
775,"Plot of two variables from the R example dataset longley (GNP, employed) including a regression line (OLS) and grid."
776,"The bias, variance and mean squared error of an estimator for a histogram with origin x0 = 0 evaluated at x = 0.001 and X ~ N(0,1) is depicted in the plot. The binwidth minimizing  the MSE h MSE is depicted by a black point."
777,The regression curve and data for a simulated model. The variable Yis depends on Xis and a standard normally distrbuted
778,"Histogram of R example data nhtemp with manual choice of grids. A manual choice of grid is necessary, for example, for small samples, since R's default way of calculation is based on the Sturges formula, which performs poorly for n < 30. Histograms are commonly used to visualize data frequencies of continous variables."
779,Two plots for the empirical cumulative distribution function standardised and not standardised log-returns for the DAX and FTSE indeces. The edf for both is close enough for the Kolmogorov-Smirnov test to not reject the null hypothesis of different distributions. This is not  true for non standardised log-returns.
780,"The code produces an inteactive plot for the illustration of the Newton-Raphson method. In the example the univariate function f(x) = x^2 - 4 is optimized. An optimum is found, if the first derivative of the function at this point is equal to zero. Therefore the method looks for the root of the first derivative."
781,"The code creates two graphics. One graphic is a scatterplot of the bivariate normal cumulative distribution. Contours are plotted to illustrate for which combinations of the two variables the probability is the same. You can call them alo isolines. The other plot is a 3 dimensional scatterplot, which shows the specific value of the joint pdf."
782,MSE for k-NN regression using the Leave-one-out cross validation method.
783,"The plots for the density of a conditional or grouped variable are for the used data the same. In this case it is preferable to use the same variable as a group variable. R produces one panel with the densities for the different groups, which are drawn with different lines. Therefore a direct comparison of the different groups is possible. In this example the dependency of weight and the food of chickens is illustrated."
784,"A stable distribution can be linearly transformed and stays a stable distribtuion. The location and scale parameter for the plots are identical. Only the skewness and kurtosis varies among the graphs. If beta, the skewness parameter, is smaller than zero, the distribuion is skewed to the left. Therefore smaller values of the rv are more likely. If beta is greater than zero, the opposite is true. Alpha determines the kurtosis of the distribution. Higher values lead to thicker tails and therefore values far from the mean are more likely."
785,"Perform a cluster analysis using single, complete and average linkage algorithms on the data set ""agriculture"" from the package ""cluster"". The three resulting dendrograms are plotted to show the divergence in the results."
786,"The datset barley contains yield data from Minnesota. Average yield is plotted depnding on where it is planted (site) and which variety is used. The third conditioning variable is time creating two panels. Higher average yield is associated with greater and lighter rectangles. Therefore four variables are illustrated in one graph (yield, time, variety, site)."
787,"Two plots of the probability functions of the hypergeometric distribution (plotted as lines) vs. the binomial distribution (plotted as dots). The binomial distribution is a limiting form of the hypergeometric distribution, for sample sizes much smaller than population size. In that case, the binomial distribution can be used to approximate the hypergeometric distribution."
788,Multiple Gaussian kernel regressions for DAX log-returns on FTSE log-returns. The bandwidth is alternated to see the effect of the bandwidth for the regression results. The smootheness of the regression line increases with a higher bandwidth. This causes a smaller bias but a higher variance.
789,This Quantlet produces a plot showing the acceptance rejection method for pseudo genrated random variable. It is used to consider whether an observation belongs to one or another distribution.
790,Histogram for the height of trees from the trees package. It is possible to display density estimates for the height based on a assumed normal distribuion and estimated density.
791,Generates synthetic data in form of a partial linear model to apply simulations for causal inference estimation. The parameter of interest is the treatment or uplift effect for a binary treatment assignment.
792,"Deviations in Risk Neutral Density (RND) and Historical Density (HD) can be used to develop trading strategies. The Risk Neutral Density is estimated via Rookley’s Method, which uses Local Polynomial Regression to smooth the implied volatility surface. In order to estimate the Historical Density, a GARCH(1,1) model is used to fit the bahavior of the underlying. In the scope of this thesis two python packages were developed and deployed on pypi.org - localpoly and spd_trading."
799,R code to replicate the figures in Quantile Cross-Spectral Measures of Dependence between Economic Variables. Introduce quantile cross-spectral analysis of multiple time series which is designed to detect general dependence structures emerging in quantiles of the joint distribution in the frequency domain
811,"This project aims to predict the death of patients suffering from heart disease. In this way, it might be possible to adapt the treatments and maybe avoid heart failures in some cases."
813,Spatial analysis of Berlin rent prices and landlord premiums
818,"Creates a relational database, fills it with bitcoin transaction and does an analysis on them."
829,"Focuses on the cointegration of price and return of cryptocurrencies and macro indicators, pandemic situation and media index"
830,This Quantlet build takes in as an input a PDF filedecomposes it by words and counts them by occurance. The output is saved as a JSON file.
833,The main file which performs the Monte Carlo simulation for the coverage ratio of the bootstrap multivariate confidence band for nonparametric kernel expectile regression. The data generating model is a homogeneous model.
836,The main file which performs the Monte Carlo simulation for the coverage ratio of the theoretical multivariate confidence band for nonparametric kernel quantile regression. The data generating model is a heterogeneous model.
839,The main file which performs the Monte Carlo simulation for the coverage ratio of the bootstrap multivariate confidence band for nonparametric kernel expectile regression. The data generating model is a heterogeneous model.
842,The main file which performs the Monte Carlo simulation for the coverage ratio of the theoretical multivariate confidence band for nonparametric kernel expectile regression. The data generating model is a heterogeneous model.
845,The main file which performs the Monte Carlo simulation for the coverage ratio of the theoretical multivariate confidence band for nonparametric kernel expectile regression. The data generating model is a homogeneous model.
848,The main file which performs the Monte Carlo simulation for the coverage ratio of the bootstrap multivariate confidence band for nonparametric kernel quantile regression. The data generating model is a heterogeneous model.
851,"The main file which generates the 2 dimensional (age, years of schooling) 3D figures"
854,The main file which performs the Monte Carlo simulation for the coverage ratio of the theoretical multivariate confidence band for nonparametric kernel quantile regression. The data generating model is a homogeneous model.
857,The main file which performs the Monte Carlo simulation for the coverage ratio of the bootstrap multivariate confidence band for nonparametric kernel quantile regression. The data generating model is a homogeneous model.
860,"The supporting materials for paper ""COVID Risk Narratives - A Computational Linguistic Approach to the Econometric Identification of Narrative Risk During a Pandemic"". Techniques included - sentiment analysis (bag-of-words method), topic modelling (LDA), word embedding (word2vec), SIR epidemiological model, vector autoregression. Main use of the codes - 1) Identification of important/perennial economic narratives. 2) Monitor narratives of interest for financial forecasting."
870,"We analyze cryptoasset markets (cryptocurrencies and stablecoins) at high frequency. We investigate intraday patterns. We show that Tether plays a crucial role as a safe haven and/or store of value facilitating trading in cryptocurrencies without going through traditional currencies. Markets centered on cryptocurrencies and stablecoins play a primary role aggregating preference/technology shocks and heterogeneous opinions, instead markets centered on the US Dollar play a marginal role on price formation."
871,Compute Bollinger bands indicator for btc.
872,"Build three cryptocurrencies portfolios based on price movements forecasts with MLP, RNN and LSTM neural networks in order to outperform CRIX. We apply three different weight strategies: price, marketcap and equally weighted portfolios. We use the predictions to build quarterly rebalanced buy and hold portfolios and also long-short portfolios. We compare the performance with the baseline models."
873,Build a cryptocurrency portfolio based on price movements forecasts with recurrent neural networks in order to outperform CRIX
874,Tuning of different MLP architecture for BTC trend predictions
880,"‘Performs the Penalized Adaptive Method (PAM), a combination of propagation-separation approach and a SCAD penalty, to fit a model to a simulated data with two pre-defined change points. Computes the percentage of correctly identifing the homogeneous intervals.’"
882,This Project uses Twitter API to analyse the sentiments of Cryptocurriencies 
883,Uses Rookley Method to estimate risk neutral density. Based on data of crypto options traded on Deribit.
887,Uses parameters of option to approximates the implied volatility via the Black-Scholes-Formula (red). Compares to implied volatility from Deribit API (blue).
888,Compares the risk neutral density and the historical density of bitcoin prices. For more detailed description of estimation methods of densities see Quantlets CrypOpt_RiskNeutralDensity and CrypOpt_HistoricalDensity.
893,Uses Monte Carlo sampling on historical returns to estimate historical density of underlying of an option.
895,Comparison of the performance of CRIX against other cryptocurreny indices and the total crypto market.
896,"Market simulation without battery in microgrid, the procedure is the same as in script 3 except for not including a battery in the microgrid. Again the supply demand ratio is calculated.  Energy prices are then determined using the current tariffs in Germany and energy bills are calculated using a P2P pricing mechanism. The economic profitability of the microgrid is evaluated via self-consumption rates,  self-sufficiency of the grid and the cost of community energy as well as individual energy bills."
897,"Market simulation with a battery in a microgrid, battery charging and discharging rates in a microgrid with a battery and the supply demand ratio are calculated. Energy prices are then determined using the current tariffs in Germany and energy bills are calculated using a P2P pricing mechanism. The economic profitability of the microgrid is evaluated via self-consumption rates, self-sufficiency of the grid and a comparison of costs for the participants with and without P2P trading."
898,Descriptive statistics of energy data imports and plots the data for energy production and consumption.
901,"Battery simulation: the net energy load is calculated by subtracting aggregate consumption from aggregate production by prosumers. Then, a simulation of six different battery sizes is done."
904,"R codes used for a DEDA talk on GA. The first part shows how canonical GA finds maxima, the second part is Linear Regression using a default dataset in R."
905,Simulation of Bitcoin prices and LTV ratios using a stochastic volatility with correlated jumps model and computation of the expected value of an exemplary crypto-collateralized P2P lending contract.
906,Illustration of possible behaviour of LTV Ratios in a Crypto-based P2P Lending contract using historical Bitcoin prices
907,CC's in Portfolio Management: Investigating the benefits of introducing CRIX to the traditial portfolio through DCC GARCH modeling of Maximum Diversification Portfolio
908,Scrapes market data for bitcoin from cryptocompare.com using API access
909,"Exploratory Data Analysis for the final data frame that consists of tweet sentiments, google trend and market data"
911,"Merges all the components i.e. tweets, trends and market data that have been scraped into one file and applies sentiment analysis on the tweets"
914,Scrapes google trends data for keywords bitcoin and BTC using API access
915,Creates the csv that will be read into create_final_df. All daily tweets will be aggregated monthly and then concatted into one csv file
916,Models for the crypto volatility analysis
917,"Stable_LogDensity_Financials compares parametric densities of Normal, Stable and Cauchy with empirical financial market returns on log-scale to emphasize non-normal behavior."
918,"Stable Rescaling illustrates the principle rescaling the distribution under stable distributions, i.e. elliptically stable distributions."
919,Boundary conditions of American put option
922,Computes the exercise price of American put option using a binomial tree for assets without dividends. In order to see whether the pricing is reliable or not.
925,Computes the optimal exercise boundary problem of American put option with different volatility for assets without dividends
930,Simulates and plots various One-Factor Short-Rate Models that describe the process of the instantaneous short term interest rate.
932,"Realized volatility analysis using harModel of Dow Jones, CAC 50, FTSE 100 and Euro-USD exchange market"
933,Calculates and plot the payoff of a cliquet put/call option. When there is no reset points this will generate the pay off graph of a European put/call option.
934,"Compare and plots two possible paths of the asset price. Once the asset price hits the barrier, the option expires worthless or becomes activated (depending upon the type of barrier option), regardless of future stock price."
935,Generates and plots the path of a general Ito process.
937,Computes the optimal exercise boundary problem of American put option using a binomial tree under different volatilities
940,Calculates the historical mean and volatility of default rate bonds belong to different ratings and also the corresponding regression by exponential function fitting. Estimates the asset correlation using one-factor model.
941,"Calculates and plots BS price of a cliquet call option as a function of S, r, D, sigma and points of reset. The cliquet option is priced as sum of aligned start forward options with strike price equal to the underlying at the reset points."
942,Realized variance analysis and graphical representation of 5 seconds intraday highfrequency realized variance from the German stock market DAX and comparison with it's lagged subsample
943,Realized variance analysis and graphical representation of 5 seconds intraday highfrequency realized variance from the Mexica stock market IPC and comparison with it's lagged subsample
944,Realized volatility analysis of mexican stock market IPC using harModel predective function and som time series analysis
946,Computes American option prices and the optimal exercise boundary.
949,Generates and plots the path of a Wiener process and the corresponding Ito Integral for a given specification of c and delta_t.
952,We focus on the inverse BTC options traded in Deribit and provide codes for calculating greeks under SV.
953,Give a few examples of centrality calculation
954,Transfer adjacency matrices to network plots
955,"Plots the dynamic centrality of risk spillover network of large banks, via thresholding by only considering the 30% largest connections of the network. Window size is 252 trading days and step is 22 trading days"
956,Compare network centralities using the example of Minnesota road network
957,"Plots the average risk spillover effects after thresholding, by only considering the 30% largest connections of the network, with various centrality scores attached to each bank"
958,"Understanding Cryptocurrencies (UCC). Cryptocurrencies refer to a type of digital cash that use distributed ledger - or blockchain technology - to provide secure transactions. These currencies are generally misunderstood. While initially dismissed as fads or bubbles, most large central banks are considering launching their own version of national cryptocurrencies. In contrast to most data in financial economics, there is a plethora of detailed (free) data on the history of every transaction for the cryptocurrency complex. Further, there is little empirically-oriented research on this new asset class. This is an extraordinary research opportunity for academia. We provide a starting point by giving an insight into Cryptocurrency mechanisms and detailing summary statistics and focusing on potential future research avenues in financial economics."
963,"Quantlets for the codes used in my master thesis. The data published here are artificial and only for the purpose to illustrate how the codes work. This thesis uses the concept of generalized quantiles to compute probabilistic forecasts of volume weighted average prices (VWAP) stemming from the German intraday market for electricity contracts. These prices exhibit extreme values in both directions and correlate inter- and intradaily. Dimensions are reduced by functional principal component and factorisable sparse tail event curve techniques. The dependency structure of the factor scores for generalized quantiles is analyzed with a VAR model that allows for incorporation of exogenous information such as renewable energy production forecasts. Price forecasts from both models are evaluated with root mean squared error and mean absolute error. Interval forecasts are evaluated, to which share the interval captures observed prices. Supplementary material for this thesis is available online."
964,Forecast of daily VWAP curves with FASTEC VAR model. Generate table with error measures and plots of daily curves.
965,Application of FASTEC on the training data. Fit VAR model on factor laodings. Generate plots of daily curves and tables with error measures.
966,Plot of difference between DA spot prices and VWAP of intraday trading vs forecast error of residual load and renewable energy production
967,"Application of FPCA on the training data.  Fit VAR and Arima model on FPC scores. Generate plots and tables with error measures, FPC explanatory power, FPC scores and FPC eigenfunctions."
968,Forecast of daily VWAP curves with FPCA VAR / FPCA ARIMA model. Generate table with error measures and plots of daily curves.
969,Polygonplot that marks enrgy porduction sources for a winter and summer day.
970,Sequence of plots that show daily price and residual load movements.
971,Plot and tables to describe VWAP and exogenous variables
972,Plot intradaily curves of VWAP and residual laod as surface plot.
973,Conducts a Diebold-Marino tests on the different models.
974,"Analyzes mergers & acquisitions on the German energy market, including approximation with different distributions and forecasting with ARIMA and GARCH"
975,Scalable implementation of Lee / Mykland (2012) and Ait-Sahalia / Jacod / Li (2012) Jump tests for noisy high frequency data
978,"Solution to the attempt of finding an optimal exercise strategy for a CP2P contract based on Aave´s lending pool mechanism. Plotting the function shows, that there is no root and therefore no optimal exercise strategy, if continuous price paths are considered."
979,Illustration of Aave´s coin liquidation mechanism based on a theoretical CP2P contract for June 2021 and historical ETH prices. It is assumed that the fictional borrower does not take action to avoid liquidations.
980,Empirical analysis of on-chain data from Aave v1.
981,LVU thesis: This notebook creates the table for the comparison of performance metrics between Instrumented Principal Component Analysis and Conditional Autoencoder.
983,LVU thesis: This notebook creates the plots resulting from the analysis of variable importance for the estimation of conditional risk exposures.
985,LVU thesis: This notebook creates the box plot of maximum extreme returns in the raw data.
987,LVU thesis: This notebook creates the plots resulting from the analysis of pricing performance over time compared to CRIX returns.
989,LVU thesis: This notebook creates the plot that demonstrates an overfitting example regarding training and validation loss.
991,LVU thesis: This notebook creates the table for the descriptive statistics of the input features.
993,LVU thesis: This notebook creates the plots resulting from the analysis of days used for the estimation of conditional risk premia.
995,LVU thesis: This notebook creates the plots resulting from the analysis of different window sizes for the training of the Conditional Autoencoder.
997,LVU thesis: These notebooks analyzes the Conditional Autoencoder's performance for varying training window lengths.
1003,LVU thesis: This notebook analyzes the predictive performance of the Conditional Autoencoder for varying numbers of days used for the estimation of risk premia.
1005,LVU thesis: This notebook regresses the estimated latent factors on observable factors proxied by long-short portfolios.
1007,LVU thesis: This notebook calculates the performance of IPCA on the same data used for training of the Conditional Autoencoder.
1010,LVU thesis: This notebook analyzes the variable importance for the estimation of conditional risk exposures.
1012,LVU thesis: This notebook calculates the 27 observed asset characteristics from the raw data.
1014,"Creates simulation data for different scenarios mimicking the real life scenarios, applies the local parametric approach to detect change points, regime shifts and structural breaks and then compares the estimates from fixed rolling window estimates and LPA approach using RMSE. It also provides all the plots along with the confidence intervals "
1015,"Applies the local parametric approach to detect change points, regime shifts and structural breaks in real life datasets and then compares the estimates from fixed rolling window estimates and LPA approach using RMSE. It also provides all the plots. The data is however downloaded with special permissions from Thomson Reuters and is not publicly available. If you have permission to access the data, please get in touch with us with permission and we will share the data with you"
1016,"Simulates time series data (AR1, sine curve, white noise) and applies the UMAP VizTech to reflect local and global structure of the data. "
1017,"Simulates the path of a First-order autoregressive (AR-1) process over 50 time points. Epsilon terms/innovations are normally distributed - N(0,1). Model: X_t = 100 + r(X_t-1 - 100) + e_t. Mean reverting to 100, 0 < r < 1."
1018,"Similarity of both random walk and AR-1 (autoregressive process) to actual stock prices. The blue path is a random walk over 50 time points. The red path is a First-order autoregressive (AR-1) process over 50 time points. Model: X_t = 100 + r(X_t-1 - 100) + e_t. Mean reverting to 100, 0 < r < 1. In each case epsilon terms/innovations are normally distributed - N(0,1)."
1019,"Simulates the path of a random walk over 50 time points. Epsilon terms/innovations are normally distributed - N(0,1)."
1020,"Comparison of GMM, 2SLS and OLS based on simulated data"
1021,Test Capital Asset Pricing Theory using Generaliyed Method of Moments.
1022,Implementation of logistic ordinal regression (aka proportional odds) model
1023,"Logistic Regression A logistic regression is an example of a <Generalized Linear Model (GLM)>.
The input values are the recorded O-ring data from the space shuttle launches before 1986, and the fit indicates the likelihood of failure for an O-ring.
Taken from http://www.brightstat.com/index.php?option=com_content&task=view&id=41&Itemid=1&limit=1&limitstart=2"
1024,"Short demonstration of Python for scientific data analysis This script covers the following points: * Plotting a sine wave * Generating a column matrix of data * Writing data to a text-file, and reading data from a text-file * Waiting for a button-press to continue the program exectution (Note: this does NOT work in ipython, if you run it with inline figures!) * Using a dictionary, which is similar to MATLAB structures * Extracting data which fulfill a certain condition * Calculating the best-fit-line to noisy data * Formatting text-output * Waiting for a keyboard-press * Calculating confidence intervals for line-fits * Saving figures For such a short program, the definition of a <main> function, and calling it by default when the module is imported by the main program, is a bit superfluous. But it shows good Python coding style."
1025,"Show different ways to present statistical data The examples contain: - scatter plots - histograms - cumulative density functions - KDE-plots - boxplots - errorbars - violinplots - barplots - grouped boxplots - pieplots - scatterplots, with markersize proportional to value of corresponding variable - 3D surface and wireframe plots"
1026,"Multiple Regression - Shows how to calculate the best fit to a plane in 3D, and how to find the corresponding statistical parameters. - Demonstrates how to make a 3d plot. - Example of multiscatterplot, for visualizing correlations in three- to six-dimensional datasets."
1027,Simple manipulations of normal distribution functions. - Different displays of normally distributed data - Compare different samples from a normal distribution - Work with the cumulative distribution function (CDF)
1028,"Practical demonstration of the central limit theorem, based on the uniform distribution"
1029,Example of a one-and two-sided binomial test.
1030,Plots of different continuous distribution functions. - T-distribution - F-distribution - Chi2-distribution - Exponential - Weibull
1031,"Different discrete distribution functions. - Binomial distribution - Poisson distribution (PMF, CDF, and PPF)"
1032,"Demonstration of the package <lifelines> Based on the demo-code in http://lifelines.readthedocs.org, by Cam Davidson-Pilon"
1033,"Common formatting and print commands, for the book ""Introduction to Statistics with Python"". These commands ensure a common layout, and reduce the code required to generate plots in the other modules."
1034,Two-way Analysis of Variance (ANOVA) The model is formulated using the <patsy> formula description. This is very similar to the way models are expressed in R.
1035,Example of a Kruskal-Wallis test (for not normally distributed data)
1036,Comparison of two groups - Analysis of paired data - Analysis of unpaired data
1037,"Analysis of one group of data
This script shows how to - Use a t-test for a single mean - Use a non-parametric test (Wilcoxon signed rank) to check a single mean - Compare the values from the t-distribution with those of a normal distribution"
1038,"Analysis of Variance (ANOVA) - Levene test - ANOVA - oneway - Do a simple one-way ANOVA, using statsmodels - Show how the ANOVA can be done by hand. - For the comparison of two groups, a one-way ANOVA is equivalent to a T-test: t^2 = F"
1039,"Multiple testing This script provides an example, where three treatments are compared. It first performs a one-way ANOVA, to see if there is a difference between the groups. Then it performs multiple comparisons, to check which of the groups are different.
This dataset is taken from an R-tutorial, and contains a hypothetical sample of 30 participants who are divided into three stress reduction treatment groups (mental, physical, and medical). The values are represented on a scale that ranges from 1 to 5. This dataset can be conceptualized as a comparison between three stress treatment programs, one using mental methods, one using physical training, and one using medication. The values represent how effective the treatment programs were at reducing participant's stress levels, with higher numbers indicating higher effectiveness. Taken from an example by Josef Perktold (http://jpktd.blogspot.co.at/)"
1040,"Simple linear models. - <model_formulas> is based on examples in Kaplan's book <Statistical Modeling>. - <polynomial_regression> shows how to work with simple design matrices, like MATLAB's <regress> command."
1041,Code for generating Anscombe's Quartet. Very closely based on the code from the seaborn-documentation http://web.stanford.edu/~mwaskom/software/seaborn/examples/anscombes_quartet.html Note that this program requires a web-connection to load the dataset!
1042,"Linear regression fit
Parameters ---------- x : ndarray Input / Predictor y : ndarray Input / Estimator alpha : float Confidence limit [default=0.05] newx : float or ndarray Values for which the fit and the prediction limits are calculated (optional) plotFlag: int (optional) 1 = plot, 0 = no_plot [default]
Returns ------- a : float Intercept b : float Slope ci : ndarray Lower and upper confidence interval for the slope info : dictionary, containing return information on - residuals - var_res - sd_res - alpha - tval - df newy : list(ndarray) Predictions for (newx, newx-ciPrediction, newx+ciPrediction)
Examples -------- >>> import numpy as np >>> from fitLine import fitLine >>> x = np.r_[0:10:11j] >>> y = x**2 >>> (a,b,(ci_a, ci_b),_)=fitLine(x,y)
Notes ----- Example data and formulas are taken from D. Altman, <Practical Statistics for Medicine>"
1043,"Analysis of multivariate data - Regression line - Correlation (Pearson-rho, Spearman-rho, and Kendall-tau)"
1044,Three different ways to fit a linear model: - analytically - using the statsmodels tools - using the formula-based approach
1045,"Example of bootstrapping the confidence interval for the mean of a sample distribution This function requires <bootstrap.py>, which is available from https://github.com/cgevans/scikits-bootstrap"
1046,"Calculate the sample size for experiments, for normally distributed groups, for: - Experiments with one single group - Comparing two groups"
1047,"Graphical and quantitative check, if a given distribution is normal. - For small sample-numbers (<50), you should use the Shapiro-Wilk test or the ""normaltest"" - for intermediate sample numbers, the Lilliefors-test is good since the original Kolmogorov-Smirnov-test is unreliable when mean and std of the distribution are not known. - the Kolmogorov-Smirnov(Kolmogorov-Smirnov) test should only be used for large sample numbers (>300)"
1048,Analysis of categorical data - Analysis of one proportion - Chi-square test - Fisher exact test - McNemar's test - Cochran's Q test
1049,"Example of PyMC - The Challenger Disaster This example uses Bayesian methods to find the  mean and the 95% confidence intervals for the likelihood of an O-ring failure in a space shuttle, as a function of the ambient temperature. Input data are the recorded O-ring performances of the space shuttles before 1986."
1050,"Plots the Correlation of an index, here CORE, against crypto currencies"
1051,Cumulative return with the equally risk weighted strategy and SP500 core with a window of 100 days
1052,Cumulative return of the Hybrid Strategy and SP500 core with a window of 100 days
1053,Negative excess return frequency with respect to window length for SP500 core
1054,Cumulative return of the different strategies and the core for a window length of 100 days
1055,Cumulative return of the different cores with an initial investment of 1 dollar
1056,Cumulative return ofthe Naive Strategy and SP500 core with a window of 100 days
1057,Cumulative return with the Minimum Variance strategy and SP500 core with a window of 100 days
1058,Cumulative return of the different strategies and the core with their optimal window
1059,Cumulative return ofthe Volume weighted Strategy and SP500 core with a window length of 100 days
1060,Kernel regression to estimate the conditional instantaneous moments of CP2P borrowing rate.
1061,"Interest rate plots of USDC hourly borrowing rate on Compound, AAVE v1 and v2."
1062,This Quantlet builds two efficient frontiers from 5 stocks to demonstrate sensitivity of weights to change of the mean estimator
1065,This Quantlet builds robust global minimum variance portfolio for random 600 constituents of Russell3000 for the period 20100101-20201231. The robust procedure based on projected gradient descent technique with median-of-means estimator as described in the paper. The performance of portfolios is compared with benchmark portfolios
1069,This Quantlet builds robust global minimum variance portfolio for 81 S&P100 constituents for the period 20000101-20201231. The robust procedure based on projected gradient descent technique with median-of-means estimator as described in the paper. The performance of portfolios is compared with benchmark portfolios
1073,"Finds the best linear model by choosing x regressors from an amount of n possible variables. It fits the regressors with the dependent variable and shows different quality criteria like R^2 and BIC-criteria. Additionally, it shows how to reduce the Google Correlate Dataset to deselect useless words. In a further step a OLS regression summaries the results to give more information about the model. Additionally, it plots a Heat-map of qualitative regressors and a scatterplot with one selected regressor and the dependent variable."
1074,"Decision Tree and Random Forest: Builds a classification tree to predict, wheather the customer choose the Citrus Hill (CH) or the Minute Maid (MM) Orange Juice. A number of characteristics is used in order to grow a tree. The second part builds a random forest with the same dataset. A visualization of the error terms are shown."
1075,Shows how to get a time series from Google Trend with R. Shows some outputs of linear regression with a business cycle time series (Unemployment rate). Main focus points on visualization of the two time series.
1076,Creates two stacked barplots with legend. It is a panel series. The barplots are ordered by year and within the plots by amount/frequency. The left barplot shows the unemployment rate for Germany and its states by amount. The right barplot shows the frequency of the search term 'Arbeitsamt' for Germany and its states. The bars are plotted all together with a corresponding legend.
1077,Training a simple LSTM model for weather forecasting task and comparing with a MLP. The data is downloaded automatically. To run just install the requirements with pip inside your environment.
1078,"Create and train four neural networks corresponding to different architectures (Dense, RNN, GRU and LSTM) on Fashion Mnist dataset."
1079,Train with keras several MLP with one hidden layer estimating a sinus function to compare activation functions and layer size (neurons number) effect.
1080,Gradient descent implementation from scratch in order to train an artificial neural network to fit a regression of city-cycle fuel consumption in miles per gallon.
1081,"Calculates and plots the Minimum Spanning Tree and the Delaunay Triangulation for the 'Eye distribution', as well as the histogram and Boxplot of the edges lengths of the Scatter Plot"
1083,Calculates and plots the Delaunay Triangulation for a Scatter Plot
1088,Simulates different types of distributions and the respective plots; evaluation of scagnostic characteristics for the different types of distributions for multiple amounts of data points
1090,Plots the alpha shapes
1091,Calculates and plots a Minimum Spanning Tree from a Scatter Plot
1093,Analysis of how the scagnostic coefficients behave for different types of distribution with different amounts of data points.
1103,"Time series of estimated weekly (left panel, rolling windows covering 1800 observations) and daily (right panel, rolling windows covering 360 observations) EACD(1, 1) parameters and functions thereof based on seasonally adjusted 1-minute trading volumes for Intel Corporation (INTC) at each minute from 22 February to 31 December 2008"
1104,"Estimates intra-day periodicity components for cumulative one minute trading volumes (in units of 100,000 and plotted against the time of the day) of selected companies at NASDAQ on 2 September (lower, lowest 30-day trading volume) and 30 October 2008 (upper, highest 30-day volume)"
1107,"Estimated EACD(1, 1) parameters and functions thereof based on seasonally adjusted 1-minute trading volumes for all selected stocks at each minute from 22 February to 31 December 2008"
1114,"Estimate univariate, stationary Autoregressive Moving Average (ARMA) models using Reversible Jump Markov Chain Monte Carlo"
1150,Represents large documents with at least partially overlapping vocabulary in a 3d vector space
1151,Uses a decision tree regression to give a visual example of overfitting
1152,Contains applications based on Python gensim to train and use Latent Dirichlet Allocation (LDA) and Dynamic Topic Models (DTM)
1155,Plots index terms of large text corpora with respect to their document frequency in the corpus
1156,Plots the SNRI in comparison to the Granger causlity measure of Billio et al. (2012) and the total connectedness of Diebold and Yilmaz (2014).
1157,"Plots the average risk spillover effects after thresholding, by only considering the 30% largest connections of the network."
1158,Plot of the systemic network risk index and its cubic spline interpolation.
1159,"Plots the returns, the VaR and the CoVaR of eight global systemically important banks from the US."
1160,Plots the average risk spillover effects over the whole estimation period as well as over the period within three months of the Lehman bankruptcy.
1161,"CRIX.TERES_EVT estimates the risk of the CRIX index in moving windows using an Extreme Value Approach (EVT) and the Tail Event Risk Expected Shortfall (TERES) methodology. To implement the TERES methodology, either the Laplace or a stable distribution can be chosen. If a stable distribution is implemented, the index parameter alpha needs to be estimated first for each of the moving windows. This estimation takes a lot of time. Therefore, estimation results are provided for the example data set crix.csv. Shape and Scale parameter estimated during the EVT approach are also provided in the final table of results."
1162,"An integration of social media characteristics into an econometric framework requires modeling a high dimensional dynamic network with dimensions of parameter $\Theta$ typically much larger than the number of observations. To cope with this problem we impose two structural assumptions onto the singular value decomposition of $\Theta = U D V^{\top}$. Firstly, the matrix with probabilities of connections between the nodes of a network has a rank much lower than the number of nodes. Therefore, there is limited amount of non-zero elements on the diagonal of $D$ and the whole operator admits a lower dimensional factorisation. Secondly, in observed social networks only a small portion of users are highly-affecting, leading to a sparsity regularization imposed on singular vectors $V.$
Using a novel dataset of 1069K messages from 30K users posted on the microblogging platform StockTwits during a 4-year period (01.2014-12.2018) and quantifying their opinions via natural language processing, we model their dynamic opinions network and further separate the network into communities. With a sparsity regularization, we are able to identify important nodes in the network."
1167,Analyzes transaction data obtained by Bitcoin_Evaluation.R. Connects and clusters Bitcoin addresses to approximate the true users. Returns a table of how well the users are assigned to the optimal clusters.
1172,Simulates a network of employees and firms over a time period of ten weeks with flexible parameters regarding the frequency with which transaction types take place. Saves a file containing the transaction data and the corresponding truth table used to simulate the data.
1183,"Reads, summarizes and plots the dataset consisting of the DEM/USD and GBP/USD foreign exchange rates from 31/12/1979 to 01/04/1994."
1184,Generates the Latex code for tables of regression results in Risk Analysis of Cryptos as Alternative Asset Class.
1185,Plots word evolution for the former crypto-currency exchange Mt Gox.
1186,Shows VaRs of Joe copula model introduced in the paper. The alphas are chosen eqaul to .001.
1187,Plots the temperature of different events relevant for the crypto-currency market.
1188,Estimates the Power Law parameter Alpha as well as the goodness of fit of right tail of wealth distribution of Bitcoin.
1189,Shows VaRs of Gumbel copula model introduced in the paper. The alphas are chosen eqaul to .001.
1190,"computes the values of the expected returns and the expected shortfall (ES) of the optimal portfolios with different holding weights % for the 1st underlying asset under an EGARCH(1,1) model, where the number of assets is 2"
1191,"Estimates the bivariate GARCH model, its (co)variance processes and plots them."
1193,Shows VaRs of DVine copula model introduced in the paper. The alphas are chosen eqaul to .001.
1194,"Performs quantile LASSO regression in a moving window by using BIC and GACV criteria to choose penalty parameter (lambda). The simulated data contains a break point, after which a combination of changes of variance of the error term, correlation structure of the design matrix and number of active parameters (q) in the model is simulated. Plots time series of lambda in quantile LASSO regression. Furthermore, the cardinality of the active set q, the L2-norm of the residuals and the L1-norm of the parameter beta are plotted. All of the plots contain results from a number of simulations and the average over all of them."
1196,Computes the values of the self-financing trading strategy and the FTSE TWSE Taiwan 100 Index for three different economic conditions with different fixed bounds of risk.
1197,Shows VaRs of CVine copula model introduced in the paper. The alphas are chosen eqaul to .001.
1198,Plots time evolvement and sample autocorrelation function for IBM. The upper panel presents the time-evolvement of the monthly realized beta over the period raging from 2001 to 2006 with 95%-confidence interval. The sample autocorrelation function of realized betas in the lower panel indicates the significant serial correlation.
1199,Shows VaRs of Clayton copula model introduced in the paper. The alphas are chosen eqaul to .001.
1200,"Performs quantile LASSO regression in a moving window by using BIC and GACV criteria to choose penalty parameter (lambda). The simulated data contains a break point after which the parameter beta changes. Plots time series of lambda in quantile LASSO regression. Furthermore, the cardinality of the active set q, the L2-norm of the residuals and the L1-norm of the parameter beta are plotted. All of the plots contain results from a number of simulations and the average over all of them."
1202,Loads output from dynamic topic model and provides tools for analysis.
1204,"Plots kernel density estimation of the realized volatility and of correspondingly standardiyed returns for IBM, 2001-2006. The dotted line depicts the density of the correspondingly fitted normal distribution. The left column depicts the kernel density estimates based on a log scale."
1205,Shows VaRs of HAC-Gumbel copula model introduced in the paper. The alphas are chosen eqaul to .001.
1206,"Computes the values of the expected returns and the SRM with power utility of the optimal portfolios with different holding weights for the 1st underlying asset under an EGARCH(1,1) model, where the number of assets is 2."
1208,computes the optimal expected returns with a generating function of the SRM related to the HARA utility
1209,Estimates the Power Law parameter Alpha for wealth distribution of Bitcoin and Auroracoin.
1210,"Plots time series of average lambda taken from FinancialRiskMeter together with the implied volatility index reported by the Chicago Board Options Exchange. Daily observations are collected from 6 July 2007 to 14 July 2016 and normalized to interval (0,1)."
1211,Plots the time series of the goodness of fit of Power Law model for both Bitcoin and Auroracoin.
1212,Estimates the Power Law parameter Alpha as well as the goodness of fit of right tail of wealth distribution of Auroracoin.
1213,Plots word evolution for different cryptocurrency mining technologies.
1214,"Plots time series of average lambda from quantile LASSO regression computed in XFGTVP_FRM together with systemic risk measures such as the implied volatility index reported by the Chicago Board Options Exchange (VIX) and the Standard & Poor""s 500 stock market index (S&P500) downloaded from Yahoo Finance, CoVaR computed with single index model (CoVaR_S) and linear regression model (CoVaR_L) from TENET_SIM and TENET_Linear, financial turbulence, the composite indicator of systemic risk (CISS) downloaded from ECB Statistical Data Warehouse, the volatility connectedness index (VC) obtained from financialconnectedness.org and credit spread and yield slope downloaded from Federal Reserve Board. All of the measures including average lambda are normalized to interval (0,1)."
1215,Simulates and plots the Dirichlet distribution with various values for alpha.
1217,Generates 3 histgrams of wealth distribution for Bitcion and Auroracoin.
1218,Plots frequency of words of the NASDAQ data set regarding sectors.
1219,Shows VaRs of HAC-Clayton copula model introduced in the paper. The alphas are chosen eqaul to .001.
1220,Shows VaRs of Student-t copula model introduced in the paper. The alphas are chosen eqaul to .001.
1221,"Plots event partition over varying k parameters with k being either 10, 30 or 50 topics."
1222,Estimates the Power Law parameter Alpha as well as the goodness of fit of wealth distribution of Bitcoin with Xmin = 1 by default.
1223,"Uses the estimated parameters from XFGmvol02 to simulate two bivariate processes with and without interaction terms, forecast (ex-ante) bivariate densities by means of kernel density estimation and compare them at the actual relizations of the exchange rate process."
1224,Plots time evolvement and sample autocorrelation function of the realized volatility for IBM. The upper panel shows the evolvement of daily realized volatility over the analyzed sample period 2001-2006. The lower panel presents the implied sample autocorrelation functions (ACFs).
1225,Preprocesses textual and meta data from a crypto forum database and generates input for dynamic topic model.
1227,"Plots realized volatility signature for IBM, 2001-2006. Average time between trades: 6.78 seconds."
1228,"Performs quantile LASSO regression in a moving window by using BIC and GACV criteria to choose penalty parameter (lambda). The input data are daily logarithmic stock returns of 200 largest U.S. financial companies and 6 macro variables. Each of the selected companies is taken as a dependent variable and is regressed by means of linear quantile regression with L1-penalty term on the remaining ones together with the macro variables. Plots time series of lambda in quantile LASSO regression. Furthermore, the cardinality of the active set q, the L2-norm of the residuals and the L1-norm of the parameter beta are plotted. All of the plots contain results from a number of regressions (equal to number of selected companies) and the average over all of them. Writes resulting time series of average lambda chosen by BIC and GACV into a .csv file"
1230,Shows VaRs of HAC-Frank copula model introduced in the paper. The alphas are chosen eqaul to .001.
1232,Shows VaRs of Gaussian copula model introduced in the paper. The alphas are chosen eqaul to .001.
1233,Uses the estimated parameters from XFGmvol02 to simulate and plot (co)variance processes for the BEKK-type model and a bivariate process where the covariance and the interaction terms are set to zero.
1234,Animated illustration of alpha shapes for different values of alpha.
1235,A pretty Christmas Tree Gif made in Python
1236,"This Quantlet is an example of training case-based reasoning and compared with other machine learning methods. The dataset is the German credit data publicly available in UCI. The credit reform data used in paper is confidential and can be collected based on requests from the Blockchain Research Center (BRC, https://blockchain-research-center.de/)"
1239,"Simulations are carried out to illustrate the behavior of the test under the null and also the power of the test under plausible alternatives. An economic application considers the causal relations between the crude oil price, the USD/GBP exchange rate, and the gold price in the gold market."
1242,"Show the portfolio performance, one of which is constructed by minimum spanning tree."
1243,Compare hierarchical clustering and clustering by minimum spanning tree. The data set is S&P 500 index.
1244,"Give an example of minimum spanning tree, using S&P 500 index for clustering."
1247,Show the evolution of minimum spanning tree using varying time window.
1249,Give an example of 8 points to constuct MST .
1251,Show cryptocurency price using in the dataset.
1252,Compare the weights of different portfolios using different parameter gamma.
1253,Give distance matrix for minimum spanning tree using 10 cryptocurrencies.
1254,Give an example of using hierarchical clustering to illustrate how to choose the number of clusters by minimum spanning tree. The data set is S&P 500 index.
1255,Give an example of using minimum spanning tree for clustering. Divide vertices into three groups.
1258,Give an example using hierarchical clustering to choose number of clusters in minimun spanning tree.
1259,Generate SCRIX
1262,"This chapter first introduces the backshift operator which is one way to make a nonstationary time series stationary. It presents the AR, MA and ARMA models and discusses their properties. Also distinguishes the ARMA model from the ARMA process."
1263,‘Quant analysis of financial time series´
1264,Quantile loss function and Quantile LASSO function applying generalized approximate cross validation (GACV)
1265,Create scatterplot with asymptotic and bootstrap confidence bands as well as linear regression and quantile regression
1267,Calculate closeness centrality based on given adjacency matrix and plot closeness centralities of network members
1268,VaR forecasting based on quantile regression and validation of forecast by backtesting
1269,Map geographic polygons of waterways and bodies of water in the city of Berlin (data from Berlin Senate)
1270,Map geographic information on the municipal structure and historic divisions of Berlin
1272,Map geographic polygons of green spaces in the city of Berlin (data from Berlin Senate)
1280,"Clustering results displayed in the reduced space by applying different types of dimension reduction methods, including PCA and tSNE."
1282,Performs a monotone transformation to the estimated stimulus utilities of the car example by applying the pool-adjacent-violators (PAV) algorithm. Plotly technology is used (D3.js and Java Script)
1283,The document similarity of quantlets is calculated based on their keywords. For this purpose quantlets are taken from the MVA book and BCS project. First the keywords are transformed into the vector representation. Then the scalar product is applied calculating so the similarity measure. The advanced term-term correlation model additionally uses the term-term correlation matrix between the terms of all documents. Finally the k-means algorithm with the Euclidean norm is applied for clustering (four clusters) and the data are represented via MDS (multidimensional scaling) showing metric MDS for BCS quantlets and metric MDS for MVA quantlets. Plotly technology is used for visualization(D3.js and Java Script)
1284,Plotting DApps tSNE
1285,"BigQuery on Kaggle platform to obtain dataset with ethereum hashes, contracts"
1287,Classification of smart contracts using traditional and deep learning methods
1295,Scraping source codes given hashes
1296,scraping Dapp contract hashes using API
1297,"obtaining Dapp smart contracts using API, parsing source code"
1299,Plotting contracts over time
1300,"Parsing of smart contracts, data frame creation"
1303,The performances of various Ordered Pairwise Partitioning algorithm variants are compared to the performances of nominal classification algorithms and to the performance of the Ordinal Logistic Regression Model (Proportional Odds Model). For the analysis the models are applied to predict the number of abalone rings as a proxy of the age of the shellfish.
1306,Use complex numbers as parameters to fit an elephant that wiggle its trunk. Creates an MP4 file of the wiggling elephant
1307,Animated illustration of sine  curve.
1308,Explainable ML method Case-Based Reasoning for financial risk detection
1311,"Calculates the stock price on the nodes in the implied tree, the transition probability tree, the Arrow-Debreu tree and local volatility using Barle and Cakici's method."
1312,"Plots the combination of a long call and a short call where the exercise price of the short call is higher than the exercise price of the long call, i.e. bull call spread."
1313,Calculates the price of a European call option according to Black Scholes.
1314,"Produces a plot of the sample path for a stochastic process X_t = (-1)^t * X, t = 1, 2,... with a random variable X."
1315,Compares the asymptotic and bootstrapped confidence band of the conditional quantile curve for Bank of America and Citigroup weekly returns.
1316,Calculates the Black-Scholes call price of a stock and produces a table of the costs of hedging.
1317,Plots the time series of daily returns and log returns for Coca-Cola company from 1 January 2002 to 30 November 2004.
1318,"Compares different extrapolation methods (constant, linear and quadratic) used for calculating option prices and IVs (Implied Volatilities)."
1319,Computes the probabilities for 5 states of a state-dependent binomial process.
1320,Plots call option prices as a function of strikes for 85<K<115. The implied volatility function may be fixed to the strike prices (sticky strike) or moneyness K/S (sticky moneyness). Compares the relative difference of both approaches.
1321,"Plots the right tail of the logarithmic empirical distribution of the portfolio (Bayer, BMW, Siemens) negative log-returns from 1992-01-01 to 2006-09-21."
1322,"Provides backtesting results for Value-at-Risk under the Block Maxima Model with 0.05 level for the portfolio formed by Bayer, BMW, Siemens shares during from 1992-01-01 to 2006-09-01."
1323,"Compares the prices of the risk reversal functions for the following implied volatility curves, given as a function of strike price: (i) f1(K) = 0.000167 x K^2 - 0.03645 x K + 2.080 (ii) f2(K) = 0.000167 x K^2 - 0.03645 x K + 2.090 (iii)f3(K) = 0.000167 x K^2 - 0.03517 x K + 1.952. The plots compare the functions f2 and f3 to the function f1."
1324,Calculates and compares the price of a plain vanilla call option with a power call option.
1325,"Uses linear interpolation of option prices C1 and C3 and implied volatilities in order to approximate the price of another option C2. All options have the same strike, but vary in maturity with tau1<tau2<tau3."
1326,"Produces a PP and a QQ Plot of the daily log-returns from 1992-01-01 to 2006-12-29 of a portfolio of Bayer, BMW and Siemens stock."
1327,Performs an empirical analysis using the data on DAX and Dow Jones index from the period from 1997 to 2004.
1328,"Plots a sample path of Brownian Bridge U_t=W_t-tW_1 in interval [0, 1]."
1329,Produces a contour plot of the Gumbel copula density for theta=2.
1330,"Calculates the stock price on the nodes in the implied tree, the transition probability tree, the Arrow-Debreu tree and local volatility using Derman and Kani's method."
1331,Plots the time series of daily stock prices for Coca-Cola company from 1 January 2002 to 30 November 2004.
1332,Plots the autocorrelation function of an MA(3) (moving average) process.
1333,Calculates and compare the prices of the calendar spread for the following implied volatility curves given as a function of maturity. (i) f1(tau) = 0.15 x tau + 0.05. (ii) f2(tau) = 0.15 x tau + 0.06. (iii)f3(tau) = 0.10 x tau + 0.075. The plots compare the functions f2 and f3 to the function f1.
1334,"Simulates a standard Wiener process Y_t=W_(T+t)-W_T, for T>0."
1335,"Plots the combination of a long call and a long put where the call strike is larger than the put strike, i.e. a strangle strategy."
1336,Calculates the summary statistics for the EUR/USD and GBP/USD exchange rates and plots the time series of daily returns on both exchange rates for the period from 1 January 2002 to 1 January 2009.
1337,Plots the loss distribution of portfolio of two zero coupon bonds with zero recovery.
1338,Plots a strap option strategy. It uses a long position in one put and two (or more) calls in anticipation of upward price movement.
1339,Computes European/American option prices using a binomial tree for assets without dividends.
1340,"Plots the combination of a long call and a long put with the same strike, i.e. a straddle option strategy."
1341,Produces a graphic visualisation of an Ornstein-Uhlenbeck process with different initial values.
1342,"Simulates 2 price paths of the barrier option. When the price hits the barrier (lower path), the option expires worthless."
1343,Plots the probability density function (pdf) of the chi-squared distribution for different degrees of freedom.
1345,Plots the loss distribution in the simplified Bernoulli model with default probabilities coming from Beta distribution.
1346,Draws a payoff graph of a collar option strategy at the expiration date.
1347,Plots a strip option strategy. Uses a long position in one call and two (or more) puts in anticipation of downward price movement.
1348,"Plots density functions of DAX (upper panel) and Dow Jones (lower panel) index and the normal density (dashed line), estimated nonparametrically with Gaussian kernel."
1349,Plots the autocorrelation function of daily stock prices for Coca-Cola company from 1 January 2002 to 30 November 2004.
1350,"Analyses the consequences resulting from the delta hedge, e.g. the number of shares purchased, the costs of buying shares, the revenues due to selling shares and the cumulative costs."
1351,Plots quantile and expectile curves for standard normal distribution and constructs 95% uniform confidence bands for the expectile function.
1352,Calculates the performance measure of a Stop-Loss strategy for an increasing hedging frequency delta_t.
1353,"Simulates a standard Wiener process X_t=c^(-0.5)*W_ct, for c>0."
1354,"Provides backtesting results for Value-at-Risk under the Peaks Over Treshold model with generalized Pareto distribution with 0.05 level for the portfolio formed by Bayer, BMW, Siemens shares during from 1992-01-01 to 2006-09-01."
1355,"Uses linear interpolation of option prices C1 and C3 and implied volatilities in order to approximate the price of another option C2. All options have the same maturity, but vary in strike with K1<K2<K3."
1356,Plots the loss distribution in the simplified Poisson model with default probabilities comming from Gamma distribution.
1357,Plots loss distributions in linear and semi-log scale in the simplified Bernoulli model (red line) and simplified Poisson model (blue line).
1358,"Reads data for DAX and FTSE 100 from 1 Jan to 31 Dec 2007, plots the autocorrelation function and the partial autocorrelation function for returns, squared returns and absolute returns for the two indexes. Performs Ljung-Box and ARCH test statistics."
1359,"Simulates and plots a standard Wiener process W_t on 1000 equidistant points in interval [0, 1]."
1360,Plots a butterfly option strategy either produced with call or put options. The combination of two long puts/calls with strike prices of K1 and K3 and two shorted puts/calls with strike price K2=0.5*(K1+K3).
1361,"Provides and plots shape, scale and location parameters estimated for calculating Value-at-Risk (VaR) with Block Maxima Model."
1362,"Provides and plots threshold, shape and scale parameters estimated for calculating Value-at-Risk (VaR) with Peaks Over Treshold model."
1363,Plots the theoretical (red) and empirical (blue) mean excess function of the Frechet distribution with alpha = 2.
1364,Models the volatility processes for the DAX and FTSE 100 returns from 1 January 1998 to 31 December 2007 as an ARCH(6) process.
1365,"Calculates the Delta, the Gamma and the Theta of a portfolio."
1366,"Calculates risk neutral probabilities of the up, middle, and down movements such that the price of the call option with strike K is equal to the price of the hedging portfolio minimizing the quadratic hedging error at time t = 0."
1367,Calculates the price of a European product call option of Allianz and Munich RE stock prices.
1368,Construct a network plot from an adjacency matrix for a network of 28 systemically important financial institutions (SIFIs). The plot is built using a force-directed Fruchterman-Reingold graph drawing algorithm. The binary adjacency matrix is constructed via a spacings-based approach applied to a Spearman rank correlation matrix
1369,Evaluates the mean absolute forecast errors of the AWS model for SP500 data in years 2001--2005 and plots the forecasted volatility.
1370,"Produces the exact ruin probability in infinite time for mixture of 2 exponentials claims. Needs ""ruinmix2exps.m"" function."
1373,"Plots the effect of changing the correlation on the shape of the smile. Requires GarmanKohlhagen.m, HestonVanillaSmile.m and HestonVanilla.m function."
1377,Produces the plot of the PCS catastrophe loss data. Available in R and Matlab
1379,"Presents the statistical properties of the minimum spanning tree as a function of time windows sizes for elements of the chosen WIG 20 companies (close.csv). The Theil index based distance is used. Requires theil.m, mst.m, manh.m to run the program. Note that the {statistical toolbox} from Matlab is required as well."
1384,"Plots three sample gamma pdfs, Gamma(alpha, beta), on linear and Semi-logarithmic scales. Note, that the first one (black solid line) is an exponential law, while the last one (dashed blue line) is a chi-square distribution with nu=6 degrees of freedom. It also plots three sample log-normal pdfs, LogN(mu,sigma), on linear and semi-logarithmic plots. For small sigma the log-normal distribution resembles the Gaussian."
1386,Produces the relative error of the approximations (with respect to exact method) of ruin probability in infinite time treated as a function of the initial capital for the mixture of 2 exponentials distibution claims.
1387,Calculates the minimum spanning tree (MST) and presents the results as a set of links for company data set (close.csv). The data contains 20 stocks listet in S&P 500. The links are presented in the order of attachments. The matlab version equires mst.m to run the program.
1390,"Plots the Danish fire data simulation results for a NHPP with log-normal claim sizes and a NHPP with Burr / Pareto claim sizes. The lines are the sample 0.001, 0.01, 0.05, 0.25, 0.50, 0.75, 0.95, 0.99, 0.999-quantile lines based on 3000 trajectories of the risk process (dfl.dat / dfl.txt). Requires simNHPPRP.m, simNHPPRPRT.m, quantiles.m, Paretornd.m, simNHPP.m, Burrrnd.m, mixexprnd.m, simHPP.m from the Quantnet website."
1398,"Presents the analysis of the network evolution. The procedure generate networks in moving time window and calculate the frequency of connections between elements of the chosen WIG 20 companies (gwp.csv). The bidirectional minimum length path is constructed. Requires ultra.m, bmlp.m to run the program."
1402,"Produces the ruin probability in infinite time for mixture of 2 exponentials distribution claims given by De Vylder approximation. Needs the ""moments.m"" function."
1405,Plots the quarterly number of losses for the Danish fire data and the aggregate number of losses and the mean value funtion E(N_t) of the calibrated HPP and NHPP. Clearly the latter model gives a better fit to the data (dfl.dat/ dfl.txt). Requires getQnumber.m from Quantnet website.
1408,"Plots a probability density function (pdf) of a mixture of two exponential distributions with mixing parameter a = 0.5 superimposed on the pdfs of the component distributions as well as semi-logarithmic plots. In the semi-logarithmic plot, the exponential pdfs are straight lines with slopes -beta. Note, the curvature of the pdf of the mixture if two exponentials. Requires mixexppdf.m to run the program (see quantnet)."
1411,"Presents the rate (in percent) of connections between companies in the case of TD, subset of WIG 20, MST; only nonzero values are presented. The Theil index based distance is used. Requires theil.m, mst.m, manh.m to run the program."
1416,Produces the exact ruin probability in infinite time for gamma claims.
1417,"Plots the marginal pdfs in the Black-Scholes (GBM) and Heston models for [kappa, theta, sigma, rho] = [2, 0.04, 0.3, -0.05] as well as log-linear scale of the Heston marginal pdfs. The tails of the Heston marginal pdfs are exponential. Requires pdfHeston.m function."
1419,Plots the coupon-bearing CAT bond price for the Burr claim amounts and a non-homogeneous Poisson process governing the flow of losses.
1426,"Produces the ruin probability in infinite time for mixture of 2 exponentials distribution claims given by Renyi approximation. Needs the ""moments.m"" function."
1429,"Plots densities and log-densities of symmetric hyperbolic, TSD, NIG and Gaussian distributions having the same variance. Requires ghypstat.m (requires besselk.m from Symbolic Math toolbox), hyppdf.m, nigpdf.m, tstabpdf3.m, and tstabpdf.m functions."
1435,"Produces the plot of an aggregate loss process together with a real-world trajectory, two sample quantile lines and mean of the process."
1441,"Plots the EUR/USD market smile on July 1, 2004 and the fit obtained with the Heston model for tau = 1 week, 1 month, 3 months, 6 months, 1 year, 2 years, the term structure of the volatility of volatility and the correlation visualizing the problem of fitting the smile in the short term. Requires HestonVanilla.m, HestonVanillaSmile.m, and GarmanKohlhagen.m function."
1445,"Produces the ruin probability in infinite time for mixture of 2 exponentials distribution claims given by Lundberg approximation. Needs the ""moments.m"" function."
1448,"Plots the estimation output of the tvARCH(1) model for SP500 data including the predicted volatilities and forecasting errors relative to GARCH; the numerical values of mean absolute errors are also provided. For faster, but less precise computation, the bandwidth can be set to a fixed value at the end of the for loop."
1449,"Plots in a first panel shapes of the mean excess function e(x) for the log-normal, gamma (with alpha<1 & alpha>1) and a mixture of two exponential distributions. The second panel illustrates shapes of the mean excess function e(x) for the Pareto, Weibull (with tau<1 and tau>1) and the Burr distribution. STFloss07 requires the function mef.m (available on quantnet)."
1452,"Presents the statistical properties of the minimum spanning tree as a function of time windows sizes for elements of the chosen S&P 500 companies (close.csv). The Theil index based distance is used. Requires theil.m, mst.m, manh.m to run the program."
1457,Calculates the strike of a variance of a given maturity using the potrfolio of options of a given strike range.
1458,"Produces the ruin probability in infinite time for mixture of 2 exponentials distribution claims given by heavy-light traffic approximation. Needs the ""moments.m"" function."
1461,"Presents the analysis of the network evolution. The procedure generate networks in moving time window and calculate the frequency of connections between elements of the chosen S&P 500 companies (close.csv). The unidirectional minimum length path is constructed. Requires ultra.m, umlp.m to run the program."
1465,"Presents statistical poperties of the minimum spanning tree, bidirectional minimum length path and unidirectional length path for company data (close.csv) The S&P 500 subset is analysed."
1471,"Presents the analysis of the network evolution. The procedure generate networks in moving time window and calculate the frequency of connections between elements of the chosen WIG 20 companies (gwp.csv). The unidirectional minimum length path is constructed. Requires umlp.m, ultra.m and {Statistical Toolbox} from Matlab to run the quantlet."
1475,"Produces the ruin probability in infinite time for mixture of 2 exponentials distribution claims given by light traffic approximation. Needs the ""moments.m"" function."
1478,"Computes the mean absolute forecast error of the GARCH volatility forecast based on the number ""ghist"" of past observations and plots the forecast."
1479,"STFstab03.R compares S0 and S parameterizations and plots both. STFstab03.m shows (top panels) semilog and loglog plots of symmetric 1.7-stable, symmetric tempered stable (TSD) with alpha = 1.7 abd lambda = 0.2, and Gaussian pdfs. Bottom panels show semilog and loglog plots of symmetric TSD pdfs with alpha = 1.7 and four truncation coefficients: lambda = 5, 0.5, 0.2, 0.01. Note, that for large lambdas the distribution approaches the Gaussian (though with a different scale) anf for small lambdas the stable law with the same shape parameter alpha. Requires ""stabpdf_fft.m"", ""tstabpdf.m"", and ""tstabpdf3.m"" functions."
1484,"Plots the CAT bond price, for the bond paying only coupons, for the Burr claim amounts and a non-homogeneous Poisson process governing the flow of losses."
1491,"Creates plots of the GBM, Heston spot and Heston volatility processes."
1494,"Plots the empirical distribution function (edf) of a 10-element log-normally distributed sample with parameters mu=0.5 and sigma=0.5 and the approximation of the edf by a continuous, piecewise linear function superimposed on the theoretical distribution function."
1496,Plots the empirical mean excess function e_n(x) for the Danish fire loss data (dfl.dat or dfl.txt). Requires samplemef.m from the Quantnet website.
1499,"Presents the rate (in percent) of connections between companies in the case of TD, subset of S&P 500, MST; only nonzero values are presented. The Theil index based distance is used. Requires theil.m, mst.m, manh.m to run the program."
1504,"Produces the ruin probability in infinite time for mixture of 2 exponentials distribution claims given by Beekman-Bowers approximation. Needs the ""moments.m"" function."
1507,"STFstab01.R creates a) a semilog plot of symmetric stable probability density function for different alphas b) a double logarithmic plot of right tails of symmetric stable disribution functions for different alphas. STFstab01.m calculates the log returns of the DIJA daily closing values from the period January 3, 2000 - December 31, 2009 and plots the Gaussian fit to the empirical culmulative distribution function (cdf) of the returns on a double logarithmic scale. Requires the ""empcdf.m"" function."
1510,Plots the difference between zero-coupon CAT bond prices in the Burr and lognormal claim amount cases for a non-homogeneous Poisson process governing the flow of losses.
1517,Produces the exact ruin probability in infinite time for exponential claims.
1518,Plots the daily log-returns of S&P 500 in the years  2001 to 2005.
1519,Produces the plots of non-homogeneous Poisson processes with linear and sinusoidal intensity.
1520,"Presents the analysis of the network evolution. The procedure generate networks in moving time window and calculate the frequency of connections between elements of the chosen WIG 20 companies (gwp.csv). The minimum spanning tree is constructed. Requires ultra.m, mst.m to run the program."
1524,Produces the ruin probability in infinite time for mixture of 2 exponentials distribution claims given by 4-moment gamma De Vylder approximation.
1525,Evaluates the mean absolute forecast errors of the AWS model for SP500 data in years 2001-2005.
1526,"Plots the effect of changing model parameters on the shape of the smile: the innitial variance v_0, volatility of variance sigma, long-run variance theta, and mean revision level kappa. Requires GarmanKohlhagen.m, HestonVanillaSmile.m and HestonVanilla.m function."
1530,"Produces the ruin probability in infinite time for mixture of 2 exponentials distribution claims given by Cramer-Lundberg approximation. Needs the following functions: moments.m, adjR.m, mgfs.m"
1535,"Presents a comparison of the mean distance between nodes generated by minimum spanning tree, unidirectional minimum length path and bidirectional length path. The subset of S&P 500 and WIG 20 is analysed. The Theil index based distance is used. Requires umlp.m,bmlp.m, theil.m, mst.m, manh.m to run the program."
1542,"Plots three sample Pareto pdfs, Par(alpha, lambda), on linear and double-logarithmic scales. The thick power-law tails of the Pareto distribution (asymptotically linear in the log-log scale) are clearly visible. Requires Paretopdf.m to run the program (see quantnet)."
1545,Plots the data and the GARCH volatility forecast based on all historical data.
1546,Plots the zero-coupon CAT bond price for the Burr claim amounts and the non-homogeneous Poisson process governing the flow of losses.
1553,"STFstab02.R creates a) a plot of stable probability density function for different betas b) a plot of probability density functions of Gaussian, Cauchy, Levy distributions. STFstab02.m shows a semi-logarithmic plot of symmetric (beta=mu=0) stable densities for four values of alpha and a plot of stable densities for alpha = 1.2 and four values of beta. Requires the ""stabpdf_fft.m"" function."
1556,"Compares the mean distance between nodes generated by minimum spanning tree, unidirectional minimum length path and bidirectional length path The subset of WIG 20 (gwp.csv) and S&P 500 (close.csv) is analysed. Requires mst.m, umlp.m, bmlp.m, ultra.m to run the quantlet."
1562,"Produces the ruin probability in infinite time for mixture of 2 exponentials distribution claims given by heavy traffic approximation. Needs the ""moments.m"" function."
1565,"Plots the implied volatility of DAX options on January, 4th 1999, using kernel regression employing a quartic kernel with bandwidth=c(0.05, 0.3) for moneyness and time t maturity respectively."
1566,"Presents statistical poperties of the minimum spanning tree bidirectional minimum length path and unidirectional length path. The WIG 20 subset is analysed (gwp.csv) Requires mst.m, umlp.m, bmlp.m, ultra.m and {Statistical Toolbox} from Matlab to run the quantlet."
1572,"Presents the analysis of the network evolution. The procedure generate networks in moving time window and calculate the frequency of connections between elements of the chosen S&P 500 companies (close.csv). The bidirectional minimum length path is constructed. Requires ultra.m, bmlp.m to run the program."
1576,"Produces the ruin probability in infinite time for mixture of 2 exponentials distribution claims given by exponential approximation. Needs the ""moments.m"" function."
1579,"Plots the EUR/USD market smile on July 22, 2010 and the fit obtained with the Heston model for tau = 1 week, 1 month, 3 months, 6 months  1 year, 2 years, the term structure of the volatility of volatility and the correlation visualizing the problem of fitting the smile in the short term. Requires HestonVanillaFitSmile.m, HestonVanilla.m, and GarmanKohlhagen.m function."
1583,Produces the plot of the aggregate quarterly number of losses for the PCS data together with different mean value functions.
1585,Plots the area of two different groups. It refers to the four plots in the SVM chapter.
1586,"Plots three sample Burr pdfs, Burr(alpha, lambda, tau), on linear and double-logarithmic scales and three sample Weibull pdfs, Weib(beta, tau), on a linear and semi-logarithmic scales. We can see that for tau<1 the tails are much heavier and they look like power-law. Requires Burrpdf.m to run the program (see quantnet)."
1589,"Presents the analysis of the network evolution. The procedure generate networks in moving time window and calculate the frequency of connections between elements of the chosen S&P 500 companies (close.csv). The minimum spanning tree is constructed. Requires ultra.m, mst.m to run the program."
1593,Generates a vector of pseudo random variables with mixture of exponentials distributions. Required by STFloss10
1595,Gives the quantile parameter estimates of a stable distribution. Method is based on J.H.McCulloch (1986) 'Simple Consistent Estimators of Stable Distribution Parameters'
1596,"Evaluates the mean absolute forecast errors of the tvARCH model for SP500 data in year 2001--2005 (the order of tvARCH is determined by variable ""p""). For faster, but less precise computation, the bandwidth can be set to a fixed value at the end of the for loop."
1597,Generates non-homogeneous Poisson process and plot 2 trajectories from non-homogeneous processes.
1599,Generates and plots an homogeneous Poisson process with an intensity lambda.
1601,Samples trajectories of the spot price and volatility processes in the Heston model.
1602,Contains regression parameter estimates of a stable distribution. Required by STFstab05.m and STFstab06.m function.
1603,Generates the aggregate loss process driven by the homogeneous Poisson process.
1605,Generates a vector of pseudo random variables coming from a Pareto distribution.
1606,Computes quantiles of trajectories and plots quantiles and median of a trajectory of a homogeneous poisson process.
1607,Generates and plots the aggregate loss process driven by the non-homogeneous Poisson process.
1609,"SIMGBM Simulates Geometric Brownian Motion (GBM). Parameters for starting value, drift, volatility and step size can be chosen freely."
1610,Plots the difference between zero-coupon CAT bond prices in the Burr claim amount case for two different non-homogeneous Poisson processes governing the flow of losses.
1618,"Plots (log)-returns of WIG20 index with GARCH(1,1)-based daily volatility estimate sigma_t, as well as sigma_t filtered WIG20 returns. Requires edftests.m, nigest.m, hypest.m, stabreg.m, hypcdf.m, nigcdf.m, and stabcdf.m functions."
1626,Calculates the Anderson-Darling test statistic for Pareto distribution. The function is required by STFloss08t.
1627,"Contains the empirical distribution function (edf) goodness-of-fit statistics. Required by STFstab05.m and STFstab06.m function. Computes the Anderson-Darling A2 and Kolmogorov (Kolmogorov-Smirnov) D goodness-of-fit statistics for a given data vector X and a vector of estimated parameters of one of the following distributions: Gaussian, hyperbolic, NIG, stable."
1628,Computes price of the zero-coupon CAT bond for the given claim amount distribution and the homogeneous Poisson process governing the flow of losses.
1630,Obtains the European FX option price in the Heston model using the Lewis-Lipton formula. Required by STFhes03.m function.
1631,Contains the stable probability density function (pdf). Required by STFstab03.m and STFstab04.m function.
1632,Computes the price of the coupon-bearing CAT bond for the given claim amount distribution and non-homogeneous Poisson process governing the flow of losses.
1634,Estimates the parameters of Burr distribution. Parameter estimation can be done by maximum likelihood (ML) or optimization based on Anderson-Darling statistic (A2). Required by STFloss08t.
1635,"Returns the cumulative density distribution of the Burr distribution. The default parameters are: ALPHA  = 1, LAMBDA = 1, TAU = 2.  Required by STFloss08t."
1636,Contains the European FX option pricing formula. Required by STFhes04.m function.
1637,"Simulated trajectories of the risk process. [t,y]=SIMNHPPRP(u,theta,lambda,parlambda,distrib,params,T,N) returns N simulated trajectories of the risk process with claim sizes from the distribution specified in DISTRIB with parameters in PARAMS. The claim arrival process corresponds to the non-homogeneous Poisson process with inensity specified by LAMBDA (0 - sine function, 1 - linear function, 3 - sine square function) with parameters in PARLAMBDA. THETA is the relative safety loading, U the initial capital and T the time horizon. A function reuqired by STFloss10."
1638,Estimates parameters from a Normal Inverse Gaussian distribution (NIG) by maximum likelihood method. This function is required by STFstab05.m and STFstab06.m.
1639,Simulates values of the EDF statistics for the lognormal distribution for given values of MU and SIGMA. Required by STFloss08t.
1640,Returns expected value and variance of generalized hyperbolic distribution. Required by STFstab04.m function.
1641,Returns a mixed exponential probability density function (pdf). A function required by STFloss03.m
1642,Contains the Normal Inverse Gaussian (NIG) cumulative distribution function (cdf). Required by STFstab05.m and STFstab06.m function.
1643,"Simulates values of the EDF statistics for the Pareto distribution with given parameters ALPHA, LAMBDA. Required by STFloss08t."
1644,Returns the value of the sample mean excess function for the vector DATA in the points from vector XAXIS. A function required by STFloss08.
1645,"Calculates distribution of a stable distribution. Auxiliary function, required by STFstab03.m and STFstab04.m function."
1646,"Estimates the parameters of a Pareto distribution. Applicable methods are: maximum likelihood (ML), methods of moments (MoM) or optimized based on Anderson-Darling statistics (A2) Function required by STFloss08t."
1647,"Estimates hyperbolic distribution. Auxiliary Function, Required by STFstab05.m and STFstab06.m function."
1648,"Plots the empirical cumulative distribution function (cdf) for a vector x, in support from (-inf, max(x)). Infimum of support can be specified. Required by STFstab01.m, STFstab05.m and STFstab06.m functions."
1649,Calculates the Anderson-Darling test statistic for the Burr distribution. Required by STFloss08t.
1650,Converts the given time series into time series of Theil index. A function required by STFdmm11.R.
1652,(Alpha-)stable cumulative distribution function (cdf). Required by STFstab05.m and STFstab06.m function.
1653,"Generates N trajectories of the non-homogeneous Poisson process with intensity specified by LAMBDA (0 - sine function, 1 - linear function, 2 - sine square function) with paramters in PARLAMBDA. T is the time horizon. The function uses thining method. A function required by STFloss01."
1654,Returns probability density function for pareto distribution. A function required by STFloss05.
1655,Generates N trajectories of the homogeneous Poisson process with intensity LAMBDA. T is the time horizon. A function required by STFloss01.
1656,Calculates the Anderson-Darling test statistic for a lognormal distribution. Required by STFloss08t.
1657,Contains the hyperbolic cumulative distribution function (cdf). Required by STFstab05.m and STFstab06.m function.
1658,"Computes quantile lines of the risk process specified by [T, DATA] in equally spaced points with the step size specified by STEP. PERC is the vector with orders of quantiles. A function reuqired by STFloss10."
1659,"Calculates the ultrametric distance between time series. Auxiliary  matlab function, required by STFdmm02 - STFdmm10."
1662,Generates a minimum spanning tree. The result is presented as a set of links between nodes.
1663,Contains the European FX option price in the Heston model. Required by STFhes03.m function.
1664,"Calculates the variance vega of equally weighted options"" portfolio, weighted by 1/K and by 1/K^2."
1665,Simulates values of the EDF statistics for the Burr distribution. Required by STFloss08t
1666,"Calculates the strike of a variance of a given maturity using the potrfolio of options of a given strike range. The code needs ""blsprice"" function from MATLAB toolbox."
1667,Returns the moment generating function or its k-th derivative (up to third) for mixture of two exponential distributions claimsSTFruin05.m.
1668,"Shows the linear approximation of the log payoff with the portfolio of options of a given number of strikes and given range. The code needs ""blsprice"" function from MATLAB toolbox."
1669,Returns the k-th moment (up to fourth) of the mixture of 2 exponentials distribution claims. Needed by STFruin05.m.
1670,"Calculates random numbers from Burr-Distribution. The default values for the parameters ALPHA, LAMBDA, TAU, M, N are 1, 1, 2, 1, 1 respectively. Uses the inversion method. Required by STFloss10"
1676,Returns quarterly number of losses. Auxiliary function required by STFloss09
1677,Gives the maximum likelihood function for the Normal Inverse Gaussian (NIG) distribution to plugin into the command fminsearch.
1678,Computes price of the zero-coupon CAT bond for the given claim amount distribution and the non-homogeneous Poisson process governing the flow of losses.
1680,"Estimates the parameters of a log-normal distribution. Possible methods for estimation are: maximum likelihood, method of moments Required by STFloss08t."
1681,Calculates the probability density function (PDF) of an Normal Inverse Gaussian (NIG) distribution. Required by STFstab04.m function.
1682,"Returns values of the mean excess function for the following distributions: Lognormal, Gamma, Weibull, Pareto, Burr, mixture of exponential distributions. A function required by STFloss07."
1683,"Returns the cumulative density distribution of the Burr distribution. The default parameters are: ALPHA  = 1, LAMBDA = 1, TAU = 2.  Required by STFloss06."
1684,Returns the adjustment coefficient R for a mixture of two exponential parameters. Needed by STFruin05.m.
1685,Contains the hyperbolic probability distribution function (pdf). Function is required by STFstab04.m.
1686,Calculates unidirectional minimum length path algorithm.  The result is presented as a set of links between nodes.
1687,Produces the exact ruin probability in infinite time for insurance collective risk model with mixture of 2 exponentials distribution claims. This function is used by STFruin04.m.
1688,Returns cumulative distribution function (cdf) of a pareto distribution. A function required by STFloss08t.
1689,Computes price of the CAT bond paying only coupons for the given claim amount distribution and the non-homogeneous Poisson process governing the flow of losses.
1691,Contains bidirectional minimum length path (BMLP) algorithm. Result is a set of links between nodes.
1694,Fits the Heston model to the FX market implied volatility smile. Required by STFhes07.m function.
1695,Gives the Manhattan distance between time series normalised by the time series length. A function required by STFdmm11.R. and STFdmm11.m
1697,"Returns the real-life trajectory of the risk process with claim sizes specified by VAL and the moments of losses by TIME. The premium corresponds to the non-homogeneous Poisson process with inensity specified by LAMBDA (0 - sine function, 1 - linear function, 3 - sine square function) with parameters in PARLAMBDA and mean claim size from a distribution specified by DISTRIB with parameters in PARAMS. THETA is the relative safety loading, U the initial capital and T the time horizon. A function required by STFloss10"
1698,Contains the stable probability density function (pdf) via FFT. Required by STFstab02.m
1699,Gives the maximum likelihood function for the hyperbolic distribution to plug in the command fminsearch.
1700,Contains the probability density function in the Heston stochastic volatility model. Required by STFhes02.m function.
1701,Plots the difference between zero-coupon CAT bond prices in the Burr claim amount case for a homogeneous and a non-homogeneous Poisson process governing the flow of losses.
1710,Contains the volatility smile implied by the Heston model. Required by STFhes04.m function.
1711,"Plots (log)-returns of DJIA index with GARCH(1,1)-based daily volatility estimate sigma_t as well as sigma_t filtered DJIA returns. It also provides left tail plots of the cdf of (standardized) returns, filtered returns with four fitted distributions. The corresponding right tails are as well. Requires edftests.m, empcdf.m, nigest.m, hypest.m, stabreg.m, hypcdf.m, nigcdf.m, stabcdf.m."
1720,"Provides parameter estimates obtained via the A-squared minimization scheme and test statistics for the fire loss amounts. The corresponding p-values based on 1000 simulated samples. Requires estBurr.m, estln.m, estPareto.m, cvln.m, cvPareto.m, cvBurr.m, ADcritBurr.m, ADcritPareto.m, ADcritln.m, Burrcdf.m, Paretocdf.m, Burrrnd.m, Paretornd.m from the Quantnet website."
1729,"Compares the option prices obtained using the analytical Heston formula (1993) and the FFT method (Carr-Madan formula via FFT + Simpson""s, Carr-Madan using adaptive Gauss-Kronrod quadrature, and Lipton""s formula using adaptive Gauss-Kronrod quadrature). Requires HestonFFTVanilla.m, HestonVanillaLipton.m and HestonVanilla.m function."
1733,Plots sample trajectories of a NHPP with linear intensity lambda(t)=a+b*t (figure 1) and sample trajectories of a NHPP with periodic intensity lambda(t)=a+b*cos(2*pi*t). The black processes are HPP with b=0.
1737,Simulation of price paths with Poisson Generating Function and the Merton Model.
1738,"This Quantlet simulates ARMA(2,2) - autoregressive moving average process and draws the true ACF and PACF"
1739,"This Quantlet plots monthly time series of returns of Procter and Gamble from 1961 to 2016 and  their ACF and PACF (Example, 2.4 Figures 2.8-2.9 in the book)"
1740,This Quantlet performs additive and multiplicative decomposition of Chinese quarterly GDP from 1992 to 2017 and  plots results (Figures 2.21-2.23 in the book)
1745,This Quantlet builds plots time series and ACF of Chinese quarterly GDP from 1992 to 2017
1746,"This Quantlet simulates and plots ARMA(2,2) - autoregressive moving average process time series, its ACF and PACF"
1748,"This Quantlet builds log-returns and double difference of log series for quarterly exchange rate of GBP to NZD and plots time series and their ACF   for the time span from March 1991 to September 2000 (example 2.3, Figures 2.5-2.7 from the Book)"
1754,"This Quantlet produces three simulated sample-paths of the standard Brownian motion using the function simulSBM(n, seed, fig)"
1755,This Quantlet generates Gaussian white noise
1758,"This Quantlet  plots time series and additive decomposition results of monthly employed total persons from February 1978 to November 2018 (in thousands) (example 2.5, Figures 2.13-2.16 from the Book)."
1762,"This Quantlet plots  the monthly mean of North Atlantic Oscillation (NAO) index since January 1950 till December 2019, ots ACF and PACF, further fits ARMA model and performs Ljung-Box statistics"
1763,"This Quantlet draws 1000 random numbers  from the normal  distribution, produces histogram and time series plot"
1764,This Quantlet generates three simulated paths of a standard Gaussian random walk
1768,This Quantlet demonstrates the fitting of the ARMA model to simulated time series
1770,This Quantlet simulates and plots AR(2) time series and its ACF and PACF.
1773,"This Quantlet build and plots time series, log  and their rolling std of the of the anti-diabetic drug sales in Australia, as well as seasonally differenced series for the period from July 1991 to June 2008."
1774,"This Quantlet produces time series plot, the bar charts and lag plots to vizualize a data of the numbers of boys and girls born in USA per year from 1940 to 2002."
1777,"This Quantlet  plots time series and their ACF&PACF of quarterly exchange rate of GBP to NZD  for the time span from March 1991 to September 2000 (example 2.2, Figures 2.1-2.3 from the Book)."
1781,"This Quantlet plots yearly mean total sunspot number for the period 1700 - 2017, obtained from Royal Observatory of Belgium, Brussels"
1782,"This Quantlet plots time series of monthly temperature volatility for the southern hemisphere, its first order difference and its ACF for the period from January 1850 to December 2007"
1783,This Quantlet builds time series of monthly recorded milk average per cow at a dairy cattle ranch in China from Jan.1962 to Dec.1975. It produces yearly and monthly box plots of this dataset
1785,"This Quantlet simulates and plots MA(2) -  moving average process time series, its ACF and PACF"
1786,This Quantlet produces Chaos like a White Noise and saves in csv file
1793,This Quantlet simulates and plots causal and noncausal AR(5) - autoregressive processes as describe in Example 3.9 in the book
1795,This Quantlet  produces the examples for various ways of composition of simple time series and plots results(Figures 2.10-2.11 from the Book).
1796,This Quantlet produces and plot time series of seasonally and firstly differenced Chinese quarterly GDP and its ACF for the period from 1992 to 2017.
1797,"This Quantlet produces plots of closing daily prices, log-returns and their ACF for Time series of BTC daily prices from June 23, 2017 to June 22, 2018."
1798,Plots the time series of a trading day on Bitstamp & BTC/USD exchange rate
1799,"Scalable implementation of Lee / Mykland (2012) and Ait-Sahalia / Jacod (2012) Jump tests for noisy high frequency data; best used in combination with IRTG dataset (for access, ask IRTG 1792)"
1801,Computes the trinomaial tree of option prices for the american put using the information given on table 26
1802,Simulation study for the power and level of the Augmented Dickey-Fuller Test for different AR- and MA-parameters as well as different number of lags.
1803,"Downloads BTC/USDT transaction data from Binance API aggregated per minute from 2017-01-01 until 2020-01-30. Selects close prices and divides them into 1000 subsets. For each subset, Hurst Exponent is calculated and an AR Model is fitted. The close price for the following 7 periods is made and MSE is calculated. One-sided t-test indicates that large Hurst Exponents are related to a smaller MSE, hence indicates prediction quality."
1806,"Computes the density function of the loss in a portfolio of bonds or other credit instruments using a one-factor t copula. The marginal densities of default times are implemented as poisson distributions using constant default intensities. After computing the portfolio loss densities for various values of the dependence parameter rho, these densities are plotted."
1807,"Plots two realizations of an MA(1) (moving average) process with MA coefficient beta,  simulate from an arima model with innovations n1 and n2."
1810,Plot interactive plots of higher order greeks by Python.
1823,Hedge portfolios by delta hedging and charm adjusted delta hedging methods.
1826,"Returns the Portfolio Allocation in a one-period Binomial-Model, for different Pricing Kernels and utility-functions"
1828,"Computes Barrier option prices using Monte Carlo method for assets with/without continuous dividends. barrier option types: up-and-out, up-and-in, down-and-out, down-and-in"
1829,"Computes Barrier option prices using a binomial tree for assets with/without continuous dividends. barrier option types: up-and-out, up-and-in, down-and-out, down-and-in"
1830,"Builds and plots Mean-Variance and Mean-CVaR efficient frontiers, cunstructed on daily basis"
1831,Implements bootstrap with block-length selection proposed by Politis and White (2004). of returns to construct combinations of asset allocation moodels
1833,"Implements portfolio optimization rules, applied to cryptocurrencies and 16 traditional assets: Mean-Variance, ERC, Maximum Diversification, Mean-CVaR"
1835,Calculates traditional descriptive statistics for 55 cryptocurrencies and traditional assets and correlation matrix
1837,Plots portfolio compositions for 8 monthly rebalanced portfolios with 55 cryptocurrencies and 16 traditional financial assests
1838,"Calculates and plots dynamic risk contribution, measured by volatility for 8 portfolio rules considered"
1839,Plots pdf of top-10 CC returns distributions
1841,"Calculates: Sharpe ratio, Adjusted Sharpe ratio, Turnover, CEQ for 8 portfolios with cryptocurrencies"
1842,"Implements mean-variance spanning tests from Kan, Zhou (2012), tests from (Wolf, Ledoit 2008) to check significance of difference for CEQ and Sharpe ratios for 9 portfolio strategies with CC"
1846,"Calculates diversification measures: diversification ratio, PDI and Effective N for 8 portfolios, constructed from cryptocurrencies and 16 traditional assets"
1852,"Plots the first 4 Hermite polynomials on the given grid of values, probabilistic version."
1853,"Plots the highest singular values as provided by the LSA process for the given term document matrix TDM of the Quantlets. Additionally, the histogram of the matrix values in the TDM is plotted via ggplot2 package. The high sparsity of the TDM becomes evident. The mean value is indicated via a dashed vertical line."
1854,"Big Data visualization of LSA via heat maps. Heat maps with color key of the LSA decomposition matrices are produced via gplots package. m_a is the original term document matrix TDM (BVSM) containing Quantlets, m_lsa the corresponding representation in the LSA model reduced to the ﬁrst k dimensions and error_matrix is the approximation error matrix of the SVD truncation."
1855,"Big Data visualization of LSA via heat maps. Heat maps with color key of the LSA decomposition matrices are produced via gplots package. The truncated SVD factor matrices U, V and d_s (diagonal Sigma matrix) are extracted from the LSA space. The LSA process is applied on the term document matrix TDM of the Quantlets."
1856,"The SVD factor matrices U, V and the resulting LSA matrices m_lsa and error_matrix from the LSA process are extracted and calculated. The LSA process is applied on the term document matrix TDM of the Quantlets. Their histograms and boxplots are displayed using ggplot : Implementation of the Grammar of Graphics in R."
1857,"The function ""LSA.PC.rotation"" determines the proper sign / rotation of the LSA components (semantic space principal components) and extracts the top words (top_terms) of each component (PC) for the given LSA space which is created from the term document matrix TDM of the Quantlets by means of the SVD process. The (positive and accordingly negative) part is chosen from those terms, where the biggest subtotal is concentrated since the singular vectors are unique up to scalar multiples of modulus one, in the case of real matrices +/- 1. Finally, the top words with the highest weights are taken as ""prototypes"" for the semantic space principal components’ topics (PC topics). That way the function ""LSA.PC.rotation"" allows the determination of possible labels for the LSA PC’s. The function requires as input the LSA space which is provided via the function ""lsa"" (from the R package lsa). The remaining parameters of ""LSA.PC.rotation"" are optional allowing finer control of the output (number of PC’s, terms/topic etc.)"
1858,"Big Data visualization of LSA via heat maps. The LSA process is applied on the term document matrix TDM of the Quantlets. Heat maps with color key of the semantic kernel and its subsets are produced via gplots package. The semantic kernel U_kernel is extracted from the LSA space. For demonstration purpose some sub matrices are taken from the full semantic kernel allowing the exact determination of ""semantically correlated"" term pairs. Additionally, histograms of the semantic kernel values are plotted via ggplot2 package from different aspects."
1859,Plots a histogram of correlations to make Simpson's paradox visible.
1860,"Divides the 91*109*91 voxels data into 30*35*30 (or any other dimensions) cubes for each subject, related to the paper Spatial Functional Principal Component Analysis with Applications to Brain Image Data by Yingxing Li, Chen Huang and Wolfgang Härdle."
1861,"Conducts the whole procedure, including preprocessing data, estimation (getting loadings and factors, smoothing factors, updating loadings) and risk attitude analysis, related to the paper Spatial Functional Principal Component Analysis with Applications to Brain Image Data by Yingxing Li, Chen Huang and Wolfgang Härdle."
1862,Financial Risk Meter Based on Expectiles
1863,Build dynamic network model for risk premia
1864,Estimate the arbitrage free Nelson Siegel model
1869,Sentiment Analysis of the WTO-IMF COVID-19 Vaccine Trade Tracker
1870,Create a filter with Fourier Transformation for a picture of a farm in Brazil.
1871,Demonstrating a basic image filter using Fourier Transformation
1872,Analyzing Speeches by Putin between 2012 and 2022 on the topic of Ukraine to investigate if we can see a shift in Sentiment
1874,Creating a (retro) filter using Fourier Transformation for a picture of Laufenburg (Baden) old town
1875,Creating a filter using Fourier Transformation for a picture of a snake or data center
1876,Homework No. 1 and No. 2. Fourier Transformation of two images using Nympy in Python
1878,"Combining data on wine quality for white and red wine. Use PCA, Spectral Clustering and LLE to find out whether clusters can be determined from this joint data set."
1879,Creating a filter using Fourier Transformation for a picture of the station Konstablerwache in Frankfurt
1880,"Sentiment Analysis of the article ""Russia-Ukraine war at a glance: what we know on day 254 of the invasion"" from Guardian"
1881,This Quantlet uses Reddit Titles on the subreddit r/Bitcoin from the start of 2020 until 19th November of 2022 as input for VADER and RoBERTa sentiment score calculation. Subsequently using OLS and Random Forest models aims at determining whether the sentiment scores help explain Bitcoin log returns.
1887,Demonstrating a basic image filter.
1888,Sentiment Analysis of the WTOs current news page as well as the response of WTO to Covid 19 Pandemic from 22/06/22
1890,Creating a filter using Fourier Transformation for a picture of St Gallen
1891,"We provide results for ""Does non-linear factorization of financial returns help build better portfolio?"", Spilak, WK Härdle (2022). Please refer to github Wiki for a detailed description on how to use the code!"
2122,To extract and via word embedding methods to visualise financial reports.
2127,Simulation of Poisson Process and Compound Poisson Process with Poisson Generating Function.
2128,Joint Limit order book ask and bid price predition for multiple predicton horizons with a neural networks. This script analysis and visualizes the number of events in an order book of order book size L.
2129,"Joint Limit order book ask and bid price predition for multiple predicton horizons with a neural networks. This script computes the MSE loss for the trained model for the fixed time lag application for training, validation and test dataset."
2130,"Joint Limit order book ask and bid price predition for multiple predicton horizons with a neural networks. This script creates plots meant for analysis of the fixed order book level application for training, validation and test dataset."
2131,"Joint Limit order book ask and bid price predition for multiple predicton horizons with a neural networks. This script computes the MSE loss for the trained model for the fixed order book level application for training, validation and test dataset."
2132,"Joint Limit order book ask and bid price predition for multiple predicton horizons with a neural networks. This script creates plots meant for analysis of the fixed time lag application for training, validation and test dataset."
2133,Joint Limit order book ask and bid price predition for multiple predicton horizons with a neural networks. This script analysis and visualizes properties of the limit order book ask and bid prices.
2134,Generates and plots the path of two stocks and marks the theoretical buying and selling time points in continuous time of a stop-loss hedging strategy
2135,Calculates the RMA and EMA of a portfolio of assets
2137,"Plots option price as a function of current stock price for Asian call and European call. Asian call price is calculated using Monte-Carlo Simulation, the European call price is calculated using the Black-Scholes model."
2140,Computes and plots the optimal hedge ratio for a given dataset. The hedge ratio is optimal under the assumption of gaussian distributed returns. The optimal hedge ratio minimized the variance of the hedge portfolio. The example uses daily data from DAX and FDAX (11/2014 - 11/2016).
2142,Visualizes the payoff of an Asian average strike and average price call option for a generated stock path with average price between price at maturity and strike price.
2143,Calculates performance of stop-loss and delta hedging for different observation frequancies
2145,Generates and plots the path of two stocks and marks the corresponding buying and selling time points of a stop-loss hedging strategy
2146,Calculates the prices and 95% Confidence Intervalls of Asian options using Monte Carlo Simulation.
2148,Plots a log-log-graph of the performance of stop-loss and delta hedging for decreasing observation frequancy
2150,Calculates the price of European options using Monte Carlo Simulation and Black-Scholes-Model.
2151,"Computes and plots the optimal hedge ratio for a given dataset. The optimal hedge ratio minimized the variance of the hedge portfolio. A Dynamic conditional correlation (DCC) GARCH model is estimated. Using a rolling window approach, forecasts for the hedge ratio (as the ratio of conditional covariance and conditional variance of the future) are made daily. The example uses daily data from DAX and FDAX (11/2014 - 11/2016)."
2152,Calculates performance of delta-gamma hedging for N simulated stock path trajectories
2153,Calculates the price and 95% confidence intervall of European options using Monte Carlo Simulation for several numbers of MC samples and visualizes the convergence of the MC estimate to the true value.
2156,Calculates performance of delta-gamma hedging for different stock values
2157,Generates and plots a graphical describtion of the logic behind delta hedging and calulates corresponding heding costs for given parameters
2158,Merges the Penn World Table with the World Bank Development Indicators.
2159,Simulates data and evaluates the performance of the estimators for the regression parameters and for the interactive fixed effect components.
2160,"Estimates regression parameters, factors and factor loadings by our projection-based IFE estimator."
2161,Plots the TPE per researcher for each university Unit (also inflation adjusted)
2162,"Plots a time series plot, where each line represents the mean citations or publications of researchers per university Unit and year."
2163,Plots a sankey-plot of publications published in research fields
2164,Plots a chord diagram of the number of co-authorships by researchers aggregated w.r.t. university Units
2165,L1-norm quantile regression is applied
2166,"In this part of our simulation we adopt our methodology in quantile regression with (high dimensional) single index model case. Firstly we generate the initial value of the parameter beta by using Li, Y. and Zhu, J. (2008)`s L1 norm quantile regression function. Then we run a two-step iteration. For the first step of the iteration we use local polynomial regression to estimate the link function and its first derivative. For the second step of the iteration we apply Li and Zhu`s L1 norm quantile regression function again to estimate the parameter beta. This two two-step procedure iterates many times until the rate of convergence achieves. Then we input this final estimated beta into the local polynomial function again to calculate the final link function. The performance of the final estimated beta can be evaluated by three criteria: dev, acc and angle. The performance of the final estimated link function can be evaluated by two criteria: err and ase. For the bandwidth selection we adopt Yu, K. and Jones, M. C. (1998)`s methodology"
2168,"We focus on the inverse BTC options traded in Deribit and provide codes to calibrate the BS, SV, SVJ, and SVCJ models and to calculate their deltas."
2179,This is the application of  DYTEC algorithms to the temperature data
2180,This is the application of  DYTEC algorithms to the hurricanes data
2181,This is the simulation of  DYTEC algorithms
2182,Estimates a Markov switching autoregressive model and plots the estimated states for CO2 spot price data from 2008 to 2011
2183,Estimates a Markov switching autoregressive model and plots the estimated states for CO2 futures price data from 2008 to 2011
2186,Forecasts the realized volatility of a portfolio from cryptocurrencies through of RNNs.
2187,"Imports the raw data from six cryptocurrencies, builds a price weighted portfolio from all of them and calculated log returns and realized volatility. Additionally, it gives more information about the data with some graphs and descriptive statistics."
2188,Forecasts the realized volatility of a portfolio from cryptocurrencies by the means of the Heterogeneous autoregressive (HAR) model and a hybrid between HAR and a simple feedforward neural network (FNN-HAR).
2189,Checks the predicting accuracy of various models through Diebold Mariano tests.
2190,Backests if the realized volatility is a good approach for Value at Risk. Backtesting algorithms like unconditional convergence test (Kupiec test) and conditional convergence test (Christoffersen test) are applied.
2191,Determines and plots the decision boundaries of a linear SVM classifier for different regularization parameters C.
2192,Determines and plots the decision boundaries of a SVM classifier with RBF kernel for spiral data
2193,Determines and plots the decision boundary of a SVM classifier with polynomial kernel of order 2 using the Swiss banknote dataset.
2194,Determines and plots the decision boundary of a SVM classifier with linear kernel using the first 2 principal components of the Swiss banknote dataset.
2195,Determines and plots the decision boundaries of a SVM classifier for different kernels and regularization parameters C.
2196,SVM classification model for defaults of P2P loans. Compares the out-of-sample predictive performance to a baseline logistic regression model.
2197,"XFGiv00 calculates the Black and Scholes price for a European call or put option for a given spot price S, strike price K, risk-free interest rate r, volatility sigma, time to maturity tau and a task (0 = put option, 1 = call option)."
2200,Computes the Nadaraya-Watson estimator for univariate regression. Required by XFGIBT02.m and XFGIBT03.m.
2201,Plots and computes 99% Value-at-Risk for a given time series. Graphical representation of reliability of VaR calculation.
2202,"Calculates kernel density estimate for given data set. Kernel can have one of the folllowing forms: Quadratic, Epanechnikow, Triangular or Uniform"
2203,"Outputs SPD estimations using Implied Binomial Tree and Rookley""s method for DAX option data on Jan 3, 1997 with time to maturity being 77 days."
2204,Shows the MSCI industry breakdown of the portfolio consisting of 279 assets with total notional EUR 13.7bn. The industry breakdown is presented in a bar chart.
2206,"Illustrates the computation of the Value at Risk using the Delta-Gamma method. ""PS"": Plain vanilla sampling method ""MS"": Moment matching sampling method ""SS"": Stratified Latin Hypercube sampling method ""IS"": Importance sampling method"
2207,"Presents risk aversion calculated with respect to different methods. The dotted blue, dashed-dotted green and solid red lines represent SCA-decreasing steps, SCA - equal steps and SCA - incresing steps correspondingly."
2208,"XFGiv03 shows the term structure of implied volatility indices. These indices, representing different option maturities, measure volatility implied in ATM European call and put options. The VDAX calculations are based on Black and Scholes formula. Term structures for ATM DAX options can be derived from VDAX subindices for any given trading day since 18 March 1996. Shapes of the term structure on subsequent trading days are shown in a plot. If we compare the volatility structure of 27 October 1997 (blue line) with that of 28 Octorber 1997 (green line), we easily notice an overnight upward shift in the levels implied volatilities. In addition, it displays an inversion as short term volatilities are higher than long term ones. Only a couple of weeks later, on 17 November (skyblue line) and 20 November (red line), the term structure had normalized at lower levels and showed its typical shape again. Apparently, during the market tumble in fall 1997, the ATM term structure shifted and changed its shape considerably over time."
2209,"Generates a P-P Plot for Backtesting of 1) basic simulation, mean adjustment, volatility updating and volatility updating and mean adjustment simulations as well as of 2) benchmark curve, spread curve, conservative approach and simultaneous simulation. All simulated data follow chapter 2.4 in XFG2."
2210,"Outputs SPD estimates for DAX option data on Jan 3, 1997 on Jan 31, 1997 and the bootstrap confidence intervals for the first date. Both SPDs correspond to the same maturity (49 days) and are displayed on a moneyness metric. Computes the State-Price Density for European options using the result of Breeden and Litzenberger and compares with estimates of Rookley's method. For more insight, please follow the description of the subroutines."
2211,"Plots the real (blue line) and the imaginary part (red line) of the characteristic function for a distribution, which is close to a chi^2 distribution with one degree of freedom."
2212,Displays spread curves for the time period: 1 January 1999 to 19 May 2004. Note: The time period differs from XFG book.
2217,"Performes a Least Squares Kernel Smoothing technique for implied volatility smile estimation. To achieve good forecast of asset price volatility the Gaussian shaped vega function is used as weight. A quartic kernel is employed. Given global convexity, the Golden section search is implemented with estimation tolerance 10e-5. The data consists of January and February 2001 tick statistics of the DAX futures contracts and DAX index options. Since the data are transaction data containing potential misprints, a filter in deleting all observations whose implied volatility is bigger than 0.7 and less than 0.1 is applied. Plots of observed option price data are presented. From the lower left to upper right put prices are displayed, from upper left to lower right (normalized) call prices. Moreover, graphs of least squares kernel smoothed implied volatility smile for 17 days to expiry (January 02, 2001) with bandwidth = 0.025 and 95 percent confidence bands and 14 days to expiry (February 02, 2001) with bandwidth = 0.015 and 95 percent confidence bands are shown. Regression smoothing is made employing a Nadaraya Watson kernel regression estimate for quartic kernel"
2218,"Computes and plots the local polynomial state-price density for January 10, 1997 of Dax data (1997-2001) for tau=0.125,0.25,0.375 employing Breeden and Litzenberger (BL) method and a semi- parametric specification of the Black-Scholes option pricing function. The analytic formula of the BL method uses an estimate of the volatility smile and its first and second derivative to calculate the SPD, delta and gamma. Figures of state-price density, delta and gamma are presented for tau=0.125,0.25,0.375 with blue, black and red lines, respectively. The date can be modified to any day in the set of date=c(3,6,7,8,9,10,13,14,15,16,17, 20,21,22,23,24,27,28,29,30,31)."
2219,"Shows the R^2 of a one-dimensional regression of the 279 MSCI asset returns with respect to its composite factor, modeled as the sum of the industry and country factor. The underlying factor model is based on 24 MSCI industries and 7 MSCI regions. The weighted average R^2 is 0.5327. The R^2 is required to obtain the portfolio correlation structure."
2220,"Computes and plots the local polynomial state-price density for January 10, 1997 for a tau sequence from 0 to 1, employing Breeden and Litzenberger (BL) method and a semi-parametric specification of the Black-Scholes option pricing function. The analytic formula of the BL method uses an estimate of the volatility smile and its first and second derivative to calculate the SPD. The SPD is shown for different values of moneyness and time to maturity."
2221,"Calculates different summary statistics, such as minimum, maximum, mean, median and standard errors for the INAAA data"
2222,"Plots US Treasury Yields (3M, 1Y, 2Y, 5Y, 10Y) from 0 to 2000 days"
2223,XFGiv04 explains variance components of PCA for the ATM implied volatilities of the ATM implied volatility data set (implvola.dat). The two dominant PCs together explain around 83 percent of the total variance in implied ATM volatilities for DAX options.
2224,Displays averaged yields for different times to maturities and different risk levels.
2226,"Outputs SPD estimates for DAX option data from Jan 3, 1997 to Jan 31, 1997 and the bootstrap confidence intervals for the first date. Both SPDs correspond to the same maturity (49 days). Computes the State-Price Density for European options using the result of Breeden and Litzenberger and compares with estimates of Rookley""s method. For more insight, please follow the description of the subroutines."
2227,"Estimates a common principle components model for the implied volatility data (XFGvolsurf01.dat, XFGvolsurf02.dat, XFGvolsurf03.dat) and computes a likelihood ratio test."
2228,"Provides an overview of the data used in the estimation of the SV, SVt and SVJ model. The data consists of daily continuously compounded returns of the DAX index, the Dow Jones index and the GBP/USD FX rate. All time series cover the period from 1 January, 1991 to 21 March, 2007, yielding 4,231 observations."
2229,Counts 99% Value-at-Risk exceedences over a profit and loss function for basic simulation data.
2231,Is a generic function that approximates the density of a distribution function by numerically inverting its characteristic function.
2235,Approximates the quantile from a cumulative distribution function (CDF) that is given on a grid.
2237,Computes the a-quantile for the class of quadratic forms of Gaussian vectors uses Fourier inversion to approximate the cumulative distribution function (CDF).
2239,"XFGqDGtest plots the 1%-quantile from the two one-parametric sub-families of the class of quadratic forms of Gaussian vectors, as defined by the functions XFGdg11par and XFGdg12par"
2240,Shows Expected Loss/Risk Ratio for the Expected Shortfall Contribution at different quantiles.
2241,Computes Cornish-Fisher approximations for increasing orders for a specific example
2242,"Displays first and second eigenvectors of the VDAX index for the ATM implied volatility data (implvola.dat). Taking only the first two factors, the time series of implied ATM volatilities can be represented by a factor model of reduced dimension: x(t,j) = gamma(j,1)*y(t,1) + gamma(j,2)*y(t,2) + epsilon(t). gamma(j) are sensitivities of the implied volatility time series to shocks of the principal components. A shock of the first factor tends to affect all maturities in a similar way, causing a non-parallel shift of the term structure. A shock of the second factor, on the other hand, has a strong negative impact on the front maturity but a positive impact on the longer ones, thus causing a change of curvature in the term structure of implied volatilities."
2244,Shows the MSCI regions breakdown of the portfolio consisting of 279 assets with total notional EUR 13.7bn. The regions breakdown is presented in a bar chart.
2245,"Plots empirical vs. normal distribution. We visualize symmetry and leptokursis of the distribution of absolute spread changes for the INAAA 10Y data, where we plot the empirical distribution of absolute spreads around the mean spread in an averaged shifted histogram and the normal distribution with the variance estimated from historical data."
2246,"Plots the cumulant generating function for a distribution, which is close to normal distribution. The graph is close to a parabola."
2248,"Presents the risk aversion when the weights are directly set to 0.1 at the 50%, 90%, 95%-quantiles, 0.15 at the 99% and 99.9%- quantiles and 0.4 at the 99.98%-quantile."
2250,"Estimates an implied volatility surface applying the Black and Scholes formula, an iterative algorithm to estimate the implied volatility employing the bisection method, and localized kernel regression using the Nadaraya Watson estimator with bandwidths bw = c(250,0.5) for moneyness and time to maturity for a quartic kernel. A 3D graphical illustration is given."
2251,Test Adaptive Weights Clustering on several data sets.
2253,Plots the log-prices of the 20 largest cryptocurrencies over a period from July 2017 - February 2020.
2254,Implementation of a cointegration-based trading algorithm. Visualization of the trading strategy and comparison to CRIX as an industry benchmark
2255,Simulation and Evaluation of the Co-Intensity VECM.
2256,Estimates a nonlinear VECM specification using cryptocurrency data. Plots the nonlinear part of the model
2257,Scrape and merge the prices of the N largest cryptocurrencies over a specified time period
2258,Estimates the vector error correction model coefficients in a multivariate time series of cryptocurrencies.
2260,Determines the cointegration rank with a Wachter QQ plot.
2262,"Simulates heteroscedastic data and fits the 10% quantile, median and 90% quantile."
2263,"Plots the mean squared error, 50% tilted absolute error, 90% tilted absolute error"
2264,Transaction graph analysis and temporal community detection for cryptocurriencies
2265,Benchmarking the strength of correlation between sentiment in the market and cryptocurrency performance by conducting a sentiment analysis using Twitter data
2266,Clustering 9999 cryptopunks into 200 clusters based on Hierarchical Clustering
2269,"We perform a sequential non-parametric change point detection method in distribution for BTC and ETH, individually. Then, we apply the PCA on both time series and use the same method to find change points. A sensitivity analysis is conducted, showing that the number of estimated change points reduces as we decrease the starting period, as well as when we increase the ARL. "
2270,"Working on the Elliptic labeled Bitcoin transaction dataset, an attempt is made to detect illicit activities using different Machine Learning models"
2271,"Trying to discover links between Cryptopunk prices and their attributes, using mainly Linear Regression"
2282,"Analysis of Bitcoin fluctuations using sentiments from tweets that were scraped from Twitter. The script can scrape the last 7 days of tweets and minutely BTC prices, and display a cross-correlation measure between sentiment scores and measures (return and volatility). The script includes a simple user interface, where the user can choose to use sample data or scrape real time."
2283,Scraping all the Swiss National Bank Monetary Policy Assessments from 2000 to 2020. Reading in the pdf files and save as excel file
2284,Using a sentiment indicator build from SNB monetary policy assessments to forecast Switzerlands GDP
2285,Building a sentiment indicator with counts of positive and negative words of the SNB monetary policy assessments
2286,Cleaning unstructured text data and performing an LDA analysis of quarterly Swiss National Bank Monetary Policy Assessments
2287,Using Polarity Based Sentiment Analysis and linear regression to find the effect of Reddit Submissions on Bitcoin and Ethereum
2289,Plotting various perfomance graphes for simulated technical trading strategies. This quantlet should ideally be executed after SDA_2020_St_Gallen_01_DataImport.
2290,Plotting Results from the Monte Carlo Simulation
2298,"This quantlet is part of other quantlets and should ideally be executed after the quantlets before (see parent folders). Here the monte carlo simulations performed in the quantlet 'SDA_2020_St_Gallen_02_Simulations' are plotted. First, a plot showing the performance of the q-learning algorithm with 100 randomly generated initial q-tables. Second, a plot showing the performance of the q-learning algorithm in comaprison to a simple Buy and Hold and considers the confidence intervals."
2300,"Fetching, Preprocessing and Plotting data. This Quantle is part of other quantlets and represents the first step. The overall goal is to build a trading bot, which analyzes a single timeseries of crypto currency prices (BTC/USD) to identify optimal conditions for entry and exit trades in real-time. This quantlet retrieves data from a github repository, which is minutely data of BTC/USD from Binance of the years 2013 to 2019."
2301,Scrape S&P 500 component stocks from the start of pandemic and Cluster their performance using different clusering methods
2302,Use Variational Bayes (VB) to approximate the density of Gaussian Mixture Models (GMM)
2303,"Visualization Example for the Fast Text Word Embeddings. Pretrained FastText word embeddings vectors in German language are used. Dimensionality reduction fromm 300 to 3 via PCA. One sees, that the semantics are nicely extracted and capitals respectively countries are positioned close to each other and could be seperated along the second principal component."
2304,"Random Benchmarks to evaluate the trained Email Processing Time model. 3 different Benchmarks were applied. A stratified random classifier, a classifier always selecting the most frequent class and a classifier selecting the two most frequent classes based on their probabilities."
2305,"Simulation of datasets for the estimation of the CATE. Different settings are possible, e.g random assignment, confounding (selection-bias), linear and non-linear dependencies."
2308,Files to estimate the CATE from the empirical dataset (401(k) eligibility). Uses all meta-learners as well as causal BART and causal forest.
2311,Two model method to estimate the conditional average treatment effect (CATE) via a variety of machine learning (ML) methods.
2312,X-learner method to estimate the conditional average treatment effect (CATE) via a variety of machine learning (ML) methods.
2313,Causal Bayesian additive regression tree method to estimate the conditional average treatment effect (CATE) via a variety of machine learning (ML) methods.
2314,A simple example of treatment effect heterogeneity given characteristics.
2315,Single model method to estimate the conditional average treatment effect (CATE) via a variety of machine learning (ML) methods.
2316,Causal Forest implementation from the GRF package to estimate the conditional average treatment effect (CATE) via a variety of machine learning (ML) methods.
2317,Estimates confidence intervals based on the bootstrapping and a gaussian assumption
2318,Causal Boosting (regression tree) method to estimate the conditional average treatment effect (CATE) via a variety of machine learning (ML) methods.
2319,Creates a training and test-set for causal inference analysis.
2320,Shows the advantage of cross-fitting vs. single estimation on a test-set.
2321,Inverse probability weighting (transformed-outcome) method to estimate the conditional average treatment effect (CATE) via a variety of machine learning (ML) methods.
2322,Doubly-Robust method to estimate the conditional average treatment effect (CATE) via a variety of machine learning (ML) methods.
2323,R-learner (or orthogonal-learner) method to estimate the conditional average treatment effect (CATE) via a variety of machine learning (ML) methods.
2324,Files to estimate the CATE from the empirical dataset (Microcredit in Morocco). Uses all meta-learners as well as causal BART and causal forest.
2326,"Simulates an alpha stable Lévy process and plots the simulated data, log returns and the density of log returns. "
2327,"Implementation of a naive Fast Fourier Transform achieving the arithmetic but cnot the computational complexity of O(N log N).  The following is a Radix-2 Decimation-in-Time Cooley-Tukey FFT. The supplied data has to have a number of observations that is a power of 2; that means, log2(N) is an integer."
2328,"We perform image processing by applying kernel filters using convolution and the fast Fourier transform. To do so, we use the 'smoothie' package and its included kernel2dsmooth() package. We apply these transformations on the image of Mount Rushmore."
2329,"NIC_LevyPoisson simulates a compound poisson process with drift b and jump intensity lambda. Additionally the simulated data, log returns and density of log returns are plotted."
2330,"Simulates a jump diffusion process with drift b and jump intensity lambda. The simulated process consists of a Gaussian component and a jump component. Additionally, the simulated data, the gaussian component, the jump components, log returns and the density of log returns are plotted. "
2331,Plots the order of computations of the Discrete Fourier Transform (DFT) and the Fast Fourier Transform (FFT) as a function of the sample size.
2332,Example of a moving average filter through convolution which has been applied on a random walk using the Fast Fourier Transform.
2333,"‘Performs the Penalized Adaptive Method (PAM), a combination of propagation-separation approach and a SCAD penalty, to forecast a model with possible parameter changes on a given dataset. The model is then used for forecasting over a 1-year horizon. The input data are monthly observations of k-year excess bond risk premia, k = 2, 3, 4, 5, forward rates and other pre-defined macro variables. Computes RMSPE and MAPE for the forecasted values. Plots the time series of the predicted vs. observed values of bond risk excess premia over the prediction interval.’"
2335,"Extract text data of Abstracts from SFB website and explore the abstracts, draw words cloud."
2336,Clustering CRIX Currency Index and its volatility VCRIX with Gaussian Mixture Model (GMM)
2338,Calculate the whole sample and moving window network connectedness of industry portfolios
2339,"Plot the interdependency networks of the return data of 49 USA industries                       under different stress situations (median, lowertail and uppertail) with larger size denoting high in or out centrality scores"
2340,Regress the moving-window connectedness and clustering coefficients of industry network on volatilities of Fama-French three risk factors
2341,"Plot the image maps of the predictive power of 49 USA industries return data                        under different stress situations (median, lowertail and uppertail)"
2342,"Calculate monthly level excess return of 12 network centrality-based trading strategies and their Sharpe ratio, in order to compare with market portfolio"
2343,Calculate the root mean square error (RMSE) of the one-month forward return prediction under different tail levels and compare the results
2347,Calculate the whole sample and moving window network clustering coefficients of industry portfolios
2348,Calculate the in- and out- eigenvector centrality scores of directed and weighted graphs
2349,Calculate the annualised log cumulative excess return of 12 network centrality-based trading strategies and compare them with market portfolio
2350,Plot the quantile-quantile plot of the monthly return of 49 industry portfolios in USA
2351,calculate the summary statistics of moving window beta coefficients to give the direction information about industry return prediction
2352,"Generate the beta coefficients and lambda coefficients of the 1-step generalized predictive model on the return data of 49 USA industries                       under different stress situations (median, lowertail and uppertail) by using quantile lasso regression to reduce dimensions"
2355,Computes the ADF and KPSS test statistics for 20 biggest companies (by market capitalisation) of FTSE100 and DAX. Estimated ADF and KPSS test with only a constant and a constant plus linear trend. Adds asterisk to indicate significance at 5%-confidence level
2357,"Plots two realizations of an MA(1) (moving average) process with MA coefficient = beta, random normal innovations and n=n1 (above) and n=n2 (below)."
2358,"Computes the first order autocorrelation of the returns, squared returns and absolute returns as well as the skewness, kurtosis and Jarque-Bera test statistic for German and British blue chips, 2004 - 2014."
2359,"Simulates Augmented Dickey-Fuller tests for stationary and non-stationary ARMA processes. The simulated process x(t) is x(t) = alpha x(t-1) + beta epsilon(t-1) + epsilon(t), where the error term is a Gaussian White Noise process. Each process is simulated 1000 times. For each simulated process the number of lags (p) included for the ADF test varies between 3 to 11."
2360,"Monte Carlo Simulation of ARCH(1)-process with 1000 repetitions. Estimates alpha-parameter with Quasi Maximum Likelihood method. Shows summary statistics for different sample sizes: 1) mean of all estimates; 2) average deviation from true alpha; 3) percentage of parameters estimated equal or lager than one. Default parameter settings: alpha = 0, omega = 0.2"
2361,"Reads the date, DAX index values, stock prices of 20 largest companies at Frankfurt Stock Exchange (FSE), FTSE 100 index values and stock prices of 20 largest companies at London Stock Exchange (LSE) and estimates various GARCH models for the DAX and FTSE 100 daily return processes from 2006 to 2016."
2362,Plots the exact and conditional likelihood function of an MA(1) (moving average) process.
2364,"Reads the date, DAX index values, stock prices of 30 largest companies at Frankfurt Stock Exchange (FSE), FTSE 100 index values and stock prices of 100 largest companies at London Stock Exchange (LSE) and plots the right sides of the logged empirical distributions of the DAX and FTSE 100 daily returns from 2004 to 2014 (m=10)."
2365,Hill quantile estimation based on random numbers drawn from standardized Pareto distribution.
2366,Estimates the expected shortfall of a sample y using expectiles. The estimation requires an expectile level such that it is equal to the value at risk with the predetermined risk level. This is obtained from a normal Laplace mixture. Estimation is executed using moving windows of subsamples.
2367,"Uses a GARCH(1,1) model and a constant mean to standardze the data input."
2368,Calculates and compares the expectile and quantile functions for all risk levels under different distribution.
2369,Estimates the theoretical expected shortfall of a sample for a normal Laplace mixture. This is done using a expectile based method. The calculation requires a expectile level such that it is equal to the value at risk with the predetermined risk level. This and the relevant quantile are obtained from a normal Laplace mixture.
2370,Calculates the expectile value which is equal to the quantile for all risk levels under a special distribution and compares the expectile vs. quantile in plots.
2371,Compares the average rejection rate of the null hypotheses for the individual and simultaneous inference (confidence intervals constructed by asymptotics or bootstrap).
2372,Compares the prediction norm with the penalty level selected equation by equation or jointly over equations in a regression system under iid data setting.
2373,Compares the prediction norm with the penalty level selected equation by equation or jointly over equations in a regression system with dependent data.
2374,This Quantlet computes the moments and the Sharpe Ratio for several cryptocurrency indices. Additionally the Probabilistic Sharpe Ratio (Bailey and Lopez de Prado 2012) are computed.
2375,This Quantlet computes and visualizes the SVCJ estimates of different rolling window sizes. The file SVCJ_fit.R is from Ivan Perez (2018) and fits the SVCJ model to the CRIX data. Each line refers to time-series estimates of the respective parameter
2378,"his Quantlet uses SVCJ estimates of the Quanltet SVCJrw_graph_parameters and clusters several pairs of them by k-means. Thereby some patterns of the dynamics of cryptocurrencies are observable, and those are visualized with the CRIX."
2379,do fixed rolling window exercises using 60 and 500 observations for the two selected return time series DAX and SP 500 with the multivariate CAViaR model.
2384,estimate the quantile values at levels 5% and 1% with the localising multivariate CAViaR model under two risk cases.
2393,"do the adaptive estimation for localising multivariate CAViaR model, under two risk cases at quantile level 1%."
2403,"do the adaptive estimation for localising multivariate CAViaR model, under two risk cases at quantile level 5%."
2413,Plot the return time series of DAX and SP 500.
2414,"plot the adaptive selected intervals of localising multivariate CAViaR model, at two quantile levels 5% and 1% with two risk cases."
2416,"Clustering and Principal Component Analysis of Cryptocurrency Daily Trading Data from 08/08/20 until 10/23/20 for Bitcoin, Ripple, LiteCoin, Ethereum, BitcoinCash"
2419,"This Quantlet uses various statistical techniques to analyze data related to BTC price, transactions, sentiment, and energy demand to forecast the future energy demand of the BTC network."
2424,"Counts positive and negative words using the lexicon by Loughran and McDonald. Please install required Python packages before usage: os, io, collections, nltk."
2425,"Counts positive and negative words using the lexicon by Loughran and McDonald and the lexicon by Bing Liu. Then, the results are evaluated by using the training set of Malo et al. (2014) and computing a confusion matrix.
This training set is available at https://www.researchgate.net/publication/251231364_FinancialPhraseBank-v10 but a pickled and preprocessed version is available in this quantlet folder.
Please install required Python packages before usage: os, io, collections, nltk, pickle."
2426,"Shows basic functionalities of natural language processing: sentence and word tokenization, part-of-speech tagging, lemmatization, stopword removal and word counting. POS-tags are converted to WordNet compatible tags such that the WordNet lemmatizer can be used. Please install required Python packages before usage: os, io, types, collections, textblob, nltk."
2427,"Plots simulation results of stock volatility based on the regression results of the attention panels. In the panel model, sentiment projections of three different sentiment lexica have been used. Furthermore, the mean curve and corresponding confidence bands are plotted to investigate the asymmetric reaction of volatility given positive and negative sentiment."
2428,Plots locations and customer ratings of German breweries. Red indicates bad scores while green indicates good scores. The size of each bubble represents the number of beers that are produced by an individual brewery.
2429,"Plots simulation results of stock volatility based on the regression results of the entire panel. In the panel model, sentiment projections of three different sentiment lexica have been used. Furthermore, the mean curve and corresponding confidence bands are plotted to investigate the asymmetric reaction of volatility given positive and negative sentiment."
2430,"Estimates regularized linear model for sentiment classification with stochastic gradient descent (SGD) learning. The results are evaluated by using the training set of Malo et al. (2014) and computing a confusion matrix.
This training set is available at https://www.researchgate.net/publication/251231364_FinancialPhraseBank-v10 but a pickled and preprocessed version is available in this quantlet folder.
Please install required Python packages before usage: os, io, nltk, pickle, sklearn, pandas, numpy."
2431,"Counts positive and negative words using the lexicon by Liu et al. Please install required Python packages before usage: os, io, collections, nltk."
2432,"Optimizes least squares loss function via stochastic gradient descent. The results of each iteration are plotted for different choices of eta. eta = 1/i (black), eta = 1/1000 (blue), eta = 1/1500 (green), eta = 1/2000 (orange), eta = 1/2500 (purple)"
2433,Estimates a VECM of the prices using a FGLS estimator and plots the residuals
2437,"Computes summary statistics, normality tests and tests for autocorrelation and GARCH effects of the price data"
2439,"Computes and plots nonparametric estimates of the unconditional variances and correlations of crude oil, biodiesel and rapeseed using kernel methods. The bandwidth is determined using a likelihood cross-validation criterion. Furthermore, pointwise confidence intervals are computed based on 200 bootstrap experiments."
2446,"Estimates the cointegration relationship between crude oil, biodiesel and rapeseed using a feasible generalized least square (FGLS) estimator and estimates a vector error correction model (VECM) of the three price series."
2449,"Computes and plots estimates of the conditional variances and correlations of crude oil, biodiesel and rapeseed. Estimates are obtained using a dynamic conditional correlation (DCC) model, where the conditional variances follow an EGARCH(1,1) process."
2453,"Shows a plot of the raw data and deseasonalized log prices of crude oil, biodiesel and rapeseed"
2455,"We provide results for ""Tail-risk protection: Machine Learning meets modern Econometrics"", Spilak, WK Härdle (2020). Please refer to README2.md for a detailed description on how to use the code."
2461,"Do a first econometrics analysis of btc log returns following Chen et al (2017) in `MLvsGARCHecon_1.R`, then build a rolling forecast with selected model `MLvsGARCHecon_2.R`"
2469,Use ICA to recover independent original source
2470,"Use distributional characteristics such as fourier power spectrum, moments, quantiles, global we optimums, as well as the measures for long term dependencies, risk and noise to summarise the information from crypto time series and conduct clustering via spectral clustering"
2472,Generate plots to illustrate the BERT result and importance of words
2496,Shows expected loss/risk ratio for the expected shortfall contribution at different quantiles.
2507,"Pots the 1%-quantile from the two one-parametric sub-families of the class of quadratic forms of Gaussian vectors, as defined by the functions XFGdg11par and XFGdg12par"
2514,"Computes the values of the expected returns and the expected shortfall (ES) of the optimal portfolios with different holding weights % for the 1st underlying asset under an EGARCH(1,1) model, where the number of assets is 2"
2516,"Plots an estimated state price density of the stock prices estimated by implied binomial tree and plots the estimated implied local volatility surface. Require IBTbc.m, IBTdk.m, IBTimpliedvola.m, IBTblackscholes.m, IBTcrr.m, IBTlocsigma.m, IBTsdisplot.m, IBTvolaplot.m, regxest.m,"
2520,nw computes the Nadaraya-Watson Estimator
2522,"XFGIBT01 Outputs implied binomial trees computed by DK and BC algorithm of stock prices, transition probabilities and Arrow-Debreu prices, respectively, with a parabolic implied volatility (see IBTimpliedvola). Require IBTblackscholes.m, IBTdk.m, IBTbc.m, IBTimpliedvola.m, IBTresort.m, IBTcrr.m"
2524,"Computes the values of the expected returns and the SRM of the optimal portfolios with different holding weights for the 1st underlying asset under an EGARCH(1,1) model, where the number of assets is 2"
2525,"Outputs a plot of state price density estimations, from Derman and Kani IBT, Barle and Cakici IBT, and Monte-Carlo simulation. Require Eulerscheme.m, XFGIBTcdk.m, IBTsdisplot.m, XFGIBTcbc.m, regxest.m, and optionprice.m"
2530,"Plot News Network Trigered Investor Attention Indices (NNTA) with its 2 components (value weighted NNTA index, NNTA^{sz}, and eigenvector centrality weighted NNTA index, NNTA^{ctr}.)"
2531,"Plot news surprise index. The definition of ""surprise"" follows uncertainty definition of Kyle, Sydney and Serena (2015)"
2532,Counts positive and negative words using the lexicon by Bing Liu
2533,Plot market wide news optimism index and news disagreement index weighted by individual firm's market beta.
2534,Reports the size performance of 2SQR(1) and 2SQR(2) with five different types of persistent predictor in simulation.
2535,"Plots the power curves by 2SQR(1), 2SQR(2) and IVX-QR in simulation."
2536,"Plots the kernel density of the coefficients estimated in BQPR model in simulation, which is consistent and asymptotically follows normal distribution."
2537,"Plots the one step ahead estimated VaR(0.05) and VaR(0.95) by 2SQR(2), with univariate or bivariate predictors, and real observations."
2538,Plots the kernel density of the statistics estimated by BQPR model in simulation.
2539,Reports the predictability (p-values in the hypothesis tests) of US stock returns by 2SQR(1) and 2SQR(2).
2540,"This Quantlet allows to visualize the indices under review, to compute higher moments and to compute several comparison measures."
2541,generate additive model and plot shapley values against both functions of the DGP.
2542,Predict housing price using the Boston housing data set. Then calculate SHAP values and plot the respective figures.
2543,This code create images for the animation that shows the connection between eigenvalues of Laplacian matrix and the number of connected components in the graph
2544,"This quantlet was originally writen by Eliza Zinovyeva which is also available on Quantlet repository under HClustering. The current code is modified for application of Spectral Clustering on the same data using the same methodology.
Generally, the code uses the keywords from Quantlet data, tokenizes the words to vectors and applies spectral clustering using scikit python library to cluster the quantlets into different number of clusters. It also uses PCA and TSNE for visualisation of the clusters"
2545,This code is modified and simpler version of original code available on scikit learn python library. The codes creates a dataset of noisy circles and performs clustering using K-means and SC
2546,This code provides a step-wise example of Spectral Clustering and uses simple linear algebra to perform SC on a hand-made simple theoretical example
2547,Pink noise is a signal with a frequency spectrum such that the power spectral density is inversly proportional to the frequency of the signal. We generate the pink noise using AR model with high positive alpha and produce ACF and PACF plots.
2548,Antisocial Online behaivor detection. This part is dedicated to lexicon-based approaches. The list is taken from http://www.bannedwordlist.com/
2549,"Antisocial Online behaivor detection. Contains traditional machine learning methods: ridge regression, SVM, random forest, lightGBM"
2553,"Antisocial Online behaivor detection. This part is dedicated to interpretability using LIME framework [Ribeiro, M. T. et. al  (2016, August). ""Why should i trust you?"" Explaining the predictions of any classifier.]"
2555,"Antisocial Online behaivor detection. Contains benchmarking on different models deep learning models: GRU, LSTM, BGRU (bidirectional), BLSTM, BGRU + attention, BGRU + global average pooling, BGRU + global maximum pooling. Furthermore contains HAN and psHAN"
2561,"Antisocial Online behaivor detection. This part is dedicated to pre-trained deep transformers, such as BERT and DistilBERTI"
2577,"Antisocial Online behaivor detection. Contains data preparation procedure for traditional, deep learning models, including hierarchical attention model"
2581,Computes desciptive statistics and correlation of complexity measures and bank variables
2582,Computes distances between all sentences in a given version of the GBA and aggregates them to a single complexity measure.
2584,"Preprocesses sample data, complexity measures and text descriptive data to be included in regressions or when computing descriptives."
2585,Plots the different complexity measures and their yearly aggregates over time
2586,Preprocesses the scraped webpages into python dictionaries ready to use in the analysis.
2587,Computes sentence embeddings according to Le et al. (2014) doc2vec using gensim
2588,Computes word embeddings according to Mikolov et al. (2013) word2vec using gensim
2589,Computes regressions and figures for economic effects of the dataset split in small and in large banks.
2590,"Computes regressions, tests for fixed effects and heteroscedasticity, figures for economic effects and conducts the resdiual analysis"
2592,Scrapes the webpage http://www.buzer.de that contains all changes to the German Banking Act since 2006.
2593,Plots the per capita GDP in different countries over a time-series. In a second step data will be normalized and plotted.
2594,Plots the first 100 simulated values of a random walk.
2595,Computes a bootstrap confidence interval for the correlation coefficient between body and brain of the mammals data (MASS package).
2596,Plots the regression line with a 95% confidence interval and a shaded 95% prediction interval.
2597,Shows a loop that approximates the golden ratio and that stops at a certain point.
2598,"COPlcpinVaR computes from the results of fitting a Aparch(1, 1) model to daily returns, see COPlcpinaparch, and choosing the HAC model, see COPlcpinres, the profit and loss function (dots) with the 99%-VaR bound (solid line) and the time points of the exceedances (pluses)."
2599,"COPlcpinaparch fits to the daily returns of the indices Dow Jones (DJ), DAX and NIKKEI in the time span [01.01.1985; 23.12.2010] a aparch(1,1) model and gives back the mu, the parameters of the model, skewness and shape, Ljung-Box and the Kolmogorov-Smirnov test statistic. Also are given in the line beneath of the values the corresponding standard deviations."
2600,The Quantlet COPhelperfunctions includes several routines for the Quantlet COPlcpexres.
2601,"COPhac4firmstree6 is used to give a tree plot of a 5-dim HAC. Here the Gumbel generator is used. The parameter for X3, X4, X4 equals 2.005 and for ((X3, X4, X5), X2, X1) is 1.005. In the plot all parameters have been transferred into the tau form."
2602,"COPapp1return gives time series plots of 3 companies' daily returns. The three companies Apple (APL), the Hewlett Packard (HP) and Microsoft (MSFT) are contained. In the figures the window from 04.01.2006 to 04.11.2009 is considered."
2603,"Computes from the results of fitting a Aparch(1,1) model to daily returns, see COPlcpinaparch, when which copula fits the data best. Used are Gumbel and Clayton copula."
2605,"COPhac5pp gives a plot of theoretical probabilities against empirical probabilities under the context of a 5-dim HAC. Here the Gumbel generator is used. The parameter for X3, X4, X4 equals 2.005 and for ((X3, X4, X5), X2, X1) is 1.005. Here 1500 random numbers have been drawn from the aforementioned HAC and this sample then will be computed to obtain the theoretical probabilities and the empirical probabilities. Hence for every drawn random number there exist a theoretical probability and an empirical probability. We plot for every such pair in black points in the figure and at last a line is fitted according to all the scatter points in red."
2620,"COPhac4firmsscatter gives pairwise scatterplots from ARMA-GARCH residuals provided from the HAC package, including Chevron Corporation (CVX), Exxon Mobil Corporation (XOM), Royal Dutch Shell (RDSA) and Total (FP) covering n = 283 observations from 2011-02-02 to 2012-03-19."
2621,"COPhac4firmstree5 is used to give a tree plot of a 10-dim HAC, where the Gumbel generator is used. The parameter for Z3 and Z4 is 3 and for (Y1, Y2, (Z3, Z4)) is 2.5. The parameter for Z1 and Z2 is 2, and for X1 and X3 is 2.4. The parameter for ((Y1, Y2, (Z3, Z4)), (Z1, Z2), (X1, X2), X3, X4) is 1.5. The difference from COPhac4firmstree4 is employing colours, red for frames and blue for expressions and lines for knots."
2622,"COPapp1residual shows pairwise scatter plots of based on AR(1)-GARCH(1,1) fitted residuals. The 3 upper triangular plots show the pairwise residuals scatter points. The 3 lower triangular plots show the scatter points computed from empirical cdf of the residuals. Companies Apple (AAPL), the Hewlett Packard (HP) and Microsoft (MSFT) are contained."
2623,COPdaxreturn gives a DAX returns' time series plot with a window from 1986-01-03 to 2008-12-30.
2624,"COPcorrelation2 gives a plot of 100 scatter points generated respectively by uniform distribution and normal distribution with a drift generated by the aforementioned uniform distribution. Also the three different correlations are computed, the correlations of Pearson, Kendall and Spearman."
2625,"COPhac5varsmpscatter gives a plot of pairwise scatter plot of an HAC sample with 5 dimensions and a Gumbel generator. The parameter for X3, X4, X4 equals 2.005 and for ((X3, X4, X5), X2, X1) it is 1.005. According to these structure we construct an HAC with a Gumbel generator. Then 1500 random numbers are drawn from this HAC. Hence, for every variable we obtain 1500 numbers. Then for each pair of two variables we can obtain a scatter plot."
2627,"COPhac3dscatter2 gives samples generated from an hierarchical Archimedean copula (HAC), where the red points stand for the 1100 scatter points drawn from the HAC. The generator of this HAC is Gumbel and the marginals here contain the Student-t CDF for variable x1, the normal CDF for x2 and the normal CDF for x3. The parameter for x1 and x3 in the first hierarchy is 10 and in the second hierarchy the parameter is 2. The blue points on the vertical plane stand for the projection of the red points onto the plane x1 and x3. The blue points on the bottom plane stand for the projection of the red points onto the plane x1 and x2."
2628,"Six bivariate copula plots and their corresponding contour plots are created including the Gaussian copula, the Gumbel copula and the Clayton copula. In the figure, the upper blue picture is a normal copula with 2 Gaussian margins and an exchangeable parameter equal to 0.71. The lower blue plot is a normal copula with 2 Student-t margins with degree of freedom equal to 3. The upper red plot is a Gumbel copula with 2 standard normal margins and an exchange- able parameter is 2. The lower red plot is also a Gumbel copula with 2 t margins under degree of freedom of 3. The upper green plot is a Clayton copula with 2 standard normal margins with an exchangeable parameter of 2. The lower green plot uses the same copula with the upper green but 2 t margins with degree of freedom equal to 3."
2629,This quantlet plots a histogram of daily DAX 30 index levels from 1986-01-02 until 2012-12-30.
2630,"COPlcpexVaR computes from the results of fitting a Garch(1, 1) model to daily returns, see COPlcpexgarch, and choosing the HAC model, see COPlcpexres, the profit and loss function (dots) with the 99%-VaR bound (solid line) and the time points of the exceedances (pluses)."
2631,"COPlcpexgarch fits to the daily returns of the exchange rates JPN/USD, GBP/USD and EUR/USD in the time span [4.1.1999; 14.8.2009] a Garch(1,1) model and gives back the mu, the parameters of the model, skewness and shape, Ljung-Box and the Kolmogorov-Smirnov test statistic. Also are given in the line beneath of the values the corresponding standard deviations."
2632,"COPcorrelation1 gives a plot of 100 scatter points generated respectively by uniform and normal distribution. Also the three different correlations are computed, the correlations of Pearson, Kendall and Spearman."
2634,"COPhac4firmstree4 is used to give a tree plot of a 10-dim HAC, where the Gumbel generator is used. The parameter for Z3 and Z4 is 3 and for (Y1, Y2, (Z3, Z4)) is 2.5. The parameter for Z1 and Z2 is 2, and for X1 and X3 is 2.4. The parameter for ((Y1, Y2, (Z3, Z4)), (Z1, Z2), (X1, X2), X3, X4) is 1.5."
2635,"COPdensitydaxreturn gives the DAX histogram compared to the 3 densities including the standard normal density in red, the Student-t density in green with degrees of freedom equal to 5 and the estimated Epanechnikov kernel density in blue with the bandwidth computed following that 0.9 times the minimum of the standard deviation and the interquartile range divided by 1.34 times the sample size to the negative one-fifth power, i.e. Silverman's rule-of-thumb."
2636,"COPhac3dscatter1 gives samples generated from an HAC copula, where the red points stands for the 1100 scatter points drawn from the hierarchical Archimedean copula. In this case we use a Gumbel generator function and the marginals contain a Student-t for variabel x2, the normal for variable x1 and also the normal for x3. The parameter for x1 and x2 in the first hierarchy is 10 and in the second hierarchy the parameter is 2. The blue points on the vertical plane stand for the projection of the red points onto the plane x1 and x3. The blue points on the bottom plane stand for the projection of the red points onto the plane x1 and x2."
2637,COPdaxtimeseries gives a time series plot of the DAX index levels with the window from 1986-01-02 to 2008-12-30.
2638,"COPhac4firmstree3 is used to give a tree plot of a 4-dim HAC. And after the fitting the 4 firms data to the HAC the parameters are also estimated. Here the HAC is employed under the Gumbel generator and margins is given by the empirical CDF. The data is from ARMA-GARCH residuals from HAC package, including Chevron Corporation (CVX), Exxon Mobil Corporation (XOM), Royal Dutch Shell (RDSA) and Total (FP) covering n = 283 observations from 20110202 to 20120319."
2639,"COPlcpexres computes from the results of fitting a Garch(1,1) model to daily returns, see COPlcpexgarch, when which copula fits the data best. Used are Gumbel and Clayton copula."
2640,"COPdaxnormhist gives histogram of DAX returns compared to the normal CDF. Here the normal simulations are based on the parameters that mean = 0.0002 and standard deviation = 0.0141. In the plot, the blue straps plus the purple straps stand for the DAX returns' histogram, and the pink straps plus the purple straps stand for the histogram of the normal simulations with the aforementioned parameters. And the purple straps stand for the over- lapped parts of the both histograms. Here the number of the points of the time series used in the computation is 5771, which is the same number as the number of the normal simulations"
2641,Explanatory example for dimensionality reduction t-SNE vs PCA accompanying the hierarchical clustering project
2643,Motivation example on image segmentation with k-means clustering
2644,Hierarchical clustering on qunatlets
2645,"Identify and remove outliers, perform oversampling, select the 44 most important features of the dataset"
2647,"Preprocess raw data from WRDS, create separate .csv files for balance sheet, income statement and cash flow reports, Identify and match the companies that went bankrupt or disappeared, create labels for each type. Calculate financial ratios and save into a separate .csv file. Collect data of macroeconomic indicators from World Bank database, map into companies panel."
2650,"Create summary statistics for financial ratios and make visualizations for macroeconomic indicators, polarity scores and overview of defaults"
2652,"Collect textual data from companies annual reports, perform sentiment analysis, report polarity scores."
2654,Implement machine learning algorithms for default prediction 
2658,Web scrape data for Crypto Assets and SP500 constituents from the internet. The tickers are scraped from wikipedia and timeseries data is downloaded.
2660,"This Quantlet scrapes the news website Finanzen.net for pre-specified articles that are linked to a specific stock. Afterwards, this Quantlet assigns sentiment weights to the words based on the stock movement followed by the publication."
2661,This Quantlet calculates a test sample to validate the predictions made with data from the web scraping Quantlet.
2662,This Quantlet takes the output of the web scraping and forecasting Quantlets to visualize the results as a WordCloud and a scatterplot for the prediction vs realized return and volatility of the stock in the test sample.
2663,Implementation of three different portfolio optimisation methods. The classical Markowitz Minimum Variance technique is compared to the Inverse Variance Portfolio and a new approach called Hierarchical Risk Parity (HRP). HRP combines graph theory and machine learning techniques to determine the optimal allocation to assets based on the information contained in the covariance matrix. Strategies are applied to Crypto Currency Assets and Stocks and evaluated on risk/return profile.
2664,The repository contains 3 parts : 1. Person of interests 2. Natural language process (LDA) 3. Network
2701,Plot the 3D CDS curves of the selected 10 banks.
2703,"Estimate the level, slope, and curvature factor based on DNS model with the whole data sample, and then calculate the network connectedness for each factor with variance decomposition."
2707,"Forecast CDS spreads with network information in 2011-2013 under DNS model, and calculate the difference of RMSE between DNS model with or without other bank information. Further, perform a DM test to compare the out-of-sample performance."
2711,"Estimate the level, slope, and curvature factor based on DNS model with one-year rolling window (260 observations), and then calculate the daynamic of network connectedness for each factor with variance decomposition."
2714,"Group the dynamics of connectedness measures in US and European banks for level, slope, curvature factor respectively."
2715,Compares changes in Body Mass Index for the KNHANES 1998 and 2015 datasets
2716,Computes t-test to compare kcal intake by macronutrients from KNHANES nutrition survey of 1998 and 2015
2717,Plots the South Korean GDP per Capita (in current USD) data from 1967 - 2015 using ggplot2.
2718,Demographic analysis of the 1998 and 2016 KNHANES datasets using sample weights.
2719,Plots the kilocalorie intake by Macronutrients of the South Korean population from 1998 - 2015 using a area chart. Data is based on KNHANES datasets.
2720,Computes Principal Component Analysis on KNHANES Nutrition Survey dataset aiming to derive key dietary patterns based on 23 food groups
2721,Summarizes food group intake by kilocalories from the 1998 and 2015 KNHANES datasets.
2722,Computes k-means Cluster Analysis on 1998 and 2015 KNHANES Nutrition Survey dataset aiming to derive key dietary patterns based on 23 food groups
2723,Upload your Code as an NFT on the Algorand Platform
2726,Analyzes demographic trend (fPCA) and forecasts mortality and fertility in Japan using Hyndman-Ullah method.
2727,Analyzes demographic trend (SVD) and forecasts mortality in Japan and Taiwan using Lee-Carter method.
2728,"Compares and plots forecast accuracy using Lee-Carter and Hyndman-Ullah methods for mortality data of Japan, and computes Diebold Mariano statistics."
2729,Plots time series graphs of mortality and fertility of Japan and Taiwan based on historical data sets.
2730,Computes the percent of rejection in example 5 (in the JBES paper Analysis of Deviance for Hypothesis Testing in Generalized Partially Linear Models).
2731,Computes the percent of rejection in example 2 (in the JBES paper Analysis of Deviance for Hypothesis Testing in Generalized Partially Linear Models).
2732,Computes the percent of rejection under H1 in example 1 (in the JBES paper Analysis of Deviance for Hypothesis Testing in Generalized Partially Linear Models).
2733,Computes the percent of rejection in example 3 when z1 and z2 are non-orthogonal to x (in the JBES paper Analysis of Deviance for Hypothesis Testing in Generalized Partially Linear Models).
2734,Computes the percent of rejection under H0 in example 1 with a=0 and random design (in the JBES paper Analysis of Deviance for Hypothesis Testing in Generalized Partially Linear Models).
2735,Computes the percent of rejection in example 4 when z1 and z2 are non-orthogonal to x (in the JBES paper Analysis of Deviance for Hypothesis Testing in Generalized Partially Linear Models).
2737,Computes the percent of rejection under H0 in example 1 with a=0 and fixed design (in the JBES paper Analysis of Deviance for Hypothesis Testing in Generalized Partially Linear Models).
2738,Computes the percent of rejection in example 3 when z1 and z2 are orthogonal to x (in the JBES paper Analysis of Deviance for Hypothesis Testing in Generalized Partially Linear Models).
2739,Computes the percent of rejection in example 4 when z1 and z2 are orthogonal to x (in the JBES paper Analysis of Deviance for Hypothesis Testing in Generalized Partially Linear Models).
2740,This is the application of  PrincipalExpectile algorithms to the fMRI data
2741,‘The source code of PrincipalExpectile algorithms'
2742,"This is the source code of TopDown, BottomUp algorithms"
2743,"This is the application of TopDown, BottomUp and PrincipalExpectile algorithms to the Chinese weather data"
2744,This is the application of  PrincipalExpectile algorithms to the Growth data
2745,"Calculates Value at Risk and Conditional Value at Risk using three different methods, namely Historical Simulation, Analytical Models and Monte Carlo Simulation. In Historical Simulation VaR and CVaR are calculated using equally weighted as well as differently weighted returns. As Analytical models normal distribution, Student's t-distribution and Cornish-Fisher Expansion were implemented to compute VaR and CVaR. In Monte Carlo Simulation the calculation of VaR and CVaR is based on the generation of normally distributed random log-returns using Geometric Brownian Motion or alternatively on the generation of Student's t-distributed random log-returns."
2746,Predictive model of donation behaviour. Tries to predict the party donated to based on donor characteristics. Also attemps to visualize effect of predictor variables.
2747,"Infers gender of contributors based on first names by using, census data of first names and adds the gender to the data."
2748,Generates the required data set from the raw data
2749,"Select variables to forcast the house price using random forest method, with the parameter taken from the tuning result."
2751,Performs first statistical analysis and plots state-averaged data
2752,Tune parameters for the support vector machine using corss validation. The optimal parameter will be used to foreast house price.
2753,Uses a shapefile of California to draw polygons of California zip codes are and fill them with a color representing the candidate who has collected the largest amount of donations in total in the corresponding zip code. The filled polygons are then plotted by longitute and latitute on a Google map image of California.
2754,Classifies the occupation with keywords and classifications taken from https://github.com/datasciencedojo/DataMiningFEC/blob/master/6%20Bucketing%20Occupation%20Groups.R
2755,"Process the raw data for further analysis, including impute NA values, convert category variables into numeric."
2763,Uses the IRS tax records for California to construct a proxy the wealth of a a zip code.
2764,Performs a principal component analysis in order to identify an underlying concept ''attractiveness of counties and county-level cities''
2765,Performs an exploratory factor analysis with four different extraction methods in order to (1) check the robustness of the results gained by principal component analysis to (2) identify the underlying concept (attractiveness of counties and county-level cities).
2766,"Prepares the FEC data set for California, i.e. the broken data set P00000001-CA.csv is loaded and transformed into a proper data frame. Moreover, column names and variable types are adjusted, invalid or NA values deleted and some more data manipulation is done."
2767,"Select variables to forcast the house price using support vector machine method, with the parameter taken from the tuning result."
2768,Displays PCA result in a appropriate and easy to interpret structure like a table or a heat map. Takes a look at relations between the individual observations and their potential similarities by conducting the k-Means clustering algorithm.
2770,"Optimizes a given portfolio, calculates the efficient frontier and calculates the portfolios net liquidation value over time"
2771,"Assigns to each zip code the candidate with the largest sum of contributions and the highest number of contributors, respectively."
2772,"This quantlet covers all plots for the three quantlets VaR_and_CVaR_Quantlet, Testing_Quantlet and PortfolioOptimization. For VaR_and_CVaR_Quantlet it plots different distributions (normal, Student's t and empirical distributions) with corresponding VaR and CVaR calculated within VaR_and_CVaR_Quantlet. For the Testing_Quantlet it plots two different distributions in order to compare these distributions as well as plotting the results of the Kupiec Test for the VaR and McNeal & Frey test for the CVaR. For the PortfolioOptimization Quantlet, it plots the portfolios weights and let liquidation value over time as well as the efficient frontiers."
2773,Tune parameters of the xgboost model using cross validation. The optimal parameter will be used to foreast house price.
2776,Calculates correlation between candidate gender and contributor gender and performs a Chisquare test of independence.
2777,Tune parameters for the random forest model using corss validation. The optimal parameter will be used to foreast house price.
2779,"Calculates the most common summary statistics (sum, mean, median, min, max, standard deviation) for the amount of money contributed to each candidate."
2780,Runs various statistical tests and a linear regression
2781,"Creates a function to automatically provide historical stock price data for the VaR and CVaR analysis. After downloading time series data for the required stocks from the internet, they are converted into data frame and all occurring NA observations are imputed. Currency effects are adjusted for american stocks. Finally, the returns of single stocks and those of a hypothetical portfolio are calculated."
2782,"Contains two different kind of tests. The function allows to execute three distribution tests, namely the Kolmogorow-Smirnov test, Kuiper´s test and the Anderson-Darling test, to find out whether data is normally distributed or Student´s t distributed (one-sample distribution test). Futhermore the Kolmogorov-Smirnov test performs a two-sample distribution test, to find out whether two datasets follow the same distribution. The second type of test is an unconditional coverage test, namely Kupiec's test, to test the Values at Risk. Futhermore a test for the CVaR based on McNeil and Frey is implemented."
2783,"Select variables to forcast the house price using xgboosting algorithm, with the parameter taken from the tuning result."
2784,Predictive accuracy analysis of option pricing models using high frequency data
2786,Creates a sentiment analysis of posts related to crypto currencies. The data for the analysis is taken from a crypto currency forum and is from the years 2013 to 2019. For the analysis the vader package is used.
2789,"Creates a word cloud for one of the three given texts (an Austrian novel - ""Der Mann ohne Eigenschaften"" by Robert Musil, a songtext - ""99 Luftballons"" by Nena or a poem - ""Gemeinsam"" by Hanna Schnyders) in the shape of a man with hat, a balloon or a star. A word cloud is a data visualization technique used for representing text data in which the size of each word indicates its frequency or importance."
2791,An Analysis of CRX daily log returns - are there higher returns at lower risk on index rebalancing days.
2797,"Applies random forests to classify real trees, based on Marius Sterlings DEDA_RandForest."
2798,"Shows an interactive interface to show the areas of type 1 and type 2 errors in a t test. The user can interactively choose the test type (two sided, less or greater), the hypothetical mean, the significance level and the sample size. Also, a table with summary statistics regarding the sample, population and the test is given. The sample can be drawn for variables of the data sets BOSTON HOUSING and USCRIME."
2800,Shows the pdf and the cdf of the normal distribution. Mean and variance can be changed interactively. The user can also plot the exponential distribution with adjustable parameter lambda.
2802,"Estimates and plots the confidence interval for the difference of two means from two groups. The user can interactivaly choose the confidence level, the respective sample sizes, and whether to assume equal or unequal variance. In the lower panel, the population is shown and the interquartile range of the sample is indicated by boxes. Also, the user can choose between three data sets and set the variable and group variable used to compute draw the samples."
2804,"Produces an interactive interface to show the histogram. The default settings show the histogram of the variable TOTALAREA of the USCRIME data set. The number of bins can be chosen and there are variables of three data sets available: CARS, DECATHLON, USCRIME."
2806,"Shows an interactive interface with 4 plots for univariate data (dotplot, histogram, boxplot, ECDF). The user can choose the dotplot type (overplot, jitter, stack), the number of bins in the histogram, and whether additional lines are shown to indicate mean and variance. The lower panel summarizes robust and non-robust location and dispersion parameters for the selected variable. Furthermore, the user can choose variables of the data sets USCRIME, CARS and DECATHLON."
2808,"Shows estimated parameters for univariate data sample. The user can interactively choose the parameter that is estimated (mean, median, standard deviation, interquartile range) and the sample size. Also, variables of the data sets CARS, USCRIME and BOSTONHOUSING are available. The upper panel shows a histogram of the parameter estimates of all previously drawn samples. The lower panel shows a scatterplot of the whole population (green) and the current sample (orange). A box indicates the interquartile range and the mean."
2810,"Provides an interactive interface of the confidence interval length in the cases of both known and unknown standard deviation. The user can choose the confidence level, sample size and standard deviation. Additionally, a line is shown which indicates the sample size needed for a specific confidence interval length."
2812,"Shows either a 2D or 3D scatterplot and computes Pearson's correlation and Spearman's rank correlation for variables of the data sets CARS, DECATHLON and USCRIME."
2814,"Shows the one dimensional dotplot in a choosable type (overplot, jitter, stack). One can add the mean and the median to show the location parameters of one dimensional data as vertical lines. The user can choose between variables of the CARS and USCRIME data sets."
2816,"Shows an interactive interface to show the the rejection area in a test of proportion which uses a normal approximation. The user can interactively choose the test type (two sided, less or greater), the hypothetical proportion, the significance level (in %) and the sample size. Also, a table with summary statistics regarding the sample, population and the test is given. The sample can be drawn for variables of the data sets BOSTON HOUSING and CREDIT."
2818,Produces an interactive interface to show the probability mass function (PDF) of the binomial distribution. The default settings produce a histogram of the binomial distribution with parameters n = 10 and p = 0.5. The user can interactively choose a different number of draws (n) and a different probability of success per draw (p). The user can also choose to show the CDF of the binomial distribution with a step function. The user can choose two other distributions: the hyper-geometric distribution and the Poisson distribution.
2820,"Shows an interactive interface to test for the equality of two means. The user can select the significance level alpha and the sample size for variable 1 and variable 2, the data set and the respective variables. The upper panel shows the theoretical t-distribution and respective rejection areas depending on whether we assume equal or unequal variances of the variables. The lower panel shows scatter plots of the population and the sample for both variables including boxed indicating the interquartile range and the mean. The samples can be drawn from ALLBUS2012-GENERAL, ALLBUS2002-GENERAL, and ALLBUS2004-GENERAL."
2822,"Shows either a 2D or 3D scatterplot. One has also the option to compute Pearson's correlation and Spearman's rank correlation for variables of the data sets CARS, DECATHLON and USCRIME."
2824,"Gives an interactive interface including options of sample size, hypothetical mean and significance level for user to test the mean value of a sample drawing from a chosen data set."
2826,"Shows the conditional probability that the dice is either fair or loaded given the number of rolled sixes (X) in the upper panel. The lower panel shows a bar plot of the conditional probability to roll a specific number of sixes given a fair/loaded dice. The user can interactively choose (1) the number of rolls, (2) the number of rolled sixes for the upper panel and (3) the probability to roll a six with the loaded dice."
2828,"Shows an interactive interface to show the coverage of confidence intervals for proportions. Samples are drawn from the whole population and the mean with corresponding confidence interval is estimated. The user can check whether the estimated confidence interval covers the population mean (green dashed line). One can adjust the confidence level (1-alpha) and the sample size (n). The upper panel shows the previously estimated confidence intervals and the lower panel shows the distribution of the whole population and sample. The user can choose variables from the data sets CREDIT, BOSTONHOUSING and TITANIC."
2830,Produces an interactive interface to show the probability mass function (PDF) of the Poisson distribution. The default settings produce a histogram of the Poisson distribution with parameters Lambda = 2. The user can interactively choose a different number of parameter (Lambda). The user can also choose to show the CDF of the Poisson distribution with a step function. The user can choose two other distributions: the binomial distribution and the hypergeometric distribution.
2832,"Computes a contingency table. Additionally, four different correlation measures can be given: Chi-square, contingency  correlation, corrected contingency correlation and Cramer's V. The user can interactively choose between variables of the data sets TITANIC and HAIR.EYE.COLOR."
2835,Shows the pdf and cdf of the exponential distribution. The parameter lambda can be chosen by the user. One can also interactively plot the normal distribution with exchangeable mean and variance.
2837,Shows a plot of relative frequency of successful trials under two scenarios of success probabilities of 1/3 and 2/3. The user can choose an option of guest points and a type of guest's decisions.
2839,"Produces an interactive interface to show the probability mass function (PDF) of the hypergeometric distribution. The default settings produce a histogram of the hypergeometric distribution with parameters n = 8, N = 20 and M = 6. The user can interactively choose a different number of draws (n), a different number of population size (N) and a different number of success states in the population (M). The user can also choose to show the CDF of the hypergeometric distribution with a step function. The user can choose two other distributions: the binomial distribution and the Poisson distribution."
2841,"Shows the one dimensional dotplot in a choosable type (overplot, jitter, stack). One can add the mean and the median to show the location parameters of one dimensional data as vertical lines. The measures range and interquartile range (IQR) can be shown. The user can choose between variables of the CARS and USCRIME data sets."
2843,"Shows a two dimensional contingency table. In the table, the correlation measures Spearman's rho and Kendall's tau can be shown. The user can interactively choose the row and column variables."
2845,"Plots linear correlation between 42 score values of Handelsblatt (HB), RePEc (RP) and Google Scholar (GS) rankings in an upper triangular matrix. The values are clustered."
2846,Produces scatter and hexagon plots between the number of citations listed in RePEc (RP) and Google Scholar (GS) rankings
2847,"Creates boxplots of the selected VWL and BWL Handelsblatt (HB) rankings (Lifework (LW), Current research (CR), Under 40 (U40))"
2849,"Produces the mosaic plot of Top-700 scientists of Handelsblatt (HB), RePEc (RP) and Google Scholar (GS) each one"
2850,Creates a parallel coordinates plot for GS citation for the period from 2008 till 2015 with quartiles
2851,"Creates a quantile regression of HB VWL 2015 score and HB BWL 2014 score, furthermore a QQ-plot of BWL LW against a theoretical distribution of choice"
2852,Produces scatter and hexagon plots between the h-index score listed in RePEc (RP) and Google Scholar (GS) rankings
2853,"Creates a parallel coordinates plot for the main scores of HB, RP and GS"
2854,"Produces mosaic plot and histograms for the count of researchers  JEL codes (a subject field classification) over the different rankings Handelsblatt (HB), RePEc (RP) and Google Scholar (GS)"
2855,"Plots contour plots of the 2 dimensional kernel density estimates of the main scores (HB, GS and RP) versus each other."
2856,"Creates scatterplots of the main scores of Handelsblatt (HB), RePEc (RP) and Google Scholar (GS) rankings"
2857,Creates boxplots for Google Scholar (GS) citations for the period from 2008 till 2015
2858,"Produces the mosaic plot of number of researchers, when merging of Handelsblatt (HB), RePEc (RP) and Google Scholar (GS) rankings takes place"
2859,"Creates boxplots of the main scores of Handelsblatt (HB), RePEc (RP) and Google Scholar (GS) rankings over the age intervalls: <35, 36-40, 41-45, 46-50, 51-55, 56-60, 61-65, 66-70, 71>"
2861,"Produces hexagon plots between age and top ranking scores of Handelsblatt (HB), RePEc (RP) and Google Scholar (GS) rankings"
2862,"Plots histograms of Handelsblatt (HB) common score, RePEc (RP) average score and Google Scholar (GS) citations"
2863,"Plots a 3 dimensional kernel density estimate of the main scores (of HB, GS and RP)."
2864,"Creates parallel coordinates plots of of the selected VWL and BWL Handelsblatt (HB) rankings (Lifework (LW), Current research (CR), Under 40 (U40))"
2865,"Creates scatterplots of the main scores of Handelsblatt (HB), RePEc (RP) and Google Scholar (GS) against the researchers age"
2866,Creates a parallel coordinates plot of the ranks of all scores of RePEc
2867,Generates a figure corresponding to the 12th iteration of the Sierpinski triangle
2869,Generates figures corresponding to the first six iterations of the Sierpinski triangle
2870,Generates two figures of the Mandelbrot set at different scales
2872,Generates a plot of daily BTC returns from 2012 to 2023
2873,Generates plots of ACF of BTC absolute returns and squared returns
2874,Produces ARIMA estimation results using CRIX data.
2875,"Produces volatility clustering plot of Crix return, ACF and PACF plots of squared residuals derived from ARIMA model."
2876,Produces ARCH estimation results using ARIMA model residuals.
2877,Produces student t-GARCH estimation results using ARIMA model residuals.
2879,Produces econometric analysis results using CRIX data.
2880,Produces GARCH estimation results using ARIMA model residuals.
2881,Produces DCC-GARCH estimation results using ARIMA model residuals.
2882,Provides backtesting results for Value-at-Risk with Block Maxima Model. Corresponds to exercise 16.9 in SFS.
2884,"Shows the rate of convergence to infinity for the stable distributed random variables is higher than for standard normal variables. Plots the convergence rate of maximum for n random variables with a standard normal cdf and with a 1:1-stable cdf. Refers to exercise 16.2 in SFS. Requires the quantlet ""stabrnd.m"" for MatLab."
2887,"Fits a Generalized Extreme Value Distribution to the negative log-returns of a portfolio (Bayer, BMW, Siemens) for the time period from 1992-01-01 to 2006-09-21 and produces a QQ-plot and a PP-plot. Corresponds to exercise 16.5 in SFS."
2889,"Estimates the parameters of a Generalized Pareto Distribution for the negative log-returns of a portfolio (Bayer, BMW, Siemens) for the time period from 1992-01-01 to 2006-09-21 and produces a QQ-plot and PP-plot. Corresponds to exercise 16.5 in SFS."
2891,Simulation of 500 random normal (left) and 1.5-stable (right) normal variables with 25% and 75% quantiles (black lines and 2.5% and 97.5% quantiles (red lines) of the distributions. Refers to exercise 16.1 in SFS. SFSheavytail depends on the function stabrnd for MatLab.
2894,Contains a MatLab function which is needed for the Qs SFSheavytail and SFSmsr1.
2895,Simulates a random stock price movement in discrete time with delta t = 1 day (upper panel) and 1 hour (lower panel) respectively for a period of one year.
2897,"Plots the empirical mean excess function, the mean excess function of generalized Pareto distribution, the mean excess function of Pareto distribution with parameter estimated with Hill estimator for the negative log-returns of portfolio (Bayer, BMW, Siemens), time period: from 1992-01-01 to 2006-09-21. Refers to exercise 16.8 in SFS."
2899,Provides backtesting results for Value at Risk computed with Peaks Over Treshold model with generalized Pareto distribution. Corresponds to exercise 16.9 in SFS.
2901,Plots the theoretical and empirical mean excess function e(u) of the Frechet distribution with alpha = 2.
2903,"Black Scholes formula for vector inputs, MatLab function needed for SFSstoploss."
2904,This repository contains python scripts to scrape information from LinkedIn and analyze talent distributions by gender and locations.
2905,"Crawl user-published reviews on Yelp and apply text mining and machine learing techniques using Python. Web Scraping and Word Cloud, Sentiment Analysis using Textblob, Sentiment Classification using SVM"
2908,"This repository contains code for Python Webscraping, Sentiment Analysis, Topic Models (Latent Dirichlet Allocation) and the generation of Word Clouds. Analysis is performed on several speeches, namely ""What to the Slave is the Fourth of July"" by Frederick Douglass and multiple speeches by Donald J. Trump."
2915,Extract sentiment from Bitcoin related news and use sentiment to forecast price
2924,These program scrapes data from sueddeutsche.de and faz.net and trains with these data a decision tree and a logistic regresion.
2925,"['Demonstrate Linear Opearation and Fourier Transform Using Nympy', ""Draw plot normal distributed random variables and sample's PDF and CDF using Matplotlib"", 'Demonstrate different kernel density estimations', 'Use Black-Scholes Model to price options', 'Simulate implied volatility and plot volatility smile']"
2932,Introduce the conditional execution and how to iterate by using
2933,"['Demonstrate decision tree', 'Demonstrate applied SVM on NLP sentiment classification']"
2935,"Introduce importing packages, reading and writing files, using pandas to read and write structured data."
2936,"Introduce basic syntax, like numeric and string, and basic data structure, like list, tuple, set and dict in Python"
2938,"['Demonstrate getting data from webpage API and scraping news information from nasdaq.com', 'Introducing functional programming and object-oriented programming']"
2950,"Empirical results for crypto data. Clustering based on risk measuring variables, PCA or Factor Analysis. Select best crypto from each cluster and calculate Portfolio Allocation"
2951,"Simulation Study: Simulate Time Series, calculate 12 risk measuring variables and use them as input for PCA/Factor Analysis. Cluster based on Principal Components/Factors and calculate the Adjusted Rand Index to see whether the method works."
2952,"Empirical results for stock data. Clustering based on risk measuring variables, PCA or Factor Analysis. Select best asset from each cluster and calculate Portfolio Allocation"
2953,"Plots four probability density functions (top) and four tails (bottom) in comparison of the NIG, the Laplace, the Cauchy and the Gauss distribution"
2959,"Simulation of Euler discretized hedge models including the Geometric Brownian Motion (Black-Scholes), Merton Jump Diffusion and the dynamics under Heston's Stochastic Volatility Model."
2960,"Descriptive Statistics including Histograms, Boxplots, QQnorm plots, Stem, Scatterplots, Shapiro Wilk Test and a Linear Regression Analysis of eight variables of the US and the EU, including the currency exchange rate, GDP data of the US and the EU, import and export between the EU and the US,price of crude oil, gold and silver."
2961,Animation of the USD/EUR currency exchange rate along the import/export between the US and the EU
2962,The quantlet produces an animation of moving USD dollar and euro Signs in front of  a world map
2963,reads in a textfile and prepares the dataframe.
2964,produces Choreopleth Map of the European countries and the US according to GDP data for the years 2000-2014.
2965,Please refer to the original library RobustM at https://github.com/QuantLet/RobustM. This repository is just a version with main script in python. Also read the README2.md file to see how to use the code.
2966,"Training a GAN on dataset of 1000 samples of a Gamma distribution. Input of the generator is a std. normal distribution, the GAN is trained for 4000 epochs. Every 100 epochs histograms of generated and training data are plotted."
2968,Time series simulation and analysis with GAN. Visualisation with recurrence plots.
2975,MSRlog_returns plots the scatterplot of the log-returns of DEM/USD and GBP/USD from 01.12.1979 to 01.04.1994.
2977,Provides a scatterplot of daily standardized log-returns of BMW versus Volkswagen using data from 1999 to 2006.
2979,Computes Value-at-Risk with Clayton copula model and plots the time-varying parameters over time.
2980,Plots log-returns of DEM/USD and GBP/USD from 01.12.1979 to 01.04.1994.
2982,"Computes and displays extreme value distributions: Frechet, Gumbel and Weibull."
2984,Plots the tail dependence coefficient of t-Student copula as the function of correlation and the number of degrees of freedom.
2986,"Plots the standardized log-returns of Bayer, BMW, Siemens and Volkswagen. Uses data from 1999 to 2006."
2988,Generates generalized Pareto densities for different shape parameters and plots the resulting densities.
2990,Provides a scatterplot of daily standardized log-returns of BMW versus Volkswagen.
2992,Produces a plot of confidence intervals and confidence bands for dataset of measurements of spinal bone mineral density at 99% level.
2993,"Plots tail-dependence coefficient lambda versus regular variation index alpha for correlation coefficients rho = 0.5, 0.3, 0.1."
2994,Computes Value-at-Risk with Gumbel copula model for financial data from Siemens and Bayer (1999 - 2006).
2995,Produces simple plot of the Frank copula density with parameter p = 2.
2997,Generates numbers coming from a uniform distribution. The function uses Fibonacci method.
2999,"Generates CDFs for normal distribution of a pseudo random variable with extreme value ( Gumbel, Frechet and Weibull) and a randomnormal distributed variable."
3001,"MSRvar_clayton_GARCHn Computes Value-at-Risk with Clayton copula model when margins are modelled with GARCH(1,1) process."
3002,"Calculates and plots the daily log-returns of the portfolio from 1992 to 2006 and the estimated density and log-density, respectively (nonparametric), kernel density estimator, gaussian kernel, bandwidth given by Silverman's rule-of-thumb."
3004,"Provides parameters estimated for calculating Value-at-Risk with Block Maxima Model. Insert b- location parameter, k- shape parameter, a- scale parameter"
3006,Computes number of degrees of freedom and correlation parameter rho for t-Student copula.
3007,"Plots the function 2 * P{X_1 > F^(-1)_1(v) | X_2 = F^(-1)_2(v)} for a bivariate Student t distribution with correlation coeffcients -0.8, -0.6, . . . , 0.6, 0.8."
3009,Estimates tail dependence coefficient for two stocks when the joint distribution is modeled with bivariate t-Student distribution. 
3010,"MSRtailHill calculates value of Hill estimator for the negative log-returns of portfolio (Bayer, BMW, Siemens), time period: from 1992-01-01 to 2006-09-21."
3012,Provides parameters estimated for calculating Value-at-Risk with Peaks Over Treshold model for financial data.
3014,Produces simple plot of the Clayton copula density with parameter p = 2.
3016,Computes Value-at-Risk with Frank copula model for real financial data from Bayer and Siemens (1999 -2006).
3017,"Estimates the parameters of Generalized Pareto Distribution for the negative log-returns of portfolio (Bayer, BMW, Siemens), time period: from 1992-01-01 to 2006-09-21, and produces QQ-plot and PP-plot."
3019,Produces a simple plot of the Gumbel copula density with parameter p = 2.
3021,Plots daily log-returns of the BMW stock from 1992-01-01 to 2006-12-29.
3023,Provides a scatterplot of daily standardized log-returns of Siemens versus Volkswagen.
3025,Produces a PP and a QQ Plot of the Daily Return of the Portfolio.
3027,Computes Value-at-Risk with Peaks Over Treshold model with generalized Pareto distribution for data from Siemens and BMW (1999 - 2006).
3028,Computes Value-at-Risk with t-Student copula model for Bayer and Siemens data (1999 - 2006).
3030,"Plots the closing prices of Bayer (black), BMW (red), Siemens (blue) and Volkswagen (green)."
3032,Plots the t-Student copula density with parameter r = 0.4 and DoF = 3.
3034,"Plots daily log-returns of the equally weighted portfolio of Bayer, BMW and Siemens from 1992-01-01 to 2006-12-29."
3035,Plots daily log-returns of the Siemens stock from 1992-01-01 to 2006-12-29.
3037,Plots daily log-returns of the Bayer stock from 1992-01-01 to 2006-12-29.
3038,"Calculates upper tail dependence for various thresholds k for any combination of BMW, Siemens, VW and Bayer based on data from 1999 to 2006."
3040,"Plots the function 2 * P{X_1 > F^(-1)_1(v) | X_2 = F^(-1)_2(v)} for a bivariate normal distribution with correlation coeffcients -0.8, -0.6, . . . , 0.6, 0.8."
3042,"Plots the three copula functions: maximum (M(x,y)), minimum (W(x,y)) and product (P(x,y))"
3044,Produces a PP plot of the pseudo random variables with Gumbel distribution against theoretical Gumbel distribution.
3046,Provides a nonparametric estimation of upper tail dependence coefficient by heuristic plateau-finding algorithm.
3047,"Plots the right tail of the logarithmic empirical distribution of the portfolio (Bayer, BMW, Siemens) returns from 1992-01-01 to 2006-09-21."
3048,"Computes Value-at-Risk with Block Maxima Model for time-series of closing prices from Bayer, BMW and Siemens (1999 - 2006)."
3049,"Reads data from Human Mortality Database and calculate kt, and will be called in MuPoMo_main_twopop and MuPoMo_main_multipop."
3050,"Estimates and forecasts mortality rates based on a semi-parametric approach, which applies parametric modelling for multiple nonparametric smoothed curves with the shape-related nonlinear variation."
3051,"Estimates and forecasts mortality rates based on a semi-parametric approach, which applies parametric modelling for multiple nonparametric curves with the shape-related nonlinear variation. Uses random two countries to demonstrate two-country case."
3052,"Regenerates reference curve (common trend) based on normalized optimal theta parameters and smoothed original kt, and will be called in MuPoMo_main_multipop."
3053,"Normalizes optimal shape variation parameters theta estimated from MuPoMo_optimization, and will be called in MuPoMo_main_multipop."
3054,"Optimizes shape variation parameters theta based on kt and reference curve to update new kt, and will be called in MuPoMo_main_twopop and MuPoMo_main_multipop."
3055,"Forecasts mortality rates based on a semi-parametric approach, and bootstraps time series with innovation to compute the prediction intervals. Uses random two countries to demonstrate."
3056,"Computes an example of forecast for electricity spot prices with the (m)SCARX model. (m)SCARX calculates a day-ahead prediction using decoposition of the raw electricity prices into a seasonal component and remaining stochastic residuals. Based on J. Nowotarski, R. Weron (2016) On the importance of the long-term seasonal component in day-ahead electricity price forecasting, Energy Economics 57, 228-235 (doi: 10.1016/j.eneco.2016.05.009)."
3059,"Calculates a series of day ahead interval forecasts of electricity spot price. The QRA method uses a set of individual point forecasts and the output is a pair of quantile forecasts. Based on: J. Nowotarski, R. Weron (2015) Computing electricity spot price prediction intervals using quantile regression and forecast averaging, Computational Statistics 30(3), 791-803 (doi: 10.1007/s00180-014-0523-0). "
3061,"Computes a day-ahead probabilistic forecast (99 quantiles) of electricity spot price for the last task of the price track in the Global Energy Forecasting Competition 2014. Based on K. Maciejowska, J. Nowotarski (2016) A hybrid model for GEFCom2014 probabilistic electricity price forecasting. International Journal of Forecasting 32(3), 1051-1056."
3063,"Computes an example of forecast combination for electricity spot prices. Uses 4 methods: simple average, OLS averaging restricted regression and IRMSE averaging. Based on J. Nowotarski, E. Raviv, S. Trueck, R. Weron (2014) An empirical comparison of alternate schemes for combining electricity spot price forecasts, Energy Economics 46, 395-412 (doi: 10.1016/j.eneco.2014.07.014)."
3067,"Generate the network structure, simulate the reponses, and fit the NAR model."
3069,generates the averaged lambda series by using 100 largest financial firms stock returns and 6 macro variables.
3072,"plot FRM with 100 firms and with 200 firms, show the correlation of them"
3073,"plot FRM and lambda of one firm, i.e. the lambda series of Wells Fargo (WFC)"
3074,Quantile-Simulation evaluates the influence of the sample size on the estimated quantile under different parametric assumptions
3075,plot the percentage of market capitalization of 700 firms
3076,"compare the FRM and Google Trends (GT) by using correlation analysis and causality analysis, plot the FRM and GT series, plot the autocorrelation function of residuals"
3077,"compare the FRM and SRISK by using correlation analysis and causality analysis, plot the FRM and SRISK series"
3078,"compare the FRM and VIX by using correlation analysis and causality analysis, plot the FRM and VIX series, plot the autocorrelation function of residuals"
3079,"download 106 variables from the internet automatically. 100 largest financial firms stock returns from Yahoo Finance, 3 macro variables (VIX, S&P 500 and iShares Dow Jones US Real Estate) from Yahoo Finance, another 3 macro variables (3 month treasury change, slope of yield curve and credit spread) from Federal Reserve Board."
3080,parallel compute FRM series for 100 firms
3083,"plot 100 firms based FRM with window size 63 and window size 126, plot the cross correlation of them"
3084,"‚Computes the arfima(p,d,q) time series and plots two examples with different n and d‘"
3085,Calculates the explained sample variance and the cumulative variance using principal components (in percentages).
3087,"Shows the time plot of the Value at Risk (VaR) forecasts and exceedances based on Rectangular Moving Average (RMA), Exponentially Moving Average (EMA) and Delta-Normal model."
3090,Plots the Zomma of a call/put option. Zomma is divided by 100 to reflect a one-percentage point change in volatility.
3092,Computes European-style lookback call option prices (lookback option) using a binomial tree for assets without dividends.
3093,Estimates the Hill-quantile value given a Hill-estimation of gamma.
3095,"Plots two pseudo random variables with uniform distribution and N(0,1) distribution with dependence structure given by Clayton copula, theta=0.79."
3097,"Fits a Generalized Extreme Value distribution to the negative log-returns of a portfolio (Bayer, BMW, Siemens, VW) for the time period from 2000-01-01 to 2012-12-31 and produces a P-P plot."
3098,"Simulates and plots a path of a Ornstein-Uhlenbeck (OU) process with beta=0.1, gamma=0.01 around mean=1."
3099,Simulates a Wiener process.
3100,Plots the conditional log-likelihood function of a generated ARCH(1) process.
3101,Computes numerical approximation to a normal cumulative distribution function (cdf) using method a).
3105,Computes European/American option prices using a binomial tree for assets with dividends as a percentage of the stock price amount.
3108,"Generates and plots generalized standard Pareto densities for different shape parameters: gamma=0.5 (dash-dot line), gamma=0 (solid line), gamma=-0.5 (broken line)."
3109,Hill estimator for the GP1 model.
3110,Shows a 3-month U.S. Treasury bill daily yield from 1998 to 2008 as an approximation of the short rate.
3111,"Computes European Call-on-a-Call option prices (compound option, Option on Option) using a binomial tree for assets without dividends."
3112,"Plots the parameters estimated for calculating Value-at-Risk with Peaks Over Treshold model. These parameters were estimated with a moving window of size 250 for the portfolio composed by Bayer, BMW, siemens and Volkswagen."
3114,Plots the Vomma of a call option (Vomma or Volga or DvegaDvol) as a function of the time to maturity and the asset price. Vomma is divided by 10000 to reflect a one-percentage point move in volatility.
3115,Estimates a GPLM for the Credit data using profile likelihood.
3116,Shows the time plot of the exceedances at the 80% significance level from the Value at Risk (VaR) forecasts for Rectangular Moving Average (RMA) and Exponentially Moving Average (EMA) models.
3118,"Simulates and plots a path of a geometric Brownian motion (GBM) using 4 different methods: direct integration, Euler scheme, Milstein scheme, second order Milstein scheme."
3119,Plots the Vega of a call option as a function of the time to maturity and the asset price.
3120,"Fits a Generalized Extreme Value distribution to the negative log-returns of a portfolio (Bayer, BMW, Siemens, VW) for the time period from 2000-01-01 to 2012-12-31 and produces a Q-Q plot."
3121,Claculates expected loss of the equity tranche using the one factor Gaussian model with one-year default probability computed from the iTraxx index Series 8 with 5 years maturity on 20071022. Package pracma for numerical integration is required.
3124,Shows the time plot of the Value at Risk (VaR) forecasts and the associated changes of the Profit & Loss (P&L) of the portfolio for Rectangular Moving Average (RMA) and Exponentially Moving Average (EMA) models.
3126,Simulation of discrete observations of a Geometric Brownian Motion (GBM) via direct integration (method=1) or Euler scheme (method=2). The process follows the stochastic differential equation: dX(t) = mu X(t) dt + sigma X(t) dW(t).
3128,"Plots the Black implied volatility structure (blue) of LIBOR caplets observed on 2002-08-13 and the calibrated volatility structure (red) with parameters a, b, c, d."
3130,"tests back VaR estimate results ""var"" from block_max routine in a observation window h (e.g 250) for the portfolio ""x"" and gives VaR estimates and outliers."
3131,"According to the VaR methodology the profit and loss (P&L) is plotted against the time. The dots represent the empirical P&L stated by the data. The four curves under the dots correspond to the lower 5% (yellow), 1% (green), 0.5% (blue) and 0.1% (dark yellow) quantiles of the a estimated Gumbel-copula."
3132,"Reads the date, DAX index values, stock prices of 20 largest companies at Frankfurt Stock Exchange (FSE), FTSE 100 index values and stock prices of 20 largest companies at London Stock Exchange (LSE) and computes the Least Square (LS) and the Hill estimators of the tail index for all 42 analysed return processes."
3134,"Plots the Ultima of a call option (Ultima or DvommaDvol) as a function of the time to maturity and the asset price. Ultima is divided by 1,000,000 to reflect a one-percentage point change in volatility."
3135,"Computes and plots the kurtosis function of a GARCH(1,1) (generalised autoregressive conditional heteroscedasticity) process for different parameters."
3137,Produces 3d plot of hyperplanes of pseudo-random uniform numbers for given values of sample size and seed
3139,Computes a local linear estimate of the News Impact Curve for the DAX return. The model is y(t)=f(y(t-1)).
3143,"Plots the likelihood function of a GARCH(1,1) process."
3144,"SFEtailGPareto_pp estimates the parameters of Generalized Pareto Distribution for the negative log-returns of portfolio (Bayer, BMW, Siemens)"
3146,"Plots density, distribution, density contour and distribution contour of a Gaussian (normal) copula with 2 and 3 dimensions and parameter=0.5."
3147,"Illustrates the (univariate) Central Limit Theorem (CLT). n*1000 sets of n-dimensional Bernoulli samples are generated and used to approximate the distribution of t = sqrt(n)*(mean(x)-mu)/sigma -> N(0,1). The estimated density (red) of t is shown together with the standard normal (green)."
3149,"Calculates the price of a European call option with the help of the ""Put-Call-Parity""."
3150,Displays scatterplots from GARCH residuals (upper triangular) and from residuals mapped on unit square by the cumulative distribution function (lower triangular).
3151,Produces scatterplots of Monte Carlo sample (5000 realisations) of pseudo random variable with uniform (top) and marginal distributions (bottom). Dependence structure is given by t-copula with df=3 and parameter=0.5.
3152,"Plots the Black-Scholes price as a function of S_t, which is modelled as a geometric Brownian motion."
3154,Generates uniform random numbers using Fibonacci Algorithm and produces a plot of generated numbers.
3155,Plots the time series of daily EUR/USD FX rates and returns with a window from 2002-01-01 to 2009-01-01.
3156,Compares the logarithmic density of generated binomial processes with the logarithmic density of normally distributed random variables.
3157,"Plots (joint) pdf contours of a standard normal and a t3 distributed variable with normal, Clayton, Frank and Gumbel Copula."
3158,Calculates the covariance matrix of the first differences of the VDAX data.
3160,"Simulates a Cox-Ingersol-Ross process and plots it. The number of observations to be simulated as well as the parameters of the CIR model can be set by the user. The initial value is assumed as the mean, but it can be set by the user as well."
3162,"Simulates signal S_t = 2 * cos {2 * pi * (t / 50) + 0.6 * pi}, with epsilon_1 ~ N(0,1) and epsilon_2 ~ N(0,25) and plots the results"
3163,"Produces a PP and a QQ Plot of the portfolio of Bayer, BMW, Siemens and Volkswagen for the period 1 January 2002 - 31 December 2012, on daily basis."
3164,"Computes the Black-Scholes price of a European call option using different approximations of the normal distribution. Optionally, option parameters may be given interactively as user input."
3166,"Generates uniform random numbers using RANDU generator and produces a 3d plot of generated numbers where the pairs from the hyperplains can be visible. Shows that the points generated by the algorithm are lying on the straight lines with c = 1 and c = 0, whose points form a lattice."
3167,Shows zero-coupon U.S Treasury bond yields with maturities from 1 month up to 10 years observed on 30 March 2010.
3168,Computes numerical approximation to a normal cumulative distribution function (cdf) using method b).
3170,Calculates and plots the factor loadings of the first and second principal components of implied DAX volatilities at the money.
3172,Log-likelihood function used in SFEVasiml.
3173,Plots the autocorrelation function of an AR(1) (autoregressive) process.
3176,"Reads the date, DAX index values, stock prices of 20 largest companies at Frankfurt Stock Exchange (FSE), FTSE 100 index values and stock prices of 20 largest companies at London Stock Exchange (LSE) and estimates the volatility of the DAX and FTSE 100 daily return processes from 1998 to 2007."
3181,"Computes and displays extreme value distributions using the pdf (probability density functions) with adjusted parameter alpha: Frechet, Gumbel and Weibull."
3183,Plots the performances of an insured and of a non-insured portfolio as a function of the stock price. In the second part the table of portfolio values and returns for different stock prices is given.
3184,Plots the Theta of a call option as a function of the time to maturity and the asset price.
3185,Generates pseudo-random uniform numbers and shows hyperplane pairs of the generate numbers
3187,Plots and compares the density of generated binomial processes with the density of normally distributed random variables.
3189,Provides Value at Risk estimates computed with Block Maxima Model with generalized extreme value.
3190,"Simulates 2 stock price paths, including the Black-Scholes Delta depending on n=50 time steps and the stock price S1/S2 for given exercise price K=100, initial stock price S0=98, volatility sig=0.2, interest rate r=0.05, and the time interval (T-t0) = 20 weeks. In the second part it plots delta vs stock price for different time to maturities."
3191,"Plots the empirical mean excess function, the mean excess function of generalized Pareto distribution, the mean excess function of Pareto distribution with parameter estimated with Hill estimator for the negative log-returns of portfolio (Bayer, BMW, Siemens, VW) for the time period from 2000-01-01 to 2012-12-31."
3193,"Plots the Value-at-Risk estimation at 0.05 level for the portfolio composed by Bayer, BMW, siemens and Volkswagen. The Value at Risk is computed by means of the Block Maxima Model."
3195,Produces a PP and a QQ plot of the daily return of the optimal (variance efficient) portfolio
3197,"Produces descriptive statistics for daily log returns of FTSE 100 and DAX stocks: Bayer, Siemens and Volkswagen, for the period 3 January 2000 - 30 October 2006."
3198,Plots the time series of a Gaussian white noise.
3200,Visualizes the reliability of the Value at Risk (VaR) forecasts based on Rectangular Moving Average (RMA) and Exponentially Moving Average (EMA) models.
3202,"Plots the closing stock prices of 4 DAX companies: Bayer (black), BMW (red), Siemens (blue) and Volkswagen (green) for the period 1 January 2002 - 31 December 2012, on daily basis."
3203,"Reads the date, DAX index values, stock prices of 20 largest companies at Frankfurt Stock Exchange (FSE), FTSE 100 index values and stock prices of 20 largest companies at London Stock Exchange (LSE) and plots the time series of the DAX and FTSE 100 daily returns from 1998 to 2007."
3205,Computes the spectrum (frequency domain representation) of a white noise process
3206,"Generates and plots 5 paths of a Wiener process with c=1, delta_t=0.5."
3208,Plots the autocorrelation function of a MA(2) (moving average) process for different parameters.
3211,Estimates parameters of a multivariate GARCH model for the daily log-returns of DAX and FTSE100 from 10.05.2004 to 07.05.2014. It also plots the estimated variance and covariance processes.
3212,Plots a simulated price process (St) as a solid line and the maximum process (Mt) as an increasing dotted line.
3214,Computes Asian call option prices (asian option) using a binomial tree for assets without dividends.
3216,Plots the time series of daily DAFOX returns with a window from 1993 to 1996.
3217,"Computes plots showing the decomposition of a time series into sinusoids with frequencies f={0.4, 0.1, 0.04}."
3218,Plots the error surface of Q(w) with sigmoid function and threshold function as activation function.
3220,Plots the Volga of a call option as a function of the time to maturity and the asset price.
3221,SFEVasiml shows the estimation of Vasicek model using 2600 observations of the yields of the US 3 month treasury bill from 19980102 to 20080522.
3224,"Reads the date, DAX index values, stock prices of 20 largest companies at Frankfurt Stock Exchange (FSE), FTSE 100 index values and stock prices of 20 largest companies at London Stock Exchange (LSE) and estimates the volatility of the DAX and FTSE 100 daily return processes from 1998 to 2007 using a GARCH(1,1) model."
3226,Generates uniform random numbers using RANDU generator and produces a 3d plot of generated numbers where the hyperplains can be visible.
3227,"SFEtailGPareto_qq estimates the parameters of Generalized Pareto Distribution for the negative log-returns of portfolio (Bayer, BMW, Siemens)"
3229,Plots a simulated Ornstein-Uhlenbeck process using the parameters given by the user
3231,Plots the speed of a call option as a function of the time to maturity and the asset price.
3232,plots the time series from estimated dependence parameter for the copula families Clayton and Gumbel. The empirical residuals are calculated using moving window of size 250
3233,"Reads the date, DAX index values, stock prices of 20 largest companies at Frankfurt Stock Exchange (FSE), FTSE 100 index values and stock prices of 20 largest companies at London Stock Exchange (LSE) and computes descriptive statistics of the DAX and FTSE 100 daily return processes from 1998 to 2007."
3234,Computes European/American option prices using a binomial tree for assets with fixed amount dividends.
3236,Generates and plots five paths of a trinomial process with p = q = 0.25. (2sigma)-intervals around the trend (which is zero) are given as well.
3237,"Plots the parameters estimated for calculating Value-at-Risk with block maximal model. These parameters were estimated with a moving window of size 250 for the portfolio composed by Bayer, BMW, siemens and Volkswagen."
3239,"Plots the Value-at-Risk estimation at 0.05 level for the portfolio composed by Bayer, BMW, siemens and Volkswagen. The Value at Risk is computed by means of the Peaks Over Treshold model with generalized Pareto distribution using a moving window of size 250."
3241,"Plots the time series of a GARCH(1,1) process."
3243,"Computes and plots the variance efficient portfolio of Bayer, BMW, Siemens and Volkswagen log-returns for the period 1 January 2000 - 31 December 2012."
3244,"plots the time series from estimated dependence parameter for the copula families Gaussian, Gumbel and Clayton. The dependence parameter is estimated using the IFM method. The empirical residuals are calculated using moving window of size 250"
3245,Computes the base tranche correlation for a Gaussian credit default model.
3248,Plots the Delta of a call option as a function of the time to maturity and the asset price.
3250,"Plots conditional volatility forecasts for log-returns of FTSE and DAX stocks (Bayer, Siemens and Volkswagen) based on the Fractional Integrated Asymmetric Power ARCH (FIAPARCH, blue) model and the Hyperbolic-GARCH (HYGARCH, red) model."
3252,Simulates 3 stock price path for a given specification of a geometric Brownian motion. For each of these paths the positions and cumulative costs of a Delta hedging strategy are plotted.
3253,Plots the Gamma of a call option as a function of the time to maturity and the asset price.
3254,Computes European/American option prices using a binomial tree for assets with/without continuous dividends.
3257,Simulates instataneous short rates using the Cox-Ingersol-Ross process. It also plots these simulated rates versus time.
3258,Plots the partial autocorrelation function of an AR(2) (autoregressive) process.
3262,Plots the Black implied volatility structure of EUR caplets.
3263,Produces a graphic visualisation of the implied volatility surface of the DAX option. The original options are shown as blue points.
3265,Plots the DvegaDtime of a call/put option (DvegaDtime or Vega bleed). DvegaDtime is divided by 36500 to reflect a one-day move and a one-percentage point change in volatility.
3266,"Computes the first order auto correlation of the returns, squared returns and absolute returns as well as the skewness, kurtosis and Jarque-Bera test statistic for German blue chips, 1974 - 1996."
3269,Provides Value at Risk estimates computed with Peaks Over Treshold model with generalized Pareto distribution.
3270,"Computes and plots the density of the loss-profit function for the variance efficient portfolio of Bayer, BMW, Siemens and Volkswagen log-returns for the period  1 January 2000 - 31 December 2012."
3271,"Computes the Black-Scholes price of a European call option. Optionally, option parameters may be given interactively as user input."
3274,Defines the log-likelihood function of the CIR model used in SFECIRmle.
3276,"Computes the implied compound correlation smile from the one factor Gaussian model using data from 20071022, iTraxx Series 8 with 5 years maturity."
3277,"Computes base compound correlation smile from the one factor Gaussian model using data from 20071022, iTraxx Series 8 with 5 years maturity."
3281,Compares and plots densities of lognormally and normally distributed random variables.
3283,Shows the forecasting of the exchange rate time series JPY/USD using RBF (radial basis function) neural networks considering 3 periods of time dependency. 80% of data is taken as training set and 20% as validation set.
3284,"Shows the normal probability plot (PP Plot) of the pseudo random variables with extreme value distributions: Weibull, Frechet and Gumbel."
3286,Calculates MLE estimates of the parameters of the Calibrating Interest Rate Model using the yields of the US 3 month treasury bill (1998-2008).
3289,Shows the conditional volatilities estimated from the log returns of the exchange rate time series BP/USD together with some financial indicators using RBF (radial basis function) neural networks considering 3 periods of time dependency.
3290,External function for estimating the value at risk(VaR).
3291,"SFEclose plots the closing prices of Bayer (black), BMW (red), Siemens (blue) and Volkswagen (green)."
3293,"Reads the date, DAX index values, stock prices of 20 largest companies at Frankfurt Stock Exchange (FSE), FTSE 100 index values and stock prices of 20 largest companies at London Stock Exchange (LSE), estimates parameters of a multivariate GARCH model and plots the estimated variance and covariance processes for the DAX and FTSE 100 daily return processes from 1998 to 2007."
3294,Computes European chooser option prices using a binomial tree for assets without dividends.
3295,"Shows an intersection of the implied volatility surface, illustrates time to maturity structure of implied DAX volatilities at the money."
3297,Compares CIR and Vasicek models with the real data.
3298,"Computes time series plots for the time domain and the frequency domain, i.e. the periodogram"
3299,Displays the yield curve for given parameters under the model of Cox-Ingersoll-Ross (CIR).
3302,"Creates a grid of perspective plots of the densities of the Frank, Gumbel, AMH, and Clayton copulae. The chosen copula parameters are 8, 1.5, 0.9, and 0.9, respectively."
3303,plots the DAX daily log-returns from 10.05.2004 to 07.05.2014 toghether with the normal density for the log-returns of the last four years of the time series.
3305,"Simulates 2 stock price paths, including the Black-Scholes Delta depending on n=50 time steps and the stock price S1/S2 for given exercise price K=100, initial stock price S0=98, volatility sig=0.2, interest rate r=0.05, and the time interval (T-t0) = 20 weeks. In the second part it plots delta dependencies on steps and stock prices."
3306,Transforms uniformly distributed random variables to sets of independent standard normally distributed random numbers using the Box Muller algorithm.
3308,Plots the autocorrelation function of an MA(1) (moving average) process.
3310,"Calculates and plots BS price as a function of S, K, r, tau, sigma and the payoff at maturity."
3312,Plots a simulated Vasicek process using the parameters given by the user
3314,Plots the autocorrelation function of an AR(2) (autoregressive) process.
3316,"External function, see SFEVaRqqplot."
3317,"Plots distribution and distribution contour of Frechet-Hoeffding upper bound, lower bound and product (independence) copula into a grid."
3318,Plots the fractional Brownian motion with Hurst exponent H1=0.2 (top) and H2=0.8 (bottom).
3319,Computes numerical approximation to a normal cumulative distribution function (cdf) using method d).
3321,"Plots daily log returns of FTSE 100 and DAX stocks (Bayer, Siemens and Volkswagen), for the period 3 January 2000 - 30 October 2006."
3323,"Computes the density function of the loss in a portfolio of bonds or other credit instruments using a one-factor Gaussian copula. The marginal densities of default times are implemented as poisson distributions using constant default intensities. After computing the portfolio loss densities for various values of the dependence parameter rho, these densities are plotted."
3324,Plots the Color of a call option (Color or Gamma bleed or DgammaDtime) as a function of the time to maturity and the asset price. Color is divided by 365 to reflect the effect of a single day passing by.
3326,An interactive example of VaR and CVaR calculation f or different distributional parameters.
3327,Compares a kernel estimation of the density of DAX returns distribution with a kernel estimation of a normal distribution with the same mean and variance. The data is taken from the Xetra-DAX 1999.
3331,Calculates the exercise price applying Newton's method and gets the put option price using the Black Scholes model.
3333,Plots the Vanna of a call option as a function of the time to maturity and the asset price.
3334,Computes the implied tranche correlation by a Gauss model for credit default options.
3336,Applies first differencing and 12-month centered moving average filter to SOI data and plots the results in time and frequency representation
3337,"Plots dependence parameters of Gaussian, Clayton and Gumbel copulae, estimated using the IFM method."
3338,Produces a plot and summary statistics for DAX monthly log-returns from July 2004 to May 2014.
3340,"plots the contour of the Gumbel, Clayton and Normal copula. These copulae are based on Normal margins. The fitted, empirical residuals are included in the plot as red dots."
3341,Calculates a price of a bond using CIR model.
3343,Produces plots and autocorrelation functions of fractional Gaussian noise with 2 different Hurst parameters.
3344,"Reads the date, DAX index values, stock prices of 20 largest companies at Frankfurt Stock Exchange (FSE), FTSE 100 index values and stock prices of 20 largest companies at London Stock Exchange (LSE) and estimates various GARCH models for the DAX and FTSE 100 daily return processes from 1998 to 2007."
3345,"Plots the default probability for KMV model as a function of the state of the economy for 3 different states of the economy: bad (y=-3), typical (y=0) and good(y=3)."
3347,"Simulates AR(1), AR(2), MA(1), SAR(1) processes and computes the theoretical spectral densities."
3348,Computes the lower expected tranche loss.
3349,Plots the partial autocorrelation function of an MA(2) (moving average) process.
3351,Plots the density and distribution function of a binomial distributed random variable with different parameters and calculates probabilities.
3353,Generates and plots 3 paths of a binomial process with p = 0.6. (2sigma)-intervals around the trend are given as well.
3355,Computes numerical approximation to a normal cumulative distribution function (cdf) using method c).
3357,"Minimizes the error function Q(w) using sigmoid function as activation function, following method of descending gradients."
3359,Calculates expected loss of the equity tranche calculated using the one factor Gaussian model with one-year default probability computed from the iTraxx index Series 8 with 5 years maturity on 20071022.
3362,"Plots the situation for a down-and-out option with two possible paths of the asset price. Once the price hits the barrier, the option expires worthless regardless of any further evolution of the price."
3364,"Plots the Kernel density estimator of the residuals and of the normal density for the log-returns of the Deutsche Telekom and Volkswagen from  10.05.2004 to 07.05.2014,"
3365,"Plots density, distribution, density contour and distribution contour of a t-copula with 2 dimensions, df=3 and parameter=0.2."
3366,Plots the Charm of a call option (Charm or Delta bleed). Charm is divided by 365 to reflect the effect of a single day passing by.
3368,Simulates and plots a Vasicek process for the instantaneous short term interest rate.
3369,"Produces plots of the fractional integrated ARMA - ARFIMA(p,d,q) models."
3371,"Generates uniform random numbers using RANDU generator and produces a 2d plot of generated numbers where the pairs from the hyperplains can be visible. Shows that the points generated by the algorithm are lying on 5 straight lines, with c = 0; 1; 2; 3; 4."
3372,"Reads the date, DAX index values, stock prices of 20 largest companies at Frankfurt Stock Exchange (FSE), FTSE 100 index values and stock prices of 20 largest companies at London Stock Exchange (LSE) and plots the right sides of the logged empirical distributions of the DAX and FTSE 100 daily returns from 1998 to 2007 (m=10)."
3373,Black-Scholes price function. It is used for calculation of put and call option prices in the Quantlet SFEbsprices.
3374,None
3378,"Builds 60 mins candles using on 5 minutes price data (Open, High, Low,Close price) of 11 cryptocurrencies and produces plots in pdf format"
3381,plots itraday volatility (absolute 5min returns) of 12 cryptocurrencies
3383,plots HF (5mins) returns of 12 cryptocurenicies
3385,calculates correlation coefficients for 12 CCs based on intraday price data (5min) for negative and positive moves of the market and stores them to latex table.'
3389,Plot benefit of strategic placement of orders according to VFAR predictions over equal-splitting strategy
3390,Plots bid ask curve for AEZS at random time point with LOB data from 02Jan2015 till 06Mar2015
3391,Plot estimated bid ask curve using VFAR approach for AEZS with LOB data from 02Jan2015 till 06Mar2015
3392,"Estimates the sample cross correlation function between log-accumulated volumes at best bid and ask price for AEZS from 2rd Jan,2015 till 6th Mar,2015."
3393,Plots forecasted bid ask curve using VFAR approach for AEZS with LOB data from 02Jan2015 till 06Mar2015
3394,Plots dynamics of forecasted bid ask curve using VFAR approach for AEZS with LOB data from 02Jan2015 till 06Mar2015.
3395,Produces QQ plot for residuals using VFAR approach for AEZS with LOB data from 02Jan2015 till 06Mar2015
3396,Plot MAPE of VFAR versus Random Walk for multiple step forecasts for AEZS
3397,Visualizes k means and EM clustering with Old Faithful and Iris data
3398,"‘Performs the Penalized Adaptive Method (PAM), a combination of propagation-separation approach and a SCAD penalty, to fit a model with possible parameter changes on a given dataset. Compares the fit to models of Cochrane and Piazzesi (2005) and Ludvigson and Ng (2009). The input data are monthly observations of k-year excess bond risk premia, k = 2, 3, 4, 5, forward rates and other pre-defined macro variables. Computes RMSE, MAE, R^2 and adjusted R^2 for the fitted models. Plots the time series of the fitted vs. observed values of bond risk excess premia.'"
3400,Plot suitable for pairwise comparison of time series and their autocorrelation functions
3402,Estimate of order of integration d based on GPH estimator for long memory investigation.
3404,Simulation of the Euler-discretized Stochastic Volatility with Correlated Jumps model and Monte Carlo Option Pricing
3406,"Scrapes Data on Market Capitalization, Price and Volume of all coins listed at CoinGecko.com, using the API provided by CoinGecko"
3407,(S)ARIMA Model for Rough Rice Futures Price from 2011 to 2021
3408,TukeyQuantiles calculates quantiles for g- and h- modified standard normal random variables
3409,TukeyContour produces bivariate parametric contour plots of tukey transformed variables
3410,TukeyPCurve produces a plot of a parametric curve
3411,TukeyQQ plots quantiles based on a simulated sample from Tukey g- and h- transformed variables vs standard normal quantiles. TukeyQQ also plots quantiles based on daily Ether returns
3412,TukeyQuantilePlot produces plots displaying the effect on quantiles after the tukey g- and h- transformation
3413,TukeyEmpirical estimates parameters g and h by matching the skew and the kurtosis of Tukey g- and h- transformations to the empirical sample equivalents
3414,TukeyPDF produces parametric plots of Tukey g- and h-transformed variables
3415,Load crix and vcrix data from a CSV file.
3416,"Extracts year of introduction, capacity and pricing data for hard drives from an online HTML table using basic pattern matching."
3417,"Calculates parameters of log-linear regression between log(HDD price per gigabyte) and time. HC3 standard errors are used. The R^2 value is also provided. Furthermore, the data set is enriched by adding months as new variable."
3418,Extracts year of introduction and transistor count for Intel microprocessors introduced between 1971 and 2008 from the Intel Microprocessor Quick Reference Guide using XPath and pattern matching.
3419,Plots results of regression with price per gigabyte of storage space as dependent variable (CMBhddregp). Observations are plotted as scatter and regression results are represented as blue line. The plot axes are customized.
3420,Calculates parameters of log-linear regression between transistor count and year of introduction. HC3 standard errors are used. The R^2 value is also provided.
3421,Plots results of log-linear regression between transistor count and year. Observations are plotted as scatter and regression results are represented as blue line. The plot axes are customized.
3422,Generates JSON files from a MySQL table to be used by CMBbubbles for visualization using D3.js and CoffeeScript.
3423,"Visualizes a collection of computers clustered by various means using D3.js and CoffeeScript. On hovering brief information of the specific computer appears (including producer, model name and picture). A live example can be found at cm.wiwi.hu-berlin.de/viz. Extended upon original work by Jim Vallandingham."
3424,Load BTC options from a CSV file and plot in multiple colors.
3425,"Example code for Random Forest, extraction of single trees, feature importance, plotting classification maps and comparing random forest."
3428,Predicts the creditability from German Solvency Data with a Tree (CART) and a Random Forest.
3430,"Pricing kernels implicit in option prices play a key role in assessing the risk aversion over equity returns. We deal with nonparametric estimation of the pricing kernel (PK) given by the ratio of the risk-neutral density estimator and the historical density (HD). The former density can be represented as the second derivative w.r.t. the European call option price function, which we estimate by nonparametric regression. HD is estimated nonparametrically too. In this framework, we develop the asymptotic distribution theory of the Empirical Pricing Kernel (EPK) in the L∞ sense. Particularly, to evaluate the overall variation of the pricing kernel, we develop a uniform confidence band of the EPK. Furthermore, as an alternative to the asymptotic approach, we propose a bootstrap confidence band. The developed theory is helpful for testing parametric specifications of pricing kernels and has a direct extension to estimating risk aversion patterns. The established results are assessed and compared in a Monte-Carlo study. As a real application, we test risk aversion over time induced by the EPK."
3432,"Use LSTM to predict stock price.The main steps are: 1. import tensorflow 2. transform data format 3. create training data and testing data 4. use LSTM to fit the model 5. predict 6. plot results (blue: omitted, orange: train set, green: test set, red: prediction) note that the evaluation of the LSTM fit depends on the training interval. For fun we trained from the ""future"" in the 2nd train valley plot."
3433,"This Quantlet provides the code to draw PeppaPig by using the library named turtle. The focus lies on the configuration of the pen size, the drawing speed and the fill of the shape. The project can be used in stimulate interest of children in python. Reference:https://blog.csdn.net/weixin_33701632/article/details/111928743"
3435,Summarize 5 topics from 130 collected abstracts with LDA method
3436,Create a word cloud picture to make a short description of 130 abstracts
3437,Computes and plots the values for two bivariate normally distributed random variables with a specified mean and covariance matrix.
3438,Plots an 8 points example for minimum spanning trees (MST)
3439,"Performs cluster analysis for US health 2005 data. On the transformed data we perform a principal component analysis and a cluster analysis employing Eulcidean distance and Ward linkage algorithm. Plots of principal components and the dendrogram are presented. After extraction of 4 clusters, the principal components with the four clusters (denoted by different colours) are shown. The graphs and clustering differs from R due to different algorithms."
3440,"Employs the Ward algorithm using squared Euclidean distance and single linkage method using simple Euclidean distance and squared Euclidean distance matrices to perform a cluster analysis on an 8 points example. Three plots are generated. First plot schows the 8 points, second plot the dendrogram for squared Euclidean distance and single linkage while the third plot presents the dendrogram for Euclidean distance using single linkage algorithm"
3442,employs the Ward method and complete linkage using squared Euclidean distance matrices to perform a cluster analysis on an 8 points example
3444,"Performs a cluster analysis for the US cereal data from the R-package MASS. On the transformed data will be performed a principal component analysis and a cluster analysis employing Euclidean distance with the Ward linkage algorithm. Plots of principal components and the dendrogram are presented. After extraction of 3 clusters, the principal components with the 3 clusters (denoted by different colours) are shown"
3445,Employs the centroid linkage using squared Euclidean distance matrices to perform a cluster analysis on an 8 points example
3446,Employs the average linkage using squared Euclidean distance matrices to perform a cluster analysis on an 8 points example
3447,Performs a k-means cluster analysis with the standard algorithm (Lloyds) on the 2005 US health data set. Four clusters are extracted and plotted with different colours for the first two principal components.
3449,"Produces plot of Andrews curves from the annual population dataset (annualpopu.rda) to detect homogeneous subgroups and multidimensional outliers. Apparently, there are 2 subgroups, years 1975 to 1979 form a cluster with higher values, years 1970 to 1974 form a group with lower values."
3451,performs a k-means cluster analysis with the standard algorithm (Lloyds) on an 8 points example
3453,Employs the spectral clustering algorithm on an 8 points example
3454,"performes a linear regression and an analysis of variance (ANOVA) for three marketing strategies. A company decides to compare the effect of three marketing strategies: 1. advertisement in local newspaper 2. presence of sales assistant 3. special presentation in shop windows, on the sales of their portfolio in 30 shops. The 30 shops were divided into 3 groups of 10 shops. The sales using the strategies 1, 2, and 3 were y1 = (9, 11, 10, 12, 7, 11, 12, 10, 11, 13) y2 = (10, 15, 11, 15, 15, 13, 7, 15, 13, 10) and y3 = (18, 14, 17, 9, 14, 17, 16, 14, 17, 15) respectively. A null hypothesis of equality of means of the three groups is tested. The standard approach of using ANOVA leads to an F-test. The alternative proposition is to use a linear factor model using the strategies as regression variables to build curve that corresponds to the alternative hypothesis of the ANOVA with three horizontal lines, each for one strategy. The F-test of testing equality of the coefficients to zero corresponds to the null of testing the marketing strategies having no effect. The output shows rejection of the null hypothesis. For more information see Exercise 3.20."
3456,Plots a surface of intradaily electricity load curves over time
3457,Computes generalized quantiles of electicity demand and models their dynamics over time using FPCA and multivariate time-series techniques
3460,Conducts a simulation study of the proposed methodology and computes the MSE
3463,Plots different asymmetric loss functions
3464,Plots the asymmetric quantile/expectile loss functions (for different quantile/expectile levels).
3465,Plots the expectile curves (pointwise estimation by LAWS) under several tau levels with simulated data.
3500,AdaBoost classification model for defaults of P2P loans. Compares the out-of-sample predictive performance to a baseline logistic regression model.
3501,"Alpha shape, convex hull, minimum spanning tree and Delaunay Triangulation of a simulated point cloud and the scatterplot of the CRIX and VCRIX."
3503,Grading of all Quantlets within one GitHub repository with the use of the classes modules/QUANTLET.py and modules/METAFILE.py.
3504,Shiny application of the Visualization of cross tabulation of questionnaire by media layers based on gender and age. The visualization used association rules and the biplot of correspondence analysis.
3509,Visualization of cross tabulation of questionnaire by media layers based on gender and age. The visualization used association rules and the biplot of correspondence analysis.
3512,"RESIDUAL'S INFLUENCE INDEX (RINFIN), BAD LEVERAGE AND UNMASKING IN HIGH DIMENSIONAL L2-REGRESSION"
3513,"Produces the QQ plots for four simulated samples of N(0,1), N(5,1), N(0,9) and N(5,9). QQ-plots compare empirical quantiles of a distribution with theoretical quantiles of the standard normal distribution."
3514,Produces the curves for the binomial distribution function and the pseudoinverse function under the probability=0.4 and the size=5.
3515,Plots and compares the confidence intervals for relative likelihood and relative log-likelihood functions.
3516,Produces the QQ plots for simulated samples of standard normal distribution and exponential distribution with sample size 100. QQ-plots compare empirical quantiles of a distribution with theoretical quantiles of the standard normal distribution .
3517,"Plots the power functions of t test, with fixed u = 2.3 and different significant levels."
3518,"Produces the plots of evaluating denisities of Archimedean copulae under (size=500, size of parameters=3, dimension=2) and (size=500, size of parameters=6, dimension=2)."
3519,"Supervised randomization integrates the targeting model into randomized controlled trials and A/B tests as a stochastic policy. By replacing random targeting with supervised randomization, we reduce the costs of running randomized experiments or allow continuous collection of randomized trial data. We show how to fully correct downstream analysis for the bias effect by the supervised treatment allocation with the true probabilites to receive treatment, which are logged at the time of targeting."
3528,A novel SARSA-IS algorithm to optimize the policy in the presence of rare disasters
3532,A DEIS algorithm (TD learning incorporated with importance sampling) to evaluate the policy in the presence of rare disasters
3536,A conventional SARSA algorithm to optimize the policy in an epsilon-greedy way
3539,"Bitcoin Pricing Kernels are inferred using a novel data set from Deribit, one of the largest Bitcoin derivatives exchanges. The Pricing Kernels improve the understanding of investor sentiment and allow arbitrage-free pricing of various instruments. Bootstrapped confidence bands are estimated in order to validate the results. Investors are heterogeneous in their risk profiles. The empirical Pricing Kernels are U-shaped for short-dated instruments. These are rarely traded at-the-money and are mainly used for speculation. In case of long-dated instruments, empirical Pricing Kernels are generally decreasing, although increasing in the right tail. Their traders are risk averse, yet hedging their risk of being priced out of a dynamic market."
3540,"Trains the neural network. The input values are configured in net_config.json. Additionaly, the target drawdown 'D_target' can be set in nnagent.py in the function loss_function8. To run the code, open the command line and enter first 'python main.py --mode=generate --repeat=1' and then 'python main.py --mode=train --processes=1'. The output is then located in the 'train_package'. The basis for the code was provided by Z. Jiang and can be found here: 'https://github.com/ZhengyaoJiang/PGPortfolio/blob/master' Note that the command 'python main.py --mode=download_data' used to download data from Poloniex, but may not work anymore. A database is provided in the 'database' folder, to allow to replicate the experiments in the Thesis and do other experiments in the time range '01.07.2015 - 31.10.2018'. The database is in the rar format and needs to be extracted before running the code."
3541,Outputs the performance of the algorithm for the test period for different target drawdowns as a plot. Additionally plots performance of (modified) CRIX.
3542,Outputs the leverage for each time period as a plot.
3543,"Draws figures of all monthly coin returns (relative to Bitcoin) for all coins used in the Thesis experiments (from 07/2015 to 10/2018, if available)."
3544,Outputs the drawdown for each time period as a plot.
3545,Outputs a plot that displays the weight (in %) for the 5 least held coins for different target drawdowns in Experiment 1. A higher value means better diversification.
3546,"Outputs descriptive statistics for all coins used in the experiments (prices relative to Bitcoin, from 07/2015 to 10/2018, if available)."
3547,Draws figures of all coin prices used in the Thesis experiments relative to Bitcoin (from 07/2015 to 10/2018).
3549,Outputs the performance of the algorithm for the test period as a plot. Additionally plots performance of (modified) CRIX and the algorithm without leverage.
3550,Simulates futures prices based on an estimated HAR model with an MC simulation and plots volatility structure
3551,Builds a classification model to predict by combining the shrinking methods with the adaptive methods. model can automatically detect the homogenous interval and the active macro-factor set
3560,This package uses Local Polynomial Regression to create a fit to the data. The model conveniently also estimates the first and second derivative of the fit. A Cross Validation finds the optimal bandwdith for the fit in case it is unknown.
3561,Multi-label Topic Modelling With Prior Knowledge using LDA-based models
3571,"Creates and trains four neural networks corresponding to different architectures (Dense, RNN, GRU and LSTM) on Fashion Mnist dataset."
3572,Function for calculating the Cornish Fisher approximation for given quantile levels. Plots the extreme quantiles of the CF approximation vs. quantiles of the normal distribution.
3573,Creates density plots with VaR and Expected Shortfall of Deutsche Bank daily stock returns in a time period from April 2017 - April 2019. Compares the nonparametric kernel density estimate with a parametric maximum likelihood estimate assuming normal distributed returns.
3574,Estimates different copula models. Simulation of meta-distribution and calculation of VaR and ES.
3575,We present a simple approach to forecasting conditional probability distributions of asset returns. We work with a parsimonious specification of ordered binary choice regression that imposes a connection on sign predictability across different quantiles. The model forecasts the future conditional probability distributions of returns quite precisely when using a past indicator and past volatility proxy as predictors. Direct benefits of the model are revealed in an empirical application to 29 most liquid U.S. stocks. The forecast probability distribution is translated to significant economic gains in a simple trading strategy. Our approach can also be useful in many other applications where conditional distribution forecasts are desired.
3576,"shows the interaction of two R packages rgithubQ and yamldebugger. Two GitHub API search examples for Quantlet extraction are given and their YAML meta data are parsed. Some YAML data fields are retrieved by means of the yamldebugger and the results are aggregated as data frames. Additionally, real time QuantNet@GitHub statistics via the ""qnet.stats"" function from the rgithubQ package are generated."
3577,"shows the interaction of three R packages rgithubQ, yamldebugger and TManalyzerQ. Two GitHub API search examples for Quantlet extraction are given and their YAML meta data are parsed. Some sample queries are embedded into three text mining models BVSM, GVSM(TT) and LSA, the desired similarity method from the ""proxy"" package is applied and a comprehensive IR analysis is performed."
3583,Generates the Latex code for tables of regression results in 'Risk Analysis of Cryptos as Alternative Asset Class'.
3589,Classifies if a greyscale picture belongs to a particular person by using k-Nearest Neighbour as classification algorithm.
3590,Classifies if a greyscale picture belongs to a particular person by using Support Vector Machines as classification algorithm.
3591,Use Principle Component Analysis to project the data into subspace to reduce the dimension greatly and get the eigenfaces which are corresponding to the eigenvalues of original dataset.
3592,Classifies if a greyscale picture belongs to a particular person by using Fisher Linear Discrimination Analysis as classification algorithm.
3593,"Analyzes mergers & acquisitions on the German energy market, including approximation with Poisson distributions with time varying lambdas"
3594,Computes densities of house prices among the districts of Berlin and thereby identifies the local price mark-up due to the reputation of the district.
3595,Multi Class Address Classification of unknown Bitcoin Addresses through the full transaction history of every address and the previously computed features.
3598,Scraping and preprocessing of labeled btc addresses from differen websites. Collection of specified transactions from a full bitcoin blockchain node hosted on Google Big Query
3605,"Data Cleaning, Merging and Preprocessing and application of the common input heuristic for of all transactions in order to get additional labels for unknown addresses. Feature Engineering for Multi Class Address Classification"
3609,"Exploration of Scraped Addresses, Transaction Flow (Sankey Diagram), Transaction and Category Details, Time Series Chart and Transaction Categories. As well as analyzing the impact of different transaction categories on the Bitcoin Price through different metrics"
3610,Compare the weighted BBW of the total coins used in the analysis to FRM Crypto and get the boxplot of individual lambdas(individual FRM for each coin)
3611,"Retrieval of FRM Crypto as well as the quantile regression coefficients based on the returns of the
12 coins and 2 macro-economic variables with window size 48,  using quantile LASSO regression and
GACV method provided in the conventional FRM model. Moreover, transform the retrieved coefficients file
into a row-wise daily coefficient matrix, based on which we will create a moving network plot in section 8."
3612,"Generate network graphics for each our as well as a gif of moving network to analyse the relationship between the coins and macroeconomic variables for ""high-lambda"" or risky period. The node size corresponds with the number of appearance of a variable in 12 regressions, and the arrow width represents the absolute coefficient size. Here, it has done for 15th-21st of Dec 2017."
3613,"Selecting the coins for analysis
 1) Analysis period: 2017 Jan - 2018 Nov
 2) Input data: weekly market capitalization data of the top 200 coins
 3) Selecting criteria:
 3.1) Top 15 coins according to the average market cap
 Result: 15 coins ['BTC' 'ETH' 'XRP' 'BCH' 'LTC' 'EOS' 'ADA' 'XLM' 'MIOTA' 'TRX' 'BSV' 'USDT' 'DASH' 'BNB' 'NEO']
 Market Cap share of the selected coins: 0.899
 3.2) Consistent appearance in the top 200 chart based on the market cap
 Result: 28 coins ['BCN' 'BTC' 'BTS' 'DASH' 'DCR' 'DGB' 'DGD' 'DOGE' 'ETC' 'ETH' 'FCT' 'GNT' 'LSK' 'LTC' 'MAID' 'NEO' 'REP' 'SC' 'STEEM' 'STRAT' 'USDT' 'WAVES' 'XEM' 'XLM' 'XMR' 'XRP' 'XZC' 'ZEC']
 Market Cap share: 0.849
 3.3) coins satidfy 1) and 2) above
 Result: 8 coins  ['BTC' 'DASH' 'ETH' 'LTC' 'NEO' 'USDT' 'XLM' 'XRP']
 Market Cap share of the selected coins: 0.795
 4) Final Data Input
 ['BTC' 'ETH' 'XRP' 'BCH' 'LTC' 'EOS' 'XMR' 'NEO' 'MIOTA' 'DASH' 'ETC' 'ZEC']
 Market Cap share of the selected coins: 0.88"
3614,1) Calculate Bollinger Bands Width for 12 coins and the weighted BBW according to market cap share (window size = 48h) 2) Compare the BBW of BTC to its price movement
3615,"1)scrape the top 200 coin prices and total market cap of the Crptocurrency market over 2 years
 2)souce: https://coinmarketcap.com/historical/
 3)startdate='20170101', enddate='20191120', weekly data"
3616,Analyze the price/return of the 12 coins and the 2 macro-economic indices
3617,"1) Retrieval of the hourly price data of cryptocurrencies via API call 2) Calculation of the hourly returns 3) Retrieval of the daily macro-economic variables via API call and transform it into an hourly data 4) For macro Index S&P 500 and VIX, NaN (value on weekends and holidays) were filled with the value of previous the non NaN value"
3618,Computes kernel density estimates of the diagonal of the genuine and forged swiss bank notes. The bandwidth parameter are chosen by Silverman rule of thumb.
3620,Computes a two dimensional scatterplot of assistants and sales from the pullovers data set.
3621,Shows a screeplot of the eigenvalues for the PCA of the Swiss bank notes. It computes the correlations between the variables and the principal components and displays the first two of them.
3623,Calculates and plots the odds-ratios of the probability of taking drugs in a logit model for a gender-age group combination and tests two models (one without a curvature and more general model with a curvature term).
3624,"The document similarity of quantlets is calculated based on their keywords. For this purpose quantlets are taken from the MVA book, BCS project and from the entire Quantnet. First the keywords are transformed into the vector representation. Then the scalar product is applied calculating so the similarity measure. The advanced term-term correlation model additionally uses the term-term correlation matrix between the terms of all documents. Finally the k-means algorithm with Euclidean norm is applied for clustering (four clusters) and the data are represented via MDS (multidimensional scaling) showing metric MDS for BCS quantlets, metric MDS for MVA quantlets and metric MDS for all quantlets."
3625,Computes boxplots for the length (X1 variable) of the genuine and forged banknotes from the Swiss bank data.
3627,Computes boxplots for the diagonal (X6 variable) of the genuine and forged banknotes from the Swiss bank data.
3629,Calculates the actual error rate (AER) for price and clusters of Boston houses.
3630,"Performs a standardized regression using the Lasso methodology. The estimates become nonzero at a point that means the variables enter the model equation sequentially as the scaled shrinkage parameter increases. The Lasso technique results in variable selection. Finally, the resulting Lasso estimates are plotted."
3632,"Performs a correspondence analysis for the US crime, shows the eigenvalues of the singular value decomposition of the chi-matrix and displays graphically its factorial decomposition."
3633,Performs classification analysis and plots the classification tree for the US bankruptcy data with 84 companies employing the twoing rule.
3634,"Computes a test statistic for the variance of the US energy company data. H0: empirical variance is equal to the theoretical variance, given unknown mean."
3635,Computes parallel coordinates plot for car data.
3636,Computes the five-number summary and a boxplot for world cities.
3639,Generates a data set and applies the sliced inverse regression algorithm (SIR) for dimension reduction.
3640,Computes a linear regression of mileage on weight and displacement for car data set
3642,Plots the initial configuration of the points for nonmetric MDS for car marks data.
3645,"Calculates a regression (LSE) of price of blue pullovers X2, advertisement cost X3 and sales assistants X4 on sales X1 for a unrestricted and a restricted model and runs a LR and F-test for the classic pullovers data."
3646,Computes the pool adjacent violator algorithm (PAV).
3648,Produces a three-way contingency table for the drug consumption data from Everitt and Dunn.
3649,Computes Flury faces for observations 91 to 110 of the Swiss bank notes data.
3651,Performs classification analysis and plots the classification tree for the US bankruptcy data with 84 companies employing the Gini index and a constraint.
3652,"The document similarity of quantlets is calculated based on their keywords. For this purpose quantlets are taken from the MVA book, BCS project and the whole Quantnet. First the keywords are transformed into the vector representation, basis model is used. Finally the k-means algorithm is applied for clustering (four clusters) and the data are represented via MDS (multidimensional scaling)."
3653,Computes conditional normal densities f(x2|x1) where the joint distribution has mean=(0|0) and cov=(1|-0.8)~(-0.8|2).
3655,Computes a two dimensional scatterplot of mileage and weight from the car data set.
3657,Plots probability density functions and cumulative density functions of Gaussian mixture and Gaussian distributions.
3658,"Performs a standardized regression using the group Lasso methodology. The estimates of groups of variables become nonzero at a point that means these groups of variables enter the model equation sequentially as the shrinkage parameter increases. The group Lasso technique excludes some of the groups from the model and all coefficients in the remaining groups are non zero. Finally, the resulting group Lasso estimates are plotted."
3659,Computes Flury faces for the Swiss bank notes data.
3661,Computes the map of the cities by application of multidimensional scaling. Reflects and rotates the figure by 90 degrees.
3663,Computes parallel coordinates plot for variables mileage and weight of the car data set.
3664,Plots an SVM classification plot of bankruptcy data set with sigma = 100 and C = 1.
3665,Computes the Chi-Square test statistic for the Eye-Hair data.
3667,Generates a data set and applies the sliced inverse regression algorithm (SIR II) for dimension reduction.
3668,Computes the optimal portfolio weights with monthly returns of six US firms from Jan 2000 to Dec 2009. The optimal portfolio is compared with an equally weighted one.
3669,Computes parallel coordinates plot for variables weight and displacement of the car data set.
3670,Computes the averaged shifted histogram for the diagonal of all Swiss bank notes.
3672,Computes a two dimensional scatterplot of X5 vs. X6 (upper inner frame vs. diagonal) of the Swiss bank notes.
3674,"Shows a screeplot of the eigenvalues for the PCA of the standardized Swiss bank notes. Additionally, it computes the correlations between the variables and the principal components and displays the first two of them."
3675,Hexagon plot between Income and Flat size.
3676,"Performs a correspondence analysis for the Belgian journal data, shows the eigenvalues of the singular value decomposition of the chi-matrix and displays graphically its factorial decomposition."
3678,Computes the effects of financial characteristics on bankrupt with logit model.
3679,Plots an SVM classification plot of bankruptcy data set with sigma = 2 and C = 1.
3680,Performs cluster analysis for the transformed Boston housing data.
3682,Calculates and plots the correlations of the first three PCs with the original variables for the standardized Boston housing data.
3683,"Computes boxplots for variables headroom, rear seat clearance and trunk space of the car data set."
3684,Performs a PCA for the standardized time budget data and shows the first two principal components for the individuals and the variables.
3685,Plots an example of population files.
3687,"Plots four probability density functions and four tails in comparison of the NIG, the Laplace, the Cauchy and the Gauss distribution."
3688,Computes parallel coordinates plot with intersection.
3689,"Performs a correspondence analysis for the baccalaureat data, shows the eigenvalues of the singular value decomposition of the chi-matrix and displays graphically its factorial decomposition."
3690,"Plots the scatterplot matrix for the transformed Boston housing data variables X1, ... , X5 and X14."
3691,Performs a multidimensional scaling (MDS) for the car marks data and shows the plots for the MDS and for the MDS/variables correlations.
3692,"Reads the Boston housing data and spheres them to run Exploratory Projection Pursuit (EPP) on them. We select n (=50) randomly chosen one-dimensional projections in the space of the data. For each set of projected data the Sibson Jones index is applied, which considers the deviations from the normal density for univariate data, to judge the interestingness- of the projection."
3694,"Gives a contour plot of the kernel density estimate of variables X4, X5 and X6 of the Swiss bank notes."
3696,Performs classification analysis for the US bankruptcy data with 84 companies employing the Gini index and a constraint. It plots a decision tree.
3697,Computes a three dimensional scatterplot for X4 vs. X6 vs. X5 (lower inner frame vs. diagonal vs. upper inner frame) of the Swiss bank notes.
3699,Performs factor analysis based on 3 factors for the transformed Boston housing data using three different methods.
3701,"Plots three probability density functions and three cumulative density functions of the t-distribution with different degrees of freedom (t3 stands for t-distribution with degree of freedom 3, etc.)"
3702,"Computes parallel coordinates plot for variables headroom, rear seat clearance and trunk space of the car data set."
3703,Plots a section of the linear regression of the sales (X1) on price (X2) for the pullovers data. Graphical representation of the relationship: total variation = explained variation + unexplained variation.
3704,Computes a two dimensional scatterplot of price and sales from the pullovers data set.
3706,Plots an SVM classification plot of bankruptcy data set with sigma = 0.5 and C = 200.
3707,Shows an example for the simplicial depth in 2 dimensions.
3708,Hexagon plot between Age and Time for computer per week.
3709,Draws bootstrap samples from a simulate data standard normal dataset and plots their empirical distribution functions (edf).
3710,The document similarity of quantlets is calculated based on their keywords. For this purpose quantlets are taken from the MVA book and BCS project. First the keywords are transformed into the vector representation. Then the scalar product is applied calculating so the similarity measure. The advanced term-term correlation model additionally uses the term-term correlation matrix between the terms of all documents. Finally the k-means algorithm with the Euclidean norm is applied for clustering (four clusters) and the data are represented via MDS (multidimensional scaling) showing metric MDS for BCS quantlets and metric MDS for MVA quantlets.
3711,"Computes parallel coordinates plot for variables displacement, gear ratio for high gear and company headquarters of the car data set."
3712,Plots the area of two different groups of orange peel data via svm classification using anipotropic Gaussian kernel.
3713,Performs a canonical correlation analysis for the US crime and US health data.
3714,Computes a parallel coordinate plot for the observations of the full bank note data set.
3716,MVAspecclust computes the clusters for the exemplary data based on the Euclidean distance and predefined number of clusters.
3718,"Plots the Lasso solution under the least squares loss function. The contour plots of the quadratic form objective function are produced which are centered at the least squares solution (beta_1,beta_2)^t = (6,7)^t in the two-dimensional case for the case of both the non-orthonormal and orthonormal design. The tuning parameter s being equal to 4."
3720,Generates a 3D surface plot of the Gumbel-Hougaard copula for theta = 3.
3721,Performs a linear regression for the subset of the transformed Boston housing data.
3722,Performs cluster analysis for the french food data after standardization of the variables.
3724,Produces a hexagon and a scatter plot between Age and Net income.
3725,"Performs a PCA for the standardized Swiss bank notes and shows the first three principal components in two-dimensional scatterplots. Additionally, a screeplot of the eigenvalues is displayed."
3726,Demonstrates maximum likelihood discrimination rule (ML rule) and calculates the apparent error rate for Swiss banknotes.
3728,Plots 3D response surfaces and a contour plot for the variable y and the two factors that explain the variation of y via the quadratic response model.
3729,"Calculates and plots the simplicial depth for a two-dimensional, 10 point distribution according to depth."
3730,Gives a contour plot of the kernel density estimate of variables X5 and X6 of the Swiss bank notes.
3732,Provides a scatterplot of daily standardized log-returns of Siemens versus Bayer.
3733,"Performs a logit model using the Lasso methodology. The estimates become nonzero at a point that means the variables enter the model equation sequentially as the shrinkage parameter increases. The Lasso technique results in variable selection. Finally, the resulting Lasso estimates and predictions are plotted."
3734,Performs a canonical correlation analysis for the car marks data and shows a plot of the first canonical variables.
3735,"Plots three probability density functions and three cumulative density functions of the GH, Hyperbolic and NIG distributions."
3736,Tests the equality of 2 groups of US Company data and computes the F-statistic and the critical value of the test and the simultaneous confidence intervals.
3737,"Plots three probability density functions and three cumulative density functions of the Laplace-distribution with different scale (L1 stands for Laplace-distribution with scale 1, etc)."
3738,Illustrates the PAV algorithm for nonmetric MDS for car marks data.
3741,Computes Parallel Coordinates Plot (PCP) for Boston Housing data.
3742,"Gives contour plots of Gumbel-Hougard copula for different theta values (1, 2, 3, 10, 30 and 100)."
3743,Performs a PCA for the standardized US company data without IBM and General Motors. It shows the first two principal components and screeplot of the eigenvalues.
3744,Shows monthly returns of six US firms from Jan 2000 to Dec 2009.
3745,Produces Gumbel-Hougaard copula sampling for fixed parameters sigma and theta.
3746,Computes the Sibson Jones index which considers the deviations from the normal density for univariate data.
3747,Computes a linear regression of column 5 (upper inner frame) and column 4 (lower inner frame) for the genuine Swiss bank notes.
3748,Plots the scatterplot matrix for Boston housing data.
3749,Plots the area of two different groups of spiral data via svm classification using anisotropic Gaussian kernel.
3751,"Plots different kernel functions: Uniform, Triangle, Epanechnikov, Quartic (biweight), Gaussian."
3753,Computes a spectral decomposition of the French food data and gives a representation of food expenditures and family types in two dimensions.
3755,Computes the univariate densities of X4 and X5 of the genuine Swiss bank notes.
3758,"Computes Andrew''s Curves for the observations 96-105 of the Swiss bank notes data. The order of the variables is 6,5,4,3,2,1."
3760,Demonstrates the differences of the pdf curves of a standard Gaussian distribution and a Cauchy distribution with location parameter mu = 0 and scale parameter sigma = 1.
3761,"Performs a PCA for the standardized Boston housing data. Determines the explained variance for the eigenvalues, which determine the relevant principal components."
3762,Computes the map of the cities by application of multidimensional scaling.
3764,Presents the coefficient estimates based on the saturated model with first and second order interactions as well as the coefficient estimates for the restricted model with first interactions for the drug data. It also computes G-squared deviance and Chi-squared test statistics.
3765,Computes boxplots for the 14 variables of Boston Housing data.
3766,Computes a two dimensional scatterplot of two correlated normal random variables.
3767,Performs a spectral decomposition of a 2 by 2 covariance matrix.
3768,Computes a two dimensional scatterplot of X4 vs. X5 (upper inner frame vs. lower) of the Swiss bank notes data.
3770,"Performs a PCA for the standardized US company data, shows the first two principal components and screeplot of the eigenvalues."
3771,Tests the equality of 2 groups of Boston Housing data and computes the F-statistic and the critical value of the test and the simultaneous confidence intervals.
3772,"Computes a Draftman Plot for columns 3, 4, 5 and 6 of the Swiss bank notes data."
3774,Performs a monotone transformation to the estimated stimulus utilities of the car example by applying the pool-adjacent-violators (PAV) algorithm.
3777,Computes and plots a linear regression of sales on price from the pullovers data set.
3778,Computes the monotonic regression and shows the rank order of dissimilarities for the pool-adjacent violators (PAV) algorithm.
3780,Demonstrates maximum likelihood discrimination rule (ML rule) for the Boston housing data.
3781,"Computes a scatterplot of a normal sample and the contour ellipses for mu =(3,2) and sigma = (1,-1.5)~(-1.5,4)."
3782,Shows a screeplot of the eigenvalues for the PCA of the standardized US company data. It computes the correlations between the variables and the principal components and displays the first two of them.
3783,Shows the first iteration of the PAV algorithm for nonmetric MDS for car marks data.
3786,Interprets the 7 crime variables of the dataset as contingency table and computes the distance between the US states depending on these variables for building appropriate clusters.
3788,Estimates a common principal components model for the implied volatility data and computes a likelihood ratio test.
3789,"Plots three probability density functions and three cumulative density functions of the Cauchy distribution with m = 0 and different scale parameters (s=1, s=1.5, s=2)."
3790,Computes a parallel coordinate plot for the observations 96-105 of the Swiss bank notes data.
3792,"Displays graphically maximum likelihood discrimination rule (ML rule) for 2 normal distributed samples. The inner interval is the allocation set for N(1,0.25), the outer intervals are for observations from N(0,1)."
3794,Produces Gumbel-Hougaard and Clayton copula sampling for fixed parameter theta.
3795,Computes 4 histograms for the diagonal of the forged Swiss bank notes. The histograms are different with respect to their origin.
3797,Computes classification ratio dynamic over the number of terminal nodes.
3798,Builds a linear regression model for the complete transformed Boston housing data.
3799,"Rewrites the 5.3 example using the classic blue pullover example and show the mean, covariance and correlation matrix of data."
3800,"Plots four tails of probability density functions of the t-distribution and one tail of the Gauss distribution with different degrees of freedom (t3 stands for t-distribution with 3 degrees of freedom, etc)."
3801,"Computes the five-number summary and boxplots for the mileage (X14 variable) of US, Japanese and European cars."
3805,Performs a factor analysis for the car marks data and shows the eigenvalues of r-psi and a plot of the factor loadings for the first two factors.
3807,"Computes the ANCOVA model with Boston housing data. We add binary variable to the ANCOVA model, and try to check the effect of the new factors on the dependent variable."
3808,"Performs a Fisher discrimination analysis of the Swiss bank notes, computes the misclassification rates for the whole dataset and displays nonparametric density estimates of the projected data."
3811,Calculates descriptive statistics for the Boston housing data and their transformations.
3812,Gives plots of the product of univariate and joint kernel density estimates of variables X4 and X5 of the Swiss bank notes.
3813,Computes 4 histograms for the diagonal of the forged Swiss bank notes. The histograms are different with respect to their binwidth.
3815,Performs classification analysis and plots the classification tree for the US bankruptcy data with 84 companies employing the Gini rule.
3816,Computes the optimal portfolio weights with monthly returns of IBM and Forward Industries from Jan 2000 to Dec 2009. The optimal portfolio is compared with an equally weighted one.
3817,Reads the Swiss banknote data and spheres them to run Exploratory Projection Pursuit (EPP) on them. We select n (=50) randomly chosen one-dimensional projections in the six-dimensional space of the data. For each set of projected data the Friedman-Tukey-Index is applied to judge the interestingness of the projection.
3818,"Illustrates the (univariate) Central Limit Theorem (CLT). n*1000 sets of n-dimensional Bernoulli samples are generated and used to approximate the distribution of t = sqrt(n)*(mean(x)-mu)/sigma -> N(0,1). The estimated density (blue) of t is shown together with the standard normal (red)."
3820,"Computes the Jaccard, simple matching and Tanimoto proximity coefficients for binary car data."
3821,"Produces a contingency table for the drug consumption data from Everitt and Dunn and calculates Jaccard, simple matching and Tanimoto proximity measures for cluster analysis."
3823,"Plots four tails of probability density functions of the GH distribution with different lambda, NIG and the Hyperbolic distributions (f=0.5 stands for GH-distribution with lambda=0.5, etc)."
3824,"Performs a PCA for the Swiss bank notes and shows the first three principal components in two-dimensional scatterplots. Additionally, a screeplot of the eigenvalues is displayed."
3827,"Computes Andrew's Curves for the observations 96-105 of the Swiss bank notes data. The order of the variables is 1,2,3,4,5,6."
3829,Computes the Chi-Square test statistics for the car data.
3831,Plot of hexagon algorithm.
3832,Simulates a projection of a (normal) point cloud which may capture different proportions of the variance.
3834,Performs cluster analysis for 8 data points.
3836,Performs a PCA for the standardized French food data and shows the first two principal components for the individuals and the variables. The normalization corresponds to that of Lebart/Morineau/Fenelon.
3838,"Illustrates the 2D Central Limit Theorem (CLT). n*2000 sets of n-dimensional Bernoulli samples are generated and used to approximate the distribution of t = sqrt(n)*(mean(x)-mu)/sigma -> N(0,1). The estimated joint density is shown."
3839,Computes parallel coordinates plot with cubic spline interpolation.
3840,"Performs a PCA for the rescaled Swiss bank notes. X1, X2, X3, X6 are taken in cm instead of mm. It shows the first three principal components in two-dimensional scatterplots. Additionally, a screeplot of the eigenvalues is displayed."
3843,"Performs LASSO regression in a moving window by using BIC criterion to choose penalty parameter (lambda). The simulated data contains a break point after which the condition number of a matrix [t(X)X] changes. Plots time series of lambda in LASSO regression. Furthermore, the cardinality of the active set q, the L2-norm of the residuals, the L1-norm of the parameter beta and the condition number of the squared design matrix [t(X)X] are plotted. All of the plots contain results from a number of simulations and the average over all of them."
3844,"Performs LASSO regression in a moving window by using BIC criterion to choose penalty parameter (lambda). The simulated data contains a break point after which the variance of the error term changes. Plots time series of lambda in LASSO regression. Furthermore, the cardinality of the active set q, the L2-norm of the residuals, the L1-norm of the parameter beta and the condition number of the squared design matrix [t(X)X] are plotted. All of the plots contain results from a number of simulations and the average over all of them."
3845,"Performs LASSO regression in a moving window by using BIC criterion to choose penalty parameter (lambda). The simulated data contains a break point after which the cardinality of active set q changes. Plots time series of lambda in LASSO regression. Furthermore, the cardinality of the active set q, the L2-norm of the residuals, the L1-norm of the parameter beta and the condition number of the squared design matrix [t(X)X] are plotted. All of the plots contain results from a number of simulations and the average over all of them."
3846,"Performs LASSO regression in a moving window by using BIC criterion to choose penalty parameter (lambda). The simulated data contains a break point after which the parameter beta changes. Plots time series of lambda in LASSO regression. Furthermore, the cardinality of the active set q, the L2-norm of the residuals, the L1-norm of the parameter beta and the condition number of the squared design matrix [t(X)X] are plotted. All of the plots contain results from a number of simulations and the average over all of them."
3855,Sierpinski plots the Sierpinski triangle
3856,Use different models to correctly predict fraudulent and valid transactions.
3857,Using UMAP  to analysis the pattern of fraudulent activities.
3858,Exploratory analysis of data to extract the pattern of fraudulent activities.
3859,Use complex numbers as parameters to fit a Animal. Creates an MP4 file of the nodding doggy.
3860,Use complex numbers as parameters to fit an elegant rhino. Creates an MP4 file of the elegant rhino.
3861,Use complex numbers as parameters to fit a doggy that nods its head. Creates an MP4 file of the nodding doggy.
3862,Use complex numbers as parameters to flat fish that shaking its tail. Creates an MP4 file of the flat fish.
3863,"Using complex number and Spyder to draw a fish, and to generate a PNG and a MP4 file."
3864,Use complex numbers as parameters to draw a bird with big mouth (or called Toucan).
3865,Draw any animal using python
3866,"Using complex number and basic python to draw a parrot, and creating its png and MP4 file."
3867,"['Study the relationship between press release and market performance with the case of Pfizer during the pandemic.', 'data is generated described in sub folders']"
3868,"['Study the relationship between press release and market performance with the case of Pfizer during the pandemic.', {'Step 4': 'predit the win rate by using sentiment score. (Data is in the main folder)'}]"
3869,"['Study the relationship between press release and market performance with the case of Pfizer during the pandemic.', {'Step 1': 'web sscraping data from Pfizer website and yahoo finance, also the google trends for Pfizer.'}, {'Step 2': 'calcualte the sentiment score of each press release.'}]"
3870,"['Study the relationship between press release and market performance with the case of Pfizer during the pandemic.', {'Step 3': 'generate the data and regress the market performance variables on press release.'}]"
3871,"['Study the relationship between press release and market performance with the case of Pfizer during the pandemic.', {'Step 5': 'use UMAP to reduce dimension and conduct clustering. (the input data is in the main folder)'}]"
3872,This study is to predict the closing price of the Taiwan Top 50 Tracker Fund using different machine learning algorithms and compare their performances.
3873,Using the fund data from Securities Investment Trust & Consulting Association of The R.O.C. to do the hierarchical clustering. Then observe the result of clustering for size and total expense ratio of the funds.
3874,Hierarchical clustering for customer data.
3875,Using hierarchical clustering to group stations of Taipei MRT（Taipei Mass Rapid Transit）based on the passenger flow in a station from 2010 to 2021.
3876,Performs a PCA and a cluster analysis for 11 chosen Mobile OS Usage during 2009-2014 from Freedata Visualization Software.com
3877,Draw the clustering diagram and dendrogram with iris data by using Agglomerative Clustering.
3878,Performs a PCA and a cluster analysis for 20 randomly chosen bonds from FISD dataset.
3880,Performs a hierarchical clustering to assess the debt level of each country in the euro area.
3881,Spectral clustering for log population and log crime rate per capita
3882,"After getting the lyrics of a song, make a wordcloud to discover which kind of words used the most in the song."
3883,Compare two Lover Songs with their frequency words and their topics (LDA)
3884,"Make the word cloud by extracing the words from Michael Jackson's song ""We are the word""."
3885,Using LDA to learn the topic representation of a song and word distribution of the song
3886,Compare three Country Songs with their frequency words and their topics (LDA)
3888,"Using web scraping to get YouBike JSON data from Taiwan government open data platform, and make a interactive map."
3889,"['Demonstrate getting data from webpage API and scraping data information from taipei public dataset', 'Find the nearest available youbikes in Zhongshan district']"
3890,"['Demonstrate getting data from webpage API and scraping data information from taipei public dataset', 'Find the nearest avaiable youbike in taipei.']"
3891,"Getting data from webpage API and scraping, then making the location information of YouBike station all over Taipei city."
3892,"['Demonstrate getting data from webpage API and scraping', 'Maping the location of electric vehicle fleet stations in New York City']"
3893,Using json module to unpack Taichung YouBik dataset and obtaining the instantaneous information of YouBike in Taichung city.
3895,Use the CRIX data as an example to predict market return by Kalman filter
3896,Use the CRIX data to predict the return by Kalman filter
3897,Use the Kalman filter to predict market return using the CRIX data as an example.
3898,Use the Apple stock price data to predict the return by Kalman filter
3900,"['Estimate CAPM coefficients using GMM', 'test inefficiencies (violation of moment conditions) with J-test', 'include a momentum factor in the analysis']"
3901,"['plot of monthly returns of BTC, ETH, BNB, XRP, ADA and CRIX']"
3902,"['plot of prices in USD of five cryptocurrencies (BTC, ETH, BNB, XRP, ADA)']"
3903,['plot of Royalton Crix index']
3904,Using web scraping to obtain word frequency plots
3905,"Using the CreditReform data, make a decision tree classifier and draw the tree plot."
3906,Predicts the firm leverage from FISD Data with a Tree (CART) and a Random Forest.
3908,Using CART and Random Forest to predict the stock price direction of Apple
3909,Using UMAP to reduce the dimensionality of the fraud detection data.
3910,"['Study the relationship between press release and market performance with the case of Pfizer during the pandemic.', 'use UMAP to reduce dimension and conduct clustering.']"
3912,Using UMAP and t-SNE to perform dimension reduction on Taiwan Index 50 data from 6 dimensions to 2 dimensions.
3913,Using Web Scraping to extract and study information from texts and perform sentiment analysis
3914,"The files contain code for Python Webscraping and Sentiment Analysis by using Martin Luther King, Jr.'s speech ""I Have A Dream"" ."
3915,"['Demonstrate getting data from webpage API and scraping the speech of Jerome Powell from federalreserve.gov', 'Introducing functional programming and object-oriented programming']"
3916,"Python web scrapping and seniment analysis of John F. Kennedy's speech ""Ich bin ein Berliner""."
3917,"[""Sentiment analysis on Kennedy's speech 'Measuring everything except what makes life worthwhile'"", 'Introducing functional programming and object-oriented programming']"
3918,"Using web scraping technic to get the speech of the chairman of the Federal Reserve, then visualize the frequent words and implement sentiment analysis."
3919,"Scraping the speech transcript of each party's leader from the web, and analyzing the difference of sentiment in their speech."
3920,"['Demonstrate getting data from webpage API and scraping the speech of Donald Trump from CNN', 'Introducing functional programming and object-oriented programming']"
3922,"Retrieves the most frequent search queries entered into the search field of the QuantNet visualization, called QuantNetXploRer. After every keystroke the search queries are tracked via Google Analytics. For better analysis, search queries with less than 3 characters are omitted. The API query specifies the parameters dimensions, metrics and filters conditioning the desired Event Tracking criteria."
3923,"Extracts recent download statistics from Google Analytics starting November 2013 for each Quantlet. Top ten countries are taken and the final results are presented as an R data frame. The API query specifies the parameters dimensions, metrics and filters conditioning the desired Event Tracking criteria."
3924,"Extracts recent download statistics from Google Analytics starting November 2013 for each Quantlet. Furthermore, short explanations (based on the desciptions in the meta information) of the specific Quantlets are added and the final results are presented as an R data frame. The API query specifies the parameters dimensions, metrics and filters conditioning the desired Event Tracking criteria."
3925,Computes the estimate of long memory parameter d.
3927,"Estimates the parameters of a Generalized Pareto distribution for the negative log-returns of portfolio (Bayer, BMW, Siemens, VW)for the time period from 2000-01-01 to 2012-12-31 and produces a P-P plot."
3928,Plots the Silverman's approximation to the effective spline kernel: K(u)=0.5*exp(-|u|/sqrt(2))*sin(|u|/sqrt(2)+pi/4).
3929,"Computes time series plots for the time domain and the frequency domain, i.e. the periodogram."
3930,Applies first differencing and 12-month centered moving average filter to SOI data and plots the results in time and frequency representation.
3931,"The contour diagrams of the Gumbel, Clayton and Normal copula are plotted in three coordinate plans. The fitted, empirical observations are added as red dots."
3932,"Estimates the parameters of a Generalized Pareto distribution for the negative log-returns of portfolio (Bayer, BMW, Siemens, VW) for the time period from 2000-01-01 to 2012-12-31 and produces a Q-Q plot."
3933,"According to the VaR methodology the profit and loss (P&L) is plotted against the time. The dots represent the empirical P&L stated by the data. The yellow plot indicates the lower alpha quantile. The residuals of an estimated GARCH(1,1) model are used to estimate the marginal distributions of the different series. The GARCH model is based on a window of 250 days. Based on the marginal distribution a copula is calculated. Corresponding to the previous results a one step ahead forecast is computed and the procedure starts again. Finally 12 plots are produced. Each plot contents a graph with regard to one copula (Gumbel, Clayton and Normal) combined with one quantile (5%, 1%, 0.5% and 0.1%)."
3935,"Two plots of stable distributions are generated. The upper plot shows the density function and the lower it's log. The parameters can be adjusted by a panel, where the four parameter (alpha=(0,2]; beta=[-1,1]; gamma>0; delta=[-2,2]) of an alpha-stable distribution can be modified. The user can choose between three parametrization (S1; S2; S3) and the exact stable distributions Normal, Cauchy and Levy."
3936,Computes the statistical features of DAX monthly returns.
3937,Plots the Kernel density estimator of the residuals and of the normal density for the log-returns of DEM/USD and GBP/USD from 01.12.1979 to 01.04.1994.
3939,Shows the rate of convergence to infinity for the stable distributed random variables is higher than for standard normal variables. Plots the convergence rate of maximum for n random variables with a standard normal cdf and with a 1.1-stable cdf. Refers to exercise 16.2 in SFS.
3940,Simulation of 500 random normal (left) and 1.5-stable (right) normal variables with 25% and 75% quantiles (black lines and 2.5% and 97.5% quantiles (red lines) of the distributions. Refers to exercise 16.1 in SFS.
3941,Plots the density of right and left skewed asymmetric normal distributions.
3942,"Plots three PDFs of the Binomial distribution with different probabilities of success (p) in each trial. The number of trials is set to 20. The line colours correspond to p = 0.1 (blue), p = 0.5 (green) and p = 0.8 (dark red)."
3943,Transforms a random variable with a given mean and variance to an asymptotic standard normal distribution.
3944,"Plots the average of n Bernoulli trials (each taking a value of +/- 1, with probability 0.5) to show the law of large numbers and the law of the iterated logarithm. Plots of sample mean (red), its asymptotic standard deviation (dark-green) and its bound given by law of iterated logarithm (blue)."
3945,Calculate the normal and edgeworth approximations of the normalized sample of size 5 from an exponential pdf.
3946,"Simulates data from the standard normal distribution N(0,1) and then plots the PDF function."
3947,Shows an example of random variables not following asymptotic normality. For this specific example a set of Cauchy random variables was selected and its asymptotic distribution was compared with binomial and normal distributions.
3948,"Plots the points showing law of large numbers. As the sample size becomes larger, the sample mean converges to the theoretical (true) mean of the distribution."
3950,"Simulates data from a multivariate normal distribution, estimates the PDF by kernel density estimate and plots the result."
3951,Simulates data from normal distribution and plots its cumulative density function (cdf).
3952,"Plots of left-skewed normal distribution (shape parameter alpha = - 5), symmetric normal and right-skewed normal distribution (shape parameter alpha = 5)."
3953,"Displays an example of which meets the convergence in mean square but not convergence in almost sure series of the example. We select n = 400 and plot three cases: the initial value of Z = 0.5 (blue), 0.2 (green), and 0.8 (dark red)."
3955,Plots Value at Risk and Expected Shortfall in one figure and shows the relationship between VaR and ES.
3956,"In this paper, to investigate the realized volatility of the highly volatile cryptocurrencies asset, we employ threhold realized variance method to separate jump component apart from continuous process. Despite that the jump process doesn’t improve explanation power, we can still find that jump brings information. The one-day lagged threshold jump component has significant positive impact on the future realized volatility, this suggest that we are likely to find cryptocurrency market more volatile after jumps happened one day before. However, the un-threshold jump component is negatively correlated to the future realized volatility significantly."
3957,"This quantlet includes two time series analysis code files. The first one is the HAR forecasting model calibration, in-sample and out-of-sample forecasting. The second one is calculating the realized utility function documented on Bollerslev et al (2018)."
3962,"This quantlet includes two main code files. The first one named CAL_RV_BPV.py is the estimation of realized volatility, bipower volatility, threshold bipower volatility and separated jumps, threshold jumps. The second one named PLT_RV_BPV.py generates all related figures in the paper. Also some other auxiliary files that generate different statistics results."
3971,"This quantlet includes mainly the data processing part for the RCVJ_Forecasting project. Read and write data, clean the old CRIX high frequency data and functions to retrieve data from DYOS."
3982,"Reproduces the empirical results of financial data in the paper FASTEC, Factorizable sparse tail event curves by Chao, Haerdle and Yuan (2015), including plot of daily log returns of 230 firms and tau-spread analysis."
3983,"Reproduces the empirical results of Chinese temperature of 2008 in the paper FASTEC, Factorizable sparse tail event curves by Chao, Haerdle and Yuan (2015), including plot of temperature curves and tau-spread analysis."
3984,Illustrates the restricted strong convexity property of expectile loss function with a particular example. The red line shows the lower error bounds.
3985,Solves the optimization problem of smooth convex function with nonsmooth convex regularizer by Fast Iterative Shrinkage Thresholding Algorithm. Uses the expectile loss function and nuclear norm to do the factor analysis for the Chinese temperature data from 1957 to 2009.
3986,Fitting the bandwidth of Nadaraya-Watson estimator to simulated data on an interval using cross-validation and generalised cross-validation in R and Python
3989,Networks R Code
3990,Produces volatility plots based on historical hf CRIX data
3992,SHA 256 R Code
3993,"Produces plots based on historical hf CRIX data (CRIX Plot, CRIX Volatility and Cryptocurrency Correlations)"
3995,Produces full and 24h ts plots based on historical hf CRIX data (CRIX Plot)
3997,Produces GARCH plots based on historical hf CRIX data
3998,"['Delaunay Triangulation over a set of 10 points', 'Several plots of Alpha shapes over the same 10 points for different values of Alpha']"
3999,Several plots of Alpha Shapes over the scatter plot 0x=CRIX vs. 0y=VCRIX for different values of Alpha
4000,Time Series of daily CRIX data in the period 2 May 2016 to 10 Nov 2018; all together 923 data points Time Series of the VCRIX (CRIX volatility)
4001,Demonstrates Mersenne-Twister in Python framework for random number generation
4002,"Draws figures for White, Pink and Blue noise in time-domain and frequency-domain, ACF, PCF and applied Fourier transform"
4003,‘Shows Monte Carlo simulation results of a QML estimation of an ARCH(1) model'
4004,Investigates time series of a random walk
4005,Investigates time series of real financial data and finds appropriate moving average model
4006,Make example of simple decomposition
4007,Compare periodogram and spectrum
4010,Visualise the SOI index and its periodogram
4012,Generates OLS and ML estimates from data and simulates the underlying CIR process
4013,Investigates time series of real financial data and finds an appropriate autoregressive model
4014,"Calculates the value of an IRS based on a Bond, FRA and Forward Rate approach."
4016,Investigates time series of white noise
4020,Procedure of constructing BetaBoost model for credit scoring problem. Two types of base models can be selected for the model. The quality of the model is evaluated by artificial dataset. The dataset was artificially created using separate multivariate gaussian distributions for each of the classes. The parameters for the distributions were estimated using Kaggle GMSC dataset.
4021,Wiggly Animal
4022,estimation of SoNIC model on AAPL and BTC sentiment weights
4023,"stability simulations for SoNIC model with N=100; K=2, 5; T=100,200,500,1000,2000"
4025,stability analysis and choice of the number of clustering of SoNIC model on AAPL and BTC sentiment weights
4026,"simulation study for SoNIC model with parameters N=T=100, s=1, K=2..30"
4028,"Comparison of SoNIC with VAR and l1 penalized VAR on AAPL and BTC datasets. We also make comparison with zero Theta, which corresponds to no causality."
4029,"Performs the Unified yield curve Taylor rule in forecasting thed exchange rates, a combination of yield factors are tested to have the forecasting ability in forecasting the exchange rates. To fit a model with possible structural changes, we set several switching structures on a given dataset. The model is then used for forecasting over 3-12 monthos horizons. The input data are monthly observations of exchange rates and the pre-defined yield factors. Computes MAE for the forecasted values. Plots the time series ofthe predicted vs. observed values of exchange rates over the prediction interval."
4033,Shiny App using the SVCJ (Stochastic Volatility with Correlated Jumps) model to estimate Option Prices for some cryptocurrencies and the CRIX Index
4034,"This Quantlet allows to plot the considered indices, as well as their returns, density functions, correlations and volatilities, in order to compare the CC indices to the traditional indices."
4035,Tail event driven networks of SIFIs
4039,Calculate and plot uniform bootstrap confidence bands for the ProShares UltraShort S&P500 (SDS) LETF option implied volatility at the time-to-maturity 0.7 years
4041,Compute and plot the dynamics of stochastic factor loadings of the SPY LETF option implied volatility surface as well as the CBOE Volatility Index (VIX) over the period Sep. 2014 - Jul. 2015. The estimation of the dynamic semiparametric factor model with B-spline basis is performed for this purpose
4042,Calculate and plot uniform bootstrap confidence bands for the ProShares UltraShort S&P500 (SDS) LETF option implied volatility at the time-to-maturity 0.6 years
4044,Calculate and plot uniform bootstrap confidence bands for the ProShares UltraPro S&P500 (UPRO) LETF option implied volatility at the time-to-maturity 0.5 years
4046,Computes and plots the root mean squared prediction error of the dynamic one-step-ahead forecast of the implied volatility surface of the SPY LETF call option. The estimation of the dynamic semiparametric factor model with B-spline basis is performed for this purpose
4047,Compute and plot the true SSO LETF option and moneyness-scaling predicted IV via the dynamic semiparametric factor model with B-spline basis for specific time-to-maturity points
4048,Calculate and plot uniform bootstrap confidence bands for the ProShares UltraShort S&P500 LETF option implied volatility at the time-to-maturity 0.5 years
4050,Compute and plot the implied volatility surface for the SPY LETF option computed via the dynamic semiparametric factor model with B-spline basis. Also plot the real-data IV ticks for the given day
4051,"Compute and plot the estimated factor functions for the SPY ETF option data via the dynamic semiparametric factor                        model. The third order is assumed, so 4 factor functions are computed"
4052,Plot the implied volatilities for 4 leveraged ETFs versus the unleveraged SPY ETF implied volatility after moneyness scaling
4053,Calculate and plot uniform bootstrap confidence bands for the ProShares UltraPro S&P500 (UPRO) LETF option implied volatility at the time-to-maturity 0.7 years
4055,Calculate and plot uniform bootstrap confidence bands for the ProShares Ultra S&P500 LETF option implied      			                      volatility at the time-to-maturity 0.5 years
4057,Calculate and plot uniform bootstrap confidence bands for the SPDR S&P 500 ETF (SPY) option implied volatility at the time-to-maturity 0.5 years
4059,Calculate and plot uniform bootstrap confidence bands for the SPDR S&P500 (SPY) LETF option implied volatility at the time-to-maturity 0.7 years
4061,Calculate and plot uniform bootstrap confidence bands for the ProShares Ultra S&P500 (SSO) LETF option implied volatility at the time-to-maturity 0.7 years
4063,Calculate and plot uniform bootstrap confidence bands for the ProShares Ultra S&P500 (SSO) LETF option implied volatility at the time-to-maturity 0.6 years
4065,Plot the implied volatilities for 4 leveraged ETFs versus the unleveraged SPY ETF implied volatility before moneyness scaling
4066,Calculate and plot uniform bootstrap confidence bands for the ProShares UltraPro S&P500 (UPRO) LETF option implied volatility at the time-to-maturity 0.6 years
4068,Calculate and plot uniform bootstrap confidence bands for the SPDR S&P500 (SPY) ETF option implied volatility at the time-to-maturity 0.6 years
4070,This folder contains the quantlet SMLSMmergedata which merges the output file of quantlet SMLSMgetpanel and quantlet SMLSMprocessraw for the master thesis Supervised Machine Learning Sentiment Measures
4071,This folder contains 1 quantlet plotting the estimated sentiment measures of AAPL in 2016 for the master thesis Supervised Machine Learning Sentiment Measures
4072,This folder contains 1 quantlet estimating sentiment measures based on the Random Forest algorithm with one- and two-gram as inputs for the master thesis Supervised Machine Learning Sentiment Measures
4073,This folder contains 1 quantlet estimating sentiment measures based on a FinBERT as the encoder along with a dropout & dense layer for the master thesis Supervised Machine Learning Sentiment Measures
4074,This folder contains the quantlet SMLSMporterstemm which stems words according to the Porter algorithm for the master thesis Supervised Machine Learning Sentiment Measures
4075,This folder contains the quantlet SMLSMgetpanel which extracts variables from the WRDS database for the master thesis Supervised Machine Learning Sentiment Measures
4076,This folder contains 1 quantlet estimating dictionary-based sentiment measures for the master thesis Supervised Machine Learning Sentiment Measures
4077,This folder contains the quantlet SMLSMprocessrawdata which processes the raw NASDAQ news data set for the master thesis Supervised Machine Learning Sentiment Measures
4078,This folder contains 1 quantlet estimating sentiment measures based on the Random Forest algorithm with inputs from FinBERT for the master thesis Supervised Machine Learning Sentiment Measures
4079,This folder contains 1 quantlet regressing the abnormal returns on sentiment measures and the control variables for the master thesis Supervised Machine Learning Sentiment Measures
4080,This folder contains 1 quantlet containing the code to produce the descriptive table and the correlation matrix in the master thesis Supervised Machine Learning Sentiment Measures
4081,Produces simulation plots of misclustering rate under various model specifications
4083,Produces plots of cumulative portfolio returns using contrarian strategy for Cryptocurrencies
4084,"Bitcoin Pricing Kernels are inferred using a novel data set from Deribit, one of the largest Bitcoin derivatives exchanges. This enables arbitrage-free pricing of various instruments. State Price Densities are estimated nonparametrically. The underlying asset process is viewed through the lens of a Stochastic Volatility with Correlated Jumps (SVCJ) framework. Shape invariant pricing kernels are reported. Market inefficiencies are assessed based on the shape of the pricing kernels. A trading strategy that exploits these inefficiencies is evaluated."
4087,Kelly_Bernoulli illustrates growth-optimal betting under repeated Bernoulli trials
4088,Stable_Kelly_MomentDistribution calculates a simulated distribution of sample moments under limited data points. The goal is to show non-normal behavior for low-frequency data.
4089,"Stable_Kelly_MomentIncrease shows the sample moment convergence for increasing data points, favoring non-normal limits."
4093,Kelly_Bernoulli_GUI illustrates growth-optimal betting under repeated Bernoulli trials. The according GUI visualizes/analyzes the Kelly trajectories.
4098,"S&P500 Daily Returns Rt = 100 × ln(Pt/Pt_1) from January 2, 1990 to December 31, 2018"
4099,Monthly Excess Returns of Apple and Market Risk Premium and Posterior distribution of alpha and beta
4100,"S&P500 Monthly Returns, Posterior median of alpha and beta with confidence bands"
4101,"Using predictive regression model QBLL to obtain time-varying betas, and time-varying volatility"
4102,"This Quantlet contains one of two analyses that explore the relationship between weather data and stock returns. TrainNeuralNetwork.py sequentially runs different configurations of neural networks, aiming to predict stock returns from weather data."
4103,This Quantlet contains one of two analyses that explore the relationship between weather data and stock returns. RunOLSandARMA.py estimates parameters of different linear and autoregressive models.
4104,Testing Missing Completely at Random using Instrumental Variables
4106,Estimate the parameters in a Varying-Coefficient CARE Model using local linear regresion.
4107,Clustering Ethereum Smart Contracts
4108,Scrape the Etherscan API to get source code of smart contracts given the list of their hashes
4109,"From Source codes of Ethereum Smart Contracts create a data frame with features such as comments, source code, functions"
4110,Produces the estimation results derived from the joint modeling of IE dynamics with macroeconomic factor. Graphic showing the common inflation factor and the model residuals.
4111,Shows the estimation results derived from the AFNS model in a multi-maturity term structure for France. Graphic showing the filtered and predicted state variables.
4112,Produces the estimation results derived from the joint modeling of IE dynamics. Graphic showing the common inflation factor and the model residuals.
4113,Compares the estimated three-year and five-year forecast of IE for each European country. The inflation expectation (IE) is estimated by model-implied BEIR and plotted in gray.
4114,Shows the estimation results derived from the AFNS model in a multi-maturity term structure for Sweden. Graphic showing the filtered and predicted state variables.
4115,"Plots the breakeven inflation rate (BEIR)across five different industrialized European countries - U.K., Germany, France, Italy and Sweden."
4116,Shows the estimation results derived from the AFNS model in a multi-maturity term structure for Italy. Graphic showing the filtered and predicted state variables.
4117,Plots the model residuals estimated from AFNS model in a multi-maturity term structue for all the five sample countries.
4118,Shows the estimation results derived from the AFNS model in a multi-maturity term structure for U.K.. Graphic showing the filtered and predicted state variables.
4119,Shows the estimation results derived from the AFNS model in a multi-maturity term structure for Germany. Graphic showing the filtered and predicted state variables.
4120,"Plots cumulative returns of 3 TEDAS_gestalts for Mutual funds and German stocks samples against benchmark strategies: Risk-parity, Mean-variance OGARCH and 60/40 portfolio, as well as corresponding cores (DAX30 - for German stocks sample,  S&P500 - for Mutual Funds)"
4121,produces autocorrelation plots for 4 different hedge funds returns
4122,"Testimates the cumulative return for 4 strategies from the latest update of the TEDAS project: TEDAS Advanced, TEDAS Expert, TEDAS Naive and RR"
4123,"estimates the model dimension by Lasso quantile regression in the TEDAS approach for different market benchmarks: S&P 500, NIKKEI225, FTSE100, DAX30, UPRO, TQQQ"
4124,"estimates the cumulative return for 3 strategies from the TEDAS project: with  Cornish-Fisher dynamic Value-at-Risk, expected utility and risk-return allocations for daily data"
4128,"Constructs latex table with performance measures of of 3 TEDAS gestalts for Mutual funds and German stocks samples and benchmark strategies : Risk-parity, Mean-variance OGARCH and 60/40 portfolio, as well as corresponding cores (DAX30 - for German stocks sample,  S&P500 - for Mutual Funds). The following traditional performance and risk measures for allocation strategies are presented : cumulative returns, Sharpe ratios, Maximum drawdowns"
4130,"estimates the dynamic Value-at-Risk for different market benchmarks: S&P 500, NIKKEI225, FTSE100, DAX30, UPRO, TQQQ"
4133,"Estimates machine learning models for one or more data sets with binary classification task. Data sets are split into training and testing data, models are estimated on the training data and stored. For each data set, model predictions on training and testing data are stacked into dataframes which are also stored."
4153,"Contains the data preparation and the EWMA based derivation of VCRIX, volatility index for crypto-currencies on the basis of CRIX."
4154,"Contains the graph of CRIX and VCRIX with respective scales, and a graph of VCRIX and Bitcoin returns over the course of May 2016 to May 2018."
4156,"creates data frames of merged census data, crime data, taxi data separately for violent and property crime"
4158,Takes a data file with census variables and another file with crime data to create correlation plots and maps
4160,takes a cleaned violent crime file and estimates five models in eight different variable settings
4164,This file takes aggregated violent crime counts for New York City and plots them on a map of census tracts
4166,This file takes aggregated property crime counts for New York City and plots them on a map of census tracts
4168,This file takes a database with a taxi trip destinations and origins and creates a map of the flows
4170,"For any given set of predictions and a known ground truth, this file evaluates the predictions in different ways by optimizing cutoffs and bagged model selection"
4171,takes a cleaned property crime file and estimates five models in eight different variable settings
4175,Gets Foursquare data from API for each NYC census tract and creates a plot for each category
4176,"After having obtained tweet ids from the Datorium Gesis repository, this file calls the Twitter API to obtain the rehydrated tweets"
4177,"Using a spatial CAR model, this file evaluates the best combination of features on a hold-out samples within the full training sample"
4179,Takes census data and subsets it to relevant New York City census tracts
4180,Kalman filter and DCS-t filter.
4184,"Heckman correction and Huberization. Due to the size of datasets, they will be not uploaded here."
4185,"Quarterly sales in USD millions of post war, contemporary, ultra contemporary, NFT art markets, plotted in order. The dotted line is the cumulative sales on NFT."
4186,Preliminary analysis for NFT data.
4187,QQ plot and box plot in time series.
4189,illustration of score and degree of freedom.
4190,Test autocorrelation and causality.
4191,"Use a GARCH(1,1) model and a constant mean to standardize the data input, the log returns of CRIX"
4192,"Shows the approximations of VaR, ES and level ratio of expectile vs. quantile with varying epsilon and given alpha"
4193,"Times series of log returns of CRIX and normal Q-Q plot, and Mean excess function and log Mean excess function for both tails"
4194,"Shows the approximations of VaR, ES and level ratio of expectile vs. quantile with varying alpha and given epsilon"
4195,"Shows the approximations of VaR, ES with varying alpha and epsilon, a power function of alpha with exponent tau,
based on the pre-supposed ideal standard normal model, or the contamination model H"
4196,"Shows small relative error of VaR, ES with the estimation given by the influence function and small epsilon"
4197,"Obtains estimations of parameters involved in the normal-Laplace contaminations by expectation-maximization (EM) algorithm
Gives estimations of VaR and ES by historical simulations, based on the Laplace distribution and mixed model. Shows the tails of log returns of CRIX
during 2014.04-2018.01 is heavier than that before, and the approximation based on Laplace at scaled level performs well"
4206,Computes a monotonic regression using pool-adjacent-violators (PAV) algorithm.
4219,"Shows a scree plot of the eigenvalues for the PCA of the standardized Swiss bank notes. Additionally, it computes the correlations between the variables and the principal components and displays the first two of them."
4241,Plots the initial configuration of the points for nonmetric MDS for car brands data.
4253,"Computes the effect of weight and the displacement on the mileage. Uses linear regression to extract the coefficients of the regressors. Additionally, the program checks whether the origin of the car has an effect on the response while considering different levels."
4289,Simulates a spiral dataset and clusters it in four groups by means of a spectral clustering algorithm. It plots the simulated data and its clusters
4333,"Computes Andrew''s Curves for the observations 96-105 of the Swiss bank notes data. The order of the variables is 1,2,3,4,5,6."
4367,Illustrates the PAV algorithm for nonmetric MDS for car brands data.
4373,Performs a canonical correlation analysis for the car brands data and shows a plot of the first canonical variables.
4383,"Estimates a common principal components model for the implied volatility data and computes a likelihood ratio test. The computation of the common principal components uses a part of the code of the function ""FCPC"" that belongs to the R-Package ""multigroup"" (https://CRAN.R-project.org/package=multigroup)."
4431,Shows the first iteration of the PAV algorithm for nonmetric MDS for car brands data.
4447,"Convert quantlets, Source code and metainfo files, to pandas data frame "
4448,"We extracts essential parts of code (comments, function names, argument names) from the source code"
4449,This script loads the quantlets from LvB organization on Github. You need to provide your API token a token.txt file
4450,"Shows a time series of the percentage of tenure positions for U.S. institutional organizations (universities, collages). It also includes the amount of doctorate holders over time."
4451,This Boxplot plots the dependence between tenure status and the degree of satisfaction with intellectual challenge by different age groups. We also plot the arithmetic mean as a additional measurement. 
4452,"Plots three probability density functions (top) and three cumulative density functions (bottom) of the Laplace-distribution with different scale (L1 stands for Laplace-distribution with scale 1, etc)."
4453,Analysis the regulatory risk of cryptocurrency market using the scrapped news data from Coindesk.com
4454,Plots MDS representation of k-medoids clustering with cluster labeling via the most frequent terms of cluster centroids for the LSA model of YAML text corpus and gives some cluster info.
4455,"Cleans YAML text corpus containing Quantnet metainfo.txt files from empty fields, merges similar fields and plots the number of documents containing each field."
4456,Plots heatmaps of the SVD components and weights of the singular values for the LSA model of YAML text corpus.
4457,"Determines the proper sign of LSA components and extracts the top words of each component for YAML text corpus. The (positive and accordingly negative) part is chosen from those terms, where the biggest subtotal is concentrated. At the end top 3 words are taken, the sign is always changed to +, their values are multiplied by the singular value for the proper scaling and then rounded. Returns a list."
4458,"Plots heatmaps of the original matrix, the truncated matrix and the error matrix for the LSA model of YAML text corpus."
4459,Plots the number of char symbols of all different fields in YAML text corpus containing Quantnet metainfo.txt files.
4460,Hybrid model for Electricity Price Forecasting EPF
4463,"This folder contains the quantlet for the creation of the CRIX stablecoin for the master thesis ""In search for stability in crypto-assets: An Index-Pegged Stablecoin"""
4480,"Simulates the market mechanism (blind double auction), which was implemented by Mengelkamp et al. (2018) in smart contract on a private Ethereum Blockchain, with real consumption and production data in three different supply scenarios (balanced, over-, and undersupply) and with true and predicted energy consumption values."
4501,Generates plots and tables to evaluate the outcomes of three market simulations of a blind double auction as implemented in a smart contract by Mengelkamp et al. (2018) with and without predictions of energy consumption values.
4515,"Calculates and plots the no. of different test steps per time point for the two approaches of LCP. The results from LCP correspond to the approach with pre-simulated critical values and the approach with adaptively simulated critical values. In addition, the dynamics of the difference in the ML criterion are plotted as well. The estimated model is a three-dimensional HAC with Gumbel generators."
4516,"Plots the dynamics of structure, the dependence parameters, ML and the length of the intervals of homogeneity for the LCP estimation results based on pre-simulated critical values (on the one hand) and adaptively simulated critical values (on the other hand). The estimated model is a three-dimensional HAC with Gumbel generators. The underlying data corresponds to residuals from fitting GARCH (1,1) to log returns of DAX, Dow Jones and Nikkei."
4517,"Plots the BIC calculated in a rolling window with r = 250 for hierarchical Archimedean copula (HAC), Archimedean copula (AC) and Gauss copula. The HAC and AC are based on Gumbel generators. In addition, the plot includes the dynamics of the parameters of HAC. It includes dots where the structure of HAC changes and the L2 norm of the difference in the matrix of dependence parameters. The data underlying the BIC are residuals from fitting GARCH(1,1) to log returns of DAX, Dow Jones and Nikkei."
4518,"Calculates and plots Kendalls tau and Pearsons correlation coefficient in a rolling window of m = 250 observations for the pairs of residuals from fitting GARCH(1,1) to the log returns of indices (DAX, Dow Jones and Nikkei)."
4519,Plots the distribution of the adaptively simulated critical values around the respective pre-simulated critical value for all test steps of LCP. The red points mark the pre-simulated critical values.
4520,Scraping tool to gather raw code from all available .R scripts in the Quantlet repository.
4522,"Use HDBSCAN clustering algorithm to cluster spatially heterogeneous university points in lat, lng space within Berlin, DE."
4524,"Example Quantlet for charting spatial data collected for this project. This file plots all transit data for the city of Berlin, DE."
4525,"Ordinary Least Squares regression (OLS) for analysis of correlation between real estate listing prices and social / location-based features for real estate within Berlin, DE."
4526,"Use HDBSCAN clustering algorithm to cluster spatially heterogeneous medical points in lat, lng space within Berlin, DE."
4528,"Geographically Weighted Regression (GWR) for analysis of correlation between real estate listing prices and social / location-based features for real estate within Berlin, DE, taking spatial colinearity into consideration."
4529,"Plotting script for visualising the growth in property sale prices in Berlin, DE over the last 11 years (by district)."
4530,"Uses Markov Chains and LSTMs to generate new text in the style of some input corpus. Includes the weights for a pretrained LSTM model for Donald Trump tweets. Both techniques can in principle be used for any kind of natural language. The LSTM model should also be able to deal with highly structured text such as computer code - but that might require more nodes per layer and more training time. If you get an error, make sure you have both tensorflow installed and numpy upgraded to the latest version."
4532,"This quantlet shows the convergence of cryptocurrencies over time, by computing Likelihood Ratio from binary logistic regression."
4541,Produces the Ensemble XGB model that creates artificial features during training. Model is tested using real data for Polish companies
4542,"COPretaparch fits a AR(1)-GJR-GARCH(2,1) model to the daily returns of the indices Dow Jones (DJ) and DAX and a AR(1)-GJR-GARCH(1,1) model to the daily returns of the stocks of Volkswagen (VW) and Thyssen-Krupp (TK). The considered time span is 26.08.2005 to 13.08.2015. It returns the mu, the parameters of the model, skewness and shape, Ljung-Box and the Kolmogorov-Smirnov test statistic. The corresponding standard deviations are given in the line below the values. Finally, the parameters of the distribution of the obtained residuals are estimated."
4543,"Plots the p-values obtained from a rank-based Goodness-of-Fit test applied to moving windows of size 250 testing Frank, Gumbel, and  Clayton to be the adequate copula to model the dependence between series of AR-GJR-GARCH residuals from returns of DAX and DJ as well as from returns of Volkswagen and Thyssen-Krupp. The dependence parameter of the tested model was estimated using Maximum Likelihood. The considered time span is 26.08.2005 to 13.08.2015."
4544,Simulates 150 random samples of size 500 from a bivariate Clayton copula and estimates the dependence parameter theta via maximum likelihood and the inversion of Kendall's tau assuming a Gumbel copula. The obtained estimates are summarised in a table.
4545,"Summarises the results of the simulation study for the samples from convex sums of Gumbel and Clayton. Estimates for the copula dependence parameter theta were obtained using ML, the inversion of Kendall's tau, and a p-value weighted average of the two in combination with the Frank, the Gumbel, and the Clayton copula. The value of alpha s indicated in the name of each data  matrix. Standard deviations are given in the lines below the respective values."
4546,"COPapp2residual shows pairwise scatter plots of based on AR(1)-GJR-GARCH(1,1) / AR(1)-GJR-GARCH(2,1) fitted residuals. The upper triangular plots show the pairwise residuals scatter points. The lower  triangular plots show the scatter points computed from empirical cdf of the residuals. The companies Volkswagen (VW) and Thyssen-Krupp (TK) as well as the stock indices DAX and DJIA are contained."
4547,"Returns boxplots of the estimates obtained for the copula dependence parameter in a simulation study using ML, the inversion of Kendall's tau and a p-value weighted average of the two estimates in combination with the Gumbel and the Clayton copula. True copula and size of simulated random sample are indicated by the title of each plot. Distribution of estimates obtained using a misspecified copula are illustrated by blue boxplots, those obtained with the the true copula are depicted by grey boxplots."
4548,"Calculates the ratio of exceedances and the relative distance between theoretical and empirical alpha for VaR estimates obtained using the maximum likelihood estimator, the inversion of Kendall's tau and a p-value weighted average of the two in combination with the Frank, the Gumbel, and the Clayton copula for a series of AR-GJR-GARCH residuals from returns  of Volkswagen and Thyssen-Krupp. The considered time span is 26.08.2005 to 13.08.2015. The moving window is 250."
4549,"Plots the dynamically estimated dependence parameter theta over time. The dependence between series of AR-GJR-GARCH residuals from returns of DAX and DJ as well as from returns of Volkswagen and Thyssen-Krupp was estimated using the Frank, the Gumbel and the Clayton copula in combination with maximum like- likelihood, the inversion of Kendall's tau and a p-value weighted average of the two. The considered time span is 26.08.2005 to 13.08.2015. The moving window is 250."
4550,"Calculates the ratio of exceedances and the relative distance between theoretical and emprirical alpha for VaR estimates obtained using the maximum likelihood estimator, the inversion of Kendall's tau and a p-value weighted average of the two in combination with the Frank, the Gumbel, and the Clayton copula for a series of AR-GJR-GARCH residuals from returns  of DJ and DAX. The considered time span is 26.08.2005 to 13.08.2015. The moving window is 250."
4551,"Plots the percentage relative bias of the estimates obtained for the copula dependence parameter theta in a simulation study using ML, the inversion of Kendall's tau, and a p-value weighted average of the two in combination with the Gumbel and the Clayton copula. The observations come from convex sums of Gumbel and Clayton. The estimator is indicated by line colour and shape, the copula used in estimation is indicated by the fill colour of each shape."
4552,"Plots the dependence parameter theta as a function of Kendall's tau for the Frank, the Gumbel, the Clayton, the Joe, the t-EV, the Galambos, the Husler-Reiss, and the Plackett copula."
4553,"Summarises the results of the simulation study for the large samples. Estimates for the copula dependence parameter theta were obtained using ML, the inversion of Kendall's tau, and a p-value weighted average of the two in combination with the Frank, the Gumbel, and the Clayton copula. The true copula is indicated by the name of each data matrix. Standard deviations are given in the lines below the respective values."
4554,"Plots the percentage relative bias of the estimates obtained for the copula dependence parameter theta in a simulation study using ML, the inversion of Kendall's tau, and a p-value weighted average of the two in combination with the Frank, the Gumbel, and the Clayton copula. True copula is indicated by the title of each plot. Some estimates were obtained under  mis- specification. The estimator is indicated by line colour and shape, the copula used in estimation is indicated by the fill colour of each shape."
4555,"Plots the difference between the dynamically estimated estimated for dependence parameter theta using maximum likelihood, the inversion of Kendall's tau and a p-value weighted average of the two in combination with the Frank, the Gumbel, and the Clayton copula for a series of AR-GJR-GARCH residuals from returns  of DAX and DJ as well as from returns of Volkswagen and Thyssen-Krupp. The considered time span is 26.08.2005 to 13.08.2015. The moving window is 250."
4556,"Contains the Value strategy for the cross-sectional portfolio, which involves buying (selling) assets that are undervaluated (overvaluated) relative to a measure of fundamental value.  For our approach, we apply the algorithm presented by Baz et al. (2015) to generate the trading signal, based on the fundamental value represented by the real exchange rate. Based on the generated trading signals, we create the cross sectional portfolio."
4559,"Contains the Momentum strategy for the cross-sectional portfolio. For our approach, we apply the algorithm presented by Baz et al. (2015) to generate the momentum signal, based on three crossovers of exponential moving averages with different time horizons. The three different crossovers identify short-, medium-, and long-term trends respectively. A signal is generated for each time horizon and then all these signals are combined to build a trade signal. Based on the generated trading signals, we create the portfolio with weights prespecified by user."
4562,"Contains the Carry trade strategy for foreign exchanges, which involves borrowing and subsequently selling a low-interest currency to fund the purchase of a higher-yielding currency."
4565,Plots of Dirichlet distribution.
4568,Understanding Cryptocurrencies
4569,Performs the Unified term structure of exchange rates in solving the exchange rate forecasting puzzle in short forecasting horizon.
4570,Uplifted hierarchical risk parity based FRM portfolio construction.
4588,"Computes normalized principal components for US crime data set which consists of the reported number of crimes in the 50 US states in 1985. The crimes were classified according to 7 categories: murder, rape, robbery, assault, burglary, larceny, and auto theft. The data set also contains identification of the resion: Northeast, Midwest, South, West. After scaling the variables, a NPCA is perfomed the reported felonies. A scatterplot of the first two principal components, a screeplot and a plot of the correlations of the first two PCs with the original variables."
4601,Employs the Ward method and complete linkage using squared Euclidean distance matrices to perform a cluster analysis on an 8 points example.
4633,Computes kernel density estimates of the diagonal of the genuine and forged swiss bank notes. The bandwidth parameter are chosen by Silverman's rule of thumb.
4657,"Computes Fisher's linear discrimination function (LDA) either for the 20 bank notes from Exercise 12.6 or for a random set of banknotes. The discrimination function is then applied to the the entire bank data set. With the linear discrimination function based on the 20 bank notes from Ex. 12.6 only 6 bank notes out of the entire 200  are misclassified, which leads to an error rate of 3 percent."
4681,"Performs a factor analysis on the variables 'Length','Height Left','Height Right','Inner Frame Lower', 'Inner Frame Upper' and 'Diagonal' in the bank data set using iterated principal factors method (iPFM) with and without manual rotation by an angle of 7*pi/12 counterclockwise. Estimated factor loadings, communalities and specific variances are presented in a table, plots of original and rotated factor loadings are given"
4715,"calculates the covariance matrix (and eigevalues) of the Swiss Bank (bank2.dat) dataset and the variance of the counterfeit bank notes (observations 101-200) after they were lineary transformed by a vector a = (1, 1, 1, 1, 1, 1)"
4721,Reads the Swiss bank notes data and spheres it to run Fisher's LDA and PC projection on it.
4748,"computes Fisher's linear discrimination function for the wais data set. The apparent error rate (APER) and the actual error rate (AER) are computed. APER = 0.2449, AER = 0.3061."
4750,"Tests equality of matrices in Härdle & Simar (2015), Exercise 7.14. Test of equal covariance matrices of 4 subtests of the Wechseler Adult Intelligence Scale (WAIS) for 2 categories of people. group 1: n1=37 without senile factor, group 2: n2=12 with senile factor."
4761,"Estimates the canonical variables for a subset of variables of the car marks data set: X - (price, value), Y - (economy, service, design, sportiness, safety, easy handling). A scatterplot of the second 2 canonical variables are presented. The relationship between the two canonical variables is moderate."
4778,"Firstly, a normally distributed white noise is simulated. Then, the Ljung-Box statistic is computed for several lags."
4779,"The OLS residuals are computed for a simulated VARMA(1,1) model in state space form."
4780,An ARIMA model for the airline series of Box and Jenkins (1976) is identified and estimated.
4781,"Firstly, a time series following a VARMA model is simulated. Then, the model is estimated by the Hannan-Rissanen method."
4782,"A series following an ARMA(2,1) is simulated and the theoretical and sample autocorrelations and partial autocorrelations are computed."
4783,"A transfer function model is estimated. The series is tf2, documented in the software package SCA."
4784,A structural model for the airline series of Box and Jenkins (1976) is estimated and Kalman smoothing is used to estimate the components.
4785,"A VARMA(0,3)(0,1) is identified for the two series in Example 6.1 of Tsay (2014)."
4786,"A VARMA(0,3)(0,1) with missing observations is estimated for two series after differencing with one regular and one seasonal difference. The data are taken from Example 6.1 in Tsay (2014)."
4787,The canonical decomposition of an ARIMA model is performed based on the partial fraction decomposition of the                  pseudospectrum.
4788,"An ARIMA model for the ozone series of Box and Tiao (1975) is identified using the BIC and AIC criteria, both computed by means of the profile likelihood."
4789,A smooth trend is estimated for the quarterly IPI series of USA.
4790,A Spanish series of trade balance is used to illustrate the canonical decomposition.
4791,A VAR(2) model is estimated. This is Example 2.3 in Tsay (2014).
4792,"The recursive residuals are computed for a VARMA(4,1) model in state space form."
4793,A cycle is estimated for the quarterly IPI series of USA using a structural model.
4794,A multivariate structual model with common slopes for eight series of the housing market in the USA is estimated.
4795,Several ARIMA and transfer function models are handled in one single run of the program.
4796,A structural model with complex seasonal patterns is estimated. The series is used in Delivera et al (2011).
4797,The four series of Example 6.2 in Tsay (2014) are considered. A VARMAX model is identified and estimated.
4798,An MA(1) model is identified and estimated for the three series of Example 3.2 in Tsay (2014).
4799,A VARMA model in echelon form is identified and estimated for the data in Nsiri and Roy (1996)
4800,A multivariate state space model for the temporal disaggregation of the Spanish GDP is estimated.
4801,"Firstly, a time series following a transfer function model with one input is simulated. Then, the model is identified and estimated."
4802,"Firstly, a time series following VARMAX model is simulated. Then, the model is estimated by the Hannan-Rissanen method."
4803,The smoothed periodogram of the cycle of the quarterly German IPI series is computed and displayed.
4804,A macroeconomic model with a common cycle is put into state space form and estimated.
4805,A multivariate structural model with a common slope is estimated.
4806,"Firstly, a time series following structural model is simulated. Then, the model is estimated."
4807,"The first three covariance matrices of a VARMA(2,1) are computed. Example 3.7 in Tsay (2014)."
4808,A multivariate structural model with a common slope is estimated and a smooth cycle is obtained by means of a band pass filter.
4809,A white noise series with 100 observations normally distributed with zero mean and unit variance is simulated.
4810,The autocovariances of a vector moving averrage process of order one are first computed and then the spectral factorization of these covariances is performed using two methods.
4811,"Firstly, a time series following a MA(1) model is simulated. Then, some cross correlation matrices are computed."
4813,A VAR and a VARMA model for the data in Exercise 8.2 of Reinsel (1997) are estimated
4814,"The echelon form of a VARMA, VARMAX or time invariant state space model is computed by polynomial methods."
4815,The Example in Paragraph 5.11 in Tsay (2014) is considered. 
4816,"A VARMA(0,3)(0,1) is estimated for two series after differencing with one regular and one seasonal difference. This is Example 6.1 in Tsay (2014)."
4817,A spectral analysis of three series is performed to see if two of them lead or lag the third one.
4818,A VAR model for the Mink and Muskrat data is identified and estimated. See Reinsel (1997)
4819,"Firstly, a time series following an ARIMA model is simulated. Then, the model is identified and estimated."
4820,"Firstly, a normally distributed white noise is simulated. Then, the Portmanteau statistic is computed for several lags."
4821,Analysis the word evolution of Cryptocurrency regulation news during 2013-2018 using Dynamic Topic Model 
4823,Compare two Christmas Songs with their frequency words and their topics (LDA)
4824,word cloud of the president of the People's Republic of China Xi Jinping's speech in 19th National Congress of the Communist Party of China
4826,"Analyse the word frequency and the topic distribution of Shakespeare's works: Hamlet, Julius Caesar and Romeo and Juliet"
4830,Simulated article using different Dirichlet hyperparameters to show the mechanism of LDA
4831,"LDA application with NASDAQ news, the word cloud, and word frequency"
4834,"Loads original hurricane data, adjust data (order, missing values, delete zeros), compute and plot annual expectile curves"
4835,Change point test with hypothesis that mean functions of hurricane expectile curves do not change with year
4836,Plots the squared norms of normalized differences of mean functions for expectile curves of hurricanes for differentlevels of tau. Plots the norm of estimated slope functions beta of hurricane data and plots the estimated beta functions.
4837,Appliaction of a simple GAN framework with Wasserstein loss on CRIX stacking images
4838,Time series data preparation for simulation and analysis with GAN. Visualisation with stacking image of the CRIX time series both for the original series and its log returns.
4839,"Estimating a Multivariate Conditional Auto-Regressive Value at Risk model (MV-CAViaR) requires adaptation to possibly time varying parameter. Following the strategy of Spokoiny (2009) one can consider a sequence of included time intervals with the same end point, testing each of them agains the largest included one for homogeneity. Performing the tests subsequently one can choose the largest interval that is not rejected for estimation in order to have the least variance with moderately small bias. The standard way to test homogeneity is via Change Point detection. The critical values can be estimated using Multiplier Bootstrap procedure Spokoiny, Zhilova (2013). Based on a joint work with Xiu Xu and Wolfgang Härdle we implement interval homogeneity test with bootstrap-simulated critical values."
4847,"This repository contains supplementary material for the paper ""Soul of the Community: An Attempt to Assess Attachment to a Community"" to enable the reader to reproduce the analysis."

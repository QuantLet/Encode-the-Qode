{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jd7S7ZIIS6Fn"
   },
   "source": [
    "################################################################################\n",
    "# 1. Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QjyD6T4D1XM4"
   },
   "outputs": [],
   "source": [
    "%pip install levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g4K0YWK11RjR"
   },
   "outputs": [],
   "source": [
    "QPATH = \"Quantlet/3-data-preprocessing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xz1ksIsd1SYO"
   },
   "outputs": [],
   "source": [
    "# PREPARE WORKING DIRECTORY\n",
    "\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "import os\n",
    "if IN_COLAB:\n",
    "  os.chdir(f'/content/drive/MyDrive/ColabNotebooks/IRTG/Encode_the_Qode/Encode-the-Qode/{QPATH}')\n",
    "else:\n",
    "  os.chdir('./')\n",
    "\n",
    "#sys.path.append('../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TNkVUfU21UsR"
   },
   "outputs": [],
   "source": [
    "# PACKAGES\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "import re\n",
    "import sys\n",
    "from IPython.display import display\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import importlib\n",
    "import preprocessing_utils\n",
    "importlib.reload(preprocessing_utils)\n",
    "from preprocessing_utils import *\n",
    "\n",
    "#import importlib\n",
    "#importlib.reload(preprocessing_utils)\n",
    "#from preprocessing_utils import *\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from Levenshtein import distance\n",
    "\n",
    "# SETTINGS\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "pd.set_option('display.max_colwidth', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46Fkzpboq6sy"
   },
   "source": [
    "# 2. Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E-8ZANoE1bRo"
   },
   "outputs": [],
   "source": [
    "#with open('../../data/preprocessed/Quantlet/Parsed_Qs_with_code_25062023.pkl', 'rb') as file:\n",
    "#  df = pickle.load(file)\n",
    "with open('../../data/preprocessed/Quantlet/Qs_reduced_23092023.pkl', 'rb') as file:\n",
    "  df = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xGutX1P3okqN"
   },
   "outputs": [],
   "source": [
    "RS = 42\n",
    "CLEAN_ALL = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6iRcIcEg39Ks"
   },
   "outputs": [],
   "source": [
    "df = df[df.metainfo_file!='empty']\n",
    "print(df.shape)\n",
    "\n",
    "# Parse metainfo file\n",
    "if 'Keywords' not in df.columns:\n",
    "  meta_info = pd.DataFrame(columns=['Quantlet', 'Description', 'Keywords', 'Other'])\n",
    "\n",
    "  meta_info[['Quantlet', 'Description', 'Keywords', 'Authors', 'Other']] = df.apply(\n",
    "      lambda x: parse_meta(x),\n",
    "      axis='columns',\n",
    "      result_type='expand'\n",
    "      )\n",
    "\n",
    "  for col in meta_info.columns:\n",
    "      meta_info[col] = meta_info[col].astype(str)\n",
    "\n",
    "  df = pd.concat([df, meta_info], axis=1)\n",
    "\n",
    "  del df['metainfo_file']\n",
    "  del df['Other']\n",
    "  del df['script_name_no_ext']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cukmnLJ8F5GF"
   },
   "outputs": [],
   "source": [
    "# PREPARE THE SCRIPT\n",
    "df['code_script'] = df['code_script'].apply(lambda x: [line for line in x if len(line)>0])\n",
    "df['code_script'] = df['code_script'].apply(lambda x: ' '.join(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aI_G9aGQ8bY_"
   },
   "outputs": [],
   "source": [
    "df['scr_n'] = df['code_script'].apply(len)\n",
    "df['description_len'] = df['Description'].apply(len)\n",
    "df['description_n_words'] = df['Description'].apply(lambda x: len(x.split()))\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Reset Index\n",
    "df_long = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ddH64easIa7T"
   },
   "outputs": [],
   "source": [
    "# ADD REPO INFORMATION\n",
    "df_long['repo'] = df_long['folder_name'].str.split('QuantLet/', expand=True)[1].str.split('/', expand=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HSsWU3MmGSj9"
   },
   "outputs": [],
   "source": [
    "df_long.repo.value_counts()\n",
    "\n",
    "# 4 groups\n",
    "\n",
    "# no neighbors\n",
    "# less than 5 neighbors\n",
    "# between 5 and 10 neighbors\n",
    "# more than 10 neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b85fj5AA5Ac6"
   },
   "outputs": [],
   "source": [
    "# ANALYZE LENGTH OF THE CODE SNIPPET\n",
    "\n",
    "df_long['code_len'] = df_long['code_script'].progress_apply(len)\n",
    "\n",
    "# REMOVE CODE LINE DUPLICATES\n",
    "df_long['code_script'] = df_long['code_script'].progress_apply(remove_dup_lines)\n",
    "df_long['new_len'] = df_long['code_script'].progress_apply(len)\n",
    "\n",
    "# REMOVE TOO SIMILAR LINES\n",
    "# we want to get as much information\n",
    "df_long['code_script'] = df_long['code_script'].progress_apply(remove_too_similar_line)\n",
    "df_long['new_len'] = df_long['code_script'].progress_apply(len)\n",
    "\n",
    "# REMOVE TOO SIMILAR TOKENS\n",
    "df_long['code_script'] = df_long['code_script'].progress_apply(remove_too_similar_token)\n",
    "df_long['new_len2'] = df_long['code_script'].progress_apply(len)\n",
    "\n",
    "df_long = df_long.reset_index(drop=True)\n",
    "df_long = df_long.drop(list(df_long[df_long['new_len2']==0].index)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KouzsOuAlGX9"
   },
   "outputs": [],
   "source": [
    "#df_long['code_script'] = df_long['code_script'].progress_apply(cut_300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a5kfMIbOIsH6"
   },
   "outputs": [],
   "source": [
    "if CLEAN_ALL:\n",
    "    df_long['code_script'] = df_long['code_script'].progress_apply(greedy_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CmQLCYodJUqs"
   },
   "outputs": [],
   "source": [
    "df_long['Q_ID'] = df_long.index\n",
    "\n",
    "df_long.to_csv('../../data/preprocessed/Quantlet/full_20231007.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wTr5ZFiOKEND"
   },
   "outputs": [],
   "source": [
    "# SPLIT THE DATA\n",
    "labelled_qs, test_qs = train_test_split(list(df_long.Quantlet.unique()),\n",
    "                                     test_size=0.1,\n",
    "                                     random_state=RS)\n",
    "train_qs, val_qs = train_test_split(labelled_qs,\n",
    "                      test_size=0.1,\n",
    "                      random_state=RS)\n",
    "\n",
    "\n",
    "train = df_long[df_long['Quantlet'].isin(set(train_qs))].reset_index(drop=True)\n",
    "val   = df_long[df_long['Quantlet'].isin(set(val_qs))].reset_index(drop=True)\n",
    "test  = df_long[df_long['Quantlet'].isin(set(test_qs))].reset_index(drop=True)\n",
    "\n",
    "full_train = pd.concat([train, val], axis=0).sample(frac=1, random_state=RS).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UIINKvxN_0P3"
   },
   "outputs": [],
   "source": [
    "full_train.to_csv('../../data/preprocessed/Quantlet/full_train_df_20231007.csv', index=False)\n",
    "train.to_csv('../../data/preprocessed/Quantlet/train_df_20231007.csv', index=False)\n",
    "val.to_csv('../../data/preprocessed/Quantlet/val_df_20231007.csv', index=False)\n",
    "test.to_csv('../../data/preprocessed/Quantlet/test_df_20231007.csv', index=False)\n",
    "\n",
    "\n",
    "print(train.shape)\n",
    "print(train['type_script'].value_counts(normalize=True))\n",
    "print(val.shape)\n",
    "print(val['type_script'].value_counts(normalize=True))\n",
    "print(test.shape)\n",
    "print(test['type_script'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q1FyNqapMVUl"
   },
   "outputs": [],
   "source": [
    "for MODE in ['no_context', 'author', 'repo', 'both']:\n",
    "    full_train = pd.read_csv('../../data/preprocessed/Quantlet/full_train_df_20231007.csv')\n",
    "    train = pd.read_csv('../../data/preprocessed/Quantlet/train_df_20231007.csv')\n",
    "    val   = pd.read_csv('../../data/preprocessed/Quantlet/val_df_20231007.csv')\n",
    "    test  = pd.read_csv('../../data/preprocessed/Quantlet/test_df_20231007.csv')\n",
    "\n",
    "    # FIX NA\n",
    "    test.loc[test['Quantlet'].isna(), 'Quantlet'] = 'XFGexp_rtn_SRM_2d_DOENST RUN'\n",
    "    train['Authors'] = train['Authors'].fillna('Unknown')\n",
    "    val['Authors']   = val['Authors'].fillna('Unknown')\n",
    "    test['Authors']  = test['Authors'].fillna('Unknown')\n",
    "\n",
    "\n",
    "    if MODE=='both':\n",
    "      train.loc[:,'code_script'] = '# repo: ' + train['repo'] + '\\n '  + '# author: ' + train['Authors'] + '\\n '  + train['code_script']\n",
    "      val.loc[:,'code_script']   = '# repo: ' + val['repo']   + '\\n '  + '# author: ' + val['Authors']   + '\\n '  + val['code_script']\n",
    "      test.loc[:,'code_script'] = '# repo: ' + test['repo']  + '\\n '  + '# author: ' + test['Authors']  + '\\n '  + test['code_script']\n",
    "\n",
    "    elif MODE=='repo':\n",
    "      train.loc[:,'code_script'] = '# repo: ' + train['repo'] + '\\n ' + train['code_script']\n",
    "      val.loc[:,'code_script']   = '# repo: ' + val['repo'] + '\\n ' + val['code_script']\n",
    "      test.loc[:,'code_script'] = '# repo: ' + test['repo'] + '\\n ' + test['code_script']\n",
    "\n",
    "    #elif add_quantlet:\n",
    "    #   train.loc[:,'code_script'] = '# quantlet: ' + train['Quantlet'] + '\\n ' + train['code_script']\n",
    "    #   val.loc[:,'code_script']   = '# quantlet: ' + val['Quantlet'] + '\\n ' + val['code_script']\n",
    "    #   test.loc[:,'code_script'] = '# quantlet: ' + test['Quantlet'] + '\\n ' + test['code_script']\n",
    "\n",
    "    elif MODE=='author':\n",
    "      train.loc[:,'code_script'] = '# author: ' + train['Authors'] + '\\n ' + train['code_script']\n",
    "      val.loc[:,'code_script']   = '# author: ' + val['Authors'] + '\\n ' + val['code_script']\n",
    "      test.loc[:,'code_script'] = '# author: ' + test['Authors'] + '\\n ' + test['code_script']\n",
    "\n",
    "    train_dataset_json = {'version' : '1.1',\n",
    "                        'data' : [{'input_sequence'  : train['code_script'].iloc[i],\n",
    "                                    'output_sequence' : train['Description'].iloc[i]} for i in range(train.shape[0])]}\n",
    "    val_dataset_json = {'version' : '1.1',\n",
    "                        'data' : [{'input_sequence'  : val['code_script'].iloc[i],\n",
    "                                    'output_sequence' : val['Description'].iloc[i]} for i in range(val.shape[0])]}\n",
    "\n",
    "    full_train_dataset_json = {'version' : '1.1',\n",
    "                                'data' : [{'input_sequence'  : full_train['code_script'].iloc[i],\n",
    "                                            'output_sequence' : full_train['Description'].iloc[i]} for i in range(full_train.shape[0])]}\n",
    "\n",
    "    test_dataset_json = {'version' : '1.1',\n",
    "                        'data' : [{'input_sequence'  : test['code_script'].iloc[i],\n",
    "                                    'output_sequence' : test['Description'].iloc[i]} for i in range(test.shape[0])]}\n",
    "\n",
    "\n",
    "    with open(f'../../data/preprocessed/Quantlet/{MODE}/full_train_dataset_20231007.json', 'w') as f:\n",
    "        json.dump(full_train_dataset_json, f)\n",
    "\n",
    "    with open(f'../../data/preprocessed/Quantlet/{MODE}/train_dataset_20231007.json', 'w') as f:\n",
    "        json.dump(train_dataset_json, f)\n",
    "\n",
    "    with open(f'../../data/preprocessed/Quantlet/{MODE}/val_dataset_20231007.json', 'w') as f:\n",
    "        json.dump(val_dataset_json, f)\n",
    "\n",
    "    with open(f'../../data/preprocessed/Quantlet/{MODE}/test_dataset_20231007.json', 'w') as f:\n",
    "        json.dump(test_dataset_json, f)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPi/TOFNsA1zfJLBDfzjn9z",
   "mount_file_id": "1lM2sDoEx5jeUuR5QcYONErkcXVm-i9K0",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{"version": "1.0", "data": [{"input_sequence": "import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nfrom sklearn.metrics import mean_absolute_error\nfrom tensorflow.keras.layers import LSTM,GRU\nfrom tensorflow.keras.layers import Dense, SimpleRNN, Dropout,Flatten,Activation\nbtc_path = os.path.abspath(\"result_BTC.csv\")\nbtc_data=pd.read_csv(btc_path)\ndata = btc_data['close']\nclose_train=data.iloc[:len(data)-260]\nclose_test=data.iloc[len(close_train):]\nclose_train=np.array(close_train)\nclose_train=close_train.reshape(close_train.shape[0],1)\nfrom sklearn.preprocessing import MinMaxScaler\nscaler=MinMaxScaler(feature_range=(0,1))\nclose_scaled=scaler.fit_transform(close_train)\ntimestep=24\nx_train=[]\nfor i in range(timestep,close_scaled.shape[0]):\nx_train.append(close_scaled[i-timestep:i,0])\nx_train,y_train=np.array(x_train),np.array(y_train)\nx_train=x_train.reshape(x_train.shape[0],x_train.shape[1],1)\nprint(\"x_train shape= \",x_train.shape)\nmodel=Sequential()\nmodel.add(GRU(200,input_shape=(None,1),activation=\"relu\"))\nmodel.add(Dense(1))\nmodel.compile(loss=\"mean_squared_error\",optimizer=\"adam\")\nmodel.summary()\ninputs=data[len(data)-len(close_test)-timestep:]\ninputs=inputs.values.reshape(-1,1)\ninputs=scaler.transform(inputs)\nx_test=[]\nfor i in range(timestep,inputs.shape[0]):\nx_test.append(inputs[i-timestep:i,0])\nx_test=np.array(x_test)\nx_test=x_test.reshape(x_test.shape[0],x_test.shape[1],1)\npredicted_data=model.predict(x_test)\npredicted_data=scaler.inverse_transform(predicted_data)\ndata_test=np.array(close_test)\ndata_test=data_test.reshape(len(data_test),1)\nplt.figure(figsize=(8,4), dpi=80, facecolor='w',\nplt.plot(data_test,color=\"r\",label=\"true result\")\nplt.legend()\nplt.xlabel(\"Time(260 hours)\")\nplt.ylabel(\"Close Values\")\nplt.grid(True)\ntest_series = pd.Series(data_test.flatten().tolist())\ndata = eth_data['close']\n\nclose_scaled=scaler.fit_transform(close_train)\nclose_scaled.shape\ndata = bnb_data['close']\nclose_train.shape\n", "output_sequence": "use LSTM and GRU to optimize prediction"}, {"input_sequence": "!pip install requests\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas\nimport numpy as np\n!pip install matplotlib\nimport os\nfrom os import path\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\ncwd_dir = os.getcwd()\ntitle_txt=[]\nall_titles=[]\nheadline_all=[]\nfor i in range(0,10):\n#url='https://www.investing.com/commodities/crude-oil-news/'+'2'\n#https://www.reuters.com/news/archive/oil?view=page&page=2&pageSize=10\nr = Request('https://www.reuters.com/news/archive/oil?view=page&page='+str(i)+'&pageSize=10', headers={'User-Agent': 'Mozilla/5.0'})\n#print(r)\nresponse = urlopen(r).read()\n#print(response)\nsite = BeautifulSoup(response, \"html.parser\")\n#icontents = site.find_all(class_=\"js-external-link-wrapper articleItem\")\ncontents=site.find_all('article', {'class': 'story'})\n#print(contents)\ntitles=[]\nfor content in contents:\ncontent_title = content.find_all('h3')\nif content_title == []:\ncontinue\nnews_title = content_title[0].text.strip()\n#print(news_title)\ntitles.append(news_title)\nheadline_all.append(news_title+'\\n')\ntext=\"\".join(titles)\n#print(text)\nall_titles.append(text)\ntitle_txt.append( \"\\nPage\"+str(i)+\"\\n\"+text)\nprint(i)\nall_text=\"\".join(all_titles)\nprint(all_text)\nprint(headline_all)\nwith open(cwd_dir + '/Title_all_10_new.txt', 'w') as title_all_f:\ntitle_all_f.writelines(title_txt)\nwith open(cwd_dir+'/Headline_all_10_new.txt','w') as headline_all_f:\nheadline_all_f.writelines(headline_all)\n#all_text=all_text+text\nstopword=stopwords.words('english')\nnewStopWords=['hours','last','ago','year','despite','large','shares','well','following','shows','let','Ever','price','UPDATE','UK','Cooper','focus','say','says']\n#stopword=stopword.append('hours')\nstopword.extend(newStopWords)\nwc=WordCloud(mode='RGBA', stopwords=stopword,background_color=None)\nplt.figure(figsize=(50,50))\nplt.imshow(wc, interpolation='bilinear')\nplt.axis(\"off\")\n!pip install gensim\nimport re\nimport nltk\nnltk.download('punkt')\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.corpus import stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer\nimport string\nimport gensim\nfrom gensim import corpora\nimport pandas as pd\nd = os.getcwd()\ntext_pre = open(path.join(d, 'Headline_all_10.txt'), encoding = \"utf8\").read()\ndoc_l = str.split(text_pre, sep = 'Page')\ndoc_complete = doc_l\ndoc_out = []\nfor l in doc_l:\n\ncleantextprep = str(l)\n# Regex cleaning\nexpression = \"[^a-zA-Z ]\" # keep only letters, numbers and whitespace\ncleantextCAP = re.sub(expression, '', cleantextprep) # apply regex\ncleantext = cleantextCAP.lower() # lower case\nbound = ''.join(cleantext)\ndoc_out.append(bound)\nstop = set(stopwords.words('english'))\nstop.add('Page')\nexclude = set(string.punctuation)\nlemma = WordNetLemmatizer()\nnltk.download('wordnet')\ndef clean(doc):\nstop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\npunc_free = ''.join(ch for ch in stop_free if ch not in exclude)\nnormalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\nreturn normalized\ndictionary = corpora.Dictionary(doc_clean)\ndoc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\nLda = gensim.models.ldamodel.LdaModel\nldamodel = Lda(doc_term_matrix, num_topics=2, id2word = dictionary, passes=20)\nK=2\ntopicWordProbMat=ldamodel.print_topics(K)\ncolumns = ['1','2']\ndf = pd.DataFrame(columns = columns)\npd.set_option('display.width', 1000)\nzz = np.zeros(shape=(20,K))\nlast_number=0\nDC={}\nfor x in range (20):\ndata = pd.DataFrame({columns[0]:\"\",\ncolumns[1]:\"\",\n# columns[4]:\"\",\n},index=[0])\ndf=df.append(data,ignore_index=True)\nfor line in topicWordProbMat:\ntp, w = line\nprobs=w.split(\"+\")\ny=0\nfor pr in probs:\n\na=pr.split(\"*\")\ndf.iloc[y,tp] = a[1]\n\nif a[1] in DC:\nzz[DC[a[1]]][tp]=a[0]\nelse:\nzz[last_number][tp]=a[0]\nDC[a[1]]=last_number\nlast_number=last_number+1\ny=y+1\nprint (df)\nzz=np.resize(zz,(len(DC.keys()),zz.shape[1]))\nfor val, key in enumerate(DC.keys()):\nplt.text(-2.5, val + 0.5, key,\nhorizontalalignment='center',\n)\n#plt.imshow(zz, cmap='hot', interpolation='nearest')\n#plt.show()\nplt.yticks([])\nplt.savefig(\"heatmap_oil_news_10_new.png\", transparent = True)\nwords=str(cleantext).split()\ndict1 = {}\nfor word in words:\ndict1[word] = dict1.get(word,0) + 1\nkeys = list(dict1)\nfiltered_words = [word for word in keys if word not in stopwords.words('english')]\ndict2 = dict((k, dict1[k]) for k in filtered_words if k in filtered_words)\ndef SequenceSelection(dictionary, length, startindex = 0):\n# Test input\nlengthDict = len(dictionary)\nif length > lengthDict:\nreturn print(\"length is longer than dictionary length\");\nelse:\nd = dictionary\nitems = [(v, k) for k, v in d.items()]\nitems.sort()\nitems.reverse()\nitemsOut = [(k, v) for v, k in items]\nhighest = itemsOut[startindex:startindex + length]\ndd = dict(highest)\nwanted_keys = dd.keys()\ndictshow = dict((k, d[k]) for k in wanted_keys if k in d)\nreturn dictshow;\ndictshow = SequenceSelection(dictionary = dict2, length = 90, startindex = 0)\nn = range(len(dictshow))\nplt.figure(figsize=(20,10))\nplt.bar(n, dictshow.values(), align='center')\nplt.xticks(n, dictshow.keys(), rotation = 'vertical')\nplt.title(\"Most frequent Words\")\n#preprocess text\nfrom string import punctuation\npath = os.getcwd()\nf = open(os.path.join(path, 'Headline_all_10.txt'), 'rt', encoding='utf-8')\ntext_file = f.read().split('\\n')\ntext_lower = [text.lower() for text in text_file]\ntext_letters = [''.join(c for c in s if c not in punctuation) for s in text_lower]\ntext_final = [re.sub(r'[^A-Za-z]+', ' ', x) for x in text_letters]\nwith open(os.path.join(path, 'Headline_cleaned_10.txt'), 'w') as fw:\nfor text in text_final:\nfw.write(text)\n#hieratical\n!pip install scipy\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem.snowball import SnowballStemmer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.manifold import MDS\nfrom scipy.cluster.hierarchy import ward, dendrogram\ndef tokenize_and_stem(text_file):\n# declaring stemmer and stopwords language\nstemmer = SnowballStemmer(\"english\")\nstop_words = set(stopwords.words('english'))\nwords = word_tokenize(text_file)\nfiltered = [w for w in words if w not in stop_words]\nstems = [stemmer.stem(t) for t in filtered]\ndata = pd.read_csv(os.path.join(path, 'Headline_cleaned_10.txt'), names=['text'])\nstop_words = set(stopwords.words('english'))\ndata['text'] = data['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\ntfidf_vectorizer = TfidfVectorizer(max_features=200000,\nuse_idf=True,\ntokenizer=tokenize_and_stem)\ntfidf_matrix = tfidf_vectorizer.fit_transform(data['text'])\ndistance = 1 - cosine_similarity(tfidf_matrix)\nlinkage_matrix = ward(distance)\nfig, ax = plt.subplots(figsize=(15, 20))\nax = dendrogram(linkage_matrix, orientation=\"top\", labels=data.values)\nplt.tight_layout()\nplt.title('Oil News Headlines using Ward Hierarchical Method')\nfrom sklearn.cluster import KMeans\nterms = tfidf_vectorizer.get_feature_names()\nkm = KMeans(n_clusters=7, init='k-means++', max_iter=300, n_init=1, verbose=0, random_state=3425)\nkm.fit(tfidf_matrix)\nlabels = km.labels_\nclusters = labels.tolist()\nmds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=1)\npos = mds.fit_transform(distance)\nxs, ys = pos[:, 0], pos[:, 1]\nfor x, y, in zip(xs, ys):\nplt.scatter(x, y)\nplt.title('MDS output of Oil News Headlines')\nplt.savefig(os.path.join(path, 'MDS_10_new.png'))\ndf = pd.DataFrame(dict(label=clusters, data=data['text'], x=xs, y=ys))\ndf.to_csv(os.path.join(path, 'kmeans_clustered_DF_10_new.txt'), sep=',')\nlabel_color_map = {0: 'red',\n1: 'blue',\ncsv = open(os.path.join(path, 'kmeans_clustered_output_10_new.txt'), 'w')\ncsv.write('Cluster Oil Headline\\n')\nfig, ax = plt.subplots(figsize=(17, 9))\nfor index, row in df.iterrows():\ncluster = row['label']\nlabel_color = label_color_map[row['label']]\nlabel_text = row['data']\nax.plot(row['x'], row['y'], marker='o', ms=12, c=label_color)\nrow = str(cluster) + ',' + label_text + '\\n'\ncsv.write(row)\n# ax.legend(numpoints=1)\nfor i in range(len(df)):\nax.text(df.iloc[i]['x'], df.iloc[i]['label'], size=8)\nplt.title('Oil News Headlines using KMeans Clustering')\nembedding = LocallyLinearEmbedding(n_components=2)\npos = embedding.fit_transform(distance)\nplt.title('LLE output of Oil News Headlines')\nplt.savefig(os.path.join(path, 'LLE_10_new.png'))\ndf.to_csv(os.path.join(path, 'kmeans_clustered_DF_10_LLE_new.txt'), sep=',')\n", "output_sequence": "Project_Group11 file"}, {"input_sequence": "!pip install pandas\nimport types\nimport pandas as pd\nimport numpy as np\nfrom keras.preprocessing import sequence\ndf_data_1 =pd.DataFrame(pd.read_csv(\"./DCOILBRENTEU.csv\"))\ndf_data_1 = df_data_1[df_data_1.DCOILBRENTEU != \".\"]\n!pip install matplotlib\nimport matplotlib.pyplot as plt\ndf_data_1.iloc[:,1:2]=df_data_1.iloc[:,1:2].values.astype(np.float)\ndf_data_1_plot = df_data_1.iloc[:,1:2].values\ndf_data_1_plot\nplt.plot(df_data_1_plot, color = 'red', label = 'Crude Oil Prices')\nplt.title('Crude Oil Prices Historical Data')\nplt.xlabel('Time (Days)')\nplt.ylabel('Crude Oil Prices')\nplt.legend()\n# defining the batch size and number of epochs\nbatch_size = 64\nepochs = 200\nlength = len(df_data_1)\nprint(length)\n#test set will be 10% of entire data set\n#length *= 1 - 0.1\n#get the length of training data set\ndef get_train_length(dataset, batch_size, test_percent):\n# substract test_percent to be excluded from training, reserved for testset\nlength = len(dataset)\nlength *= 1 - test_percent\ntrain_length_values = []\nfor x in range(int(length) - 100,int(length)):\nmodulo=x%batch_size\nif (modulo == 0):\ntrain_length_values.append(x)\nprint(x)\nlength = get_train_length(df_data_1, batch_size, 0.1)\n#Adding timesteps * 2\nupper_train = length + timesteps*2\ndf_data_1_train = df_data_1[0:upper_train]\ntraining_set = df_data_1_train.iloc[:,1:2].values\n# Feature Scaling i.e we scale each and every value between 0 and 1\nfrom sklearn.preprocessing import MinMaxScaler\nsc = MinMaxScaler(feature_range = (0, 1))\ntraining_set_scaled = sc.fit_transform(np.float64(training_set))\nX_train = []\n# Creating a data structure with n timesteps\nprint(length + timesteps)\nfor i in range(timesteps, length + timesteps):\nX_train.append(training_set_scaled[i-timesteps:i,0])\nprint(len(X_train))\nprint(np.array(X_train).shape)\n# Reshaping\nX_train, = np.array(X_train),\nX_train = np.reshape(X_train, (X_train.shape[0], 1))\nprint(X_train.shape)\n# Building the LSTM\n# Importing the Keras libraries and packages\nfrom keras.layers import Dense\n# Initialising the LSTM Model with MAE Loss-Function\n# Using Functional API\ninputs_1_mae = Input(batch_shape=(batch_size,timesteps,1))\nlstm_1_mae = LSTM(10, stateful=True, return_sequences=True)(inputs_1_mae)\n#dropout_1=Dropout(0.5)(lstm_1_mae)\nlstm_2_mae = LSTM(10, stateful=True, return_sequences=True)(lstm_1_mae)\noutput_1_mae = Dense(units = 1)(lstm_2_mae)\nregressor_mae = Model(inputs=inputs_1_mae, outputs = output_1_mae)\nregressor_mae.compile(optimizer='adam', loss = 'mae')\n# 1st LSTM Layer\nparameters = 4 * 10 * (1 + 10 + 1)\n# 2nd LSTM Layer\nparameters = 4 * 10 * (10 + 10 + 1)\nfor i in range(epochs):\nprint(\"Epoch: \" + str(i))\nregressor_mae.fit(X_train, y_train, shuffle=False, epochs = 1, batch_size = batch_size)\n!pwd\ndef get_test_length(dataset, batch_size):\n\ntest_length_values = []\nfor x in range(len(dataset) - 200, len(dataset) - timesteps*2):\nmodulo=(x-upper_train)%batch_size\ntest_length_values.append(x)\ntest_length = get_test_length(df_data_1, batch_size)\nprint(test_length)\nupper_test = test_length + timesteps*2\ntestset_length = test_length - upper_train\n# construct test set\n#subsetting\ndf_data_1_test = df_data_1[upper_train:upper_test]\ntest_set = df_data_1_test.iloc[:,1:2].values\n#scaling\nscaled_real_bcg_values_test = sc.fit_transform(np.float64(test_set))\n#creating input data\nX_test = []\nfor i in range(timesteps, testset_length + timesteps):\nX_test.append(scaled_real_bcg_values_test[i-timesteps:i, 0])\nX_test = np.array(X_test)\n#reshaping\n#prediction\npredicted_bcg_values_test_mae = regressor_mae.predict(X_test, batch_size=batch_size)\nregressor_mae.reset_states()\nprint(predicted_bcg_values_test_mae.shape)\npredicted_bcg_values_test_mae = np.reshape(predicted_bcg_values_test_mae,\n(predicted_bcg_values_test_mae.shape[0],\n#inverse transform\npredicted_bcg_values_test_mae = sc.inverse_transform(predicted_bcg_values_test_mae)\n#creating y_test data\ny_test = []\nfor j in range(0, testset_length - timesteps):\ny_test = np.append(y_test, predicted_bcg_values_test_mae[j, timesteps-1])\n# reshaping\ny_test = np.reshape(y_test, (y_test.shape[0], 1))\n# Visualising the results\nplt.plot(test_set[timesteps:len(y_test)], color = 'red', label = 'Real Crude Oil Prices')\nplt.plot(y_test[0:len(y_test) - timesteps], color = 'blue', label = 'Predicted Crude Oil Prices')\nplt.title('Crude Oil Prices Prediction - MAE')\nplt.xlabel('Time')\nimport math\nfrom sklearn.metrics import mean_squared_error\nrmse = math.sqrt(mean_squared_error(test_set[timesteps:len(y_test)], y_test[0:len(y_test) - timesteps]))\nfrom sklearn.metrics import mean_absolute_error\nmae = mean_absolute_error(test_set[timesteps:len(y_test)], y_test[0:len(y_test) - timesteps])\nimport h5py\n", "output_sequence": "Project_Group11 file"}, {"input_sequence": "from collections import defaultdict\nimport os\nimport jieba\n#from snownlp import SnowNLP\ndef seg_word(sentence):\n\"\"\"\u4f7f\u7528jieba\u5bf9\u6587\u6863\u5206\u8bcd\"\"\"\nseg_list = jieba.cut(sentence)\nseg_result = [element for element in seg_list]\n#\u8bfb\u53d6\u505c\u7528\u8bcd\u6587\u4ef6\nstopwords = set()\nfr = codecs.open('stopwords.txt', 'r', 'utf-8')\nfor word in fr:\nstopwords.add(word.strip())\nfr.close()\n#\u53bb\u9664\u505c\u7528\u8bcd\nword_list=list(filter(lambda x: x not in stopwords, seg_result))\nprint(word_list)\n\n#\u8bfb\u53d6\u60c5\u611f\u8bcd\u5178\uff08\u9700\u8981\u6839\u636e\u8bcd\u9891\u7edf\u8ba1\u8fdb\u884c\u6269\u5145+\u7f51\u7edc\u8bcd\u5178\u7684\u5220\u51cf\uff09\ndef create_sendic():\n#\u8bfb\u53d6\u56fd\u7acb\u53f0\u6e7e\u7684\u5927\u5b66\u60c5\u611f\u8bcd\u5178\nsen_file1 = open('NTU_positive.txt', 'r+', encoding='utf-8')\nsen_list1 = sen_file1.read().splitlines()\nprint(len(sen_list1))\nsen_dict = defaultdict()\n#print(len(sen_list3))\n'''#\u8bfb\u53d6Boson\u7f51\u7edc\u60c5\u611f\u8bcd\u5178\nsen_file = open('BosonNLP.txt', 'r+', encoding='utf-8')\nsen_list = sen_file.read().splitlines()'''\n\n#\u6574\u5408\u5b57\u5178\nfor s in sen_list1:\nsen_dict[s] = 1\nfor s in sen_list2:\nsen_dict[s] =-1\n#\u5bf9\u6bcf\u4e00\u884c\u5185\u5bb9\u6839\u636e\u7a7a\u683c\u5206\u9694\uff0c\u7d22\u5f150\u662f\u60c5\u611f\u8bcd\uff0c1\u662f\u60c5\u611f\u5206\u503c\n'''for s in sen_list:\nif len(s.split(' ')) == 2:\nsen_dict[s.split(' ')[0]] = s.split(' ')[1]\nfor key in sen_dict.keys():\nif float(sen_dict[key])>0:\nelse:\nsen_dict[key]=-1'''\nreturn sen_dict\n#\u8bfb\u53d6\u5426\u5b9a\u8bcd\u6587\u4ef6\u5e76\u521b\u5efa\u5217\u8868\ndef create_notlist():\nnot_word_file = open('notDic.txt', 'r+', encoding='utf-8')\nnot_word_list = not_word_file.read().splitlines()\nreturn not_word_list\n#\u83b7\u5f97\u6bcf\u6761\u8bc4\u8bba\u7684\u6700\u7ec8\u8bc4\u5206\ndef caculate_score(word_list,sen_dict,not_word_list):\nsen_score=dict()\nfor i in range(0,len(word_list)):\nif word_list[i] in sen_dict.keys():\nprint(word_list[i])\nif word_list[i-1] in not_word_list or word_list[i-2] in not_word_list:\nsen_score[word_list[i]]=sen_dict[word_list[i]]*(-1)\nelse:\nsen_score[word_list[i]]=sen_dict[word_list[i]]\nsum_score=0\nfor score in sen_score.values():\nsum_score+=score\nif sum_score>0:\nfinal_score=1\nelif sum_score<0:\nfinal_score=-1\nelse:\nfinal_score=0\nprint(final_score)\nalist=['\u89c2\u671b\u4e2d','\u683c\u529b\u8981\u5b8c\u86cb\uff0c\u522b\u4e70\u4e86']\nsocre_dic=dict()\nfor sentence in alist:\nword_list=seg_word(sentence)\nsen_dict=create_sendic()\nnot_word_list=create_notlist()\nfinal_score=caculate_score(word_list,sen_dict,not_word_list)\nsocre_dic[sentence]=final_score\nprint(socre_dic)\nalist=['\u683c\u529b\u8981\u5b8c\u86cb\uff0c\u522b\u4e70\u4e86','\u4eca\u5929\u5403\u4e86\u9e21\u86cb\u997c']\n'''\nfor sentence in alist:\ns = SnowNLP(sentence)\nprint(s.sentiments)\nprint(s.keywords(3))\n", "output_sequence": "Project_Group8 file"}, {"input_sequence": "import os\nimport jieba\nfrom collections import OrderedDict\nimport numpy as np\nimport pandas as pd\nimport csv\nwith open('stopwords.txt', 'r+', encoding='utf-8') as f:\nstop_words = set(f.read().splitlines())\nwith open('notDic.txt', 'r+', encoding='utf-8') as f:\nnot_words = set(f.read().splitlines())\nwith open('NTU_positive.txt', 'r+', encoding='utf-8') as f:\nNTU_positive = set(f.read().splitlines())\nwith open('NTU_negative.txt', 'r+', encoding='utf-8') as f:\nNTU_negative = set(f.read().splitlines())\n# read manual dictionary\nmanual_df = pd.read_csv('manual_dict.csv')\nmanual_pos = set(manual_df[manual_df.value == 1].word)\npos_set = NTU_positive\npos_set.update(manual_pos)\nsen_dict = dict()\nfor word in pos_set:\nsen_dict[word] = 1\nfor word in neg_set:\nsen_dict[word] =-1\ndef seg_word(sentence):\n\"\"\"\u4f7f\u7528jieba\u5bf9\u6587\u6863\u5206\u8bcd\"\"\"\nseg_list = list(jieba.cut(sentence))\n#\u53bb\u9664\u505c\u7528\u8bcd\nword_list=list(filter(lambda x: x not in stop_words, seg_list))\nreturn word_list\ndef calculate_score_by_list(word_list):\nsen_index_list = list(map(lambda x: sen_dict.get(x,0),word_list))\nif len(not_word_index_list) >= 2:\nroll_1 = np.roll(not_word_index_list,1)\nroll_1[0] = 0\nnot_word_index_list = -1 * np.logical_xor(roll_1,roll_2)\nnot_word_index_list[not_word_index_list == 0] = 1\nelif len(not_word_index_list) >= 1:\nnot_word_index_list = -1 * roll_1\nelse:\nreturn 0\nscore = np.sum(sen_index_list * not_word_index_list)\nfinal_score = 1 if score > 0 else -1 if score < 0 else 0\nreturn final_score\ndef score(sentence):\nword_list = seg_word(sentence)\nscore_ = calculate_score_by_list(word_list)\nreturn score_\n# read data\ndf = pd.read_csv('merge_data.csv')\n'''\n# This part is to build dictionary manually, whose name is manual_dict.csv\n# word count\nword_count_dict = dict()\nfor num, sentence in enumerate(df['text_a']):\nif type(sentence) is str:\nword_list = seg_word(sentence)\nfor word in word_list:\nword_count_dict[word] = word_count_dict.get(word,0) + 1\nif num % 10000 == 0:\nprint(num)\n# enter value by the order of the count of key\norder_word_cout_dict = OrderedDict(sorted(word_count_dict.items(),key=lambda item:item[1],reverse=True))\ndict_df = list()\nfor key in order_word_cout_dict.keys():\nquit = False\nif key not in stop_words and key not in not_words and key not in NTU_positive and key not in NTU_negative:\nwhile True:\nprint(f'{key} with frequency {order_word_cout_dict[key]} times')\nprint(f'Whats the score of {key}? 1 for pos, -1 for neg, 0 for neither, q for stop and quit ! ')\nscore = input()\nif score == '1' or score == '-1' or score == '0':\ndict_df.append([key,order_word_cout_dict[key], int(score)])\nbreak\nelif score == 'q':\nquit = True\nif quit:\nbreak\n# save manual dictionary\ndict_df = pd.DataFrame(dict_df, columns=['word','count','value'])\ndict_df.to_csv('manual_dict.csv')\n# calculate score_list\nscore_list = list(map(lambda content:0 if type(content) is float else score(content), df['text_a']))\n# scoring\nsingle_stock_manual_score = pd.DataFrame(columns=['stock_id','news_id','score'])\n", "output_sequence": "Project_Group8 file"}, {"input_sequence": "import lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.metrics import roc_auc_score, roc_curve,auc\nimport warnings\nfrom sklearn.preprocessing import LabelEncoder\nimport pandas as pd\nimport gc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score,precision_score,recall_score,roc_auc_score,accuracy_score,roc_curve\nseed = 623\n# data\u662f\u539f\u59cb\u6570\u636e\u5927\u8868\uff0c\u5305\u542b\u6240\u6709\u7279\u5f81\ndata1 = pd.read_csv('data_1.csv', index_col=0)\ndata.rename(columns = {'label_x':'label'}, inplace = True)\n# \u521b\u9020\u4e00\u4e2a\u539f\u59cb\u6570\u636e\u7684\u5907\u4efd\uff0c\u5bf9\u6570\u636e\u8fdb\u884c\u64cd\u4f5c\ndf = data.copy()\ndf_train = df[~df['label'].isnull()].copy()\nsample = df_train.sample(frac=0.1, random_state=623)\ntrain_labels = sample.label\ntrain_features = sample.drop(columns='label')\nfrom feature_selector import FeatureSelector\nfs.identify_all(selection_params = {'missing_threshold': 0.6,\n'correlation_threshold': 0.9,\n'task': 'classification',\ngood_cols = train_removed.columns.tolist()\ndf_train_clear = df_train[good_cols]\ndf_train_clear.insert(loc=0, column='label', value=df_train.label)\ndf_clear = pd.concat([df_train_clear, df_test_clear], axis=0)\n# df_clear['user'] = df.index\ndf_clear.reset_index(level=0, inplace=True)\n", "output_sequence": "Feature_Engineering"}, {"input_sequence": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport gc\ndf = pd.read_csv('feature_base.csv')\ndata1 = df_train.groupby(\"label\")[\"user\"].count()\ncolors = ['tan', 'grey']\nplt.rcParams['savefig.dpi'] = 200\npie, ax = plt.subplots(figsize=[5,5])\nlabels = ['good', 'bad']\nplt.pie(x=data1, autopct=\"%.1f%%\", explode=[0.05]*2, labels=labels, colors=colors, pctdistance=0.5)\nplt.title(\"Label Distribution\")\nplt.legend([\"good\", \"bad\"])\nax = sns.boxplot( x=df_train['label'], y=df_train['product7_fail_ratio'], palette=\"Blues\")\nplt.title(\"feature: product7_fail_cnt / product7_cnt\")\nax = sns.boxplot( x=df_train['label'], y=df_train['using_time_op1_cnt_ratio'], palette=\"Blues\")\nplt.title(\"feature: using_time / op1_cnt\")\nax = sns.boxplot( x=df_train['label'], y=df_train['login_cnt_period2_login_cnt_ratio'], palette=\"Blues\")\nplt.title(\"feature: login_cnt_period2 / login_cnt\")\nax = sns.boxplot( x=df_train['label'], y=df_train['login_cnt_period1_login_cnt_ratio'], palette=\"Blues\")\nplt.title(\"feature: login_cnt_period1 / login_cnt\")\ntrans = pd.read_csv('train_trans_new.csv', index_col=0)\ntrans_df = pd.merge(trans, label, on='user')\n#transform variable tm_diff\ndef transform_time(x):\nday = int(x.split(' ')[0])\nhour = int(x.split(' ')[2].split('.')[0].split(':')[0])\nreturn 86400*day+3600*hour+60*minute+second\ntrans_df['day'] = trans_df['tm_diff'].apply(lambda x: int(x.split(' ')[0]))\ntrans_df['hour'] = trans_df['tm_diff'].apply(lambda x: int(x.split(' ')[2].split('.')[0].split(':')[0]))\ntrans_df['week'] = trans_df['day'].apply(lambda x: x % 7)\n# using timestamp to reorganize the sequence of each user's behaviour\ntrans_df = trans_df.sort_values(by=['user', 'timestamp'])\ntrans_df.reset_index(inplace=True, drop=True)\ndf_good = trans_df[trans_df['label'] == 0]\ndf_train = df[~df['label'].isnull()]\ndf_good = df_train[df_train['label'] == 0]\nsns.set()\n#\u7528\u884c\u548c\u5217\u6807\u7b7e\u7ed8\u5236\ntemp = df_train.pivot(\"age\", \"sex\", 'label')\n# \u7ed8\u5236x-y-z\u7684\u70ed\u529b\u56fe\uff0c\u6bd4\u5982 \u5e74-\u6708-\u9500\u91cf \u7684\u70ed\u529b\u56fe\nf, ax = plt.subplots(figsize=(9, 6))\nsns.heatmap(flights, ax=ax)\n#\u8bbe\u7f6e\u5750\u6807\u5b57\u4f53\u65b9\u5411\nlabel_y = ax.get_yticklabels()\nplt.setp(label_y, rotation=360, horizontalalignment='right')\nlabel_x = ax.get_xticklabels()\nplt.setp(label_x, rotation=45, horizontalalignment='right')\n", "output_sequence": "Feature_Engineering"}, {"input_sequence": "# import packages\nimport pandas as pd\n# load data\ndf_train_label = pd.read_csv('train_label.csv')\n# random split the dataset into training set and test set\ntrain_set, test_set = train_test_split(data, test_size=0.2, random_state=623)\nprint(train_set.shape)\ntrain_id = train_set['user']\n# use the random id to slice the orginal dataset\ntrain_base = df_train_base[df_train_base['user'].isin(train_id.tolist())]\ntrain_base.to_csv('train_base_new.csv')\ntest_trans.to_csv('test_trans_new.csv')\n", "output_sequence": "Feature_Engineering"}, {"input_sequence": "# import packages\nimport pandas as pd\nimport numpy as np\nfrom category_encoders.count import CountEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction.text import CountVectorizer,\nfrom sklearn.preprocessing import MinMaxScaler\nimport lightgbm as lgb\nfrom tqdm import tqdm\nfrom tqdm import tqdm_notebook\nimport gc\nimport warnings\nwarnings.filterwarnings('ignore')\nimport toad\ntrain_label = pd.read_csv('train_label_new.csv', index_col=0)\ntrain_op = train_op.drop(columns=['Unnamed: 0'])\ntest_base = pd.read_csv('test_base_new.csv', index_col=0)\npd.set_option('display.max_columns', 11)\n# concatenate base and label\ntrain_df = train_base.copy()\ntrain_df = train_label.merge(train_df, on=['user'], how='left')\ndata = pd.concat([train_df, test_df], axis=0, ignore_index=True)\n# del train_base, test_base\n# concatenate train and test dataset of operation and transaction\nop_df = pd.concat([train_op, test_op], axis=0, ignore_index=True)\n# base = toad.detector.detect(data)\n# Since this feature have too many null values, so we drop this feature\n# Transfer into integer\nfor col in ['balance', 'balance_avg', 'balance2','balance2_avg', 'product1_amount',\n'product3_amount',\n# Interactive item\ncate_features = ['sex', 'provider', 'level', 'verified', 'regist_type', 'agreement1',\n'province', 'city', 'service3']\nfor f1 in tqdm(cate_features):\ndata['{}_{}'.format(f1, f2)] = data[f1] + '_' + data[f2]\n\nfor f in tqdm(cate_features):\n# using Label Encoder to deal with categorical features\nfor col in tqdm([col for col in original_cate if col not in ['user']]):\nle = LabelEncoder()\ndata[col].fillna('-1', inplace=True)\n# using Count Encoder to categorical features\ntrain_base = pd.read_csv('train_base_new.csv')\ndf = pd.concat([train_base,test_base], axis=0, ignore_index=True)\ndf_category=df.select_dtypes('object')\ndf_category_nunique = df_category.nunique()\nA_cnt_features = [col for col in df_category_nunique.index if df_category_nunique.loc[col] > 5 and col!='user']\nfrequency_fea = pd.DataFrame()\nfrequency_fea['user'] = df_category['user'].values\nfor col in tqdm_notebook(A_cnt_features):\ndf_category[col] = df_category[col].fillna(-999)\nfrequency_fea[col + '_cnt'] = df_category[col].map(df_category[col].value_counts())\n\ndata=data.merge(frequency_fea,on=\"user\",how=\"left\")\ndense_features = ['age', 'using_time', 'card_a_cnt', 'op1_cnt', 'service1_cnt',\n'agreement_total', 'acc_count', 'login_cnt_period1', 'ip_cnt', 'login_cnt_avg', 'login_days_cnt', 'balance', 'balance_avg',\n'balance1', 'balance2_avg', 'product1_amount',\n'product6_amount',\nmin_max = MinMaxScaler()\n# generate features from experience\ndata['product7_fail_ratio'] = data['product7_fail_cnt'] / data['product7_cnt']\ndata['city_count'] = data.groupby(['city'])['user'].transform('count')\ndata['card_cnt'] = data['card_a_cnt'] + data['card_b_cnt'] + data['card_c_cnt'] + data['card_d_cnt']\ndata['acc_card_ratio'] = data['acc_count'] / data['card_cnt']\ndata['login_cnt'] = data['login_cnt_period1'] + data['login_cnt_period2']\ndata['login_cnt_period2_login_cnt_ratio'] = data['login_cnt_period2'] / data['login_cnt']\n#transform variable tm_diff\ndef transform_time(x):\nday = int(x.split(' ')[0])\nhour = int(x.split(' ')[2].split('.')[0].split(':')[0])\nreturn 86400*day+3600*hour+60*minute+second\n# extract elements from tm_diff\n# timestamp means how many seconds passed after the start point\nop_df['day'] = op_df['tm_diff'].apply(lambda x: int(x.split(' ')[0]))\ntrans_df['timestamp'] = trans_df['tm_diff'].apply(lambda x: transform_time(x))\nop_df['hour'] = op_df['tm_diff'].apply(lambda x: int(x.split(' ')[2].split('.')[0].split(':')[0]))\ntrans_df['week'] = trans_df['day'].apply(lambda x: x % 7)\nop_df['min'] = op_df['tm_diff'].apply(lambda x: int(x.split(' ')[2].split('.')[0].split(':')[1]))\n# using timestamp to reorganize the sequence of each user's behaviour\ntrans_df = trans_df.sort_values(by=['user', 'timestamp'])\ntrans_df.reset_index(inplace=True, drop=True)\n# use hour to group the behaviours in trans and op\ntrans_df[\"time\"]=\"time\"\ntrans_df.loc[trans_df.hour<=6,\"time\"]=\"night\"\ntrans_df.loc[(trans_df.hour>6)&(trans_df.hour<=12),\"time\"]=\"morning\"\nop_df[\"time\"]=\"time\"\nop_df.loc[op_df.hour<=6,\"time\"]=\"night\"\nop_df.loc[(op_df.hour>6)&(op_df.hour<=12),\"time\"]=\"morning\"\n# define a function that calculate statistics about transaction amount\ndef gen_user_amount_features(df):\ngroup_df = df.groupby(['user'])['amount'].agg([\n('user_amount_mean', 'mean'),\n]).reset_index()\nreturn group_df\n# extract features about transcation amount\n# define unique value of each user\ndef gen_user_nunique_features(df, value, prefix):\ngroup_df = df.groupby(['user'])[value].agg([\n('user_{}_{}_nuniq'.format(prefix, value), 'nunique')]\n).reset_index()\n# extract features from trans dataframe\nfor col in tqdm(['day', 'platform', 'tunnel_in', 'type1', 'ip', 'ip_3']):\ndata = data.merge(gen_user_nunique_features(df=trans_df, value=col, prefix='trans'), on=['user'], how='left')\n# transaction amount per day\ndata['user_amount_per_days'] = data['user_amount_sum'] / data['user_trans_day_nuniq']\n# transcation amount each time\n# define a function to get the transcation amount with each group\ndef gen_user_group_amount_features(df, value):\ngroup_df = df.pivot_table(index='user',\ncolumns=value,\naggfunc=['count', 'sum'])\ngroup_df.columns = ['user_{}_{}_amount_{}'.format(value, f[1], f[0]) for f in group_df.columns]\ngroup_df.reset_index(inplace=True)\n# group by platform\ndata = data.merge(gen_user_group_amount_features(df=trans_df, value='platform'), on=['user'], how='left')\n# gourp by type1\ndata = data.merge(gen_user_group_amount_features(df=trans_df, value='type1'), on=['user'], how='left')\n# group by type2\ndata = data.merge(gen_user_group_amount_features(df=trans_df, value='type2'), on=['user'], how='left')\n# group by time\ndata = data.merge(gen_user_group_amount_features(df=trans_df, value='time'), on=['user'], how='left')\n#group by week\n# add time axis, define a function to calculate users' transcation amount linked with feature 'days'\ndef gen_user_window_amount_features(df, window):\ngroup_df = df[df['day']>window].groupby('user')['amount'].agg([\n('user_amount_mean_{}d'.format(window), 'mean'),\n# extract amount feature within transcations after 7 days\ndata = data.merge(gen_user_window_amount_features(df=trans_df, window=7), on=['user'], how='left')\n# extract amount feature within transcations after 14 days\ndata = data.merge(gen_user_window_amount_features(df=trans_df, window=14), on=['user'], how='left')\n# extract amount feature within transcations after 21 days\ndata = data.merge(gen_user_window_amount_features(df=trans_df, window=21), on=['user'], how='left')\n# extract amount feature within transcations after 28 days\n# define a function to calculate users' transcation amount linked with feature 'hours'\ngroup_df = df[df['hour']>window].groupby('user')['amount'].agg([\n('user_amount_mean_{}h'.format(window), 'mean'),\n# extract amount feature within transcations after 6 a.m.\ndata = data.merge(gen_user_window_amount_features(df=trans_df, window=6), on=['user'], how='left')\n# extract amount feature within transcations after 12 p.m.\ndata = data.merge(gen_user_window_amount_features(df=trans_df, window=12), on=['user'], how='left')\n# extract amount feature within transcations after 18 p.m.\n# for some features, we consider that null comes from some reason, so we generate features about missing value\ndef gen_user_null_features(df, value, prefix):\ndf['is_null'] = 0\ndf.loc[df[value].isnull(), 'is_null'] = 1\ngroup_df = df.groupby(['user'])['is_null'].agg([('user_{}_{}_null_cnt'.format(prefix, value), 'sum'),\n('user_{}_{}_null_ratio'.format(prefix, value),'mean')]).reset_index()\n# extract missing value and ratio within ip address\n# extract the first apperance of \"type1 == 45a1168437c708ff\"\ngroup_df = trans_df[trans_df['type1']=='45a1168437c708ff'].groupby(['user'])['day'].agg([('user_type1_45a1168437c708ff_min_day', 'min')]).reset_index()\ndata = data.merge(group_df, on=['user'], how='left')\n# define the transaction amount of each users per day and hour\ndef per_hour_amt(df,value1,value2):\ngroup_df=df[['user',value1,value2, 'amount']]\ngroup_df=group_df.groupby(['user',value1,value2])[\"amount\"].agg('sum').reset_index()\ngroup_df=group_df[['user','amount']]\ngroup_df=group_df.groupby('user')['amount'].agg([\n('per_hour_amt_sum','sum'),\n# extract transcation per day and hour features\n# define a function to calculate each user's transcation amount group by tunnel_in and tunnel_out\ndef tunnel_in_out_amt(df,value1,value2):\n('tunnel_in_out_amt_sum','sum'),\n# extract trascation amount group by tunnel_in and tunnel_out\n# define a function to calculate each user's transcation amount group by \"ip\"\ndef day_ip_amt(df,value1,value2):\n('day_ip_amt_sum','sum'),\n# extract trascation amount group by \"ip\"\n# define a function to find the gap between transcations of each user for each hour\ndef amt_gap(df,value1,value2):\ngroup_df['last_amount']=group_df.groupby('user')['amount'].shift(1)\ngroup_df['amount_gap']=abs(group_df[\"amount\"]-group_df[\"last_amount\"])\ngroup_df=group_df[['user','amount_gap']]\n('amt_gap_sum','sum'),\n# extract features from the amount of transcation group by time gap\n# define a function to calculate the sum of gap ratio of amount for each user\ndef gap_amt_rate(df,value1,value2):\ngroup_df=df[['user',value1,value2,'amount']]\ngroup_df=group_df.groupby(['user', 'day', 'hour'])['amount'].agg('sum').reset_index()\ngroup_df['gap_rate']=group_df['amount']/group_df['last_amount']\ngroup_df=group_df[['user','gap_rate']]\ngroup_df=group_df.groupby('user')['gap_rate'].agg('sum').reset_index()\n# extract the gap ratio feature\n# generate the amount of transaction per day\ndef per_day_amt(df,value):\ngroup_df=df[['user',value, 'amount']]\ngroup_df=group_df.groupby(['user',value])[\"amount\"].agg('sum').reset_index()\n('per_day_amt_mean','mean'),\n# extract features using the function\n# define a function to find the gap between transcations of each user for each transcation day\ndef day_amt_gap(df,value):\n('day_amt_gap_mean','mean'),\n# extract amount gap per day\n# define a function to calculate the gap ratio per day\ndef day_gap_amt_rate(df,value):\ngroup_df=df[['user',value,'amount']]\ngroup_df=group_df.groupby(['user', value])['amount'].agg('sum').reset_index()\ngroup_df['day_gap_rate']=group_df['amount']/group_df['last_amount']\ngroup_df=group_df[['user','day_gap_rate']]\ngroup_df=group_df.groupby('user')['day_gap_rate'].agg('sum').reset_index()\n# generate amount gap ratio per day\n# define a function to calculate the type1 transcation per day\ndef day_type_amt(df,value1,value2):\n('day_type1_amt_sum','sum'),\n# extract type1 transcation amount per day\n# define a function to calculate the type2 transcation per day\ndef day_type2_amt(df,value1,value2):\n('day_type2_amt_sum','sum'),\n# extract type2 transcation amount per day\nfrom gensim.models import Word2Vec\nimport multiprocessing\ndef w2v_feat(data_frame, feat, mode):\nfor i in feat:\nif data_frame[i].dtype != 'object':\ndata_frame[i] = data_frame[i].astype(str)\ndata_frame.fillna('nan', inplace=True)\nprint(f'Start {mode} word2vec ...')\nmodel = Word2Vec(data_frame[feat].values.tolist(), size=5, window=2, min_count=1,\nworkers=multiprocessing.cpu_count(), iter=10)\nstat_list = ['min', 'max', 'std']\nnew_all = pd.DataFrame()\nfor m, t in enumerate(feat):\nprint(f'Start gen feat of {t} ...')\ntmp = []\nfor i in data_frame[t].unique():\ntmp_v = [i]\ntmp_v.extend(model[i])\ntmp_df = pd.DataFrame(tmp)\nw2c_list = [f'w2c_trans_{t}_{n}' for n in range(5)]\ntmp_df.columns = [t] + w2c_list\ntmp_df = data_frame[['user', t]].merge(tmp_df, on=t)\ntmp_df = tmp_df.drop_duplicates().groupby('user').agg(stat_list).reset_index()\ntmp_df.columns = ['user'] + [f'{p}_{q}' for p in w2c_list for q in stat_list]\nif m == 0:\nnew_all = pd.concat([new_all, tmp_df], axis=1)\nelse:\nnew_all = pd.merge(new_all, tmp_df, how='left', on='user')\nreturn new_all\n# generate word2vec features\ntrans_feat=[\"platform\",\"tunnel_in\",\"tunnel_out\",\"amount\",\"type1\",\"type2\",\"ip\",\"day\",\"hour\"]\n# using Count Encoder to categorical features in df_trans\nfor i in [\"ip\",\"ip_3\",\"amount\"]:\ntrans_df[\"count_{}_trans\".format(i)]=CountEncoder().fit_transform(trans_df[i])\ngroup_df=trans_df.groupby('user')[\"count_{}_trans\".format(i)].agg([\n(\"count_{}_trans_max\".format(i),'max'),\ndata=data.merge(group_df,on=\"user\",how=\"left\")\ntrans_df.drop(columns=trans_df.columns[-3:],inplace=True)\n# extract frequncy features for features which have more than five unique values\ntrans_df_category=trans_df.drop(columns=[\"tm_diff\",\"time\"])\ntrans_df_category_nunique = trans_df_category.nunique()\nA_cnt_features = [col for col in trans_df_category_nunique.index if trans_df_category_nunique.loc[col] > 5 and col!='user']\n# print(len(A_cnt_features))\nfrequency_fea['user'] = trans_df_category['user'].values\ntrans_df_category[col] = trans_df_category[col].fillna(-999)\nfrequency_fea[col + '_cnt'] = trans_df_category[col].map(trans_df_category[col].value_counts())\nfor i in tqdm_notebook(frequency_fea.columns[1:]):\ngroup_df=frequency_fea[[\"user\",i]]\ngroup_df=group_df.groupby(\"user\")[i].agg([\n('freq_{}_max'.format(i),'max'),\n('freq_{}_std'.format(i),'std')]).reset_index()\ndef gen_user_tfidf_features(df, value):\ndf[value] = df[value].astype(str)\ndf[value].fillna('-1', inplace=True)\ngroup_df = df.groupby(['user']).apply(lambda x: x[value].tolist()).reset_index()\ngroup_df.columns = ['user', 'list']\ngroup_df['list'] = group_df['list'].apply(lambda x: ','.join(x))\nenc_vec = TfidfVectorizer()\ntfidf_vec = enc_vec.fit_transform(group_df['list'])\n# use SVD method to reduce the dimension of this sparse matrix\nsvd_enc = TruncatedSVD(n_components=10, n_iter=20, random_state=623)\nvec_svd = svd_enc.fit_transform(tfidf_vec)\nvec_svd = pd.DataFrame(vec_svd)\nvec_svd.columns = ['svd_tfidf_{}_{}'.format(value, i) for i in range(10)]\ngroup_df = pd.concat([group_df, vec_svd], axis=1)\ndel group_df['list']\ndata = data.merge(gen_user_tfidf_features(df=op_df, value='op_mode'), on=['user'], how='left')\ndef gen_user_countvec_features(df, value):\nenc_vec = CountVectorizer()\nvec_svd.columns = ['svd_countvec_{}_{}'.format(value, i) for i in range(10)]\ndata = data.merge(gen_user_countvec_features(df=op_df, value='op_mode'), on=['user'], how='left')\nw2c_list = [f'w2c_op_{t}_{n}' for n in range(5)]\n#\u751f\u6210word2vec\u7279\u5f81\nop_feat=[\"op_type\",\"op_mode\",\"op_device\",\"ip\",\"channel\",\"day\",\"hour\"]\n# using Counter Encoder to generate features from op_df\nfor i in [\"ip\",\"ip_3\",\"op_device\",\"op_type\",\"op_mode\"]:\nop_df[\"count_{}_op\".format(i)]=CountEncoder().fit_transform(op_df[i])\ngroup_df=op_df.groupby('user')[\"count_{}_op\".format(i)].agg([\n(\"count_{}_op_max\".format(i),'max'),\n# encode the frequency of opeartion and calculate statistics\nop_df_category=op_df.drop(columns=[\"tm_diff\",\"time\"])\nop_df_category_nunique = op_df_category.nunique()\nA_cnt_features = [col for col in op_df_category_nunique.index if op_df_category_nunique.loc[col] > 5 and col!='user']\nprint(len(A_cnt_features))\nfrequency_fea['user'] = op_df_category['user'].values\nop_df_category[col] = op_df_category[col].fillna(-999)\nfrequency_fea[col + '_cnt'] = op_df_category[col].map(op_df_category[col].value_counts())\n('freq_{}_max'.format(i),'max'),\n# define a function to calculate the possible values of each feature for each user\n('user_{}_{}_nuniq'.format(prefix, value),'nunique')\n# extract number of unique values of each user\nfor col in tqdm(['op_type', 'op_mode', 'ip', 'channel', 'ip_3', 'day']):\n# define a function to count how many times of one operation type happens group by day\ndef gen_user_window_op_features(df, window):\ngroup_df = df[df['day']>window].groupby('user')['op_type'].agg([\n('user_op_cnt_{}d'.format(window),'count')\nop_df[\"day\"]=op_df[\"day\"].astype(int)\n# extract operation type counting after 5 days\ndata = data.merge(gen_user_window_op_features(df=op_df, window=5), on=['user'], how='left')\n# extract operation type counting after 10 days\n# define a function to count how many times of one operation type happens group by hour\ndef gen_op_window_hour_features(df, window):\ngroup_df = df[df['hour']>window].groupby('user')['op_type'].agg([\n('user_op_cnt_{}h'.format(window),'count')]).reset_index()\nop_df[\"hour\"]=op_df[\"hour\"].astype(int)\n# extract operation type counting after 6\ndata = data.merge(gen_op_window_hour_features(df=op_df, window=6), on=['user'], how='left')\n# extract operation type counting after 12\ndata = data.merge(gen_op_window_hour_features(df=op_df, window=12), on=['user'], how='left')\n# extract operation type counting after 18\n# define a function to count how many times of one operation type happens group by time(i.e morning, afternoon, evening and night)\ndef gen_user_group_op_features(df, value):\nvalues='op_type',\ngroup_df.columns = ['user_{}_{}_op_{}'.format(value, f[1], f[0]) for f in group_df.columns]\n# extrate operation type counts group by time\n# counting how many times each user operated the app\nop_count = op_df[['user']]\nop_count['op_count'] = 1\nop_count = op_count.groupby('user').agg('count').reset_index()\ndata = pd.merge(data, op_count, on='user', how='left')\n# del op_count\n# calculate how many times of operation happened per day\n# define a function to calculate the statistics of the counting of operations of each user in per hour per day\ndef day_per_hour_cnt(df,value1,value2):\ngroup_df = op_df[['user', 'day', 'hour']]\ngroup_df['everyday_everyhour'] = 1\ngroup_df = group_df.groupby(['user', 'day', 'hour']).agg('count').reset_index()\ngroup_df = group_df.drop(['day', 'hour'],axis = 1)\ngroup_df = group_df.groupby('user')['everyday_everyhour'].agg([\n('day_per_hour_mean','mean'),\n('day_per_hour_std','std')]).reset_index()\n# define a function to calculate the statistics of the counting of operations of each user in per day\nfrequence_one_day = op_df[['user', 'day']]\nfrequence_one_day['everyday'] = 1\nfrequence_one_day = frequence_one_day.groupby(['user', 'day']).agg('count').reset_index()\nfrequence_one_day = frequence_one_day.drop('day', axis=1)\nfrequence_one_day = frequence_one_day.groupby('user')['everyday'].agg([\n('per_day_mean','mean'),\n('per_day_std','std')]).reset_index()\n# counting the opeartions happened in the morning for each user\nfrequence_morning = op_df[op_df.time==\"morning\"][['user', 'day','hour']]\nfrequence_morning['everyday_morning'] = 1\nfrequence_morning = frequence_morning.groupby(['user', 'day', 'hour']).agg('count').reset_index()\nfrequence_morning = frequence_morning[['user', 'everyday_morning']]\n('per_mor_mean','mean'),\n('per_mor_std','std')]).reset_index()\ndata = data.merge(frequence_morning, on='user', how='left')\n# counting the opeartions happened in the afternoon for each user\nfrequence_afternoon = op_df[op_df.time==\"afternoon\"][['user', 'day','hour']]\nfrequence_afternoon['everyday_afternoon'] = 1\nfrequence_afternoon = frequence_afternoon.groupby(['user', 'day', 'hour']).agg('count').reset_index()\nfrequence_afternoon = frequence_afternoon[['user', 'everyday_afternoon']]\n('per_after_mean','mean'),\n('per_after_std','std')]).reset_index()\ndata = data.merge(frequence_afternoon, on='user', how='left')\n# counting the opeartions happened in the evening for each user\nfrequence_evening = op_df[op_df.time==\"evening\"][['user', 'day','hour']]\nfrequence_evening['everyday_evening'] = 1\nfrequence_evening = frequence_evening.groupby(['user', 'day', 'hour']).agg('count').reset_index()\nfrequence_evening = frequence_evening[['user', 'everyday_evening']]\n('per_eve_mean','mean'),\n('per_eve_std','std')]).reset_index()\ndata = data.merge(frequence_evening, on='user', how='left')\n# counting the opeartions happened at night for each user\nfrequence_night = op_df[op_df.time==\"night\"][['user', 'day','hour']]\nfrequence_night['everyday_night'] = 1\nfrequence_night = frequence_night.groupby(['user', 'day', 'hour']).agg('count').reset_index()\nfrequence_night = frequence_night[['user', 'everyday_night']]\n('per_night_mean','mean'),\n('per_night_std','std')]).reset_index()\ndata = data.merge(frequence_night, on='user', how='left')\n# calculate the statistics of the operation counting between two days\nfrequence_one_day_gap = op_df[['user', 'day']]\nfrequence_one_day_gap['everyday'] = 1\nfrequence_one_day_gap = frequence_one_day_gap.groupby(['user', 'day']).agg('count').reset_index()\nfrequence_one_day_gap['everyday_before'] = frequence_one_day_gap.groupby('user')['everyday'].shift(1)\nfrequence_one_day_gap['everyday_before_gap'] = abs(frequence_one_day_gap['everyday'] - frequence_one_day_gap['everyday_before'])\nfrequence_one_day_gap = frequence_one_day_gap[['user', 'everyday_before_gap']].groupby('user')['everyday_before_gap'].agg([\n('op_day_gap_mean','mean'),\n('op_day_gap_std','std')]).reset_index()\ndata = data.merge(frequence_one_day_gap, on='user', how='left')\n# calculate the statistics of the operation counting between two hours\nfrequence_one_hour_gap = op_df[['user', 'day', 'hour']]\nfrequence_one_hour_gap['everyhour'] = 1\nfrequence_one_hour_gap = frequence_one_hour_gap.groupby(['user', 'day', 'hour']).agg('count').reset_index()\nfrequence_one_hour_gap['everyhour_before'] = frequence_one_hour_gap.groupby('user')['everyhour'].shift(1)\nfrequence_one_hour_gap['everyhour_before_gap'] = abs(frequence_one_hour_gap['everyhour'] - frequence_one_hour_gap['everyhour_before'])\nfrequence_one_hour_gap = frequence_one_hour_gap[['user', 'everyhour_before_gap']].groupby('user')['everyhour_before_gap'].agg([\n('hour_gap_mean','mean'),\n('hour_gap_std','std')]).reset_index()\ndata = data.merge(frequence_one_hour_gap, on='user', how='left')\n# calculate the ratio of the number of the operations between two days\nfrequence_one_day_rate = op_df[['user', 'day']]\nfrequence_one_day_rate = frequence_one_day_rate.groupby(['user', 'day']).agg('count').reset_index()\nfrequence_one_day_rate['everyday_before'] = frequence_one_day_rate.groupby('user')['everyday'].shift(1)\nfrequence_one_day_rate['everyday_before_rate'] = frequence_one_day_rate['everyday'] / frequence_one_day_rate['everyday_before']\nfrequence_one_day_rate = frequence_one_day_rate[['user', 'everyday_before_rate']].groupby('user')['everyday_before_rate'].agg('sum').reset_index()\ndata = data.merge(frequence_one_day_rate, on='user', how='left')\n# calculate the ratio of the number of the operations between two hours\nfrequence_one_hour_rate = op_df[['user', 'day', 'hour']]\nfrequence_one_hour_rate['everyhour'] = 1\nfrequence_one_hour_rate = frequence_one_hour_rate.groupby(['user', 'day', 'hour']).agg('count').reset_index()\nfrequence_one_hour_rate['everyhour_before'] = frequence_one_hour_rate.groupby('user')['everyhour'].shift(1)\nfrequence_one_hour_rate['everyhour_before_rate'] = frequence_one_hour_rate['everyhour'] - frequence_one_hour_rate['everyhour_before']\nfrequence_one_hour_rate = frequence_one_hour_rate[['user', 'everyhour_before_rate']].groupby('user')['everyhour_before_rate'].agg('sum').reset_index()\ndata = data.merge(frequence_one_hour_rate, on='user', how='left')\n# calculate the statistics of the number of operatins of each user happened per second\nevery_second = op_df[['user', 'second']]\nevery_second['operation_second_op'] = 1\nevery_second = every_second.groupby(['user', 'second']).agg('count').reset_index()\nevery_second = every_second[['user', 'operation_second_op']]\n('per_sec_max','max'),\n('per_sec_std','std')]).reset_index()\ndata = data.merge(every_second, on='user', how='left')\n# define a function to calculate the statistics of the number of operatins of each user happened per minutes\nevery_minute = op_df[['user', 'min']]\nevery_minute['operation_minute_op'] = 1\nevery_minute = every_minute.groupby(['user', 'min']).agg('count').reset_index()\nevery_minute = every_minute[['user', 'operation_minute_op']]\n('per_minute_max','max'),\n('per_minute_std','std')]).reset_index()\ndata = data.merge(every_minute, on='user', how='left')\n# calculate the statistics of the number of different devices one may use per day\ndev_per_day_cnt=op_df[op_df.op_device!=\"nan\"][[\"user\",\"day\",\"op_device\"]].drop_duplicates()\ndev_per_day_cnt=pd.DataFrame(dev_per_day_cnt.groupby([\"user\",\"day\"])[\"op_device\"].nunique()).reset_index()\ndev_per_day_cnt=dev_per_day_cnt[[\"user\",\"op_device\"]]\n(\"dev_nun_mean\",\"mean\"),\n]).reset_index()\ndata=data.merge(dev_per_day_cnt, on='user', how='left')\n# calculate the statistics of the number of different devices one may use per hour\ndev_per_hour_cnt=op_df[op_df.op_device!=\"nan\"][[\"user\",\"day\",\"hour\",\"op_device\"]].drop_duplicates()\ndev_per_hour_cnt=pd.DataFrame(dev_per_hour_cnt.groupby([\"user\",\"day\",\"hour\"])[\"op_device\"].nunique()).reset_index()\ndev_per_hour_cnt=dev_per_hour_cnt[[\"user\",\"op_device\"]]\n(\"dev_hour_nun_mean\",\"mean\"),\ndata=data.merge(dev_per_hour_cnt, on='user', how='left')\n# calculate the statistics of the number of different ip one may use per day\nip_per_day_cnt=op_df[op_df.ip!=\"nan\"][[\"user\",\"day\",\"ip\"]].drop_duplicates()\nip_per_day_cnt=pd.DataFrame(ip_per_day_cnt.groupby([\"user\",\"day\"])[\"ip\"].nunique()).reset_index()\nip_per_day_cnt=ip_per_day_cnt[[\"user\",\"ip\"]]\nip_per_day_cnt=ip_per_day_cnt.groupby(['user'])[\"ip\"].agg([\n(\"ip_nun_mean\",\"mean\"),\ndata=data.merge(ip_per_day_cnt, on='user', how='left')\n# calculate the statistics of the number of different ip one may use per hour\nip_per_hour_cnt=op_df[op_df.ip!=\"nan\"][[\"user\",\"day\",\"hour\",\"ip\"]].drop_duplicates()\nip_per_hour_cnt=pd.DataFrame(ip_per_hour_cnt.groupby([\"user\",\"day\",\"hour\"])[\"ip\"].nunique()).reset_index()\nip_per_hour_cnt=ip_per_hour_cnt[[\"user\",\"ip\"]]\nip_per_hour_cnt=ip_per_hour_cnt.groupby(['user'])[\"ip\"].agg([\n(\"ip_hour_nun_max\",\"max\"),\ndata=data.merge(ip_per_hour_cnt, on='user', how='left')\n# calculate the statistics of the number of different ip one may use per day per minute\nip_per_min_cnt=op_df[op_df.ip!=\"nan\"][[\"user\",\"day\",\"hour\",\"min\",\"ip\"]].drop_duplicates()\nip_per_min_cnt=pd.DataFrame(ip_per_min_cnt.groupby([\"user\",\"day\",\"hour\",\"min\"])[\"ip\"].nunique()).reset_index()\nip_per_min_cnt=ip_per_min_cnt[[\"user\",\"ip\"]]\nip_per_min_cnt=ip_per_min_cnt.groupby(['user'])[\"ip\"].agg([\n(\"ip_min_nun_max\",\"max\"),\ndata=data.merge(ip_per_min_cnt, on='user', how='left')\n# save the generate features\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom datetime import datetime, timedelta\nfrom collections import Counter\nimport math\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.feature_selection import SelectPercentile, f_classif, chi2\nimport os\nwarnings.simplefilter('ignore')\ntqdm.pandas()\n%matplotlib inline\npd.set_option('max_columns', None)\nseed = 623\n# read_data\ndf_train_label = pd.read_csv('train_label_new.csv', index_col=0)\ndf_train_op = df_train_op.drop(columns=['Unnamed: 0'])\ndf_test_base = pd.read_csv('test_base_new.csv', index_col=0)\ndf_trans = df_train_trans.append(df_test_trans)\ndf_trans = df_trans.reset_index(drop=True)\ndf_op = df_train_op.append(df_test_op)\ndef parse_time(tm):\ndays, _, time = tm.split(' ')\ntime = time.split('.')[0]\ntime = '2020-1-1 ' + time\ntime = datetime.strptime(time, '%Y-%m-%d %H:%M:%S')\ntime = (time + timedelta(days=int(days)))\nreturn time\ndf_trans['date'] = df_trans['tm_diff'].apply(parse_time)\ndf_trans['day'] = df_trans['date'].dt.day\ndf_op['date'] = df_op['tm_diff'].apply(parse_time)\ndf_op['day'] = df_op['date'].dt.day\ndf_trans.sort_values(['user', 'date'], inplace=True)\ndf_op = df_op.reset_index(drop=True)\ndf_train = df_train_base.merge(df_train_label, how='left')\ndf_test = df_test_base\nos.makedirs('model', exist_ok=True)\ndef w2v_emb(df, f1, prefix):\nemb_size = 32\nmodel_path = 'model/{}_w2v_{}_{}_{}.m'.format(prefix, f1, emb_size)\nif os.path.exists(embedding_path):\nembedding = pd.read_pickle(embedding_path)\nreturn embedding\ntmp = df.groupby(f1, as_index=False)[f2].agg(\n{'{}_{}_list'.format(f1, f2): list})\nsentences = tmp['{}_{}_list'.format(f1, f2)].values.tolist()\ndel tmp['{}_{}_list'.format(f1, f2)]\nfor i in range(len(sentences)):\nsentences[i] = [str(x) for x in sentences[i]]\nif os.path.exists(model_path):\nmodel = Word2Vec.load(model_path)\nelse:\nmodel = Word2Vec(sentences,\nsize=emb_size,\nmodel.save(model_path)\nemb_matrix = []\nfor seq in sentences:\nvec = []\nfor w in seq:\nif w in model:\nvec.append(model[w])\nif len(vec) > 0:\nemb_matrix.append(np.mean(vec, axis=0))\ndf_emb = pd.DataFrame(emb_matrix)\ndf_emb.columns = [\n'{}_{}_{}_emb_{}'.format(prefix, f1, i) for i in range(emb_size)\n]\nembedding = pd.concat([tmp, df_emb], axis=1)\nembedding.to_pickle(embedding_path)\ndef tfidf_emb(df, f1, prefix):\ndf[f2] = df[f2].astype(str)\ndf[f2].fillna('-1', inplace=True)\ngroup_df = df.groupby([f1]).apply(\nlambda x: x[f2].tolist()).reset_index()\ngroup_df.columns = [f1, 'list']\nsvd_enc = TruncatedSVD(n_components=emb_size, n_iter=20, random_state=seed)\nvec_svd.columns = ['{}_svd_tfidf_{}_{}'.format(prefix,\nf2, i) for i in range(emb_size)]\ndef countvec_emb(df, f1, f2):\nvec_svd.columns = ['svd_countvec_{}_{}'.format(\ndef add_trend_feature(arr, abs_values=False):\nidx = np.array(range(len(arr)))\nif abs_values:\narr = np.abs(arr)\nlr = LinearRegression()\nlr.fit(idx.reshape(-1, 1), arr)\ndf_op['date_diff'] = df_op.groupby('user')['date'].diff()\ndf_op['op_second_diff'] = df_op['date_diff'].dt.seconds\ndf_op['op_hour_diff'] = df_op['op_second_diff'] / 3600\nfor window in [15, 3, 5]:\nfor col in tqdm(['op_type', 'op_mode', 'net_type', 'channel', 'hour']):\ndf_temp = df_op[df_op['day'] > 15 - window][['user', col]].copy()\ndf_temp['tmp'] = 1\ndf_temp = df_temp.pivot_table(index='user', columns=col,\nvalues='tmp', aggfunc=np.sum).reset_index().fillna(0)\ndf_temp.columns = [c if c == 'user' else 'op_{}_{}_count_{}d'.format(\ncol, c, window) for c in df_temp.columns]\nfor col in tqdm(['op_type', 'op_mode', 'net_type', 'channel']):\ndf_temp = df_op[['user', 'hour', col]].copy()\ndf_temp = df_temp.pivot_table(index='user', columns=col,\nvalues='hour', aggfunc=['mean', 'std', 'max', 'min']).fillna(0)\ndf_temp.columns = ['op_{}_{}_hour_{}'.format(col, f[1], f[0]) for f in df_temp.columns]\ndf_temp.reset_index(inplace=True)\ndf_temp.rename({'index': 'user'}, inplace=True, axis=1)\ndf_temp = df_op.groupby(['user', 'op_device']).size().reset_index()\ndf_temp.drop([0], axis=1, inplace=True)\ndf_temp = df_temp.sort_values(\nby=['user', 'op_device'], ascending=['asc', 'asc'])\ndf_temp.drop_duplicates('user', keep='last', inplace=True)\nfor f in ['hour', 'day', 'op_second_diff']:\ndf_temp = df_op.groupby('user')[f].agg([\n('op_{}_mean'.format(f), 'mean'),\ndf_trans['date_diff'] = df_trans.groupby('user')['date'].diff()\ndf_trans['trans_second_diff'] = df_trans['date_diff'].dt.seconds\ndf_trans['trans_hour_diff'] = df_trans['trans_second_diff'] / 3600\nfor col in tqdm(['platform', 'tunnel_in', 'type1', 'hour']):\ndf_temp = df_trans.pivot_table(\nindex='user', columns=col, values='amount', aggfunc=['sum', 'mean', 'std', 'median']).fillna(0)\ndf_temp.columns = ['trans_{}_{}_amount_{}'.format(col, f[1], f[0]) for f in df_temp.columns]\nfor window in tqdm([31, 1, 3, 5, 7, 10, 15]):\ndf_temp = df_trans[df_trans['day'] > 31-window].groupby('user')['amount'].agg([\n('trans_amount_mean_{}d'.format(window), 'mean'),\nfor window in [3, 5, 10]:\nfor col in ['type1', 'type2']:\ndf_temp = df_trans[df_trans['day'] > 31 - window].pivot_table(\nindex='user', columns=col, values='amount', aggfunc=['sum']).fillna(0)\ndf_temp.columns = ['trans_{}_{}_amount_{}_{}d'.format(col, f[1], window) for f in df_temp.columns]\ndf_temp.reset_index(inplace=True)\ndf_temp.rename({'index': 'user'}, inplace=True, axis=1)\nfor f in ['ip', 'ip_3']:\ndf_temp = df_trans.groupby(['user'])[f].agg([\n('trans_{}_count'.format(f), 'count')\nfor f in ['hour', 'trans_day_diff']:\ndf_temp = df_trans.groupby('user')[f].agg([\n('trans_{}_mean'.format(f), 'mean'),\nfor f in [\n'balance', 'balance_avg',\n'balance2_avg', 'product1_amount',\n'product3_amount',\n]:\ndf_feature[f] = df_feature[f].apply(lambda x: int(\ncate_features = ['sex', 'provider', 'level', 'verified', 'regist_type', 'agreement1', 'province', 'city', 'service3',\n'service3_level']\ndf_feature['{}_{}'.format(f1, f2)] = df_feature[f1] + '_' + df_feature[f2]\ndf_feature[dense_features] = min_max.fit_transform(df_feature[dense_features].values)\nfor f1 in tqdm(dense_features):\nif f1 != f2:\ndf_feature['product7_fail_ratio'] = df_feature[\n'product7_fail_cnt'] / df_feature['product7_cnt']\ndf_feature['card_cnt'] = df_feature['card_a_cnt'] + df_feature[\n'card_b_cnt'] + df_feature['card_c_cnt'] + df_feature['card_d_cnt']\ndf_feature['acc_card_ratio'] = df_feature['acc_count'] / df_feature['card_cnt']\ndf_feature['login_cnt'] = df_feature['login_cnt_period1'] + \\\ndf_feature['login_cnt_period2']\ndf_feature['login_cnt_period2_login_cnt_ratio'] = df_feature['login_cnt_period2'] / \\\ndf_feature['login_cnt']\ndf_feature['login_cnt_period1_login_cnt_ratio'] = df_feature['login_cnt_period1'] / \\\ndf_feature['op2_cnt']\ndf_feature['using_time_op1_cnt_ratio'] = df_feature['using_time'] / \\\n# \u6b3a\u8bc8\u7387\ndef stat(df, df_merge, group_by, agg):\ngroup = df.groupby(group_by).agg(agg)\ncolumns = []\nfor on, methods in agg.items():\nfor method in methods:\ncolumns.append('{}_{}_{}'.format('_'.join(group_by), on, method))\ngroup.columns = columns\ngroup.reset_index(inplace=True)\ndf_merge = df_merge.merge(group, on=group_by, how='left')\ndel (group)\ngc.collect()\nreturn df_merge\ndef statis_feat(df_know, df_unknow):\ndf_unknow = stat(df_know, df_unknow, ['province'], {'label': ['mean']})\ndf_unknow = stat(df_know, df_unknow, [\n'city', 'level'], {'label': ['mean']})\ndf_unknow = stat(df_know, df_unknow, ['op_device'], {'label': ['mean']})\n'age', 'op_device'], {'label': ['mean']})\ndf_unknow = stat(df_know, df_unknow, ['using_time'], {'label': ['mean']})\n'city', 'op_device'], {'label': ['mean']})\ndf_unknow = stat(df_know, df_unknow, ['age', 'city'], {'label': ['mean']})\n'op_device', 'level'], {'label': ['mean']})\ndf_train = df_feature[~df_feature['label'].isnull()]\ndf_train = df_train.reset_index(drop=True)\ndf_stas_feat = None\nkf = StratifiedKFold(n_splits=5, random_state=seed, shuffle=True)\nfor train_index, val_index in kf.split(df_train, df_train['label']):\ndf_fold_train = df_train.iloc[train_index]\ndf_fold_val = statis_feat(df_fold_train, df_fold_val)\ndf_stas_feat = pd.concat([df_stas_feat, df_fold_val], axis=0)\ndel (df_fold_train)\ndf_test = statis_feat(df_train, df_test)\ndf_feature = pd.concat([df_stas_feat, df_test], axis=0)\ndf_feature = df_feature.reset_index(drop=True)\ndel (df_stas_feat)\ndel (df_train)\n", "output_sequence": "Feature_Engineering"}, {"input_sequence": "# install and load packages\nlibraries = c(\"dplyr\", \"MASS\", \"gridExtra\", \"ggplot2\", \"Matrix\", \"parallel\", \"caTools\",\n\"np\", \"RColorBrewer\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# set working directory\nsetwd(\"/home/rama/Masterarbeit/pricing_kernels_and_implied_volatility\")\nload(\"epk3VolaIntervalsVDAX2m/locLinBWvdax2m.RData\")\ntimeToMaturity = 2/12\nhorizonPhysicalDensity = 300\nfor (trading_year in 2012) {\n\nyear = trading_year\n# path to Data\npathToData = paste(\"epk3VolaIntervalsVDAX2m/C_\", as.character(year), \"vdax2m\",\n\".csv\", sep = \"\")\n# read option data\nC_2012 = read.csv(pathToData, sep = \",\", header = TRUE)\nC_2012 = na.omit(C_2012)\n# load Dax and VDax data\nbothIndexesDax = read.csv(\"epk3VolaIntervalsVDAX2m/timeSeriesDaxVdax2m.csv\")\nbothIndexesDax = bothIndexesDax[, c(\"Date\", \"DAX\", \"VDAX\")]\nbothIndexesDax$Date = as.Date(as.character(bothIndexesDax$Date), \"%Y-%m-%d\")\nbothIndexesDax = bothIndexesDax[bothIndexesDax$Date >= as.Date(\"2000-01-01\"), ]\n# for selection of quantiles\nbothIndexesDaxQuant = bothIndexesDax[bothIndexesDax$Date <= as.Date(paste(as.character(year),\n\"-12-31\", sep = \"\")), ]\nbothIndexesDaxQuant = bothIndexesDaxQuant[bothIndexesDaxQuant$Date >= as.Date(paste(as.character(year -\n1), \"-12-31\", sep = \"\")), ]\nttmFraction = timeToMaturity\nspecifyQuant = c(0.05, 0.95)\nlowVolaIntUp = quantile(bothIndexesDaxQuant$VDAX, 0.35)\nvola_levels = as.character(seq(from = as.numeric(quantile(bothIndexesDaxQuant$VDAX,\nspecifyQuant))[1],\nto = as.numeric(quantile(bothIndexesDaxQuant$VDAX, specifyQuant))[2],\nlength.out = 20))\nlistRndPDpk = list()\nfor (vola_item in vola_levels) {\nprint(vola_item)\nVDAX_level = as.numeric(vola_item)\n\n# add ttm in days\nC_2012$ttmDays = (C_2012$TTM) * 365\n# moneyness(Strike/S_0)\nC_2012$moneyness = C_2012$EXERCISE_PRICE/C_2012$DAX\nmoneyness_est = seq(0.75, 1.5, length.out = 100)\n# option price\nC = C_2012[, \"settlement\"]\n# discount factor\nD = exp(-(C_2012$euriborRate) * C_2012$TTM)\n# scale by discount factor\nC = C/D\n# scale by forward price\nC = C/C_2012$DAX\n# subset option data used for estimation\noptionData = cbind(C, C_2012[, c(\"TTM\", \"moneyness\", \"VDAX\", \"Date\")])\noptionData$Date = as.Date(as.character(optionData$Date))\n# bandwidth specification as used for estimtion of the RND\nhTTM_RND = bandwidthMonKfoldCVyears[[as.character(year)]]$matur\n# Function performing local linear kernel regression\nlocLinRegression = function(moneynessValue, tauValue, data = optionData,\nh.TTM, h.moneyness, h.VDAX, output = \"C\") {\n\n# u_i, independent variables\nregressors = data[, c(\"TTM\", \"moneyness\", \"VDAX\")]\n# response(dependent) variable of regression equation\nDependentVar = data[, output]\nn = nrow(regressors)\nKernel = rep(0, n)\n# 3dim kernel function\nfor (j in 1:n) {\n\nKernel[j] = (dnorm((regressors$TTM[j] - tauValue)/h.TTM, mean = 0,\nsd = 1, log = FALSE)/h.TTM) * (dnorm((regressors$VDAX[j] - vdaxValue)/h.VDAX,\nmean = 0, sd = 1, log = FALSE)/h.VDAX) * (dnorm((regressors$moneyness[j] -\nmoneynessValue)/h.moneyness, mean = 0, sd = 1, log = FALSE)/h.moneyness)\n}\nKernel_matrix = Diagonal(n = length(Kernel), Kernel)\nregressors_minus_u = cbind(regressors$TTM - tauValue, regressors$VDAX -\nvdaxValue, regressors$moneyness - moneynessValue)\nOmega = cbind(1, regressors_minus_u)\ninvertedExpression = tryCatch(solve(t(Omega) %*% Kernel_matrix %*% Omega),\nerror = function(c) NA)\nif (is.element(NA, as.vector(invertedExpression))) {\nFirstOrderDerMon = NA\nconstTerm = NA\n} else {\nestCoef = invertedExpression %*% t(Omega) %*% Kernel_matrix %*% DependentVar\nFirstOrderDerMon = estCoef[4]\nout = list(firstOrderDerivativeMoneyness = FirstOrderDerMon, constTerm = constTerm)\nout\n}\n# 1dim RND by numerical differentiation of obtained 1st order derivative\ncoefficientsAll = mclapply(moneyness_est, locLinRegression, tauValue = ttmFraction,\nvdaxValue = VDAX_level, h.TTM = hTTM_RND, h.moneyness = hMon_RND, h.VDAX = hVDAX_RND)\nregression1DerMon = sapply(coefficientsAll, FUN = function(listItem) {\nlistItem[[\"firstOrderDerivativeMoneyness\"]]\n})\n# increment moneynes to compute derivtive\ndelta = 0.005\nmoneyness_est0 = moneyness_est + delta\n# compute regression at the changed moneyness\ncoefficientsAll0 = mclapply(moneyness_est0, locLinRegression, tauValue = ttmFraction,\nvdaxValue = VDAX_level, h.TTM = hTTM_RND, h.moneyness = hMon_RND, h.VDAX = hVDAX_RND)\nregression1DerMon0 = sapply(coefficientsAll0, FUN = function(listItem) {\nRND_given_ttm_final = (regression1DerMon0 - regression1DerMon)/delta\nRND_given_ttm = abs(RND_given_ttm_final * moneyness_est)\nexcess_return = log(moneyness_est)\n# plot(excess_return, abs(RND_given_ttm), type='p') trapz(excess_return,\n# abs(RND_given_ttm))\n# calculation of the conditional variance\noptionDataForResidualsIndex = seq(1, nrow(optionData), 280)\noptionDataForResiduals = optionData[optionDataForResidualsIndex, ]\nreplicateOptionDataForResiduals = lapply(1:nrow(optionDataForResiduals), FUN = function(index) optionDataForResiduals)\n# function to perform loc lin regression used for calculation of conditional\n# variance\nlocLinRegressionForRes = function(moneynessValue, tauValue, data = optionData,\noutput = \"C\") {\n# bandwidth specification\nh.TTM = sd(regressors$TTM) * nrow(regressors)^(-1/(4 + 3))\nestCoef = invertedExpression %*% t(Omega) %*% Kernel_matrix %*% DependentVar\n# preparation to compute residuals\nforResiduals = mcmapply(locLinRegressionForRes, optionDataForResiduals$moneyness,\noptionDataForResiduals$TTM, data = replicateOptionDataForResiduals)\nestimOptionPrice = unlist(forResiduals[\"constTerm\", ])\noptionDataForResiduals$estimOptionPrice = estimOptionPrice\n# squared residuals\noptionDataForResiduals$squaredResid = (optionDataForResiduals$C - optionDataForResiduals$estimOptionPrice)^2\n# local linear regresson with squared residual as a dependent variable\ncoefficientsResidualsReg = mclapply(moneyness_est, locLinRegressionForRes,\ntauValue = ttmFraction, vdaxValue = VDAX_level, data = optionDataForResiduals,\noutput = \"squaredResid\")\nconditionalVariance = sapply(coefficientsResidualsReg, FUN = function(listItem) {\nlistItem[[\"constTerm\"]]\nkernelConstant = 0.2115711\n# bandwidth specification for estimtion of the joint density, ROT\nhTTMjointDen = sd(optionData$TTM) * nrow(optionData)^(-1/(4 + 3)) * (4/5)^(1/(3 +\n4))\nhMonjointDen = sd(optionData$moneyness) * nrow(optionData)^(-1/(4 + 3)) * (4/5)^(1/(3 +\n4))\nhVDAXjointDen = sd(optionData$VDAX) * nrow(optionData)^(-1/(4 + 3)) * (4/5)^(1/(3 +\n4))\n# joint density of moneyness, tau, vola\njoinDensityRegressors = function(pointMon, pointTTM, pointVDAX, hMon, hTTM,\nhVDAX, data = optionData) {\njointDens = mean((1/hMon) * (1/hTTM) * (1/hVDAX) * dnorm((data$moneyness -\npointMon)/hMon,\nmean = 0, sd = 1, log = FALSE) * dnorm((data$TTM -\npointTTM)/hTTM, mean = 0, sd = 1, log = FALSE) * dnorm((data$VDAX -\npointVDAX)/hVDAX, mean = 0, sd = 1, log = FALSE))\njointDens\njointDenValues = unlist(mclapply(moneyness_est, joinDensityRegressors, pointTTM = ttmFraction,\npointVDAX = VDAX_level, hMon = hMonjointDen,\ndata = optionData))\nsigmaSquared = (moneyness_est^2) * (1/(3 * sqrt(pi))^3) * kernelConstant *\nabs(conditionalVariance) / jointDenValues\nfactorDistribution = nrow(optionData) * ((hMon_RND)^4) * (hMon_RND * hTTM_RND *\nhVDAX_RND)\nrndVariance = abs(sigmaSquared/factorDistribution)\nrndLocLinWithCI = data.frame(excess_return,\nRND_given_ttm,\nrndLocLinUp = RND_given_ttm + qnorm(0.95, 0, 1) * sqrt(rndVariance),\ncolnames(rndLocLinWithCI) = c(\"market_return\", \"RNDlocLin\", \"rndLocLinUp\",\n\"rndLocLinDown\")\n# Local const estimation of PD\n# used in contemporaneous approach for calculation of physical density\ndateHistDensity = as.Date(paste(as.character(year), \"-12-31\", sep = \"\"))\n# physical density estimation (using contemporaneous method)\n# maturity for which historical density to be calculated\ntauDays = round(ttmFraction * 365, 0)\n# how many observations from past to use for estimation of hist. density\nhorizon = horizonPhysicalDensity\n# take only those rows that are earlier than dateHistDensity\nbothIndexesDax = bothIndexesDax[bothIndexesDax$Date <= dateHistDensity, ]\n# supposed to contain returns of the index observed over each maturity\nreturnsDaxVdax = bothIndexesDax[1:horizon, c(\"DAX\", \"VDAX\")]\ncolnames(returnsDaxVdax) = c(\"DaxReturns\", \"VDAXlevel\")\nlengthBothIndexes = length(bothIndexesDax[, 1])\nfor (i in 1:horizon) {\nreturnsDaxVdax[i, 1] = log(bothIndexesDax$DAX[lengthBothIndexes - i]/(bothIndexesDax$DAX[lengthBothIndexes -\ni - tauDays]))\nreturnsDaxVdax[i, 2] = bothIndexesDax$VDAX[lengthBothIndexes - i]\n# Local const regression(NW) for esimation of conditional physical density\n# specify bandwidth\nbwC = npcdensbw(xdat = returnsDaxVdax$VDAXlevel, ydat = returnsDaxVdax$DaxReturns,\nbwmethod = \"normal-reference\")\nhDaxReturnsJoint = 1.5 * bwC$ybw\nhVDAXJoint = bwC$xbw\n# conditional kernel density estimation\nconditDensity = function(pointReturns, pointVDAX, hDaxReturnsJoint, hVDAXJoint,\nhVDAXmarginal, returnsDaxVdax) {\njointDens = mean((1/hDaxReturnsJoint) * (1/hVDAXJoint) * dnorm((returnsDaxVdax[,\n1] - pointReturns)/hDaxReturnsJoint, mean = 0, sd = 1, log = FALSE) *\ndnorm((returnsDaxVdax[, 2] - pointVDAX)/hVDAXJoint, mean = 0, sd = 1,\nlog = FALSE))\nmarginalDenVDAX = mean((1/hVDAXmarginal) * dnorm((returnsDaxVdax[, 2] -\npointVDAX)/hVDAXmarginal, mean = 0, sd = 1, log = FALSE))\noutCondDensity = jointDens/marginalDenVDAX\nnFactor = nrow(returnsDaxVdax)\nc1 = hDaxReturnsJoint\nconfInt = ((1/(4 * 3.14)) * outCondDensity/(c1 * c2 * marginalDenVDAX))/nFactor\nout = list(conditionalDensity = outCondDensity, confidenceInterval = confInt)\n# range of log returns (to calculate values of the density at these points)\neurostoxx_range = excess_return # from file with RND estimation\neurostoxx_hist_densityValues = lapply(eurostoxx_range, conditDensity,\npointVDAX = VDAX_level,\nhDaxReturnsJoint = hDaxReturnsJoint,\n# get density values, and variance separately\ndensityValues = sapply(eurostoxx_hist_densityValues, FUN = function(listItem) {\nlistItem[[1]]\nvariancePD = sapply(eurostoxx_hist_densityValues, FUN = function(listItem) {\nlistItem[[2]]\n# data frame with density values and CI\ndataPDlocConstWithCI = data.frame(returns = eurostoxx_range, PhysicalDensityLocalConstValue = densityValues,\npdLocalConstUpperBound = densityValues + qnorm(0.95, 0, 1) * sqrt(variancePD),\n# density integrates to 1 trapz(eurostoxx_range, densityValues)\n# EPK\nPK_value = (rndLocLinWithCI$RNDlocLin)/(dataPDlocConstWithCI$PhysicalDensityLocalConstValue)\nvarianceEPK = (1/(dataPDlocConstWithCI$PhysicalDensityLocalConstValue))^2 * rndVariance +\n((1/(dataPDlocConstWithCI$PhysicalDensityLocalConstValue))^4) * ((rndLocLinWithCI$RNDlocLin)^2) *\nvariancePD\nrndPDpk = data.frame(returns = eurostoxx_range,\nPricingKernel = PK_value,\npkUpperBound = PK_value + qnorm(0.95, 0, 1) * sqrt(varianceEPK),\nPhysicalDensityLocalConstValue = densityValues,\npdLocalConstUpperBound = densityValues + qnorm(0.95, 0, 1) * sqrt(variancePD),\nRND_given_ttm,\nrndLocLinUp = RND_given_ttm + qnorm(0.95, 0, 1) * sqrt(rndVariance),\nlistRndPDpk[[vola_item]] = rndPDpk\n}\n# save as RData\nsave(vola_levels,\nlistRndPDpk,\nyear,\nlowVolaIntUp,\nfile = paste0(\"epk3VolaIntervalsVDAX2m/listRndPDpkVDAX2m\", year, \".RData\"))\n# quantiles of VDAX used for CI\nquantile_20 = quantile(bothIndexesDaxQuant$VDAX, 0.2)\n#\n# definition of funtions for plotting of PKs, RNDs\nplotPK = function(yearItem){\n# load precomputed objects\nload(paste0(\"epk3VolaIntervalsVDAX2m/listRndPDpkVDAX2m\", as.character(yearItem), \".RData\"))\n# add aditional label to every plot\nremarkPlot = \"VDAX2m\"\n# prepare for plotting\nfor(itemN in (1:length(vola_levels))){\nlistRndPDpk[[vola_levels[itemN]]]$VDAXlevel = vola_levels[itemN]\n# bind all elements of the list in one data frame\ndataAllVDAXlevels = do.call(\"rbind\", listRndPDpk)\n# Plotting together with CI\n# small volatility interval\npkAllsmallVola = ggplot(data = dataAllVDAXlevels %>% filter(VDAXlevel <= as.numeric(lowVolaIntUp)),\naes(x = returns, y = PricingKernel, colour = VDAXlevel)) +\ngeom_line() +\ncoord_cartesian(xlim = c(-0.2, 0.2), ylim = c(0, 5)) +\ntheme_bw() +\ntheme(legend.position = \"none\") +\nggtitle(paste0(\"low volatility (between \",\nas.character(format(round(as.numeric(lowVolaIntDown), 2),\nnsmall = 2)),\n\" and \",\nas.character(format(round(as.numeric(lowVolaIntUp), 2), nsmall = 2)),\n\")\")) +\nxlab(\"\") +\nscale_colour_brewer(palette = \"PuOr\")\npkAllsmallVolaCI = ggplot(data = dataAllVDAXlevels %>%\nfilter(VDAXlevel ==\nas.numeric(vola_levels)[as.numeric(vola_levels)\n>= as.numeric(quantile_20)\naes(x = returns)) +\ngeom_line(aes(y = PricingKernel), size = 0.5, colour = \"black\") +\ngeom_ribbon(aes(ymin = pkLowerBound, ymax = pkUpperBound), alpha = 0.5) +\ncoord_cartesian(xlim = c(-0.2, 0.2), ylim = c(0, 5)) +\ntheme_bw() +\ntheme(legend.position = \"none\") +\nas.character(format(round(as.numeric(lowVolaIntDown),\n2), nsmall = 2)),\nxlab(\"\") +\n# medium volatility interval\npkAllmediumVola = ggplot(data = dataAllVDAXlevels %>%\nfilter(VDAXlevel > as.numeric(lowVolaIntUp),\naes(x = returns, y = PricingKernel, colour = VDAXlevel)) +\ngeom_line() +\ntheme(legend.position=\"none\") +\nggtitle(paste0(\"medium volatility (between \",\nas.character(format(round(as.numeric(lowVolaIntUp), 2), nsmall = 2)),\nylab(\"PK\") +\n# take first VDAX level that is not less than 50% quantile\npkAllmediumVolaCI = ggplot(data = dataAllVDAXlevels %>%\nfilter(VDAXlevel ==\nas.numeric(vola_levels)[as.numeric(vola_levels)\n>= as.numeric(quantile_50)\naes(x = returns))+\nas.character(format(round(as.numeric(mediumVolaIntUp), 2), nsmall = 2)),\n# high volatility\npkAllhighVola = ggplot(data = dataAllVDAXlevels %>%\nfilter(VDAXlevel > as.numeric(mediumVolaIntUp)),\naes(x = returns, y = PricingKernel, colour = VDAXlevel)) +\nggtitle(paste0(\"high volatility (between \",\nas.character(format(round(as.numeric(highVolaIntUp), 2), nsmall = 2)),\nscale_colour_brewer(palette=\"PuOr\")\npkAllhighVolaCI = ggplot(data = dataAllVDAXlevels %>%\nfilter(VDAXlevel ==\nas.numeric(vola_levels)[as.numeric(vola_levels)\n>= as.numeric(quantile_80)\naes(x = returns)) +\ncoord_cartesian(xlim = c(-0.2, 0.2), ylim = c(0, 5) ) +\n# allocate plots on the one page\nout = grid.arrange(pkAllsmallVola,\npkAllsmallVolaCI,\n# specify name of the plot to be saved\nplotName = paste(\"epk3VolaIntervalsVDAX2m/\",\n\"_\",\nas.character(year),\nggsave(plotName, out, width = 9, height = 12)\nplotPD = function(yearItem){\n# Plotting together with CI\n# small volatility interval\npdAllsmallVola = ggplot(data = dataAllVDAXlevels %>%\nfilter(VDAXlevel <= as.numeric(lowVolaIntUp)),\naes(x = returns,\ny = PhysicalDensityLocalConstValue,\ncolour = VDAXlevel)) +\ncoord_cartesian(xlim = c(-0.2, 0.2), ylim = c(0, 10)) +\nas.character(format(round(as.numeric(lowVolaIntDown), 2), nsmall = 2)),\nylab(\"PD\") +\npdAllsmallVolaCI = ggplot(data = dataAllVDAXlevels %>%\ngeom_line(aes(y = PhysicalDensityLocalConstValue), size = 0.5, colour=\"black\") +\ngeom_ribbon(aes(ymin = pdLocalConstLowerBound, ymax = pdLocalConstUpperBound), alpha=0.5) +\nylab(\"PD\")\npdAllmediumVola = ggplot(data = dataAllVDAXlevels %>%\naes(x = returns,\ny = PhysicalDensityLocalConstValue,\ncolour = VDAXlevel)) +\ncoord_cartesian(xlim = c(-0.2, 0.2), ylim=c(0, 10)) +\npdAllmediumVolaCI = ggplot(data=dataAllVDAXlevels %>%\naes(x = returns)) +\ngeom_line(aes(y = PhysicalDensityLocalConstValue), size = 0.5, colour = \"black\") +\ngeom_ribbon(aes(ymin = pdLocalConstLowerBound, ymax = pdLocalConstUpperBound), alpha = 0.5) +\npdAllhighVola = ggplot(data=dataAllVDAXlevels %>%\naes(x = returns, y = PhysicalDensityLocalConstValue, colour = VDAXlevel )) +\ncoord_cartesian(xlim = c(-0.2, 0.2), ylim = c(0, 10) ) +\nas.character(format(round(as.numeric(highVolaIntUp), 2), nsmall = 2)),\npdAllhighVolaCI = ggplot(data = dataAllVDAXlevels %>%\naes(x = returns))+\ncoord_cartesian(xlim = c(-0.2, 0.2), ylim = c(0, 8) ) +\nout = grid.arrange(pdAllsmallVola,\npdAllsmallVolaCI,\n\"_PD\",\nplotRND = function(yearItem){\naes(x = returns, y = RND_given_ttm, colour = VDAXlevel)) +\nylab(\"RND\") +\ngeom_line(aes(y = RND_given_ttm), size = 0.5, colour = \"black\") +\ngeom_ribbon(aes(ymin = rndLocLinDown, ymax = rndLocLinUp), alpha = 0.5) +\nylab(\"RND\")\naes(x = returns, y = RND_given_ttm, colour = VDAXlevel )) +\n\")\" )) +\npdAllmediumVolaCI = ggplot(data = dataAllVDAXlevels %>%\n\")\" ))+\npdAllhighVola = ggplot(data = dataAllVDAXlevels %>% filter(VDAXlevel > as.numeric(mediumVolaIntUp)),\naes(x = returns, y = RND_given_ttm, colour = VDAXlevel)) +\n\")\")) +\nylab(\"RND\") +\n\"RND\",\nplotPK(year)\n}\n", "output_sequence": "Estimates and plots (yearly) empirical pricing kernels (EPK), risk neutral densities (RND) and physical densities (PD) of DAX 30 index return conditional on time to maturity 2 months and different levels of VDAX-NEW-Subindex 2 (20 equally spaced numbers from 5% to 95% quantile of VDAX-NEW-Subindex 2 in a given year). Local linear kernel regression is used for estimation of the conditional risk neutral density and local constant kernel regression is used for estimation of conditional physical density. EPK is obtained as a ratio of RND and PD. The panels on the right-hand side of figures depict EPK, RND, PD conditional on 20%, 50% and 80% quantiles of VDAX-NEW-Subindex 2 with 95% confidence intervals. Colors from red to blue correspond to increasing values of volatility within each interval. All results are shown on a continuously compounded 2-months period returns scale."}, {"input_sequence": "# install and load packages\nlibraries = c(\"dplyr\", \"MASS\", \"gridExtra\", \"ggplot2\", \"Matrix\", \"parallel\", \"caTools\",\n\"np\", \"RColorBrewer\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# set working directory\nsetwd(\"/home/rama/Masterarbeit/pricing_kernels_and_implied_volatility\")\n# load Dax and VDax data\nbothIndexesDax = read.csv(\"termStructurePK/timeSeriesDaxVdax.csv\")\n# define functions\npk1mVDAX = function(bothIndexesDax, numberPK){\n\nfor (trading_year in 2012) {\n\nload(\"termStructurePK/locLinBW.RData\")\ntimeToMaturity = 1/12\nhorizonPhysicalDensity = 300\nyear = trading_year\n# path to Data\npathToData = paste(\"termStructurePK/C_\", as.character(year), \".csv\", sep = \"\")\n# read option data\nC_2012 = read.csv(pathToData, sep = \",\", header = TRUE)\nC_2012 = na.omit(C_2012)\nbothIndexesDax = bothIndexesDax[, c(\"Date\", \"DAX\", \"VDAX\")]\nbothIndexesDax$Date = as.Date(as.character(bothIndexesDax$Date), \"%Y-%m-%d\")\nbothIndexesDax = bothIndexesDax[bothIndexesDax$Date >= as.Date(\"2000-01-01\"), ]\n# for selection of quantiles\nbothIndexesDaxQuant = bothIndexesDax[bothIndexesDax$Date <= as.Date(paste(as.character(year),\n\"-12-31\", sep = \"\")), ]\nbothIndexesDaxQuant = bothIndexesDaxQuant[bothIndexesDaxQuant$Date >= as.Date(paste(as.character(year -\n1), \"-12-31\", sep = \"\")), ]\nttmFraction = timeToMaturity\nspecifyQuant = c(0.05, 0.95)\nlowVolaIntUp = quantile(bothIndexesDaxQuant$VDAX, 0.35)\nvola_levels = as.character(seq(from = as.numeric(lowVolaIntUp),\nto = as.numeric(mediumVolaIntUp),\nlength.out = numberPK))\nlistRndPDpk = list()\nfor (vola_item in vola_levels) {\nprint(vola_item)\nVDAX_level = as.numeric(vola_item)\n\n# add ttm in days\nC_2012$ttmDays = (C_2012$TTM) * 365\n# moneyness(Strike/S_0)\nC_2012$moneyness = C_2012$EXERCISE_PRICE/C_2012$DAX\nmoneyness_est = seq(0.75, 1.5, length.out = 100)\n# option price\nC = C_2012[, \"settlement\"]\n# discount factor\nD = exp(-(C_2012$euriborRate) * C_2012$TTM)\n# scale by discount factor\nC = C/D\n# scale by forward price\nC = C/C_2012$DAX\n# subset option data used for estimation\noptionData = cbind(C, C_2012[, c(\"TTM\", \"moneyness\", \"VDAX\", \"Date\")])\noptionData$Date = as.Date(as.character(optionData$Date))\n# bandwidth specification as used for estimtion of the RND\nhTTM_RND = bandwidthMonKfoldCVyears[[as.character(year)]]$matur\n# Function performing local linear kernel regression\nlocLinRegression = function(moneynessValue, tauValue, data = optionData,\nh.TTM, h.moneyness, h.VDAX, output = \"C\") {\n\n# u_i, independent variables\nregressors = data[, c(\"TTM\", \"moneyness\", \"VDAX\")]\n# response(dependent) variable of regression equation\nDependentVar = data[, output]\nn = nrow(regressors)\nKernel = rep(0, n)\n# 3dim kernel function\nfor (j in 1:n) {\n\nKernel[j] = (dnorm((regressors$TTM[j] - tauValue)/h.TTM, mean = 0,\nsd = 1, log = FALSE)/h.TTM) * (dnorm((regressors$VDAX[j] - vdaxValue)/h.VDAX,\nmean = 0, sd = 1, log = FALSE)/h.VDAX) * (dnorm((regressors$moneyness[j] -\nmoneynessValue)/h.moneyness, mean = 0, sd = 1, log = FALSE)/h.moneyness)\n}\nKernel_matrix = Diagonal(n = length(Kernel), Kernel)\nregressors_minus_u = cbind(regressors$TTM - tauValue, regressors$VDAX -\nvdaxValue, regressors$moneyness - moneynessValue)\nOmega = cbind(1, regressors_minus_u)\ninvertedExpression = tryCatch(solve(t(Omega) %*% Kernel_matrix %*% Omega),\nerror = function(c) NA)\nif (is.element(NA, as.vector(invertedExpression))) {\nFirstOrderDerMon = NA\nconstTerm = NA\n} else {\nestCoef = invertedExpression %*% t(Omega) %*% Kernel_matrix %*% DependentVar\nFirstOrderDerMon = estCoef[4]\nout = list(firstOrderDerivativeMoneyness = FirstOrderDerMon, constTerm = constTerm)\nout\n}\n# 1dim RND by numerical differentiation of obtained 1st order derivative\ncoefficientsAll = mclapply(moneyness_est, locLinRegression, tauValue = ttmFraction,\nvdaxValue = VDAX_level, h.TTM = hTTM_RND, h.moneyness = hMon_RND, h.VDAX = hVDAX_RND)\nregression1DerMon = sapply(coefficientsAll, FUN = function(listItem) {\nlistItem[[\"firstOrderDerivativeMoneyness\"]]\n})\n# increment moneynes to compute derivtive\ndelta = 0.005\nmoneyness_est0 = moneyness_est + delta\n# compute regression at the changed moneyness\ncoefficientsAll0 = mclapply(moneyness_est0, locLinRegression, tauValue = ttmFraction,\nvdaxValue = VDAX_level, h.TTM = hTTM_RND, h.moneyness = hMon_RND, h.VDAX = hVDAX_RND)\nregression1DerMon0 = sapply(coefficientsAll0, FUN = function(listItem) {\nRND_given_ttm_final = (regression1DerMon0 - regression1DerMon)/delta\nRND_given_ttm = abs(RND_given_ttm_final * moneyness_est)\nexcess_return = log(moneyness_est)\n# plot(excess_return, abs(RND_given_ttm), type='p') trapz(excess_return,\n# abs(RND_given_ttm))\n# calculation of the conditional variance\noptionDataForResidualsIndex = seq(1, nrow(optionData), 200)\noptionDataForResiduals = optionData[optionDataForResidualsIndex, ]\nreplicateOptionDataForResiduals = lapply(1:nrow(optionDataForResiduals), FUN = function(index) optionDataForResiduals)\n# function to perform loc lin regression used for calculation of conditional\n# variance\nlocLinRegressionForRes = function(moneynessValue, tauValue, data = optionData,\noutput = \"C\") {\n# bandwidth specification\nh.TTM = sd(regressors$TTM) * nrow(regressors)^(-1/(4 + 3))\nestCoef = invertedExpression %*% t(Omega) %*% Kernel_matrix %*% DependentVar\n# preparation to compute residuals\nforResiduals = mcmapply(locLinRegressionForRes, optionDataForResiduals$moneyness,\noptionDataForResiduals$TTM, data = replicateOptionDataForResiduals)\nestimOptionPrice = unlist(forResiduals[\"constTerm\", ])\noptionDataForResiduals$estimOptionPrice = estimOptionPrice\n# squared residuals\noptionDataForResiduals$squaredResid = (optionDataForResiduals$C - optionDataForResiduals$estimOptionPrice)^2\n# local linear regresson with squared residual as a dependent variable\ncoefficientsResidualsReg = mclapply(moneyness_est, locLinRegressionForRes,\ntauValue = ttmFraction, vdaxValue = VDAX_level, data = optionDataForResiduals,\noutput = \"squaredResid\")\nconditionalVariance = sapply(coefficientsResidualsReg, FUN = function(listItem) {\nlistItem[[\"constTerm\"]]\nkernelConstant = 0.2115711\n# bandwidth specification for estimtion of the joint density, ROT\nhTTMjointDen = sd(optionData$TTM) * nrow(optionData)^(-1/(4 + 3)) * (4/5)^(1/(3 +\n4))\nhMonjointDen = sd(optionData$moneyness) * nrow(optionData)^(-1/(4 + 3)) * (4/5)^(1/(3 +\n4))\nhVDAXjointDen = sd(optionData$VDAX) * nrow(optionData)^(-1/(4 + 3)) * (4/5)^(1/(3 +\n4))\n# joint density of moneyness, tau, vola\njoinDensityRegressors = function(pointMon, pointTTM, pointVDAX, hMon, hTTM,\nhVDAX, data = optionData) {\njointDens = mean((1/hMon) * (1/hTTM) * (1/hVDAX) * dnorm((data$moneyness -\npointMon)/hMon,\nmean = 0, sd = 1, log = FALSE) * dnorm((data$TTM -\npointTTM)/hTTM, mean = 0, sd = 1, log = FALSE) * dnorm((data$VDAX -\npointVDAX)/hVDAX, mean = 0, sd = 1, log = FALSE))\njointDens\njointDenValues = unlist(mclapply(moneyness_est, joinDensityRegressors, pointTTM = ttmFraction,\npointVDAX = VDAX_level, hMon = hMonjointDen,\ndata = optionData))\nsigmaSquared = (moneyness_est^2) * (1/(3 * sqrt(pi))^3) * kernelConstant *\nabs(conditionalVariance) / jointDenValues\nfactorDistribution = nrow(optionData) * ((hMon_RND)^4) * (hMon_RND * hTTM_RND *\nhVDAX_RND)\nrndVariance = abs(sigmaSquared/factorDistribution)\nrndLocLinWithCI = data.frame(excess_return,\nRND_given_ttm,\nrndLocLinUp = RND_given_ttm + qnorm(0.95, 0, 1) * sqrt(rndVariance),\ncolnames(rndLocLinWithCI) = c(\"market_return\", \"RNDlocLin\", \"rndLocLinUp\",\n\"rndLocLinDown\")\n# Local const estimation of PD\n# used in contemporaneous approach for calculation of physical density\ndateHistDensity = as.Date(paste(as.character(year), \"-12-31\", sep = \"\"))\n# physical density estimation (using contemporaneous method)\n# maturity for which historical density to be calculated\ntauDays = round(ttmFraction * 365, 0)\n# how many observations from past to use for estimation of hist. density\nhorizon = horizonPhysicalDensity\n# take only those rows that are earlier than dateHistDensity\nbothIndexesDax = bothIndexesDax[bothIndexesDax$Date <= dateHistDensity, ]\n# supposed to contain returns of the index observed over each maturity\nreturnsDaxVdax = bothIndexesDax[1:horizon, c(\"DAX\", \"VDAX\")]\ncolnames(returnsDaxVdax) = c(\"DaxReturns\", \"VDAXlevel\")\nlengthBothIndexes = length(bothIndexesDax[, 1])\nfor (i in 1:horizon) {\nreturnsDaxVdax[i, 1] = log(bothIndexesDax$DAX[lengthBothIndexes - i]/(bothIndexesDax$DAX[lengthBothIndexes -\ni - tauDays]))\nreturnsDaxVdax[i, 2] = bothIndexesDax$VDAX[lengthBothIndexes - i]\n# Local const regression(NW) for esimation of conditional physical density\n# specify bandwidth\nbwC = npcdensbw(xdat = returnsDaxVdax$VDAXlevel, ydat = returnsDaxVdax$DaxReturns,\nbwmethod = \"normal-reference\")\nhDaxReturnsJoint = 1.5 * bwC$ybw\nhVDAXJoint = bwC$xbw\n# conditional kernel density estimation\nconditDensity = function(pointReturns, pointVDAX, hDaxReturnsJoint, hVDAXJoint,\nhVDAXmarginal, returnsDaxVdax) {\njointDens = mean((1/hDaxReturnsJoint) * (1/hVDAXJoint) * dnorm((returnsDaxVdax[,\n1] - pointReturns)/hDaxReturnsJoint, mean = 0, sd = 1, log = FALSE) *\ndnorm((returnsDaxVdax[, 2] - pointVDAX)/hVDAXJoint, mean = 0, sd = 1,\nlog = FALSE))\nmarginalDenVDAX = mean((1/hVDAXmarginal) * dnorm((returnsDaxVdax[, 2] -\npointVDAX)/hVDAXmarginal, mean = 0, sd = 1, log = FALSE))\noutCondDensity = jointDens/marginalDenVDAX\nnFactor = nrow(returnsDaxVdax)\nc1 = hDaxReturnsJoint\nconfInt = ((1/(4 * 3.14)) * outCondDensity/(c1 * c2 * marginalDenVDAX))/nFactor\nout = list(conditionalDensity = outCondDensity, confidenceInterval = confInt)\n# range of log returns (to calculate values of the density at these points)\neurostoxx_range = excess_return # from file with RND estimation\neurostoxx_hist_densityValues = lapply(eurostoxx_range, conditDensity,\npointVDAX = VDAX_level,\nhDaxReturnsJoint = hDaxReturnsJoint,\n# get density values, and variance separately\ndensityValues = sapply(eurostoxx_hist_densityValues, FUN = function(listItem) {\nlistItem[[1]]\nvariancePD = sapply(eurostoxx_hist_densityValues, FUN = function(listItem) {\nlistItem[[2]]\n# data frame with density values and CI\ndataPDlocConstWithCI = data.frame(returns = eurostoxx_range, PhysicalDensityLocalConstValue = densityValues,\npdLocalConstUpperBound = densityValues + qnorm(0.95, 0, 1) * sqrt(variancePD),\n# density integrates to 1 trapz(eurostoxx_range, densityValues)\n# EPK\nPK_value = (rndLocLinWithCI$RNDlocLin)/(dataPDlocConstWithCI$PhysicalDensityLocalConstValue)\nvarianceEPK = (1/(dataPDlocConstWithCI$PhysicalDensityLocalConstValue))^2 * rndVariance +\n((1/(dataPDlocConstWithCI$PhysicalDensityLocalConstValue))^4) * ((rndLocLinWithCI$RNDlocLin)^2) *\nvariancePD\nrndPDpk = data.frame(returns = eurostoxx_range,\nPricingKernel = PK_value,\npkUpperBound = PK_value + qnorm(0.95, 0, 1) * sqrt(varianceEPK),\nPhysicalDensityLocalConstValue = densityValues,\npdLocalConstUpperBound = densityValues + qnorm(0.95, 0, 1) * sqrt(variancePD),\nRND_given_ttm,\nrndLocLinUp = RND_given_ttm + qnorm(0.95, 0, 1) * sqrt(rndVariance),\nlistRndPDpk[[vola_item]] = rndPDpk\n}\n# save as RData\nsave(vola_levels,\nlistRndPDpk,\nyear,\nlowVolaIntUp,\nfile = paste0(\"termStructurePK/listRndPDpkMain\", year, \".RData\"))\n# quantiles of VDAX used for CI\nquantile_20 = quantile(bothIndexesDaxQuant$VDAX, 0.2)\n#\n# definition of funtions for plotting of PKs, RNDs\nplotPK = function(yearItem){\n# load precomputed objects\nload(paste0(\"termStructurePK/listRndPDpkMain\", as.character(yearItem), \".RData\"))\n# add aditional label to every plot\nremarkPlot = \"main\"\n# prepare for plotting\nfor(itemN in (1:length(vola_levels))){\nlistRndPDpk[[vola_levels[itemN]]]$VDAXlevel = vola_levels[itemN]\n# bind all elements of the list in one data frame\ndataAllVDAXlevels = do.call(\"rbind\", listRndPDpk)\n# medium volatility interval\npkAllmediumVola = ggplot(data = dataAllVDAXlevels %>%\nfilter(VDAXlevel > as.numeric(lowVolaIntUp),\naes(x = returns, y = PricingKernel, colour = VDAXlevel)) +\ngeom_line() +\ncoord_cartesian(xlim = c(-0.2, 0.2), ylim = c(0, 5)) +\ntheme_bw() +\ntheme(legend.position=\"none\") +\nggtitle(paste0(\"medium volatility (between \",\nas.character(format(round(as.numeric(lowVolaIntUp), 2), nsmall = 2)),\n\" and \",\nas.character(format(round(as.numeric(mediumVolaIntUp), 2), nsmall = 2)),\n\")\", \"\\n\" ,\"TTM is 1 month, VDAX-NEW\")) +\nxlab(\"\") +\nscale_colour_brewer(palette = \"PuOr\")\n# allocate plots on the one page\nout = pkAllmediumVola\n\n# specify name of the plot to be saved\nplotName = paste(\"termStructurePK/\",\n\"_\",\nas.character(year),\nggsave(plotName, out, width = 9, height = 12)\nplotPK(year)\n}\n}\npk1mVDAX1m = function(bothIndexesDax, numberPK){\ntimeToMaturity = 1/12\nhorizonPhysicalDensity = 300\nload(\"termStructurePK/locLinBWvdax1m.RData\")\npathToData = paste(\"termStructurePK/C_\", as.character(year), \"vdax1m\",\n\".csv\", sep = \"\")\n# load Dax and VDax data\nvola_levels = as.character(seq(from = as.numeric(lowVolaIntUp),\nto = as.numeric(mediumVolaIntUp),\n# take sub sample for calculation of residuals, to make computations faster\noptionDataForResidualsIndex = seq(1, nrow(optionData), 280)\nsigmaSquared = (moneyness_est^2) * (1/(3 * sqrt(pi))^3) * kernelConstant *\nrndVariance = abs(sigmaSquared/factorDistribution)\nfile = paste0(\"termStructurePK/listRndPDpkVDAX1m\", year, \".RData\"))\nremarkPlot = \"VDAX1m\"\n\")\", \"\\n\", \"TTM is 1 month, VDAX-NEW-Subindex 1\")) +\n\npk2mVDAX2m = function(bothIndexesDax, numberPK){\ntimeToMaturity = 2/12\nload(\"termStructurePK/locLinBWvdax2m.RData\")\npathToData = paste(\"termStructurePK/C_\", as.character(year), \"vdax2m\",\nfile = paste0(\"termStructurePK/listRndPDpkVDAX2m\", year, \".RData\"))\nremarkPlot = \"VDAX2m\"\n\")\", \"\\n\", \"TTM is 2 months, VDAX-NEW-Subindex 2\")) +\npk3mVDAX3m = function(bothIndexesDax, numberPK){\ntimeToMaturity = 3/12\nload(\"termStructurePK/locLinBWvdax3m.RData\")\npathToData = paste(\"termStructurePK/C_\", as.character(year), \"vdax3m\",\nfile = paste0(\"termStructurePK/listRndPDpkVDAX3m\", year, \".RData\"))\nremarkPlot = \"VDAX3m\"\n\")\", \"\\n\" ,\"TTM is 3 months, VDAX-NEW-Subindex 3\")) +\n# execute functions\npk1mVDAX(bothIndexesDax, 10)\n", "output_sequence": "Estimates and plots (yearly) empirical pricing kernels (EPK) of DAX 30 index return conditional on times to maturity 1, 2 and 3 months and different levels of VDAX-NEW (10 equally spaced numbers from 35% to 65% quantile of VDAX-NEW in a given year). VDAX-NEW (and VDAX-NEW-Subindex 1), VDAX-NEW-Subindex 2 and VDAX-NEW-Subindex 3 were used for estimation of pricing kernels condiotioned by time to maturity 1, 2 and 3 months respectively. Local linear kernel regression is used for estimation of the conditional risk neutral density and local constant kernel regression is used for estimation of conditional physical density. Colors from red to blue correspond to increasing values of volatility."}, {"input_sequence": "# Cross Validation Parameter\nptrain = 0.85\nnfolds = 5\nnrepeats = 1\n# Tuning Parameter Values for Random Forest\nmtry.tuning = seq(from = 20, to = 160, by = 20)\n# Tuning Paramter Values for Gradient Boosting Machine\nntrees.tuning = c(100, 500, 1000)\nintdepth.tuning = 1\nshrinkage.tuning = seq(0.01, 0.15, by = 0.01)\nminobs.tuning = 10\n# install and load packages\nlibraries = c(\"ggplot2\", \"randomForest\", \"caret\", \"doParallel\", \"gbm\", \"xtable\", \"rpart\",\"rpart.plot\", \"Hmisc\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n#Read in training data\ndf = read.csv(\"train_preprocessed.csv\")\n# set rownumbers in dataframe to NULL\ndf$X = NULL\n# Simple Random Forest Model without parameter tuning for variable importance\nrf = randomForest(logSalePrice ~ ., data = df)\nimportance = rf$importance[order(rf$importance, decreasing = TRUE), , drop = FALSE]\nimportance.normalized = data.frame(variable = rownames(importance), Normalized_Importance = importance/importance[1])\nnames(importance.normalized)[2] = \"Normalized_Importance\"\nrownames(importance.normalized) = NULL\n# Plot Variable Importance\nimportance.plot = ggplot(data = importance.normalized[1:10, ], aes(x = variable, y = Normalized_Importance)) + geom_col() +\nscale_x_discrete(limits = importance.normalized[1:10, ]$variable) + theme_classic() + ggtitle(\"Random Forest Variable Importance Top 10\") +\nxlab(\"Variable\") + ylab(\"Normalized Variable Importance\")\nggsave(importance.plot, filename = \"rf_imp.png\", width = 20, height = 10, units = 'cm')\n# Export Variable Importance as Latex Table\nrows.per.table = 30\nrow.indices = seq(from = 1, to = nrow(importance.normalized), by = rows.per.table)\nlatex.vector = character(0)\nfor(i in 1:length(row.indices)){\ncap = paste0(\"Random Forest Variable Importances Table:\",i)\nlab = paste0(\"tab:importance\",i)\nimportance.normalized_latex = xtable(importance.normalized[row.indices[i]:min(row.indices[i] + rows.per.table - 1, nrow(importance.normalized)),,drop = FALSE], caption = cap, label =lab)\nlatex.vector = c(latex.vector, print(importance.normalized_latex))\n}\nall.latex = paste(latex.vector, collapse = \"\\n\")\nwriteLines(all.latex, con = \"importance.tex\")\n# importance.normalized_latex = print(importance.normalized_latex)\n# importance.normalized_latex = gsub(importance.normalized_latex, pattern = \"\\\\begin{table}[ht]\\n\\\\centering\\n\", replacement = \"\", fixed = TRUE)\n# importance.normalized_latex = gsub(importance.normalized_latex, pattern = \"\\\\begin{tabular}{rr}\", replacement = \"\\\\begin{longtable}{| p{.20\\\\textwidth} | p{.80\\\\textwidth} |} \", fixed = TRUE)\n# importance.normalized_latex = paste0(importance.normalized_latex, \"\\n\\\\end{longtable}\")\n# Parameter Tuning based on caret package Choose Valdation Method\nCrossValidation = trainControl(method = \"repeatedcv\", number = nfolds, p = ptrain, repeats = nrepeats)\n# Create Random Forest Tuning Grid:\nrfGrid = expand.grid(mtry = mtry.tuning)\n#Determine Number of Cores to use (All Cores but one)\ncores = max(c(detectCores() - 1, 1))\n#Set Up Cluster for parallel processing\ncl = makeCluster(cores)\nregisterDoParallel(cl)\nset.seed(123)\n#Tune Model\nrftuned = train(logSalePrice ~ ., data = df, method = \"rf\", importance = TRUE, trControl = CrossValidation, verbose = TRUE,\ntuneGrid = rfGrid)\nstopCluster(cl)\n# Save RF-Model\nsave(rftuned, file = \"rf.RData\")\n# Analyse Random Forest Results\nrfresults = rftuned$results\n# Export Results as Latex Table\nrfresults_latex = xtable(rfresults, caption = \"Random Forest tuning results\", label = \"tab:rfresults\")\nprint(rfresults_latex, file = \"rfresults.tex\")\n# Plot RMSE vs mtry\nrfrmse = ggplot(data = rfresults, aes(x = mtry, y = RMSE)) + geom_line() + theme_classic() + ggtitle(\"Root Mean Squared Error vs. Tuning Parameter mtry\") +\nxlab(\"mtry\") + ylab(\"RMSE\")\nggsave(rfrmse, filename = \"rf_rmse.png\", width = 20, height = 10, units = 'cm')\n# Plot RSquared vs mtry\nrfrsq = ggplot(data = rfresults, aes(x = mtry, y = Rsquared)) + geom_line() + theme_classic() + ggtitle(\"R Squared vs. Tuning Parameter mtry\") +\nxlab(\"mtry\") + ylab(\"R Squared\")\nggsave(rfrsq, filename = \"rf_rsq.png\", width = 20, height = 10, units = 'cm')\n# Tune Gradient Boosting Machine Create Tuning Grid\ngbmGrid = expand.grid(n.trees = ntrees.tuning, interaction.depth = intdepth.tuning, shrinkage = shrinkage.tuning, n.minobsinnode = minobs.tuning)\n# Start Cluster to paralellize\ngbmtuned = train(logSalePrice ~ ., data = df, method = \"gbm\", trControl = CrossValidation, verbose = TRUE, tuneGrid = gbmGrid)\nplot(gbmtuned)\n# Save GBM-Model\nsave(gbmtuned, file = \"gbm.RData\")\ngbmresults = gbmtuned$results\ngbmresults$n.trees = as.factor(gbmresults$n.trees)\n# Export GBM Tuning Results as latex table Export Results as Latex Table\ngbmresults_latex = xtable(gbmresults, caption = \"Gradient Boosting Machine tuning results\", label = \"tab:gmbresults\")\nprint(gbmresults_latex, file = \"gbmresults.tex\")\n# Plot RMSE vs. shrinkage for each value of n.trees\ngbmrmse = ggplot(data = gbmresults, aes(x = shrinkage, y = RMSE, colour = n.trees)) + geom_line() + theme_classic() + ggtitle(\"Root Mean Squared Error vs. Tuning Parameter Shrinkage\") +\nxlab(\"Shrinkage\") + ylab(\"RMSE\")\nggsave(gbmrmse, filename = \"gbm_rmse.png\", width = 20, height = 10, units = 'cm')\n# Plot RSquared vs. shrinkage for each value of n.trees\ngbmrsq = ggplot(data = gbmresults, aes(x = shrinkage, y = Rsquared, col = n.trees)) + geom_line() + theme_classic() + ggtitle(\"R Squared vs. Tuning Parameter Shrinkage\") +\nxlab(\"Shrinkage\") + ylab(\"R Squared\")\nggsave(gbmrsq, filename = \"gbm_rsq.png\", width = 20, height = 10, units = 'cm')\n#Visualise single regression tree for illustration in Report\ntree = rpart(logSalePrice ~ ., data = df)\npdf(file = \"tree.png\", width = 7, height = 7)\nrpart.plot(tree, compress = TRUE)\n", "output_sequence": "Reads in preprocessed train data and, trains and tunes RF and GBM Models and saves both the models as well as tuning results in graphical and tabular form"}, {"input_sequence": "#This program applies the Block Maxima method to annual losses of DJIA\nimport pandas as pd\nimport datetime\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import genextreme as gev\nfrom scipy import stats\nfrom scipy.stats import kstest\nimport quandl\ndf = quandl.get(\"BCB/UDJIAD1\", start_date=\"1929-01-01\", end_date=\"2018-12-31\")\nlog_returns = np.log(1 + df.pct_change())\nlog_returns.plot(figsize=(10, 6),legend = False, title='Log-returns');\nmax_loss= -log_returns.resample('Y').min()\nmax=np.array(max_loss)\nt = np.linspace(1,np.size(max_loss),np.size(max_loss))\nfig, ax = plt.subplots(figsize=(10, 6))\nplt.plot(t, max_loss,\"o\")\nplt.show()\nfit= gev.fit(max_loss)\nm=np.max(max_loss)\n#plot the fit\nshape, loc, scale = gev.fit(max_loss)\nl = loc + scale / shape\nx = np.linspace(l+0.00001, l+0.00001+m, num=100)\nplt.plot(x, gev.pdf(x, *fit),color='r', label='Fitted GEV')\nplt.hist(max,normed=True, alpha=0.6,color='blue', label='Annual maximum losses')\nax.legend(loc='best', frameon=False)\n#x.plot(t, pdf)\nprint('Shape')\nshape=-shape\nprint(shape)\nprint('Location')\nprint(loc)\nprint('Scale')\nx = np.linspace(0, 0.25, num=len(max_loss))\ntime=1/(1-gev.cdf(x,*fit))\nplt.plot(x,time,color='r', label='GEV return time')\nret=1/(1-gev.cdf(0.10,*fit))\nprint('Return time (years) of an annual maximum loss higher than 10 percent =%.3f' %ret )\nprob=(1-gev.cdf(x,*fit))\nplt.plot(x,prob,color='r', label='GEV P(X>u)')\nprob=(1-gev.cdf(0.10,*fit))\n", "output_sequence": "This program applies the Block Maxima method to annual losses of DJIA."}, {"input_sequence": "# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Sat Apr 20 13:36:27 2019\n\n@author: Luisa\n'''\nComputing Silhouette Scores and Calinski Harabaz Scores for k-means,\nHierchical Ward, Hierarchical Average and Complete Clustering\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom scipy.cluster.hierarchy import linkage, dendrogram, ClusterNode\nfrom scipy.spatial.distance import pdist\nfrom pylab import rcParams\nimport seaborn as sb\nimport sklearn\nfrom sklearn.cluster import AgglomerativeClustering\nimport sklearn.metrics as sm\n# Input: matrix is supposed to be a term-document matrix\n# SILHOUETTE SCORE\nscore=[]\nK= range(3,30)\nfor k in K:\nclusterer = KMeans (n_clusters=k) #using k-means++ here by default\npreds = clusterer.fit_predict(matrix)\ncenters = clusterer.cluster_centers_\nscore.append(silhouette_score(matrix, preds, metric='euclidean'))\nZ = linkage(matrix, method='complete')\nscoreH = []\nHclustering = AgglomerativeClustering(n_clusters =k, affinity='euclidean',linkage='ward')\nHclustering.fit(matrix)\nlabels = Hclustering.labels_\nscoreH.append(silhouette_score(matrix, labels))\nscoreH_complete = []\nfor k in K:\nH_complete_clustering = AgglomerativeClustering(n_clusters =k, affinity='euclidean',linkage='complete')\nH_complete_clustering.fit(matrix)\nlabels_complete = H_complete_clustering.labels_\nscoreH_complete.append(silhouette_score(matrix, labels_complete))\nscoreH_average = []\nH_average_clustering = AgglomerativeClustering(n_clusters =k, affinity='euclidean',linkage='average')\nH_average_clustering.fit(matrix)\nlabels_average = H_average_clustering.labels_\nscoreH_average.append(silhouette_score(matrix, labels_average))\n# inluding classical k-means\nscore_kmeans=[]\nclus_kmeans = KMeans(n_clusters=k, init='random')\npreds_kmeans = clus_kmeans.fit_predict(matrix)\ncenters_kmeans = clus_kmeans.cluster_centers_\nscore_kmeans.append(silhouette_score(matrix, preds_kmeans, metric='euclidean'))\nscore_cos=[]\nclus_cos = KMeans(n_clusters=k, distance=nltk.cluster.util.cosine_distance, repeats=25)\npreds_cos = clus_cos.fit_predict(matrix)\ncenters_cos = clus_cos.cluster_centers_\nscore_cos.append(silhouette_score(matrix, preds_cos, metric='euclidean'))\n\nplt.plot(K,scoreH, color='blue', label=\"Hierarchical Ward\")\nplt.plot(K,score, color='red', linestyle='dotted', label=\"k-means++\")\nplt.plot(K,scoreH_complete, color='blue', label=\"Hierarchical Complete\", linestyle='dashed')\nplt.plot(K, scoreH_average, color='blue', linestyle='dotted', label=\"Hierarchical Average\")\nplt.plot(K, score_kmeans, color='red', label='classical k-means')\nplt.xlabel('Number of Clusters')\nplt.ylabel('Silhouette Score')\nplt.legend()\nplt.savefig('SilhouetteScore.png', format='png', dpi=800)\n# CALINSKI HARABAZ SCORE\nscore_CH = []\nclusterer = KMeans (n_clusters=k) # k-means++\nscore_CH.append(calinski_harabaz_score(matrix, preds))\nscore_kmeans_CH=[]\nclus_kmeans = KMeans(n_clusters=k, init='random') #classical k-means\nscore_kmeans_CH.append(calinski_harabaz_score(matrix, preds_kmeans))\nscoreH_CH = []\nscoreH_CH.append(calinski_harabaz_score(matrix, labels))\nscoreH_complete_CH = []\nscoreH_complete_CH.append(calinski_harabaz_score(matrix, labels_complete))\nscoreH_average_CH = []\nscoreH_average_CH.append(calinski_harabaz_score(matrix, labels_average))\nplt.plot(K,scoreH_CH, color='blue', label=\"Hierarchical Ward\")\nplt.plot(K,score_CH, color='red', label=\"k-Means++\", linestyle='dotted')\nplt.plot(K,scoreH_complete_CH, color='blue', label=\"Hierarchical Complete\", linestyle='dashed')\nplt.plot(K, scoreH_average_CH, color='blue', linestyle='dotted', label=\"Hierarchical Average\")\nplt.plot(K, score_kmeans_CH, color='red', label=\"k-means\")\nplt.ylabel('Calinski Harabaz Score')\nplt.savefig('CalinskiHarabazScore.png', format='png', dpi=800)\n", "output_sequence": "Computing and plotting the Silhouette Score as well as the Calinski Harabaz Score for a k-means and several hierarchical clusterings (different distance measures)"}, {"input_sequence": "\"\"\"Scraper base module containing general scraper infrastructure methods.\"\"\"\nfrom selenium import webdriver\nfrom selenium.webdriver.common.keys import Keys\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport time\nclass Actions(ActionChains):\ndef wait(self, time_s: float):\nself._actions.append(lambda: time.sleep(time_s))\nreturn self\nclass ScraperBase:\ndef __init__(self, fresh):\nself.driver = self.driver_bootup()\nif fresh:\nself.query_terms = self.user_defined_terms()\ndef driver_bootup(self):\n\"\"\"Selenium driver \"\"\"\nchrome_options = Options()\n# chrome_options.add_argument(\"--headless\")\nchrome_options.add_experimental_option('prefs', {'profile.managed_default_content_settings.images': 2})\ndriver = webdriver.Chrome(chrome_options=chrome_options)\ndriver.minimize_window()\nreturn driver\ndef user_defined_terms(self):\n\"\"\"Prompt user for search terms.\"\"\"\nprint('Please enter query terms (words \u2013 maximum 3) separated each by a space.')\nuser_input = input('Enter terms: ')\nquery_terms = [value if i == 0 else f'+{value}' for i, value in enumerate(str(user_input).split())]\nif len(query_terms) > 3:\nquery_terms = query_terms[:3]\nelif len(query_terms) < 3:\npadded = ['', '']\nfor i, value in enumerate(query_terms):\npadded[i] = value\nquery_terms = padded\nreturn query_terms\n", "output_sequence": "Interactive, terminal based scraping tool built to collect news story text from the New York Times, BBC, CNN, and Reuters. The user chooses the desired news source and enters 1-3 search terms. Resulting publications are saved to a .csv file for further processing needs of the user."}, {"input_sequence": "\"\"\"Module containing Reuters site-specific scraping methods.\"\"\"\nfrom bs4 import BeautifulSoup\nfrom scraper_base import ScraperBase, Actions\nimport pandas as pd\nimport time\nclass ReutersScraper:\n\"\"\"Selenium connection to Reuters search page with bound methods.\"\"\"\ndef __init__(self):\nself.base = ScraperBase(fresh=True)\nself.driver = self.base.driver\nself.query_terms = self.base.query_terms\ndef query_reuters(self):\n\"\"\"Capture URL addresses for search results.\"\"\"\nquery_base = 'https://www.reuters.com/search/news?sortBy=&dateRange=&blob={}{}{}'\nquery_string = query_base.format(*self.query_terms)\nself.driver.get(query_string)\nself.driver.implicitly_wait(20)\nself.driver.find_element_by_id('_evidon-accept-button').click()\nfor i in range(0, 100):\ntry:\nreuters_more_btn = self.driver.find_elements_by_class_name('search-result-more-txt')\nActions(self.driver).click(reuters_more_btn[0]).wait(.8).perform()\nexcept:\nraw_html = BeautifulSoup(self.driver.page_source, features='lxml')\nreturn raw_html\ndef story_extractor(self, raw_html):\n\"\"\"Extract dictionary of articles and publication dates.\"\"\"\nstories = raw_html.find_all('div', {'class': 'search-result-indiv'})\npublished_dates = [article.findChild('h5', recursive=True).get_text() for article in stories]\nheadlines = [article.findChild('h3', recursive=True).findChild('a', recursive=True).get_text() for article in stories]\nquery = [self.query_terms for i in range(0, len(headlines))]\nindex = list(range(0, len(headlines)))\nelements_extracted = {a[0]: [a[4], a[1]] for a in list(zip(index, source, query, published_dates, headlines, hyperlinks))}\nprint(f'Keyword search results: {len(elements_extracted)}')\nreturn elements_extracted\ndef content_fetcher(self, elements_extracted):\n\"\"\"Navigate to hyperlink and fetch article text.\"\"\"\ndef nav_scrape(row, iteration):\n\"\"\"DataFrame wrapper for inserting scraped text into appropriate column.\"\"\"\nprint(f'Scraping source #{row.name}', end='\\r')\nhyperlink = row['hyperlink']\nself.driver.get(hyperlink)\nself.driver.implicitly_wait(1)\nraw_html = BeautifulSoup(self.driver.page_source, features='lxml')\nif raw_html.find('div', {'class': 'StandardArticleBody_body'}):\ntext = ' '.join([p.get_text() for p in story.find_all('p')])\nreturn text\ncontent_dataframe = pd.DataFrame.from_dict(elements_extracted, orient='index', columns = ['headline', 'hyperlink', 'published', 'query', 'source'])\ncontent_dataframe['content'] = content_dataframe.apply(nav_scrape, iteration=1, axis=1)\nreturn content_dataframe\n", "output_sequence": "Interactive, terminal based scraping tool built to collect news story text from the New York Times, BBC, CNN, and Reuters. The user chooses the desired news source and enters 1-3 search terms. Resulting publications are saved to a .csv file for further processing needs of the user."}, {"input_sequence": "\"\"\"Module containing NYT site-specific scraping methods.\"\"\"\nfrom bs4 import BeautifulSoup\nfrom scraper_base import ScraperBase, Actions\nimport pandas as pd\nimport time\nclass NYTScraper:\n\"\"\"Selenium connection to NYT search page with bound methods.\"\"\"\ndef __init__(self):\nself.base = ScraperBase(fresh=True)\nself.driver = self.base.driver\nself.query_terms = self.base.query_terms\ndef query_nyt(self):\n\"\"\"Capture URL addresses for search results.\"\"\"\nquery_base = 'https://www.nytimes.com/search?query={}{}{}'\nquery_string = query_base.format(*self.query_terms)\nself.driver.get(query_string)\nself.driver.implicitly_wait(20)\ntry:\ntopic = True\nnyt_topic = self.driver.find_elements_by_class_name('css-107jdae')[0]\ntopic_page = nyt_topic.find_element_by_tag_name('a')\npage_url = topic_page.get_attribute('href')\nself.driver.get(page_url)\nbtns = self.driver.find_elements_by_tag_name('button')\nnyt_more_btn = [btn for btn in btns if btn.text.lower() == 'show more']\nActions(self.driver).click(nyt_more_btn[0]).wait(1).perform()\ncurrent_browser_height = self.driver.execute_script('return document.body.scrollHeight')\nwhile topic:\nself.driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\ntime.sleep(3.5)\nnew_browser_height = self.driver.execute_script('return document.body.scrollHeight')\nif new_browser_height == current_browser_height:\nbreak\ncurrent_browser_height = new_browser_height\nexcept:\ntopic = False\nfor i in range(0, 80):\ntry:\nbtns = self.driver.find_elements_by_tag_name('button')\nnyt_more_btn = [btn for btn in btns if btn.text.lower() == 'show more']\nActions(self.driver).click(nyt_more_btn[0]).wait(1).perform()\nexcept:\nraw_html = BeautifulSoup(self.driver.page_source, features='lxml')\nreturn (raw_html, topic)\ndef story_extractor(self, raw_html, topic):\n\"\"\"Extract dictionary of articles and publication dates.\"\"\"\nif topic:\nstories = raw_html.find('div', {'class': 'stream'}).find_all('li')\nstories = [story for story in stories if 'story-id' in story.get('id')]\npublished_dates = [article.findChild('time', recursive=True)['datetime'] for article in stories]\nquery = [self.query_terms for i in range(0, len(headlines))]\nindex = list(range(0, len(headlines)))\nelse:\nstories = raw_html.find_all('li', {'class': 'css-1l4w6pd'})\npublished_dates = [article.findChild('time', recursive=True).get_text() for article in stories]\nhyperlinks = ['https://www.nytimes.com' + article.findChild('a', recursive=True)['href'] for article in stories]\nelements_extracted = {a[0]: [a[4], a[1]] for a in list(zip(index, source, query, published_dates, headlines, hyperlinks))}\nprint(f'Keyword search results: {len(elements_extracted)}')\nreturn elements_extracted\ndef content_fetcher(self, elements_extracted):\n\"\"\"Navigate to hyperlink and fetch article text.\"\"\"\ndef nav_scrape(row):\n\"\"\"DataFrame wrapper for inserting scraped text into appropriate column.\"\"\"\nif (row.name + 1) % 3 == 0:\nself.driver.quit()\nself.driver = ScraperBase(fresh=False).driver\nprint(f'Scraping Source #{row.name}', end='\\r')\nhyperlink = row['hyperlink']\nself.driver.get(hyperlink)\nself.driver.implicitly_wait(1)\nraw_html = BeautifulSoup(self.driver.page_source, features='lxml')\nif raw_html.find('p', {'class': 'story-body-text'}):\ntext = ' '.join([p.get_text() for p in raw_html.find_all('p', {'class': 'story-body-text'})])\nreturn text\nelif raw_html.find('p', {'class': 'css-18icg9x'}):\ntext = ' '.join([p.get_text() for p in raw_html.find_all('p', {'class': 'css-18icg9x'})])\ncontent_dataframe = pd.DataFrame.from_dict(elements_extracted, orient='index', columns = ['headline', 'hyperlink', 'published', 'query', 'source'])\ncontent_dataframe['content'] = content_dataframe.apply(nav_scrape, axis=1)\nreturn content_dataframe\n", "output_sequence": "Interactive, terminal based scraping tool built to collect news story text from the New York Times, BBC, CNN, and Reuters. The user chooses the desired news source and enters 1-3 search terms. Resulting publications are saved to a .csv file for further processing needs of the user."}, {"input_sequence": "\"\"\"Module containing BBC site-specific scraping methods.\"\"\"\nfrom bs4 import BeautifulSoup\nfrom scraper_base import ScraperBase, Actions\nimport pandas as pd\nimport time\nclass BBCScraper:\n\"\"\"Selenium connection to BBC search page with bound methods.\"\"\"\ndef __init__(self):\nself.base = ScraperBase(fresh=True)\nself.driver = self.base.driver\nself.query_terms = self.base.query_terms\ndef query_bbc(self):\n\"\"\"Capture URL addresses for search results.\"\"\"\nquery_base = 'https://www.bbc.co.uk/search?q={}{}{}&filter=news'\nquery_string = query_base.format(*self.query_terms)\nself.driver.get(query_string)\nself.driver.implicitly_wait(15)\nfor i in range(0, 50):\ntry:\nbbc_more_btn = self.driver.find_elements_by_class_name('more')\nActions(self.driver).click(bbc_more_btn[0]).wait(.6).perform()\nexcept:\nraw_html = BeautifulSoup(self.driver.page_source, features='lxml')\nreturn raw_html\ndef story_extractor(self, raw_html):\n\"\"\"Extract dictionary of articles and publication dates.\"\"\"\npublished_dates = [article.findChild('time', recursive=True)['datetime'] for article in raw_html.find_all(lambda tag: tag.has_attr('data-result-number'))]\nquery = [self.query_terms for i in range(0, len(headlines))]\nindex = list(range(0, len(headlines)))\nelements_extracted = {a[0]: [a[4], a[1]] for a in list(zip(index, source, query, published_dates, headlines, hyperlinks))}\nprint(f'Keyword search results: {len(elements_extracted)}')\nreturn elements_extracted\ndef content_fetcher(self, elements_extracted):\n\"\"\"Navigate to hyperlink and fetch article text.\"\"\"\ndef nav_scrape(row, iteration):\n\"\"\"DataFrame wrapper for inserting scraped text into appropriate column.\"\"\"\nif (row.name + 1) % 30 == 0:\nself.driver.quit()\nself.driver = ScraperBase(fresh=False).driver\nprint(f'Scraping source #{row.name}', end='\\r')\nhyperlink = row['hyperlink']\nself.driver.get(hyperlink)\nself.driver.implicitly_wait(1)\nraw_html = BeautifulSoup(self.driver.page_source, features='lxml')\nif raw_html.find('div', {'class': 'story-body__inner'}) or raw_html.find('div', {'property': 'articleBody'}):\nstory = raw_html.find('div', {'class': 'story-body__inner'})\ntext = ' '.join([p.get_text() for p in story.find_all('p')])\nreturn text\nelif raw_html.find('div', {'class': 'synopsis-toggle__long'}):\nstory = raw_html.find('div', {'class': 'vxp-media__summary'})\ntext = ' '.join([p.get_text() for p in story.find_all('p') if not p.findChild('i')])\nelse:\nif iteration == 1:\nnav_scrape(row, 2)\ncontent_dataframe = pd.DataFrame.from_dict(elements_extracted, orient='index', columns = ['headline', 'hyperlink', 'published', 'query', 'source'])\ncontent_dataframe['content'] = content_dataframe.apply(nav_scrape, iteration=1, axis=1)\nreturn content_dataframe\n", "output_sequence": "Interactive, terminal based scraping tool built to collect news story text from the New York Times, BBC, CNN, and Reuters. The user chooses the desired news source and enters 1-3 search terms. Resulting publications are saved to a .csv file for further processing needs of the user."}, {"input_sequence": "\"\"\"Main module for scraping suite.\"\"\"\nimport sys\nsys.path.append('../')\nimport requirements as req\npackage_installer = req.PackageInstaller()\nfrom pick import pick\nfrom source_selector import source_selector\ndef main():\n\"\"\"Main function for news source data scraping tool.\"\"\"\ntitle = 'Please select a news source from which to gather text data (press ENTER to select indicated option): '\noptions = ['BBC', 'NYT', 'Reuters', 'CNN']\nselected_source = pick(options, title, multi_select=False, min_selection_count=1)[0].lower()\nsource_data, query = source_selector(selected_source)\nsource_data.to_csv(f'../data/scraped/{selected_source}_{query}.csv')\nif __name__ == '__main__':\nmain()\n", "output_sequence": "Interactive, terminal based scraping tool built to collect news story text from the New York Times, BBC, CNN, and Reuters. The user chooses the desired news source and enters 1-3 search terms. Resulting publications are saved to a .csv file for further processing needs of the user."}, {"input_sequence": "\"\"\"Module for source selection and further forking of tasks to sub-modules.\"\"\"\nfrom scraper_bbc import BBCScraper\nfrom scraper_reuters import ReutersScraper\ndef source_selector(selected_source):\n\"\"\"Filter function to select correct course of action (scraper method).\"\"\"\nif selected_source == 'bbc':\nscraper = BBCScraper()\nterms = '_'.join(term for term in scraper.query_terms if len(term) > 0)\nraw_html = scraper.query_bbc()\nelements_extracted = scraper.story_extractor(raw_html)\ncontent_dataframe = scraper.content_fetcher(elements_extracted)\nscraper.driver.quit()\nelif selected_source == 'nyt':\nscraper = NYTScraper()\nraw_html, topic = scraper.query_nyt()\nelements_extracted = scraper.story_extractor(raw_html, topic)\nelif selected_source == 'reuters':\nscraper = ReutersScraper()\nraw_html = scraper.query_reuters()\nelif selected_source == 'cnn':\nscraper = CNNScraper()\nelements_extracted = scraper.query_cnn()\ncontent_dataframe = content_dataframe.loc[~content_dataframe['content'].isnull()]\nreturn content_dataframe, terms\n", "output_sequence": "Interactive, terminal based scraping tool built to collect news story text from the New York Times, BBC, CNN, and Reuters. The user chooses the desired news source and enters 1-3 search terms. Resulting publications are saved to a .csv file for further processing needs of the user."}, {"input_sequence": "\"\"\"Module containing CNN site-specific scraping methods.\"\"\"\nfrom bs4 import BeautifulSoup\nfrom scraper_base import ScraperBase, Actions\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.webdriver.common.by import By\nfrom pprint import pprint\nimport pandas as pd\nimport time\nclass CNNScraper:\n\"\"\"Selenium connection to CNN search page with bound methods.\"\"\"\ndef __init__(self):\nself.base = ScraperBase(fresh=True)\nself.driver = self.base.driver\nself.query_terms = self.base.query_terms\ndef query_cnn(self):\n\"\"\"Capture URL addresses for search results.\"\"\"\nquery_base = 'https://edition.cnn.com/search/?q={}{}{}&type=article'\nquery_string = query_base.format(*self.query_terms)\nself.driver.get(query_string)\nself.driver.implicitly_wait(20)\ntotal_item_string = self.driver.find_element_by_class_name('cnn-search__results-count').text\ntotal_item_count = int(re.search('.*out\\sof\\s(\\d+).*', total_item_string).group(1))\nsearch_range = [str(value) for value in list(range(0, total_item_count, 30))]\nelements_extracted_all = {}\nfor grouping in search_range:\nquery_terms_copy = self.query_terms[:]\nquery_flex = 'https://edition.cnn.com/search/?size=30&q={}{}{}&type=article&from={}'\nquery_string = query_flex.format(*query_terms_copy)\nself.driver.get(query_string)\nfull_page_element = WebDriverWait(self.driver, 5).until(EC.presence_of_element_located((By.CLASS_NAME, 'cnn-search__result--article')))\nraw_html = BeautifulSoup(self.driver.page_source, features='lxml')\nelements_extracted_all.update(self.story_extractor(raw_html, int(grouping)))\nprint(f'Keyword search results: {len(elements_extracted_all)}')\nreturn elements_extracted_all\ndef story_extractor(self, raw_html, base):\n\"\"\"Extract dictionary of articles and publication dates.\"\"\"\nstories = raw_html.find_all('div', {'class': 'cnn-search__result--article'})\npublished_dates = [article.findChild('span', {'class': 'icon--timestamp'}).findNext('span').get_text() for article in stories]\nheadlines = [article.findChild('h3', {'class': 'cnn-search__result-headline'}).get_text().strip().strip('\\n') for article in stories]\nhyperlinks = ['http://' + link if link[:3] == 'www' else link for link in hyperlinks]\nquery = [self.query_terms for i in range(0, len(headlines))]\nindex = list(range(base, base + 30))\nelements_extracted = {a[0]: [a[4], a[1]] for a in list(zip(index, source, query, published_dates, headlines, hyperlinks))}\nreturn elements_extracted\ndef content_fetcher(self, elements_extracted):\n\"\"\"Navigate to hyperlink and fetch article text.\"\"\"\ndef nav_scrape(row, iteration):\n\"\"\"DataFrame wrapper for inserting scraped text into appropriate column.\"\"\"\nprint(f'Scraping source #{row.name}', end='\\r')\nhyperlink = row['hyperlink']\nself.driver.get(hyperlink)\ntime.sleep(1.2)\nif raw_html.find('section', {'class': 'zn-body-text'}):\ntext = ' '.join([div.get_text() for div in story.find_all(class_='zn-body__paragraph')])\nreturn text\nelif raw_html.find('div', {'id': 'storytext'}):\ntext = ' '.join([p.get_text() for p in story.find_all('p')])\ncontent_dataframe = pd.DataFrame.from_dict(elements_extracted, orient='index', columns = ['headline', 'hyperlink', 'published', 'query', 'source'])\ncontent_dataframe['content'] = content_dataframe.apply(nav_scrape, iteration=1, axis=1)\nreturn content_dataframe\n", "output_sequence": "Interactive, terminal based scraping tool built to collect news story text from the New York Times, BBC, CNN, and Reuters. The user chooses the desired news source and enters 1-3 search terms. Resulting publications are saved to a .csv file for further processing needs of the user."}, {"input_sequence": "%sort CVaR descending and plot a simple graph, where\n%minimum easily seen and one can see a convergence\nmat1 = array2table(mat)\nchr = [{'one', 'two', 'three', 'four', 'five','six','seven','eight','nine'}]\ntrans = array2table(transpose(chr))\nmat1 (:,10) = trans\nsorted = sortrows(mat1,-5)\ny = table2array(sorted(:,5))\nplot(y)\nset(gca,'xticklabel',x.')\ntitle('CVaR-Convergence')\nxlabel('Iterations')\nylabel('CVaR-Value')\n", "output_sequence": "Plotting the CVaR for all sets of different initial weights and see it converging"}, {"input_sequence": "import numpy as np\n# Pass an numpy array and generate the subsets:\ndef generate_Xtrain_yTrain_Xtest_Ytest(data,train_size_percentage,timesteps_in,timesteps_out):\ntrain_size=int(len(data)*train_size_percentage)\ntrain_data, test_data = data[:train_size],\nX_train_data, y_train_data=split_sequence(train_data,timesteps_in,timesteps_out)\nreturn X_train_data, y_train_data,X_test_data, y_test_data\nETH=ethereum[\"Hourly_returns\"].sort_index(ascending=True).values.astype('float32')\n# We don't want to set the window size too big as this would increase the computational cost enormously\n# However the window size should not be set too small as the LSTM may then never reach the global minimumof the error function\n# test the LSTM with these four values:\ntimesteps=[12,24,48,96]\nrmse_ar=[]\nfor time in timesteps:\nX_train, X_test, y_test=generate_Xtrain_yTrain_Xtest_Ytest(ETH,0.8,time,24)\nmodel=multi_step_LSTM(n_steps_in=time)\nmodel.fit(X_train, y_train,verbose=0)\nrmse=round(model.evaluate(X_test,y_test,verbose=0),4)\nrmse_ar.append(rmse)\n\nsns.set(style=\"white\")\nax =sns.barplot(timesteps,rmse_ar,color=\"blue\")\nax.set(xlabel='Window Size', ylabel='Root-mean-squared Error', title=\"Results for Choosing the Window Size\")\nplt.show()\n", "output_sequence": "Select the optimal window-size for the training of the LSTM"}, {"input_sequence": "# clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# install and load packages\nlibraries = c(\"foreign\", \"gstat\", \"spacetime\", \"fossil\", \"tseries\", \"mvtnorm\",\n\"xts\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# load dataset of five countries\nload(\"fivecountries.RData\")\nresultsres = commonans0915mac$vt #uk, germany, france, italy, sweden\ntransformC = function(data, method = \"uniform\") {\nf1 = function(x) {\nfn = ecdf(x)\nreturn(y)\n}\nnr = nrow(data)\ndata1 = sapply(1:nc, function(i) f1(data[, i]))\ncolnames(data1) = colnames(data)\ndata1\n}\n# the basic setting for optimization\nnu = 0.5\nreltol = 0.01\ntrace = TRUE\nmaxit = 50000\nsigma = 1\ncutoff = 2000\ntlag = 20\nnamelist = c(\"UK\", \"Germany\", \"France\", \"Italy\", \"Sweden\")\nlocation = read.csv(\"location_country.csv\")\nrownames(location) = location[, 1]\nlocation = location[namelist, ]\n## transform to copulae\nd3 = t(resultsres)\nd4 = qnorm(transformC(d3) * nrow(d3)/(nrow(d3) + 1)) #formula 3.12 in paper\nlocation = SpatialPoints(location, proj4string = CRS(\"+proj=longlat +datum=WGS84\"))\nstart = c(2009, 11)\ntime = seq(ISOdate(start[1], start[2], 28), ISOdate(end[1], end[2], 28),\nby = \"1 months\")\ndata = d4\nda = as.data.frame(as.vector(t(data)))\ncolnames(da) = \"inf\"\ninflation = STFDF(location, time, da)\nsummary(sort(spDists(inflation@sp)))\nvvd4 = variogram(inf ~ 1, inflation, width = 400, cutoff = 2000, tlags = 0:35)\nvvd4[1, 2:3] = 0\nna_indi = which(is.na(vvd4), arr.ind = TRUE)\nvvd4t = vvd4\nfor (i in 1:nrow(na_indi)) {\nvvd4t[na_indi[i, 1], na_indi[i, 2]] = 0.5 *\n(vvd4[na_indi[i, 1] - 1, na_indi[i, 2]] + vvd4[na_indi[i, 1] + 1, na_indi[i, 2]])\nstart = c(2009, 11)\ntime = seq(ISOdate(start[1], start[2], 28), ISOdate(end[1], end[2], 28),\nby = \"1 months\")\nd3 = t(resultsres)\nd4 = qnorm(transformC(d3) * nrow(d3)/(nrow(d3) + 1)) #formula 3.12 in paper\ndata = d4\nda = as.data.frame(as.vector(t(data)))\n#inflation = STFDF(location, time, da)\ndd1 = (spDists(inflation@sp))\ndd = dd1\nrm(location, time, da, inflation, dd1)\n# WLS as, initial parameter, Chapter 5.2 set multiple initial value for\n# WLS estimation\na = c(1e-10, 0.5, 1)\nb = c(1e-20, 1e-05, 0.1, 0.3)\n# choice from multiple optimal points of WLS estimates\nfit.count = function(vv, a, b, beta) {\nop = expand.grid(1:length(a), 1:length(b),\nfitv = function(vv, a, b, beta) {\nreturn(c(k$par, k$value))\nk = sapply(1:nrow(op), function(i) fitv(vv, a = a[op[i, 1]],\nb = b[op[i, 2]], beta = beta[op[i, 3]]))\nxabc = function(x, abc) {\nabc[x]\nop[, 1] = xabc(op[, 1], a)\no = cbind(op, t(k))\ncolnames(o) = c(\"a_i\", \"beta_i\", \"a\", \"beta\", \"value\")\no\n# WLS estimate\nfit.vv = function(vv, a = 0.01, b = 0.005, beta = 0.1, nu) {\nsigma = 1\npar0 = c(a, b, beta)\nfit.model = function(data0, par0, nu, sigma) {\na = par0[1]\nweightFun = data0$np/data0$gamma^2\nweightFun[1] = 0\ngammaMod = sigma - sapply(1:nrow(data0), function(i)\nKernel(data0$avgDist[i], data0$timelag[i], nu, a, b, beta, sigma))\nsum = sum(weightFun * (data0$gamma - gammaMod)^2)\nreturn(sum)\nvv[1, 2:3] = 0\nk = optim(par = par0, fit.model, data0 = vv, nu = nu, sigma = sigma,\nmethod = \"L-BFGS-B\", lower = c(1e-10, 0.1))\nk\nKernel = function(h, u, nu, a, b, beta, sigma) {\n# same as in GeoCopula\ny = rep(1, length = length(h))\nidx1 = which(h == 0)\nc1 = a^2 * u^2 + 1\ny[idx1] = sigma * beta/c1^nu/c2\ny[idx2] = sigma * 2 * beta/c1^nu/c2/gamma(nu) * (b/2 * h[idx2] * (c1/c2)^0.5)^nu *\nbesselK(b * h[idx2] * (c1/c2)^0.5, nu)\nopti = fit.count(vvd4t, a, b, beta)\nsave(opti, file = \"opti.RData\")\n", "output_sequence": "Produces the estimation results using GeoCopula approach and saves it as RDate file."}, {"input_sequence": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Tue Dec 29 11:27:17 2020\n@author: jane_hsieh\nArticle:\n1. For package of HMM\u2013 hmmlearn please refers to its tutorial:\nhttps://hmmlearn.readthedocs.io/en/latest/tutorial.html\n2. More example, refer to <Hidden Markov Model>\nhttps://medium.com/@kangeugine/hidden-markov-model-7681c22f5b9\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n#====================== 0. Input: Observation O_t, and true Hidden states (for comparison) ===================================\nimport Generate_Dice_Sample as GDice\n#0.1.---------------- Set parameters for sampling & those of Hidden Markov Model(HMM) --------------------------------\n## Sampling-related parameters\nseed = 123 # integer or None (i.e., random)\nN = 5000 #sample size\n## HMM-related parameters\npi_0 = 0.5 # initial prob of hiddent state p(S_0 = 0) [0: fair dice; 1: Loaded dice]; p(S_0=1) = 1-pi_0\np01 = 0.05 # Transition prob from 0(F) to 1(L)); p00 = 1- p01\n### Emission probs: pF vs pL are distributions of Fair vs Loaded Dice with point = [1,2,...,6], respectively\n### Note: length of Dice, pF, pL must be the same\nDice = [i+1 for i in range(6)]\npL=[0.1, 0.1, 0.5]\n#2.0.------------------------ Generate sequence of {O_t}, from t=0,...,N-1 ------------------------\n## To reproduce the experiment, seed is fixed; o.w. you can set seed = None for fully random gereration\nHid, Obs = GDice.GenerateFLDice(N=N, p01=p01, p10=p10, pL=pL, Dice = Dice, pi_0=pi_0, seed=seed)\nHid = pd.DataFrame(Hid, columns=['Hid'])\n#Obs.to_csv('Sample_Obs.csv')\n## Data Visualization --------------------------------------------------------------------------------------------\ndata = pd.concat([Hid, Obs],axis=1)\ndef plot_timeseries(axes, x, y, color, xlabel, linestyle='-', marker='None'):\naxes.plot(x, y, color=color, linestyle = linestyle, marker = marker, ms = 1.6)\naxes.set_xlabel(xlabel)\naxes.set_ylabel(ylabel, color=color)\naxes.tick_params('y', colors=color)\nn= 300\nsample = data[:n]\ndel data\nfig, ax = plt.subplots(figsize = (10,5))\nplot_timeseries(ax, sample.index, sample['Obs'],'blue', 'index', 'Dice Observations', linestyle='None', marker='o')\nax2 = ax.twinx()\nplot_timeseries(ax2, sample.index, sample['Hid'],'orange', 'index', 'Hidden States')\nax2.set_yticks([0,1])\nax.set_title('First {} cases of dice sequence'.format(n))\nplt.show()\nfig.savefig('First {} cases of dice sequence.png'.format(n), dpi = 300)\n#======================================== 1. Model Training and Prediction ==================================================\nfrom hmmlearn import hmm\n##load model\u2013 MultinomialHMM which is Hidden Markov Model with multinomial (discrete) emissions\nmodel = hmm.MultinomialHMM(n_components=2, algorithm='viterbi', n_iter=1000, tol=0.001)\n'''\nn_components: #{hidden states}\n##Training and Prediction of Hidden states\nmodel.fit(Obs)\nHid_pre = model.predict(Obs)\nlogprob = model.score(Obs) #log-probability of sequence Obs\nTo prevent local maximum of EM algorithm, the model is trained for several times (it = 0,...,10),\neach time with random initial parameter values.\nThe finest model with the highest log-probability of sequence Obs is choosed and reported at last\nfor it in range(10):\nmodel1 = hmm.MultinomialHMM(n_components=2, algorithm='viterbi', n_iter=1000, tol=0.001)\nmodel1.fit(Obs)\nlogprob1 = model1.score(Obs)\nprint('[{}] compare logprob ={} vs. logprob1={}'.format(it, logprob,\nif logprob1 > logprob:\nmodel = model1\nlogprob = model.score(Obs)\nprint('model updated')\n\nprint('The possible highest log-probability found in all iteration is: {}'.format(logprob))\n## To make consistent of the symbols of Hid_pre with Hid, transform the symbols of Hid_pre as follows:\ntemp = list(map(lambda x: 0 if x==1 else 1, Hid_pre))\n##Monitoring convergence of EM algorithm\nprint(model.monitor_)\nConvergenceMonitor(\nhistory=[-853.4456094067322, -853.4359720533312],\niter=62,\nn_iter=1000,\ntol=0.01,\nverbose=False,\n)\n##Does the algorithm converge?\nprint(\"Does the algorithm converge? \\n\",model.monitor_.converged) #True\n# ---------------------------------------------------------------------------------------------------------------------------------------\n# --------------------------- Problem 1. [Evaluation] Given a known model what is the likelihood of sequence Obs happening? ----------------------\nimport math\nlogprob = model.score(Obs)\nprint(\"The prob with seq. Obs given the fitted model is {}\".format(math.exp(logprob)))\n## Suppose new seq. X, compute its prob.\nX = np.array([1,6,6,4]).reshape(-1, 1)\nlogprobX = model.score(X)\nprint(\"The prob with seq.\u2013{} given the fitted model is {}\".format(X.tolist(), math.exp(logprobX)))\n# --------------------------- Problem 2. [Decoding] Given a known model and sequence Obs, what is the optimal hidden state sequence? ------------\nlogprob, Hid_pre = model.decode(Obs)\nprint(\"The prob with seq.\u2013{} given the fitted model is {}\".format(X.tolist(), math.exp(logprobX)))\ntemp = pd.concat([Hid, pd.DataFrame(Hid_pre, columns=['Hid_pre'])], axis = 1)\n## Check prediction accuracy\n### confusion matrix\n#Notice: check if the labels from true hidden seq. (Hid) are consistent with those form predicted seq. (Hid_pre)\n#If yes:\nconf_mat = pd.crosstab(Hid.values.squeeze(), Hid_pre,\nrownames=['True'],\n#o.w., as below:\nHid = Hid.squeeze().map(lambda x: 1 if x==0 else 0) #Now 0=Loaded dice, 1=Fair dice\nconf_mat = pd.crosstab(Hid, Hid_pre,\n\nprint('Confusion matrix: \\n', conf_mat)\n### acc\nfrom sklearn.metrics import accuracy_score\nacc = accuracy_score(Hid, Hid_pre)\nprint('Accuracy: \\n', acc)\n#????????????????????????????????????????????????????\n#temp.plot(linestyle='None', markersize = 10.0)\n#plt.show()\n#---------------------------- Problem 3. [Learning] Given sequence Obs and number of hidden states, what is the optimal model which maximizes the probability of O?-------\n#Initial state occupation distribution\nprint(\"Estimated initial state occupation distribution: \\n {} \\n\".format( model.startprob_))\n#Matrix of transition probabilities between states. dim= (n_components,\nprint(\"Estimated matrix of transition probabilities between states: \\n {} \\n\".format(model.transmat_ ))\n#Probability of emitting a given symbol when in each state. dim= ((n_components, n_features))\nprint(\"Probability of emitting a given symbol when in each state \\n {}\".format(model.emissionprob_))\n", "output_sequence": "Use standard HMM package (i.e., hmmlearn) to analyze the data with multinomial (discrete) emissions \u2013 in Casino case of fair and loaded dice data"}, {"input_sequence": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Tue Dec 22 10:54:50 2020\n@author: Jane Hsieh (Hsing-Chuan, Hsieh)\n@article:\nGenerate random sample of dice (with Faif and Loaded dice used interchangeably)\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n#0. ================================================ Functions ===============================================\ndef InitialState(pi_0=0.5):\n'''\nfunction to generate initial state S_0 out of {F,L} of the dice with prob-p(S_0=F) = p(F)=pi_0 (0:F; 1:L)\nInput:\npi_0: defaul= 0.5 which is the initial prob of hiddent state p(S_0 = 0) [0: fair dice; 1: Loaded dice]\n(pi1 := p(S_0 = 1) = 1-pi_0)\nOutput:\nS_0: initial generated hidden state\n'''\nS_0 = np.random.choice(2, 1, p=[pi_0, 1-pi_0])[0] #Default: replace=True\nreturn S_0\n\ndef EmitObs(S_t, Dice, pF, pL):\nfunction to generate observation O_t according to the mechanism of hidden state S_t --> Emit Observation\nInput:\nS_t: current hidden state\nDice: face point of dice, e.g., [1,2,...,6]\npF: distributions of Fair Dice, say, with point = [1,2,...,6]\nif S_t == 0:\n#Fair dice state with prob = pF\nO_t = np.random.choice(Dice, 1, replace=True, p=pF)[0]\nif S_t == 1:\n#Loaded dice state with prob = pL\nO_t = np.random.choice(Dice, 1, replace=True, p=pL)[0]\nreturn O_t\ndef TransitState(S_t, p01, p10):\nfunction to generate the new hidden state S_t according to the mechanism of previous hidden state S_t\np01: Transition prob from 0(F) to 1(L)); p00 = 1- p01\nS_new = np.random.choice(2, 1, replace=True, p=[1-p01, p01])[0]\nreturn S_new\ndef GenerateFLDice(N, p01, p10, pL, pF=[1/6 for i in range(6)], Dice = [i+1 for i in range(6)], pi_0=0.5, seed=None):\nGenerate random sample of dice (with predefined Fair(R) and Loaded(L) dice used interchangeably)\nN: sample size\nseed: random seed\u2013 None or int\nDice: face point of the dice; default = [1,2,...,6]\npF, pL: distributions of Fair vs Loaded Dice (length of pF, pL must be the same with Dice)\npi_0: initial probability of hidden state S_0 \u2013 p(S_0 = 0) [0: fair dice; 1: Loaded dice]\np01, p10: Transition prob of hidden state from 0(F) to 1(L)) & from 1(L) to 0(F), respectively\nSample_S: sequence of hidden states {S_t} from t=0,...,N-1\nnp.random.seed(seed)\nSample_S = []\nt=0\n# Generate initial hidden state S_t & the Observed state O_t out of it\nS_t = InitialState(pi_0)\nO_t = EmitObs(S_t, Dice, pF, pL)\nSample_S.append(S_t)\nt += 1\nwhile t < N:\n#Generate New S_t at next time (i.e., t+1) point according to previous mechanism of S_t;\n#and O_t out of New S_t\nS_t = TransitState(S_t, p01, p10)\nO_t = EmitObs(S_t, Dice, pF, pL)\n\nSample_S.append(S_t)\nt += 1\nreturn Sample_S,\n", "output_sequence": "Use standard HMM package (i.e., hmmlearn) to analyze the data with multinomial (discrete) emissions \u2013 in Casino case of fair and loaded dice data"}, {"input_sequence": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on 2022/06/29\n@author: jane_hsieh\n@Resources:\n1. eigen-portfolio-construction with PCA:\nhttps://github.com/titmy/eigen-portfolio-construction\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import kurtosis\nimport os\nos.getcwd()\nos.chdir(\"/Users/jane_hsieh/Library/CloudStorage/OneDrive-\u570b\u7acb\u967d\u660e\u4ea4\u901a\u5927\u5b78/Data Science Analysis Templates/Machine Learning/Part 9 - Dimensionality Reduction/SectionExtra 3 - Independent Component Analysis (ICA)/FRM Data Analysis_Porfolio Construction_with ICA-PCA\")\ndata_dir = './Data'\noutput_dir = './Output'\n# ==================================== 0. Input data: FRM prices / returns ====================================================\n# returns\ndf_return = pd.read_csv(data_dir+'/FRM_CHHKTW_Time_Series_Returns_20201030.csv', parse_dates=['Date'], index_col = 'Date')\ncol_chosen = [28, 159, 172, 153, 108, 41, 154, 107, 173, 27, 122, 64, 163, 105] #[28, 159, 172, 153] #\ncol_chosen = [i-1 for i in col_chosen]\nstocks = df_return.columns[col_chosen];print(stocks)\n#df_return = df_return.iloc[:,col_chosen]\n#df_return.columns\ndf = df_return.iloc[:,col_chosen]\ndel df_return\ndf.columns = [i.replace('.EQUITY', '') for i in df.columns]\ndf.columns\n# prices\ndf_price = pd.read_csv(data_dir+'/FRM_SHSZ300HSITWSE_Stock_Prices_update_20201030.csv', parse_dates=['Date'], index_col = 'Date')\nstocks2 = df_price.columns[col_chosen];print(stocks2) #check if stocks2==\ndf_price = df_price.iloc[:,col_chosen]\ndf_price.columns = df.columns\n## 0.1 Missing data imputation ---------------------------------------------------------------\nprint(\"Numnber of missing data for returns:: \\n\", df.isnull().sum())\n'''\n#Only sporatic missing points, hence we simply perform linear interpolation method to those missing values\ndf_return.fillna(0) #since supposed stock price has no change for missing value; i.e., df_price.fillna(method='ffill', inplace=True)\ndf_price.fillna(method='ffill', inplace=True)\n## 0.2 Visualization: Multidimensional Time Series Data Plot (FRM) ---------------------------------------------------------------\nstart = '2019-01-02'\nend = '2020-10-30'\ndf[start: end].plot(figsize=(15,6))\n#plt.legend(fancybox=True, framealpha=0.0, loc = 'upper right', prop={'size': 8})\nplt.legend(bbox_to_anchor=(1.02, 1),loc='upper left', borderaxespad=0.,\nfancybox=True, framealpha=0.0, prop={'size': 8})\nplt.title(f'Daily returns of each stock (FRM@Asia) from {start} to {end}')\nplt.show()\nplt.savefig(data_dir+f'/Multidimensional Daily Returns (FRM) from {start} to {end} ({len(col_chosen)}D).png', transparent = True)\nplt.close()\ndf_price[start: end].plot(figsize=(15,6))\nplt.title(f'Daily prices of each stock (FRM@Asia) from {start} to {end}')\nplt.savefig(data_dir+f'/Multidimensional Daily Prices (FRM) from {start} to {end} ({len(col_chosen)}D).png', transparent = True)\n# ======================================= 1. Data Analysis: Portfolio construction (via PCA vs ICA) =========================================================\n## -------------------------------------- Functions --------------------------------------------------------------\ndef Rescale_W(W):\n'''\ninput: W_IC - dataframe; weigh matrix with dim=(n_components, n_variables)\noutput: W_IC _rs - dataframe; scaling each w (row) to sum to 1\nW_rs = list()\nfor w in W.values:\n#print(w)\n# scaling values to sum to 1\nnormalized_values = w / w.sum()\nW_rs.append(normalized_values)\nW_rs = pd.DataFrame(W_rs, columns=W.columns, index=W.index)\nreturn W_rs\n## Truncate weights\ndef Trunc(x, threshold=1):\nif x>=threshold:\nelse:\nxt=x\nreturn xt\ndef sharpe_ratio(ts_returns, periods_per_year=252):\n\"\"\"\nsharpe_ratio - Calculates annualized return, annualized vol, and annualized sharpe ratio,\nwhere sharpe ratio is defined as annualized return divided by annualized volatility\n\nArguments:\nts_returns - pd.Series of returns of a single eigen portfolio\n\nReturn:\na tuple of three doubles: annualized return, volatility, and sharpe ratio\nannualized_return = 0.\nn_years = ts_returns.shape[0] / periods_per_year\nannualized_return = np.power(np.prod(1+ts_returns),(1/n_years))-1\nannualized_vol = ts_returns.std() * np.sqrt(periods_per_year)\nannualized_sharpe = annualized_return / annualized_vol\nmean = ts_returns.mean()\nk = kurtosis(ts_returns)\nreturn annualized_return, mean, var, k #Ann_return, Ann_Sharpe, annualized_vol, mean, volatility(var), kurtosis\n## -------------------------------------- 1.0 Preprocessing --------------------------------------------------------------\n## Split df in training and testing set\n#import datetime\ntrain_ratio = 0.85\ntrain_end = int(len(df)*train_ratio)#datetime.datetime(2012, 3, 26)\n# for daily returns\ndf_train = df[:train_end].copy(); print('Dim. of Train dataset:', df_train.shape)\nprint('Train dataset:', df_train.shape)\n# Standardize the mixed signals (i.e., mean=0, std=1)\nfrom sklearn.preprocessing import StandardScaler\nsc_df = StandardScaler()\ndf_train_rs = sc_df.fit_transform(df_train)\ndf_train_rs = pd.DataFrame(df_train_rs, columns=df.columns, index = df.index[:train_end])\nprint('Check the means of training data:\\n', df_train_rs .mean(axis=0) )\n# for daily prices\ntrain_end_date = df_train .index[-1]\ndf_price_train = df_price[:train_end_date].copy(); print('Dim. of Train dataset:', df_price_train.shape)\n# ======================================= 2. Data Analysis: Portfolio construction via ICA) =========================================================\n# Number of components\nk = 5\n## -------------------------------------- 2.1 ICA Analysis --------------------------------------------------------------\n### ICA --------------------------------------------------------------\nfrom sklearn.decomposition import FastICA\ntransformer = FastICA(n_components=k, random_state=0, whiten='unit-variance')\nY_pred = transformer.fit_transform(df_train_rs)\nY_pred.shape\nY_pred = pd.DataFrame(Y_pred)\nY_pred.columns= ['IC'+str(i+1) for i in range(Y_pred.shape[1])]\nY_pred.set_index(df_train_rs.index, inplace=True)\nprint('Check if Means of ICs (all Means should be centored at 0 under ICA assumption):\\n',Y_pred.mean(axis=0) )\nprint('Check SDs of ICs (all variances/SDs are constrained the same under ICA whiteing assumption):\\n',Y_pred.std(axis=0) )\n## Rescale Y_pred s.t. rest of Var(Y_pred) have same scalse as the first IC (SD=1); actually, since each IC has equal SD, such operation is same as rescaling individual IC themselves\nY_pred_rs = (Y_pred -Y_pred.mean(axis=0)[0]\nprint('Check if Means of ICs (all Means should be centored at 0 under ICA assumption):\\n',Y_pred_rs.mean(axis=0) )\nprint('Check if SDs of ICs are 1):\\n',Y_pred_rs.std(axis=0) )\n### Visualization: TS for ICs ---------------------------------------------------------------\nY_pred.plot(figsize=(15,6))\nplt.legend(fancybox=True, framealpha=0.0, loc = 'upper right', prop={'size': 8})\nplt.title('Daily Portforlio Returns of ICs')\nplt.savefig(output_dir+f'/Portfolio Returns of {k} ICs ({int(df.shape[1])}D).png', transparent = True)\nY_pred_rs.plot(figsize=(15,6))\nplt.title('Daily Portforlio Returns of ICs (Standardized w.r.t. 1st IC))')\nplt.ylim((-8,8))\nplt.savefig(output_dir+f'/Portfolio Returns of {k} Standardized ICs ({int(df.shape[1])}D).png', transparent = True)\n## -------------------------------------- 2.2 Portforlio Weights (per IC) --------------------------------------------------------------\n### Derive weight matrix and re-scaling: W (weight matrix to transform raw data X into ICs Y) ---------------------------------------------------------------\n# Investigate W\nW_IC = pd.DataFrame(transformer.components_, columns=df.columns)\nW_IC.set_index(Y_pred.columns, inplace=True)\n#W_IC.to_csv(output_dir+f'/W matrix of {k} ICs ({int(df.shape[1])}D).csv')\n## normalized each IC so that w sum up to 1\nW_IC_rs = Rescale_W(W_IC)\nprint('Check sum of each IC:\\n', W_IC_rs.sum(axis=1))\n#decide threshod for truncation\nk1, k2 = W_IC_rs.shape\n#plt.hist(W_IC_rs.values.flatten(), bins=k1*k2)\n#plt.close()\nthreshold = 5\nW_IC_rs1 = W_IC_rs.applymap(Trunc, threshold = threshold)\nW_IC_rs1 = Rescale_W(W_IC_rs1)\nprint('Check sum of each IC:\\n', W_IC_rs1.sum(axis=1))\n# normalized each IC so that L2-norm of w sum up to 1 vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv\ntempt = np.diag(np.dot(W_IC, W_IC.T))\nW_IC_rs2 = W_IC.mul(1/np.sqrt(tempt), axis=0)\n# Customize the heatmap oW weight matrix -------------------------------------------------------------------\nW_IC_alias = W_IC_rs2 #W_IC #W_IC_rs\n#vnames = [name for name in globals() if globals()[name] is W_IC_alias]\n#print(vnames)\nsns.set(rc={'figure.figsize':(15,4)})\nsns.heatmap(W_IC_alias, #W_IC_rs1\nannot=True,\nlinewidths=0.4,\nannot_kws={\"size\": 10})\nplt.title(f'Portfolio construction with {k} ICs (weights rescaled ')\nplt.xticks(rotation=45, fontsize=7)\nplt.yticks(rotation=0)\nplt.savefig(output_dir+f'/Rescaled W matrix of {k} ICs - W_IC_rs ({int(df.shape[1])}D).png', transparent = True)\n## -------------------------------------- 2.3 Portforlio (per IC) --------------------------------------------------------------\nperiods_per_year=252\nW_IC_alias = W_IC_rs2 #W_IC #\n# portfolio per IC --------------------------------------------------------------------\nif df_price_train is not None:\nY = np.dot(df_price_train, W_IC_alias.T)\nY = pd.DataFrame(Y, columns=W_IC_alias.index, index=df_price_train.index)\n# Calculate returns of each ICs\nY_returns = Y.pct_change()[1:] #daily: periods=1 for daily return\n# Calculate importance of each IC\nIC_w = Y_returns.agg([np.mean, np.var])\nIC_w =IC_w.append(pd.Series(kurtosis(Y_returns), name='kurtosis', index=W_IC_alias.index))\n### Optimum Portforlio Construction ---------------------------------------------------------------\nY_w = Y * ( np.sign(IC_w.loc['mean',:]) * (np.abs(IC_w.loc['mean',:])/IC_w.loc['kurtosis',:])**(1/3) )\nP_ICA = Y_w.sum(axis=1)\nP_ICA_returns = P_ICA.pct_change()[1:] #daily: periods=1 for daily return\nY_returns_w = Y_returns * ( np.sign(IC_w.loc['mean',:]) * (np.abs(IC_w.loc['mean',:])/IC_w.loc['kurtosis',:])**(1/3) )\nP_ICA = Y_returns_w.sum(axis=1)\n## Visualization\nax = Y_returns.plot(figsize=(15,6), linewidth=1.3, linestyle='dotted') #Y_returns_w\nP_ICA_returns.plot(ax=ax, label='Optimum Portfolio', alpha=0.5, linewidth=3) #,\nplt.legend(fancybox=True, framealpha=0.0, prop={'size': 8}) #loc = 'upper right'\nplt.title(f'Portfolio returns from {k} ICs')\n#plt.ylim((-0.2,0.4))\nplt.show()\n# !!!!!!!\nplt.savefig(output_dir+f'/True Portfolio returns from {k} ICs with w - W_IC_rs2 ({int(df.shape[1])}D).png', transparent = True)\nplt.close()\nax = Y_returns['2019-02-12':].plot(figsize=(15,6), linewidth=1.3, linestyle='dotted') #Y_returns_w\nP_ICA_returns['2019-02-12':].plot(ax=ax, label='Optimum Portfolio', alpha=0.5, linewidth=3) #,\nplt.savefig(output_dir+f'/True Portfolio returns from {k} ICs with w - W_IC_rs2 ({int(df.shape[1])}D) 2.png', transparent = True)\nplt.close()\n### Calculate cumprod of (Optimum ) Portforlio Returns ---------------------------------------------------------------\nY_returns_cumprod = np.cumprod(Y_returns + 1)\nY_returns_cumprod.plot(title=f'Cumprod portfolio returns from {k} ICs (with W_IC)', figsize=(15,6), linewidth=3)\nplt.savefig(output_dir+f'/True Cumprod portfolio returns from {k} ICs (with W_IC_rs2).png', transparent = True)\nplt.close()\nax = Y_returns_cumprod.plot(figsize=(15,6), linewidth=1.3) #Y_returns_w\nP_ICA_returns_cumprod.plot(ax=ax, label='Optimum Portfolio', alpha=0.5, linewidth=3) #,\nplt.title(f'Cumprod portfolio returns from {k} ICs')\n#ax = Y_returns_cumprod.plot(figsize=(15,6), linewidth=1.3) #Y_returns_w\nP_ICA_returns_cumprod.plot() #, ax=ax, alpha=0.5, linewidth=3\nplt.title(f'Cumprod of optimum portfolio returns from {k} ICs')\nplt.savefig(output_dir+f'/True Cumprod optimum portfolio returns from {k} ICs (with W_IC_rs2).png', transparent = True)\n### Calculate sharpe ---------------------------------------------------------------\nsharpe_ICA = Y_returns.apply(sharpe_ratio, periods_per_year=periods_per_year, axis=0) #index: er, vol, sharpe\nsharpe_ICA.columns = W_IC_alias.index\nsharpe_ICA.index = ['Ann_return', 'Ann_Sharpe', 'Ann_Volatility', 'mean', 'var', 'kurtosis']\nsharpe_P_ICA = pd.DataFrame(sharpe_ratio(P_ICA_returns), columns=['Portfolio'], index=sharpe_ICA.index )\nsharpe_ICA = pd.concat([sharpe_ICA, sharpe_P_ICA], axis=1)\n\nif df_price_test is not None:\nY_test = np.dot(df_price_test, W_IC_alias.T)\nY_test = pd.DataFrame(Y_test, columns=W_IC_alias.index, index=df_price_test.index)\nY_returns_test = Y_test.pct_change()[1:] #daily: periods=1 for daily return\nY_test_w = Y_test * ( np.sign(IC_w.loc['mean',:]) * (np.abs(IC_w.loc['mean',:])/IC_w.loc['kurtosis',:])**(1/3) )\nP_ICA_test = Y_test_w.sum(axis=1)\nP_ICA_test_returns = P_ICA_test.pct_change()[1:] #daily: periods=1 for daily return\nY_returns_test_w = Y_returns_test * ( np.sign(IC_w.loc['mean',:]) * (np.abs(IC_w.loc['mean',:])/IC_w.loc['kurtosis',:])**(1/3) )\nP_ICA_test = Y_returns_test_w.sum(axis=1)\n## Visualization\nax = Y_returns_test.plot(figsize=(15,6), linewidth=1.3) #Y_returns_w\nP_ICA_test_returns.plot(ax=ax, label='Optimum Portfolio', alpha=0.5, linewidth=3) #,\nplt.savefig(output_dir+f'/Test- True Portfolio returns from {k} ICs with w - W_IC_rs2 ({int(df.shape[1])}D).png', transparent = True)\nY_returns_test_cumprod = np.cumprod(Y_returns_test + 1)\nP_ICA_test_returns_cumprod = np.cumprod(P_ICA_test_returns + 1)\nplt.savefig(output_dir+f'/Test- True Cumprod portfolio returns from {k} ICs (with W_IC_rs2).png', transparent = True)\nax = Y_returns_test_cumprod.plot(figsize=(15,6), linewidth=1.3) #Y_returns_w\nP_ICA_test_returns_cumprod.plot(ax=ax, label='Optimum Portfolio', alpha=0.5, linewidth=3) #,\nplt.savefig(output_dir+f'/Test- True Cumprod weighted portfolio returns from {k} ICs (with W_IC_rs2).png', transparent = True)\nsharpe_ICA_t = Y_returns_test.apply(sharpe_ratio, periods_per_year=periods_per_year, axis=0) #index: er, vol, sharpe\nsharpe_ICA_t.columns = W_IC_alias.index\nsharpe_ICA_t.index = ['Ann_return', 'Ann_Sharpe', 'Ann_Volatility', 'mean', 'var', 'kurtosis']\nsharpe_P_ICA = pd.DataFrame(sharpe_ratio(P_ICA_test_returns), columns=['Portfolio'], index=sharpe_ICA_t.index )\nsharpe_ICA_t = pd.concat([sharpe_ICA_t, sharpe_P_ICA], axis=1)\n\n# ======================================= 3. Data Analysis: Portfolio construction via PCA =========================================================\n## -------------------------------------- 3.1 PCA Analysis --------------------------------------------------------------\n### Preprocessing Data --------------------------------------------------------------\nsc_X = StandardScaler()\nmatrix = sc_X.fit_transform(df)\nprint(matrix.mean(axis=0) )\n### PCA --------------------------------------------------------------\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=k) #n_components=None or integer\nY_pred_PCA = pca.fit_transform(df_train_rs)\n# rescale Y s.t. var(Yi) = 1 (in order to compare with ICA results)\nsc_Y = StandardScaler()\nY_pred_PCA = sc_Y.fit_transform(Y_pred_PCA)\nY_pred_PCA.mean(axis=0)\nY_pred_PCA = pd.DataFrame(Y_pred_PCA)\nY_pred_PCA.columns= ['PC'+str(i+1) for i in range(k)]\nY_pred_PCA.set_index(df_train_rs.index, inplace=True)\n#Change direction of PC1 (only for better visualization, won't change results)\n#Y_pred_PCA.loc[:, 'PC1'] = -Y_pred_PCA.loc[:, 'PC1']\nprint('Check if Means of PCs (all Means should be centored at 0 under PCA assumption):\\n',Y_pred_PCA.mean(axis=0) )\nprint('Check SDs of PCs (should be equivelant to eigenvalues):\\n',Y_pred_PCA.std(axis=0) )\nY_pred_PCA_rs = (Y_pred_PCA -Y_pred_PCA.mean(axis=0)[0]\nprint('Check if Means of ICs (all Means should be centored at 0 under ICA assumption):\\n',Y_pred_PCA_rs.mean(axis=0) )\nprint('Check if SDs of ICs are 1):\\n',Y_pred_PCA_rs.std(axis=0) )\n### Visualization: PCs ---------------------------------------------------------------\nY_pred_PCA.plot(figsize=(15,6))\nplt.title('Daily Portforlio Returns of PCs')\nplt.savefig(output_dir+f'/Portfolio Returns of {k} PCs ({int(df.shape[1])}D).png', transparent = True)\nY_pred_PCA_rs.plot(figsize=(15,6))\nplt.title('Daily Portforlio Returns of PCs (Standardized w.r.t. 1st PC)')\nplt.savefig(output_dir+f'/Portfolio Returns of {k} Standardized PCs ({int(df.shape[1])}D).png', transparent = True)\n### Visualization: W (weight matrix to transform raw data X into ICs Y) ---------------------------------------------------------------\nW_PC = pd.DataFrame(pca.components_, columns=df_train.columns) ##each row is a PC -> PC1 = pca.components_[0,:]\nW_PC.set_index(Y_pred_PCA.columns, inplace=True)\n#W_PC.to_csv(output_dir+f'/W matrix of {k} PCs ({int(df.shape[1])}D).csv')\n#Similary, change sign of weights due fo change direction of PC1 (only for better visualization, won't change results)\n#W_PC.loc['PC1', :] = -W_PC.loc['PC1', :]\n# Customize the heatmap oW weight matrix\n#sns.color_palette(\"vlag\", as_cmap=True)\nsns.heatmap(W_PC,\nplt.title(f'Portfolio (weights) construction with {k} PCA')\nplt.xticks(rotation=30, fontsize=7)\nplt.savefig(output_dir+f'/W matrix of {k} PCs ({int(df.shape[1])}D).png', transparent = True)\nW_PC_rs = Rescale_W(W_PC)\nprint('Check sum of each PC:\\n', W_PC_rs.sum(axis=1))\nk1, k2 = W_PC_rs.shape\n#plt.hist(W_PC_rs.values.flatten(), bins=k1*k2)\nW_PC_rs1 = W_PC_rs.applymap(Trunc, threshold = threshold)\nW_PC_rs1 = Rescale_W(W_PC_rs1)\nprint('Check sum of each PC:\\n', W_PC_rs1.sum(axis=1))\n# For PCA, L2-norm of w is already constrained to 1, so no need for normalization again\ntempt = np.diag(np.dot(W_PC, W_PC.T)); print('Check L2-norm of each PC:\\n', tempt)\n#W_PC_rs2 = W_IC\nW_PC_alias = W_PC #W_PC_rs\nsns.heatmap(W_PC_alias, #W_IC_rs1\nplt.title(f'Portfolio construction with {k} PCs (weights rescaled)')\nplt.savefig(output_dir+f'/Rescaled W matrix of {k} PCs - W_PC ({int(df.shape[1])}D).png', transparent = True)\n## -------------------------------------- 2.3 Portforlio (per PC) --------------------------------------------------------------\n# portfolio per PC --------------------------------------------------------------------\nY_PCA = np.dot(df_price_train, W_PC_alias.T)\nY_PCA = pd.DataFrame(Y_PCA, columns=W_PC_alias.index, index=df_price_train.index)\n# Calculate returns of each PCs\nY_PCA_returns = Y_PCA.pct_change()[1:] #daily: periods=1 for daily return\n# Calculate importance of each PC\nPC_w = Y_PCA_returns.agg([np.mean, np.var])\nPC_w =PC_w.append(pd.Series(kurtosis(Y_PCA_returns), name='kurtosis', index=W_PC_alias.index))\nY_PCA_w = Y_PCA * ( PC_w.loc['mean',:]/PC_w.loc['var',:] )\nP_PCA = Y_PCA_w.sum(axis=1)\nP_PCA_returns = P_PCA.pct_change()[1:] #daily: periods=1 for daily return\nY_PCA_returns_w = Y_PCA_returns * ( PC_w.loc['mean',:]/PC_w.loc['var',:] )\nP_PCA = Y_PCA_returns_w.sum(axis=1)\nax = Y_PCA_returns.plot(figsize=(15,6), linewidth=1.3, linestyle='dotted') #Y_PCA_returns_w\nP_PCA_returns.plot(ax=ax, label='Optimum Portfolio', alpha=0.5, linewidth=3) #,\nplt.title(f'Portfolio returns from {k} PCs')\nplt.savefig(output_dir+f'/True Portfolio returns from {k} PCs with w - W_PC ({int(df.shape[1])}D).png', transparent = True)\nax = Y_PCA_returns['2019-02-12':].plot(figsize=(15,6), linewidth=1.3, linestyle='dotted') #Y_PCA_returns_w\nP_PCA_returns['2019-02-12':].plot(ax=ax, label='Optimum Portfolio', alpha=0.5, linewidth=3) #,\nplt.savefig(output_dir+f'/True Portfolio weighted returns from {k} PCs with w - W_PC ({int(df.shape[1])}D) 2.png', transparent = True)\nY_PCA_returns_cumprod = np.cumprod(Y_PCA_returns + 1)\nY_PCA_returns_cumprod.plot(title=f'Cumprod portfolio returns from {k} PCs (with W_PC)', figsize=(15,6), linewidth=3)\nplt.savefig(output_dir+f'/True Cumprod portfolio returns from {k} PCs (with W_PC).png', transparent = True)\nax = Y_PCA_returns_cumprod.plot(figsize=(15,6), linewidth=1.3) #Y_PCA_returns_w\nP_PCA_returns_cumprod.plot(ax=ax, label='Optimum Portfolio', alpha=0.5, linewidth=3) #,\nplt.title(f'Cumprod portfolio returns from {k} PCs')\nplt.savefig(output_dir+f'/True Cumprod weighted portfolio returns from {k} PCs (with W_PC).png', transparent = True)\n#ax = Y_PCA_returns_cumprod.plot(figsize=(15,6), linewidth=1.3) #Y_returns_w\nP_PCA_returns_cumprod.plot() #, ax=ax, alpha=0.5, linewidth=3\nplt.title(f'Cumprod of optimum portfolio returns from {k} PCs')\nplt.savefig(output_dir+f'/True Cumprod optimum portfolio returns from {k} PCs (with W_PC).png', transparent = True)\nsharpe_PCA = Y_PCA_returns.apply(sharpe_ratio, periods_per_year=periods_per_year, axis=0) #index: er, vol, sharpe\nsharpe_PCA.columns = W_PC_alias.index\nsharpe_PCA.index = ['Ann_return', 'Ann_Sharpe', 'Ann_Volatility', 'mean', 'var', 'kurtosis']\nsharpe_P_PCA = pd.DataFrame(sharpe_ratio(P_PCA_returns), columns=['Portfolio'], index=sharpe_PCA.index )\nsharpe_PCA = pd.concat([sharpe_PCA, sharpe_P_PCA], axis=1)\nY_PCA_test = np.dot(df_price_test, W_PC_alias.T)\nY_PCA_test = pd.DataFrame(Y_PCA_test, columns=W_PC_alias.index, index=df_price_test.index)\nY_PCA_returns_test = Y_PCA_test.pct_change()[1:] #daily: periods=1 for daily return\nY_PCA_test_w = Y_PCA_test * ( PC_w.loc['mean',:]/PC_w.loc['var',:] )\nP_PCA_test = Y_PCA_test_w.sum(axis=1)\nP_PCA_returns_test = P_PCA_test.pct_change()[1:] #daily: periods=1 for daily return\nY_PCA_returns_test_w = Y_PCA_returns_test * ( PC_w.loc['mean',:]/PC_w.loc['var',:] )\nP_PCA_test = Y_PCA_returns_test_w.sum(axis=1)\nax = Y_PCA_returns_test.plot(figsize=(15,6), linewidth=1.3) #Y_PCA_returns_w\nP_PCA_returns_test.plot(ax=ax, label='Optimum Portfolio', alpha=0.5, linewidth=3) #,\nplt.savefig(output_dir+f'/Test- True Portfolio weighted returns from {k} PCs with w - W_PC ({int(df.shape[1])}D).png', transparent = True)\nY_PCA_returns_test_cumprod = np.cumprod(Y_PCA_returns_test + 1)\nplt.savefig(output_dir+f'/Test- True Cumprod portfolio returns from {k} PCs (with W_PC).png', transparent = True)\nax = Y_PCA_returns_test_cumprod.plot(figsize=(15,6), linewidth=1.3) #Y_PCA_returns_w\nP_PCA_returns_test_cumprod.plot(ax=ax, label='Optimum Portfolio', alpha=0.5, linewidth=3) #,\nsharpe_PCA_t = Y_PCA_returns_test.apply(sharpe_ratio, periods_per_year=periods_per_year, axis=0) #index: er, vol, sharpe\nsharpe_PCA_t.columns = W_PC_alias.index\nsharpe_PCA_t.index = ['Ann_return', 'Ann_Sharpe', 'Ann_Volatility', 'mean', 'var', 'kurtosis']\nsharpe_P_PCA = pd.DataFrame(sharpe_ratio(P_PCA_returns_test), columns=['Portfolio'], index=sharpe_PCA_t.index )\nsharpe_PCA_t = pd.concat([sharpe_PCA_t, sharpe_P_PCA], axis=1)\n# ======================================= 5. Summary & Visualization of Results =========================================================\n\n### Heatmaps of W (weight) matrix from PCA/FA/ICA --------------------------------------------------------------\nfig, ax = plt.subplots(2, 1, sharex=True, figsize=(30,12)) #sharey=True,\n# Plot seperated signals: PCA\nannot_kws={\"size\": 10}, ax = ax[0])\nax[0].legend(fancybox=True, framealpha=0.0, loc = 'upper right', prop={'size': 8})\nfig.text(0.5, 0.90, f'Portfolio weights with first {k} PCs', ha='center', fontsize=12)\n# Plot seperated signals: ICA\nsns.heatmap(W_IC_rs2,\nannot_kws={\"size\": 10}, ax = ax[1])\nax[1].legend(fancybox=True, framealpha=0.0, loc = 'upper right', prop={'size': 8})\nfig.text(0.5, 0.48, f'Portfolio weights with first {k} ICs', ha='center', fontsize=12)\nfig.text(0.5, 0.04, 'Equities', ha='center', fontsize=14, fontweight ='bold')\nplt.xticks(rotation=20, fontsize=7)\nplt.suptitle(\"Portfolio (weights) construction with PCA or ICA\", fontsize=14, fontweight ='bold')\nplt.savefig(output_dir+\"/Compare estimates of (rescaled) W matrix from PCA-ICA.png\", transparent=True)\n\n### Visualization of Results from PCA/ICA --------------------------------------------------------------\n## Estimation of portfolio returns from each portfolio as well as from the optimum portfolio -------------------\nfig, ax = plt.subplots(2, 1, sharex=True, figsize=(30,12)) #sharey=True,\nax[0].plot(Y_PCA_returns.index, Y_PCA_returns, label=Y_PCA_returns.columns, linewidth=1.3, linestyle='dotted') #\nax[0].plot(P_PCA_returns.index, P_PCA_returns, label='Optimum Portfolio', alpha=0.5, linewidth=3)\nax[0].legend(fancybox=True, framealpha=0.0, loc = 'upper center', ncol=len(Y_returns.columns)+1, prop={'size': 8})\nfig.text(0.5, 0.90, f'Portfolio returns from {k} PCs', ha='center', fontsize=12)\nax[1].plot(Y_returns.index, Y_returns, label=Y_returns.columns, linewidth=1.3, linestyle='dotted') #\nax[1].plot(P_ICA_returns.index, P_ICA_returns, label='Optimum Portfolio', alpha=0.5, linewidth=3)\nax[1].legend(fancybox=True, framealpha=0.0, loc = 'upper center', ncol=len(Y_returns.columns)+1, prop={'size': 8})\nfig.text(0.5, 0.48, f'Portfolio returns from {k} ICs', ha='center', fontsize=12)\nfig.text(0.5, 0.04, 'Date', ha='center', fontsize=14, fontweight ='bold')\nfig.text(0.04, 0.5, 'Portfolio Returns', va='center', rotation='vertical', fontsize=14, fontweight ='bold')\n#plt.suptitle(\"A comparision of portfolio returns by PCA and ICA\", fontsize=14, fontweight ='bold')\nplt.savefig(output_dir+\"/Compare estimates of portfolio returns from PCA-ICA.png\", transparent=True)\nfig, ax = plt.subplots(2, 1, sharex=True, figsize=(30,12)) #\nax[0].plot(Y_PCA_returns['2019-02-12':].index, label=Y_PCA_returns.columns, linewidth=1.3, linestyle='dotted') #\n#ax[0]\nax[1].plot(Y_returns['2019-02-12':].index, Y_returns['2019-02-12':], label=Y_returns.columns, linewidth=1.3, linestyle='dotted') #\nplt.savefig(output_dir+\"/Compare estimates of portfolio returns from PCA-ICA 2.png\", transparent=True)\n#------------------------------------------------------------------------------------------------------------------\n## Estimation of cumulative returns from each portfolio as well as from the optimum portfolio -------------------\nfig, ax = plt.subplots(2, 1, sharex=True, figsize=(30,12)) #, sharey=True\nax[0].plot(Y_PCA_returns_cumprod.index, Y_PCA_returns_cumprod, label=Y_PCA_returns_cumprod.columns, linewidth=1.3, linestyle='dotted') #\nax[0].legend(fancybox=True, framealpha=0.0, loc = 'upper center', ncol=len(Y_PCA_returns_cumprod.columns)+1, prop={'size': 8})\nfig.text(0.5, 0.90, f'Cumprod portfolio returns from {k} PCs', ha='center', fontsize=12)\nax[1].plot(Y_returns_cumprod.index, Y_returns_cumprod, label=Y_returns_cumprod.columns, linewidth=1.3, linestyle='dotted') #\nax[1].plot(P_ICA_returns_cumprod.index, P_ICA_returns_cumprod, label='Optimum Portfolio', alpha=0.5, linewidth=3)\nax[1].legend(fancybox=True, framealpha=0.0, loc = 'upper center', ncol=len(Y_returns_cumprod.columns)+1, prop={'size': 8})\nfig.text(0.5, 0.48, f'Cumprod portfolio returns from {k} ICs', ha='center', fontsize=12)\nfig.text(0.04, 0.5, 'Cumulative Portfolio Returns', va='center', rotation='vertical', fontsize=14, fontweight ='bold')\nplt.savefig(output_dir+\"/Compare cumulative returns from PCA-ICA.png\", transparent=True)\nfig, ax = plt.subplots(2, 1, sharex=True, figsize=(30,12)) #, sharey=True\nax[1].plot(Y_returns_cumprod.iloc[:,1:].index, linewidth=1.3, linestyle='dotted') #\nplt.savefig(output_dir+\"/Compare cumulative returns from PCA-ICA 2.png\", transparent=True)\n# -------------------------------------- for testing sets ---------------------------------------------------------\nfig, ax = plt.subplots(2, 1, sharex=True,sharey=True, figsize=(30,12)) #\nax[0].plot(Y_PCA_returns_test.index, Y_PCA_returns_test, label=Y_PCA_returns_test.columns, linewidth=1.3, linestyle='dotted') #\nax[1].plot(P_ICA_test_returns.index, P_ICA_test_returns, label='Optimum Portfolio', alpha=0.5, linewidth=3)\nplt.savefig(output_dir+\"/Test- Compare estimates of portfolio returns from PCA-ICA.png\", transparent=True)\nax[0].plot(Y_PCA_returns_test_cumprod.index, linewidth=1.3, linestyle='dotted') #\nax[1].plot(P_ICA_test_returns_cumprod.index, label='Optimum Portfolio', alpha=0.5, linewidth=3)\nplt.savefig(output_dir+\"/Test- Compare cumulative returns from PCA-ICA.png\", transparent=True)\n", "output_sequence": "Analyze financial data (daily returns of 15 stocks in Asia market) - show the difference between results of PCA and ICA"}, {"input_sequence": "# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Mon May 21 12:56:02 2018\n@author: ivan\nimport requests\nfrom bs4 import BeautifulSoup\nimport time\n## ------\n# EXAMPLE\n# -------\n# Erase the ''' to acttivate the lines for the example:\n'''\nurl = \"https://bitcoinchain.com/block_explorer/400\"\nresponse = requests.get(url)\nsoup = BeautifulSoup(response.content, \"html.parser\")\nstat_table = soup.find_all('table', class_ = 'table table-hover b-blocks__table')[0]\nprint(stat_table.text)\ndatasaved = \"delete\"\nfor row in stat_table.find_all(\"tr\"):\ndata = \"\"\nfor cell in row.find_all(\"td\"):\ndata = data + \",\" + cell.get_text(strip=True).replace(\",\",\"\")\ndatasaved = datasaved + data + \"\\n\"\nprint(datasaved)\n# --------------------\n# CODE TO GET THE DATA\n# 2017 blocks are those in pages 215:780, so range should be range(215,780)\nstart = time.time()\nfor i in range(780,1329):\nurl = \"https://bitcoinchain.com/block_explorer/{}\".format(i)\ntime.sleep(2)\nresponse = requests.get(url)\nsoup = BeautifulSoup(response.content, \"html.parser\")\nstat_table = soup.find_all('table', class_ = 'table table-hover b-blocks__table')[0]\n\ndatasaved = \"delete\"\nwith open (\"bitcoinchain.csv\", \"a\") as r:\n#r.write(header)\nfor row in stat_table.find_all(\"tr\"):\ndata = \"\"\nfor cell in row.find_all(\"td\"):\ndata = data + \",\" + cell.get_text(strip=True).replace(\",\",\"\")\ndatasaved = datasaved + data + \"\\n\"\nr.write(datasaved)\nelapsed_time = time.time() - start\ntime.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))\n\n", "output_sequence": "Quantify the degree of Mining Pool concentration for different cryptocurrencies"}, {"input_sequence": "# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Fri May 25 10:36:16 2018\n@author: ivan\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nfrom PIL import Image\n# The following code includes \"unknown miner\" as a single group (On the second\n# part of the code the \"unkown miner\" is excluded and the analysis is repeated)\n# BITCOIN data loading\n# Do no forget to change the current directory\nbtc = pd.read_csv(\"BTC.csv\", parse_dates=[\"time\"], index_col=\"time\")\nbtc_pools = btc.resample(\"M\")[\"guessed_miner\"].value_counts()\nbtc_pools = btc_pools.to_frame()\nbtc_pools.columns = [\"value\"]\nbtc_pools = btc_pools.reset_index()\n# BITCOIN CASH data loading\nbtc_cash = pd.read_csv(\"BTC cash.csv\", parse_dates=[\"time\"], index_col=\"time\")\nbtc_cash_pools = btc_cash.resample(\"M\")[\"guessed_miner\"].value_counts()\nbtc_cash_pools = btc_cash_pools.to_frame()\nbtc_cash_pools.columns = [\"value\"]\nbtc_cash_pools = btc_cash_pools.reset_index()\n# ETH beta data loading\neth = pd.read_csv(\"ETH beta.csv\", parse_dates=[\"time\"], index_col=\"time\")\neth.index = pd.to_datetime(eth.index,unit='s')\neth_pools = eth.to_period('M').groupby('time')['miner'].value_counts()\n#eth_pools = eth.resample(\"M\")[\"miner\"].value_counts()\neth_pools = eth_pools.to_frame()\neth_pools.columns = [\"value\"]\neth_pools = eth_pools.reset_index()\neth_pools.columns = [\"time\", \"guessed_miner\", \"value\"]\n#eth_pools.to_csv(\"eth_pools\")\n# LITECOIN data loading\nltc = pd.read_csv(\"ltc.csv\", parse_dates=[\"unixtime\"], index_col=\"unixtime\")\nltc.index = pd.to_datetime(ltc.index,unit='s')\nltc.miner = ltc.miner.fillna(\"Unknown\")\nltc_pools = ltc.resample(\"M\")[\"miner\"].value_counts()\nltc_pools = ltc_pools.to_frame()\nltc_pools.columns = [\"value\"]\nltc_pools = ltc_pools.reset_index()\nltc_pools.columns = [\"time\", \"guessed_miner\", \"value\"]\n# DASH data loading\ndash = pd.read_csv(\"dash.csv\", parse_dates=[\"unixtime\"], index_col=\"unixtime\")\ndash.index = pd.to_datetime(dash.index,unit='s')\ndash.miner = dash.miner.fillna(\"Unknown\")\n#dash_pools = dash.to_period('M').groupby('unixtime')['miner'].value_counts()\ndash_pools = dash.resample(\"M\")[\"miner\"].value_counts()\ndash_pools = dash_pools.to_frame()\ndash_pools.columns = [\"value\"]\ndash_pools = dash_pools.reset_index()\ndash_pools.columns = [\"time\", \"guessed_miner\", \"value\"]\n# GINI CALCULATION\n# Define Gini Formula\n# (according to: https://en.wikipedia.org/wiki/Gini_coefficient, section: Alternate expressions)\ndef gini(arr):\n## first sort\nsorted_arr = arr.copy()\nsorted_arr.sort_values()\nn = arr.size\ncoef_ = 2. / n\nconst_ = (n + 1.) / n\nweighted_sum = sum([(i+1)*yi for i, yi in enumerate(sorted_arr)])\nreturn coef_*weighted_sum/(sorted_arr.sum()) - const_\n# Generate data frame with Gini Index\ngini_df = []\nfor i,j,k,l,m in zip(btc_pools.time.unique(),btc_pools.time.unique(),eth_pools.time.unique(),\nbtc_pools.time.unique(),btc_pools.time.unique()):\ngini_btc = -gini(btc_pools[btc_pools.time == i][\"value\"])\ngini_df.append((i, gini_btc, gini_btc_cash, gini_eth,gini_ltc, gini_dash))\ngini_df = pd.DataFrame(gini_df, columns=('time', 'btc', 'bch','eth','ltc','dash'))\ngini_df.set_index('time', inplace=True)\n# Plot GINI\ngini_df[\"2016\":\"2018-04\"].plot().legend(loc='upper center',\nbbox_to_anchor=(0.5, -0.12), ncol=5, frameon=False)\nax1 = plt.axes()\nx_axis = ax1.axes.get_xaxis()\nx_axis.set_label_text('')\nplt.subplots_adjust(bottom=0.2)\nplt.savefig('plot1.png', transparent=True)\n# Save plot with transparent background\nimg = Image.open('plot1.png')\nimg = img.convert(\"RGBA\")\ndatas = img.getdata()\nnewData = []\nfor item in datas:\nif item[0] == 255 and item[1] == 255 and item[2] == 255:\nnewData.append((255, 255, 0))\nelse:\nnewData.append(item)\nimg.putdata(newData)\nimg.save(\"plot1_trans.png\", \"PNG\")\n# Plot Lorenz Curve\nbtc_lorenz = btc[\"2016\":\"2018-04\"][\"guessed_miner\"].value_counts().values\nbtc_lorenz = np.sort(btc_lorenz)\nbtc_lorenz = btc_lorenz.cumsum() / btc_lorenz.sum()\nbtc_lorenz = np.insert(btc_lorenz, 0, 0)\nbtc_lorenz.columns = [\"btc\"]\nbch_lorenz = btc_cash[\"2016\":\"2018-04\"][\"guessed_miner\"].value_counts().values\nbch_lorenz = np.sort(bch_lorenz)\nbch_lorenz = bch_lorenz.cumsum() / bch_lorenz.sum()\nbch_lorenz = np.insert(bch_lorenz, 0, 0)\nbch_lorenz.columns = [\"bch\"]\neth_lorenz = eth[\"2016\":\"2018-04\"][\"miner\"].value_counts().values\neth_lorenz = np.sort(eth_lorenz)\neth_lorenz = eth_lorenz.cumsum() / eth_lorenz.sum()\neth_lorenz = np.insert(eth_lorenz, 0, 0)\neth_lorenz.columns = [\"eth\"]\nltc_lorenz = ltc[\"2016\":\"2018-04\"][\"miner\"].value_counts().values\nltc_lorenz = np.sort(ltc_lorenz)\nltc_lorenz = ltc_lorenz.cumsum() / ltc_lorenz.sum()\nltc_lorenz = np.insert(ltc_lorenz, 0, 0)\nltc_lorenz.columns = [\"ltc\"]\ndash_lorenz = dash[\"2016\":\"2018-04\"][\"miner\"].value_counts().values\ndash_lorenz = np.sort(dash_lorenz)\ndash_lorenz = dash_lorenz.cumsum() / dash_lorenz.sum()\ndash_lorenz = np.insert(dash_lorenz, 0, 0)\ndash_lorenz.columns = [\"ltc\"]\nplt.plot(np.arange(btc_lorenz.size)/(btc_lorenz.size-1), btc_lorenz, label = \"btc\")\nplt.plot([0,1], [0,1], '--', color='black')\nplt.legend(loc='upper center',\nplt.savefig('plot9.png', transparent=True)\nplt.show()\nimg = Image.open('plot9.png')\nimg.save(\"plot9_trans.png\", \"PNG\")\n\n# Data for table of mining pool participation\nbtc[\"2016\":\"2018-04\"][\"guessed_miner\"].value_counts()/np.sum(btc[\"2016\":\"2018-04\"][\"guessed_miner\"].value_counts())\n# HERFINDAHL HIRSCHMAN INDEX (HHI) CALCULATION\ndef hhi(arr):\nn = sum(arr)\nsquared_sum = sum([(i/n*100)**2 for i in sorted_arr])\nreturn squared_sum\n# Generate data frame with HHI Index\nhhi_df = []\nfor i,j,k,l,m in zip(btc_pools.time.unique(),btc_pools.time.unique(),\neth_pools.time.unique(),btc_pools.time.unique(),btc_pools.time.unique()):\nhhi_btc = hhi(btc_pools[btc_pools.time == i][\"value\"])\nhhi_df.append((i, hhi_btc, hhi_btc_cash, hhi_eth, hhi_dash))\nhhi_df = pd.DataFrame(hhi_df, columns=('time', 'btc', \"ltc\", \"dash\"))\nhhi_df.set_index('time', inplace=True)\n# Plots HHI\nhhi_df[\"2016\":\"2018-04\"].plot().legend(loc = \"upper center\",\nplt.savefig('plot3.png', transparent=True)\nimg = Image.open('plot3.png')\nimg.save(\"plot3_trans.png\", \"PNG\")\n# Participation of the big 3 (cumulative participation of the 3 most important pools)\nnp.sum((btc[\"2016\":\"2018-04\"][\"guessed_miner\"].value_counts()/np.sum(btc[\"2016\":\"2018-04\"][\"guessed_miner\"].value_counts()))[0:3])\n###############################################################################\n# Excluding Unknown Miner\nbtc_pools2 = btc_pools[btc_pools.guessed_miner != \"Unknown\"]\n# Generate data frame with Gini Index (without unknown miner)\ngini_df2 = []\nfor i,j,k,l,m in zip(btc_pools2.time.unique(),btc_pools2.time.unique(),eth_pools2.time.unique(),\nbtc_pools2.time.unique(),btc_pools2.time.unique()):\ngini_btc = -gini(btc_pools2[btc_pools2.time == i][\"value\"])\ngini_df2.append((i, gini_btc, gini_btc_cash, gini_eth,gini_ltc, gini_dash))\ngini_df2 = pd.DataFrame(gini_df2, columns=('time', 'btc', 'bch','eth','ltc','dash'))\ngini_df2.set_index('time', inplace=True)\n# PLOTS GINI (without unknown miner)\ngini_df2[\"2016\":\"2018-04\"].plot().legend(loc = \"upper center\",\nplt.savefig('plot2.png', transparent=True)\nimg = Image.open('plot2.png')\nimg.save(\"plot2_trans.png\", \"PNG\")\n# Data for table of mining pool participation (exluding unknown miners)\nbtc[btc.guessed_miner != \"Unknown\"][\"2016\":\"2018-04\"][\"guessed_miner\"].value_counts()/np.sum(btc[btc.guessed_miner != \"Unknown\"][\"2016\":\"2018-04\"][\"guessed_miner\"].value_counts())\n# Generate data frame with HHI Index (without unknown miner)\nhhi_df2 = []\nfor i,j,k,l,m in zip(btc_pools2.time.unique(),btc_pools2.time.unique(),\neth_pools2.time.unique(),btc_pools2.time.unique(),btc_pools2.time.unique()):\nhhi_btc = hhi(btc_pools2[btc_pools2.time == i][\"value\"])\nhhi_df2.append((i, hhi_btc, hhi_btc_cash, hhi_eth, hhi_dash))\nhhi_df2 = pd.DataFrame(hhi_df2, columns=('time', 'btc', \"ltc\", \"dash\"))\nhhi_df2.set_index('time', inplace=True)\n# Plot HHI\nhhi_df2[\"2016\":\"2018-04\"].plot().legend(loc = \"upper center\",\nplt.savefig('plot4.png', transparent=True)\nimg = Image.open('plot4.png')\nimg.save(\"plot4_trans.png\", \"PNG\")\nnp.sum((btc[btc.guessed_miner != \"Unknown\"][\"2016\":\"2018-04\"][\"guessed_miner\"].value_counts()/np.sum(btc[btc.guessed_miner != \"Unknown\"][\"2016\":\"2018-04\"][\"guessed_miner\"].value_counts()))[0:3])\n# Plots GINI vs Price\n# BTC price data loading\nbtc_price = pd.read_csv(\"btc_price.csv\", parse_dates=[\"date\"], index_col=\"date\")\nbtc_price.columns = [\"price\", \"difficulty\"]\nbtc_price = btc_price[\"2016\":\"2018-04\"]\nbtc_price.price = btc_price.price.astype(float)\nbtc_price = btc_price.resample(\"M\").mean()\nax1 = gini_df[\"2016\":\"2018-04\"][\"btc\"].plot(label = \"btc gini\")\nax2 = btc_price[\"2016\":\"2018-04\"][\"price\"].plot(label = \"price (right axis)\", secondary_y=True)\nax1.set_ylim(0, 1)\nh1, l1 = ax1.get_legend_handles_labels()\nplt.legend(h1+h2, l1+l2, loc=2,frameon=False)\nplt.savefig('plot5.png', transparent=True)\nimg = Image.open('plot5.png')\nimg.save(\"plot5_trans.png\", \"PNG\")\n# ETH price data loading\neth_price = pd.read_csv(\"eth_price.csv\", parse_dates=[\"date\"], index_col=\"date\")\neth_price.columns = [\"price\", \"difficulty\"]\neth_price = eth_price[\"2016\":\"2018-04\"]\neth_price.price = eth_price.price.astype(float)\neth_price = eth_price.resample(\"M\").mean()\nax1 = gini_df[\"2016\":\"2018-04\"][\"eth\"].plot(label = \"eth gini\")\nax2 = eth_price[\"2016\":\"2018-04\"][\"price\"].plot(label = \"price (right axis)\", secondary_y=True)\nplt.legend(h1+h2, l1+l2, loc=2, frameon=False)\nplt.savefig('plot6.png', transparent=True)\nimg = Image.open('plot6.png')\nimg.save(\"plot6_trans.png\", \"PNG\")\n# LTC price data loading\nltc_price = pd.read_csv(\"ltc_price.csv\", parse_dates=[\"date\"], index_col=\"date\")\nltc_price.columns = [\"price\", \"difficulty\"]\nltc_price = ltc_price[\"2016\":\"2018-04\"]\nltc_price.price = ltc_price.price.astype(float)\nltc_price = ltc_price.resample(\"M\").mean()\nax1 = gini_df[\"2016\":\"2018-04\"][\"ltc\"].plot(label = \"ltc gini\")\nax2 = ltc_price[\"2016\":\"2018-04\"][\"price\"].plot(label = \"price (right axis)\", secondary_y=True)\nplt.savefig('plot7.png', transparent=True)\nimg = Image.open('plot7.png')\nimg.save(\"plot7_trans.png\", \"PNG\")\n# DASH price data loading\ndash_price = pd.read_csv(\"dash_price.csv\", parse_dates=[\"date\"], index_col=\"date\")\ndash_price.columns = [\"price\", \"difficulty\"]\ndash_price = dash_price[\"2016\":\"2018-04\"]\ndash_price.price = dash_price.price.astype(float)\ndash_price = dash_price.resample(\"M\").mean()\nax1 = gini_df[\"2016\":\"2018-04\"][\"dash\"].plot(label = \"dash_gini\")\nax2 = dash_price[\"2016\":\"2018-04\"][\"price\"].plot(label = \"price (right axis)\",secondary_y=True)\nplt.savefig('plot8.png', transparent=True)\nimg = Image.open('plot8.png')\nimg.save(\"plot8_trans.png\", \"PNG\")\n# Plots HHI vs Price\n# BTC\nax1 = hhi_df[\"2016\":\"2018-04\"][\"btc\"].plot(label = \"btc HHI\")\nplt.legend(h1+h2, l1+l2, loc = \"upper center\", frameon = False,)\nplt.savefig('plot10.png', transparent=True)\nimg = Image.open('plot10.png')\nimg.save(\"plot10_trans.png\", \"PNG\")\n# ETH\nax1 = hhi_df[\"2016\":\"2018-04\"][\"eth\"].plot(label = \"eth HHI\")\nplt.legend(h1+h2, l1+l2, loc=\"upper center\", frameon = False)\nplt.savefig('plot11.png', transparent=True)\nimg = Image.open('plot11.png')\nimg.save(\"plot11_trans.png\", \"PNG\")\n# LTC\nax1 = hhi_df[\"2016\":\"2018-04\"][\"ltc\"].plot(label = \"ltc HHI\")\nax2 = ltc_price[\"2016\":\"2018-04\"][\"price\"].plot(label = \"price (right axis)\",secondary_y=True)\nplt.savefig('plot12.png', transparent=True)\nimg = Image.open('plot12.png')\nimg.save(\"plot12_trans.png\", \"PNG\")\n# DASH\nax1 = hhi_df[\"2016\":\"2018-04\"][\"dash\"].plot(label = \"dash HHI\")\nax2 = dash_price[\"2016\":\"2018-04\"][\"price\"].plot(label = \"price (right axis)\", secondary_y=True)\nplt.savefig('plot13.png', transparent=True)\nimg = Image.open('plot13.png')\nimg.save(\"plot13_trans.png\", \"PNG\")\n# Plots HHI vs difficulty\nax1 = hhi_df[\"2016\":\"2018-04\"][\"btc\"].plot()\nax2 = btc_price[\"2016\":\"2018-04\"][\"difficulty\"].plot(secondary_y=True)\nplt.legend(h1+h2, l1+l2, loc=2)\nax1 = hhi_df[\"2016\":\"2018-04\"][\"eth beta\"].plot()\nax2 = eth_price[\"2016\":\"2018-04\"][\"difficulty\"].plot(secondary_y=True)\nax1 = hhi_df[\"2016\":\"2018-04\"][\"ltc\"].plot()\nax2 = ltc_price[\"2016\":\"2018-04\"][\"difficulty\"].plot(secondary_y=True)\nax1 = hhi_df[\"2016\":\"2018-04\"][\"dash\"].plot()\nax2 = dash_price[\"2016\":\"2018-04\"][\"difficulty\"].plot(secondary_y=True)\n# VAR model\nimport statsmodels.api as sm\nfrom statsmodels.tsa.api import VAR, DynamicVAR\nfrom statsmodels.tsa.base.datetools import dates_from_str\n# Prepare data for the VAR model\ndatavar = hhi_df[0:]\ndatavar.columns = [\"btc hhi\", \"bch hhi\", \"eth hhi\", \"ltc hhi\",\"dash hhi\"]\ndatavar.insert(loc = 5, column = \"btc price\", value = btc_price.price)\n# VAR model for BTC\ndatavar1 = datavar[['btc hhi','btc price']]\ndatavar1 = np.log(datavar1).diff().dropna()\ndatavar1.columns = [\"btc hhi logreturn\", \"btc price logreturn\"]\nmodel1 = VAR(datavar1)\nresults1 = model1.fit(2)\nresults1.summary()\n# save results as image\nplt.rc('figure', figsize=(12, 7))\n#plt.text(0.01, 0.05, str(model.summary()), {'fontsize': 12}) old approach\nplt.text(0.01, 0.05, str(results1.summary()), {'fontsize': 10}, fontproperties = 'monospace') # approach improved by OP -> monospace!\nplt.axis('off')\nplt.tight_layout()\nplt.savefig('VAR1.png')\n# Save image with transparent background\nimg = Image.open('VAR1.png')\nimg.save(\"VAR1_trans.png\", \"PNG\")\n# Granger causality test (\"price causing hhi)\nresults1.test_causality(\"btc hhi logreturn\",\"btc price logreturn\")\nimg = Image.open('granger1.png')\nimg.save(\"granger1_trans.png\", \"PNG\")\n# Granger causality test (\"hhi causing price)\nresults1.test_causality(\"btc price logreturn\", \"btc hhi logreturn\")\nimg = Image.open('granger2.png')\nimg.save(\"granger2_trans.png\", \"PNG\")\n# VAR model for BTC in 2017 - 2018\ndatavar2 = datavar[['btc hhi','btc price']][\"2017\"]\ndatavar2 = np.log(datavar2).diff().dropna()\ndatavar2.columns = [\"btc hhi logreturn\", \"btc price logreturn\"]\nmodel2 = VAR(datavar2)\nresults2 = model2.fit(2)\nresults2.summary()\nplt.text(0.01, 0.05, str(results2.summary()), {'fontsize': 10}, fontproperties = 'monospace') # approach improved by OP -> monospace!\nplt.savefig('VAR2.png')\nimg = Image.open('VAR2.png')\nimg.save(\"VAR2_trans.png\", \"PNG\")\n", "output_sequence": "Quantify the degree of Mining Pool concentration for different cryptocurrencies"}, {"input_sequence": "# blink.py\nimport RPi.GPIO as GPIO\nimport time\nGPIO.setmode(GPIO.BOARD)\nGPIO.setup(7, GPIO.OUT)\nwhile True:\nGPIO.output(7,True)\ntime.sleep(0.2)\nGPIO.output(7,False)\nimport os\nfrom PIL import ImageFont\n\ntext = ((\"Raspberry Pi \", (255, 0, 0)), (\"and \", (0, 255, 0)), (\"Adafruit\", (0, 255)))\nfont = ImageFont.truetype(\"/usr/share/fonts/truetype/freefont/FreeSans.ttf\", 16)\nall_text = \"\"\nfor text_color_pair in text:\nt = text_color_pair[0]\nall_text = all_text + t\nprint(all_text)\nwidth, ignore = font.getsize(all_text)\nprint(width)\nim = Image.new(\"RGB\", (width + 30, 16), \"black\")\ndraw = ImageDraw.Draw(im)\nx = 0;\nc = text_color_pair[1]\nprint(\"t=\" + t + \" + str(c) + \" + str(x))\ndraw.text((x, 0), t, c, font=font)\nx = x + font.getsize(t)[0]\nim.save(\"test.ppm\")\nos.system(\"./led-matrix 1 test.ppm\")\nimport urllib, json\nurl = \"http://maps.googleapis.com/maps/api/geocode/json?address=google\"\nresponse = urllib.urlopen(url)\ndata = json.loads(response.read())\nprint data\nimport urllib.request, json\nwith urllib.request.urlopen(\"http://maps.googleapis.com/maps/api/geocode/json?address=google\") as url:\ndata = json.loads(url.read().decode())\n", "output_sequence": "Python Based Raspberry Pi Zero W OLED Setup live BTC ticker"}, {"input_sequence": "import subprocess\nimport time\nimport Adafruit_GPIO.SPI as SPI\nimport Adafruit_SSD1306\nimport Image\nimport urllib2\n\n# Raspberry Pi pin configuration:\nRST = 24\n# 128x64 display with hardware I2C:\ndisp = Adafruit_SSD1306.SSD1306_128_64(rst=RST)\n# Initialize library.\ndisp.begin()\n# Clear display.\ndisp.clear()\ndisp.display()\n# Create blank image for drawing.\n# Make sure to create image with mode '1' for 1-bit color.\nwidth = disp.width\nheight = disp.height\nimage = Image.new('1', (width, height))\n# Get drawing object to draw on image.\ndraw = ImageDraw.Draw(image)\n# Load default font.\n# font = ImageFont.load_default()\n# font = ImageFont.truetype('Ubuntu-M.ttf', 14)\n# Display Image\ndisp.image(image)\n# definitions\ntimerTime = time.time()\nfrom time import sleep\n# import requests, json\n# def getBitcoinPrice():\n# URL = 'https://www.bitstamp.net/api/ticker/'\n# try:\n# r = requests.get(URL)\n# priceFloat = float(json.loads(r.text)['last'])\n# return priceFloat\n# except requests.ConnectionError:\n# print \"Error querying Bitstamp API\"\nwhile True:\ntime.sleep(2)\ndraw.rectangle((0, 0, width, height), outline=0, fill=0)\n# now = time.time()\n#Load requests via pip!\n#$ sudo pip install requests\n#import urllib.request, json\n#with urllib.request.urlopen(\"http://thecrix.de/data/crix_hf.json\") as url:\n# data = json.loads(url.read().decode())[-10:]\n#def last_n_crix_values(n):\n# with urllib.request.urlopen(\"http://thecrix.de/data/crix_hf.json\") as url:\n# data = json.loads(url.read().decode())[-n:]\n#return data\n#import urllib.request, json\n#import pandas as pd\n# with urllib.request.urlopen(\"http://thecrix.de/data/crix_hf.json\") as url:\n# data = json.loads(url.read().decode())[-n:]\n# return data\n#pd.DataFrame(last_n_crix_values(2)).values\n#d = last_n_crix_values(2)\n#print(pd.DataFrame(d).values)\n#print([i['date'] for i in d])\n#print([[i['date'],i['price']] for i in d])\nimport urllib.request, json\nimport pandas as pd\ndef last_n_crix_values(n):\nwith urllib.request.urlopen(\"http://thecrix.de/data/crix_hf.json\") as url:\ndata = json.loads(url.read().decode())[-n:]\nreturn data\nd = last_n_crix_values(2)\npd.DataFrame(d).values\nddate = [i['date'] for i in d]\n#[[i['date'], i['price']] for i in d]\ndraw.text((1,26), str('Date - ') + ' /', font=fontsmall, fill=255)\ndraw.text((64,1), ' /' + str('pprice'), font=fontlarge, fill=255)\nsleep(1)\n", "output_sequence": "Python Based Raspberry Pi Zero W OLED Setup live BTC ticker"}, {"input_sequence": "# Copyright (c) 2017 Adafruit Industries\n# Author: Tony DiCola & James DeVito\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n# THE SOFTWARE.\nimport time\nimport Adafruit_GPIO.SPI as SPI\nimport Adafruit_SSD1306\nfrom PIL import Image\nimport subprocess\n# Raspberry Pi pin configuration:\nRST = None # on the PiOLED this pin isnt used\n# Note the following are only used with SPI:\nDC = 23\nSPI_PORT = 0\nSPI_DEVICE = 0\n# Beaglebone Black pin configuration:\n# RST = 'P9_12'\n# SPI_PORT = 1\n# SPI_DEVICE = 0\n# 128x32 display with hardware I2C:\ndisp = Adafruit_SSD1306.SSD1306_128_32(rst=RST)\n# 128x64 display with hardware I2C:\n# disp = Adafruit_SSD1306.SSD1306_128_64(rst=RST)\n# Note you can change the I2C address by passing an i2c_address parameter like:\n# disp = Adafruit_SSD1306.SSD1306_128_64(rst=RST, i2c_address=0x3C)\n# Alternatively you can specify an explicit I2C bus number, for example\n# with the 128x32 display you would use:\n# disp = Adafruit_SSD1306.SSD1306_128_32(rst=RST, i2c_bus=2)\n# 128x32 display with hardware SPI:\n# disp = Adafruit_SSD1306.SSD1306_128_32(rst=RST, dc=DC, spi=SPI.SpiDev(SPI_PORT, SPI_DEVICE, max_speed_hz=8000000))\n# 128x64 display with hardware SPI:\n# disp = Adafruit_SSD1306.SSD1306_128_64(rst=RST, dc=DC, spi=SPI.SpiDev(SPI_PORT, SPI_DEVICE, max_speed_hz=8000000))\n# Alternatively you can specify a software SPI implementation by providing\n# digital GPIO pin numbers for all the required display pins. For example\n# on a Raspberry Pi with the 128x32 display you might use:\n# disp = Adafruit_SSD1306.SSD1306_128_32(rst=RST, dc=DC, sclk=18, din=25, cs=22)\n# Initialize library.\ndisp.begin()\n# Clear display.\ndisp.clear()\ndisp.display()\n# Create blank image for drawing.\n# Make sure to create image with mode '1' for 1-bit color.\nwidth = disp.width\nheight = disp.height\nimage = Image.new('1', (width, height))\n# Get drawing object to draw on image.\ndraw = ImageDraw.Draw(image)\n# Draw a black filled box to clear the image.\ndraw.rectangle((0,0,width,height), outline=0, fill=0)\n# Draw some shapes.\n# First define some constants to allow easy resizing of shapes.\npadding = -2\ntop = padding\nbottom = height-padding\n# Move left to right keeping track of the current x position for drawing shapes.\nx = 0\n# Load default font.\nfont = ImageFont.load_default()\n# Alternatively load a TTF font. Make sure the .ttf font file is in the same directory as the python script!\n# Some other nice fonts to try: http://www.dafont.com/bitmap.php\n# font = ImageFont.truetype('Minecraftia.ttf', 8)\nwhile True:\n# Draw a black filled box to clear the image.\ndraw.rectangle((0,0,width,height), outline=0, fill=0)\n# Shell scripts for system monitoring from here : https://unix.stackexchange.com/questions/119126/command-to-display-memory-usage-disk-usage-and-cpu-load\ncmd = \"hostname -I | cut -d\\' \\' -f1\"\nIP = subprocess.check_output(cmd, shell = True )\ncmd = \"top -bn1 | grep load | awk '{printf \\\"CPU Load: %.2f\\\", $(NF-2)}'\"\nCPU = subprocess.check_output(cmd, shell = True )\ncmd = \"free -m | awk 'NR==2{printf \\\"Mem: %s/%sMB %.2f%%\\\", $3,$2,$3*100/$2 }'\"\nMemUsage = subprocess.check_output(cmd, shell = True )\ncmd = \"df -h | awk '$NF==\\\"/\\\"{printf \\\"Disk: %d/%dGB %s\\\", $3,$2,$5}'\"\nDisk = subprocess.check_output(cmd, shell = True )\n# Write two lines of text.\ndraw.text((x, top), \"IP: \" + str(IP), font=font, fill=255)\n# Display image.\ndisp.image(image)\ndisp.display()\ntime.sleep(.1)\n", "output_sequence": "Python Based Raspberry Pi Zero W OLED Setup live BTC ticker"}, {"input_sequence": "import subprocess\nimport time\nimport Adafruit_GPIO.SPI as SPI\nimport Adafruit_SSD1306\nimport Image\nimport urllib2\n\n# Raspberry Pi pin configuration:\nRST = 24\n# 128x64 display with hardware I2C:\ndisp = Adafruit_SSD1306.SSD1306_128_64(rst=RST)\n# Initialize library.\ndisp.begin()\n# Clear display.\ndisp.clear()\ndisp.display()\n# Create blank image for drawing.\n# Make sure to create image with mode '1' for 1-bit color.\nwidth = disp.width\nheight = disp.height\nimage = Image.new('1', (width, height))\n# Get drawing object to draw on image.\ndraw = ImageDraw.Draw(image)\n# Load default font.\n# font = ImageFont.load_default()\n# font = ImageFont.truetype('Ubuntu-M.ttf', 14)\n# Display Image\ndisp.image(image)\n# definitions\ntimerTime = time.time()\nfrom time import sleep\n# import requests, json\n# def getBitcoinPrice():\n# URL = 'https://www.bitstamp.net/api/ticker/'\n# try:\n# r = requests.get(URL)\n# priceFloat = float(json.loads(r.text)['last'])\n# return priceFloat\n# except requests.ConnectionError:\n# print \"Error querying Bitstamp API\"\nwhile True:\ntime.sleep(2)\ndraw.rectangle((0, 0, width, height), outline=0, fill=0)\n# now = time.time()\np = 1\nq = 0.026\n# print getBitcoinPrice() / p * q\n# sleep(1)\nurlusd = 'http://api.coindesk.com/v1/bpi/currentprice/usd.json'\njsonURLusd=urllib2.urlopen(urlusd)\njsonObjectusd=json.load(jsonURLusd)\n\nbbl = jsonObjectusd['bpi']['USD']['rate']\nbitbut = round(jsonObjectusd['bpi']['USD']['rate_float'], 3)\nbbc = round(bitbut / p * q, 2)\nurl=\"http://api.coindesk.com/v1/bpi/currentprice/GBP.json\"\njsonURL=urllib2.urlopen(url)\njsonObject=json.load(jsonURL)\npbl = jsonObject['bpi']['GBP']['rate']\npoundbit = round(jsonObject['bpi']['GBP']['rate_float'], 3)\npbc = round(poundbit / p * q, 2)\n# print 'USD Bitcoin Price'\n# print bbl\n# print 'USD Price Calculated'\n# print bbc\n# print 'GBP Bitcoin Price'\n# print pbl\n# print 'GBP Bitcoin Calculated'\n# print pbc\ndraw.text((1,26), str('USD - ') + str(bitbut), font=fontsmall, fill=255)\nsleep(1)\ndisp.image(image)\n", "output_sequence": "Python Based Raspberry Pi Zero W OLED Setup live BTC ticker"}, {"input_sequence": "# Copyright (c) 2014 Adafruit Industries\n# Author: Tony DiCola\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n# THE SOFTWARE.\nimport time\nimport Adafruit_GPIO.SPI as SPI\nimport Adafruit_SSD1306\nfrom PIL import Image\n# Raspberry Pi pin configuration:\nRST = 24\n# Note the following are only used with SPI:\nDC = 23\nSPI_PORT = 0\nSPI_DEVICE = 0\n# Beaglebone Black pin configuration:\n# RST = 'P9_12'\n# SPI_PORT = 1\n# SPI_DEVICE = 0\n# 128x32 display with hardware I2C:\ndisp = Adafruit_SSD1306.SSD1306_128_32(rst=RST)\n# 128x64 display with hardware I2C:\n# disp = Adafruit_SSD1306.SSD1306_128_64(rst=RST)\n# Note you can change the I2C address by passing an i2c_address parameter like:\n# disp = Adafruit_SSD1306.SSD1306_128_64(rst=RST, i2c_address=0x3C)\n# Alternatively you can specify an explicit I2C bus number, for example\n# with the 128x32 display you would use:\n# disp = Adafruit_SSD1306.SSD1306_128_32(rst=RST, i2c_bus=2)\n# 128x32 display with hardware SPI:\n# disp = Adafruit_SSD1306.SSD1306_128_32(rst=RST, dc=DC, spi=SPI.SpiDev(SPI_PORT, SPI_DEVICE, max_speed_hz=8000000))\n# 128x64 display with hardware SPI:\n# disp = Adafruit_SSD1306.SSD1306_128_64(rst=RST, dc=DC, spi=SPI.SpiDev(SPI_PORT, SPI_DEVICE, max_speed_hz=8000000))\n# Alternatively you can specify a software SPI implementation by providing\n# digital GPIO pin numbers for all the required display pins. For example\n# on a Raspberry Pi with the 128x32 display you might use:\n# disp = Adafruit_SSD1306.SSD1306_128_32(rst=RST, dc=DC, sclk=18, din=25, cs=22)\n# Initialize library.\ndisp.begin()\n# Clear display.\ndisp.clear()\ndisp.display()\n# Create blank image for drawing.\n# Make sure to create image with mode '1' for 1-bit color.\nwidth = disp.width\nheight = disp.height\nimage = Image.new('1', (width, height))\n# Get drawing object to draw on image.\ndraw = ImageDraw.Draw(image)\n# Draw a black filled box to clear the image.\ndraw.rectangle((0,0,width,height), outline=0, fill=0)\n# Draw some shapes.\n# First define some constants to allow easy resizing of shapes.\npadding = 2\nshape_width = 20\ntop = padding\nbottom = height-padding\n# Move left to right keeping track of the current x position for drawing shapes.\nx = padding\n# Draw an ellipse.\ndraw.ellipse((x, top , x+shape_width, bottom), outline=255, fill=0)\nx += shape_width+padding\n# Draw a rectangle.\ndraw.rectangle((x, top, x+shape_width, bottom), outline=255, fill=0)\n# Draw a triangle.\ndraw.polygon([(x, bottom), (x+shape_width/2, top), (x+shape_width, bottom)], outline=255, fill=0)\n# Draw an X.\ndraw.line((x, bottom, x+shape_width, top), fill=255)\n# Load default font.\nfont = ImageFont.load_default()\n# Alternatively load a TTF font. Make sure the .ttf font file is in the same directory as the python script!\n# Some other nice fonts to try: http://www.dafont.com/bitmap.php\n#font = ImageFont.truetype('Minecraftia.ttf', 8)\n# Write two lines of text.\ndraw.text((x, top), 'Hello', font=font, fill=255)\n# Display image.\ndisp.image(image)\n", "output_sequence": "Python Based Raspberry Pi Zero W OLED Setup live BTC ticker"}, {"input_sequence": "def call_emotion_api(source):\nimport http.client, urllib.request, urllib.parse, base64, sys, json\n#define dictionary\nheaders = {\n# Request headers. Replace the placeholder key below with your subscription key.\n#data: must contain binary vector of picture\n'Content-Type': 'application/octet-stream',\n#'Content-Type': 'application/json', #alternative for URL's\n#AZURE KEY:\n'Ocp-Apim-Subscription-Key': 'XXXXXXXXX', #insert subscription key\n}\n#params: if face rectangles are provided by user\nparams = urllib.parse.urlencode({\n})\n\ntry:\nbody = open(source, \"rb\").read()\nconn = http.client.HTTPSConnection('westus.api.cognitive.microsoft.com')\nconn.request(\"POST\", \"/emotion/v1.0/recognize?%s\" % params, body, headers)\nresponse = conn.getresponse()\ndata = response.read()\n#print(data)\nconn.close()\n#return json.loads(data.decode('utf8'))\nexcept Exception as e:\nprint(e.args)\n", "output_sequence": "Calls Emotion API of Microsoft Cognitive Service. Requires either a key from Microsoft Azure (www.azure.microsoft.com). Can be used to send images from hard drive to API, returns a vector of eight emotion-scores."}, {"input_sequence": "#install required packages if necessary\npkg = c('MASS')\nlapply(pkg, require, character.only = TRUE)\n#set working directory\n#setwd(...)\n#load data\navg_emo = read.csv2(\"ECB_avg_emo.csv\")\n#chose relevant columns\nemodata = avg_emo[,c(2:9,11)]\n#binary encoding:\nbinary = emodata[,1:8]\nbinary$ret_dir = NA\n#0 - neg. returns, 1 - pos. returns\nbinary$ret_dir[emodata[,9] > 0] = 1\n#redefine as factor-type\nbinary$ret_dir = as.factor(binary$ret_dir)\n#dep. and ind. variables:\nx = binary[,c(1:5, 7:8)]\ny = binary[,9]\n# fit model with bayes-rule\nfit_lda = lda(x, y, probMethod = \"Bayes\")\n# make predictions\npredictions = predict(fit_lda, x)\n# confusion table: pred. vs. true observations\ntable(predictions$class, y)\n#plot histograms and densities\nplot(fit_lda,\ndimen = 1,\ntype =\"both\",\ncol = \"lightblue\",\nmain = 'LDA')\n", "output_sequence": "Estimates a linear discriminant analysis (LDA) in order to separate positive and negative returns of Eurostoxx50 on press conference days of the European Central Bank based on facial expression scores of the provided video material. Plots resulting histograms and distributions."}, {"input_sequence": "#install required packages if necessary\npkg = c('plsdepot')\nlapply(pkg, require, character.only = TRUE)\n#path where dataset is\ndir_data = '...'\n#load data\navg_emo = read.csv2(dir_data)\n#sort data, that y-variable is in last position\nemodata = avg_emo[,c(2:9,11)]\n#estimate model\npls_fit = plsreg1(emodata[,1:8], emodata[,9], comps = 4, crosval = TRUE)\n#loads for each variable in components\npls_fit$x.loads\n#R2-fit of model\npls_fit$R2\n", "output_sequence": "Estimates a partial least squares (PLS) model. Independent variables are averaged emotion scores (anger, disgust, fear, contempt, happiness, neutrality, sadness, surprise) for all webcasts of press conferences held by the European central bank between Jan. 2011 and Sep. 2017. The dependent variable is daily return data of Eurostoxx50."}, {"input_sequence": "#load and install required packages\npkg = c('plyr', 'TTR')\nlapply(pkg, require, character.only = TRUE)\n#open data sets, set working directory if necessary\n#setwd(...)\ndax_all_days = read.csv2('DAX_all_days_aggregate.csv')\n#delete old graphic device\ndev.off()\n#generate plot\nplot(dax_press_days$time,\ntype = 'l',\naxes = FALSE,\nxlab = 'Time (CET)',\nylab = 'Average volatility',\ncol = 'darkblue',\nlwd = 1)\n#transparent background\npar(bg = NA)\n#box around plot\nbox(which = 'plot', lty = 'solid')\n#ticks and labels\naxis(side = 1, at =seq(1, 101, 6), labels = as.character(dax_press_days$time2[seq(1, 101, 6)]))\naxis(side = 2, labels = TRUE)\n#add vertical lines at 13:45, 14:30,\nlines(dax_all_days$sd, type='l', lty = 2, col = 'darkblue', lwd = 1)\nabline(v = c(58,67, 79), col = 'red')\n", "output_sequence": "Plots the volatility of DAX30 index on days where the European Central Bank is holding press conferences vs. all other days between January 2015 until September 2017. Volatilities are averaged for each group and estimated from the data, based on 5 minute data. Vertical lines indicate the press release by European central bank council at 13.45, the beginning of the press conference at 14.30, held by the president, and the approximate end at 15.30."}, {"input_sequence": "# clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# Legendre Polynomial function\nlegendre = function(x, k) {\n# x is a vector of size: (nx1) k is the polynomial order (skalar)\nn = length(x)\nif (k == 0) {\npx = (matrix(1, n, 1)/sqrt(2))\n} else {\npx = cbind((matrix(1, n, 1)/sqrt(2)), (sqrt(3) * x/sqrt(2)))\nj = 1L\nwhile (j < k) {\nj = j + 1L\npx = cbind(px, (((2 * j - 1) * x * p1 - (j - 1) * p0)/j))\n}\n}\nreturn(px)\n}\n# load data\nx = read.table(\"agg73sh.dat\")\nx = x[, c(1, 4)]\nk = 9\nlx = legendre(x[, 1], k)\nb = lm(x[, 2] ~ lx - 1)$coefficients\nmo = cbind(x[, 1], (lx %*% b))\n# plot\nplot(x, type = \"n\", ylab = \"Food\", xlab = \"Net-income\", sub = paste(\"N =\", k))\ntitle(\"Orthogonal Series Regression\")\npoints(x, pch = 19, col = \"skyblue\", cex = 0.7)\nlines(mo, lwd = 2)\n", "output_sequence": "Computes the an orthogonal series regression food expenditures on net-income for the UK 1973 expenditure data."}, {"input_sequence": "# clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# install and load packages\nlibraries = c(\"tcltk\", \"TeachingDemos\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\nkde.refresh = function(...) {\nn = slider(no = 2)\nset.seed(19240215)\nx = rnorm(n)\nh = slider(no = 1)\nxr = diff(range(x))\nng = 100\nxg = (xr + 7 * h) * (0:(ng - 1))/(ng - 1) + min(x) - 3.5 * h\nfk = matrix(0, nrow = ng, ncol = n)\nfor (j in 1:n) {\nfk[, j] = dnorm((xg - x[j])/h)/(n * h)\n}\nfh = rowSums(fk)\nylim = c(-0.1 * max(fh),\n\n# plot\nplot(xg, fh, type = \"l\", lwd = 2, ylim = ylim, main = paste(\"Construction of Density Estimate (n=\",\nn, \")\", sep = \"\"), xlab = \"Data x\", ylab = \"KDE fh, Weights Kh\")\npoints(cbind(x, rep(-0.05 * max(fh), n)), col = \"red\", pch = 3)\nlines(xg, fk[, j], col = \"blue\")\n}\nslider(sl.functions = kde.refresh, sl.names = c(\"Bandwidth\", \"Sample Size\"), sl.mins = c(0.01,\n1), sl.maxs = c(2, 100), sl.deltas = c(0.01, 1), sl.defaults = c(0.25, 10), title = \"Choose KDE Construction Parameters\")\n", "output_sequence": "Visualizes the construction of a kernel density estimate (with additional slider to choose bandwidth and sample size)."}, {"input_sequence": "# clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# install and load packages\nlibraries = c(\"KernSmooth\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# load data\nx = read.csv(\"fes76.txt\", sep = \";\", dec = \".\")\nx = x[, c(\"NIC\", \"FOO\")]\nx = x[order(x$NIC), ]\nx = x[x$FOO/x$NIC <= 3, ] # Delete unplausible food shares\nplot(x$NIC, x$FOO/x$NIC, col = \"lightblue\", pch = 20, main = \"Engel Curve\")\n# Estimate regression function using local polynomials.\nmh = locpoly(x$NIC, x$FOO/x$NIC, degree = 0, kernel = \"normal\", bandwidth = 10)\n", "output_sequence": "Computes the regression of food share on net-income for the UK 1976 expenditure data."}, {"input_sequence": "rm(list=ls(all=TRUE))\n# please change your working directory\n#setwd(\"C:/...\")\nlibrary(xts)\nload(\"data.RData\")\nhist(x=log10(na.omit(as.numeric(apply(crypto_market_xts[\"::2014-08\"], 2,\nmean, na.rm = TRUE)))),\nmain = \"Histogram of log10 Market Cap\",\nbreaks = 20, xlab = \"\", col = rgb(1,0,0,0.5), ylim = c(0,60))\nhist(x=log10(na.omit(as.numeric(apply(crypto_market_xts[\"2016-01-01::\"], 2,\n", "output_sequence": "Gives 2 histograms for the distribution of market capitalization of crypto-currencies in 2 different time periods with a high overall market capitalization."}, {"input_sequence": "%matplotlib inline\nimport pandas as pd\nimport numpy as np\nfrom sklearn.neighbors import DistanceMetric\n### load csv file, select specific regions: ME, NH, NY\n### note that column 18-22 are information data only\ndf = pd.read_csv('ushealth05.csv', usecols=range(0,17))\n#print(\"Total records: \", len(df))\n#df.head(20)\n#df_state = df.loc[df['Area'] == 'Maine']\ndf_state = df.iloc[[19, 29, 32],:]\n#print(\"\\n\\nDataset for ME, NH, NY: \")\n#df_state.head(3)\ndf_state = df_state.drop(['Area', 'All'], axis=1)\nprint(\"\\n\\nDataset for ME, NH, NY: \")\ndf_state.head(3)\n### transfer DataFrame to Numpy array\nstate_list = np.array(df_state).astype(float)\nprint(\"Data dimension: \",np.shape(state_list))\n### Calculate diatance metrics for Euclidean distance\ndist = DistanceMetric.get_metric('euclidean')\ndist_Euc = np.around(dist.pairwise(state_list), decimals=1, out=None)\nprint(\"Euclidean Distance is: \\n\", dist_Euc)\n### Calculate diatance metrics for Manhattan distance\ndist = DistanceMetric.get_metric('manhattan')\ndist_Man = np.around(dist.pairwise(state_list), decimals=2, out=None)\n### Calculate diatance metrics for Maximum distance\ndist = DistanceMetric.get_metric('chebyshev')\ndist_Max = np.around(dist.pairwise(state_list), decimals=2, out=None)\n", "output_sequence": "Distance measure for continuous variables using Euclidian/Mannheim/Maximum methods"}, {"input_sequence": "## Drawing Normally-distributed samples with the Box-Muller method\nAssume we have two independent standard Normal random Cartesian variables $X$ and $Y$ follows standard normal distribution. The joint distribution $p(x,y)$ is:\n$$p(x,y) = p(x)p(y) = \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{x^2}{2}}\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{y^2}{2}} = \\frac{1}{2\\pi}e^{-\\frac{x^2 + y^2}{2}} $$\nIn polar corordinates, $x^2 + y^2 = r^2$ where $x = r\\cos(\\theta)$ $y = r\\sin(\\theta)$\nthen $p(x,y) = \\left ( \\frac{1}{2\\pi} \\right ) \\left ( e^{\\frac{-r^2}{2}} \\right )$\nwhich is the product of two density functions, an exponential distribution over squared radii: $r^2 \\sim Exp(\\frac{1}{2})$ and a uniform distribution over angles: $\\theta \\sim Unif(0,2\\pi)$\nFurthermore if we have:\n$Exp(\\lambda) = \\frac{-\\log (Unif(0,1))}{\\lambda}$\nthen $r \\sim \\sqrt{-2\\log (Unif(0,1))}$\nSo in order to generate a normal distrubution we can do as follows:\n1. Generate $u_1,u_2 \\sim Unif(0,1)$\n2. ransform the variables into radius and angle representation $r = \\sqrt{-2\\log(u_1)}$ , and $\\theta = 2\\pi u_2$\n3. Transform radius and angle into Cartesian coordinates: $x = r \\cos(\\theta)$, $y = r \\sin(\\theta)$\nfrom numpy import random, sqrt, log, sin, cos, pi\n# transformation function\ndef gaussian(u1,u2):\nz1 = sqrt(-2*log(u1))*cos(2*pi*u2)\n# uniformly distributed values between 0 and 1\nu1 = random.rand(1000)\n# plotting the values before and after the transformation\nfigure()\nsubplot(221)\nhist(z1)\nsubplot(222)\nhist(z2)\n", "output_sequence": "generate normal distribution using Box-Muller method"}, {"input_sequence": "import requests\nimport json\nimport pandas as pd\ndf = pd.read_json(\"https://data.ntpc.gov.tw/api/datasets/71CD1490-A2DF-4198-BEF1-318479775E8A/json\")\ndf.head(25)\nimport os\nos.environ['PROJ_LIB'] = 'C:/Users/User/Anaconda3/pkgs/proj4-5.2.0-h6538335_1006/Library/share/'\nfrom mpl_toolkits.basemap import Basemap\nimport matplotlib.pyplot as plt\nimport numpy as np\nfig = plt.figure(figsize=(6, 9))\nax = fig.add_axes([0.025, 0.025, 0.925])\nmap = Basemap(llcrnrlon = 121.2, llcrnrlat = 24.7, urcrnrlon = 122.0, urcrnrlat = 25.3,resolution = 'h', epsg = 3415)\nmap.drawmapboundary(fill_color = 'aqua')\nmap.drawcoastlines()\nparallels = np.arange(24., 26., 0.1)\nmap.drawparallels(parallels,labels=[1, 0, 0], fontsize=7, linewidth=0.3)\nmeridians = np.arange(120., 122.5, 0.1)\nmap.drawmeridians(meridians,labels=[0, 0, 1], fontsize=7, linewidth=0.3)\nx_list = df[\"lng\"].tolist()\nx, y = map(x_list, y_list)\nmap.scatter(x, y, marker = 'o', c='coral')\nplt.title(\"Taiwan bike dataset - Geo Location\", fontsize=14)\n", "output_sequence": "A Web Crawler for UBike website, and visualizes dataset with scatter plot"}, {"input_sequence": "clear all; clc;\ny_t_001_Th1 = load('y_t_001_Th1');\ntau = 0.01;\nA_t_001_Th1 = (1 : 1 : n_t_001_Th1)';\nLI_t_001_Th1 = arrayfun(@(x) LCARE_Test_Statistics_LR(y_t_001_Th1(:, x), ...\ntau), A_t_001_Th1, 'UniformOutput', false);\nLR_t_001_Th1 = reshape(cell2mat(LI_t_001_Th1), [], n_t_001_Th1);\nL_A_Th1_001 = LR_t_001_Th1(1 : 166, :);\nCARE_Intervals = load('CARE_Intervals');\nT_n_k = CARE_Intervals(2 : 1 : 11, 2) - CARE_Intervals(1, 2);\nT_k_Th1_001 = zeros(10, 1000);\nT_k_Th1_001(1, :) = max(L_LR_Th1_001(1 : T_n_k(1), :));\nfor j = 2 : 1 : length(T_n_k)\n\nT_k_Th1_001(j, :) = max(L_LR_Th1_001((T_n_k(j - 1) + 1) : 1 : T_n_k(j), :));\nend\nsave('T_k_Th1_001', '-ascii');\n", "output_sequence": "Evaluates the likelihood ratio test statistics for parameter constellation theta1 and expectile leve 0.01"}, {"input_sequence": "function Out = LCARE_Estimation_Loglik(y, tau)\nObjective = @(th) - LCARE_Loglik(y, tau, th);\nOptions = optimset('Display', 'off', 'Algorithm', 'interior-point', ...\n'TolX', 1e-3, 'TolFun', 1e-3, 'MaxIter', 1000);\nth0 = [0.10, 1.00, 0.01]';\nA = [0 0 -1];\nb = 0;\n\n[~, lik_value] = fmincon(Objective, th0, A, b, [], Options);\nOut = -lik_value;\nend\n", "output_sequence": "Evaluates the likelihood ratio test statistics for parameter constellation theta1 and expectile leve 0.01"}, {"input_sequence": "function Out = LCARE_Loglik(y, tau, th)\nn = length(y);\ny_lag = y(1 : 1 : n - 1, 1);\ny_lag_plus = (y_lag .* (y_lag > 0)) .^ 2;\ne = y(2 : n, 1) - th(1) - th(2) * y_lag - th(3) * y_lag_plus ...\n- th(4) * y_lag_neg;\nrho_tau = abs(tau - 1 * (e <= 0)) .* e .^ 2;\nf_y = 2 / sqrt(th(5)) * (sqrt(pi / abs(tau - 1)) + ...\nsqrt(pi / tau))^(-1) * exp (- rho_tau / th(5));\nf_y(f_y == 0) = 1;\nOut = sum(log(f_y));\n", "output_sequence": "Evaluates the likelihood ratio test statistics for parameter constellation theta1 and expectile leve 0.01"}, {"input_sequence": "function Out = LCARE_Test_Statistics_LR(y, tau)\nCARE_Intervals = load('CARE_Intervals');\nB = (CARE_Intervals(2, 4) : -1 : CARE_Intervals(11, 3))' + 1;\nL_B = arrayfun(@(x) LCARE_Estimation_Loglik(y(x : 1 : ...\nCARE_Intervals(1, 10), tau), B);\nk = 1;\nJ1 = (CARE_Intervals(k + 1, 4) : - 1 : CARE_Intervals(k + 1, 3))';\nL_A_k1 = arrayfun(@(x) LCARE_Estimation_Loglik(y(CARE_Intervals(k + 1, 5)...\n: 1 : x, 1), tau), J1);\nL_I_2 = LCARE_Estimation_Loglik(y(CARE_Intervals(k + 1, 9) : 1 : ...\nCARE_Intervals(k + 1, 10), tau) * ones(length(J1), 1);\nk = 2;\nJ2 = (CARE_Intervals(k + 1, 4) : - 1 : CARE_Intervals(k + 1, 3))';\nL_A_k2 = arrayfun(@(x) LCARE_Estimation_Loglik(y(CARE_Intervals(k + 1, 5)...\n: 1 : x, 1), tau), J2);\nL_I_3 = LCARE_Estimation_Loglik(y(CARE_Intervals(k + 1, 9) : 1 : ...\nCARE_Intervals(k + 1, 10), tau) *ones(length(J2), 1);\nk = 3;\nJ3 = (CARE_Intervals(k + 1, 4) : - 1 : CARE_Intervals(k + 1, 3))';\nL_A_k3 = arrayfun(@(x) LCARE_Estimation_Loglik(y(CARE_Intervals(k + 1, 5)...\n: 1 : x, 1), tau), J3);\nL_I_4 = LCARE_Estimation_Loglik(y(CARE_Intervals(k + 1, 9) : 1 : ...\nCARE_Intervals(k + 1, 10), tau) * ones(length(J3), 1);\nk = 4;\nJ4 = (CARE_Intervals(k + 1, 4) : - 1 : CARE_Intervals(k + 1, 3))';\nL_A_k4 = arrayfun(@(x) LCARE_Estimation_Loglik(y(CARE_Intervals(k + 1, 5)...\n: 1 : x, 1), tau), J4);\nL_I_5 = LCARE_Estimation_Loglik(y(CARE_Intervals(k + 1, 9) : 1 : ...\nCARE_Intervals(k + 1, 10), tau) * ones(length(J4), 1);\nk = 5;\nJ5 = (CARE_Intervals(k + 1, 4) : - 1 : CARE_Intervals(k + 1, 3))';\nL_A_k5 = arrayfun(@(x) LCARE_Estimation_Loglik(y(CARE_Intervals(k + 1, 5)...\n: 1 : x, 1), tau), J5);\nL_I_6 = LCARE_Estimation_Loglik(y(CARE_Intervals(k + 1, 9) : 1 : ...\nCARE_Intervals(k + 1, 10), tau) * ones(length(J5), 1);\nk = 6;\nJ6 = (CARE_Intervals(k + 1, 4) : - 1 : CARE_Intervals(k + 1, 3))';\nL_A_k6 = arrayfun(@(x) LCARE_Estimation_Loglik(y(CARE_Intervals(k + 1, 5)...\n: 1 : x, 1), tau), J6);\nL_I_7 = LCARE_Estimation_Loglik(y(CARE_Intervals(k + 1, 9) : 1 : ...\nCARE_Intervals(k + 1, 10), tau) * ones(length(J6), 1);\nk = 7;\nJ7 = (CARE_Intervals(k + 1, 4) : - 1 : CARE_Intervals(k + 1, 3))';\nL_A_k7 = arrayfun(@(x) LCARE_Estimation_Loglik(y(CARE_Intervals(k + 1, 5)...\n: 1 : x, 1), tau), J7);\nL_I_8 = LCARE_Estimation_Loglik(y(CARE_Intervals(k + 1, 9) : 1 : ...\nCARE_Intervals(k + 1, 10), tau) * ones(length(J7), 1);\nk = 8;\nJ8 = (CARE_Intervals(k + 1, 4) : - 1 : CARE_Intervals(k + 1, 3))';\nL_A_k8 = arrayfun(@(x) LCARE_Estimation_Loglik(y(CARE_Intervals(k + 1, 5)...\n: 1 : x, 1), tau), J8);\nL_I_9 = LCARE_Estimation_Loglik(y(CARE_Intervals(k + 1, 9) : 1 : ...\nCARE_Intervals(k + 1, 10), tau) * ones(length(J8), 1);\nk = 9;\nJ9 = (CARE_Intervals(k + 1, 4) : - 1 : CARE_Intervals(k + 1, 3))';\nL_A_k9 = arrayfun(@(x) LCARE_Estimation_Loglik(y(CARE_Intervals(k + 1, 5)...\n: 1 : x, 1), tau), J9);\nL_I_10 = LCARE_Estimation_Loglik(y(CARE_Intervals(k + 1, 9) : 1 : ...\nCARE_Intervals(k + 1, 10), tau) * ones(length(J9), 1);\nk = 10;\nJ10 = (CARE_Intervals(k + 1, 4) : - 1 : CARE_Intervals(k + 1, 3))';\nL_A_k10 = arrayfun(@(x) LCARE_Estimation_Loglik(y(CARE_Intervals(k + 1, 5)...\n: 1 : x, 1), tau), J10);\nL_I_11 = LCARE_Estimation_Loglik(y(CARE_Intervals(k + 1, 9) : 1 : ...\nCARE_Intervals(k + 1, 10), tau) * ones(length(J10), 1);\nL_A = [L_A_k1; ...\nL_A_k8; L_A_k10];\nL_I = [L_I_2; ...\nL_I_10;\nL_LR = L_A + L_B - L_I;\nOut = [L_A; L_B; L_LR];\nend\n", "output_sequence": "Evaluates the likelihood ratio test statistics for parameter constellation theta1 and expectile leve 0.01"}, {"input_sequence": "function Out = LCARE_Estimation(y, tau)\nObjective = @(th) - LCARE_Loglik(y, tau, th);\nOptions = optimset('Display', 'off', 'Algorithm', 'interior-point', ...\n'TolX', 1e-3, 'TolFun', 1e-3, 'MaxIter', 1000);\nth0 = [0.00, 0.01]';\nA = [0 0 -1];\nb = 0;\n\n[th_est, lik_value] = fmincon(Objective, th0, A, b, [], ...\n[], Options);\nOut = [th_est', lik_value]';\nend\n", "output_sequence": "Estimates the parameters of CARE model with fix rolling window for selected indices. Selected rolling window lengths: 1 month (20 days), 3 months (60 days), 6 months (125 days) and 12 months (250 days) for tau = 0.01."}, {"input_sequence": "clear all; clc;\ndata = load('DataIndices.dat');\nr_DAX = diff(log(data(:, 1)));\ntau = 0.01;\nroll = [20 60 125 250];\nn = length(data) - 1;\nfirst = 261;\nparas_DAX = zeros(n - first + 1, 5 * length(roll));\nfor i = 1 : 1 : length(roll)\n\n[out_DAX] = LCARE_Estimation_Rolling(r_DAX, roll(i), first, tau);\nparas_DAX(:, (5 * i - 4) : 5 * i) = out_DAX(:, 1 : 5);\nend\nsave tau_001.mat;\nfigure;\nsubplot(2, 3, 1); plot(paras_DAX(:, 2), '-b', 'LineWidth', 1.5);\ntitle('DAX');\nylabel('Value'); ylim([-3, 3]); xlim([1, 2348]);\nYear = {'2006', '2014'}; hold on;\nset(gca, 'xtick', [1 1045 2088]);\nEstimate = {'-3', '0', '3'}; hold on; set(gca, 'ytick', [-3 0 3]);\nset(gca, 'xticklabel', Year); set(gca, 'yticklabel', Estimate);\nsubplot(2, 3, 4); plot(paras_DAX(:, 17), '-b', 'LineWidth', 1.5);\ntitle('FTSE 100');\nylabel(''); ylim([-3, 3]); xlim([1, 2348]);\nsubplot(2, 3, 5); plot(paras_FTSE(:, 17), '-b', 'LineWidth', 1.5);\ntitle('S&P 500');\nsubplot(2, 3, 6); plot(paras_SP(:, 17), '-b', 'LineWidth', 1.5);\n", "output_sequence": "Estimates the parameters of CARE model with fix rolling window for selected indices. Selected rolling window lengths: 1 month (20 days), 3 months (60 days), 6 months (125 days) and 12 months (250 days) for tau = 0.01."}, {"input_sequence": "function Out = LCARE_Estimation_Rolling(y, rolling, first, tau)\n[n, ~] = size(y);\npara = zeros(n - first + 1, 5);\nfor i = first : 1 : n\n\ny_esti = y(i - rolling : i, 1);\nth_est = LCARE_Estimation(y_esti, tau);\npara(i - first + 1, :) = th_est(1 : 5)';\nend\nOut = para;\n", "output_sequence": "Estimates the parameters of CARE model with fix rolling window for selected indices. Selected rolling window lengths: 1 month (20 days), 3 months (60 days), 6 months (125 days) and 12 months (250 days) for tau = 0.01."}, {"input_sequence": "function Out = LCARE_Loglik(y, tau, th)\nn = length(y);\ny_lag = y(1 : 1 : n - 1, 1);\ny_lag_plus = (y_lag .* (y_lag > 0)) .^ 2;\nu = y(2 : n, 1) - th(1) - th(2) * y_lag - ...\nth(3) * y_lag_plus - th(4) * y_lag_neg;\nw = sqrt(abs(th(5)) / (1 - 2 / pi * (tau / sqrt(1 + tau ^ 2))^2));\nmu = - tau * w / sqrt(1 + tau^2) * sqrt(2 / pi);\n\nfe_u = 1 / w * sqrt(2 / pi) * exp(-((u - mu) .^ 2) / (2 * w^2)) .* ...\n(cdf('Normal', tau * (u - mu) / w, 0, 1));\nOut = sum(log(fe_u));\nend\n", "output_sequence": "Estimates the parameters of CARE model with fix rolling window for selected indices. Selected rolling window lengths: 1 month (20 days), 3 months (60 days), 6 months (125 days) and 12 months (250 days) for tau = 0.01."}, {"input_sequence": "function Out = LCARE_Estimation_Theta(y, tau)\nObjective = @(th) - LCARE_Loglik(y, tau, th);\nOptions = optimset('Display', 'off', 'Algorithm', 'interior-point', 'TolX', 1e-3, 'TolFun', 1e-3, 'MaxIter', 1000);\nth0 = [0.00, 0.01]';\nA = [0 0 -1];\nb = 0;\nOut = fmincon(Objective, th0, A, b, [], Options);\n\nend\n", "output_sequence": "Simulates the risk bound of a CARE model according to theta3 parameter constellation at expectile level tau = 0.05."}, {"input_sequence": "function Out = LCARE_Loglik(y, tau, th)\nn = length(y);\ny_lag = y(1 : 1 : n - 1, 1);\ny_lag_plus = (y_lag .* (y_lag > 0)) .^ 2;\ne = y(2 : n, 1) - th(1) - th(2) * y_lag - th(3) * y_lag_plus - th(4) * y_lag_neg;\nrho_tau = abs(tau - 1 * (e <= 0)) .* e.^2;\nf_y = 2 / sqrt(th(5)) * (sqrt(pi / abs(tau - 1)) + sqrt(pi / tau))^(-1) * exp (- rho_tau / th(5));\nf_y(f_y == 0) = 1;\nOut = sum(log(f_y));\nend\n", "output_sequence": "Simulates the risk bound of a CARE model according to theta3 parameter constellation at expectile level tau = 0.05."}, {"input_sequence": "clear all; clc;\n% Input\nn_0 = 20;\nTh3_005 = [0.00000, 0.12149, 0.00421, 0.17589, 0.00005]';\nTheta = Th3_005;\nc_r = 0.50;\nd_r = 1.00;\nrho = 0.25;\ntau = 0.01;\nCARE_yv = load('y_t_005_Th3');\nCARE_Intervals = load('CARE_Intervals');\nV = size(CARE_yv, 2);\n% Programme Code\nLk_Thk = zeros(K - 1, V, K - 1);\nfor v = 1 : 1 : V\nfor j = 1 : 1 : K - 1\nLk_Thk(k, v, j) = LCARE_Estimation_Loglik(CARE_yv(CARE_Intervals(k + 1, 7) : end, v), tau);\nend\nend\nRB = [max(mean(abs(Lk_Thk(:, :, 1) - Lk_Th(:, :, 1)) .^ c_r, 2));\nCARE_RB_Th3_005 = rho * RB ./ [K - 1, K - 1]';\nCARE_LR_c_Th3_005 = abs(Lk_Thk - Lk_Thl) .^ c_r;\n% Output\nOut.Lk_Thk = Lk_Thk;\nsave('CARE_RB_Th3_005', '-ascii');\nsave('CARE_risk_bound_Th3_005.mat');\n", "output_sequence": "Simulates the risk bound of a CARE model according to theta3 parameter constellation at expectile level tau = 0.05."}, {"input_sequence": "function Out = LCARE_Estimation_Loglik(y, tau)\nObjective = @(th) -LCARE_Loglik(y, tau, th);\nOptions = optimset('Display', 'off', 'Algorithm', 'interior-point', 'TolX', 1e-3, 'TolFun', 1e-3, 'MaxIter', 1000);\nth0 = [0.10, 1.00, 0.01]';\nA = [0 0 -1];\nb = 0;\n\n[~, lik_value] = fmincon(Objective, th0, A, b, [], Options);\nOut = -lik_value;\nend\n", "output_sequence": "Simulates the risk bound of a CARE model according to theta3 parameter constellation at expectile level tau = 0.05."}, {"input_sequence": "### this is the code to plot the weight of portfolios formed by Markowitz method\n### with or without crypto currencies.\n# please change your working directory\n# setwd('...')\nrm(list = ls())\ngraphics.off()\nlibraries = c(\"xts\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\nload(\"processed_data.RData\")\n# write a function to conduct Markowitz portfolio formation\nMarkwtz = function(Asset, sigma = NULL, rf = NULL) {\nif (is.xts(Asset) == T)\ntime = index(Asset)\nmu_hat = colMeans(Asset, na.rm = T)\nSig_hat = cov(Asset, use = \"pairwise.complete.obs\")\nnn = ncol(Asset)\nls = rep(1, times = nn)\n\n## define intermediate variables\nAA = mu_hat %*% solve(Sig_hat, mu_hat)\n## calculate the global minimum variance value or market portfolio\nwei_Mkwtz_g = solve(Sig_hat, ls)/CC\nret_Mkwtz_g = Asset %*% wei_Mkwtz_g\nret_Mkwtz_g = xts(ret_Mkwtz_g, order.by = time)\noutlist = list(ret_Mkwtz_g = ret_Mkwtz_g, time = time, wei_Mkwtz_g = wei_Mkwtz_g)\n## calculate weight and return given the risk constraints\nif (is.null(sigma) == F && is.null(rf)) {\nlam1 = sqrt((AA * CC - BB^2)/(CC * sigma^2 - 1))\nlam2 = (BB - lam1)/CC\n# ret_Mkwtz = lam1 * sigma^2 + lam2\nwei_Mkwtz = 1/lam1 * solve(Sig_hat, mu_hat) - lam2/lam1 * solve(Sig_hat,\nls)\nret_Mkwtz = Asset %*% wei_Mkwtz\nret_Mkwtz = xts(ret_Mkwtz, order.by = time)\noutlist$ret_Mkwtz = ret_Mkwtz\noutlist$wei_Mkwtz = c(1 - sum(wei_Mkwtz),\n}\nif (is.null(sigma) == F && is.null(rf) == F) {\nwei_Mkwtz = sigma * solve(Sig_hat, mu_hat)/sqrt(AA)\nret_Mkwtz = Asset %*% wei_Mkwtz + rf * (1 - sum(wei_Mkwtz))\nreturn(outlist)\n}\n# write a function to conduct portfolio formation with or without certain\n# additional assets, here, this additional assets means crypto currencies.\nadd_asset = function(Asset, add, f, get, ...) {\nnc1 = NCOL(Asset)\naa = cbind(Asset, add)\naa = na.omit(aa)\ntimeind = index(aa)\nbb1 = f(aa, ...)\nbb2 = f(aa[, 1:nc1], ...)\nbb = list(both = bb1, asset1 = bb2, time_index = timeind)\nreturn(bb)\n# for Portugal stocks with/without Cryptos\npocp_wei = add_asset(Asset = ret_pop, add = ret_cpp, f = function(x) Markwtz(x,\nsigma = 0.002, rf = 0.9/360/100)$wei_Mkwtz)\nmaxy = max(c(pocp_wei$both[-1], pocp_wei$asset1[-1]))\nplot(pocp_wei$both[-1], type = \"p\", col = \"blue\", pch = 20, panel.first = grid(),\nxlab = \"assets\", ylab = \"weights\", main = \"Weights of Portogal Stocks with/without Cryptos\")\npoints(pocp_wei$asset1[-1], type = \"p\", col = \"brown\", pch = 20)\nabline(v = ncol(ret_pop) + 0.5, lty = 2, col = \"black\", lwd = 2)\n# for DAX30 stocks with/without Cryptos\ndacp_wei = add_asset(Asset = ret_dap, add = ret_cpp, f = function(x) Markwtz(x,\nmaxy = max(c(dacp_wei$both[-1], dacp_wei$asset1[-1]))\nplot(dacp_wei$both[-1], type = \"p\", col = \"blue\", pch = 20, ylim = c(miny, maxy),\npanel.first = grid(), xlab = \"assets\", ylab = \"weights\", main = \"Weights of DAX30 Stocks with/without Cryptos\")\npoints(dacp_wei$asset1[-1], type = \"p\", col = \"brown\", pch = 20)\nabline(v = ncol(ret_dap) + 0.5, lty = 2, col = \"black\", lwd = 2)\n", "output_sequence": "Add Crypto currencies to portfolios including Portugal stocks or DAX30 stocks to check whether there are return increase"}, {"input_sequence": "rm(list = ls(all = T))\ngraphics.off()\ny = c(9, 11, 7, 11, 7, 15, 9, 14, 15)\na = rep(0, times = 10)\nx2 = c(a, b, a)\nlm1 = lm(y ~ x2 + x3) # linear model y = b0 + b1*x2 + b2*x3\nsummary(lm1)\nanova(lm1)\n", "output_sequence": "Estimates a linear model (ANOVA). The plot presents results of the estimation of a linear model"}, {"input_sequence": "# install.packages('VGAM') install.packages('KernSmooth')\nrm(list = ls(all = TRUE))\nlibrary(KernSmooth)\nlibrary(VGAM)\nlibrary(expectreg)\nlibrary(mboost)\n##### ---------------------------------- functions to estimate the local quantile regression\nlprq_xx <- function(x, y, h, tau = NA, xx) {\n# modified from lprq, s.t. we can specify where (xx) to estimate the quantiles, uses Gausian Kernel parameters:\n# x=explanatory variable; y= dependent variable; h=bandwidth; tau=quantile level, if unspecified the function estimates\n# RANDOM quantiles; xx=grid points WHERE to estimate\nfv <- xx\nfor (i in 1:length(xx)) {\nz <- x - xx[i]\nwx <- dnorm(z/h)\nif (tau == NA) {\nr <- rq(y ~ z, weights = wx, tau = runif(1), ci = FALSE)\n} else {\nr <- rq(y ~ z, weights = wx, tau = tau, ci = FALSE)\n}\nfv[i] <- r$coef[1]\n}\nlist(xx = xx, fv = fv, dv = dv)\n}\nlocal.expectile <- function(y, x, z, h, q) {\n# function to compute local expectile; uses Epanechnikov kernel parameters: x=explanatory variable; y= dependent variable;\n# z= grid points; h=bandwidth, q=expectile level\nx_0 = rep(1, length(x) * length(z))\nx_1 = t(x_0)\nx0 = x * x_0 #grid points\nw1 = 0.5\ny0 = y * x_0 #initial weight\ny11 = 0.5 * y * x_1 #initial expectile value\nx1 = z * x_1\nx10 = x0 - t(x1)\nx11 = kernel(x10/h) # Epanechnikov kernel weights\nit = 1 # count for the iterations, total 100 iterations\ndw1 = 1 # convergence indicator, if dw1=0 convergence is achieved\nwhile (dw1 != 0 & it < 100) {\nv1 = ((y0 < t(y11)) - 2 * q * (y0 < t(y11)) + q) * x11\nv2 = y * v1\nf1 = apply(v1, 2, mean)\nf3 = f2/f1\nw01 <- w1\nw1 <- as.vector(ifelse(y0 > t(f3 * x_1), q, 1 - q))\ndw1 <- sum(w1 != w01, na.rm = TRUE) # if the weights do not change the algorithm has converged\ny1 = f3\ny11 = f3 * x_1\nreturn(y1) # returns the local expectile of y\n\nkernelq <- function(u) {\ndnorm(u, mean = 0, sd = 1)\n} # returns Gausian kernel weights\nkernel <- function(x) {\n0.75 * (1 - x^2) * (abs(x) <= 1)\n##### -------------------------------------- Simulation of the data and estimation of the expectile parameters to be specified\ncoverate = 0\narea.cover = 0\nB = 500\nn = 500 # number of observation\nq = 0.95 # expectile level\nh = 0.2 # bandwidth\nalpha = 0.05 # confidence level\ngridn = n # the length of grid\ncc = 1/4 # kernel constant\nset.seed(10 * 3) # fix the random generator seed\nx = runif(n, 0, 2)\ny = 1.5 * x + 2 * sin(pi * x) + rnorm(n) # simulate the data with standard normal noise\ne_theor = 1.5 * x + 2 * sin(pi * x) + enorm(q) # compute the theoretical expectile\nz = x\nv = x # v for the nonparametric part\nbound = c(0, 1)\nyuv = sort(v) # sort the grid points\nyur = y[order(v)] # reorder y wrt the grid\n##### -------------------------------------- the expectile and confidence bands for tau=0.95 and alpha = 0.05\nh12 = (0.5 * (1 - 0.5)/dnorm(qnorm(0.5))^2)^0.2 * h\nb2 = max(h/sd(yuv) * sd(yur), h12^5/(h)^3, h/10)\nlambda = 1/(2 * sqrt(pi)) # this is for normal kernel, if quartic kernel, value is 5/7\ndelta = 1/4 #log(h)/log(n)\\t\\t\\t\\t\\t\\t# compute the constants\ndn = sqrt(2 * delta * log(n)) + (2 * delta * log(n))^(-1/2) * log(cc/2/pi)\n# compute binned approximation to the kernel density estimate\nfxd = bkde(yuv, gridsize = gridn, range.x = c(min(z), max(z)))\nvalues = local.expectile(y, x, z, h, q)\nyy1 = y - values[order(x)]\nfx_yy = bkde(sort(yy1), gridsize = gridn, range.x = c(min(yy1), 0))\ndelta_yy = c(min(yy1), fx_yy$x)\nsigma = q^2 * mean(yy1^2) + (1 - 2 * q) * sum(yy1^2 * fx_yy$y * diff(delta_yy))\nf_x = ecdf(yy1)\nfx = q + (1 - 2 * q) * f_x(0)\nbandt = (fxd$y)^(1/2) * fx\ncn = log(2) - log(abs(log(1 - alpha)))\n# determine the corridor\nband = (n * h)^(-1/2) * sqrt(lambda * sigma) * bandt^(-1) * (dn + cn * (2 * delta * log(n))^(-1/2))\ncoverate1 = min(e_theor[order(x)] <= values[order(x)] + band, e_theor[order(x)] >= values[order(x)] - band)\ncoverate = coverate + coverate1\narea.cover = mean(band) + area.cover\n##### --------------------------------------- plots\nX11()\n# plot the theoretical and the estimated expectile curves and the 95% confidens bands\nplot(x, y, type = \"p\", pch = 20, ylim = c(min(y), max(y)), xlab = \"X\", ylab = \"Y\", cex.lab = 1.8, cex.axis = 1.8)\nlines(sort(x), values[order(x)], col = \"blue\", lwd = 2, type = \"l\")\n# Plot the quantile and expectile curves for standard normal distribution.\nx = seq(1e-04, 0.9999, length = 1000)\ny1 = enorm(x)\ny = cbind(y1, y2, 0)\nmatplot(x, y, type = \"l\", lty = 1, cex.axis = 2, cex.lab = 2, xlab = \"tau\", ylab = \"\", lwd = 3, col = c(3, 4, 2), ylim = c(-2,\n2))\n", "output_sequence": "Constructs the confidence bands for an expectile curve with a confidence level of 0.95."}, {"input_sequence": "# clear history\nrm(list = ls(all = TRUE))\ngraphics.off()\n# install/load packages\nlibraries = c(\"foreign\", \"stats\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\nDAXall = read.csv(\"DAX20000101-20111231.csv\")\ntime = seq(from = as.Date(\"2000-01-03\"), to = as.Date(\"2011-12-27\"), length.out = 626)\nDAX = DAXall[1:626, 8]\nplot(DAX ~ time, xlab = \"Time\", ylab = \"DAX30 Weekly Return\", type = \"l\", pch = 20, cex = 1, cex.axis = 1.2, cex.lab = 1.5,\nlab = c(3, 3, 0), main = \"\", col = \"blue4\")\nDAX1 = DAXall[2:209, 8]\nvar.test(DAX1, DAX2)\n", "output_sequence": "Plots the DAX returns from 2000 to 2011 and performs an F-test for variance comparison of specific time periods. From the plot a period of higher volatility is followed by a period of lower volatility and again followed by a period of higher volatility. To proof this finding, the time series is split into 3 intervals, each of which is tested against the other for variance comparison. The variances of the first and the third period differ significantly from the second period."}, {"input_sequence": "# install.packages('decon')\nlibrary(decon)\nn = 3000\nx = rnorm(n, 3, 2)\nsig = 0.9\nu = sig * rnorm(n)\nw = x + u\ne = rnorm(n, 0, 0.1)\ny = 5 * x^2 + e\nbw1 <- bw.dboot1(w, sig)\nm = DeconNpr(w, sig, y, error = \"normal\", from = min(x), to = max(x))\nplot(m, col = \"blue\", cex.axis = 1.2, font.axis = 1, cex.lab = 1.2, las = 1, lwd = 4, lty = 2, xlab = expression(paste(psi)),\nylab = expression(paste(m(psi))), main = \"\")\nks_uncon = ksmooth(x, y, kernel = \"normal\", 2, range.x = c(min(x), max(x)))\nlines(ks_uncon, col = \"black\", lwd = 4, lty = 1)\nks_con = ksmooth(w, y, kernel = \"normal\", 2, range.x = c(min(x), max(x)))\nlines(ks_con, col = \"red\", lwd = 4, lty = 3)\n", "output_sequence": "Plots the deconvoluted kernel regression curve, the kernel regression curve from the sample without measurement errors (i.e. kernel regression based on x) and the kernel regression curve from the sample with measurement errors (i.e. kernel regression based on z)."}, {"input_sequence": "close all\nclear all\nclc\ndisp('Please input number of draws n as') ;\ndisp(' ') ;\nn = input('[n]=');\ny = normrnd(0,1,n,1); % Generate standard normal random numbers\ny = sort(y);\ncdfplot(y) % Plot the empirical distribution function\nhold on\nf = cdf('Normal',y,0,1); % Generate normal cumulative distribution function\nplot(y,f,'r','LineWidth',2.5)\nlegend('Empirical','Theoretical','Location','NW')\ntitle('EDF and CFD')\nxlabel('X')\nylabel('EDF(X), CDF(X)')\ngrid off\n[g,y] = ecdf(y);\ng = g(2:(n+1));\n[C,I] = max(abs(f-g))\n", "output_sequence": "Draws n observations from standard normal distribution and plots its empirical distribution function (edf) vs. the normal cumulative distribution function (cdf). Number of draws can be entered interactively. The point where the edf and cdf differs most is also reported."}, {"input_sequence": "# % ------------------------------------------------------------------------- % Book: MSE %\n# ------------------------------------------------------------------------- % Description: MSEnonpara1 generate n=300 t(3)\n# distributed random sample, and draws the kernel density curve of the estimated kernel density function using a % Gaussian\n# kernel. Since the kernel density function is biased in a finite sample, one can not compare it with the true density\n# directly. One rather compare it with the expectation of the kernel density function under the true density. Then it draws\n# the expectation of the kernel density function under the true density. %\n# ------------------------------------------------------------------------- % Usage: - %\n# ------------------------------------------------------------------------- % Output: 1. Draws n_1 observations from t(3)\n# distribution % and plots the kernel density curve of the estimated kernel density function using a % Gaussian kernel. %\n# 2. generate n observations from t(3) distribution and plots the expectation of the kernel density function under the true\n# density which is t(3) distribution. % 3. generate n observations from normal distribution with mean and standard deviation\n# from the generated sample, plots the expectation of the kernel density function under the true density which is normal\n# distribution with mean and standard deviation from the generated sample. %\n# ------------------------------------------------------------------------- % Example: n_1 = 300 % n_1 = 3000 %\nn_1 = 300\nobs = rt(n_1, 3)\nden_x = density(obs, kernel = \"gaussian\", bw = \"nrd0\")\nplot(den_x, lwd = 4, lty = 1, col = \"blue\", cex.axis = 1.2, font.axis = 1, cex.lab = 1.5, las = 0, xlab = \"x\", ylab = \"f(x)\",\nmain = \"\")\nn = 1e+06\nz = rt(n, 3)\nh = den_x$bw\ng = 1:512\nfor (j in 1:length(den_x$x)) {\nsum_k = 0\nfor (i in 1:length(z)) {\nu = (den_x$x[j] - z[i])/h\nk = exp(-u^2/2)/sqrt(2 * pi)\nsum_k = sum_k + k\n}\ng[j] = 1/(h * length(z)) * sum_k\n}\nlines(den_x$x, g, lwd = 3, lty = 2, col = \"red\")\nmu = mean(obs)\nsd = sd(obs)\nq = rnorm(n, mu, sd)\nf = 1:512\nfor (i in 1:length(q)) {\nu = (den_x$x[j] - q[i])/h\nf[j] = 1/(h * length(z)) * sum_k\nlines(den_x$x, f, lwd = 4, lty = 3, col = \"black\")\n", "output_sequence": "Generates an n=300 t(3) distributed random sample, and draws the kernel density curve of the estimated kernel density function using a Gaussian kernel, since the kernel density function is biased."}, {"input_sequence": "rm(list=ls())\nlibrary(grf)\nlibrary(ggplot2)\nlibrary(NNTbiomarker)\nset.seed(42)\n# Getting the path of your current open file\ncurrent_path = rstudioapi::getActiveDocumentContext()$path\nsetwd(dirname(current_path ))\n# Function definition for data --------------------------------------------\nget_x = function(n, seed=NULL){\nset.seed(seed)\nX = expand.grid(\nX1 = seq(-0.5, 0.5, length.out = n),\nX2 = seq(0, 1, 0.02)\n)\n# X = data.frame(matrix(\n# c(\n# seq(-0.5, 0.5, length.out = n),\n# ),\n# nrow = n,\n# ))\n# colnames(X) = c('X1', 'X2')\nreturn(X)\n}\nget_x_grid = function(n){\nX2 = seq(-0.5, 0.5, length.out = n)\ntheta_triangle = function(x, width){\npmax(1 - abs((x[, 'X1']) / width), 0)\nget_y = function(X, theta, sigma, seed=NULL){\nset.seed(NULL)\nn = nrow(X)\nreturn(theta(X) + rnorm(n, 0, sigma))\n# Computing ---------------------------------------------------------------\nfor (sig in c(0, 0.1)){\nrfs = list()\nfor (n in c(50, 100, 200)){\nwidth = 0.2\nX = get_x(n, seed=42)\ntheta = function(X) theta_triangle(X, width)\nY = get_y(X, theta, sig, 42)\nrfs[[as.character(n)]] = rf = regression_forest(X, Y)\neps_tilde = Y - theta(X)\nalpha = get_sample_weights(rf)\ntheta_tilde = theta(X) + alpha %*% eps_tilde\n\np = data.frame(cbind(X, Y)) %>%\nggplot(aes(X1, X2)) +\ngeom_point(aes(colour = Y)) +\nscale_colour_gradient(low = \"#132B43\", high = \"#56B1F7\") +\ntheme_bw() +\npanel.grid.major = element_blank(),\n)\nfn = glue(\n'scatter/',\n'RF_theta_triangle___theta_tilde___',\n'sigma{formatC(sig*100, width=3, flag=\"0\")}___',\n'.png'\n)\nggsave(fn, plot = p, dpi = 300)\n}\nalpha = lapply(rfs, function(x) get_sample_weights(x))\nalpha_max = max(sapply(alpha, max))\nrf_X.orig = lapply(rfs, function(x) x$X.orig)\nx1s_subset = expand.grid(c(0, 0.18, 0.2, 0.5), c(1, 0.5))\nfor (n in as.integer(names(rfs))){\nfor (i in 1:nrow(x1s_subset)){\nx1 = x1s_subset[i, 1]\nid = argmin(rowSums(cbind(rf_X.orig[[as.character(n)]][, 1] - x1, rf_X.orig[[as.character(n)]][, 2] - x2) ** 2))[1]\nfn = glue(\n'raster/',\n'RF_theta_triangle___effective_weights___',\n'sigma{formatC(sig*100, width=3, flag=\"0\")}___',\n'x_{formatC(x1*100, width=3, flag=\"0\")}_{formatC(x2*100, width=3, flag=\"0\")}',\n'.png'\np = data.frame(cbind(rf_X.orig[[as.character(n)]], alpha = alpha[[as.character(n)]][, id])) %>%\nggplot(aes(X1, X2, fill = alpha)) +\ngeom_raster(interpolate = FALSE) +\nscale_fill_gradient2(\nlimits = c(0, alpha_max),\nname = c(expression(alpha[i](x[1], x[2])))\n) +\ntheme_bw() +\npanel.grid.major = element_blank(),\nannotate(\n\"point\",\nx = rf_X.orig[[as.character(n)]][id, 1],\npch = 4,\ncolour = 'red',\nsize = 3\n)\nggsave(fn, plot = p, dpi = 300)\n'contour/',\n)\nbreaks = c(0, 0.001, round(alpha_max, 3))\ncontour = cbind(rf_X.orig[[as.character(n)]], alpha = alpha[[as.character(n)]][, id]) %>%\nggplot(aes(X1, X2, z = alpha)) +\ngeom_contour_filled(\naes(fill = stat(level)),\nbreaks = breaks\nscale_fill_brewer(\nname = c(expression(alpha[i](x[1], x[2]))),\npalette = 'Blues',\ndirection = 1,\nguide = 'legend',\ndrop = FALSE\nggsave(fn, plot = contour, dpi = 300)\n}\n", "output_sequence": "Estimation of effective weights alpha_i(x_1, x_2) for the infeasable observations theta_tilde. The effective weights are computed by a regression forest on a grid of observations x_ij=(-0.5 + i/n1, 0 + 0.02 * j) for i=1,...,n1 and j=0,...,50, for n1=50, 100, 200 and target variable Y_i=theta(x_ij) + eps_ij, for a given theta function, here triangle function theta(x_ij) = max(0, 1 - |x_ij,1|/0.2), with Gaussian noise eps_ij with mean zero and standard deviation of 0 and 0.1."}, {"input_sequence": "rm(list=ls())\nlibrary(grf)\nlibrary(ggplot2)\nset.seed(42)\np = 1\nn = 1000\nsigma = 0.2\nX = matrix(rnorm(n * p, 0.5, sigma), nrow = n)\ncolnames(X) = paste0('X', 1:p)\ntheta = function(x){\nx\n}\nget_y = function(X, sigma, seed=NULL){\nset.seed(NULL)\nn = nrow(X)\nreturn(theta(X) + rnorm(n, 0, sigma))\ndens = list()\ndens[['truth']] = density(theta(X))\nfor (sig in c(0.1, 0.2, 0.3)){\nY = get_y(X, sig)\nrf = regression_forest(X, Y)\neps_tilde = Y - theta(X)\nalpha = get_sample_weights(rf)\ntheta_tilde = theta(X) + alpha %*% eps_tilde\ndens[[as.character(sig)]] = density(matrix(theta_tilde))\npdf('density_theta_tilde.pdf')\nplot(\ndens[['0.1']],\ncol = 'black',\nxlab = glue(\n'X1\\nN={n}, MeanBandwidth={round(mean(sapply(dens, function(x) x$bw)), 3)}'\n),\nmain = parse(text = 'Density~of~tilde(theta)(X)')\n)\nlines(dens[['0.2']], col = 'blue')\ndev.off()\npng('density_theta_tilde.png')\n", "output_sequence": "Estimation of infeasable observations of theta_tilde, based on given theta (generally unknown) and effective weights (alpha) of (generalized) random forest."}, {"input_sequence": "#install.packages(\"matrixStats\") ----\nrm(list=ls())\nlibrary(grf)\nlibrary(ggplot2)\nlibrary(rstudioapi)\nlibrary(parallel)\nlibrary(matrixStats)\nset.seed(42)\n#install.packages(\"matrixStats\") ----\n# Getting the path of your current open file ----\ncurrent_path = rstudioapi::getActiveDocumentContext()$path\nsetwd(dirname(current_path ))\n# Function definition for data --------------------------------------------\nget_x = function(n,seed){\nset.seed(seed)\nX= expand.grid(X1 = seq(-0.5, 0.5, length.out = n))\n#X= expand.grid(X1 = seq(-0.5, 0.5, length.out = n), X2 = seq(0, 1, 0.1))\nreturn(X)\n}\ntheta_triangle = function(x, width){\npmax(1 - abs((x[[1]]) / width), 0)\n}\nget_y = function(X, theta, sigma, seed=NULL, reps = 1){\nset.seed(NULL)\nn = nrow(X)\nreturn(replicate(reps,(theta(X) + rnorm(n, 0, sigma))))\n# Data initializing ------------------------------------------------------\ntau = c(0.5)\nsig = 0.1\nwidth = 0.2\nc = 1\nb = 100 #number of bootstraps for MBS\nreps = 10 #repititions for confidence interval\ngrids = 100 #grid points for CBs\nn= 500\nset.seed(100)\nT_stats = list()\ncoverages = list()\nnode_size = 3\n## Estimation ---------------\nT_stat = list()\nptm <- proc.time()\nX = get_x(n, seed=42) #no replications for X because X is deterministic\ntheta_fun = function(X) theta_triangle(X, width)\ntheta_true = theta_triangle(X, width) + qnorm(tau)*sig\nY = get_y(X, theta_fun, sig, NULL,reps)\nrand_for = function(j) grf::quantile_forest( X ,data.matrix(Y[,j]),\nquantiles = tau, min.node.size = node_size)\nrf = lapply(1:reps, rand_for)\nw = sapply(1:reps,function(j) get_sample_weights(rf[[j]]))\nobjective_fun = function(theta,Y,alpha)\nsum(((Y-theta)) * as.matrix(alpha) * (tau - (Y <= theta)))\ntheta_hat = lapply(1:reps, function(j) sapply(1:nrow(X), function(k)\noptimize(f=objective_fun,interval = c(0,1),\ntol = 0.0001, Y=Y[,j], alpha=w[[j]][k,])[1]))\n## Calculations for test set ----\nX_test = get_x(grids, seed=50)\nY_test = get_y(X_test, theta_fun, sig, NULL,reps)\ntheta_hat_test = lapply(1:reps, function(j) approx(X$X1,unlist(theta_hat[[j]]),\nxout = X_test$X)[[\"y\"]])\nrand_for = function(j) grf::quantile_forest( X_test ,data.matrix(Y_test[,j]),\nquantile = tau, min.node.size = node_size)\nw_test = lapply(1:reps,function(j) get_sample_weights(rf[[j]]))\ntheta_true_test = theta_triangle(X_test, width) + qnorm(tau)*sig\n## just for simplicity, renaming test sets as original sets\nX = X_test\ntheta_hat = theta_hat_test\nw = w_test\nkde = lapply(1:reps, function (j) density(Y[,j], n=n)) #estimation of the density\nf_Y= sapply(1:reps, function(j) unlist(approx(kde[[j]][[\"x\"]], kde[[j]][[\"y\"]], xout = c(theta_hat[[1]]))[2]))\nV_hat = 1/f_Y\nH_hat = sapply(1:reps, function(j) sapply(1:nrow(X),\nfunction(k) n *((var(w[[j]][k,]*(tau - (Y[,j] <= unlist(theta_hat[[j]]))))))))\nsigma_hat = (f_Y^(-2)*H_hat)^(1/2)\ne_multipliers = lapply(1:reps, function(j) lapply(1:b, function(j) rnorm(nrow(X), 0, 1)))\n(w[[k]] %*% ((H_hat[,k]^(-1/2)) * (tau - (Y[,k] <= unlist(theta_hat[[k]]))) * e_multipliers[[k]][[j]]))@x))\n## Confidence interval with test stat\nalpha_sig = 0.05\nT_stat_abs = lapply(1:reps, function(j) abs(t(T_stat[[j]])))\nq_star = lapply(1:reps, function(j) colQuantiles(T_stat_abs[[j]] , probs= c(1-alpha_sig)))\nCI = lapply(1:reps, function(j) list(unlist(theta_hat[[j]])-(q_star[[j]]*sigma_hat[j]),\nunlist(theta_hat[[j]])+(q_star[[j]]*sigma_hat[j])))\nprint(proc.time() - ptm)\n## Calculating the coverage ----\nT_stats[[as.character(n)]] = T_stat\nq_norm = qnorm(1-alpha_sig)\nCI_std = lapply(1:reps, function(j) list(unlist(theta_hat[[j]])-(q_norm*sigma_hat[j]),\nunlist(theta_hat[[j]])+(q_norm*sigma_hat[j])))\ncoverage = mean(sapply(1:reps, function(k)\nsum((theta_true > CI[[k]][[1]]) & (theta_true < CI[[k]][[2]]))/nrow(X)))*100\ncoverage_std = mean(sapply(1:reps, function(k)\nsum(theta_true > CI_std[[k]][[1]] & theta_true < CI_std[[k]][[2]])/nrow(X)))*100\ncoverages[[as.character(n)]] = coverage\n## Uniform confidence bands ----\n#grid_T_stat = T_stat_abs[[1]][,seq(1, nrow(X), length.out = grids) ] #t stats for CB\ngrid_T_stat = T_stat_abs[[1]] #t stats for CB\ngrid_T_max = apply(grid_T_stat, 1, max)\ngrid_q_star = quantile(grid_T_max, 1-alpha_sig)\npd = data.frame(X1=X$X1, sigma = sigma_hat[[1]],theta_hat = unlist(theta_hat[[1]]),\ntheta_true = theta_true, CI_L = CI[[1]][[1]], CI_U = CI[[1]][[2]])\npd$grid_CI_L = pd$theta_hat-(grid_q_star*pd$sigma)\n# Plotting ----\npng(file = glue('CI_bands_','n{formatC(as.integer(n*10), width=4, flag=\"0\")}_',\n'tau{formatC(tau*10, width=3 ,flag=\"0\")}_',\n'.png'),\nwidth=1500, height=1500)\nplot(1, type=\"n\", xlab=\"X\", ylab=bquote(theta), xlim=c(-0.5, 0.5),\nylim=range(c(pd$theta_true,pd$theta_hat, pd$CI_L,pd$CI_U, pd$grid_CI_U,pd$grid_CI_L)))\npoints(pd$X1, pd$theta_true,\ncol='red', main=\"Confidence intervals\", pch=19, cex=2)\npoints(pd$X1, pd$theta_hat, col='blue', pch=19, cex=2)\nlines(pd$X1, pd$CI_L, col='black',pch = 19,type = \"b\", lty = 2, cex=0.8)\ndev.off()\n# plot with average confidence intervals (cancelled) ----\n# avg_ci_lower = 0\n# avg_theta_hat = 0\n# for (i in seq(1, reps,1)){\n# avg_ci_lower = avg_ci_lower + CI[[i]][[1]]\n# avg_theta_hat = avg_theta_hat + unlist(theta_hat[[i]])\n# }\n# avg_ci_lower = avg_ci_lower/reps\n# avg_theta_hat = avg_theta_hat/reps\n#\n# png(file = glue('CI_averaged_','n{formatC(as.integer(n), width=4, flag=\"0\")}_',\n# 'sig{formatC(sig*10, width=3 ,flag=\"0\")}_',\n# '.png'),\n# width=1500, height=1500)\n#\n# pd = data.frame(X1=X$X1, theta_hat = avg_theta_hat,\n# theta_true = theta_true, CI_L = avg_ci_lower, CI_U = avg_ci_upper)\n# pd = pd[order(X$X1),] #ordering X wrt X1\n# plot(1, type=\"n\", xlab=\"X\", ylab=bquote(theta), xlim=c(-0.5, 0.5),\n# ylim= range(pd$theta_true, pd$theta_hat, pd$CI_L,\n# points(pd$X1, pd$theta_true ,\n# ylim=range(pd$theta_true,pd$theta_hat, pd$CI_L,\n# col='red', main=\"Confidence intervals\", pch=19, cex=2)\n# points(pd$X1, pd$theta_hat, col='blue', pch=19, cex=2)\n# lines(pd$X1, pd$CI_L, col='black',pch = 19, type = \"b\", lty = 2, cex=0.8)\n# dev.off()\n# ## plot with multiple confidence intervals\n# for (num in c(1,1)){\n# png(file = glue('CI_','n{formatC(as.integer(n), width=4, flag=\"0\")}_',\n# 'sig{formatC(sig*10, width=3 ,flag=\"0\")}_',\n# '.png'),\n# width=1500, height=1500)\n#\n# plot(1, type=\"n\", xlab=\"X\", ylab=bquote(theta), xlim=c(-0.5, 0.5),\n# ylim= range(pd$theta_true, pd$theta_hat, pd$CI_L,\n# for (i in 1:num){\n# pd = data.frame(X1=X$X1, theta_hat = unlist(theta_hat[[i]]),\n# theta_true = theta_true, CI_L = CI[[i]][[1]], CI_U = CI[[i]][[2]] )\n# pd = pd[order(X$X1),] #ordering X wrt X1\n# points(pd$X1, pd$theta_true ,\n# ylim=range(pd$theta_true,pd$theta_hat, pd$CI_L,\n# col='red', main=\"Confidence intervals\", pch=19, cex=2)\n# points(pd$X1, pd$theta_hat, col='blue', pch=19, cex=2)\n# lines(pd$X1, pd$CI_L, col='black',pch = 19, type = \"b\", lty = 2, cex=0.8)\n# }\n# dev.off()\n", "output_sequence": "Estimation of infeasable observations of theta_tilde, based on given theta (generally unknown) and effective weights (alpha) of (generalized) random forest."}, {"input_sequence": "# Getting the path of your current open file ----\ncurrent_path = rstudioapi::getActiveDocumentContext()$path\nsetwd(dirname(current_path ))\n# Loading source files ----\nsource(\"support/packages_and_libraries.R\")\nsource(\"support/functions.R\")\nptm <- proc.time()\n# Data initializing ------------------------------------------------------\ntau = c(0.1)\nsig = 1\n#widths = c(0.001,0.01, 0.04, 0.1, 0.2) # used for the plot along with for loop\nwidth = 0.2\nc = 5\nn1 = 1000 #data points for x1\nb = 500 #number of bootstraps for MBS\nreps = 10 #repititions of whole simulation\ngrids_x1 = 200 #grid points for CBs\ngrids_x1_list = c(15,20,30,50, 100)\nset.seed(100)\nnode_size = c(3)\nnode_sizes = c(2,5,7,10,15) #bias controlling param of RFS\nx2_fixed = c(0.3,0.5) #fix x2\nalpha_sig = 0.05\nrfs = list()\nT_stats = list()\ncoverages = list()\ncoverages_widths_PW = list()\nT_stat = list()\nsize_bandwidth_uni = list()\n### here define calculation type 'size', 'power' or 'normal'\ncal_type = 'normal'\n## if power, provide a constant\ntheta_0s = seq(0.00001, 1, length.out = 5)\ntheta_0 = 0\n# Data generation ----\n#for (grids_x1 in grids_x1_list){\n#for (node_size in node_sizes){\n#for (theta_0 in theta_0s){\nset.seed(123)\n## Sample\nX = get_x(n1, seed=42) #no replications for X because X is deterministic\ntheta_fun = function(X) theta_triangle(X, cal_type = cal_type, c = theta_0 )\n#theta_true = theta_triangle(X, width) #+ qnorm(tau)*sig ##this is used for the case of quantile reg\n#Y = get_y(X, zero, sig, NULL,reps) #for size\nY = get_y(X, theta_fun, sig, NULL,reps)\nn = nrow(X)\n## Test set\nX_test = get_x(grids_x1)\ntheta_true_test = theta_triangle(X_test, cal_type = cal_type, c = theta_0)# + qnorm(tau)*sig\ngrids = nrow(X_test)\n## Fitting the forest ----\nrf= fit_forest(X,Y,tau = tau,node_size = node_size)\ntheta_hat_test = predict_forest(rf,X_test)\nmat <- do.call(\"cbind\",theta_hat_test)\ntheta_hat_expected = rowMeans(mat)\n## Calculations for quantile forest ----\n# kde = lapply(1:reps, function (j) density(Y_test[,j], n=nrow(X))) #estimation of the density\n# f_Y= sapply(1:reps, function(j) unlist(approx(kde[[j]][[\"x\"]], kde[[j]][[\"y\"]], xout = c(theta_hat_test[[1]]))[2]))\n# V_hat = 1/f_Y\n# psi = lapply(1:reps, function(k) t(sapply(1:nrow(X_test), function(j) tau - (Y[,k]<=theta_hat_test[[k]][j]))))\n# H_hat = sapply(1:reps, function(j) n* apply(w_test[[j]]*psi[[j]],1,var))\n# sigma_hat = (f_Y^(-2)*H_hat)^(1/2)\n# set.seed(5)\n# e_multipliers = lapply(1:reps, function(j) lapply(1:b, function(j) rnorm(n, 0, 1)))\n# (-(H_hat[,k]^(-1/2)*w_test[[k]]*psi[[k]])%*% e_multipliers[[k]][[j]])@x))\n# #T_stat = test_stat(X,Y,X_test,Y_test,theta_hat_test,w_test, tau= b)\n# T_stat_abs = lapply(1:reps, function(j) abs((T_stat[[j]])))\n## Calculation for regression forest\nV_hat = -1\npsi = lapply(1:reps, function(k) t(sapply(1:nrow(X_test), function(j) (Y[,k]-theta_hat_test[[k]][j]))))\nH_hat = sapply(1:reps, function(j) n* apply(w_test[[j]]*psi[[j]],1,var))\nsigma_hat = (H_hat)^(1/2)\ne_multipliers = lapply(1:reps, function(j) lapply(1:b, function(j) rnorm(n, 0, 1)))\n((H_hat[,k]^(-1/2)*w_test[[k]]*psi[[k]])%*% e_multipliers[[k]][[j]])@x))\nT_stat_abs = lapply(1:reps, function(j) abs((T_stat[[j]])))\n## just for simplicity, renaming test sets as original sets\n# X = X_test\n# theta_hat = theta_hat_test\n# w = w_test\n## Confidence interval with test stat ----\n### alpha quantile of test statistic\nq_star = lapply(1:reps, function(j) rowQuantiles(T_stat_abs[[j]] , probs= c(1-alpha_sig)))\nCI = confidence_interval(theta_hat_test, q_star, sigma_hat,reps)\n## alpha quantile for standard normal\nq_norm = lapply(1:reps, function(j) qnorm(1-alpha_sig/2))\nCI_std = confidence_interval(theta_hat_test, q_norm, sigma_hat,reps)\n## Calculating the coverage ----\n### Coverage of true theta\ncoverage_true_pw = coverage(theta_true_test, CI, grids, reps)\n### Coverage of expected theta\ncoverage_expected = coverage(theta_hat_expected, CI, grids, reps)\n#T_stats[[as.character(n)]] = T_stat\n#coverages[[as.character(n)]] = coverage_\n## Calculation of uniform confidence bands ----\nuniform_T_stat = lapply(1:reps, function(j) T_stat_abs[[j]])\nuniform_T_max = lapply(1:reps, function(j) apply(uniform_T_stat[[j]], 1, max)) # max of t_stats\nuniform_CI = confidence_interval(theta_hat_test, uniform_q_star, sigma_hat, reps)\ncoverage_true_uniform = coverage_uniform(theta_true_test, uniform_CI, grids,reps)\n## calculation of asymptotic uniform coverage\nstd_T_stat = lapply(1:reps,function(k) sapply(1:b, function(j) abs(rnorm(grids))))\nstd_T_max = lapply(1:reps, function(j) apply(std_T_stat[[j]], 1, max))\nstd_q_star = lapply(1:reps, function(j) quantile(std_T_max[[j]], 1-alpha_sig)) # quantile of max_t_stat\nstd_CI = confidence_interval(theta_hat_test, std_q_star, sigma_hat, reps)\ncoverage_std_uniform = coverage_uniform(theta_true_test, std_CI, grids,reps)\ncoverages_widths_PW[[as.character(grids_x1)]] = coverage_true_pw\n#coverages_uniform_std[[as.character(theta_0)]] = coverage_std_uniform\n## calulating size\nsize_pw = 1-(coverage_true_pw/100)\nsize_std_uni = 1-(coverage_std_uniform/100)\nsize_bandwidth_uni[[as.character(node_size)]] = size_uni\n#}\n## MSE for a fixed test point\nmse = rowMeans((do.call(\"cbind\",theta_hat_test)-theta_true_test)^2)[1]\nprint(proc.time() - ptm)\n# removing undesirable variables ----\nrm(ptm)\n", "output_sequence": "Estimation of infeasable observations of theta_tilde, based on given theta (generally unknown) and effective weights (alpha) of (generalized) random forest."}, {"input_sequence": "#A real Scagnostics application on the weather in Semlach\n#Authors: Alina Kohlmayer & Michaela Kordasch\n\n# Packages needed -----------------------------------------------------------\ninstall.packages(\"rJava\")\ninstall.packages(\"RColorBrewer\")\n# Libraries -----------------------------------------------------------------\nlibrary(rJava)\nlibrary(lattice)\nlibrary(alphahull)\nlibrary(scagnostics)\nlibrary(tripack)\nlibrary(RColorBrewer)\n# Data ----------------------------------------------------------------------\ndf.data <- read.csv(\"C:/Users/User/Documents/UNI_Klagenfurt/MA_8Semester/Statistical Learning/Wetter-Scagnostics-master/Wetter_Semlach_20200402.csv\",sep=\",\",dec=\".\",header=TRUE,na.strings=\"\")\ndf.w <- df.data[,-1]\nstr(df.w)\ndf <- as.data.frame(cbind(df.w$temperature, df.w$windspeed))\ncolnames(df) <- c(\"temp\", \"windspeed\")\npng(\"temp_vs_windspeed.png\")\nplot(df, pch = 19, col = \"dark red\", xlab = \"temp\", ylab = \"windspeed\", main = \"temperature vs. windspeed\")\ndev.off()\n# Scatterplot ---------------------------------------------------------------\npng(\"scatterplot.png\")\nplot(df.w)\n# DT & Minimum Spanning Tree -----------------------------------------------------\n# DT\npng(\"DT_MST.png\")\npar(mfrow = c(1, 2))\ntdf <- tri.mesh(df[,1], df[,2], duplicate = \"remove\")\nplot(tdf, xlab = \"temperature\", ylab = \"windspeed\", main = \"Delaunay Triangulation\")\n# MST\ndistance <- dist(df)\nG <- graph.adjacency(as.matrix(distance), weighted = TRUE)\nmst <- minimum.spanning.tree(G)\nedgelist <- matrix(data = as.integer(igraph::get.edgelist(mst)), ncol = 2, byrow = F)\nplot(df, pch = 16, col = \"dark red\", main = \"Minimum Spanning Tree\")\nfor (i in 1:dim(edgelist)[1]){\nfrom = edgelist[i, 1]\nlines(df[c(from, to), 'temp'], df[c(from, to), 'windspeed'])\n}\npng(\"Edgedistribution.png\")\nhist(distance, main = \"Edge length distribution of MST\")\nboxplot(distance)\n# Characteristic c values --------------------------------------------------------\ns <- scagnostics(df.w)\ns\nsw <- scagnostics(df)\nsw\n# Plotting the SPLOM -----------------------------------------------------\nm.s <- as.matrix(s)\npng(\"SPLOM.png\")\npairs(m.s1, col = \"dark red\", main = \"Scagnostics SPLOM\")\npng(\"Heatmap.png\")\nheatmap.2(s, scale=\"column\",\nmain=\"Heatmap of Scagnostics of the Weather\",\ndensity=\"density\",\n#dendrogram = \"none\",\nnotecol=\"black\",\nmargins=c(10,5), cexRow=1, trace = \"none\")\n", "output_sequence": "Calculates the scagnostic measures for the dataset and plots the SPLOM, the scagnostics SPLOM and the heat-map of the scagnostic measures"}, {"input_sequence": "# make sure the package is installed install.packages('lattice')\nrequire(lattice)\ngraphics.off()\n# Plots sepal appears on the y-axis and petal on the x-axis for each flower the sum of width and height are plotted\n# Conditional Plots\nplot1 = xyplot(Sepal.Length + Sepal.Width ~ Petal.Width + Petal.Length | Species, data = iris, auto.key = list(columns = 2,\nlines = F, points = T), par.strip.text = list(cex = 0.75)) # size of the text in the plot\n# Grouped Plots\nplot2 = xyplot(Sepal.Length + Sepal.Width ~ Petal.Width + Petal.Length, groups = Species, data = iris, auto.key = list(columns = 3,\nlines = F, points = T), par.strip.text = list(cex = 0.75))\n# it is not possible to use layout or par to get multiple plots\nprint(plot1, position = c(0, 0, 1, 1), split = c(1, 1, 2, 1), more = TRUE)\n", "output_sequence": "The plots for a conditional or grouped variable differ, even if you use the same variable. The left plot has three panels for the types of Species and contains four different combinations of iris characteristics. This plot corresponds to Species as a conditioning variable. Species as a grouping variable produces the right plot. Here four panels are created, which differ by combinations of variables and types of species."}, {"input_sequence": "par(mfrow = c(1, 2))\n# create random sample which is t-distributed\nx = rt(100, df = 3)\n# quantiles of t with 3 degrees of freedom\nqqnorm(x, col = \"red\", pch = 20, asp = 1)\n# line of perfect fit\nqqline(x)\ny = rt(100, df = 100)\n# quantiles of t with 100 degrees of freedom\nqqnorm(y, col = \"red\", pch = 20, asp = 1)\nqqline(y)\n", "output_sequence": "QQ plots compare empirical quantiles of a distribution with theoretical quantiles of the standard normal distribution. If the degrees of freedom for the t-distribution tend to infinity, the QQ plot is a 45 degree line. Quantiles of the t-distribution and normal distribution are identical. The t-distribution converges to the normal distribution in infinite samples."}, {"input_sequence": "install.packages(\"HSAUR2\") # load package to get the data\ndata(\"voting\", package = \"HSAUR2\") # load the data\nlibrary(\"MASS\") # load package for nonmetric MDS\nfit = isoMDS(voting) # fit nonmetric MDS model\nplot(fit$points[, 1], fit$points[, 2], xlab = \"x\", ylab = \"y\", type = \"n\", main = \"\") # plot the model\nsegments(-10, -0, lty = \"dotted\") # horizontal line\n", "output_sequence": "Example of non-metric Multidimensional Scaling (MDS) on the data \"voting\" from package \"HSAUR2\". This approach is useful for a number of negative eigenvalues and ordinal data."}, {"input_sequence": "# load necessary packages\nrequire(datasets)\nrequire(grDevices)\n# define log-returns for the DAX and FTSE indeces\nr.dax = diff(log(EuStockMarkets[, 1]))\n# estimated log-returns for the DAX index for different bandwidths\nnp.reg.b1 = ksmooth(r.ftse, r.dax, kernel = \"box\", bandwidth = 0.1) # h = 0.1\n# plot for the regression results\npar(cex.axis = 1.5, cex.lab = 1.5, pch = 19, cex = 1) # graphical parameters\nplot(r.ftse, r.dax, xlim = c(-0.06, 0.06), ylim = c(-0.06, 0.06), xlab = \"FTSE log-returns\", ylab = \"DAX log-returns\",\ncol = rgb(0.1, 0.8, alpha = 0.7))\nlines(np.reg.b1, col = \"red\", lwd = 2) # regression line with h = 0.1\n", "output_sequence": "Multiple unform kernel regressions for DAX log-returns on FTSE log-returns. The bandwidth is alternated to see the effect of the bandwidth for the regression results. The smootheness of the regression line increases with a higher bandwidth. This causes a smaller bias but a higher variance."}, {"input_sequence": "par(mfrow = c(1, 2))\n# rv for the chi-squared distribution\nz = seq(0, 50, length = 300)\n# degrees of freedom\ndf = c(5, 10, 25)\n# pdf of chi-squared\nplot(z, dchisq(z, df[1]), type = \"l\", xlab = \"z\", ylab = \"pdf\")\n# pdf for df=10\nlines(z, dchisq(z, df[2]), col = \"red\")\n# pdf for df=15\nlines(z, dchisq(z, df[3]), col = \"green\")\n# pdf for df=25\nlines(z, dchisq(z, df[4]), col = \"blue\")\n# order of df as above cdf of chi-squared\nplot(z, pchisq(z, df[1]), type = \"l\", xlab = \"z\", ylab = \"cdf\")\nlines(z, pchisq(z, df[2]), col = \"red\")\n", "output_sequence": "The chi-square distribution describes the sum of independent squared standard normal random variables. Most commonly used in tests regarding the sample variance. The pdf is bell shaped, moves to the right and becomes symmetric for higher degrees of freedom."}, {"input_sequence": "require(stabledist)\npar(mfrow = c(1, 2))\nz = seq(-6, 6, length = 300)\n# mu and sigma are 1 and 0 Gaussian\nn = c(2, 0, 1, 0)\n# Cauchy\nch = c(1, 0, 1, 0)\n# Levy\nl = c(0.5, 0.9999, 1, 0)\n# Gaussian pdf\nplot(z, dstable(z, n[1], 1), col = \"red\", type = \"l\", xlim = c(-5, 5), ylim = c(0, 0.5), xlab = \"z\",\nylab = \"pdf\")\n# Cauchy pdf\nlines(z, dstable(z, ch[1], 1), col = \"green\")\n# Levy pdf\nlines(z, dstable(z, l[1], 1), col = \"blue\")\n# Gaussian cdf\nplot(z, pstable(z, n[1], 1), col = \"red\", type = \"l\", xlim = c(-5, 5), ylim = c(0, 1), xlab = \"z\",\nylab = \"cdf\")\n# Cauchy cdf\nlines(z, pstable(z, ch[1], 1), col = \"green\")\n# Levy cdf\n", "output_sequence": "The code below plots the pdfs and cdfs for three special cases of the stable distribution for which the pdf and cdf have a closed form expression. The special cases are the normal, Cauchy and Levy distribution. Normal distributions have a kurtosis of three and zero skewness. The Cauchy distribution is symmetric around its mean but has thicker tails than the normal distribution. Random variables with a Levy distribution have a skewness which tends to one. Only variables which are above the mean will be observed."}, {"input_sequence": "# make sure the package is installed install.packages('lattice')\nlibrary(lattice)\nlibrary(rgl)\ngraphics.off()\n# Four-dimensional plot levelplot\nlevelplot(yield ~ site * variety | year, data = barley, scales = list(alternating = T), shrink = c(0.3, 1), region = TRUE,\ncuts = 20, col.regions = topo.colors(100), par.settings = list(axis.text = list(cex = 0.5)), par.strip.text = list(cex = 0.7),\n", "output_sequence": "The datset barley contains yield data from Minnesota. Average yield is plotted depnding on where it is planted (site) and which variety is used. The third conditioning variable is time creating two panels. Higher average yield is associated with greater and lighter rectangles. Therefore four variables are illustrated in one graph (yield, time, variety, site)."}, {"input_sequence": "#logistic regression\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import preprocessing\nfrom sklearn.metrics import accuracy_score as acc_rate\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import load_digits\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport sklearn\nimport plotly as px\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score, classification_report, roc_curve,precision_recall_curve, auc,confusion_matrix\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import plot_confusion_matrix\ndata = pd.read_csv(\"/Users/leonoretherry/Documents/St Gallen M2/smart data analytics/heart_failure_clinical_records_dataset.csv\")\nx = data.loc[:,:\"time\"]\ny = data.loc[:,[\"DEATH_EVENT\"]]\n#let's have an overview of our data set\n#Number and percentage of participants suffering from heart diseases in the study that died\ndata['DEATH_EVENT'].value_counts()\ndata['DEATH_EVENT'].value_counts(normalize=True)*100\ndata.isnull().sum()\n#overview of our explicative variables\nhist = data.hist(figsize=(10,9))\n#Let's remove the time component of our data set as it is not very representative for explaining the death event as it corresponds to the time period where the patient were followed by the study\ndata2 = data.drop(['time'], axis = 1)\nx2 = data.loc[:,:\"smoking\"]\ny2 = data.loc[:,[\"DEATH_EVENT\"]]\nx2_train, x2_test, y2_train, y2_test = train_test_split(x2, y2, test_size = 0.33, random_state = 2)\nplt.figure(figsize=(15,8))\nsns.heatmap(data2.corr(), annot=True)\n#time, ejection and serum creatinine are the more correlated variables with the death event\n#A lower level of ejection fracion increase the chance to die while a higher level of serum creatinine increase the chance to die\n#logistic regression\nscaler = preprocessing.StandardScaler().fit(x2_train)\nx2_train_scaled = scaler.transform(x2_train)\nmodel = LogisticRegression()\nmodel.fit(x2_train_scaled, y2_train)\nimport statsmodels.api as sm\nlogit_model=sm.Logit(y2,x2)\nresult=logit_model.fit()\nprint(result.summary())\nfrom sklearn import tree\n#we kept max_depth=2 as the Accuracy rate was better with 2 than 3 or 4\nclf2 = tree.DecisionTreeClassifier(criterion='entropy',\nmax_depth=2)\nclf2.fit(X=x2_train, y=y2_train)\n# make prediction\nprint('\\nThe target test data set is:\\n', y2_test)\nprint('\\nThe predicted result is:\\n', clf2.predict(x2_test))\nprint('\\nAccuracy rate is:\\n', acc_rate(y2_test, clf2.predict(x2_test)))\nplt.figure(figsize=(25,10))\nc=plot_tree(clf2,\nfeature_names= x2.columns,\nclass_names=['0','1'],\nfilled=True,\nfontsize=14)\ny2_pred=clf2.predict(x2_test)\nprint(confusion_matrix(y2_test,y2_pred))\ntarget_names=['class 0', 'class 1']\nprint(classification_report(y2_test, y2_pred, target_names=target_names))\n#Distribution of death event according to gender\nlen_data = len(data)\nlen_w = len(data[data[\"sex\"]==0])\nlen_m = len_data - len_w\nmen_died = len(data.loc[(data[\"DEATH_EVENT\"]==1) &(data['sex']==0)])\nmen_survived = len_m - men_died\nwomen_died = len(data.loc[(data[\"DEATH_EVENT\"]==1) & (data['sex']==1)])\nwomen_survived = len_w - women_died\nlabels = ['Men died','Men survived','Women survived']\nfig = go.Figure(data=[go.Pie(labels=labels, values=values,textinfo='label+percent',hole=0.4)])\nfig.update_layout(\ntitle_text=\"Distribution of DEATH EVENT according to their gender\")\nfig.show()\n# Age distribution plot\nfg=sns.FacetGrid(data, hue=\"DEATH_EVENT\", height=6,)\nfg.map(sns.kdeplot, \"age\",shade=True).add_legend(labels=[\"Alive\",\"Not alive\"])\nplt.title('Age Distribution Plot');\n# Death event as per diabetes\npd.crosstab(data.diabetes ,data.DEATH_EVENT).plot(kind='bar')\nplt.legend(title='DEATH_EVENT', loc='upper right', labels=['No death event', 'Death event'])\nplt.title('Death Event as per diabetes ')\nplt.xlabel('diabetes ')\n# Death event as per high pressure blood\npd.crosstab(data.high_blood_pressure ,data.DEATH_EVENT).plot(kind='bar')\nplt.legend(title='DEATH_EVENT', loc='upper right', labels=['Not alive', 'Alive'])\nplt.title('Death Event as per High pressure blood ')\nplt.xlabel('High pressure blood ')\n#Death event as per smokers\npd.crosstab(data.smoking ,data.DEATH_EVENT).plot(kind='bar')\nplt.title('Death Event as per smokers ')\nplt.xlabel('Smokers ')\n#Distribution of diabetics according to their gender\nmen_with_diabetes = len(data.loc[(data[\"diabetes\"]==1) & (data['sex']==1)])\nmen_without_diabetes = len_m - men_with_diabetes\nwomen_with_diabetes = len(data.loc[(data[\"diabetes\"]==1) & (data['sex']==0)])\nwomen_without_diabetes = len_w - women_with_diabetes\nlabels = ['M_diabetes','M_no_diabete','W_diabete','W_no_diabete']\nvalues = [men_with_diabetes,\ntitle_text=\"Distribution of No/diabetics according to their gender. (M for Men, W for Women)\")\n#Feature Selection according to their importance\nx = data.copy()\ny = x.loc[:,[\"DEATH_EVENT\"]]\nx = x.drop(columns=['time','DEATH_EVENT'])\nfeatures_names = x.columns\nfrom sklearn.datasets import make_classification\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.decomposition import PCA\nforest = ExtraTreesClassifier(n_estimators=250,\nrandom_state=0)\nforest.fit(x, y)\nimportances = forest.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in forest.estimators_],\naxis=0)\nindices = np.argsort(importances)[::-1]\n# Print the feature ranking\nprint(\"Feature ranking:\")\nfor f in range(x.shape[1]):\nplt.figure()\nplt.title(\"Feature importances\")\nsns.barplot(x=features_names[indices].to_numpy(), y=importances[indices], palette=\"deep\",yerr=std[indices])\nplt.xticks(range(x.shape[1]), features_names[indices].to_numpy(),rotation=80)\nplt.xlim([-1, x.shape[1]])\ndef plot_cm(cm,title):\nz = cm\nx = ['No death Event', 'Death Event']\ny = x\n# change each element of z to type string for annotations\nz_text = [[str(y) for y in x] for x in z]\n# set up figure\nfig = ff.create_annotated_heatmap(z, x=x, y=y, annotation_text=z_text, colorscale='deep')\n# add title\nfig.update_layout(title_text='<i><b>Confusion matrix {}</b></i>'.format(title),\n#xaxis = dict(title='x'),\n)\n# add custom xaxis title\nfig.add_annotation(dict(font=dict(color=\"black\",size=14),\nx=0.5,\nshowarrow=False,\ntext=\"Predicted value\",\n# add custom yaxis title\nx=-0.15,\ntext=\"Real value\",\n# adjust margins to make room for yaxis title\nfig.update_layout(margin=dict(t=50, l=20),width=750,height=750)\n# add colorbar\nfig['data'][0]['showscale'] = True\n# Testing our models' accuracy rate\nmodels= [['Logistic Regression ',LogisticRegression()],\n['KNearest Neighbor ',KNeighborsClassifier()],\n['Decision Tree Classifier ',DecisionTreeClassifier()],\n['SVM ',SVC()]]\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.33, random_state = 2)\nmodels_score = []\nfor (name,model) in models:\nmodel = model\nmodel.fit(x_train,y_train)\nmodel_pred = model.predict(x_test)\ncm_model = confusion_matrix(y_test, model_pred)\nmodels_score.append(accuracy_score(y_test,model.predict(x_test)))\nprint(name)\nprint('Validation Acuuracy: ',accuracy_score(y_test,model.predict(x_test)))\nprint('############################################')\n", "output_sequence": "This project aims to predict the death of patients suffering from heart disease. In this way, it might be possible to adapt the treatments and maybe avoid heart failures in some cases."}, {"input_sequence": "#!/usr/bin/env python\n# coding: utf-8\n# In[1]:\n#logistic regression\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import preprocessing\nfrom sklearn.metrics import accuracy_score as acc_rate\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import load_digits\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport sklearn\nimport plotly as px\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score, classification_report, roc_curve,precision_recall_curve, auc,confusion_matrix\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import plot_confusion_matrix\ndata = pd.read_csv(\"/Users/leonoretherry/Documents/St Gallen M2/smart data analytics/heart_failure_clinical_records_dataset.csv\")\nx = data.loc[:,:\"time\"]\ny = data.loc[:,[\"DEATH_EVENT\"]]\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.33, random_state = 2)\n# In[2]:\n#let's have an overview of our data set\ndata.head()\n# In[3]:\n#Number and percentage of participants suffering from heart diseases in the study that died\ndata['DEATH_EVENT'].value_counts()\ndata['DEATH_EVENT'].value_counts(normalize=True)*100\n#67% of the patient followed during the study did not die while 32 did\n# In[4]:\ndata.isnull().sum()\n#there are no missing values in our data set\n# In[5]:\n#overview of our explicative variables\nhist = data.hist(figsize=(10,9))\nplt.savefig(\"pandas_hist_01.png\", bbox_inches='tight', dpi=100)\n# In[6]:\n#Let's remove the time component of our data set as it is not very representative for explaining the death event as it corresponds to the time period where the patient were followed by the study\ndata2 = data.drop(['time'], axis = 1)\nx2 = data.loc[:,:\"smoking\"]\ny2 = data.loc[:,[\"DEATH_EVENT\"]]\nx2_train, x2_test, y2_train, y2_test = train_test_split(x2, y2, test_size = 0.33, random_state = 2)\nplt.figure(figsize=(15,8))\nsns.heatmap(data2.corr(), annot=True)\n#time, ejection and serum creatinine are the more correlated variables with the death event\n#A lower level of ejection fracion increase the chance to die while a higher level of serum creatinine increase the chance to die\n#the time is negatively correlated with the death. It can be explained by the fact that the less time the patient has been followed by the study, the less chances he would have die. However it is possible that the patient die just after the end of the study\n# In[7]:\nscaler = preprocessing.StandardScaler().fit(x2_train)\nx2_train_scaled = scaler.transform(x2_train)\nmodel = LogisticRegression()\nmodel.fit(x2_train_scaled, y2_train)\nimport statsmodels.api as sm\nlogit_model=sm.Logit(y2,x2)\nresult=logit_model.fit()\nprint(result.summary())\n#time, ejection and serum creatinine are once again the variables that have the greatest impact\n# In[8]:\nfrom sklearn import tree\n#we kept max_depth=2 as the Accuracy rate was better with 2 than 3 or 4\nclf2 = tree.DecisionTreeClassifier(criterion='entropy',\nmax_depth=2)\nclf2.fit(X=x2_train, y=y2_train)\n# make prediction\nprint('\\nThe target test data set is:\\n', y2_test)\nprint('\\nThe predicted result is:\\n', clf2.predict(x2_test))\nprint('\\nAccuracy rate is:\\n', acc_rate(y2_test, clf2.predict(x2_test)))\nplt.figure(figsize=(25,10))\nc=plot_tree(clf2,\nfeature_names= x2.columns,\nclass_names=['0','1'],\nfilled=True,\nfontsize=14)\nplt.savefig(\"decisiontreeOptimizedwithouttime.png\")\n# In[9]:\ny2_pred=clf2.predict(x2_test)\nprint(confusion_matrix(y2_test,y2_pred))\ntarget_names=['class 0', 'class 1']\nprint(classification_report(y2_test, y2_pred, target_names=target_names))\n#the model better predict when the patient is going to live rather going to die\n# In[ ]:\n#Distribution of death event according to gender\nlen_data = len(data)\nlen_w = len(data[data[\"sex\"]==0])\nlen_m = len_data - len_w\nmen_died = len(data.loc[(data[\"DEATH_EVENT\"]==1) &(data['sex']==0)])\nmen_survived = len_m - men_died\nwomen_died = len(data.loc[(data[\"DEATH_EVENT\"]==1) & (data['sex']==1)])\nwomen_survived = len_w - women_died\nlabels = ['Men died','Men survived','Women survived']\nfig = go.Figure(data=[go.Pie(labels=labels, values=values,textinfo='label+percent',hole=0.4)])\nfig.update_layout(\ntitle_text=\"Distribution of DEATH EVENT according to their gender\")\nfig.show()\n# In[11]:\n# Age distribution plot\nfg=sns.FacetGrid(data, hue=\"DEATH_EVENT\", height=6,)\nfg.map(sns.kdeplot, \"age\",shade=True).add_legend(labels=[\"Alive\",\"Not alive\"])\nplt.title('Age Distribution Plot');\nplt.show()\n# In[12]:\n# Death event as per diabetes\npd.crosstab(data.diabetes ,data.DEATH_EVENT).plot(kind='bar')\nplt.legend(title='DEATH_EVENT', loc='upper right', labels=['No death event', 'Death event'])\nplt.title('Death Event as per diabetes ')\nplt.xlabel('diabetes ')\n# In[13]:\n# Death event as per high pressure blood\npd.crosstab(data.high_blood_pressure ,data.DEATH_EVENT).plot(kind='bar')\nplt.legend(title='DEATH_EVENT', loc='upper right', labels=['Not alive', 'Alive'])\nplt.title('Death Event as per High pressure blood ')\nplt.xlabel('High pressure blood ')\n# In[14]:\n#Death event as per smokers\npd.crosstab(data.smoking ,data.DEATH_EVENT).plot(kind='bar')\nplt.title('Death Event as per smokers ')\nplt.xlabel('Smokers ')\n# In[15]:\n#Distribution of diabetics according to their gender\nmen_with_diabetes = len(data.loc[(data[\"diabetes\"]==1) & (data['sex']==1)])\nmen_without_diabetes = len_m - men_with_diabetes\nwomen_with_diabetes = len(data.loc[(data[\"diabetes\"]==1) & (data['sex']==0)])\nwomen_without_diabetes = len_w - women_with_diabetes\nlabels = ['M_diabetes','M_no_diabete','W_diabete','W_no_diabete']\nvalues = [men_with_diabetes,\ntitle_text=\"Distribution of No/diabetics according to their gender. (M for Men, W for Women)\")\n# In[16]:\n#Feature Selection according to their importance\nx = data.copy()\ny = x.loc[:,[\"DEATH_EVENT\"]]\nx = x.drop(columns=['time','DEATH_EVENT'])\nfeatures_names = x.columns\nfrom sklearn.datasets import make_classification\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.decomposition import PCA\nforest = ExtraTreesClassifier(n_estimators=250,\nrandom_state=0)\nforest.fit(x, y)\nimportances = forest.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in forest.estimators_],\naxis=0)\nindices = np.argsort(importances)[::-1]\n# Print the feature ranking\nprint(\"Feature ranking:\")\nfor f in range(x.shape[1]):\nprint(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n# In[17]:\nplt.figure()\nplt.title(\"Feature importances\")\nsns.barplot(x=features_names[indices].to_numpy(), y=importances[indices], palette=\"deep\",yerr=std[indices])\nplt.xticks(range(x.shape[1]), features_names[indices].to_numpy(),rotation=80)\nplt.xlim([-1, x.shape[1]])\n# In[18]:\ndef plot_cm(cm,title):\nz = cm\nx = ['No death Event', 'Death Event']\ny = x\n# change each element of z to type string for annotations\nz_text = [[str(y) for y in x] for x in z]\n# set up figure\nfig = ff.create_annotated_heatmap(z, x=x, y=y, annotation_text=z_text, colorscale='deep')\n# add title\nfig.update_layout(title_text='<i><b>Confusion matrix {}</b></i>'.format(title),\n#xaxis = dict(title='x'),\n)\n# add custom xaxis title\nfig.add_annotation(dict(font=dict(color=\"black\",size=14),\nx=0.5,\nshowarrow=False,\ntext=\"Predicted value\",\n# add custom yaxis title\nx=-0.15,\ntext=\"Real value\",\n# adjust margins to make room for yaxis title\nfig.update_layout(margin=dict(t=50, l=20),width=750,height=750)\n# add colorbar\nfig['data'][0]['showscale'] = True\nfig.show()\n# In[19]:\n# Testing our models' accuracy rate\nmodels= [['Logistic Regression ',LogisticRegression()],\n['KNearest Neighbor ',KNeighborsClassifier()],\n['Decision Tree Classifier ',DecisionTreeClassifier()],\n['SVM ',SVC()]]\nmodels_score = []\nfor (name,model) in models:\nmodel = model\nmodel.fit(x_train,y_train)\nmodel_pred = model.predict(x_test)\ncm_model = confusion_matrix(y_test, model_pred)\nmodels_score.append(accuracy_score(y_test,model.predict(x_test)))\nprint(name)\nprint('Validation Acuuracy: ',accuracy_score(y_test,model.predict(x_test)))\nprint('############################################')\nplot_cm(cm_model,title=name+\"model\")\n", "output_sequence": "This project aims to predict the death of patients suffering from heart disease. In this way, it might be possible to adapt the treatments and maybe avoid heart failures in some cases."}, {"input_sequence": "# Load packages\ninstall.packages(\"sf\")\nlibrary(sf)\ninstall.packages(\"tidyverse\")\nlibrary(tidyverse)\nlibrary(units)\nlibrary(jcolors)\noptions(scipen=999)\n# Import data\nmodel <- read.csv(\"/Users/ivankotik/Documents/DEDA_project/files and graphs/data after modelling.csv\")\n### Data cleaning\n# Deleting an empty row\ngeocoding <- geocoding[-1, ]\n# Changing column names\ngeocoding <- rename(geocoding, \"fulladress\" = \"query\")\n# Get the full adress\nmodel$fulladress <- paste(model$regio1, model$geo_plz, model$streetPlain, model$houseNumber)\n# Deleting a useless column\nmodel$X <- NULL\n# Joining the tables\nleft_join(model, geocoding, by = \"fulladress\") -> apartments\ndistinct(apartments) -> apartments\n# Leaving only correct coordinates\napartments[is.na(apartments[, 32]) == FALSE, ] -> apartments_clean\n# Creating the SF object out of\nst_as_sf(apartments_clean, coords = c(\"lon\", \"lat\"), crs = \"WGS84\") -> apartments_clean_points\nrm(apartments_clean)\n### Calaulating distance polygons\n# Calculating the distance\napartments_clean_points %>% st_distance((polygon %>% filter(group == \"catering\"))) %>%\n+ set_units(10, m) %>% `^`(-1) %>% apply(MARGIN = 1, FUN = sum) -> apartments_clean_points$catering_p\napartments_clean_points %>% st_distance((polygon %>% filter(group == \"activities\"))) %>%\n+ set_units(10, m) %>% `^`(-1) %>% apply(MARGIN = 1, FUN = sum) -> apartments_clean_points$activities_p\napartments_clean_points %>% st_distance((polygon %>% filter(group == \"destinations\"))) %>%\n+ set_units(10, m) %>% `^`(-1) %>% apply(MARGIN = 1, FUN = sum) -> apartments_clean_points$destinations_p\napartments_clean_points %>% st_distance((polygon %>% filter(group == \"entertainment\"))) %>%\n+ set_units(10, m) %>% `^`(-1) %>% apply(MARGIN = 1, FUN = sum) -> apartments_clean_points$entertainment_p\napartments_clean_points %>% st_distance((polygon %>% filter(group == \"health\"))) %>%\n+ set_units(10, m) %>% `^`(-1) %>% apply(MARGIN = 1, FUN = sum) -> apartments_clean_points$health_p\napartments_clean_points %>% st_distance((polygon %>% filter(group == \"kids\"))) %>%\n+ set_units(10, m) %>% `^`(-1) %>% apply(MARGIN = 1, FUN = sum) -> apartments_clean_points$kids_p\napartments_clean_points %>% st_distance((polygon %>% filter(group == \"shopping\"))) %>%\n+ set_units(10, m) %>% `^`(-1) %>% apply(MARGIN = 1, FUN = sum) -> apartments_clean_points$shopping_p\napartments_clean_points %>% st_distance((polygon %>% filter(group == \"transport\"))) %>%\n+ set_units(10, m) %>% `^`(-1) %>% apply(MARGIN = 1, FUN = sum) -> apartments_clean_points$transport_p\n### Calaulating distance multipolygons\n# Calcualte the distance between the multipolygon and points for all the groups\napartments_clean_points %>% st_distance((multipolygon %>% filter(group == \"activities\"))) %>%\n+ set_units(10, m) %>% `^`(-1) %>% apply(MARGIN = 1, FUN = sum) -> apartments_clean_points$activities_m\napartments_clean_points %>% st_distance((multipolygon %>% filter(group == \"catering\"))) %>%\n+ set_units(10, m) %>% `^`(-1) %>% apply(MARGIN = 1, FUN = sum) -> apartments_clean_points$catering_m\napartments_clean_points %>% st_distance((multipolygon %>% filter(group == \"destinations\"))) %>%\n+ set_units(10, m) %>% `^`(-1) %>% apply(MARGIN = 1, FUN = sum) -> apartments_clean_points$destinations_m\napartments_clean_points %>% st_distance((multipolygon %>% filter(group == \"entertainment\"))) %>%\n+ set_units(10, m) %>% `^`(-1) %>% apply(MARGIN = 1, FUN = sum) -> apartments_clean_points$entertainment_m\napartments_clean_points %>% st_distance((multipolygon %>% filter(group == \"health\"))) %>%\n+ set_units(10, m) %>% `^`(-1) %>% apply(MARGIN = 1, FUN = sum) -> apartments_clean_points$health_m\napartments_clean_points %>% st_distance((multipolygon %>% filter(group == \"kids\"))) %>%\n+ set_units(10, m) %>% `^`(-1) %>% apply(MARGIN = 1, FUN = sum) -> apartments_clean_points$kids_m\napartments_clean_points %>% st_distance((multipolygon %>% filter(group == \"shopping\"))) %>%\n+ set_units(10, m) %>% `^`(-1) %>% apply(MARGIN = 1, FUN = sum) -> apartments_clean_points$shopping_m\napartments_clean_points %>% st_distance((multipolygon %>% filter(group == \"park\"))) %>%\n+ set_units(10, m) %>% `^`(-1) %>% apply(MARGIN = 1, FUN = sum) -> apartments_clean_points$park_m\napartments_clean_points %>% st_distance((multipolygon %>% filter(group == \"water\"))) %>%\n+ set_units(10, m) %>% `^`(-1) %>% apply(MARGIN = 1, FUN = sum) -> apartments_clean_points$water_m\n# Combining the distances\napartments_clean_points$activities <- apartments_clean_points$activities_m + apartments_clean_points$activities_p\napartments_clean_points$transport <- apartments_clean_points$transport_p\n# Normalizing the data\napartments_clean_points$norm_activities <- (apartments_clean_points$activities - min(apartments_clean_points$activities))/\n(max(apartments_clean_points$catering) - min(apartments_clean_points$catering))\napartments_clean_points$norm_destinations <- (apartments_clean_points$destinations - min(apartments_clean_points$destinations))/\n(max(apartments_clean_points$entertainment) - min(apartments_clean_points$entertainment))\napartments_clean_points$norm_health <- (apartments_clean_points$health - min(apartments_clean_points$health))/\n(max(apartments_clean_points$kids) - min(apartments_clean_points$kids))\napartments_clean_points$norm_shopping <- (apartments_clean_points$shopping - min(apartments_clean_points$shopping))/\n(max(apartments_clean_points$transport) - min(apartments_clean_points$transport))\napartments_clean_points$norm_park <- (apartments_clean_points$park - min(apartments_clean_points$park))/\n(max(apartments_clean_points$water) - min(apartments_clean_points$water))\n# Total score calculation\napartments_clean_points$norm_total <- apartments_clean_points$norm_activities + apartments_clean_points$norm_catering +\napartments_clean_points$norm_park + apartments_clean_points$norm_water\n# Total score re-normalized\napartments_clean_points$norm_total_score <- (apartments_clean_points$norm_total - min(apartments_clean_points$norm_total))/\n### The landlord premium model\n# Calcualter the premium\napartments_clean_points$premium <- apartments_clean_points$price - (apartments_clean_points$norm_total_score *\napartments_clean_points$model2_2)\nb_shape <- st_union(berlin)\n# Adding up the scores\n### PLOTS\n# Plotting the results\n{\nggplot()+\ngeom_sf(data = berlin, alpha = 0.6)+\ngeom_sf(data = apartments_clean_points, aes(color = norm_total_score), size = 3, alpha = 0.8)+\nscale_color_jcolors_contin(\"pal4\")+\nlabs(title = \" TOTAL WEIGHT\", color = \"WEIGHT, w_i\")+\n{theme(\npanel.background = element_rect(fill = \"#222222\",\ncolour = \"#222222\",\nsize = 0.1, linetype = \"solid\"),\npanel.grid.major = element_line(size = 0.1, linetype = 'solid',\ncolour = \"white\"),\npanel.grid.minor = element_line(size = 0.1, linetype = 'solid',\ncolour = \"#222222\"),\nplot.background = element_rect(fill = \"#222222\"),\nlegend.title = element_text(colour = \"#cacaca\"),\n}\nggsave(\"weight_total.png\", dpi = 320, scale = 1)\ngeom_sf(data = apartments_clean_points, aes(color = catering), size = 3, alpha = 0.8)+\nlabs(title = \"TOTAL SCORE CATERING\", color = \"SCORE, s_i\")+\nggsave(\"score_catering.png\", dpi = 320, scale = 1)\ngeom_sf(data = apartments_clean_points, aes(color = activities), size = 3, alpha = 0.8)+\nlabs(title = \"TOTAL SCORE ACTIVITIES\", color = \"SCORE, s_i\")+\nggsave(\"score_activities.png\", dpi = 320, scale = 1)\ngeom_sf(data = berlin)+\ngeom_sf(data = apartments_clean_points, aes(color = destinations), size = 3, alpha = 0.8)+\nlabs(title = \"TOTAL SCORE DESTINATIONS\", color = \"SCORE, s_i\")+\nggsave(\"score_destinations.png\", dpi = 320, scale = 1)\ngeom_sf(data = apartments_clean_points, aes(color = entertainment), size = 3, alpha = 0.8)+\nlabs(title = \"TOTAL SCORE ENTERTAINMENT\", color = \"SCORE, s_i\")+\nggsave(\"score_entertainment.png\", dpi = 320, scale = 1)\ngeom_sf(data = apartments_clean_points, aes(color = health), size = 3, alpha = 0.8)+\nlabs(title = \"TOTAL SCORE HEALTH\", color = \"SCORE, s_i\")+\nggsave(\"score_health.png\", dpi = 320, scale = 1)\ngeom_sf(data = apartments_clean_points, aes(color = kids), size = 3, alpha = 0.8)+\nlabs(title = \"TOTAL SCORE KIDS\", color = \"SCORE, s_i\")+\nggsave(\"score_kids.png\", dpi = 320, scale = 1)\ngeom_sf(data = apartments_clean_points, aes(color = shopping), size = 3, alpha = 0.8)+\nlabs(title = \"TOTAL SCORE SHOPPING\", color = \"SCORE, s_i\")+\nggsave(\"score_shopping.png\", dpi = 320, scale = 1)\ngeom_sf(data = apartments_clean_points, aes(color = transport), size = 3, alpha = 0.8)+\nlabs(title = \"TOTAL SCORE TRANSPORT\", color = \"SCORE, s_i\")+\nggsave(\"score_transport.png\", dpi = 320, scale = 1)\n}\ngeom_sf(data = apartments_clean_points, aes(color = park), size = 3, alpha = 0.8)+\nlabs(title = \"TOTAL SCORE PARKS\", color = \"SCORE, s_i\")+\nggsave(\"score_parks.png\", dpi = 320, scale = 1)\ngeom_sf(data = apartments_clean_points, aes(color = water), size = 3, alpha = 0.8)+\nlabs(title = \"TOTAL SCORE WATER\", color = \"SCORE, s_i\")+\nggsave(\"score_water.png\", dpi = 320, scale = 1)\n# Unfulfilled multipolygon idea\ngeom_sf(data = multipolygon)+\ngeom_sf(data = st_buffer(apartments_clean_points[1:10,], set_units(2000, m)), color = \"red\", alpha = 0.5)+\nggsave(\"buffers.png\", dpi = 320, scale = 1)\n# Premiums visualized\ngeom_sf(data = berlin, fill = \"white\", alpha = 0.5)+\ngeom_sf(data = apartments_clean_points, aes(color = premium), size = 3, alpha = 0.8)+\nlabs(title = \"LANDLORD PREMIUM\", color = \"PREMIUM\")+\nggsave(\"Premiums.png\", dpi = 320, scale = 1)\n# Distribution of premium\nsummary(apartments_clean_points$premium)\ngeom_density(data = apartments_clean_points, aes(x = premium), color = \"white\")+\nannotate(geom=\"text\", x=350, y=0.0012, label=\"mean = -71.71371\", color=\"white\")+\n{theme(\n", "output_sequence": "Spatial analysis of Berlin rent prices and landlord premiums"}, {"input_sequence": "# Using the previously collected data\ndata_berlin %>% select(price, heatingType, noRooms, hasKitchen, extrawc, balcony, typeOfFlat, garden, cellar,\nyearConstructed, newlyConst, floor, numberOfFloors, lift) -> forest_data\nforest_data %>% filter(yearConstructed != 0) -> forest_data\n# installing packages\ninstall.packages(\"randomForest\")\nlibrary(randomForest)\ninstall.packages(\"caTools\")\nlibrary(caTools)\n# checking for NA's\nsum(is.na(forest_data))\n# getting rid of the NA's\nfor(i in 1:14){\nforest_data[, i] <- replace_na(forest_data[, i], 0)\n}\n# factoring the character columns\n# getting rid of the factors \"0\" (NA's)\nforest_data %>% filter(typeOfFlat != 0) -> forest_data\nlevels(factor(forest_data$typeOfFlat))\nforest_data$typeOfFlat <- factor(forest_data$typeOfFlat)\nlevels(factor(forest_data$heatingType))\nforest_data$heatingType <- factor(forest_data$heatingType)\nsummary(forest_data)\n# sampelling for training\nsample <- sample.split(forest_data$lift, SplitRatio = .75)\ntrain <- subset(forest_data, sample == TRUE)\ndim(train)\nrf <- randomForest(price ~ ., data = train)\nrf\npred <- predict(rf, newdata=test[-1])\npred\ncm <- table(test[,1], pred)\ncm\n", "output_sequence": "Spatial analysis of Berlin rent prices and landlord premiums"}, {"input_sequence": "# installing the libraries\ninstall.packages(\"sf\")\nlibrary(sf)\ninstall.packages(\"tidyverse\")\nlibrary(tidyverse)\ninstall.packages(\"maptiles\")\nlibrary(maptiles)\ninstall.packages(\"cartography\")\nlibrary(cartography)\ninstall.packages(\"geogrid\")\nlibrary(geogrid)\ninstall.packages(\"tmap\")\nlibrary(tmap)\n# loading all the shape files\nlist.files(\"C://Users//ivkot//Downloads//shape_files\")\n# the points of interest multipolygons\na_ber_poi_multipolygon <- read_sf(dsn = \"C://Users//ivkot//Downloads//shape_files//gis_osm_pois_a_free_1.shp\")\n# for mac\na_ber_poi_multipolygon <- read_sf(dsn = \"/Users/ivankotik/Documents/shape_files/gis_osm_pois_a_free_1.shp\")\n# the points of interest polygons\na_ber_poi_polygon <- read_sf(dsn = \"C://Users//ivkot//Downloads//shape_files//gis_osm_pois_free_1.shp\")\n# the landuse(parks) polygons\nb_ber_landuse_multipolygon <- read_sf(dsn = \"C://Users//ivkot//Downloads//shape_files//gis_osm_landuse_a_free_1.shp\")\n# the transport polygons\nc_ber_transport_polygon <- read_sf(dsn = \"C://Users//ivkot//Downloads//shape_files//gis_osm_transport_free_1.shp\")\n# the water objects multipolygons\nd_ber_water_multipolygons <- read_sf(dsn = \"C://Users//ivkot//Downloads//shape_files//gis_osm_water_a_free_1.shp\")\n# the Berlin city map\ne_ber_map <- read_sf(dsn = \"C://Users//ivkot//Downloads//shape_files//gis_osm_places_a_free_1.shp\")\nberlin_countour <- filter(e_ber_map, fclass == \"suburb\")\nberlin_countour <- berlin_countour[, 5:6]\nrm(e_ber_map)\n# A documentation of the layers in this shape file is available here:\n# http://download.geofabrik.de/osm-data-in-gis-formats-free.pdf\n# cleaning data\n# the a_ files\n# picking out the right \"codes\" for items: 21xx, 27(2+)x\na_ber_poi_polygon <- a_ber_poi_polygon %>% filter(((code >= 2100) & (code < 2400))|((code >= 2500) & (code < 2600))|((code >= 2720) & (code < 2800)))\n# the b_ files\n# picking out the right \"codes\" for items: 7201, 7202\nb_ber_landuse_multipolygon <- b_ber_landuse_multipolygon %>% filter((code == 7201) | (code == 7202))\n# the c_ files\n# picking out the right \"codes\" for items: 5601, 5622\nc_ber_transport_polygon <- c_ber_transport_polygon %>% filter((code == 5601) | (code == 5602) | (code == 5603) | (code == 5621) | (code == 5622))\nd_ber_water_multipolygons <- d_ber_water_multipolygons %>% filter((code == 8200) | (code == 8202))\n# subgrouping the df's\n# poi polygon\na_ber_poi_polygon$group <- NA\na_ber_poi_polygon[((a_ber_poi_polygon$code == 2201) | (a_ber_poi_polygon$code == 2202) | (a_ber_poi_polygon$code == 2203)), 6] <- \"entertainment\"\na_ber_poi_polygon[(a_ber_poi_polygon$code == 2204), 6] <- \"park\"\na_ber_poi_polygon[((a_ber_poi_polygon$code > 2205) & (a_ber_poi_polygon$code < 2300)), 6] <- \"activities\"\ntable(a_ber_poi_polygon$group)\n# poi multipolygon\na_ber_poi_multipolygon$group <- NA\na_ber_poi_multipolygon[((a_ber_poi_multipolygon$code == 2201) | (a_ber_poi_multipolygon$code == 2202) | (a_ber_poi_multipolygon$code == 2203)), 6] <- \"entertainment\"\na_ber_poi_multipolygon[(a_ber_poi_multipolygon$code == 2204), 6] <- \"park\"\na_ber_poi_multipolygon[((a_ber_poi_multipolygon$code > 2205) & (a_ber_poi_multipolygon$code < 2300)), 6] <- \"activities\"\ntable(a_ber_poi_multipolygon$group)\n# landuse multipolygons\ntable(b_ber_landuse_multipolygon$fclass)\nb_ber_landuse_multipolygon$group <- \"park\"\n# transport polygons\ntable(c_ber_transport_polygon$fclass)\nc_ber_transport_polygon$group <- \"transport\"\n# water multipolygons\ntable(d_ber_water_multipolygons$fclass)\nd_ber_water_multipolygons$group <- \"water\"\n# creating an empty dataframe\nberlin_counter <- data.frame(matrix(ncol = 0, nrow = nrow(berlin_countour)))\nberlin_counter$bezirk <- berlin_countour$name\n# checking the groups and assigning the count-value\n# poi multipolygon counts (parks by area, rest by intersection)\nberlin_counter$activities <- sapply(st_intersects(berlin_countour, filter(a_ber_poi_multipolygon, group == \"activities\")), length)\ntest <- data.frame(bezirk = st_intersection(filter(a_ber_poi_multipolygon, group == \"park\"), berlin_countour)$name.1,\ntest2 <- aggregate(test$area, by = list(test$bezirk), FUN = sum)\ntest2 <- rename(test2, bezirk = Group.1)\nberlin_counter <- left_join(berlin_counter, test2, by = \"bezirk\")\nberlin_counter$shopping <- sapply(st_intersects(berlin_countour, filter(a_ber_poi_multipolygon, group == \"shopping\")), length)\n# poi polygon count (done)\nberlin_counter$activities <- berlin_counter$activities + sapply(st_intersects(berlin_countour, filter(a_ber_poi_polygon, group == \"activities\")), length)\n# adding up more parks from landuse polygons by intersection (done)\ntable(b_ber_landuse_multipolygon$group)\ntest <- data.frame(bezirk = st_intersection(filter(b_ber_landuse_multipolygon, group == \"park\"), berlin_countour)$name.1,\nberlin_counter$park <- berlin_counter$park + berlin_counter$x\nberlin_counter$x <- NULL\n# transport polygon count (done)\ntable(c_ber_transport_polygon$group)\nberlin_counter$transport <- sapply(st_intersects(berlin_countour, filter(c_ber_transport_polygon, group == \"transport\")), length)\n# water multipolygon count (done)\ntable(d_ber_water_multipolygons$group)\ntest <- data.frame(bezirk = st_intersection(d_ber_water_multipolygons, berlin_countour)$name.1,\ntest2 <- aggregate(test$area, by = list(test$bezirk), FUN = sum)\ntest2 <- rename(test2, bezirk = Group.1)\nberlin_counter <- left_join(berlin_counter, test2, by = \"bezirk\")\nhead(berlin_counter)\nrm(test)\n# counter done\n# combining the one file\nx_berlin <- left_join(rename(berlin_countour, bezirk = name), berlin_counter, by = \"bezirk\")\n# plots [in progress]\nggplot()+\ngeom_sf(data = berlin_countour)+\ngeom_sf(data = a_ber_poi_multipolygon[13354, 5], color = \"red\")+\n{theme(\npanel.background = element_rect(fill = \"#222222\",\ncolour = \"#222222\",\nsize = 0.1, linetype = \"solid\"),\npanel.grid.major = element_line(size = 0.1, linetype = 'solid',\ncolour = \"white\"),\npanel.grid.minor = element_line(size = 0.1, linetype = 'solid',\ncolour = \"#222222\"),\nplot.background = element_rect(fill = \"#222222\"),\nlegend.title = element_text(colour = \"#cacaca\"),\n)\nggsave(\"temporary.png\", dpi = 320, scale = 1)\ngeom_sf(data = c_ber_transport_polygon, aes(color = fclass), size = 0.5)+\nlabs(title = \"TRANSPORT\", color = \"TYPES\")+\nlegend.text = element_text(colour = \"#cacaca\"),\n} # based\nggsave(\"temporary3.png\", dpi = 320, scale = 1)\n# getting palettes\ninstall.packages(\"devtools\")\ndevtools::install_github(\"jaredhuling/jcolors\")\nlibrary(jcolors)\ndisplay_all_jcolors()\n# entertainment plot\nggplot(data = x_berlin)+\ngeom_sf(aes(fill = entertainment))+\nscale_fill_jcolors_contin(\"pal11\")+\nlabs(title = \"ENTERTAINMENT OBJECTS\", fill = \"QUANTITY\")+\nggsave(\"entertainment.png\", dpi = 320, scale = 1)\n# activities plot\ngeom_sf(aes(fill = activities))+\nlabs(title = \"ACTIVITIES OBJECTS\", fill = \"QUANTITY\")+\nggsave(\"activities.png\", dpi = 320, scale = 1)\n# Shopping plot\ngeom_sf(aes(fill = shopping))+\nlabs(title = \"SHOPPING OBJECTS\", fill = \"QUANTITY\")+\nggsave(\"shopping.png\", dpi = 320, scale = 1)\n# catering plot\ngeom_sf(aes(fill = catering))+\nlabs(title = \"CATERING OBJECTS\", fill = \"QUANTITY\")+\nggsave(\"catering.png\", dpi = 320, scale = 1)\n# destinations plot\ngeom_sf(aes(fill = destinations))+\nlabs(title = \"TOURISM/SIGHTSEEING OBJECTS\", fill = \"QUANTITY\")+\nggsave(\"destinations.png\", dpi = 320, scale = 1)\n# health plot\ngeom_sf(aes(fill = health))+\nlabs(title = \"HEALTH OBJECTS\", fill = \"QUANTITY\")+\nggsave(\"health.png\", dpi = 320, scale = 1)\n# kids plot\ngeom_sf(aes(fill = kids))+\nlabs(title = \"KIDS OBJECTS\", fill = \"QUANTITY\")+\nggsave(\"kids.png\", dpi = 320, scale = 1)\n# park plot\nxx_berlin <- x_berlin\nxx_berlin$park <- as.integer(xx_berlin$park/(100*100))\noptions(scipen=999)\nggplot(data = xx_berlin)+\ngeom_sf(aes(fill = park))+\nscale_fill_jcolors_contin(palette = \"pal11\")+\nlabs(title = \"PARK OBJECTS\", fill = \"PARKS, HEC.\")+\nggsave(\"parks.png\", dpi = 320, scale = 1)\n# transport plot\ngeom_sf(aes(fill = transport))+\nlabs(fill = \"transportation\")+\nlabs(title = \"TRANSPORT OBJECTS\", fill = \"QUANTITY\")+\nggsave(\"transport.png\", dpi = 320, scale = 1)\n# water plot\nxx_berlin$water <- as.integer(xx_berlin$water/(100*100))\ngeom_sf(aes(fill = water))+\nlabs(title = \"WATER OBJECTS\", fill = \"WATER, HEC.\")+\nggsave(\"water.png\", dpi = 320, scale = 1)\nrm(xx_berlin)\n# combining the distance files\nx_distance_polygon <- bind_rows(a_ber_poi_polygon, c_ber_transport_polygon)\nx_distance_multipolygon <- bind_rows(a_ber_poi_multipolygon, b_ber_landuse_multipolygon,\nx_distance_polygon[1, 5]\nst_distance(x_distance_polygon, x_distance_polygon[1, 5])\n# all dots plot\ngeom_sf(data = x_berlin, fill = NA)+\ngeom_sf(data = x_distance_polygon, aes(color = group), size = 0.4, alpha = 0.85)+\nscale_color_jcolors(\"pal8\")+\nlabs(title = \"ALL OBJECTS\", color = \"TYPE\")+\nggsave(\"alldots.png\", dpi = 320, scale = 1)\n# poi poly plot\ngeom_sf(data = x_berlin)+\ngeom_sf(data = a_ber_poi_multipolygon, aes(fill = group))+\nlabs(title = \"ALL MULTIPOLYGON OBJECTS\", fill = \"TYPE\")+\nggsave(\"poipolyplot.png\", dpi = 320, scale = 1)\n# forest and water plot\ngeom_sf(data = b_ber_landuse_multipolygon, fill = \"lightgreen\")+\nlabs(title = \"ALL WATER AND PARK OBJECTS\", fill = \"OBJECT\")+\nggsave(\"waterforestplot.png\", dpi = 320, scale = 1)\ngeom_sf(data = x_distance_multipolygon)\nwrite.csv2(x_berlin, \"/Users/ivankotik/Documents/DEDA_project/miscellaneous/x_berlin.csv\")\nwrite.table(x_distance_multipolygon, \"/Users/ivankotik/Documents/DEDA_project/miscellaneous/x_distance_multipolygon.csv\", sep = \"|\")\nwrite_sf(x_berlin, \"/Users/ivankotik/Documents/DEDA_project/files and graphs/berlin.shp\")\n", "output_sequence": "Spatial analysis of Berlin rent prices and landlord premiums"}, {"input_sequence": "# install packages\ninstall.packages(\"ggmap\")\n# load packages\nlibrary(ggmap)\nlibrary(tmaptools)\nlibrary(RCurl)\nlibrary(jsonlite)\nlibrary(tidyverse)\nlibrary(leaflet)\nurl_nominatim_search_api <- \"https://nominatim.openstreetmap.org/search/\"\ntest_adress <- \"Berlin 10557 Heidestrasse 19\"\ncoords <- geocode_OSM(\"Berlin 10557 Heidestrasse 19\", as.sf = TRUE)\nggplot(data = berlin_countour)+\ngeom_sf()+\ngeom_sf(data = coords, color = \"red\")\nberlin_countour\n# reading the postalcodes\nplz <- read_sf(\"C://Users//ivkot//Downloads//plz-5stellig.shp//plz-5stellig.shp\")\nplz %>% filter(str_detect(note, \"Berlin\")) -> plz\nplz$midpoint <- st_centroid(plz$geometry)\nggplot(data = plz)+\ngeom_sf(data = plz$midpoint) # everything works\n# Creating an empty column for lacking adress (no need to geocode)\ndata_berlin$lacksfulladress <- NA\ndata_berlin[is.na(data_berlin[, 6]) == FALSE, 34] <- \"FALSE\"\ndata_berlin %>% filter(lacksfulladress == TRUE)\ndata_berlin %>% filter(lacksfulladress == FALSE) %>% select(full_adress)\nadresses_for_geo <- as.data.frame(data_berlin[is.na(data_berlin$streetPlain) == FALSE, 1])\nadresses_for_geo <- unlist(adresses_for_geo)\nadresses_for_geo[1:3]\n# Back to geocoding\ntest_adress <- geocode_OSM(adresses_for_geo, as.sf = TRUE)\ngeom_sf(data = test_adress, color = \"red\")\ncoord_list <- test_adress[1, ]\ncoord_list[1, ] <- NA\nlength(adresses_for_geo)\nfor(i in 1:8856) {\nplaceholderadress <- geocode_OSM(adresses_for_geo[i], as.sf = TRUE)\ncoord_list <- bind_rows(coord_list, placeholderadress)\nprint(Sys.time())\nSys.sleep(1.5)\n}\nwrite_csv(coord_list, file = \"C://Users//ivkot//Downloads//shape_files//coordinates_osm.csv\")\nsavefile <- coord_list\nwrite_csv(savefile[, c(1, 2, 3, 9)], file = \"C://Users//ivkot//Downloads//shape_files//coordinates_osm_v2.csv\")\n", "output_sequence": "Spatial analysis of Berlin rent prices and landlord premiums"}, {"input_sequence": "# Installing libraries\nlibrary(tidyverse)\ninstall.packages(\"ggfortify\")\nlibrary(ggfortify)\n# Data taken from: https://www.kaggle.com/corrieaar/apartment-rental-offers-in-germany/version/6\n# Importing data\ndata <- read_csv(\"C://Users//ivkot//Downloads//archive//immo_data.csv\")\n# for mac\ndata <- read.csv(\"/Users/ivankotik/Documents/shape_files/immo_data.csv\")\n# Checking for berlin\ntable(data$regio1)\n# Extracting Berlin\ndata %>% filter(regio1 == \"Berlin\") -> data_berlin\nrm(data)\n# Deleting columns that would not be used\ndata_berlin$telekomTvOffer <- NULL\ndata_berlin$telekomHybridUploadSpeed <- NULL\ndata_berlin$picturecount <- NULL\ndata_berlin$yearConstructedRange <- NULL\ndata_berlin$energyEfficiencyClass <- NULL\ndata_berlin$lastRefurbish <- NULL\ndata_berlin$electricityBasePrice <- NULL\n# Moving around the columns\ndata_berlin <- relocate(data_berlin, regio2, geo_plz, streetPlain, houseNumber, .after = regio1)\ndata_berlin <- relocate(data_berlin, totalRent, baseRent, serviceCharge, heatingCosts, .after = houseNumber)\ndata_berlin <- relocate(data_berlin, livingSpace, noRooms, hasKitchen, balcony, facilities, description, typeOfFlat, garden, interiorQual, condition, cellar, .after = heatingType)\ndata_berlin <- relocate(data_berlin, lift, .after = numberOfFloors)\n# Checking the \"facilities\" column for more useful characteristics\n# Guest WC's\nstr_extract(data_berlin$facilities, \"([zZ]wei.{0,4}B.d)|(ste.WC)\")\ndata_berlin$extrawc <- as.integer(sub(\"([zZ]wei.{0,4}B.d)|(ste.WC)\", 1, (str_extract(data_berlin$facilities, \"([zZ]wei.{0,4}B.d)|(ste.WC)\"))))\ndata_berlin <- relocate(data_berlin, extrawc, .after = hasKitchen)\n# Dishwashing machines\nsort(str_extract(data_berlin$facilities, \"Geschirrsp.lmaschine\"))\ndata_berlin$geschirrsp <- as.integer(sub(\"Geschirrsp.lmaschine\", 1, (str_extract(data_berlin$facilities, \"Geschirrsp.lmaschine\"))))\n# Washing machines\nsort(str_extract(data_berlin$facilities, \"Waschmaschine\"))\ndata_berlin$washingm <- as.integer(sub(\"Waschmaschine\", 1, (str_extract(data_berlin$facilities, \"Waschmaschine\"))))\n# Sorting data\ndata_berlin <- relocate(data_berlin, geschirrsp, washingm, .after = balcony)\n# Delete descriptions\ndata_berlin$description <- NULL\n# Replacing NA's with FALSE\ndata_berlin$hasKitchen <- as.logical(replace_na(data_berlin$hasKitchen, 0))\ndata_berlin$heatingCosts <- replace_na(data_berlin$heatingCosts, 0)\n# Create factors frm the 0/1 T/F columns and character vectors with groups\ndata_berlin$heatingType <- factor(data_berlin$heatingType)\ntable(data_berlin$floor) # 80 and 83 floor looks suspisious, deleting it\ndata_berlin %>% filter(floor < 27) -> data_berlin\ntable(data_berlin$noRooms) # 1.1, 2.2, 99.5 looks suspisious\ndata_berlin[data_berlin[, \"noRooms\"] == 1.1, \"noRooms\"] <- 1\ndata_berlin %>% filter(noRooms != 99.5) -> data_berlin\n# ===========================================================\n# Checking whether heatingTypes have a influence on the price\nggplot(data_berlin[data_berlin[, \"totalRent\"]<5000, ], aes(x = livingSpace, y = totalRent, color = heatingType))+\ngeom_point()+\nlabs(title = \"HEATING TYPE\", color = \"TYPES\", x = \"Living Space, m^2\", y = \"Total Rent, EUR\")+\n{theme(\npanel.background = element_rect(fill = \"#222222\",\ncolour = \"#222222\",\nsize = 0.1, linetype = \"solid\"),\npanel.grid.major = element_line(size = 0.1, linetype = 'solid',\ncolour = \"white\"),\npanel.grid.minor = element_line(size = 0.1, linetype = 'solid',\ncolour = \"#222222\"),\nplot.background = element_rect(fill = \"#222222\"),\nlegend.title = element_text(colour = \"#cacaca\"),\n)\nggsave(\"heatingtypewithoutlines.png\", dpi = 320, scale = 1)\ngeom_point(size = 0.75, alpha = 0.2)+\ngeom_smooth(se = FALSE, method = lm, size = 0.7)+\n} # based on the graps it can be seen that there is no heavy difference between the types\nggsave(\"heatingtypewithlines.png\", dpi = 320, scale = 1)\n# Making the model\n# Calculating price\ndata_berlin$price <- data_berlin$baseRent + data_berlin$serviceCharge + data_berlin$heatingCosts\ndata_berlin <- relocate(data_berlin, price, .after = houseNumber)\ndata_berlin_model <- data_berlin[, c(7, 12:30)]\ndata_berlin_model %>% filter(is.na(price) == FALSE) -> data_berlin_model\n# Including everyting\ndata_berlin_model_alltime <- data_berlin_model[, 1:19]\nsummary(lm(price ~ ., data = data_berlin_model_alltime))\n# Initial filtering of parameterts\nsummary(lm(price ~ livingSpace + hasKitchen + floor + numberOfFloors + lift, data = data_berlin_model))\nautoplot(model_1)\n# Getting read of the externalities\nggplot(data_berlin_model, aes(x = price))+\ngeom_density()\nquantile(data_berlin_model$price, 0.9) # cutoff at 2400\ndata_berlin_model %>% filter(price <= 2400) -> data_berlin_model_filter\n# modelling\nmodel_2_2 <- lm(price ~ livingSpace + hasKitchen + floor + numberOfFloors + lift, data = data_berlin_model_filter)\nsummary(model_2_2) # deleting number of floors due to insignificance\nmodel_2_2 <- lm(price ~ livingSpace + hasKitchen + floor + lift, data = data_berlin_model_filter)\nsummary(model_2_2) # deleting floors due to insignificance\nmodel_2_2 <- lm(price ~ livingSpace + hasKitchen + lift, data = data_berlin_model_filter)\nautoplot(model_2_2)\nmodel_2_3 <- lm(log(price) ~ livingSpace + hasKitchen + lift, data = data_berlin_model_filter)\nsummary(model_2_3)\ndata_berlin_model_filter$model2_2 <- model_2_2$fitted.values\nggplot(data = data_berlin_model_filter)+\ngeom_point(aes(x = livingSpace, y = price, color = lift))\nwrite.csv(data_berlin_model_withnames, \"/Users/ivankotik/Documents/DEDA_project/miscellaneous/data after modelling.csv\")\n", "output_sequence": "Spatial analysis of Berlin rent prices and landlord premiums"}, {"input_sequence": "import PyPDF2, re, time, requests, os\nimport pandas as pd\nfrom nltk.stem import PorterStemmer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nps = PorterStemmer()\n### This script downloads the full new combined PDF file\n# fetching all pdfs from a json file stored in web\nurl_text = 'https://raw.githubusercontent.com/IvanKotik/Word-cloud-Search-engine-optimisation-/419447491efef2bb3a21b0459e5bdcd352a39097/combined_pdf_json.json'\nr = requests.get(url_text)\ncombined_pdf = r.json()\n# fetching previous quantinar meta master list, i.e. all the id's and the links\nurl_master = 'https://raw.githubusercontent.com/IvanKotik/Word-cloud-Search-engine-optimisation-/master/q-master-json.json'\ne = requests.get(url_master)\nq_master_json = e.json()\n# converting the master meta list to a dataframe\nq_master = pd.DataFrame({'id' : [q_master_json[str(i)]['id'] for i in range(len(q_master_json))],\n})\n# filtering out all the quantlets that we do not have a link associated with from metadata\nq_master['url_check'] = [len(i) for i in q_master['pdf_url']]\nq_master = q_master.loc[q_master['url_check'] != 0, ]\nq_master = q_master.reset_index(drop=True)\n# fetching a fresh master meta data list\n# url_fresh_master = 'https://quantinar.com/api/flower/index'\n# t = requests.get(url_fresh_master)\n# q_fresh_json = t.json()\n# converting the fresh meta master list to a dataframing\n# q_check = pd.DataFrame({'id' : [q_fresh_json['data'][i]['id'] for i in range(len(q_fresh_json['data']))],\n# })\n# filtering out all the quantlets that we do not have a link associated with from metadata\n# q_check['url_check'] = [len(i) for i in q_check['pdf_url']]\n# q_check = q_check.loc[q_check['url_check'] != 0, ]\n# q_check = q_check.reset_index(drop=True)\n# generic stop words list, done like this to be independent of packages\nstopwords_list = {\"i\",\"me\",\"my\",\"myself\",\"we\",\"our\",\"ours\",\"ourselves\",\"you\",\"you're\",\"you've\",\"you'll\",\"you'd\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"he\",\"him\",\"his\",\"himself\",\"she\",\"she's\",\"her\",\"hers\",\"herself\",\"it\",\"it's\",\"its\",\"itself\",\"they\",\"them\",\"their\",\"theirs\",\"themselves\",\"what\",\"which\",\"who\",\"whom\",\"this\",'that',\"that'll\",\"these\",\"those\",\"am\",\"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\"being\",\"have\",\"has\",\"had\",\"having\",\"do\",\"does\",\"did\",\"doing\",\"a\",\"an\",\"the\",\"and\",\"but\",\"if\",\"or\",\"because\",\"as\",\"until\",\"while\",\"of\",\"at\",\"by\",\"for\",\"with\",\"about\",\"against\",\"between\",\"into\",\"through\",\"during\",\"before\",\"after\",\"above\",\"below\",\"to\",\"from\",\"up\",\"down\",\"in\",\"out\",\"on\",\"off\",\"over\",\"under\",\"again\",\"further\",\"then\",\"once\",\"here\",\"there\",\"when\",\"where\",\"why\",\"how\",\"all\",\"any\",\"both\",\"each\",\"few\",\"more\",\"most\",\"other\",\"some\",\"such\",\"no\",\"nor\",\"not\",\"only\",\"own\",\"same\",\"so\",\"than\",\"too\",\"very\",\"s\",\"t\",\"can\",\"will\",\"just\",\"don\",\"don't\",\"should\",\"should've\",\"now\",\"d\",\"ll\",\"m\",\"o\",\"re\",\"ve\",\"y\",\"ain\",\"aren\",\"aren't\",\"couldn\",\"couldn't\",\"didn\",\"didn't\",\"doesn\",\"doesn't\",\"hadn\",\"hadn't\",\"hasn\",\"hasn't\",\"haven\",\"haven't\",\"isn\",\"isn't\",\"ma\",\"mightn\",\"mightn't\",\"mustn\",\"mustn't\",\"needn\",\"needn't\",\"shan\",\"shan't\",\"shouldn\",\"shouldn't\",\"wasn\",\"wasn't\",\"weren\",\"weren't\",\"won\",\"won't\",\"wouldn\",\"wouldn't\", \"udcurlymod\", \"nvcpinkrddstratxbcbtsburstdgbdogeltcardrgntlskpascxrpbtcetcethomniscdashdcrfctgnonmrdynam\", \"btcomnigntclambbrdgbsclsknmrblitzltcethbtsfctdogestratsteembtcddmdbtmgroup\", \"vydytiyzjjhrtncozhjtzv\", \"vbuptqjymgcq\", \"leiowsmcwqueca\", \"uicgnihcgj\", \"hfd\", \"honxnk\", \"latexit\", \"sha\", \"base\", \"nqu\", \"xkgewckhjywtkfomismnzuo\", \"aaab\" ,\"nicbzbnswmxeizn\", \"rr\", \"ainsqucoqt\", \"mvjbfsb\", \"vjm\", \"gjcuupt\", \"ciwdfvpp\", \"vplvtn\", \"aoslgyd\", \"zsjmgynbjfx\", \"tfwnza\", \"rbwdt\", \"yaacqanbwp\", \"kromocsn\", \"gnwuzphegrwj\", \"vbuptqjymgcq\", \"ljtte\", \"douzi\", \"leqf\", \"xcueavcjx\", \"eikvzwqslao\", \"pbr\", \"yyy\", \"acirytnzldfnixzhkxycs\", \"bcfl\", \"uljw\", \"divlsnlzm\", \"vydytiyzjjhrtncozhjtzv\", \"vglzllvmmaslj\", \"jmeju\", \"kwdwjvkwcinxc\", \"urocdv\", \"latexit\", \"null\", \"sha\"}\ndef download_pdf(file_name, url):\n'''Download a PDF file via an URL'''\n# Define HTTP Headers\nheaders = {\"User-Agent\": \"Chrome/51.0.2704.103\"}\n\n# Download image\nresponse = requests.get(url, headers=headers)\n# if response is OK download the PDF and store it, else print the status\nif response.status_code == 200:\nwith open(file_name, \"wb\") as f:\nf.write(response.content)\nelse:\nprint(response.status_code)\nreturn\ndef create_string(file_name, url):\ndownload_pdf(file_name, url)\n'''Transform a PDF file to a list of string pages'''\n# opening the file\nimported_pdf = open(file_name, 'rb')\n# removing the file locally\nos.remove(file_name)\n# convert PDF to readable file\ntransformed_pdf = PyPDF2.PdfFileReader(imported_pdf, strict=False)\n# get number of pages\ntotalpages = transformed_pdf.numPages\n# read the data and store in a list\npdf_output = [transformed_pdf.getPage(i) for i in range(totalpages)]\n# extract result\npdf_output = [pdf_output[i].extractText() for i in range(totalpages)]\nreturn pdf_output, totalpages\ndef cleaning(file_name, url):\n'''PDF cleaning procedure'''\npdf_output, totalpages = create_string(file_name, url)\n# cleaning URLs\npdf_output = [re.sub(pattern = \"http[^ ]*\", repl = \" \", string = pdf_output[i]) for i in range(totalpages)]\n# cleaning symbols\npdf_output = [re.sub(pattern = \"\\\\n\", repl = \" \", string = pdf_output[i]) for i in range(totalpages)]\n# cleaning multispaces\npdf_output = [re.sub(pattern = \"\\s{2,}\", repl = \" \", string = pdf_output[i]) for i in range(totalpages)]\n# cleaning out 1-2-worders\npdf_output = [re.sub(pattern = \" .{1,2} \", repl = \" \", string = pdf_output[i]) for i in range(totalpages)]\n# lower-casing\npdf_output = [pdf_output[i].lower() for i in range(totalpages)]\npdf_output = [[ps.stem(word) for word in sentence.split(\" \")] for sentence in pdf_output]\npdf_output = [' '.join(pdf_output[i]) for i in range(len(pdf_output))]\nreturn pdf_output, totalpages\ndef combined_pdf_creator():\n'''Creating the final master-pdf dataframe'''\n# clean the first pdf\npdf_output, totalpages = cleaning(str(q_master.iloc[0, 0]), q_master.iloc[0 ,7])\n# combine the pdf\ncombined_pdf = [' '.join(pdf_output)]\n# iterate on above\nfor i in range(1, q_master.shape[0]):\nprint(i)\nt = time.process_time()\ntry:\npdf_output, totalpages = cleaning(str(q_master.iloc[i, 0]), q_master.iloc[i ,7])\ncombined_pdf.append(' '.join(pdf_output))\nexcept:\nprint('problematic file: ', str(q_master.iloc[i, 0]), q_master.iloc[i ,7])\ncombined_pdf.append(' '.join(''))\nfinally:\nprint('time elapsed: ', (time.process_time() - t))\nreturn combined_pdf\n# checking whether we have any new entries from the fresh master meta data, if true then download a new copy\nif all([any(o == q_master['id']) for o in [i for i in q_master['id']]]) == False:\ncombined_pdf = combined_pdf_creator()\ntry:\ncombined_pdf_df = pd.DataFrame({'id' : q_master['id'], \"text\" : combined_pdf})\ncombined_pdf_json = combined_pdf_df.to_json(orient='index')\nwith open(\"combined_pdf_json.json\", \"w\") as outfile:\noutfile.write(combined_pdf_json)\nexcept: print('problematic file encountered')\nelse:\ncombined_pdf = [combined_pdf[str(i)]['text'] for i in range(len(combined_pdf))]\n# combined_pdf and q_master are the outputs here\n# word occurance function, output as pdf --> word_count_json\ndef word_count(str):\ncounts = dict()\nwords = str.split()\nfor word in words:\nif word in counts:\ncounts[word] += 1\nelse:\ncounts[word] = 1\nreturn counts\nword_count_df = pd.DataFrame([word_count(i) for i in combined_pdf])\nword_count_df = word_count_df.set_index(q_master['id'])\n", "output_sequence": "This Quantlet build takes in as an input a PDF filedecomposes it by words and counts them by occurance. The output is saved as a JSON file."}, {"input_sequence": "import PyPDF2, os, re, time, requests\nimport pandas as pd\nfrom nltk.stem import PorterStemmer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nps = PorterStemmer()\n# generic stop words list, done like this to be independent of packages\nstopwords_list = {\"i\",\"me\",\"my\",\"myself\",\"we\",\"our\",\"ours\",\"ourselves\",\"you\",\"you're\",\"you've\",\"you'll\",\"you'd\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"he\",\"him\",\"his\",\"himself\",\"she\",\"she's\",\"her\",\"hers\",\"herself\",\"it\",\"it's\",\"its\",\"itself\",\"they\",\"them\",\"their\",\"theirs\",\"themselves\",\"what\",\"which\",\"who\",\"whom\",\"this\",'that',\"that'll\",\"these\",\"those\",\"am\",\"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\"being\",\"have\",\"has\",\"had\",\"having\",\"do\",\"does\",\"did\",\"doing\",\"a\",\"an\",\"the\",\"and\",\"but\",\"if\",\"or\",\"because\",\"as\",\"until\",\"while\",\"of\",\"at\",\"by\",\"for\",\"with\",\"about\",\"against\",\"between\",\"into\",\"through\",\"during\",\"before\",\"after\",\"above\",\"below\",\"to\",\"from\",\"up\",\"down\",\"in\",\"out\",\"on\",\"off\",\"over\",\"under\",\"again\",\"further\",\"then\",\"once\",\"here\",\"there\",\"when\",\"where\",\"why\",\"how\",\"all\",\"any\",\"both\",\"each\",\"few\",\"more\",\"most\",\"other\",\"some\",\"such\",\"no\",\"nor\",\"not\",\"only\",\"own\",\"same\",\"so\",\"than\",\"too\",\"very\",\"s\",\"t\",\"can\",\"will\",\"just\",\"don\",\"don't\",\"should\",\"should've\",\"now\",\"d\",\"ll\",\"m\",\"o\",\"re\",\"ve\",\"y\",\"ain\",\"aren\",\"aren't\",\"couldn\",\"couldn't\",\"didn\",\"didn't\",\"doesn\",\"doesn't\",\"hadn\",\"hadn't\",\"hasn\",\"hasn't\",\"haven\",\"haven't\",\"isn\",\"isn't\",\"ma\",\"mightn\",\"mightn't\",\"mustn\",\"mustn't\",\"needn\",\"needn't\",\"shan\",\"shan't\",\"shouldn\",\"shouldn't\",\"wasn\",\"wasn't\",\"weren\",\"weren't\",\"won\",\"won't\",\"wouldn\",\"wouldn't\", \"udcurlymod\", \"nvcpinkrddstratxbcbtsburstdgbdogeltcardrgntlskpascxrpbtcetcethomniscdashdcrfctgnonmrdynam\", \"btcomnigntclambbrdgbsclsknmrblitzltcethbtsfctdogestratsteembtcddmdbtmgroup\", \"vydytiyzjjhrtncozhjtzv\", \"vbuptqjymgcq\", \"leiowsmcwqueca\", \"uicgnihcgj\", \"hfd\", \"honxnk\", \"latexit\", \"sha\", \"base\", \"nqu\", \"xkgewckhjywtkfomismnzuo\", \"aaab\" ,\"nicbzbnswmxeizn\", \"rr\", \"ainsqucoqt\", \"mvjbfsb\", \"vjm\", \"gjcuupt\", \"ciwdfvpp\", \"vplvtn\", \"aoslgyd\", \"zsjmgynbjfx\", \"tfwnza\", \"rbwdt\", \"yaacqanbwp\", \"kromocsn\", \"gnwuzphegrwj\", \"vbuptqjymgcq\", \"ljtte\", \"douzi\", \"leqf\", \"xcueavcjx\", \"eikvzwqslao\", \"pbr\", \"yyy\", \"acirytnzldfnixzhkxycs\", \"bcfl\", \"uljw\", \"divlsnlzm\", \"vydytiyzjjhrtncozhjtzv\", \"vglzllvmmaslj\", \"jmeju\", \"kwdwjvkwcinxc\", \"urocdv\", \"latexit\", \"null\", \"sha\"}\ndef input_sequence(input_initial):\n'''Trimming input search terms to be used for the occurrence matrix. The output is a generalized stemmed input form ready for checking and a count of terms for the ngram_range.'''\n# splitting the phrase by pieces\nsearch_term = input_initial.split(' ')\n# cleaning stopwords\nsearch_term = [i for i in search_term if i not in stopwords_list]\n# count words\nsearch_term_count = len(search_term)\n# stem the words\nsearch_term = [ps.stem(i) for i in search_term]\n# create the additional variations of the phrase\nouter_list = []\nfor i in range(0, search_term_count):\ninner_list = [search_term[j : search_term_count-i+j] for j in range(i+1)]\nouter_list.append(inner_list)\nreturn search_term, search_term_count, outer_list\ndef general_occurrence(search_term_count, combined_pdf):\n'''Creation of the generalized tfidf occurance matrix based on dynamic parameters.'''\nvectorizer_general = TfidfVectorizer(smooth_idf=True, sublinear_tf=True, use_idf=True, lowercase=False, stop_words=stopwords_list, ngram_range=(search_term_count, search_term_count))\nX_general = vectorizer_general.fit_transform(combined_pdf)\nxx_general = pd.DataFrame(X_general.toarray(), columns = vectorizer_general.get_feature_names_out())\nreturn xx_general\ndef check_for_general(search_term, q_master, search_term_count, outer_list, combined_pdf, number_of_urls):\n'''Main function.'''\n# initiating a breaker function\nbreaker = 0\n# creating a list out of the pdf-json\ncombined_pdf = [combined_pdf[str(i)]['text'] for i in range(len(combined_pdf))]\n# creating the occurrence matrix for max length\nxx_general = general_occurrence(search_term_count, combined_pdf)\n# creating an empty table for results\ntest_output = xx_general.copy()\ntest_output = test_output.iloc[:,0]*0\n# first test for full match\nprint('search term: ', outer_list[0][0])\ntest = ' '.join(outer_list[0][0])\n# if test passed\nif test in list(xx_general.columns):\n# create a ranked index\nranked_indexes = xx_general[test].sort_values(ascending=False).index\nranked_indexes = list(ranked_indexes[0:number_of_urls])\n# connect back to urls\noutput_url = [q_master['id'][i] for i in ranked_indexes]\nprint('search result: present\\n')\nreturn output_url\n# if test failed drill-down\nelse:\nprint('search result: not present, drill-down\\n')\nfor y in range(1, len(outer_list)):\n# create a new occurance matrix with new ngrams\nxx_general = general_occurrence(search_term_count-y, combined_pdf)\nfor u in range(y+1):\n# drill-down phrase test\nprint('search term: ', outer_list[y][u])\n# if test passed\nif test in list(xx_general.columns):\n# sum the tfidf indexes across multiple matches\ntest_output += xx_general[test]\nprint('search result: present\\n')\n# initiate the exit from the function\nbreaker = 1\nprint('search result: not present\\n')\nif breaker == 1:\n# order the indexes by highest tfidf\nranked_indexes = test_output.sort_values(ascending=False).index\nranked_indexes = list(ranked_indexes[0:number_of_urls])\n# return urls\noutput_url = [q_master['id'][i] for i in ranked_indexes]\nreturn output_url\n# search terms\ninput_initial = 'Pricing and hedging inverse BTC options'\n# how many outputs are needed\nnumber_of_urls = 5\n##################################################################################################\n# this section should be run from se_pdfdownloader.py\n# fetching all pdfs from a json file stored in web\nurl_text = 'https://raw.githubusercontent.com/IvanKotik/Word-cloud-Search-engine-optimisation-/419447491efef2bb3a21b0459e5bdcd352a39097/combined_pdf_json.json'\nr = requests.get(url_text)\ncombined_pdf = r.json()\n# fetching previous quantinar meta master list, i.e. all the id's and the links\nurl_master = 'https://raw.githubusercontent.com/IvanKotik/Word-cloud-Search-engine-optimisation-/master/q-master-json.json'\ne = requests.get(url_master)\nq_master_json = e.json()\n# converting the master meta list to a dataframe\nq_master = pd.DataFrame({'id' : [q_master_json[str(i)]['id'] for i in range(len(q_master_json))],\n})\n# filtering out all the quantlets that we do not have a link associated with from metadata\nq_master['url_check'] = [len(i) for i in q_master['pdf_url']]\nq_master = q_master.loc[q_master['url_check'] != 0, ]\nq_master = q_master.reset_index(drop=True)\nsearch_term, search_term_count, outer_list = input_sequence(input_initial)\noutput_url = check_for_general(search_term, q_master, search_term_count, outer_list, combined_pdf, number_of_urls)\njson_output = pd.DataFrame({'id': output_url}).to_json(orient='index')\n", "output_sequence": "This Quantlet build takes in as an input a PDF filedecomposes it by words and counts them by occurance. The output is saved as a JSON file."}, {"input_sequence": "import argparse\nimport Script\nparser = argparse.ArgumentParser(description='Convert a PDF file to a JSON Metadata containing the word counts.')\nparser.add_argument('path', help='The path of the pdf you want to convert')\nparser.add_argument('-o', '--output', help='The type of output you wish to have. Can be console or file. For file output you should also add filename argument')\nparser.add_argument('-f', '--filename', help='The name of the file you want the output to.')\nargs = parser.parse_args()\nif args.output == 'console':\nprint(Script.converter_pdf_json_count(args.path, file_name=\"\"))\nelif args.output == 'file':\nif args.filename == None or args.filename == \"\":\nraise parser.error('--path argument is required when output type is file')\n", "output_sequence": "This Quantlet build takes in as an input a PDF filedecomposes it by words and counts them by occurance. The output is saved as a JSON file."}, {"input_sequence": "# clear all variables\nrm(list = ls(all = TRUE))\ngraphics.off()\n# set the working directory\n#setwd(\"C:/...\")\nlibraries = c(\"np\", \"quantreg\", \"VGAM\", \"rgl\", \"misc3d\", \"matrixStats\", \"MASS\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\nsource(\"kernel.r\")\n######################### General setting ##############################\nf<-function(x1,x2){sin(2*pi*x1)+x2}\nS <- matrix(c(1, -0.3, 1), nrow = 2)\nbb=seq(0.1,0.9,length=20)\nxx<-as.matrix(expand.grid(bb,bb))\nRep <- 2000\ntau50<-0.5\nq50<-qnorm(tau50)\nsig_0 <- 0.2 # Please change here for the other model variance: 0.5\n################ Hetergeneity sigma = f(x) #################################\nsig <- function(x1,x2){sig_0+0.8*x1*(1-x1)*x2*(1-x2)}\nf0_hom_tau50<-f(xx[,1],xx[,2])+q50*sig(xx[,1],xx[,2])\nerror_hom<-matrix(0,nrow=3,ncol=3)\narea.cc <- numeric(0)\ntemp.area<-c(0,0,0)\n######## n = 50 ################################################\nfor(k in 1:Rep){\nX <- mvrnorm(nn, mu = c(0,0), Sigma = S)\nx.biunif <- pnorm(X)\ny.biunif<-f(x.biunif[,1],x.biunif[,2])+rnorm(nn,mean=0,sd=sig(xx[,1],xx[,2]))\nbdwh<-npcdensbw(xdat=x.biunif,ydat=y.biunif,bwmethod=\"normal-reference\",ckertype=\"epanechnikov\",ckerorder=2)\nbdwh\ntry(qr.ufit50<-lcrq.conf(x.biunif,y.biunif,bwth=bdwh,d=2,tau=tau50,xx=xx))\ntry(if(length(which(qr.ufit50$lband - f0_hom_tau50>0 | qr.ufit50$hband - f0_hom_tau50<0))!=0){error_hom[1,1]<-error_hom[1,1]+1})\ntry(temp.area[1]<-sum(qr.ufit50$hband-qr.ufit50$lband))\ntry(area.cc<-rbind(area.cc,temp.area))\nprint(k)\n}\nerror_hom/Rep\n", "output_sequence": "The main file which performs the Monte Carlo simulation for the coverage ratio of the theoretical multivariate confidence band for nonparametric kernel quantile regression. The data generating model is a heterogeneous model."}, {"input_sequence": "# epanechnikov kernel\nepanech.prod<-function(x,d){ # x: d*1 vector\npp<-1\nfor(j in 1:d){\npp<-as.numeric(abs(x[j])<1)*3/4*(1-x[j]^2)*pp\n}\nreturn(pp)\nepanech.de.prod<-function(x,d){ # x: d*1 vector\npp<-as.numeric(abs(x[j])<1)*(-3/2)*x[j]*pp\n############################################################################\nquartic.prod<-function(x,d){ # x: d*1 vector\npp<-as.numeric(abs(x[j])<1)*15/16*(1-x[j]^2)^2*pp\nquartic.de.prod<-function(x,d){ # x: d*1 vector\npp<-as.numeric(abs(x[j])<1)*(-15/4)*x[j]*(1-x[j]^2)*pp\nquartic.v<-function(x,d){ # x: d*1 vector\npp<-numeric(0)\npp<-rowProds(15/16*(1-x^2)^2)\n\n###########################################################################3\ngaussian.prod<-function(x,d){\nif(d>1){\npp<-rowProds(dnorm(x,mean=0,sd=1))\nelse{\npp<-dnorm(x)\ngaussian.de.prod<-function(x,d){\npp<-(-x[j])*dnorm(x[j],mean=0,sd=1)*pp\n", "output_sequence": "The main file which performs the Monte Carlo simulation for the coverage ratio of the theoretical multivariate confidence band for nonparametric kernel quantile regression. The data generating model is a heterogeneous model."}, {"input_sequence": "# Depend on packages: quantreg, np, matrixStats, ks\n# Notice: we apply special case for kernel choice: PRODUCT kernel\nlcrq<-function(x, y, H, d, tau = 0.5, xx) # H: sequence of bandwidth, xx: a matrix m*d, x: matrix n*d, y: n*1 vector\n{\nfv <- numeric(0)\n# dv <- xx\nn <- length(y)\nfor (i in 1:length(xx[,1])) {\nll<-matrix(1,d,n)\nz <- x - t(ll*xx[i,])\nwx<-numeric(0)\nfor(k in 1:n){\nwx[k] <- epanech.prod(z[k,]/H,d)\n}\nr <- rq(y ~ 1, weights = wx, tau = tau, ci = FALSE)\nfv[i] <- r$coef[1]\n}\nlist(xx = xx, fv = fv)\n}\nllrq<-function(x, y, bwth, d, tau = 0.5, kern=\"quartic\", xx) # H: sequence of bandwidth, xx: a matrix m*d, x: matrix n*d, y: n*1 vector\nif(kern==\"epanech\"){\nK_2 <- (3/5)^d # Product kernel Epanechnikov\nSIG <- diag(d)*(3/2)*(3/5)^(d-1) # Product kernel Epanechnikov\nker <- epanech.prod\nif(kern==\"quartic\"){\nK_2 <- (5/7)^d # Product kernel quartic\nSIG <- diag(d)*(15/7)^(5/7)^(d-1) # Product kernel quartic\nker <- quartic.prod\ndv <- numeric(0)\nH <- bwth$xbw\nwx[k] <- ker(z[k,]/H,d)\nr <- rq(y ~ z, weights = wx, tau = tau, ci = FALSE)\ndv[i] <- r$coef[2]\nlist(xx = xx, fv = fv,dv=dv)\n###########################################################################\nloss.psi<-function(x,tau){\nphii<-numeric(0)\nphii<-as.numeric(x<0)-tau\nreturn(phii)\n#############################################################################\nlcrq.conf<-function(x, y, bwth, d, tau = 0.5, kern=\"quartic\", alpha=0.05,xx){\nfv <- numeric(0)\nH <- bwth$xbw*(4*(tau*(1-tau)/(dnorm(qnorm(p=tau))^2)))^(1/5)/n^(0.05) # Yu and Jones 1998 rule of thumb (simuation with normal) + Undersmoothing\ni <- 1\nH.e <- H <- bwth$xbw*(4*(tau*(1-tau)/(dnorm(qnorm(p=tau))^2)))^(1/5)\nfor (i in 1:n) {\nz <- x - t(ll*x[i,])\nwx[k] <- ker(z[k,]/H.e,d)\nfv.i[i] <- r$coef[1]\neps.hat <- y-fv.i\n\nfx.bw<-npudensbw(dat=x,bwmethod=\"normal-reference\",ckerorder=2)\nfx.objt<-npudens(fx.bw,tdat=x,edat=xx)\nfx<-fx.objt$dens\nfe_x.bw<-npcdensbw(xdat=x,ydat=eps.hat,bwmethod=\"normal-reference\",ckerorder=2)\nfe_x.objt<-npcdens(fe_x.bw,txdat=x,tydat=eps.hat,exdat=xx,eydat=seq(0,length=length(xx[,1]),by=0))\nfy_x.bw<-fe_x.bw\nfy_x.bw$xbw<-fy_x.bw$xbw/3\nfym_x.objt<-npcdens(fy_x.bw,txdat=x,tydat=y,exdat=xx,eydat=fv)\nfe_x<-fe_x.objt$condens\n\nxx.mat<-as.matrix(xx)\nvol.D <- prod(colMaxs(xx.mat)-colMins(xx.mat))\nkap <- abs(log(prod(H)^(1/d), base = n))\neta <- log(vol.D)+ kap*d*log(n)\nc_alpha <- log(2)-log(abs(log(1-alpha)))\nH_2 <- (2*pi*K_2)^(-d/2)*sqrt(det(SIG))\nd_n <- sqrt(2*eta)+sqrt(2*eta)^(-1)*(0.5*(d-1)*log(log(n^kap))+log(sqrt(2*pi)^(-1)*H_2*(2*d)^((d-1)/2)))\nband <- 1/sqrt(n*prod(H))*sqrt(tau*(1-tau)*sqrt(K_2)/fx)*(1/fe_x)*(d_n+c_alpha*sqrt(2*eta)^(-1))\nlist(xx = xx, fv = fv,hband = fv + band, lband = fv - band, xdensbw=1/fx.bw$bw,bw = H,fx=fx, fe_x=fe_x, fym_x=fym_x,eps.hat=eps.hat)\n############################################################################################################################\nlcrq.boot<-function(x, y, bwth, d, tau = 0.5, kern=\"quartic\", B=500,alpha=0.05,xx){\nH <- bwth$xbw/n^(0.05)*(4*(tau*(1-tau)/(dnorm(qnorm(p=tau))^2)))^(1/5) # Yu and Jones 1998 rule of thumb (simuation with normal) + Undersmoothing\nll<-matrix(1,d,n)\nW.xx.n<-matrix(0,nrow=length(xx[,1]),ncol=n)\nfor (i in 1:length(xx[,1])) {\nwx<-seq(0,0,length=n)\nif(prod(as.numeric(z[k,]/H<=1))==1){\n}}\nW.xx.n[i,] <- wx\n}\nH.e <- H <- bwth$xbw*(4*(tau*(1-tau)/(dnorm(qnorm(p=tau))^2)))^(1/5)\n\nfe_x.bw<-npcdensbw(xdat=x,ydat=eps.hat,bwmethod=\"normal-reference\",ckerorder=2)\nfe_x.objt<-npcdens(fe_x.bw,txdat=x,tydat=y,exdat=xx,eydat=fv)\nfex.bw$bw[1]<-fe_x$bw[1]*1.5\nfyx.objt<-npudens(fex.bw,tdat=cbind(y,x),edat=cbind(fv,xx))\nfyx<-fyx.objt$dens\n###### Bootstrap loop #####################################\nFX<-diag(1/sqrt(fx)*(fyx/fex),nrow=length(fx),ncol=length(fx)) # Diagonal matrix with 1/sqrt(f(x))being the diagonal line\nBOOT <- cbind(x,eps.hat) # epsilon is at the last column of this matrix\nsample.boot <- numeric(0)\nHH<-fe_x.bw$xbw\nHH[d+1]<-fe_x.bw$ybw\nstoch.dev <- matrix(0,nrow=length(xx[,1]),ncol=B)\nmax.dev.boot <- as.vector(seq(0,length=B,by=0))\nW.x.b <- matrix(0,nrow=length(xx[,1]),ncol=n)\ni <- 1\nfor(b in 1:B){\nbb<-sample(c(1:n),size=n,replace=TRUE)\ner<-rnorm(n*(d+1))\nE<-matrix(er,nrow=n,ncol=d+1)\nboot.data<-as.matrix(BOOT[bb,])+ E%*%diag(HH,nrow=d+1,ncol=d+1)\nx.boot <- boot.data[,1:d]\npsi.sample.boot <- as.matrix(loss.psi(sample.boot,tau))\nW.x<-matrix(0,nrow=length(xx[,1]),ncol=n)\nX.boot<-matrix(t(x.boot),nrow=n*length(xx[,1]),ncol=d,byrow=TRUE)\nXX<-xx[rep(seq_len(nrow(xx)),each=n),]\nZ<-X.boot-XX\nEff<- which(abs(t(t(Z)/H))[,1]<1 & abs(t(t(Z)/H))[,2]<1) ## Only for d=2!!\nwx<-seq(0,0,length=n*length(xx[,1]))\nwx[Eff]<-quartic.v(t(t(Z)/H)[Eff,],d)\nW.x<-matrix(wx,nrow=length(xx[,1]),ncol=n,byrow=TRUE)\ndev<-W.x%*%psi.sample.boot\nstoch.dev[,b]<-(n*prod(H))^(-1)*sqrt(1/(tau*(1-tau)))*FX%*%dev\n} #)\nstoch.dev<-stoch.dev-matrix(rowMeans(stoch.dev)[rep(seq_len(length(fv)),each=B)],nrow=length(xx[,1]),ncol=B,byrow=TRUE)\nmax.dev.boot<-colMaxs(abs(stoch.dev))\nG_alpha <- quantile(max.dev.boot, probs = 1-alpha)\nband <- sqrt(tau*(1-tau)/fx)*(1/fe_x)*G_alpha# 1/sqrt(n*prod(H))*sqrt(tau*(1-tau)*sqrt(K_2)/fx)*(1/fe_x)*(d_n+G_alpha/sqrt(2*eta))\nlist(xx = xx, fv = fv,hband = fv + band, lband = fv - band, xdensbw=1/fx.bw$bw,bw = H,fx=fx, fe_x=fe_x,eps.hat=eps.hat,max.dev=max.dev.boot,G.alpha=G_alpha,boot.bw=HH)\n", "output_sequence": "The main file which performs the Monte Carlo simulation for the coverage ratio of the theoretical multivariate confidence band for nonparametric kernel quantile regression. The data generating model is a heterogeneous model."}, {"input_sequence": "# epanechnikov kernel\nepanech.prod<-function(x,d){ # x: d*1 vector\npp<-1\nfor(j in 1:d){\npp<-as.numeric(abs(x[j])<1)*3/4*(1-x[j]^2)*pp\n}\nreturn(pp)\nepanech.de.prod<-function(x,d){ # x: d*1 vector\npp<-as.numeric(abs(x[j])<1)*(-3/2)*x[j]*pp\n############################################################################\nquartic.prod<-function(x,d){ # x: d*1 vector\npp<-as.numeric(abs(x[j])<1)*15/16*(1-x[j]^2)^2*pp\nquartic.de.prod<-function(x,d){ # x: d*1 vector\npp<-as.numeric(abs(x[j])<1)*(-15/4)*x[j]*(1-x[j]^2)*pp\nquartic.v<-function(x,d){ # x: d*1 vector\npp<-numeric(0)\npp<-rowProds(15/16*(1-x^2)^2)\n\n###########################################################################3\ngaussian.prod<-function(x,d){\nif(d>1){\npp<-rowProds(dnorm(x,mean=0,sd=1))\nelse{\npp<-dnorm(x)\ngaussian.de.prod<-function(x,d){\npp<-(-x[j])*dnorm(x[j],mean=0,sd=1)*pp\n", "output_sequence": "The main file which performs the Monte Carlo simulation for the coverage ratio of the theoretical multivariate confidence band for nonparametric kernel quantile regression. The data generating model is a homogeneous model."}, {"input_sequence": "# clear all variables\nrm(list = ls(all = TRUE))\ngraphics.off()\n# set the working directory\n#setwd(\"C:/...\")\nlibraries = c(\"np\", \"quantreg\", \"VGAM\", \"rgl\", \"misc3d\", \"matrixStats\", \"MASS\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\nsource(\"lcrq.r\")\n######################### General setting ##############################\nf<-function(x1,x2){sin(2*pi*x1)+x2}\nS <- matrix(c(1, -0.3, 1), nrow = 2)\nbb=seq(0.1,0.9,length=20)\nxx<-as.matrix(expand.grid(bb,bb))\nRep <- 2000\ntau50<-0.5\nq50<-qnorm(tau50)\nsig_0 <- 0.2 # Please change here for the other model variance: 0.5\n################ sigma = 0.2 #################################\nf0_hom_tau50<-f(xx[,1],xx[,2])+q50*sig_0\nerror_hom<-matrix(0,nrow=3,ncol=3)\narea.cc <- numeric(0)\ntemp.area<-c(0,0,0)\n######## n = 50 ################################################\nfor(k in 1:Rep){\nX <- mvrnorm(nn, mu = c(0,0), Sigma = S)\nx.biunif <- pnorm(X)\ny.biunif<-f(x.biunif[,1],x.biunif[,2])+rnorm(nn,mean=0,sd=sig_0)\nbdwh<-npcdensbw(xdat=x.biunif,ydat=y.biunif,bwmethod=\"normal-reference\",ckertype=\"epanechnikov\",ckerorder=2)\nbdwh\ntry(qr.ufit50<-lcrq.conf(x.biunif,y.biunif,bwth=bdwh,d=2,tau=tau50,xx=xx))\ntry(if(length(which(qr.ufit50$lband > f0_hom_tau50 | qr.ufit50$hband < f0_hom_tau50))!=0){error_hom[1,1]<-error_hom[1,1]+1})\ntry(temp.area[1]<-sum(qr.ufit50$hband-qr.ufit50$lband))\ntry(area.cc<-rbind(area.cc,temp.area))\nprint(k)\n}\nerror_hom/Rep\n", "output_sequence": "The main file which performs the Monte Carlo simulation for the coverage ratio of the theoretical multivariate confidence band for nonparametric kernel quantile regression. The data generating model is a homogeneous model."}, {"input_sequence": "# Depend on packages: quantreg, np, matrixStats, ks\n# Notice: we apply special case for kernel choice: PRODUCT kernel\nlcrq<-function(x, y, H, d, tau = 0.5, xx) # H: sequence of bandwidth, xx: a matrix m*d, x: matrix n*d, y: n*1 vector\n{\nfv <- numeric(0)\n# dv <- xx\nn <- length(y)\nfor (i in 1:length(xx[,1])) {\nll<-matrix(1,d,n)\nz <- x - t(ll*xx[i,])\nwx<-numeric(0)\nfor(k in 1:n){\nwx[k] <- epanech.prod(z[k,]/H,d)\n}\nr <- rq(y ~ 1, weights = wx, tau = tau, ci = FALSE)\nfv[i] <- r$coef[1]\n}\nlist(xx = xx, fv = fv)\n}\nllrq<-function(x, y, bwth, d, tau = 0.5, kern=\"quartic\", xx) # H: sequence of bandwidth, xx: a matrix m*d, x: matrix n*d, y: n*1 vector\nif(kern==\"epanech\"){\nK_2 <- (3/5)^d # Product kernel Epanechnikov\nSIG <- diag(d)*(3/2)*(3/5)^(d-1) # Product kernel Epanechnikov\nker <- epanech.prod\nif(kern==\"quartic\"){\nK_2 <- (5/7)^d # Product kernel quartic\nSIG <- diag(d)*(15/7)^(5/7)^(d-1) # Product kernel quartic\nker <- quartic.prod\ndv <- numeric(0)\nH <- bwth$xbw\nwx[k] <- ker(z[k,]/H,d)\nr <- rq(y ~ z, weights = wx, tau = tau, ci = FALSE)\ndv[i] <- r$coef[2]\nlist(xx = xx, fv = fv,dv=dv)\n###########################################################################\nloss.psi<-function(x,tau){\nphii<-numeric(0)\nphii<-as.numeric(x<0)-tau\nreturn(phii)\n#############################################################################\nlcrq.conf<-function(x, y, bwth, d, tau = 0.5, kern=\"quartic\", alpha=0.05,xx){\nfv <- numeric(0)\nH <- bwth$xbw*(4*(tau*(1-tau)/(dnorm(qnorm(p=tau))^2)))^(1/5)/n^(0.05) # Yu and Jones 1998 rule of thumb (simuation with normal) + Undersmoothing\ni <- 1\nH.e <- H <- bwth$xbw*(4*(tau*(1-tau)/(dnorm(qnorm(p=tau))^2)))^(1/5)\nfor (i in 1:n) {\nz <- x - t(ll*x[i,])\nwx[k] <- ker(z[k,]/H.e,d)\nfv.i[i] <- r$coef[1]\neps.hat <- y-fv.i\n\nfx.bw<-npudensbw(dat=x,bwmethod=\"normal-reference\",ckerorder=2)\nfx.objt<-npudens(fx.bw,tdat=x,edat=xx)\nfx<-fx.objt$dens\nfe_x.bw<-npcdensbw(xdat=x,ydat=eps.hat,bwmethod=\"normal-reference\",ckerorder=2)\nfe_x.objt<-npcdens(fe_x.bw,txdat=x,tydat=eps.hat,exdat=xx,eydat=seq(0,length=length(xx[,1]),by=0))\nfy_x.bw<-fe_x.bw\nfy_x.bw$xbw<-fy_x.bw$xbw/3\nfym_x.objt<-npcdens(fy_x.bw,txdat=x,tydat=y,exdat=xx,eydat=fv)\nfe_x<-fe_x.objt$condens\n\nxx.mat<-as.matrix(xx)\nvol.D <- prod(colMaxs(xx.mat)-colMins(xx.mat))\nkap <- abs(log(prod(H)^(1/d), base = n))\neta <- log(vol.D)+ kap*d*log(n)\nc_alpha <- log(2)-log(abs(log(1-alpha)))\nH_2 <- (2*pi*K_2)^(-d/2)*sqrt(det(SIG))\nd_n <- sqrt(2*eta)+sqrt(2*eta)^(-1)*(0.5*(d-1)*log(log(n^kap))+log(sqrt(2*pi)^(-1)*H_2*(2*d)^((d-1)/2)))\nband <- 1/sqrt(n*prod(H))*sqrt(tau*(1-tau)*sqrt(K_2)/fx)*(1/fe_x)*(d_n+c_alpha*sqrt(2*eta)^(-1))\nlist(xx = xx, fv = fv,hband = fv + band, lband = fv - band, xdensbw=1/fx.bw$bw,bw = H,fx=fx, fe_x=fe_x, fym_x=fym_x,eps.hat=eps.hat)\n############################################################################################################################\nlcrq.boot<-function(x, y, bwth, d, tau = 0.5, kern=\"quartic\", B=500,alpha=0.05,xx){\nH <- bwth$xbw/n^(0.05)*(4*(tau*(1-tau)/(dnorm(qnorm(p=tau))^2)))^(1/5) # Yu and Jones 1998 rule of thumb (simuation with normal) + Undersmoothing\nll<-matrix(1,d,n)\nW.xx.n<-matrix(0,nrow=length(xx[,1]),ncol=n)\nfor (i in 1:length(xx[,1])) {\nwx<-seq(0,0,length=n)\nif(prod(as.numeric(z[k,]/H<=1))==1){\n}}\nW.xx.n[i,] <- wx\n}\nH.e <- H <- bwth$xbw*(4*(tau*(1-tau)/(dnorm(qnorm(p=tau))^2)))^(1/5)\n\nfe_x.bw<-npcdensbw(xdat=x,ydat=eps.hat,bwmethod=\"normal-reference\",ckerorder=2)\nfe_x.objt<-npcdens(fe_x.bw,txdat=x,tydat=y,exdat=xx,eydat=fv)\nfex.bw$bw[1]<-fe_x$bw[1]*1.5\nfyx.objt<-npudens(fex.bw,tdat=cbind(y,x),edat=cbind(fv,xx))\nfyx<-fyx.objt$dens\n###### Bootstrap loop #####################################\nFX<-diag(1/sqrt(fx)*(fyx/fex),nrow=length(fx),ncol=length(fx)) # Diagonal matrix with 1/sqrt(f(x))being the diagonal line\nBOOT <- cbind(x,eps.hat) # epsilon is at the last column of this matrix\nsample.boot <- numeric(0)\nHH<-fe_x.bw$xbw\nHH[d+1]<-fe_x.bw$ybw\nstoch.dev <- matrix(0,nrow=length(xx[,1]),ncol=B)\nmax.dev.boot <- as.vector(seq(0,length=B,by=0))\nW.x.b <- matrix(0,nrow=length(xx[,1]),ncol=n)\ni <- 1\nfor(b in 1:B){\nbb<-sample(c(1:n),size=n,replace=TRUE)\ner<-rnorm(n*(d+1))\nE<-matrix(er,nrow=n,ncol=d+1)\nboot.data<-as.matrix(BOOT[bb,])+ E%*%diag(HH,nrow=d+1,ncol=d+1)\nx.boot <- boot.data[,1:d]\npsi.sample.boot <- as.matrix(loss.psi(sample.boot,tau))\nW.x<-matrix(0,nrow=length(xx[,1]),ncol=n)\nX.boot<-matrix(t(x.boot),nrow=n*length(xx[,1]),ncol=d,byrow=TRUE)\nXX<-xx[rep(seq_len(nrow(xx)),each=n),]\nZ<-X.boot-XX\nEff<- which(abs(t(t(Z)/H))[,1]<1 & abs(t(t(Z)/H))[,2]<1) ## Only for d=2!!\nwx<-seq(0,0,length=n*length(xx[,1]))\nwx[Eff]<-quartic.v(t(t(Z)/H)[Eff,],d)\nW.x<-matrix(wx,nrow=length(xx[,1]),ncol=n,byrow=TRUE)\ndev<-W.x%*%psi.sample.boot\nstoch.dev[,b]<-(n*prod(H))^(-1)*sqrt(1/(tau*(1-tau)))*FX%*%dev\n} #)\nstoch.dev<-stoch.dev-matrix(rowMeans(stoch.dev)[rep(seq_len(length(fv)),each=B)],nrow=length(xx[,1]),ncol=B,byrow=TRUE)\nmax.dev.boot<-colMaxs(abs(stoch.dev))\nG_alpha <- quantile(max.dev.boot, probs = 1-alpha)\nband <- sqrt(tau*(1-tau)/fx)*(1/fe_x)*G_alpha# 1/sqrt(n*prod(H))*sqrt(tau*(1-tau)*sqrt(K_2)/fx)*(1/fe_x)*(d_n+G_alpha/sqrt(2*eta))\nlist(xx = xx, fv = fv,hband = fv + band, lband = fv - band, xdensbw=1/fx.bw$bw,bw = H,fx=fx, fe_x=fe_x,eps.hat=eps.hat,max.dev=max.dev.boot,G.alpha=G_alpha,boot.bw=HH)\n", "output_sequence": "The main file which performs the Monte Carlo simulation for the coverage ratio of the theoretical multivariate confidence band for nonparametric kernel quantile regression. The data generating model is a homogeneous model."}, {"input_sequence": "import pandas as pd\nimport matplotlib.pyplot as plt\npath = open(\"Data.csv\", \"r\")\ndata = pd.read_csv(path,header=0)\ndata.index = pd.to_datetime(data.Index)\ndata.drop(data.columns[[0]], axis=1,inplace = True)\ntrain_date = pd.date_range(start = '2014-07-31', end ='2017-03-01', freq='D')\n#Bollinger bands graph\ndef bollinger(close, window = 20, width = 2):\nma = close.rolling(window = window).mean()\nupper = ma + width*sd\nresult = pd.concat([close, ma, upper, lower], axis = 1)\nresult.columns = ('close', 'ma_' + str(window), 'upperband_' + str(window), 'lowerband_' + str(window))\nreturn result\nbtc = data['btc']\nbollingerbands = bollinger(btc, window = 20, width = 2)\nupper = bollingerbands['upperband_20']\nfig = plt.figure(figsize=(12,6))\nax = fig.add_subplot(111)\n# Get index values for the X axis for facebook DataFrame\nx_axis = btc.index\nax.fill_between(x_axis, upper, lower, color='lightskyblue')\n# Plot Adjust Closing Price and Moving Averages\nax.plot(x_axis, btc, color='red', lw=2)\nplt.show()\n", "output_sequence": "Compute Bollinger bands indicator for btc."}, {"input_sequence": "import os\nimport pandas as pd\nimport numpy as np\nwdir = os.getcwd()\n# fix random seed for reproducibility\nseed = 7\nnp.random.seed(seed)\nfrom time import time\nfrom keras.layers import Dense\nfrom keras import backend as K\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.model_selection import GridSearchCV\nfrom keras.models import Sequential\nfrom sklearn.model_selection import TimeSeriesSplit\n# Input parameters\nvar = 'btc'\nn_in = 5 #look back window\nn_out = 2 #forecast window\nbatch_size = 256\nepochs = 1\ndef labeler2D(x):\nif x>=0:\nreturn 1\nelse:\nreturn 0\ndef series_to_supervised_multi(data, n_in=1, n_days=1, dropnan=True): #index corresponds to (t)\ndf = data.copy()\ncols, names = list(),\n# input sequence (t-n, ... t-1)\nfor i in range(n_in, 0, -1):\ncols.append(df.shift(i))\nnames += [('%s(t-%d)' % (col,i)) for col in cols[0].columns.values]\nfor i in range(0, n_days):\ncols.append(df.shift(-i))\nif i == 0:\nnames += [('%s(t)' % col) for col in cols[0].columns.values]\nelse:\nnames += [('%s(t+%d)' % (col,i)) for col in cols[0].columns.values]\nagg = pd.concat(cols, axis=1)\nagg.columns = names\n# drop rows with NaN values\nif dropnan:\nagg.dropna(inplace=True)\nreturn agg\ndef create_table(var, data, exogene):\n#log returns\nreturns = np.log(data.loc[:,var].shift(-1)/data.loc[:,var])\nreturns = returns.dropna(axis = 0, how='all')\nreturns_crix = np.log(data.loc[:,'crix'].shift(-1)/data.loc[:,'crix'])\nreturns_crix = returns_crix.dropna(axis = 0, how='all')\ndataset = pd.DataFrame()\ndataset = data.loc[:, exogene].iloc[:-1,:]\ndataset[var] = returns.values\ndataset['ma_14_%s' % var] = returns.rolling(window=14,center=False).mean()\ndataset['ma_14_%s' % var].iloc[:13] = returns[:13].mean()\ndataset['ma_30_%s' % var] = returns.rolling(window=30,center=False).mean()\ndataset['ma_30_%s' % var].iloc[:29] = returns[:29].mean()\ndataset['ma_90_%s' % var] = returns.rolling(window=90,center=False).mean()\ndataset['ma_90_%s' % var].iloc[:89] = returns[:89].mean()\ndataset['Upper_14_%s' % var] = dataset['ma_14_%s' % var] + 2*returns.rolling(window=14,center=False).var()\ndataset['Upper_30_%s' % var].iloc[:29] = dataset['ma_30_%s' % var] + 2*returns[:29].var()\ndataset['Upper_90_%s' % var] = dataset['ma_90_%s' % var] + 2*returns.rolling(window=90,center=False).var()\ndataset['Lower_14_%s' % var].iloc[:13] = dataset['ma_14_%s' % var] - 2*returns[:13].var()\ndataset['Lower_30_%s' % var] = dataset['ma_30_%s' % var] - 2*returns.rolling(window=30,center=False).var()\ndataset['vol_%s' % var] = data.loc[:, 'vol_%s' % var]\nreturn dataset\ndef baseline_model():\nK.clear_session()\nmodel = Sequential()\nmodel.add(Dense(5, input_dim=input_.shape[1], kernel_initializer='uniform', activation = 'tanh'))\nmodel.add(Dense(num_classes, activation = 'softmax'))\nmodel.compile(optimizer='rmsprop',\nloss='sparse_categorical_crossentropy',\nmetrics = ['accuracy']\n)\nreturn model\nfrom sklearn.model_selection import cross_val_score\ndef create_model_two_layers(neurons1=1, neurons2=1):\nK.clear_session()\nmodel.add(Dense(neurons1, input_dim=input_.shape[1], kernel_initializer='uniform', activation = 'tanh'))\nloss='sparse_categorical_crossentropy',\nmetrics = ['accuracy']\n)\nprint(model.summary())\ndef create_model_three_layers(neurons1=1, neurons2=1,\nmodel.add(Dense(neurons3, kernel_initializer='uniform', activation = 'tanh'))\ndef create_model_four_layers(neurons1=1, neurons2=1,\nmodel.add(Dense(neurons4, kernel_initializer='uniform', activation = 'tanh'))\ndef create_model_ten_layers(neurons1=1, neurons2=1,\n):\nmodel.add(Dense(neurons5, kernel_initializer='uniform', activation = 'tanh'))\n#############################################\npath = open(wdir + \"/Data.csv\", \"r\")\ndata = pd.read_csv(path,header=0)\ndata.index = pd.to_datetime(data.Index)\ndata.drop(data.columns[[0]], axis=1,inplace = True)\nexogene=['EURIBOR_1Y', 'EURIBOR_6M',\n'Open_price_EuroUK',\n'Low_price_EuroUS', 'Open_price_USJPY',\n'Close_price_USJPY']\ncrix = ['crix']\ndataset = create_table(var, data, exogene)\nlabeler = labeler2D\nmodel_exogene=['crix', 'EURIBOR_1Y', 'Close_price_EuroUK',\nmodel_endogene = list(filter(lambda x: 'market' not in x,\nlist(filter(lambda x: 'vol' not in x,\nlist(filter(lambda x: '_btc' in x, dataset.columns))))))\nmodel_dataset = pd.DataFrame()\nmodel_dataset = dataset.loc[:, [var] + model_endogene + model_exogene]\ntrain_size = int(model_dataset.shape[0]*0.8)\ntrain = model_dataset.iloc[:train_size, :]\n\n#Preprocessing\nscaler = StandardScaler()\nscaler.fit(train)\ndata_scaled = pd.DataFrame(scaler.transform(train), index = train.index, columns = train.columns)\ndata_sup = series_to_supervised_multi(data_scaled.loc[:,['btc']], n_in-1, n_out+1)\nmodel_data = pd.concat([data_sup, data_scaled.iloc[(n_in-1):-n_out,:].loc[:, data_scaled.columns != 'btc']], axis = 1)\noutputs = list(filter(lambda x: '(t+' in x, model_data.columns))\ninput_ = model_data.loc[:,inputs].values\n\ntarget = model_data.loc[:,outputs]\n#unscaling\ntarget = ((target*scaler.scale_[0])+scaler.mean_[0]).values\ntarget = pd.DataFrame([item.sum() for item in target])\ntarget = np.apply_along_axis(labeler, 1, target)\nnum_classes = len(np.unique(target))\n#Baseline crossvalidation\nestimator = KerasClassifier(build_fn=baseline_model, epochs=50, batch_size=32, verbose=1)\nkfold = TimeSeriesSplit(n_splits=5)\nbase_results = cross_val_score(estimator, input_, target, cv=kfold)\nprint(\"Results: %.2f (%.2f) Accuracy\" % (base_results.mean(),\n# Two layers\nprint('Creation model')\nmodel = KerasClassifier(build_fn=create_model_two_layers, epochs=epochs, batch_size=batch_size, verbose=1)\n# define the grid search parameters\nneurons1 = [5, 10, 25, 50, 100]\nparam_grid = dict(neurons1=neurons1, neurons2 = neurons2)\n#Define CV\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1, verbose=2, cv = TimeSeriesSplit(n_splits=5).split(input_, target))\ngrid_result = grid.fit(input_, target)\nprint('End gridsearch')\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_,\nmeans = grid_result.cv_results_['mean_test_score']\nfor mean, stdev, param in zip(means, stds, params):\nprint(\"%f (%f) with: %r\" % (mean, stdev, param))\n#Three layers\nmodel2 = KerasClassifier(build_fn=create_model_three_layers, epochs=epochs, batch_size=batch_size, verbose=1)\nneurons1 = [5, 10, 25, 50]\nneurons2 = neurons1\nparam_grid = dict(neurons1=neurons1, neurons2 = neurons2, = neurons3)\nt1 = time()\ngrid2= GridSearchCV(estimator=model2, param_grid=param_grid, n_jobs=1, verbose=1, cv = TimeSeriesSplit(n_splits=5).split(input_, target))\ngrid_result2 = grid2.fit(input_, target)\nt2 = time()\nprint('End gridsearch, time: ', str(t2- t1))\nprint(\"Best: %f using %s\" % (grid_result2.best_score_,\nmeans2 = grid_result2.cv_results_['mean_test_score']\nfor mean, stdev, param in zip(means2, stds2, params2):\n# Four layers\nmodel3 = KerasClassifier(build_fn=create_model_four_layers, epochs=epochs, batch_size=batch_size, verbose=1)\nneurons1 = [25]\nneurons4 = [5, 8, 10, 25, 50]\nparam_grid = dict(neurons1=neurons1, neurons2 = neurons2, = neurons3, = neurons4)\ngrid3= GridSearchCV(estimator=model3, param_grid=param_grid, n_jobs=1, verbose=1, cv = TimeSeriesSplit(n_splits=5).split(input_, target))\ngrid_result3 = grid3.fit(input_, target)\nprint(\"Best: %f using %s\" % (grid_result3.best_score_,\nmeans3 = grid_result3.cv_results_['mean_test_score']\nfor mean, stdev, param in zip(means3, stds3, params3):\n\n#0.555128 using {'neurons1': 50, 'neurons2': 15, 'neurons3': 15, 'neurons4': 15}\n# 10 layers\nmodel4 = KerasClassifier(build_fn=create_model_ten_layers, epochs=epochs, batch_size=batch_size, verbose=1)\nneurons1 = [50]\nparam_grid = dict(neurons1=neurons1, neurons2 = neurons2, = neurons3, = neurons4,\nneurons9 = neurons5, = neurons5\n\ngrid4= GridSearchCV(estimator=model4, param_grid=param_grid, n_jobs=1, verbose=1, cv = TimeSeriesSplit(n_splits=5).split(input_, target))\ngrid_result4 = grid4.fit(input_, target)\nprint(\"Best: %f using %s\" % (grid_result4.best_score_,\nmeans4 = grid_result4.cv_results_['mean_test_score']\nfor mean, stdev, param in zip(means4, stds4, params4):\nprint(\"Baseline model cross val score: %.2f (%.2f) Accuracy\" % (base_results.mean(),\nprint('Two layers: epochs: ', epochs, 'accuracy: ', grid_result.best_score_,\n'''\nSelect best parameters from grid search\n", "output_sequence": "Tuning of different MLP architecture for BTC trend predictions"}, {"input_sequence": "install.packages(\"H:/Nonpara adaptive RF/lqa_1.0-3.tar\", type=\"source\", repos=NULL)\nlibrary(lqa)\n\nATE_est = function(fY,fp,fA){\n\nfw = (fp)^(-1)\nfw[fA==0] = (1 - fp[fA==0])^(-1)\nt_ATE = fY*fw\ntt_ATE = ( sum(t_ATE[fA==1]) / sum(fw[fA==1]) ) - ( sum(t_ATE[fA==0]) / sum(fw[fA==0]) )\nreturn(tt_ATE)\n}\nOAL <- function(data,var.list){\ncreate_weights = function(fp,fA,fw){\nfw = (fp)^(-1)\nfw[fA==0] = (1 - fp[fA==0])^(-1)\nreturn(fw)\n}\nwAMD_function = function(DataM,varlist,trt.var,wgt,beta){\ntrt = untrt = diff_vec = rep(NA,length(beta))\nnames(trt) = names(untrt) = names(diff_vec) = varlist\nfor(jj in 1:length(varlist)){\nthis.var = paste(\"w\",varlist[jj],sep=\"\")\nDataM[,this.var] = DataM[,varlist[jj]] * DataM[,wgt]\ntrt[jj] = sum( DataM[DataM[,trt.var]==1, this.var ]) / sum(DataM[DataM[,trt.var]==1, wgt])\ndiff_vec[jj] = abs( trt[jj] - untrt[jj] )\n}\nwdiff_vec = diff_vec * abs(beta)\nwAMD = c( sum(wdiff_vec))\nret = list( diff_vec = diff_vec, = wdiff_vec, wAMD = wAMD )\nreturn(ret)\n# estimate outcome model\ny.form = formula(paste(\"Y~A+\",paste(var.list,collapse=\"+\")))\nlm.Y = lm(y.form,data=data)\nbetaXY = coef(lm.Y)[var.list]\nlambda_vec = c( -10, -5, -0.75, 0.49)\nnames(lambda_vec) = as.character(lambda_vec)\n# lambda_n (n)^(gamma/2 - 1) = n^(gamma_convergence_factor)\ngamma_convergence_factor = 2\n# get the gamma value for each value in the lambda vector that corresponds to convergence factor\ngamma_vals = 2*( gamma_convergence_factor - lambda_vec + 1 )\nnames(gamma_vals) = names(lambda_vec)\n## Want to save ATE, wAMD and propensity score coefficients for each lambda value\nATE = wAMD_vec = rep(NA, length(lambda_vec))\nnames(ATE) = names(wAMD_vec) = names(lambda_vec)\ncoeff_XA = as.data.frame(matrix(NA,nrow=length(var.list),ncol=length(lambda_vec)))\nnames(coeff_XA) = names(lambda_vec)\nrownames(coeff_XA) = var.list\np_hat_OAL <- as.data.frame(matrix(NA,nrow(data),length(lambda_vec)))\ncolnames(p_hat_OAL) <- names(lambda_vec)\nw.full.form = formula(paste(\"A~\",paste(var.list,collapse=\"+\")))\nfor( lil in names(lambda_vec) ){\nil = lambda_vec[lil]\n\n### create the outcome adaptive lasso penalty with coefficient specific weights determined by outcome model\noal_pen = adaptive.lasso(lambda=nrow(data)^(il),al.weights = abs(betaXY)^(-ig) )\n### run outcome-adaptive lasso model with appropriate penalty\nlogit_oal = lqa.formula( w.full.form, data=data, penalty=oal_pen, family=binomial(logit) )\ndata[,paste(\"f.pA\",lil,sep=\"\")] = predict.lqa(logit_oal)$mu.new\n# save propensity score coefficients\ncoeff_XA[var.list,lil] = coef(logit_oal)[var.list]\n# create inverse probability of treatment weights\ndata[,paste(\"w\",lil,sep=\"\")] = create_weights(fp=data[,paste(\"f.pA\",lil,sep=\"\")],fA=data$A)\n# estimate weighted absolute mean different over all covaraites using this lambda to generate weights\nwAMD_vec[lil] = wAMD_function(DataM=data,varlist=var.list,trt.var=\"A\",\nwgt=paste(\"w\",lil,sep=\"\"),beta=betaXY)$wAMD\n# save ATE estimate for this lambda value\nATE[lil] = ATE_est(fY=data$Y,fp=data[,paste(\"f.pA\",lil,sep=\"\")],fA=data$A)\n} # close loop through lambda values\n# print out wAMD for all the lambda values tried\nwAMD_vec\n# find the lambda value that creates the smallest wAMD\ntt = which.min(wAMD_vec)\n# print out ATE corresponding to smallest wAMD value\nATE[tt]\n# print out the coefficients for the propensity score that corresponds with smalles wAMD value\ncoeff_XA[,tt]\nres <- list(ATE[tt],coeff_XA[,tt])\nreturn(res)\ncreatebootstrappedData <- function(df_boot) {\nsmpl_0 <- sample((1:nrow(df_boot))[df_boot$A == 0],\nreplace = TRUE,\nsize = sum(1 - df_boot$A))\nsmpl_1 <- sample((1:nrow(df_boot))[df_boot$A == 1],\nsize = sum(df_boot$A))\nsmpl <- sample(c(smpl_0, smpl_1))\nreturn(df_boot[smpl,])\nshortreed_est <- function(data, family = binomial()){\nOAL_res <- OAL(data,var.list)\nreturn(OAL_res[[1]])\n#' Function to do one bootstrap iteration of Shortreed-based estimators\n#' @param W Covariates\n#' @param Y Outcome\n#' @param family for outcome regression for glm\none_shortreed_boot <- function(data, family = binomial()){\nn <- nrow(data)\nidx <- sample(1:n, replace = TRUE)\nYij_vec <- sapply(1:n, function(x,idx){\nsum(idx == x)\n}, idx = idx)\ndata_boot <- data[idx,]\nOAL_res <- OAL(data_boot,var.list)\nt_star <- OAL_res[[1]]\ncoef_var <- abs(OAL_res[[2]])>0.00001\nreturn(list(Yij = Yij_vec, t_star = t_star,var_sel = coef_var))\nshortreed_boot <- function(data, nboot = 5e2, family = binomial()){\nrslt <- replicate(nboot, one_shortreed_boot(data, family = family))\nall_Yijs <- Reduce(rbind, rslt[1,])\nall_tstars <- apply(Reduce(rbind, rslt[2,]), 2, unlist, use.names= FALSE)\nall_covs <- apply(all_tstars, 2, function(tstar){\ncov(x, tstar, use = \"complete.obs\")\n})\nboot_sds <- apply(all_covs, 2, function(x){ sqrt(sum(x^2)) })\nboot_ests <- colMeans(all_tstars, na.rm = TRUE)\ncis <- rbind(boot_ests - 1.96*boot_sds, boot_ests + 1.96*boot_sds)\n# Covariate selection\nvar <- Reduce(rbind, rslt[3,])\nvar_mean <- apply(var,2,mean,na.rm=T)\nreturn(list(iptw = cis[,1],var_sel = var_mean ))\niptw <- function(data, formula = \"A ~ .\", family = binomial()){\nps_fit <- glm(as.formula(formula), data = data[,c(\"A\",var.list)], family = family)\ng1hat <- predict(ps_fit, type = \"response\")\nest <- sum(data$A * data$Y /(g1hat))/sum(data$A/g1hat) - sum((1-data$A)*data$Y/(1- g1hat))/sum((1-data$A)/(1-g1hat))\nreturn(est)\niptw_one_boot <- function(data, formula = \"A ~ .\", family = binomial()){\nresamp_idx <- sample(1:nrow(data), replace = TRUE)\nreturn(iptw(data=data[resamp_idx,],\nformula = formula, family = family))\niptw_boot_ci <- function(data, formula = \"A ~ .\", nboot = 5e2, family = binomial()){\nate_star <- replicate(nboot, iptw_one_boot(data, formula, family = family))\nreturn(as.numeric(quantile(ate_star, p = c(0.025, 0.975))))\n#' Help function to check whether true value in a CI\n#' @param truth The true value\n#' @param ci A two-length vector CI\ntruth_in_ci <- function(truth, ci){\ntruth > min(ci) & truth < max(ci)\n", "output_sequence": "Tuning of different MLP architecture for BTC trend predictions"}, {"input_sequence": "install.packages(vec.pac)\nvec.pac= c(\"mvtnorm\",\"clusterGeneration\",\n\"ranger\",\"scales\",\"MASS\",\"glmnet\",\"RRF\",\"lqa\",\"ggplot2\",\"cowplot\")\nlapply(vec.pac, require, character.only = TRUE)\n\nsv_all <- list()\nS <- 10\nfor(s in 1:S){\nB<- 200\nselect_var_full <- matrix(NA,p,B)\nfor( b in 1:B) {\n\nDGP <- datagen(setting=s, n=500,p=p)\ndata <- DGP[[1]]\nvar.list <- DGP[[2]]\nbA <- 0.5\n# Normalize covariates to have mean 0 and standard deviation 1\ntemp.mean = colMeans(data[,var.list])\nTemp.mean = matrix(temp.mean,ncol=length(var.list),nrow=nrow(data),byrow=TRUE)\ndata[,var.list] = data[,var.list] - Temp.mean\ntemp.sd = apply(data[var.list],FUN=sd,MARGIN=2)\nTemp.sd = matrix(temp.sd,ncol=length(var.list),nrow=nrow(data),byrow=TRUE)\ndata[var.list] = data[,var.list] / Temp.sd\nrm(list=c(\"temp.mean\",\"Temp.mean\",\"temp.sd\",\"Temp.sd\"))\n####################################################################\n###############\n# Bootstrap iterations\nRF_out <- all_RF(data)\n# Covariate selection\nselect_var_full[,b] <- RF_out[[2]][,1]\n#select_var_reg[,b] <- rep(1:p) %in% all_var\nselect_var_reg[,b] <- RF_out[[2]][,2]\n#}\n\n# OAL\nOAL_results <- OAL(data,var.list)\nselect_var_OAL[,b] <- abs(OAL_results[[2]])>0.00001\n#cat(\"This is iteration\", b, \"out of\", B, \"\\n\")\n}\nsv_nor=apply(select_var_full,1,mean)\nsv_reg_only <- apply(select_var_reg_only,1,mean)\nsv_OAL=apply(select_var_OAL,1,mean)\nsv <- as.data.frame(c(sv_nor,sv_reg,sv_reg_only,sv_OAL))\ncolnames(sv) <- c(\"value\")\nsv$Method <- factor(rep(c(\"RF full\",\"OARF\",\"RRF\",\"OAL\"),each=p),levels=c(\"OAL\",\"RF full\",\"OARF\",\"RRF\"))\nsv$Var <- rep(c(1:p))\n#cbp <- c(\"#000000\", \"#E69F00\", \"#56B4E9\", \"#009E73\",\n# \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")\nsv_all[[s]] <- sv_all\ncat(\"This is setting\", s, \"out of\", S, \"\\n\")\nggplot(sv,aes(y=value,x=Var,color=Method))+\ngeom_line(size=1) +\ngeom_point(aes(shape=Method)) +\ntheme_cowplot() +\nlabs(y=\"Proportion of times covariates selected \",x=\"Covariates\") +\nscale_color_manual(values=c(\"#000000\", \"#E69F00\",\"#56B4E9\",\"#009E73\"))\n", "output_sequence": "Tuning of different MLP architecture for BTC trend predictions"}, {"input_sequence": "# Bootstrapping\nall_RF <- function(data){\n\nvar.list <- colnames(data[,!(names(data) %in% c(\"Y\",\"A\"))])\np <- length(var.list)\nranger_full <- matrix(NA,2,1)\n\ntrain.idx <- sample(nrow(data), 0.5 * nrow(data))\nfor(i in 1:2){\n\nif(i==1){\ndata_train <- data[train.idx, ]\n}\nif(i==2){\ndata_train <- data[-train.idx, ]\n# ranger full\np_ranger_full <- ranger(y=as.factor(data_train$A),x=data_train[,var.list],importance=\"impurity\",probability = TRUE,num.trees = 500,min.node.size = 10)\np_hat_ranger_full <- predict(p_ranger_full,data=data_test[,var.list])$predictions[,2]\n#########################\n# ranger outcome model\ny.form = formula(paste(\"Y~A+\",paste(var.list,collapse=\"+\")))\ny_ranger_out <- ranger(y.form,data=data,importance=\"impurity_corrected\",num.trees = 1000, always.split.variables = \"A\")\n# standardize var.imp\nvar.imp_0 <- ifelse(y_ranger_out$variable.importance<0,0,y_ranger_out$variable.importance)\nvar.imp <- abs(round(var.imp_0/max(var.imp_0[-1]),3))\nvar.init <- ifelse(y_ranger_out$variable.importance >= mean(y_ranger_out$variable.importance[-1]),1,0)\n#select_outcome[,b] <- var.init[-1]\n# ranger guided and regularized\n#p_ranger_reg <- ranger(y=as.factor(data$A),x=data[,var.list],importance=\"impurity\",probability = TRUE,num.trees = 500,min.node.size = 10,\n# regularization.factor = (var.imp[-1]),regularization.usedepth=FALSE, always.split.variables = c(var.list[var.init[-1]==1]))\n# RRF\nvar.init_count <- which(var.init[-1]==1)\np_rrf <- RRF(y=as.factor(data$A),x=data[,var.list],flagReg=1,feaIni = var.init_count,coefReg = var.imp[-1],ntree = 1000 )\n# final ranger model with selected features\n#fea_sel <- names(which(p_ranger_reg$variable.importance>0))\nfea_sel <- var.list[p_rrf$feaSet]\n#print(fea_sel)\nif(length(fea_sel)<2){\nsub_train <- as.data.frame(data_train[,fea_sel])\ncolnames(sub_train) <- fea_sel\nsub_test <- as.data.frame(data_test[,fea_sel])\ncolnames(sub_test) <- fea_sel\np_ranger_select <- ranger(y=as.factor(data_train$A),x=sub_train,importance=\"impurity\",probability = TRUE,num.trees = 500,min.node.size = 10)\np_hat_ranger_reg <- predict(p_ranger_select,data=sub_test)$predictions[,2]\nif(length(fea_sel)>1){\np_ranger_select <- ranger(y=as.factor(data_train$A),x=data_train[,fea_sel],importance=\"impurity\",probability = TRUE,num.trees = 500,min.node.size = 10)\np_hat_ranger_reg <- predict(p_ranger_select,data=data_test[,fea_sel])$predictions[,2]\n##########################\n## Regularized RF without initial feature space\np_ranger_reg_only <- ranger(y=as.factor(data_train$A),x=data_train[,var.list],importance=\"impurity\",probability = TRUE,num.trees = 500,min.node.size = 10,\nregularization.factor = (var.imp[-1]),regularization.usedepth=FALSE)\np_hat_ranger_reg_only <- predict(p_ranger_reg_only,data=data_test[,var.list])$predictions[,2]\n# Overlap bounding\np_hat_ranger_full <-ifelse(p_hat_ranger_full<0.025, 0.025, ifelse(p_hat_ranger_full>.975,.975, p_hat_ranger_full)) # Overlap bounding\nranger_full[i,1] <- ATE_est(data_test$Y,p_hat_ranger_full,data_test$A)\nselect_var_full <- p_ranger_full$variable.importance/max(p_ranger_full$variable.importance)>0.05\n#select_var_reg[,b] <- rep(1:p) %in% all_var\nselect_var_reg_only <- p_ranger_reg_only$variable.importance/max(p_ranger_reg_only$variable.importance)>0.05\n#}\nite_ranger_full <- mean(ranger_full)\nrf_est <- cbind(ite_ranger_full,ite_ranger_reg,ite_ranger_reg_only)\ncolnames(rf_est) <- c(\"est_RF_full\",\"est_OARF\",\"est_RRF\" )\n\nres <- list(rf_est,cbind(select_var_full,select_var_reg, select_var_reg_only))\nreturn(res)\n}\n# Bootstrapping\nall_RF_boot <- function(data,verbose){\nite_ranger_full <- matrix(NA,nboot,1)\nselect_var_reg <- matrix(NA,p,nboot)\nfor(b in 1:nboot){\nset.seed(123+b)\ndata_boot <- createbootstrappedData(data)\nranger_full <- matrix(NA,2,1)\ntrain.idx <- sample(nrow(data_boot), 0.5 * nrow(data_boot))\nfor(i in 1:2){\n\nif(i==1){\ndata_train <- data_boot[train.idx, ]\n}\nif(i==2){\ndata_train <- data_boot[-train.idx, ]\n# ranger full\np_ranger_full <- ranger(y=as.factor(data_train$A),x=data_train[,var.list],importance=\"impurity\",probability = TRUE,num.trees = 500,min.node.size = 10)\np_hat_ranger_full <- predict(p_ranger_full,data=data_test[,var.list])$predictions[,2]\n#########################\n# ranger outcome model\ny.form = formula(paste(\"Y~A+\",paste(var.list,collapse=\"+\")))\ny_ranger_out <- ranger(y.form,data=data,importance=\"impurity_corrected\",num.trees = 1000, always.split.variables = \"A\")\n# standardize var.imp\nvar.imp_0 <- ifelse(y_ranger_out$variable.importance<0,0,y_ranger_out$variable.importance)\nvar.imp <- abs(round(var.imp_0/max(var.imp_0[-1]),3))\nvar.init <- ifelse(y_ranger_out$variable.importance >= mean(y_ranger_out$variable.importance[-1]),1,0)\n#var.init\n#select_outcome[,b] <- var.init[-1]\n# ranger guided and regularized\n#p_ranger_reg <- ranger(y=as.factor(data$A),x=data[,var.list],importance=\"impurity\",probability = TRUE,num.trees = 500,min.node.size = 10,\n# regularization.factor = (var.imp[-1]),regularization.usedepth=FALSE, always.split.variables = c(var.list[var.init[-1]==1]))\n# RRF\nvar.init_count <- which(var.init[-1]==1)\np_rrf <- RRF(y=as.factor(data$A),x=data[,var.list],flagReg=1,feaIni = var.init_count,coefReg = var.imp[-1],ntree = 1000 )\n# final ranger model with selected features\n#fea_sel <- names(which(p_ranger_reg$variable.importance>0))\nfea_sel <- var.list[p_rrf$feaSet]\n#print(fea_sel)\nif(length(fea_sel)<2){\nsub_train <- as.data.frame(data_train[,fea_sel])\ncolnames(sub_train) <- fea_sel\nsub_test <- as.data.frame(data_test[,fea_sel])\ncolnames(sub_test) <- fea_sel\np_ranger_select <- ranger(y=as.factor(data_train$A),x=sub_train,importance=\"impurity\",probability = TRUE,num.trees = 500,min.node.size = 10)\np_hat_ranger_reg <- predict(p_ranger_select,data=sub_test)$predictions[,2]\nif(length(fea_sel)>1){\np_ranger_select <- ranger(y=as.factor(data_train$A),x=data_train[,fea_sel],importance=\"impurity\",probability = TRUE,num.trees = 500,min.node.size = 10)\np_hat_ranger_reg <- predict(p_ranger_select,data=data_test[,fea_sel])$predictions[,2]\n##########################\n## Regularized RF without initial feature space\np_ranger_reg_only <- ranger(y=as.factor(data_train$A),x=data_train[,var.list],importance=\"impurity\",probability = TRUE,num.trees = 500,min.node.size = 10,\nregularization.factor = (var.imp[-1]),regularization.usedepth=FALSE)\np_hat_ranger_reg_only <- predict(p_ranger_reg_only,data=data_test[,var.list])$predictions[,2]\n# Overlap bounding\np_hat_ranger_full <-ifelse(p_hat_ranger_full<0.025, 0.025, ifelse(p_hat_ranger_full>.975,.975, p_hat_ranger_full)) # Overlap bounding\nranger_full[i,1] <- ATE_est(data_test$Y,p_hat_ranger_full,data_test$A)\n# Safe selected variables\n#rep(1:p) %in% p_rrf$feaSet\n#if(i==1){sel_var <- p_ranger_reg$variable.importance/max(p_ranger_reg$variable.importance)>0.05}\n#select_var_reg[,b] <- rep(1:p) %in% all_var\nselect_var_reg_only[,b] <- p_ranger_reg_only$variable.importance/max(p_ranger_reg_only$variable.importance)>0.05\n#}\nite_ranger_full[b,1] <- mean(ranger_full)\nif(verbose==T)\n{cat(\"This is iteration\", b, \"out of\", nboot, \"\\n\")}\nrf_boot <- cbind(ite_ranger_full,ite_ranger_reg,ite_ranger_reg_only)\n#boot_sds <- apply(rf_boot, 2, function(x){ sd(x) })\n#boot_ests <- colMeans(rf_boot, na.rm = TRUE) # smoothed bootstrap\nboot_q_lower <- apply(rf_boot, 2, function(x){ quantile(x,0.05) })\nci_all <- rbind(boot_q_lower,boot_q_upper)\n# Covariate selection\nsv_nor=apply(select_var_full,1,mean)\nsv_reg_only <- apply(select_var_reg_only,1,mean)\nres <- list(ci_all,cbind(sv_nor,sv_reg,sv_reg_only))\n", "output_sequence": "Tuning of different MLP architecture for BTC trend predictions"}, {"input_sequence": "library(ranger)\nlibrary(MASS)\nlibrary(ggplot2)\n\ninstall.packages(vec.pac)\nvec.pac= c(\"mvtnorm\",\"clusterGeneration\",\n\"ranger\",\"scales\",\"MASS\",\"glmnet\",\"RRF\",\"lqa\",\"foreach\",\"doParallel\")\nlapply(vec.pac, require, character.only = TRUE)\ncl <- makeCluster(5, outfile=\"\")\nregisterDoParallel(cl)\n# Generate new data.\n# Setting works quite ok with p <- 50\nB <- 250\nr <- foreach(sim = 1:B, .combine='rbind', .inorder=FALSE, .packages=vec.pac, .errorhandling =\"remove\" ) %dopar% {\n\ncoverage <- matrix(NA,1,5)\ncolnames(coverage) <- c(\"RF_full\", \"OARF\", \"IPTW\")\nDGP <- datagen(setting=1, n=1000,p=20)\ndata <- DGP[[1]]\nvar.list <- DGP[[2]]\nbA <- 0.5\n# Normalize covariates to have mean 0 and standard deviation 1\ntemp.mean = colMeans(data[,var.list])\nTemp.mean = matrix(temp.mean,ncol=length(var.list),nrow=nrow(data),byrow=TRUE)\ndata[,var.list] = data[,var.list] - Temp.mean\ntemp.sd = apply(data[var.list],FUN=sd,MARGIN=2)\nTemp.sd = matrix(temp.sd,ncol=length(var.list),nrow=nrow(data),byrow=TRUE)\ndata[var.list] = data[,var.list] / Temp.sd\nrm(list=c(\"temp.mean\",\"Temp.mean\",\"temp.sd\",\"Temp.sd\"))\n####################################################################\n###############\n# Bootstrap iterations\nnboot <- 500\n# RF, OARF and RRF\nRF_ci <- all_RF_boot(data=data,verbose=FALSE)\nfor(method in 1:3){\ncoverage[1,method] <- truth_in_ci(truth=bA,ci=RF_ci[[1]][,method])\n}\n# OAL\nOAL_ci <- shortreed_boot(data=data,nboot=nboot)\ncoverage[1,4] <- truth_in_ci(truth=bA,ci=OAL_ci[[1]])\n# IPTW\nIPTW_ci <- iptw_boot_ci(data=data,nboot=nboot)\ncoverage[1,5] <- truth_in_ci(truth=bA,ci=IPTW_ci)\n#cat(\"This is iteration\", sim, \"out of\", B, \"\\n\")\nreturn(coverage)\n}\ncolMeans(r)\n", "output_sequence": "Tuning of different MLP architecture for BTC trend predictions"}, {"input_sequence": "### This generates the Simulation Data as a dataframe\n# by Daniel Jacob (daniel.jacob@hu-berlin.de)\n\n# Arguments to specify are:\n# N = Number of observations (real number)\n# random_d = treatment assignment: (Either T for random assignment or F for confounding on X)\n# theta = treatment effect: (Either real number for only one theta, or \"binary\" {0.1,0.3} or \"con\" for continuous values (0.1,0.3))\n# var = Size of the variance (Noise-level)\n#Required Packages\nif(!require(\"clusterGeneration\")) install.packages(\"clusterGeneration\");\nif(!require(\"mvtnorm\")) install.packages(\"mvtnorm\"); library(\"mvtnorm\")\ndatagen <- function(p=20, n=1000,bA=0.5,setting,rho=0) {\n\nmean_x = 0\nsig_x = 1\nrho = 0\n# sample size\npC = pI = pP = 2\npS = p - (pC+pI+pP)\nvar.list = c(paste(\"Xc\",1:pC,sep=\"\"),paste(\"Xp\",1:pP,sep=\"\"),paste(\"Xi\",1:pI,sep=\"\"),paste(\"Xs\",1:pS,sep=\"\"))\n# Set strength of relationship between covariates and outcome\nbeta_v = c( 0.6, 0, rep(0,p-6) )\n# Set strength of relationship between covariates and treatment\nalpha_v = c( 1, 0, 1, rep(0,p-6))\n# Settings with more covariates\nif(setting==8 | setting==10){\npC = 6\npI = pP = 2\npS = p - (pC+pI+pP)\nvar.list = c(paste(\"Xc\",1:pC,sep=\"\"),paste(\"Xp\",1:pP,sep=\"\"),paste(\"Xi\",1:pI,sep=\"\"),paste(\"Xs\",1:pS,sep=\"\"))\n# Set strength of relationship between covariates and outcome\nbeta_v = c( 0.6, 0.6,0.6,0.6, rep(0,p-8) )\n# Set strength of relationship between covariates and treatment\nalpha_v = c( 1, 1,0,0,1,1, rep(0,p-10) )}\nnames(beta_v) = names(alpha_v) = var.list\n### set true average treatment effect\n### simulate data\nSigma_x = matrix(rho*sig_x^2,nrow=length(var.list),ncol=length(var.list))\ndiag(Sigma_x) = sig_x^2\nMean_x = rep(mean_x,length(var.list))\ndata = as.data.frame(mvrnorm(n = n,mu=Mean_x,Sigma = Sigma_x,empirical = FALSE))\nnames(data) = var.list\n\n### Options for D (m_(X))\nif(setting==1) {alpha_v = c( 1, 0, 1, rep(0,p-6))\ngA_x = rowSums(data[,var.list]*matrix(alpha_v,nrow=n,ncol=length(var.list),byrow=TRUE))} # Setting 1\nif(setting==2) {alpha_v = c( 0.4, 0, 0.4, rep(0,p-6) )\ngA_x = rowSums(data[,var.list]*matrix(alpha_v,nrow=n,ncol=length(var.list),byrow=TRUE))} # Setting 2\nif(setting==3) {alpha_v = c( 1, 0, 1, rep(0,p-6) )\ngA_x = rowSums(data[,var.list]*matrix(alpha_v,nrow=n,ncol=length(var.list),byrow=TRUE))} # Setting 3\nif(setting==4) {gA_x = data[,1]*(1-data[,2]) + data[,5]*(1-data[,6])} # Setting 4\nif(setting==6) {alpha_v = c( 1, 0, 1, rep(0,p-6))\ngA_x = 2*cos(data[,2]) + rowSums(data[,var.list]*matrix(alpha_v,nrow=n,ncol=length(var.list) ,byrow=TRUE))} # Setting 6\nif(setting==7) {gA_x = 2*(data[,1]>0)*(data[,2]>1) + 2*(data[,5]>0)*(data[,6]>1) + data[,1]*data[,6]} # Setting 7\nif(setting==8) {gA_x = 2*data[,2]*(1-data[,6]) + 2*data[,1]*(data[,9]>1) + 0.5*rowSums(data[,var.list]*matrix(alpha_v,nrow=n,ncol=length(var.list),byrow=TRUE))} # Setting 8\nif(setting==9) {gA_x = 0.5*(data[,1]^2) + 0.5*data[,2] - data[,3]*data[,4] + 0.5*data[,6] + 0.5*(data[,9]^2) + 0.5*data[,10]} # Setting 9\n# settings for gY_xA\nif(setting==1 | setting==2){gY_xA = rowSums(data[,var.list]*matrix(beta_v,nrow=n,ncol=length(var.list) ,byrow=TRUE))}\nif(setting==3 | setting==4 | setting==5 | setting==6 | setting==7){\ngY_xA = 0.8*(data[,1]*data[,2]) + 0.8*data[,3]*data[,4] }\nif(setting==8 | setting==10){\ngY_xA = 0.8*(data[,1]*data[,2]) + 0.8*data[,3]*data[,4] + 0.8*(data[,5]*data[,6]) + 0.8*data[,7]*data[,8]}\npA = pnorm(gA_x)\ndata$A = as.numeric(rbinom(n,1,pA)) # simulate A\ndata$Y = gY_xA + rnorm(n=n,sd=sig_x)\ndata$Y = data$Y + data$A*bA\nout <- list()\nout[[1]] <- data\nout[[2]] <- var.list\nreturn(out)\n}\n### Example\ndataset <- datagen(setting=1, n=500)\nsummary(dataset)\n", "output_sequence": "Tuning of different MLP architecture for BTC trend predictions"}, {"input_sequence": "# Clear all variables\nrm(list = ls(all = TRUE))\ngraphics.off()\n\n# Set directory\nsetwd(\"C:/Users/xinjueli/PAM/simulations\")\n# Install and load packages\nlibraries = c(\"MASS\", \"bbmle\", \"glmnet\", \"doParallel\", \"LaplacesDemon\", \"optimx\",\n\"lars\", \"scales\", \"tilting\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)} )\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\nsource(\"PAMfunc.r\")\n# Simulation setup\nn.obs = 500 # No of observations\nseed1 = 20171110 # Seed simulation X\nr = 0.5 # Correlation parameter of X\nsd.eps = 1 # Standard deviation of epsilon\ncp1.seq = matrix(c(50,150, 100,150), nrow = 2, ncol = 2, byrow = TRUE) # Define change points\na = 3.7 # Recommended value of parameter a for SCAD\nn.boot = 10 # Number of bootstrapped multipliers\nK = 50 # Increment between adjacent sub-intervals\nmb.type = \"Exp\" # Bound, Exp or Pois\nsel = 0 # Position of s\n# Initiate cluster for parallel computing\nn.cores = detectCores() # Number of cores to be used\ncl = makeCluster(n.cores)\nregisterDoParallel(cl)\ngetDoParWorkers()\n# Define number of scenarios in every core\nn.simcores = rep((n.sim %/% n.cores),\nif (h.simcores != 0){\nn.simcores[1:h.simcores] = n.simcores[1:h.simcores] + 1\n}\n# True beta coefficients with change in t = cp1\ntmp1.1 = rep(1, 5)\ntmp1.2 = rep(0, n.par - length(tmp1.1))\nb1 = c(tmp1.1, tmp1.2)\ntmp2.1 = rep(1, 3)\ntmp2.2 = rep(0, n.par - length(tmp2.1))\nb2 = c(tmp2.1, tmp2.2)\ntmp3.1 = rep(1, 5)\ntmp3.2 = rep(0, n.par - length(tmp1.1))\nb3 = c(tmp3.1, tmp3.2)\n# Simulation of the design matrix\nmu = rep(0, n.par)\nSigma = matrix(0, nrow = n.par, ncol = n.par)\nfor (i in 1:n.par) {\nif (i == j){\nSigma[i, j] = 1\n}else {\nSigma[i, j] = r^abs(i - j)\n}\nX = list()\nfor (i in 1:n.sim){\nset.seed(seed1)\nX[[i]] = mvrnorm(n = n.obs, mu, Sigma)\n}\n# Simulation of the error term for t = 1, ..., n.obs\neps = list()\nset.seed(seed2)\neps[[i]] = rnorm(n.obs, mean = 0, sd = sd.eps)\n}\n# Function for computing Y for t = 1, ..., n.obs\nY.sim = function(cp1, cp2){\n# Computation of Y for t = 1, ..., cp1\nY1 = list()\nfor (i in 1:n.sim){\nY.tmp = numeric(0)\nfor (j in 1:cp1){\nY.tmp = c(Y.tmp, b1 %*% X[[i]][j, ] + eps[[i]][j])\nY1[[i]] = Y.tmp\n\n# Computation of Y for t = cp1, ..., cp2\nY2 = list()\nfor (j in (cp1 + 1):cp2){\nY.tmp = c(Y.tmp, b2 %*% X[[i]][j, ] + eps[[i]][j])\nY2[[i]] = Y.tmp\n# Computation of Y for t = cp2, ..., n.obs\nY3 = list()\nfor (j in (cp2 + 1):n.obs){\nY.tmp = c(Y.tmp, b3 %*% X[[i]][j, ] + eps[[i]][j])\nY3[[i]] = Y.tmp\nY = list()\nY[[i]] = c(Y1[[i]], Y3[[i]])\nY\n# Simulating mutlipliers from a bounded distribution\nrunif.mod = function(n){\nunif1 = runif(n, min = 0, max = 12)\nunif.a = unif1[which(unif1 <= 1)]\nunif.b = unif1[which((1 < unif1) & (unif1 <= 4))]\nindex.c = which(unif1 > 4)\nunif2 = cbind(c(index.a, index.b, c(unif.a, unif.c))\nunif.fin = unif2[order(unif2[, 1], decreasing = FALSE), 2]\nunif.fin\nsim.change.points = function(interval.in, s, mb.type = c(\"Bound\", \"Exp\", \"Pois\")){\nb.pts = numeric()\ninterval.index = numeric()\nk = interval.in+1\nbeta = list()\nb.pts.tmp = 0\n\n# 1.step: Fit the model for assumed homogeneous interval I_t^(1)\nX.tmp.L = X[[s]][((k1 * K) + 1):(k2 * K), ]\nn.obs.tmp.L = length(Y.tmp.L)\nobject.tmp.L = Onestep.SCAD(X.tmp.L, Y.tmp.L, a, n.obs.tmp.L)\nbeta[[k]] = object.tmp.L$beta\nwhile (k < M){\n# Fit the model over the next interval I_t^(k + 1) - I_t^(k)\nX.tmp.R = X[[s]][((k2 * K) + 1):((k2 + 1) * K), ]\nn.obs.tmp.R = length(Y.tmp.R)\nobject.tmp.R = Onestep.SCAD(X.tmp.R, Y.tmp.R, a, n.obs.tmp.R)\nbeta.tmp.R = object.tmp.R$beta\n\n# Fit the model over the whole interval I_t^(k + 1)\nX.tmp.T = X[[s]][((k1 * K) + 1):((k2 + 1) * K), ]\nn.obs.tmp.T = length(Y.tmp.T)\nobject.tmp.T = Onestep.SCAD(X.tmp.T, Y.tmp.T, a, n.obs.tmp.T)\nbeta.tmp.T = object.tmp.T$beta\nlik.T = loglik.pen(a.tmp.T, beta0.tmp.T, lambda.tmp.T,\nX.tmp.T,\n# Simulation of multipliers u_i (i = 1, ..., n.obs.tmp.T)\nset.seed = 20170424 * k * s * 10\nif (mb.type == \"Bound\"){\nmultipliers = matrix(runif.mod(n.boot * n.obs.tmp.T),\nncol = n.obs.tmp.T, nrow = n.boot) # Bounded distribution\n}\nif (mb.type == \"Exp\"){\nmultipliers = matrix(rexp(n.boot * n.obs.tmp.T, rate = 1),\nncol = n.obs.tmp.T, nrow = n.boot) # Exp(1) distribution\nif (mb.type == \"Pois\"){\nmultipliers = matrix(rpois(n.boot * n.obs.tmp.T, lambda = 1),\nncol = n.obs.tmp.T, nrow = n.boot) # Pois(1) distribution\nMB.ratios = numeric(0)\n\n# 1.step: Fit the model for assumed homogeneous interval I_t^(k,s)\nX.sel.L = X[[s]][((k1 * K) + 1):(k2 * K ), ]\nn.obs.sel.L = length(Y.sel.L)\nobject.sel.L = Onestep.SCAD(X.sel.L, Y.sel.L, a, n.obs.sel.L)\nbeta.sel.L = object.sel.L$beta\n# 2. a) step: Fit the model over the next interval I_t^(k + 1,s) - I_t^(k,s)\nX.sel.R = X[[s]][((k2 * K) + 1 ):((k2 + 1) * K), ]\nn.obs.sel.R = length(Y.sel.R)\nobject.sel.R = Onestep.SCAD(X.sel.R, Y.sel.R, a, n.obs.sel.R)\nbeta.sel.R = object.sel.R$beta\n# 2. b) step: Fit the model over the whole interval I_t^(k + 1,s)\nX.sel.T = X.tmp.T\n# 3.step: Evaluate the test statistic\n# Real likelihood ratio\nlik.sel.L = loglik.pen(a.sel.L, beta0.sel.L, lambda.sel.L,\nX.sel.L,\nlik.sel.R = loglik.pen(a.sel.R, beta0.sel.R, lambda.sel.R,\nX.sel.R,\nlik.sel.T = lik.T\nlik.sel = c(lik.sel, (((n.obs.sel.L/n.obs.sel.T) * lik.sel.L)\n- lik.sel.T))\n# 4.step: Find quantiles of LR with use of MB\nfor (l in 1:(n.boot)){\n# Multiplier bootstrap for left-hand interval I_t^(k)\nmultipl.L = multipliers[l, 1:n.obs.sel.L]\n\nboot.object.L = Onestep.SCAD.MB(X.sel.L, Y.sel.L, a, n.obs.sel.L,\nas.numeric(lambda.sel.L), multipl.L)\nboot.beta.L = boot.object.L$beta\nlik.boot.L = loglik.pen.MB(boot.a.L, boot.beta0.L,\nas.numeric(lambda.sel.L), X.sel.L,\nmultipl.L)\n# Multiplier bootstrap for right-hand interval I_t^(k + 1) - I_t^(k)\nmultipl.R = multipliers[l, (n.obs.sel.L + 1):n.obs.sel.T]\nboot.object.R = Onestep.SCAD.MB(X.sel.R, Y.sel.R, a, n.obs.sel.R,\nas.numeric(lambda.sel.R), multipl.R)\nboot.beta.R = boot.object.R$beta\nlik.boot.R = loglik.pen.MB(boot.a.R, boot.beta0.R,\nas.numeric(lambda.sel.R), X.sel.R,\nmultipl.R)\n# Multiplier bootstrap for the whole interval I_t^(k + 1) (with shift)\nmultipl = multipliers[l, ]\nboot.object.T = Onestep.SCAD.MB.shift(X.sel.L, Y.sel.L, a,\nlambda.sel.L, multipl.L,\nboot.beta.T = boot.object.T$beta\nY.boot.T = c(Y.sel.L, - rep((a.sel.R - a.sel.L), n.obs.sel.R)\n- X.sel.R %*% (beta.sel.R - beta.sel.L)))\nlik.boot.T = loglik.pen.MB(boot.a.T, boot.beta0.T,\nboot.lambda.T, X.sel.T, Y.boot.T, multipl)\nlik.ratio = c(lik.ratio, ((n.obs.sel.L/n.obs.sel.T) * lik.boot.L\n- lik.boot.T))\n}\nMB.ratios = cbind(MB.ratios, lik.ratio)\n# Find maximum over the real and bootstrapped likelihood ratios\nt.stat = max(lik.sel)\nMB.ratios.max = apply(MB.ratios, 1, max)\nq90 = quantile(MB.ratios.max, probs = 1-(0.1*k/M))\nk2 = k2 + 1\n# 5.step: Test for homogeneity and evaluate current beta estimator\nif (t.stat <= q95){\nfor (k3 in (k1 + 1):k){\nbeta[[k3]] = beta.tmp.T\n} else {\nb.pts.tmp = (k - 1) * K + 1\nk1 = k2 - 1\nbeta[[k]] = beta.tmp.R\nindex = which(beta[[(k)]]<= 0.2*max(beta[[(k)]]))\nBETA = beta[[(k)]]\nBETA[index] = 0\nif(k<=2){\nratio = 1-(length(which(BETA[6:10]!= 0))/length(b1))}\nif(k>2 && k<=3){\nratio = 1-(length(which(BETA[4:10]!= 0))/length(b1))}\nelse{\nratio = 1-(length(which(BETA[6:10]!= 0))/length(b1))}\nbreak\n}\nif(b.pts.tmp == 0){\nsim.no = s\nb.pts[sim.no] = 500\nindex = which(beta.tmp.T <= 0.4*max(beta.tmp.T))\nBeta.r[sim.no] = beta.tmp.T\nratio.sele[sim.no] = 1-(length(which((Beta.r[[sim.no]][6:10])!= 0))/length(b1))\ninterval.index[sim.no] = k\nvalue = list(b.pts[sim.no], ratio.sele[sim.no], Beta.r[sim.no], interval.index[sim.no])\nreturn(value)\nelse{\nSys.time()\nsim.no = s\nb.pts[sim.no] = b.pts.tmp\nratio.sele[sim.no] = ratio\nBeta.r[sim.no] = BETA\ninterval.index[sim.no] = k-1\nvalue = list(b.pts[sim.no], ratio.sele[sim.no], Beta.r[sim.no], interval.index[sim.no])\nresults = function(h){\nvalue.interval.1 = list()\na1 = matrix(0, 50, 2)\ns1 = ifelse(h == 1, sum(n.simcores[1:(h - 1)],1))\ns2 = sum(n.simcores[1:h])\nfor(i in s1:s2){\nvalue.interval = list()\nvalues1 = sim.change.points(0, i, mb.type)\nj = values1[[4]]\nvalue.interval = j\nvalue.ratio = values1[[2]]\nwhile(j < 9){\nvalues1 = sim.change.points(j, i, mb.type)\nvalue.interval = c(value.interval, values1[[4]])\nj = values1[[4]]}\nvalue.interval.1[[i]] = value.interval\nif(c(2, 3)%in%value.interval == c(T,T)){a1[i, ]=c(1,1)}\nelse{a1[i, ]=c(0,0)}\nvalue.ratio.1[[i]] = value.ratio\nif(c(2, 3)%in%value.interval == c(T,T)){\na2[i, ] = value.ratio[c(1,2)]}\nelse{a2[i, ]=c(0,0)}\nValue.final = list(a1[s1:s2, ], a2[s1:s2, ])\nnames(Value.final) = list(\"interval.index\", \"selection.v.ratio\")\nreturn(Value.final)\nout.sum = list()\nfinal.list = list()\nSys.time()\nY = Y.sim(100, 150)\nfinal.chpts = foreach(ipar = 1:n.cores,\n.packages = c(\"MASS\", \"bbmle\", \"glmnet\", \"doParallel\",\n\"LaplacesDemon\", \"optimx\", \"lars\", \"scales\",\n\"tilting\"),\n.combine = 'c') %dopar% results(ipar)\nfinal.list = final.chpts\nstopCluster(cl)\nsubinterval = final.list[[1]]\nfor(k in 2:n.cores){\nsubinterval = rbind(subinterval, final.list[[(2*k-1)]])\nRatio.select = final.list[[2]]\nRatio.select = rbind(Ratio.select, final.list[[(2*k)]])\ncorr.ratio.structural.break.1 = sum(subinterval[, 1])/n.sim\ncorr.ratio.v.selection.1 = mean(Ratio.select[, 1])\ncorr.ratio.structural.break.1\ncorr.ratio.v.selection.1\n", "output_sequence": "\u2018Performs the Penalized Adaptive Method (PAM), a combination of propagation-separation approach and a SCAD penalty, to fit a model to a simulated data with two pre-defined change points. Computes the percentage of correctly identifing the homogeneous intervals.\u2019"}, {"input_sequence": "# -------------------------------------------------------------------------------\n# One-step SCAD and the multiplier bootstrap\n# -------------------------------------------------------------------------------\n\n# Define function for computing derivative of the SCAD penalty\npen.prime = function(beta, lambda, a){\nindicator = ifelse(abs(beta) <= lambda, 1, 0)\ntmp = numeric(0)\nfor (jpp in 1:length(beta)){\ntmp[jpp] = max(a * lambda - abs(beta[jpp]), 0)\n}\npen.value = (lambda * indicator) + ((tmp/(a - 1)) * (1 - indicator))\npen.value\n}\n# Define function for computing the SCAD penalty\npen.scad = function(beta, lambda, a){\npen.value = numeric(0)\nfor (jps in 1:length(beta)){\nif (abs(beta[jps]) <= lambda){\npen.value[jps] = lambda * abs(beta[jps])\n} else if (lambda < abs(beta[jps]) && abs(beta[jps]) <= a * lambda){\npen.value[jps] = - ((abs(beta[jps])^2 - 2 * a * lambda * abs(beta[jps]) + lambda^2)/\n(2 * (a - 1)))\n} else {\npen.value[jps] = ((a + 1) * lambda^2)/2\n}\n# Define function for submatrix X_U\nXU.fct = function(X, beta, lambda, a){\npen = pen.prime(beta, lambda, a) # Evaluate penalty function\nif (min(pen) == 0){\nU.index = which(pen == 0)\nX.U = as.matrix(X[, U.index])\n} else {\nX.U = matrix(c(-Inf, -Inf, nrow = 2, ncol = 2)\nX.U\n# Define function for submatrix X_V\nXV.fct = function(X, beta, lambda, a){\npen = pen.prime(beta, lambda, a) # Evaluate penalty function\nif (max(pen) == 0){\nX.V = matrix(c(-Inf, -Inf, nrow = 2, ncol = 2)\nbeta.new = numeric(0)\nV.index = which(pen != 0)\npen.new = pen[V.index]\nX.V = as.matrix(X[, V.index])\nfor (jxv in 1:length(V.index)){\nX.V[, jxv] = X.V[, jxv] * lambda/ (pen.new[jxv])\nvalues = list(X.V, beta.new)\nnames(values) = c(\"Xmatrix\", \"betapar\")\nreturn(values)\n# Define function for columns' order in X_U and X_V\norder.fct = function(X, beta, lambda, a){\npen = pen.prime(beta, lambda, a) # Evaluate penalty function\norder = seq(1, dim(X)[2], 1)\nU.index = which(pen == 0)\nU.order = order[U.index]\norder.new = c(U.order, V.order)\n\norder.new\n# Define function for finding real X.V coefficients\nV.coeff.fct = function(coeff, beta, lambda, a){\npen = pen.prime(beta, lambda, a) # Evaluate penalty function\npen.new = pen[V.index]\nV.coeff = numeric(0)\nfor (jvc in 1:length(V.index)){\nV.coeff[jvc] = coeff[jvc] * lambda / (pen.new[jvc]) # Transform coeff.'s back\nV.coeff\n# Define function to select a grid of lambda values\ngrid.fct = function(n.obs, Y, X){\none = rep(1, n.obs)\nX.mean = drop(one %*% X)/n.obs\nX.cent = scale(X, X.mean, FALSE)\nX.norm = sqrt(drop(one %*% (X.cent^2)/n.obs))\nX.maxl = scale(X.cent, FALSE, X.norm)\nY.mean = drop(one %*% Y)/n.obs\nY.maxl = scale(Y, Y.mean, FALSE)\nmax.lambda.tmp = numeric(0)\nfor (j in 1:dim(X.maxl)[2]){\nmax.lambda.tmp = c(max.lambda.tmp, t(as.vector(Y.maxl)) %*% X.maxl[, j])\nmax.lambda = max.lambda.tmp[which.max(max.lambda.tmp)]\nlambda.grid = c(max.lambda/(n.obs^seq(0.49, 0.01, -0.03)*1000)\nlambda.grid\n# Define function to select a grid of lambda values for the prolonged interval\n# under homogeneity\ngrid.fct.T = function(n.obs, Y, X){\nn.obs.L = length(Y)\none = rep(1, n.obs.L)\nX.mean = drop(one %*% X)/n.obs.L\nX.norm = sqrt(drop(one %*% (X.cent^2)/n.obs.L))\nY.mean = drop(one %*% Y)/n.obs.L\n, max.lambda/(n.obs^seq(0.49, 0.01, -0.03)/100))\n# Define function to compute One-step SCAD algorithm, Li & Zou (2008)\nOnestep.SCAD = function(X, Y, a, n.obs){\nbeta.tmp = list()\nlambda.grid = grid.fct(n.obs, Y, X)\n# Normalize design X (mean = 0, var = 1) and response Y (mean = 0)\nX.orig = X\nX.norm0 = sqrt(drop(one %*% (X.cent^2)/n.obs))\nX.norm = scale(X.cent, FALSE, X.norm0)\nX = X.norm\n# 0. STEP: Find initial values of coefficients - unpenalized fit\nobject.ols = glmnet(X.norm2, Y.orig, family = \"gaussian\", alpha = 0, lambda = 0,\nstandardize = FALSE, intercept = TRUE)\n# Beta with normalisation - to be used in the next step\nbeta.fit.ols = as.vector(object.ols$beta)\n# Original unpenalized coefficients\nbeta.0 = as.vector(object.ols$beta)/X.norm0\na.0 = object.ols$a0\nmu0.hat = X.norm %*% beta.fit.ols\n# 1. STEP: Create working data\n# 1.a)\nY.star = mu0.hat # Create response according to the inital fit (not centralised)\nX.star = as.matrix(X.norm) # Scaled but nor centralised matrix X\n# Compute the best couple (lambda, beta(lambda)) for the algorithm\nfor (llam in 1:length(lambda.grid)){\n\nlambda1 = lambda.grid[llam]\n# 1.b)\nX.U = XU.fct(X.star, beta.fit.ols, lambda1, a)\nX.V = XV.tmp$Xmatrix\nord = order.fct(X.star, beta.fit.ols, lambda1, a)\n# 1.c)\nif (X.V[1, 1] != -Inf){\nY.2star = Y.star - X.U %*% ginv(t(X.U) %*% X.U) %*% t(X.U) %*% Y.star\n} else {\nY.2star = Y.star\n}\n# 2. STEP: Lasso estimation using CV\nif (dim(X.V)[2] == 1){\nV.coeff.tmp = ifelse(abs(V.beta) <= 2 * lambda1,\nsign(V.beta) * max((abs(V.beta) - lambda1), 0),\n((a - 1) * V.beta - sign(V.beta) * a * lambda1)/(a - 2))\n\nV.coeff = V.coeff.tmp\nobject = glmnet(X.V2star, Y.2star, family = \"gaussian\", alpha = 1,\nlambda = lambda1, standardize = FALSE, intercept = FALSE)\nV.coeff.tmp = as.vector(object$beta) # Extract coefficients from the fit\nV.coeff = V.coeff.fct(V.coeff.tmp, beta.fit.ols, lambda1, a)\n# 3. STEP: Coefficients associated with X_U and X_V\nU.coeff = as.vector(ginv(t(X.U) %*% X.U) %*% t(X.U)\n%*% (Y.star - X.V %*% V.coeff.tmp))\nU.coeff = numeric(0)\ncoeff.tmp2 = rbind(ord, c(U.coeff, V.coeff)) # Fitted coefficients\ncoeff = coeff.tmp2[2, order(coeff.tmp2[1, ])]\nact.set.tmp = sum(coeff != 0) # No. of nonzero coefficients\nbeta.fit = coeff/X.norm0\n} else if (X.V[1, 1] == -Inf){\n# 2. STEP: Skip - no V coefficients\nU.coeff = as.vector(ginv(t(X.U) %*% X.U) %*% t(X.U) %*% Y.star)\nV.coeff = numeric(0)\nbeta.fit = coeff/X.norm0\nact.set[[llam]] = act.set.tmp\nbeta.tmp[[llam]] = beta.fit\na0.tmp[[llam]] = Y.mean - (X.mean %*% beta.tmp[[llam]])\nbic = numeric(0)\nfor (lbic in 1:length(lambda.grid)){\nbic[lbic] = (log(n.obs) * act.set[[lbic]]/n.obs * max(log(log(n.par)),\nsqrt(n.obs)/n.par)\n+ log((t(Y.orig - rep(a0.tmp[[lbic]], n.obs) -\nX.orig %*% beta.tmp[[lbic]]) %*%\nindex = which.min(bic)\nvalues = list(a.0, a0.tmp[[index]], beta.0, beta.tmp[[index]],\nlambda.grid[index], act.set[[index]], index, bic[index])\nnames(values) = c(\"a.0\", \"a\", \"beta.0\", \"lambda\", \"act.set\", \"index\", \"bic\")\n# Define function to compute One-step SCAD algorithm combined with multiplier bootstrap\nOnestep.SCAD.MB = function(X, Y, a, n.obs, lambda.value, multipl){\nbeta.tmp = list()\nX.mb.mean.vector = t((t(X.orig) %*% multipl))/sum(multipl)\nX.mb.mean.matrix = matrix(rep(X.mb.mean.vector, n.obs),\nncol= length(X.mb.mean.vector), byrow = TRUE)\nX.mb = (X.orig - X.mb.mean.matrix) * sqrt(multipl)\nY.mb = (Y.orig - ((t(Y.orig) %*% multipl)/sum(multipl))) * sqrt(multipl)\nX.mbnorm = scale(X.mb, FALSE, X.norm0)\nobject.ols = glmnet(X.mbnorm, Y.mb, family = \"gaussian\", alpha = 0, lambda = 0,\nstandardize = FALSE, intercept = FALSE)\n# Original OLS coefficients\na.0 = (((t(Y.orig) %*% multipl)/sum(multipl))\n- ((t((t(X.orig) %*% multipl)) %*% beta.0)/sum(multipl)))\nmu0.hat = X.mbnorm %*% beta.fit.ols # Initial fit\nY.star = mu0.hat # Create response according to the inital fit\nX.star = X.mbnorm\nlambda = lambda.value\n# 1.b)\nX.U = XU.fct(X.star, beta.fit.ols, lambda, a)\nX.V = XV.tmp$Xmatrix\nV.beta = XV.tmp$betapar\nord = order.fct(X.star, beta.fit.ols, lambda, a)\n# 1.c)\nif (X.V[1, 1] != -Inf){\nY.2star = Y.star - X.U %*% ginv(t(X.U) %*% X.U) %*% t(X.U) %*% Y.star\nY.2star = Y.star\n# 2. STEP: Lasso estimation using CV\nif (dim(X.V)[2] == 1){\nV.coeff.tmp = ifelse(abs(V.beta) <= 2 * lambda,\nsign(V.beta) * max((abs(V.beta) - lambda), 0),\n((a - 1) * V.beta - sign(V.beta) * a * lambda)/(a - 2))\nV.coeff = V.coeff.tmp\nobject = glmnet(X.V2star, Y.2star, family = \"gaussian\", alpha = 1,\nlambda = lambda.value, standardize = FALSE,\nintercept = FALSE)\nV.coeff.tmp = as.vector(object$beta) # Extract coefficients from the fit\nV.coeff = V.coeff.fct(V.coeff.tmp, beta.fit.ols, lambda, a)\n# 3. STEP: Coefficients associated with X_U and X_V\nU.coeff = as.vector(ginv(t(X.U) %*% X.U) %*% t(X.U)\n%*% (Y.star - X.V %*% V.coeff.tmp))\nU.coeff = numeric(0)\ncoeff.tmp2 = rbind(ord, c(U.coeff, V.coeff)) # Fitted coefficients\ncoeff = coeff.tmp2[2, order(coeff.tmp2[1, ])]\nact.set = sum(coeff != 0) # No. of nonzero coefficients\nbeta.fit = coeff/X.norm0\n} else if (X.V[1, 1] == -Inf){\n# 2. STEP: Skip - no V coefficients\nU.coeff = as.vector(ginv(t(X.U) %*% X.U) %*% t(X.U) %*% Y.star)\nV.coeff = numeric(0)\nbeta.fit = coeff/X.norm0\nlambda.fit = lambda.value\na0.tmp = (((t(Y.orig) %*% multipl)/sum(multipl))\n- ((t((t(X.orig) %*% multipl)) %*% beta.fit)/sum(multipl)))\nbic = (log(n.obs) * act.set/n.obs * max(log(log(n.par)), sqrt(n.obs)/n.par)\n+ log((t(Y.mb - X.mb %*% beta.fit) %*% (Y.mb - X.mb %*% beta.fit))/n.obs))\nvalues = list(a.0, a0.tmp, beta.0, beta.fit, lambda.fit, act.set, Y.mean, bic)\nnames(values) = c(\"a.0\", \"a\", \"beta.0\", \"lambda\", \"act.set\", \"intercept\", \"bic\")\n# Define function to compute One-step SCAD algorithm for the prolonged interval\nOnestep.SCAD.MB.shift = function(X.tmp.L, Y.tmp.L, a, lambda.L,\nlambda.R, multipl.L, beta.L,\na.L, a.R){\nbeta.tmp = list()\nn.obs.L = length(Y.tmp.L)\n# Normalize design X.T (mean = 0, var = 1) and response Y.T (mean = 0)\nY.orig.T = c(Y.tmp.L, Y.tmp.R)\nmultipl.T = c(multipl.L,\nn.obs.T = n.obs.L + n.obs.R\none.T = rep(1, n.obs.T)\nX.mean.T = drop(one.T %*% X.orig.T)/n.obs.T\nX.cent.T = scale(X.orig.T, X.mean.T, FALSE)\nX.norm0.T = sqrt(drop(one.T %*% (X.cent.T^2)/n.obs.T))\nX.norm.T = scale(X.cent.T, FALSE, X.norm0.T)\nX.mb.mean.vector = t((t(X.orig.T) %*% multipl.T))/sum(multipl.T)\nX.mb.mean.matrix = matrix(rep(X.mb.mean.vector, n.obs.T),\nX.mb.T = (X.orig.T - X.mb.mean.matrix) * sqrt(multipl.T)\nX.mbnorm.T = scale(X.mb.T, FALSE, X.norm0.T)\nY.T = c(Y.tmp.L, - (rep((a.R - a.L), n.obs.R)\n+ X.tmp.R %*% (beta.R - beta.L))))\nY.mb.T = (Y.T - ((t(Y.T) %*% multipl.T)/sum(multipl.T))) * sqrt(multipl.T)\n# 0. STEP: Find initial values of coefficients - penalized fit\nobject.ols = glmnet(X.mbnorm.T, Y.mb.T, family = \"gaussian\", alpha = 0, lambda = 0,\nstandardize = FALSE, intercept = FALSE)\nbeta.0 = as.vector(object.ols$beta)/X.norm0.T\na.0 = (((t(Y.T) %*% multipl.T)/sum(multipl.T))\n- ((t((t(X.orig.T) %*% multipl.T)) %*% beta.0)/sum(multipl.T)))\nmu0.hat = X.mbnorm.T %*% beta.fit.ols # Initial fit\nY.star = mu0.hat # Create response according to the inital fit\nX.star = X.mbnorm.T\nindex = which(grid.fct(length(Y.tmp.L), Y.tmp.L, == lambda.L)\nlambda1 = grid.fct.T(length(Y.star), Y.tmp.L, X.tmp.L)[index]\nX.U = XU.fct(X.star, beta.fit.ols, lambda1, a)\nX.V = XV.tmp$Xmatrix\nord = order.fct(X.star, beta.fit.ols, lambda1, a)\nV.coeff.tmp = ifelse(abs(V.beta) <= 2 * lambda1,\nsign(V.beta) * max((abs(V.beta) - lambda1), 0),\n((a - 1) * V.beta - sign(V.beta) * a * lambda1)/(a - 2))\nlambda = lambda1, standardize = FALSE, intercept = FALSE)\nV.coeff = V.coeff.fct(V.coeff.tmp, beta.fit.ols, lambda1, a)\nU.coeff = as.vector(ginv(t(X.U) %*% X.U) %*% t(X.U)\nbeta.fit = coeff/X.norm0.T\ncoeff.tmp2 = rbind(ord, c(U.coeff, V.coeff)) # Fitted coefficients\ncoeff = coeff.tmp2[2, order(coeff.tmp2[1, ])]\nbeta.fit = coeff/X.norm0.T\nlambda.fit = lambda1\na0.tmp = (((t(Y.T) %*% multipl.T)/sum(multipl.T))\n- ((t((t(X.orig.T) %*% multipl.T)) %*% beta.fit)/sum(multipl.T)))\nact.set = sum(beta.fit != 0)\nvalues = list(a.0, a0.tmp, beta.0, beta.fit, lambda.fit, act.set)\nnames(values) = c(\"a.0\", \"a\", \"beta.0\", \"lambda\", \"act.set\")\n# Define function to compute bootstrapped penalized log-likelihood function\nloglik.pen.MB = function(a0, beta.0, lambda, X, Y, multipl){\nn.obs = length(Y)\none = rep(1, n.obs)\nX.mean = drop(one %*% X)/n.obs\nX.cent = scale(X, X.mean, FALSE)\nX.norm0 = sqrt(drop(one %*% (X.cent^2)/n.obs))\nX = X * sqrt(multipl)\nvar.eps = 1\nloglik1 = (- (1/(n.obs * 2 * var.eps) * t(Y - X %*% beta) %*% (Y - X %*% beta))\n- (t(pen.prime(beta.0 * X.norm0, lambda, a)) %*% abs(beta * X.norm0)))\nloglik1\n# Define function to compute bootstrapped unpenalized log-likelihood function\nloglik.MB = function(a0, beta, X, Y, multipl){\nn.obs = length(Y)\nX = X * sqrt(multipl)\nloglik1 = (- (1/(n.obs * 2) * t(Y - X %*% beta) %*% (Y - X %*% beta)))\n# Define function to compute penalized log-likelihood function\nloglik.pen = function(a0, beta.0, lambda, X, Y){\nloglik1 = (- (1/(n.obs * 2) *\n(t(Y - rep(a0, n.obs) - X %*% beta)\n# Define function to compute unpenalized log-likelihood function\nloglik = function(a0, beta, X, Y){\nloglik1 = (- (1/(n.obs * 2 ) * t(Y - rep(a0, n.obs)\n- (X %*% beta)) %*% (Y - rep(a0, n.obs)\n", "output_sequence": "\u2018Performs the Penalized Adaptive Method (PAM), a combination of propagation-separation approach and a SCAD penalty, to fit a model to a simulated data with two pre-defined change points. Computes the percentage of correctly identifing the homogeneous intervals.\u2019"}, {"input_sequence": "# Importing data\nimport pandas as pd\nindices = pd.read_excel(r'//Users/apple/Desktop/indices.xlsx')\nimport numpy as np\nfrom pandas import Series, DataFrame\nimport scipy\n# Descriptive Statistics\ndescribe=indices.describe()\n# Plotting\nplt.plot(indices.Date, indices.CRIX)\nplt.title('CRIX Values')\nplt.xlabel('Year')\nplt.ylabel('Values, $')\nplt.plot(indices.Date, indices.F5)\nplt.legend([\"CRIX\", \"F5\", \"Bitwise10\", \"CCI30\"], loc= (1.1,0.5))\nplt.title(\"Index Values\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Values, $\")\n# Standardization of index values\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\nTotal_Market_scaled = preprocessing.scale(indices.Total_Market)\n# Plotting standardized values against total market\nplt.plot(indices.Date, Total_Market_scaled)\nplt.legend([\"Total Market\", \"CRIX\"], loc = (1.1,0.5))\nplt.title(\"Standardized Values\")\nplt.ylabel(\"Values\")\nplt.plot(indices.Date, F5_scaled)\nplt.legend([\"Total Market\", \"F5\"], loc = (1.1,0.5))\nplt.plot(indices.Date, Bitwise10_scaled)\nplt.legend([\"Total Market\", \"Bitwise10\"], loc = (1.1,0.5))\nplt.plot(indices.Date, Bitwise20_scaled)\nplt.legend([\"Total Market\", \"Bitwise20\"], loc = (1.1,0.5))\nplt.plot(indices.Date, Bitwise70_scaled)\nplt.legend([\"Total Market\", \"Bitwise70\"], loc = (1.1,0.5))\nplt.plot(indices.Date, Bitwise100_scaled)\nplt.legend([\"Total Market\", \"Bitwise100\"], loc = (1.1,0.5))\nplt.plot(indices.Date, CCI30_scaled)\nplt.legend([\"Total Market\", \"CCI30\"], loc = (1.1,0.5))\n# Pairplots of index values\nA=indices[['Total_Market', 'CRIX', 'F5', 'Bitwise10', 'CCI30']]\n# Correlations of standardized index values\nindices['Total_Market_s'] = Total_Market_scaled\nindices['CRIX_s'] = CRIX_scaled\nindices['Bitwise10_s'] = Bitwise10_scaled\nplt.figure(figsize=(16,10))\nfig,axis=plt.subplots()\nsb.heatmap(scaled.corr(), mask=mask, annot=True, annot_kws={\"size\": 10}, fmt=\".2f\", cmap='RdBu_r', vmin=-1, vmax=1)\nplt.xticks(fontsize=12, rotation=60)\nplt.title(\"Correlation Coefficients (standardized values)\")\naxis.set_xticklabels(['Total Market', 'CRIX', 'F5', 'Bitwise10', 'CCI30'])\n# Computation of log returns\nindices['Total_Market_log_return'] = np.log(indices['Total_Market']/indices['Total_Market'].shift(1))\n# Plotting log returns\nplt.plot(indices.Date, indices.Total_Market_log_return)\nplt.title(\"Log Returns\")\nplt.ylabel(\"returns\")\nplt.legend([\"Total Market\"], loc= (1.1,0.5))\nplt.plot(indices.Date, indices.CRIX_log_return)\nplt.legend([\"CRIX\"], loc= (1.1,0.5))\nplt.legend([\"Total Market\", \"CRIX\"], loc= (1.1,0.5))\n# Pairplots of the log returns\nC=indices[['Total_Market_log_return', 'CRIX_log_return', 'Bitwise10_log_return', 'CCI30_log_return']]\n# Correlations of log returns\nsb.heatmap(returns.corr(), mask=mask, annot=True, annot_kws={\"size\": 10}, fmt=\".2f\", cmap='RdBu_r', vmin=-1, vmax=1)\nplt.title(\"Correlation Coefficients (log returns)\")\n# Volatility\nvar_indices=indices.var()\nreturns.var()\nvar_returns=returns.var()\nimport matplotlib.cm as cm\nfrom matplotlib.colors import Normalize\nstd = np.sqrt(var_returns)\nplt.xticks(fontsize=12, rotation=45)\nplt.yticks(np.arange(0, 0.055, 0.005))\nplt.bar(['Market', 'CRIX','F5', 'Bitwise10', 'CCI30'], std, color=palette)\nplt.title(\"Daily Historical Volatility\")\nplt.ylabel(\"%\")\n", "output_sequence": "Comparison of the performance of CRIX against other cryptocurreny indices and the total crypto market."}, {"input_sequence": "rm(list = ls(all = TRUE))\ngraphics.off()\n# some general parameters\nset.seed(1) # seed for random number generation\ndt = 1 # length of each time interval\nt = seq(1,T,by=dt) # vector containing time steps\nr = NULL # vector of instantaneous short rate\ndv = NULL # random increment, depending on\n# the Wiener Process and volatility\nSFEsimVasicek=function(a,b,sigma){ # Vasicek Short Rate Model\nfor (i in 2 : T){\ndv[i] = sigma * sqrt(dt/T) * rnorm(1,mean=0,sd=1)\nr[i] = r[i - 1] + a * (b - r[i - 1]) * (dt/T) + dv[i]\n}\nplot(t,r,type=\"l\",col=\"blue\",xlab=\"Time\",ylab=\"Instantaneous Short Rate\",\nmain=\"Simulation of a One Factor Short-Rate Model\n\\n Vasicek Model\")\n}\nSFEsimCIR=function(a,b,sigma){ # Cox-Ingersoll-Ross Short Rate Model\ndv[i] = sigma * sqrt(dt/T) * sqrt(r[i - 1]) * rnorm(1,mean=0,sd=1)\n\\n Cox-Ingersoll-Ross Model\")\ndelta=function(t){ # Deterministic function of Time\ndelta = 0.0000644 * t\nreturn(delta)\nSFEsimHullWhite=function(a,sigma){ # Hull-White Short Rate Model\nfor (i in 2:T){\nr[i] = r[i - 1] + (delta(t = i - 1) - a * r[i - 1]) * (dt/T) + dv[i]\n\\n Hull White Model\")\nSFEsimHoLee=function(sigma){ # Ho-Lee Short Rate Model\nfor (i in 2:T) {\nr[i] = r[i - 1] + delta(i - 1) * (dt/T) + dv[i]\n\\n Ho Lee Model\")\nSFEsimVasicek(a = 0.161, b = 0.014, sigma = 0.009)\n\nSFEsimHoLee(sigma = 0.009)\n", "output_sequence": "Simulates and plots various One-Factor Short-Rate Models that describe the process of the instantaneous short term interest rate."}, {"input_sequence": "# Clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# Set working directory\n# setwd('~/...') # linux/mac os\n# setwd('/Users/...') # windows\n## Install packages\n# install.packages(KernSmooth)\n# Load packages\nlibrary(KernSmooth)\nlibrary(rugarch)\nlibrary(forecast)\nlibrary(TSA)\nlibrary(matlab)\n#--------------------------------------------------------#\n# (0) Adjust the following Parameters for different data #\n# File Names\nOdata = \"odata20142212.txt\" # option data\nIdata = \"DAX.txt\" # dax index data\n# Reading Option Data\nOdataAll = read.table(Odata, header=TRUE)\nOdata1 = subset(OdataAll,OdataAll[,7]<1 & OdataAll[,4]<0.9\n& OdataAll[,6]==1)\n# Reading Index Data\ndax1 = read.table(Idata, header=TRUE)[,2]\nperiods = length(dax1)\ndax = c(1:periods)\n# Apply the foolowing loop if the data are given in reversed order\n# if not then set dax=dax1\ndax = flipud(dax1)\ndax.ts = ts(dax)\ndax.ret = log(dax.ts)[2:periods]-log(dax.ts)[1:(periods-1)]\ndax.retts = ts(dax.ret)\n# Parameters for calculation/simulation\nnumbapprox = 2000 # fineness of the grid\nN = 5000 # No. of Simulations\n# Check return series for ARMA effects, e.g. with the following function\n# auto.arima(dax.retts, max.p=10,\n# start.p=1, start.q=1,start.P=1, start.Q=1, stationary=T, seasonal=F)\np = 0\narma = c(p,q)\n# specify garch order (need to be checked)\nm = 1\ngarch = c(m,s)\n# Specify GARCH model (default is standard GARCH)\n# for changing GARCH-model + submodel, please refer to\n# rugarch package for further information\ngarchmodel = \"fGARCH\"\n# underlying distribution (default: \"sstd\" - skewed stundent t's)\n# (alternatives: \"norm\" - normal, \"ghyp\"- generalized hyperbolic)\nudist = \"sstd\"\n# set archm=T for ARCH in mean model (archpow specifies the power)\narchm = F\n# set include.mean = F if you don't want to include a mean in the mean model\ninclude.mean = T\n#--------------------#\n# (1) SPD Estimation #\n#---------------------------------------------------------------------------#\n# Function for estimation of state price densities (risk neutral densities) #\nspdbl = function(m, sigma, s, r, tau){\nrm = length(m)\nst = sqrt(tau)\nert = exp(r*tau)\n# Modified Black-Scholes scaled by S-div instead of F\nd1 = (log(m)+tau*(r+0.5*(sigma^2)))/(sigma*st)\nd2 = d1-sigma*st\nf = pnorm(d1,mean=0,sd=1)-pnorm(d2,mean=0,sd=1)/(ert*m)\n# First derivative of d1 term\nd11 = (1/(m*sigma*st))-(1/(st*(sigma^2)))*((log(m)+tau*r)*sigma1)+\n0.5*st*sigma1\n# First derivative of d2 term\nd21 = d11-st*sigma1\n# Second derivative of d1 term\nd12 = -(1/(st*(m^2)*sigma))-sigma1/(st*m*(sigma^2))+sigma2*\n(0.5*st-(log(m)+rt)/(st*(sigma^2)))+\nsigma1*(2*sigma1*(log(m)+rt)/(st*sigma^3)-1/(st*m*sigma^2))\n# Second derivative of d2 term\nd22 = d12-st*sigma2\n# Please refer to either Rookley (1997) or\n# the XploRe Finance Guide for derivations\nf1 = dnorm(d1,mean=0,sd=1)*d11+(1/ert)*((-dnorm(d2,mean=0,sd=1)*d21)/m+\npnorm(d2,mean=0,sd=1)/(m^2))\nf2 = dnorm(d1,mean=0,sd=1)*d12-d1*dnorm(d1,mean=0,sd=1)*(d11^2)-\n(1/(ert*m)*dnorm(d2,mean=0,sd=1)*d22)+\n((dnorm(d2,mean=0,sd=1)*d21)/(ert*m^2))+\n(1/(ert*m)*d2*dnorm(d2,mean=0,sd=1)*(d21^2))-\n(2*pnorm(d2,mean=0,sd=1)/(ert*(m^3)))+\n(1/(ert*(m^2))*dnorm(d2,mean=0,sd=1)*d21)\n\n# Recover strike price\nx = s/m\nc1 = -(m^2)*f1\nc2 = s*((1/x^2)*((m^2)*f2+2*m*f1))\n# Calculate the quantities of interest\ncdf = ert*c1+1\nfstar = ert*c2\nreturn(fstar)\n}\n#--------------------------#\n# Nonparametric Regression #\n\nSPDlpe = function(RawData1, gx, numbapprox, SpotPrice, Strike, maturity){\nMoneyness = as.matrix(SpotPrice/Strike)\nlm = length(Moneyness)\nY = as.matrix(RawData1[,7]) # implied volatility\ninterestrate = unique(RawData1[,3])\nbeta = matrix(0, nrow = numbapprox, ncol = 3)\nj = 1\nwhile(j < numbapprox + 1){\ni = 1\nX = matrix(0, nrow=lm, ncol=3)\n\nwhile(i < lm+1){\nX[i,] = t(c(1,(Moneyness[i]-gx[j]),(Moneyness[i]-gx[j])^2))\ni = i+1\n#Choosing bandwith\nh = (nrow(RawData1))^(-1/9)\n\nW = matrix(0, nrow=lm,\nu = (Moneyness[i]-gx[j])/h\nW[i,i] = (15/16)*(1-u^2)^2*(abs(u) <= 1)/h\ni = i+1\nbeta[j,] = t(solve(t(X)%*%W%*%X)%*%t(X)%*%W%*%Y)\nj = j+1\nG = cbind(gx,beta)\nRNDens = spdbl(G[,1], G[,2], SpotPrice,\ninterestrate, maturity)\n#Recover Grid/evaluation points\ntemp = (1/(G[,1]/SpotPrice))\nreturn(list(RNDens=RNDens, temp=temp))\n#-----------------------------------#\n# (2) Historical Density Estimation #\n#-----------------------------------------#\n# Simulation via Time Series: GARCH model #\nHDEgarch = function(garchfit, data, grid, maturity, N, start){\n# Simulation\ngarchsim = ugarchsim(garchfit, n.sim = round(maturity*365),\nn.start = 0, m.sim=N, startMethod=(\"sample\"),\nmexsimdata=TRUE)\n# head(g11sim@simulation)[2]= extracting simulated return data\nreturnsim = as.vector(head(garchsim@simulation)[2])\n# Calculating spot prices from return data\nfor(i in 1:N) {\nvalue[,i] = start*exp(sum(returnsim$seriesSim[,i]))\n# Computing density on given grid: Either by using build in function (kde(...)):\n# kde() computes bandwith h wih function hpi() which uses Wand&Jones (1994) estimator\nValDens = kde(value[1,],eval.points=grid, gridsize=length(grid),\nxmin=min(grid),\nreturn(ValDens$estimate)\n#-----------------#\n# (3) Calculation #\n# Capture different maturities\nRawData = Odata1\nmaturities = sort(unique(RawData[,4]),decreasing=F)\nSpotPrice = matrix(0, ncol = length(maturities))\n# Needed specifications for function SPDlpe()\nminK = as.numeric(quantile(RawData[,2], probs = 0.15, type = 6))\ngx = matrix(0, nrow = numbapprox, ncol = length(maturities))\n# Needed specifications for function HDEtss()\nspec = ugarchspec(variance.model = list(model = garchmodel,\ngarchOrder = garch, submodel = submodel), mean.model =\nlist(armaOrder = arma, archm=archm,archpow=archpow,\ninclude.mean=include.mean), distribution.model = udist)\n\n# for faster computation change type of solver\ngarchfit = ugarchfit(data=dax.retts, spec=spec, solver = \"hybrid\")\nvalue = matrix(0,ncol=N)\n#------------------------------------------------------#\n# Estimation of SPD's / Historical Densities and EPK's #\ntemp = matrix(0, nrow = numbapprox, ncol = length(maturities))\nfor(t in 1:length(maturities)){\nRawData1 = subset(RawData, RawData[,4] == maturities[t])\nStrike = RawData1[,2]\nSpotPrice[,t] = unique(RawData1[,1])\ngx[,t] = seq(SpotPrice[,t]/maxK,\nlength.out = numbapprox)\nspdlpe = SPDlpe(RawData1, gx[,t], numbapprox,\nSpotPrice[,t], Strike, maturities[t])\nRND[,t] = spdlpe$RNDens\nHDE[,t] = HDEgarch(garchfit, dax.retts, temp[,t],\nmaturities[t], N, SpotPrice[,t])\nEPK[,t] = RND[,t] / HDE[,t]\n# (4) Plot EPK's for all maturities #\n#Capture days to maturity/date/index price\ndays = rep(round(maturities*365))\ndate = as.character(RawData[1,8])\nindexprice = as.character(SpotPrice[,1])\n# Plot EPK's for all maturities\ndev.new()\npar(mar=c(5, 5, 5))\ntitle = paste(\"EPKs on\",date, \"for index price\", indexprice)\nplot(temp[,1],EPK[,1],col=1,type=\"l\", xlab=~S[T], ylab=\"EPK\", cex=1.5,\nxlim=c(min(temp[,1]),max(temp[,1])),ylim=c(0,4), lwd=3, cex.lab=1, main=title)\nfor(t in 2:length(maturities)){\nlines(temp[,t], EPK[,t],type=\"l\",col=t, lwd=3)\nlegend(\"topright\",legend=days,col=c(1:length(maturities)) ,\ntitle=\"Days to maturity\", lwd=2, cex=.75,xpd=T,inset=c(-0.165,0))\n# Uncomment to plot EPK's next to corresponding SPD and historical density\n# for(t in 1:length(maturities)){\n# dev.new()\n# par(mfrow=c(1,2),mar=c(10, 5, 5))\n# plot(temp[,t],RND[,t],col=\"blue\",type=\"l\", xlab=~S[T], ylab=\"Density\",\n# xlim=c(min(temp[,1]),max(temp[,1])), main=\"SPD (blue) and \\n historical density (red)\", lwd=3)\n# lines(temp[,t],HDE[,t],type=\"l\", col=\"red\", lwd=3)\n# title = cbind(paste(\"Empirical pricing kernel on\", date),\n# paste(\"with tau=\",days[t],\"and index price=\", indexprice))\n# plot(temp[,t],EPK[,t], lwd=3,col=\"black\",type=\"l\", xlab=~S[T], ylab=\"EPK\",\n# xlim=c(min(temp[,1]),max(temp[,1])),ylim=c(0,5), main=title, cex=1)\n# }\n", "output_sequence": "Simulates and plots various One-Factor Short-Rate Models that describe the process of the instantaneous short term interest rate."}, {"input_sequence": "#Close windows and clear variables\ngraphics.off()\nrm(list = ls(all=TRUE))\n# install and load packages\nlibraries = c(\"igraph\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# Example 1\n# plot the network\ng1 = graph(edges = c(1,2,1,3,1,4,1,5,1,6), n = 6, directed = FALSE)\nplot(g1, vertex.size = 30, vertex.label.cex = 2.0,\nedge.width = 2, vertex.label.color = \"black\")\n# calculate the centrality measures\n# degree centrality\ndc1 = degree(g1)\n# closeness centrality\ncc1 = closeness(g1, normalized = TRUE)\n# betweenness centrality\nbc1 = betweenness(g1, normalized = TRUE)\n# eigenvector centrality\nec1list = eigen_centrality(g1, scale = FALSE)\nec1 = ec1list$vector\n# Example 2\ng2 = graph(edges= c(1,2,2,3,3,4,4,5), n = 5, directed = FALSE)\nplot(g2, vertex.size = 30, vertex.label.cex = 2.0,\nedge.width = 2, vertex.label.color=\"black\")\ndc2 = degree(g2)\ncc2 = closeness(g2, normalized = TRUE)\nec2list = eigen_centrality(g2, scale = FALSE)\nec2 = ec2list$vector\n# Example 3\ng3 = graph(edges= c(1,2,2,3,3,4,4,5), n = 5, directed = FALSE)\nplot(g3, vertex.size = 30, vertex.label.cex = 2.0,\ndc3 = degree(g3)\ncc3 = closeness(g3, normalized = TRUE)\nec3list = eigen_centrality(g3, scale = FALSE)\nec3 = ec3list$vector\n# Example 4\ng4= graph(edges= c(1,2,1,3,1,4, 2,5,2,6,2,7, 3,8,3,9,3,10, 4,11,4,12), n=13, directed = FALSE)\nplot(g4, vertex.size =30, vertex.label.cex = 2.0,\ndc4 = degree(g4)\ncc4 = closeness(g4, normalized = TRUE)\nec4list = eigen_centrality(g4, scale = FALSE)\nec4 = ec4list$vector\n", "output_sequence": "Give a few examples of centrality calculation"}, {"input_sequence": "###############################################################################\n##\n## VWAP FPCA Test. R-Script to apply fpca model from trainperiod to ##\n## the test data. ##\n## Evaluation and plots after estimation procedure ##\n###############################################################################\n# load the R Environment with prepared data\nload(\"VWAP.RData\")\nlibraries = c(\"stargazer\", \"plyr\", \"moments\", \"zoo\",\"forecast\",\"urca\",\"expectreg\",\"fda.usc\", \"vars\",\"lattice\",\"tseries\", \"abind\",\"sm\",\"quantreg\")\nlapply(libraries,function(x)if(!(x %in% installed.packages())){install.packages(x)})\nlapply(libraries,require,quietly=TRUE,character.only=TRUE)\n# set some prarameter\nn = 65 # Train Data length\nhour = 24 # number of avialable daily observations\ntau = c(0.01, 0.05, 0.99) # expectiles of interest\nmodel = \"fourier\" # estimation of seasonal component\n# \"fourier\" includes also dummies for weekday and holidays\n# \"smooth.spline\", \"sm.regression\", \"loess\" do not include holiday and weekday effects\ntrsh = .95 # threshold how mouch should be explained by exogenous variables; apply PCA\nlag.l = 3 # (lag.l-1) determines allowed maximum lag length for VAR models.chose fiteen to allow for past two weeks as impact; 30days window: 3, 60 days window 5\np = 1 # parameter demanded by for seasonality forecast\nk = 0 # kk is used to set increasing steps, only needed to vary in testperiod\nlt = length(tau) # number of expectils\nexp.5 = which(tau %in% 0.5)# index of 0.5 expectile\nexp.l = which(tau %in% min(tau))\nholiday_dummy = holiday_dummy # dummy variable indicating public holidays\nh = dim(vwap)[1] - n # number of forecasts\nwindow = 65 # window length, how many days should be included in the VAR/ARIMA model\n#################################################################################\nstart = Sys.time()\nfor (k in 0:(h-2)){\nprint(paste(\"k is \", k, sep = \"\"))\n\nif(k == 0){\n# container to store results from estimations\n# by SC/BIC\nSC.spot.arr = array(NA, dim = c((h - 1), hour, lt))\n\n# by AIC\nAIC.spot.arr = array(NA, dim = c((h - 1), hour, lt))\n# by arima modelling\narima.spot.arr = array(NA, dim = c((h - 1), hour))\n# by Trendmodel\nDSC_Forc.mat = array(NA, dim = c((h - 1), hour))\n}\n# input endogen\nInput_list = c(t(vwap[1:(n + k), ]))\n# input exogen\nInput_list_ex1 = c(t(spot[1:(n + k + p), ]))\n# DSC: construct dummies for deseasonalization. wd and ph are only zeros\n# because sun and wind do not care for weekend or public holidays\ntempseq = as.numeric(rep(rep(c(1:7), each = 1), length.out = n + k + 2))\nWD = sapply(1:6, FUN = function(num){as.numeric(tempseq == num)})\nwd = matrix(rep(0, prod(dim(WD))) , ncol = dim(WD)[2])\nholiday_dummy = as.matrix(holiday_dummy, ncol = 1)\nph = matrix(rep(0, (prod(dim(holiday_dummy)))))\n# the seasonal component\nDSC_Forc.mat[(k + p), ] = dsc(Input_list, WD, PH = holiday_dummy, n = n, k = k, hours = hour, p = p, model = model)$forecast\n# compute desaisonalized series of prices\nresid_data_yearly = matrix(dsc(Input_list, WD = WD, PH = holiday_dummy, n = n, k = k, hours = hour, p = p, model = model)[[1]], nrow = hour)\n# actual residual load components\nDSC_Input_ex2 = dsc(Input_list_ex2, WD = WD, PH = holiday_dummy, n = n + p, k = k, hours = hour, p = p, model = model)[[1]]\nex2_data_yearly = matrix(DSC_Input_ex2 - DSC_Input_ex3 - DSC_Input_ex4, nrow = hour)\n## forecasted residual laod components\nDSC_Input_ex5 = dsc(Input_list_ex5, WD = WD, PH = holiday_dummy, n = n + p, k = k, hours = hour, p = p, model = model)[[1]]\nex3_data_yearly = matrix(DSC_Input_ex5 - DSC_Input_ex6 - DSC_Input_ex7, nrow = hour)\n# difference in residual load (actuals - forecast)\nex4_data_yearly = ex2_data_yearly - ex3_data_yearly\n# compute Principal Components of the explanatory variables\nPCA_ex = list( pca.man(t(ex1_data_yearly), tau = trsh),\n# get the scores for the window\nscores.ex = lapply(lapply(PCA_ex, \"[[\", \"scores\" ), \"[\",c((n + k + p- window):(n + k + p)),TRUE)\n#scores.elbow = lapply(scores.ex, \"[\", TRUE, 1)\n#scores.ex = scores.elbow\n# set up for estimation procedure. xsim are the knots, mfit provides 3d array to store results from expectile sheet\nxsim = seq(1/nrow(resid_data_yearly), 1, length.out = nrow(resid_data_yearly))\nif (k == 0){\nmfit = array(0, dim = c(window, hour, lt)) # hour rows, traindata length cols, slices as length of tau\n# Compute expectile sheet for every day in window\nfor (i in 1:window){\nmfit[i, , ] = expectreg.ls(resid_data_yearly[, (n - window + i)] ~ rb(xsim, \"pspline\", center = FALSE),\nestimate = \"sheet\", expectiles = tau, smooth = \"gcv\")$fitted\n}\n}else{\n# fist, expectile regression on new day, keep window length\ny = expectreg.ls(resid_data_yearly[, (n + k)] ~ rb(xsim, \"pspline\", center = FALSE),\nestimate = \"sheet\", expectiles = tau, smooth = \"gcv\")\n# add result to mfit and drop first one.\nyfit = array(y$fitted, dim = c(1, hour, length(tau)))\nmfit = abind(mfit[-1, , ], yfit, along = 1)\n# mean function of estimated expectiles\nfor (i in 1:hour){\nif (i == 1){exp.mean = array(NA, dim=c(hour,lt))}\nfor (j in 1:lt){\nexp.mean[i,j] = mean(mfit[,i,j] )\n}\n# subtract mean series\ncenter.mfit = sweep(mfit, 2:3, exp.mean)\n# compute the pc for the first 4\nresult = sapply(X = c(1:lt), FUN = fpca, fit = center.mfit, x = xsim, nh = 4)\n# extract the scores for each expectile. Get a list with 3\nscores = result[3,]\n# assign names to scores\nfor (i in 1:length(scores)){\ncolnames(scores[[i]]) = paste0(\"FPC\", seq(1, 4))\n# add a further element to the list containing nothing in order to estimate VAR/Arima without exogen variables\nscores.ex[(length(scores.ex) + 1)] = list(NULL)\n# compute forecasts for the scores\narima.forc.50 = as.list(data.frame(forc.auto.arima(scores, scores.ex, expec = tau[exp.5], tau = tau)))\ncurve.arima = lapply(X = c(1:5), KarLoe.ar, fd = result[, exp.5], mean = exp.mean[, exp.5], model = arima.forc.50, season = DSC_Forc.mat[(k + p), ], xsim = xsim)\narima.spot.arr[(k + 1), ] = curve.arima[[1]]\ncrit = c(\"SC\", \"AIC\")\nfor (IC in crit){\n# estimate models\n#IC = \"SC\"\nmodel.spot = lapply(X = c(1:lt), FUN = function(X){VAR(y = scores[[X]], exogen = scores.ex[[1]][-(window + p), ], lag.max = (lag.l - 1), type = \"const\", ic = IC)})\n# compute forecast\npred.spot = lapply(X =c(1:lt), FUN = function(X){predict(model.spot[[X]], n.ahead = p, dumvar = matrix(scores.ex[[1]][(window + p), ], nrow = p))})\npred.endo = lapply(X =c(1:lt), FUN = function(X){predict(model.endo[[X]], n.ahead = p)})\n# get the predicted scores\nforc.spot = lapply(X = c(1:lt), FUN = function(X){unlist(lapply(pred.spot[[X]]$fcst, \"[[\", 1))})\n# recompute curves with Karhunen Loeve and seasonal component\nif (IC == \"SC\"){\nSCfcurve.spot = lapply(X = c(1:lt), KarLoe, fd = result, mean = exp.mean, model = forc.spot, season = DSC_Forc.mat[(k + p), ], xsim = xsim)\nif (IC == \"AIC\"){\nAICfcurve.spot = lapply(X = c(1:lt), KarLoe, fd = result, mean = exp.mean, model = forc.spot, season = DSC_Forc.mat[(k + p), ], xsim = xsim)\nSC.spot.arr[(k + 1), , ] = matrix(unlist(SCfcurve.spot), nrow = hour)\n} # end forecast loop\nSys.time() - start\n## Plot and Evaluation ##\n## Plot: Daily Curves with forecast from best model ##\n# Table:\n# evaluate estimation\norigin = matrix(unlist(as.data.frame(vwap[c((n + p):(n + p + k)), nrow = (h - 1))\n# forecast of models at 50% expectile\nbind.50 = list(\"DA spot AIC\" = AIC.spot.arr[, , exp.5],\n\"RL actuals AIC\" = AIC.RLac.arr[, , exp.5],\n\"DA spot Arima\" = arima.spot.arr,\n\"Trend\" = DSC_Forc.mat)\nerror.eval = t(sapply(X = c(1:length(bind.50)), fit.eval, origin = origin, model = bind.50))\nTS.model = c(rep(\"VAR\",10), rep(\"ARIMA\",5), \"DA Spot\", \"Trend\")\nExogen.info = c(rep(c(\"DA Spot\", \"RL actual\", \"RL forecast\", \"RL difference\", \" - \"),3), rep(\" - \", 2))\nLag.select = c(rep(\"AIC\",5), rep(\"BIC\", 5), rep(\"AIC\",5), rep(\" - \", 2))\nerror.eval = data.frame(\"TS model\" = TS.model, \"Selection Criteria\" = Lag.select, \"Exogenous Variable\" = Exogen.info, error.eval)\nbind.models = list(AIC.spot.arr,\nAIC.RLac.arr,\nerror.insample = cbind(error.eval,\n\"within_tau.01\" = c(sapply(X = c(1:length(bind.models)), FUN = function(X){(hit.range(bind.models[[X]], origin, exp.l, exp.u))}),\nrep(NA, 7)),\n\"within_tau.05\" = c(sapply(X = c(1:length(bind.models)), FUN = function(X){(hit.range(bind.models[[X]], origin, 2, 6))}),\nrep(NA, 7)),\n\"within_tau.25\" = c(sapply(X = c(1:length(bind.models)), FUN = function(X){(hit.range(bind.models[[X]], origin, 3, 5))}),\nrep(NA, 7)))\nerror.order = error.insample[order(unlist(error.insample[, 5]), -unlist(error.insample[, 6])), ]\nstargazer(error.order, summary = FALSE, rownames = FALSE, type = \"text\")\n# Plot\n# plot of forecast curves with best forecast from VAR SC with DA spot\npar(mfrow = c(1,2),cex.lab = 1.2, cex.axis= 1.2, cex.main = 1.2 )\nfor(a in 2:3){\nfa = n + a\n# organzize the data\nplot.data = array(NA, dim = c(hour, (3 + lt)))\nplot.data[, c(1:3)] = cbind( c(t(vwap[fa, ])), c(t(spot[fa, ])), DSC_Forc.mat[a, ])\n# here can change the forecast model of interest\nfor (j in c(1:lt) ) {\nplot.data[, (j + 3)] = SC.spot.arr[a, , j]\n# plot the data.\nplot(plot.data[, 1], type = \"l\", col = \"red\",\nlty = 6,\nylim = range(plot.data),\nmain = rownames(vwap)[fa],\nxaxt = \"n\",\ncex.axis = 1.5,\nlwd = 2)\nlines(plot.data[, 2], lwd = 2, col = \"darkgreen\")\nfor (j in 1:lt){\ncol.exp = ifelse(j == exp.5, \"black\", \"grey\")\nlines(plot.data[, (j + 3)], lwd = 2, col = col.exp)\naxis(1, c(1, 7, 13, 24), c(\"00:00\", \"12:00\", cex.axis = 1.2)\n}\ndev.off()\n", "output_sequence": "Forecast of daily VWAP curves with FPCA VAR / FPCA ARIMA model. Generate table with error measures and plots of daily curves."}, {"input_sequence": "###############################################################################\n##\n## Sequenceplot: VWAP and Residual load ##\n###############################################################################\n# load the R Environment with prepared data\nload(\"VWAP.RData\")\n# moving picture:\n# define a seuence of days. chosen arbitrary\ndateseq = seq.Date(from = as.Date(\"2015-01-09\"), to = as.Date(\"2015-01-20\"), by =\"day\")\nvw = as.matrix(vwap[ which(as.Date(rownames(vwap)) %in% dateseq),])\n# loop with plots:\n# pic 1: day 1 as black line\n# pci 2: day 1 as grey line + day 2 as black line\n# pic 3: day 1 as light grey + day 2 as grey + day 3 as black line\nfor ( i in 1:dim(vw)[1]){\npar(mfrow=c(1,2))\n\n# pic 1\nif ( i == 1){\nplot(vw[i, ], type = \"l\", col = \"black\", ylim = range(vw), ylab = \"VWAP in EUR\", xlab = \"Hour\",\nxaxt = \"n\", cex.axis = 1.5, cex.lab = 1.5)\naxis(1, c(1, 7, 13, 24), c(\"00:00\", \"12:00\", cex.axis = 1.5)\n\nplot(rl[i, ], type = \"l\", col = \"black\", ylim = range(rl), ylab = \"Residual Load in MWh\", xlab = \"Hour\",\n}\n# pic 2\nif (i == 2){\nplot(vw[i,], type = \"l\", col = \"black\", ylim = range(vw), ylab = \"VWAP in EUR\", xlab = \"Hour\",\nlines(vw[i-1, ], col = colors()[113])\n# pic 3\nif ( i > 2){\nlines(vw[i-2, ], col = colors()[82])\n}\n", "output_sequence": "Sequence of plots that show daily price and residual load movements."}, {"input_sequence": "import seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import PercentFormatter\nResults_v15 = pd.DataFrame(\ndata = [\n[0.05, 0.20613789601969512, 0.02832],\n[0.1, 0.2409757037313054, 0.03043],\n[0.2, 0.21970624945838146, 0.02624],\n[0.25, 0.23083915299999536, 0.02695],\n[0.3, 0.22647751932816307, 0.02575]],\ncolumns = [\"Window size\", \"Total R squared\", \"Predictive R squared\"]\n)\nfig, (ax1, ax2) = plt.subplots(2)\nsns.lineplot(data = Results_v15, x = \"Window size\", y = \"Total R squared\", ax = ax1, color = [0, 22/265, 189/265])\nax1.set_xlabel(\"\")\nax1.yaxis.set_major_formatter(PercentFormatter(1, decimals = 0))\nax1.set_xticks([])\nax1.set_ylabel(\"$R^2_{total}$\", rotation = 0, labelpad = 20)\nsns.lineplot(data = Results_v15, x = \"Window size\", y = \"Predictive R squared\", ax = ax2, color = [0, 22/265, 189/265])\nax2.set_xlabel(\"Training window size\")\nax2.yaxis.set_major_formatter(PercentFormatter(1, decimals = 1))\nax2.set_xticks([0.05, 0.1, 0.25, 0.3])\nax2.set_xticklabels([\"5%\", \"10%\", \"30%\"])\nax2.set_ylabel(\"$R^2_{pred}$\", rotation = 0, labelpad = 20)\nplt.tight_layout()\n", "output_sequence": "LVU thesis: This notebook creates the plots resulting from the analysis of different window sizes for the training of the Conditional Autoencoder."}, {"input_sequence": "# Data fixes before feature engineering --> X and Y are already initialized\n# V12: Same as V9, but additional functionality in make_predictions for variable importance analysis\n# V13: Same as V12, but number of factors included in model directory, added intercept to X, fixed the covariates sorted portfolios now (used later for interpretation of latent factors and performance benchmark)\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import rankdata\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.regularizers import l2\nimport keras_tuner as kt\nfrom kerastuner.tuners import Hyperband\nimport tqdm\nfrom kerastuner.tuners import BayesianOptimization\nimport random\nfrom scipy.stats.mstats import winsorize\nimport pdb\nimport pickle\nfrom sklearn.preprocessing import StandardScaler\nimport os\n# Function to read .csv files. Uses only those that have a price column. Returns list of dataframes, their names\n# and the sorted times\ndef read_files(data_path, remove_coins=[]):\nfilenames = os.listdir(data_path)\nfilenames.sort()\n# Select only .csv files\nfilenames = [file for file in filenames if file[-4:] == \".csv\" and file[:-4] not in remove_coins]\n# Read files\ndflist = [pd.read_csv(data_path + \"/\" + file, index_col=\"day\") for file in filenames]\nfor df in dflist: df.index = pd.to_datetime(df.index)\n# Extract all time points in a chronologically sorted manner\nsorted_times = sort_all_times(dflist)\nreturn dflist, filenames, sorted_times\n# Function that takes a list of data frames and returns a chronologically ordered array of all times\n# Times need to be in index\ndef sort_all_times(dflist):\n# Extract list of times from each dataframe\nlist_times = [x.index for x in dflist]\n# List of sets of times from each dataframe\nlist_times_sets = [set(x) for x in list_times]\n# Unite all sets\nall_times = list_times_sets[0]\nfor i in range(len(list_times_sets)):\nall_times = all_times.union(list_times_sets[i])\n# Save as array\nall_times_array = pd.array(all_times)\n# Sort chronologically\nall_times_sorted = pd.array(sorted(all_times_array))\nreturn all_times_sorted\n# Function that takes a list of pandas Series and concatenates them all into one dataframe, where each column is one\n# of the original series. Second argument is the list of names of the columns\ndef combine_returns(list_return_dfs, list_coin_names):\nfor i in range(len(list_return_dfs)):\nif i == 0:\nprices_df = list_return_dfs[i]\nelse:\nprices_df = pd.concat([prices_df, list_return_dfs[i]], axis=1)\nprices_df.columns = [coin[:-4] for coin in list_coin_names]\nreturn prices_df\n# Function that takes as input a list of dataframes and gives as output a list of all column/covariate names\ndef all_columns(dflist):\ncolumns_list = [column for df in dflist for column in df.columns]\ncolumns_set = set(columns_list)\ncolumns_final_list = list(columns_set)\nreturn columns_final_list\n# Function that\n# takes as input:\n# list of dataframes, date as string, minimum number of coins, minimum percentage of observations per\n# variable per coin, list of dataframe names\n# gives as output:\n# List of coin names, List of covariate names, X (returns + covariates), y (returns)\n# Here covariates are rank standardized crosssectionally\ndef prepare_X_y_rank_standardized(dflist, startdate, enddate, filenames, variables):\n# Cut away all time points before the variable date\nselected_times = sorted_times[sorted_times >= startdate]\n# Cut away all time points after the variable enddate\nselected_times = selected_times[selected_times <= enddate]\ndflist_cut = [df.reindex(selected_times) for df in dflist]\n# If a data point needs to be excluded, we delete all info\nfor df in dflist_cut:\ndf.loc[df[\"exclude_training_and_testing\"] == \"True\", [\"daily excess return\", \"close\"]] = np.nan\n# Split up excess returns, rest of data and exclusion mask\nlist_of_excess_returns = [df[\"daily excess return\"] for df in dflist_cut]\ndf_returns = combine_returns(list_of_excess_returns, filenames)\nlist_of_close_prices = [df[\"close\"] for df in dflist_cut]\ndf_close_prices = combine_returns(list_of_close_prices, filenames)\nlist_of_exclusions = [df[\"exclude_testing\"] for df in dflist_cut]\nexclusion_mask = combine_returns(list_of_exclusions, filenames)\nexclusion_mask.replace(np.nan, True, inplace=True)\nlist_of_covariates = [df.loc[:, variables] for df in dflist_cut]\n# Working with a list of dataframes is computationally cumbersome. We change to a 3d numpy-array\ndata_covariates = np.array(list_of_covariates)\n# Replace NAs of covariates with cross-sectional median of other coins\ndata_covariates_nan_median = np.where(np.isnan(data_covariates),\nma.median(ma.array(data_covariates, mask=np.isnan(data_covariates)), axis=0),\ndata_covariates)\n# Rank normalize covariates to (-1, 1)\ncovariates_rank_normalized = 2 * (rankdata(data_covariates_nan_median, axis=0) - 1) \\\n/ (data_covariates.shape[0] - 1) - 1\n# Go back to list of dataframes\nlist_covariates_dfs = [pd.DataFrame(data=covariates_rank_normalized[i, :, :],\ncolumns=variables,\nfor i in np.arange(covariates_rank_normalized.shape[0])]\ncovariates_unstandardized = list_of_covariates\nreturn df_returns, list_covariates_dfs, covariates_unstandardized, df_close_prices, exclusion_mask\ndef create_covariates_nn(num_coins, num_covariates, encoding_dim, list_no_hidden, lambda_reg):\ndict_nn = {}\nhidden = [layers.Dense(no_hidden, activation='relu', kernel_regularizer=l2(lambda_reg)) for no_hidden in\nlist_no_hidden]\nfor i in range(num_coins):\nsubdict = {}\nsubdict[\"input\"] = keras.Input(shape=(num_covariates,))\nsubdict[\"hidden\"] = hidden[0](subdict[\"input\"])\nif len(hidden) > 1:\nfor layer in hidden[1:]:\nsubdict[\"hidden\"] = layer(subdict[\"hidden\"])\nsubdict[\"output\"] = layers.Dense(encoding_dim,\nname=\"betas\" + str(i),\nkernel_regularizer=l2(lambda_reg))(subdict[\"hidden\"])\n# subdict[\"model\"] = keras.Model(inputs = subdict[\"input\"], outputs = subdict[\"output\"])\ndict_nn[\"{}\".format(i)] = subdict\nreturn dict_nn\n# Create the encoder part for the returns\ndef create_returns_encoder(shape_input, encoding_dim, lambda_reg):\ndict_encoder = {}\n# These are our inputs for the factors\ndict_encoder[\"input\"] = keras.Input(shape=(shape_input,))\n# \"encoded\" is the encoded representation of the input\ndict_encoder[\"output\"] = layers.Dense(encoding_dim,\nname=\"Factors\",\nkernel_regularizer=l2(lambda_reg))(dict_encoder[\"input\"])\nreturn dict_encoder\n# Function that links the parallel networks of the covariate part with the output of the returns part\ndef link_both_parts(num_coins, input_dimension, num_covariates, encoding_dim, list_no_hidden, lambda_reg=0.01):\ndict_covars = create_covariates_nn(num_coins, num_covariates, encoding_dim, list_no_hidden, lambda_reg)\ndict_dot_product = {}\nencoder = create_returns_encoder(input_dimension, encoding_dim, lambda_reg)\ndict_dot_product[str(i)] = tf.keras.layers.Dot(axes=1)([dict_covars[str(i)][\"output\"], encoder[\"output\"]])\nconcatted = tf.keras.layers.Concatenate()([dict_dot_product[str(i)] for i in range(num_coins)])\ninput_layers = [dict_covars[str(i)][\"input\"] for i in range(num_coins)]\ninput_layers.append(encoder[\"input\"])\nfull_model = tf.keras.Model(inputs=input_layers,\noutputs=concatted,\nname=\"full_model_{0}_parallel_networks\".format(num_coins))\nreturn full_model\nclass regression_model:\ndef __init__(self, data_path=\"../Aggregate Data v3\", remove_coins=[]):\nself.data_path = data_path\nself.dflist, self.filenames, self.sorted_times = read_files(self.data_path, remove_coins=remove_coins)\nself.variables = [\"new_addresses\", \"active_addresses\", \"bm\", \"volumeto\", \"size\", \"illiq\", \"capm beta\",\n\"capm alpha\", \"ivol\", \"turnover\", \"rvol\", \"bid-ask\",\n\"detrended turnover\", \"standard deviation turnover\", \"rel to high\",\n\"volume shock\", \"r2_1\", \"r13_2\",\n\"r30_14\", \"r180_60\", \"var_5\"]\n# Even though our data starts on 2017-01-01, we delete the first 180 days because we do not have data\n# there for a lot of the features we engineered ourselves\nself.startdate = pd.to_datetime(\"2017-01-01\") + pd.to_timedelta(180, \"day\")\n# Last day where we have data for bitcoin\nself.enddate = pd.to_datetime(\"2022-03-01\")\nself.test_percent = 0.5\nself.validation_percent = 1 / 3\ndef replace_infty(self):\ndflist = self.dflist\nreplaced = []\nfor df in dflist:\nnew = df.replace(to_replace=[np.inf, -np.inf], value=np.nan)\nreplaced.append(new)\nself.dflist = replaced\ndef prepare_rank_standardize_data(self):\nself.returns, self.variables_df_list, self.variables_df_list_unstandardized, self.close_prices, self.exclusion_mask = \\\nprepare_X_y_rank_standardized(self.dflist, self.startdate, self.filenames, self.variables)\nclass cond_auto_model():\ndef __init__(\nself, model_name, data_path=\"../Aggregate Data v3\", list_covar_hidden_neurons=[32],\nlist_factors_hidden_neurons=[5],\nfactor_dim=5, predictiondays=30, remove_coins=[], full_input=True, parallel_runs=10):\nself.variables = [\"new_addresses\", \"active_addresses\", \"bm\", \"volume\", \"standard deviation volume\",\n\"size\", \"illiq\", \"capm beta\", \"max\",\n\"volume shock 30\", \"volume shock 60\", \"r2_1\", \"r13_2\",\nself.list_covar_hidden_neurons = list_covar_hidden_neurons\nself.factor_dim = factor_dim\nself.within_time_limit = True\nself.predictiondays = predictiondays\nself.callback = [keras.callbacks.EarlyStopping(\nmonitor='val_loss',\nmin_delta=0.0001,\nrestore_best_weights=True,\npatience=300)]\nself.epochs = 1500\nself.tuning_trials = 24\nself.number_parallel_models = parallel_runs\nself.list_of_lists_contemporaneous_predictions = []\nself.weights_list = []\nself.full_input = full_input\nself.name = model_name + f\" {factor_dim} factors\"\ndef initialize_X_Y(self, replace_nan_return_with=np.nan,\nregression_model.replace_infty(self)\nregression_model.prepare_rank_standardize_data(self)\nself.model_directory = \"../Models/\" + self.name\nY = self.returns.iloc[1:].copy()\n# Y = Y.reset_index(drop=True)\nself.observed_mask = 1 - np.isnan(Y)\nY = Y.replace(to_replace=np.nan, value=0)\n# Winsorize to remove extreme returns\nfor i in np.arange(len(Y)):\nY.iloc[i, :] = np.array(winsorize(Y.iloc[i, :], limits=[0, 0.05], nan_policy=\"omit\"))\nX = [df.iloc[:-1] for df in self.variables_df_list]\n# X = [x.reset_index(drop=True) for x in X]\nX = [x.replace(to_replace=np.nan, value=replace_nan_covars_with) for x in X]\n# Add intercept\nfor df in X:\ndf[\"Intercept\"] = 1\nself.X = X\nself.initialize_Z()\n# self.calculate_sorted_portfolio_returns()\nsorted_portfolios = self.sorted_portfolios\n# Split up in training and test set\nnumberdays = Y.shape[0]\ntestdays = int(np.ceil(numberdays * self.test_percent))\ntraindays_end = numberdays - testdays\ntraindays_start = 0\nvalidationdays = int(np.ceil(traindays_end * self.validation_percent))\ntraindays_end = traindays_end - validationdays\n# Add constant regressor to the inputs\nY[\"constant\"] = 1\nsorted_portfolios[\"constant\"] = 1\nY_train, Y_valid, Y_test = Y.iloc[: traindays_end], Y.iloc[\ntraindays_end: + validationdays], Y.iloc[\ntraindays_end + validationdays:]\nportfolios_train, = \\\nsorted_portfolios.iloc[: traindays_end], sorted_portfolios.iloc[\ntraindays_end: + validationdays], sorted_portfolios.iloc[\ntraindays_end + validationdays:]\nX_train, X_valid, X_test = [df.iloc[:traindays_end] for df in X], \\\n[df.iloc[traindays_end: traindays_end + validationdays] for df in X], \\\n# Remove the constant intercept from the outputs. We only needed them temporarily because in the inputs we do actually need the constant\ndel Y[\"constant\"]\nif self.full_input:\nX_train.append(Y_train)\nX_valid.append(portfolios_valid)\nself.Y_test_global = Y_test\nself.X, \\\nself.X_train, \\\nself.X_valid_global, \\\nself.X_test, \\\nself.traindays_start, self.validationdays, self.testdays = \\\nX, Y, X_train, X_valid, \\\nX_valid, X_test, traindays_start, validationdays, testdays\n# If we want to use the covariate-sorted portfolio, we need a matrix Z_t for every time point that contains all the\n# covariates of all coins at time t\ndef initialize_Z(self):\nX = np.array(self.X)\nZ = X.swapaxes(0, 1)\nZ = [pd.DataFrame(Z[i, :, :]) for i in np.arange(len(Z))]\nfor df in Z:\ndf.columns = self.X[0].columns\nZ_t = {day: df for day, df in zip(self.X[0].index, Z)}\nself.Z_t = Z_t\nself.reduce_Z_to_observed_values()\n# Reduces the matrices Z by eliminating the rows of coins that are not even observed on the day indexing the row\ndef reduce_Z_to_observed_values(self):\nY = self.Y\nobserved_coins = dict()\nfor day in Y.index:\nseries_of_returns_that_day = Y.loc[day, :]\nobserved_coins[day] = [coin for coin in\nseries_of_returns_that_day.index[~np.isnan(series_of_returns_that_day)]]\n# keep only days one day before returns\nshifted_days = [day - pd.to_timedelta(1, unit=\"day\") for day in Y.index]\nZ_t_reduced = dict()\nfor day_before in shifted_days:\nZ_t_reduced[day_before] = Z_t[day_before]\nfor day_before, day in zip(shifted_days, Y.index):\nZ_t_reduced[day_before] = Z_t_reduced[day_before].loc[observed_coins[day], :]\nself.Z_t = Z_t_reduced\nself.observed_coins = observed_coins\ndef calculate_sorted_portfolio_returns(self):\nportfolio_returns_per_day = {}\nfor day_before, day in zip(Z_t.keys(), Y.index):\nportfolio_returns = np.matmul(np.transpose(Z_t[day_before].replace(np.nan, 0)),\nnp.array(Y.loc[day, :].replace(np.nan, 0)))\nportfolio_returns_per_day[day] = portfolio_returns\nsorted_portfolios = pd.DataFrame.from_dict(portfolio_returns_per_day, orient=\"index\")\nself.sorted_portfolios = sorted_portfolios\ndef populate_covar_sorted_portfolios(self):\nobserved_coins = self.observed_coins\nportfolios_per_day = {}\nZtransposeZ = np.matmul(np.transpose(np.array(Z_t[day_before])), np.array(Z_t[day_before]))\nZtransposeZ_inverse = np.linalg.inv(ZtransposeZ)\nZ_transpose_Z_inverse_Z = np.matmul(ZtransposeZ_inverse, np.transpose(np.array(Z_t[day_before])))\nportfolio_returns = np.matmul(Z_transpose_Z_inverse_Z, Y.loc[day, observed_coins[day]])\nportfolios_per_day[day] = portfolio_returns\nsorted_portfolios = pd.DataFrame.from_dict(portfolios_per_day, orient=\"index\", columns=self.X[0].columns)\ndef shift_X_Y_train_valid_test_forwards(self):\nself.traindays_end = self.traindays_end + self.predictiondays\nif self.testdays > self.predictiondays:\nself.testdays = self.testdays - self.predictiondays\nself.Y_train, = self.Y.iloc[\nself.traindays_start: self.Y.iloc[\nself.traindays_end: + self.validationdays], self.Y.iloc[\nself.traindays_end + self.validationdays:]\nself.X_train, = [df.iloc[self.traindays_start:self.traindays_end] for df in\nself.X], \\\nself.traindays_end: + self.validationdays] for\ndf in self.X], \\\n[df.iloc[self.traindays_end + self.validationdays:] for df in\nself.X]\nself.within_time_limit = False\nprint(\"End of data frames reached.\")\ndef tune_hp(self):\nY_train, Y_valid, variables, factor_dim, list_covar_hidden_neurons = self.Y_train, self.variables, self.factor_dim, self.list_covar_hidden_neurons\nhypermodel = ca_hypermodel(Y_train, X_train, Y_valid, variables, factor_dim, list_covar_hidden_neurons)\nes = self.callback\nepochs = self.epochs\ntrials = self.tuning_trials\nmodel_directory = self.model_directory\ntuner = BayesianOptimization(\nhypermodel,\nobjective='val_loss',\noverwrite=True,\nmax_trials=trials\n)\ntuner.search(X_train, Y_train, epochs=epochs, validation_data=(X_valid, Y_valid), callbacks=es)\nhyperparameters = tuner.get_best_hyperparameters()[0]\nnumber_parallel_models = self.number_parallel_models\nmodel = tuner.hypermodel.build(hyperparameters)\nmodel.save(model_directory + \"/Hypermodel\")\n'''for i, model in enumerate(model_list):\nrandom.seed(i)\nmodel.fit(\nx = X_train,\ncallbacks = es,\nvalidation_data = (X_valid,\n)'''\n# weights = [model.get_weights() for model in model_list]\nself.best_current_hp = hyperparameters\n# self.weights_list.append(weights)\ndef load_hypermodel(self):\ndirectory = self.model_directory\nnumber_parallel = self.number_parallel_models\nmodel_list = [keras.models.load_model(directory + \"/Hypermodel\") for i in np.arange(number_parallel)]\nself.best_current_model_list = model_list\ndef fit_model(self, stepnumber):\nprint(\"Fitting step number \" + str(stepnumber) + \".\")\nX_train, X_valid, Y_train, Y_valid = self.X_train, self.Y_valid\n# Load hypermodel\nfor i, model in enumerate(self.best_current_model_list):\ntraining_history = model.fit(\nx=X_train,\ncallbacks=es,\nvalidation_data=(X_valid, Y_valid)\n)\nmodel.save(model_directory + \"/\" + str(stepnumber) + \"/\" + str(i))\ntry:\nos.makedirs(model_directory + \"/Training History/\" + str(stepnumber))\nexcept:\nprint(\"Directory \" + model_directory + \"/Training History/\" + str(stepnumber) + \" already exists.\")\nwith open(model_directory + \"/Training History/\" + str(stepnumber) + \"/\" + str(i) + \".pkl\", \"wb\") as file:\npickle.dump(training_history.history, file)\n# self.best_current_model_list = current_model_list\ndef predict_next_testperiod_contemporaneous_returns(self):\npredictiondays = self.predictiondays\nX_test = self.X_test\ncurrent_model_list = self.best_current_model_list\ncurrent_X_test = [df.iloc[:predictiondays] for df in X_test]\ncurrent_Y_pred_list = [model.predict(current_X_test) for model in current_model_list]\ncurrent_Y_pred_list = [pd.DataFrame(current_Y_pred, index=current_Y_test.index, columns=current_Y_test.columns)\nfor current_Y_pred in current_Y_pred_list]\nself.list_of_lists_contemporaneous_predictions.append(current_Y_pred_list)\ndef predict_contemporaneous_returns_from_saved(self, models, step, set_var_to_zero):\nif set_var_to_zero != \"none\":\nfor df in X_test[:-1]:\ndf[set_var_to_zero].values[:] = 0\ncurrent_model_list = models\n# Make sure that we have more enough days left to fill a prediction period. If not, shorten prediction period\n# Important for end of dataframe\nif predictiondays > Y_test.shape[0]:\nfor i, df in zip(np.arange(len(current_Y_pred_list)), current_Y_pred_list):\nif set_var_to_zero != \"none\":\ndirectory = self.model_directory + \"/Contemporaneous Predictions Variable Importance/\" + \\\nset_var_to_zero + \"/Step \" + str(step)\nelse:\ndirectory = self.model_directory + \"/Contemporaneous Predictions/Step \" + str(step)\nif not os.path.exists(directory):\nos.makedirs(directory, exist_ok=True)\ndf.to_csv(directory + \"/Parallel Network \" + str(i) + \".csv\")\n# self.list_of_lists_contemporaneous_predictions.append(current_Y_pred_list)\ndef predict_next_testperiod_future_returns(self):\nnum_coins = self.Y_train.shape[1]\nX_train = self.X_train\nX_train = [pd.concat([train, valid]) for (train, valid) in zip(X_train, X_valid)]\nY_train = self.Y_train\nY_train = pd.concat([Y_train, Y_valid])\nif predictiondays > Y_test.shape[0]: predictiondays = Y_test.shape[0]\ncurrent_X = [pd.concat([train, test]) for (train, test) in zip(X_train, X_test)]\n# Model we work with\nmodel_list = self.best_current_model_list\n# Extract the (average) factor predictions from the model\nlayer_name = 'Factors'\nfactors_layer_model_list = [keras.Model(inputs=model.input,\noutputs=model.get_layer(layer_name).output) for model in model_list]\nfactor_predictions_list = [model.predict(current_X) for model in factors_layer_model_list]\nfactor_sample_averages_list = [np.array([factor_predictions[:i].mean(axis=0) for i in\nnp.arange(start=1, stop=factor_predictions.shape[0] + 1)]) for\nfactor_predictions in factor_predictions_list]\n# Extract the beta predictions from the other part of the network\nlayer_names = [\"betas\" + str(i) for i in np.arange(num_coins)]\nbetas_layer_model_list = [[keras.Model(inputs=model.input,\noutputs=model.get_layer(layer_name).output) for\nlayer_name in layer_names] for model in model_list]\nbetas_predictions_list = [[model.predict(current_X) for model in parallel_model] for parallel_model in\nbetas_layer_model_list]\ncurrent_Y_future_pred_list = [np.array(\n[np.multiply(np.array(betas_predictions)[i][-predictiondays:, ],\nfactor_sample_averages[-(predictiondays + 1):-1, ]).sum(axis=1)\nfor i in np.arange(len(betas_predictions))]).transpose()\nfor (factor_sample_averages, betas_predictions) in\ncurrent_Y_future_pred_list = [\npd.DataFrame(current_Y_future_pred, index=current_Y_test.index, columns=current_Y_test.columns)\nfor current_Y_future_pred in current_Y_future_pred_list]\nself.list_of_lists_future_predictions.append(current_Y_future_pred_list)\ndef predict_future_returns_from_saved(self, models, step, factor_averaging_days=\"all\", set_var_to_zero=\"none\"):\ncurrent_X = [pd.concat([train, test]) for (train, test) in zip(X_train, current_X_test)]\n# If we want to test variable importance\nfor df in current_X[:-1]:\nmodel_list = models\nif factor_averaging_days == \"all\":\nfactor_sample_averages_list = [\nnp.array([factor_predictions[:i].mean(axis=0) for i in\nnp.arange(start=1, stop=factor_predictions.shape[0] + 1)]) for factor_predictions in\nfactor_predictions_list]\nnp.array([factor_predictions[np.max([i - factor_averaging_days, 0]):i].mean(axis=0) for i in\ndirectory = self.model_directory + \"/Betas and Factors Predictions Variable Importance/\" + \\\nset_var_to_zero + \"/Factors/Step \" + str(step)\ndirectory = self.model_directory + \"/Betas and Factors Predictions/Factors/Step \" + str(step)\nif not os.path.exists(directory):\nos.makedirs(directory, exist_ok=True)\nfor i, df in zip(np.arange(len(factor_predictions_list)), factor_predictions_list):\nwith open(directory + \"/\" + str(i) + \".pkl\", \"wb\") as file:\npickle.dump(df, file)\nbetas_layer_model_list = [keras.Model(\ninputs=model.input,\noutputs=[\nmodel.get_layer(layer_name).output for layer_name in layer_names]\nfor model in model_list]\nbetas_predictions_list = [model.predict(current_X) for model in betas_layer_model_list]\nset_var_to_zero + \"/Betas/Step \" + str(step)\ndirectory = self.model_directory + \"/Betas and Factors Predictions/Betas/Step \" + str(step)\nfor betas_predictions in betas_predictions_list:\npickle.dump(betas_predictions, file)\nfor i, df in zip(np.arange(len(current_Y_future_pred_list)), current_Y_future_pred_list):\nif set_var_to_zero == \"none\":\nif factor_averaging_days == \"all\":\ndirectory = self.model_directory + \"/Future Predictions/Step \" + str(step)\nelse:\ndirectory = self.model_directory + \"/Future Predictions \" + \\\nstr(factor_averaging_days) + \" days factor average/Step \" + str(step)\ndirectory = self.model_directory + \"/Future Predictions Variable Importance/\" + \\\nset_var_to_zero + \"/Step \" + str(step)\ndirectory = self.model_directory + \"/Future Predictions Variable Importance \" + \\\nstr(factor_averaging_days) + \" days factor average/\" \\\n+ set_var_to_zero + \"/Step \" + str(step)\n# self.list_of_lists_future_predictions.append(current_Y_future_pred_list)\ndef fit_all_models(self, step=0):\nstepnumber = 0\nwhile stepnumber < step:\nself.shift_X_Y_train_valid_test_forwards()\nstepnumber = stepnumber + 1\nprint(stepnumber)\nwhile self.within_time_limit:\nself.fit_model(stepnumber)\ndef reset_X_Y(self):\nself.X_train = self.X_train_global\n# Reset everything\nnumberdays = self.Y.shape[0]\ntest_percent = self.test_percent\ntestdays = int(np.ceil(numberdays * test_percent))\nvalidation_percent = self.validation_percent\nvalidationdays = int(np.ceil(traindays_end * validation_percent))\nself.traindays_start, self.validationdays, self.testdays = 0, traindays_end, validationdays, testdays\ndef load_parallel_models(self, stepnumber):\ncurrent_model_list = [keras.models.load_model(model_directory + \"/\" + str(stepnumber) + \"/\" + str(i)) for i in\nnp.arange(number_parallel)]\nreturn current_model_list\ndef make_contemporaneous_predictions(self, set_var_to_zero=\"none\"):\nself.reset_X_Y()\nprint(\"Make predictions of step number \" + str(stepnumber))\nloaded_models = self.load_parallel_models(stepnumber)\nself.predict_contemporaneous_returns_from_saved(models=loaded_models, step=stepnumber,\nset_var_to_zero=set_var_to_zero)\ndef make_future_predictions(self, set_var_to_zero=\"none\", factor_averaging_days=\"all\"):\nself.predict_future_returns_from_saved(models=loaded_models, step=stepnumber,\nfactor_averaging_days=factor_averaging_days,\ndef update_contemporaneous_Y_predictions(self, set_var_to_zero=\"none\"):\nif set_var_to_zero == \"none\":\ncontemporaneous_predictions_directory = self.model_directory + \"/Contemporaneous Predictions\"\n\"/Contemporaneous Predictions Variable Importance/\" + \\\nset_var_to_zero\nstepnumbers_model = os.listdir(contemporaneous_predictions_directory)\ntry:\nstepnumbers_model = [int(step[5:]) for step in stepnumbers_model]\nstepnumbers_model.sort()\nexcept:\nraise ValueError(\"There seem to be file names in the model directory that cannot be converted to integers.\")\nlist_of_lists_contemporaneous_predictions = [\n[\npd.read_csv(\ncontemporaneous_predictions_directory + \"/Step \" + str(step) + \"/Parallel Network \" + str(\ni) + \".csv\", index_col=\"day\"\n) for i in np.arange(number_parallel)\n] for step in stepnumbers_model\n]\nself.Y_pred_cont_list = [\npd.concat([parallel_predictions[i] for parallel_predictions in list_of_lists_contemporaneous_predictions],\naxis=0)\nfor i in np.arange(number_parallel)]\ndef update_future_Y_predictions(self, factor_averaging_days=\"all\", set_var_to_zero=\"none\"):\nif factor_averaging_days == \"all\":\nfuture_predictions_directory = self.model_directory + \"/Future Predictions\"\nfactor_averaging_days) + \" days factor average\"\nfuture_predictions_directory = self.model_directory + \"/Future Predictions Variable Importance /\" + \\\nset_var_to_zero\nfuture_predictions_directory = self.model_directory + \"/Future Predictions Variable Importance \" + \\\nstr(factor_averaging_days) + \\\nstepnumbers_model = os.listdir(future_predictions_directory)\nlist_of_lists_future_predictions = [\nfuture_predictions_directory + \"/Step \" + str(step) + \"/Parallel Network \" + str(\nself.Y_pred_future_list = [\npd.concat([parallel_predictions[i] for parallel_predictions in list_of_lists_future_predictions],\ndef calculate_contemporaneous_r2(self, masked=True):\nY_test = self.Y_test_global\nY_pred_cont_list = self.Y_pred_cont_list\nmask = 1 - self.exclusion_mask\nmask = np.array(mask.iloc[-self.testdays:, :])\nif not masked:\nmask = mask ** 0\nY_pred_cont_panel = np.array(Y_pred_cont_list)\nY_pred_cont_average = Y_pred_cont_panel.mean(axis=0)\n# Total R^2\n# For every single parallel model\nsum_squared_residuals_list = [(((np.array(Y_test) - np.array(Y_pred_cont)) ** 2)).sum().sum() for Y_pred_cont in\nY_pred_cont_list]\nmasked_sum_squared_residuals_list = [\n((((np.array(Y_test) - np.array(Y_pred_cont)) * mask) ** 2) * mask).sum().sum() for Y_pred_cont in\nY_pred_cont_list]\nsum_squared_returns = ((Y_test ** 2)).sum().sum()\nRsquared_total_list = [1 - sum_squared_residuals / sum_squared_returns\nfor sum_squared_residuals in sum_squared_residuals_list]\nmasked_Rsquared_total_list = [1 - masked_sum_squared_residuals / masked_sum_squared_returns\nfor masked_sum_squared_residuals in masked_sum_squared_residuals_list]\n# For the average\nsum_squared_residuals_average = (((np.array(Y_test) - np.array(Y_pred_cont_average)) ** 2)).sum().sum()\nmasked_sum_squared_residuals_average = (\n((np.array(Y_test) - np.array(masked_Y_pred_cont_average)) ** 2) * mask).sum().sum()\nRsquared_total_average = 1 - sum_squared_residuals_average / sum_squared_returns\nself.masked_Y_test = Y_test * mask\nself.masked_Y_pred_cont_average = masked_Y_pred_cont_average\nself.sum_squared_residuals_list = sum_squared_residuals_list\nself.masked_sum_squared_returns = masked_sum_squared_returns\nself.sum_squared_residuals_average = sum_squared_residuals_average\nself.Y_pred_cont_average = Y_pred_cont_average\nself.Rsquared_total_list = Rsquared_total_list\nself.masked_Rsquared_total_average = masked_Rsquared_total_average\ndef calculate_predictive_r2(self):\nY_pred_future_list = self.Y_pred_future_list\n# Masked: Only predictions where Y_test nonzero are counted. Y_test = 0 is assumed to only be the case when coins are eliminated\nmasked_Y_pred_future_list = [Y_pred_future * mask for Y_pred_future in Y_pred_future_list]\nY_pred_future_panel = np.array(Y_pred_future_list)\nY_pred_future_average = Y_pred_future_panel.mean(axis=0)\n# Predictive R^2\nsum_squared_residuals_predictive_list = [((np.array(Y_test) - np.array(Y_pred_future)) ** 2).sum().sum()\nfor Y_pred_future in Y_pred_future_list]\nsum_squared_returns = (Y_test ** 2).sum().sum()\nRsquared_predictive_list = [1 - sum_squared_residuals_predictive / sum_squared_returns\nfor sum_squared_residuals_predictive in sum_squared_residuals_predictive_list]\nmasked_sum_squared_residuals_predictive_list = [\n(((np.array(Y_test) - np.array(Y_pred_future)) * mask) ** 2).sum().sum()\nfor Y_pred_future in Y_pred_future_list]\nmasked_Y_test = Y_test * mask\nmasked_sum_squared_returns = ((np.array(Y_test) * mask) ** 2).sum().sum()\nmasked_Rsquared_predictive_list = [1 - sum_squared_residuals_predictive / masked_sum_squared_returns\nfor sum_squared_residuals_predictive in\nsum_squared_residuals_average = ((np.array(Y_test) - np.array(Y_pred_future_average)) ** 2).sum().sum()\nRsquared_predictive_average = 1 - sum_squared_residuals_average / sum_squared_returns\nself.masked_Y_test = masked_Y_test\nself.masked_Y_pred_future_list = masked_Y_pred_future_list\nself.Y_pred_future_average = Y_pred_future_average\nself.Rsquared_predictive_list = Rsquared_predictive_list\nself.masked_Rsquared_predictive_average = masked_Rsquared_predictive_average\nclass ca_hypermodel(kt.HyperModel):\ndef __init__(self, Y_train, Y_valid, variables, dim_factors, list_hidden_neurons):\nsuper().__init__(self)\nself.Y_train, self.variables, self.dim_factors, self.list_hidden_neurons = \\\ndef build(self, hp):\n# Build model\nconditional_autoencoder_model = link_both_parts(num_coins=self.Y_train.shape[1],\ninput_dimension=self.X_train[-1].shape[1],\n[0.005, 0.0001]))\n# Optimizer we use\nopt = keras.optimizers.Adam(\nlearning_rate=hp.Choice(\"learning rate\", [0.0001, 0.001])\n# Compile model\nconditional_autoencoder_model.compile(\nloss=\"mean_squared_error\",\noptimizer=opt\nreturn conditional_autoencoder_model\ndef fit(self, hp, model, *args, **kwargs):\n# Fit the Model\nreturn model.fit(\n*args,\nbatch_size=hp.Choice(\"batch size\", [32, 64]),\n**kwargs\n", "output_sequence": "LVU thesis: This notebook creates the plots resulting from the analysis of different window sizes for the training of the Conditional Autoencoder."}, {"input_sequence": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport my_functions_v13 as my\nimport importlib\nfull_coins_model = my.cond_auto_model(full_input = True, parallel_runs = 2, factor_dim=number_factors, model_name = \"322 coins with intercept v13 training window \"+str(window_size_training))\n#full_coins_model.make_contemporaneous_predictions()\nfull_coins_model = my.cond_auto_model(full_input = True, factor_dim=number_factors, parallel_runs = 2, model_name = \"322 coins with intercept v13 training window \"+str(window_size_training))\nfull_coins_model.test_percent = 1 - window_size_training\nY_pred_future_panel = np.array(Y_pred_list)\ndef Rsquared(end):\nsum_squared_residuals_predictive_list = [((np.array(Y_test[end - 30:end]) - np.array(Y_pred_future[end - 30:end])) ** 2).sum().sum()\nfor Y_pred_future in Y_pred_list]\nsum_squared_returns = (Y_test[end-30:end] ** 2).sum().sum()\n\nx = np.arange(len(Y_test))\nmasked_Y_pred_future_panel = np.array(masked_Y_pred_list)\nsum_squared_residuals_predictive_list = [((np.array(Y_test[:end]) - np.array(masked_Y_pred_future[:end])) ** 2).sum().sum()\nfor masked_Y_pred_future in masked_Y_pred_list]\nsum_squared_returns = (Y_test[:end] ** 2).sum().sum()\nfull_coins_model.update_contemporaneous_Y_predictions()\nfull_coins_model.update_future_Y_predictions(factor_averaging_days=80)\nfull_coins_model.calculate_contemporaneous_r2()\nY_pred_cont = full_coins_model.masked_Y_pred_cont_average[-1194:]\nprint(\n1 - ((Y_pred_cont - Y_test)**2).sum().sum() / (Y_test**2).sum().sum()\n", "output_sequence": "LVU thesis: These notebooks analyzes the Conditional Autoencoder's performance for varying training window lengths."}, {"input_sequence": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport my_functions_v13 as my\nimport importlib\nfull_coins_model = my.cond_auto_model(full_input = True, parallel_runs = 2, factor_dim=number_factors, model_name = \"322 coins with intercept v13 training window \"+str(window_size_training))\n#full_coins_model.make_contemporaneous_predictions()\nfull_coins_model = my.cond_auto_model(full_input = True, factor_dim=number_factors, parallel_runs = 2, model_name = \"322 coins with intercept v13 training window \"+str(window_size_training))\nfull_coins_model.test_percent = 1 - window_size_training\nY_pred_future_panel = np.array(Y_pred_list)\ndef Rsquared(end):\nsum_squared_residuals_predictive_list = [((np.array(Y_test[end - 30:end]) - np.array(Y_pred_future[end - 30:end])) ** 2).sum().sum()\nfor Y_pred_future in Y_pred_list]\nsum_squared_returns = (Y_test[end-30:end] ** 2).sum().sum()\n\nx = np.arange(len(Y_test))\nmasked_Y_pred_future_panel = np.array(masked_Y_pred_list)\nsum_squared_residuals_predictive_list = [((np.array(Y_test[:end]) - np.array(masked_Y_pred_future[:end])) ** 2).sum().sum()\nfor masked_Y_pred_future in masked_Y_pred_list]\nsum_squared_returns = (Y_test[:end] ** 2).sum().sum()\nfull_coins_model.update_contemporaneous_Y_predictions()\nfull_coins_model.update_future_Y_predictions(factor_averaging_days=80)\nfull_coins_model.calculate_contemporaneous_r2()\nY_pred_cont = full_coins_model.masked_Y_pred_cont_average[-1194:]\nprint(\n1 - ((Y_pred_cont - Y_test)**2).sum().sum() / (Y_test**2).sum().sum()\n", "output_sequence": "LVU thesis: These notebooks analyzes the Conditional Autoencoder's performance for varying training window lengths."}, {"input_sequence": "# Data fixes before feature engineering --> X and Y are already initialized\n# V12: Same as V9, but additional functionality in make_predictions for variable importance analysis\n# V13: Same as V12, but number of factors included in model directory, added intercept to X, fixed the covariates sorted portfolios now (used later for interpretation of latent factors and performance benchmark)\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import rankdata\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.regularizers import l2\nimport keras_tuner as kt\nfrom kerastuner.tuners import Hyperband\nimport tqdm\nfrom kerastuner.tuners import BayesianOptimization\nimport random\nfrom scipy.stats.mstats import winsorize\nimport pdb\nimport pickle\nfrom sklearn.preprocessing import StandardScaler\nimport os\n# Function to read .csv files. Uses only those that have a price column. Returns list of dataframes, their names\n# and the sorted times\ndef read_files(data_path, remove_coins=[]):\nfilenames = os.listdir(data_path)\nfilenames.sort()\n# Select only .csv files\nfilenames = [file for file in filenames if file[-4:] == \".csv\" and file[:-4] not in remove_coins]\n# Read files\ndflist = [pd.read_csv(data_path + \"/\" + file, index_col=\"day\") for file in filenames]\nfor df in dflist: df.index = pd.to_datetime(df.index)\n# Extract all time points in a chronologically sorted manner\nsorted_times = sort_all_times(dflist)\nreturn dflist, filenames, sorted_times\n# Function that takes a list of data frames and returns a chronologically ordered array of all times\n# Times need to be in index\ndef sort_all_times(dflist):\n# Extract list of times from each dataframe\nlist_times = [x.index for x in dflist]\n# List of sets of times from each dataframe\nlist_times_sets = [set(x) for x in list_times]\n# Unite all sets\nall_times = list_times_sets[0]\nfor i in range(len(list_times_sets)):\nall_times = all_times.union(list_times_sets[i])\n# Save as array\nall_times_array = pd.array(all_times)\n# Sort chronologically\nall_times_sorted = pd.array(sorted(all_times_array))\nreturn all_times_sorted\n# Function that takes a list of pandas Series and concatenates them all into one dataframe, where each column is one\n# of the original series. Second argument is the list of names of the columns\ndef combine_returns(list_return_dfs, list_coin_names):\nfor i in range(len(list_return_dfs)):\nif i == 0:\nprices_df = list_return_dfs[i]\nelse:\nprices_df = pd.concat([prices_df, list_return_dfs[i]], axis=1)\nprices_df.columns = [coin[:-4] for coin in list_coin_names]\nreturn prices_df\n# Function that takes as input a list of dataframes and gives as output a list of all column/covariate names\ndef all_columns(dflist):\ncolumns_list = [column for df in dflist for column in df.columns]\ncolumns_set = set(columns_list)\ncolumns_final_list = list(columns_set)\nreturn columns_final_list\n# Function that\n# takes as input:\n# list of dataframes, date as string, minimum number of coins, minimum percentage of observations per\n# variable per coin, list of dataframe names\n# gives as output:\n# List of coin names, List of covariate names, X (returns + covariates), y (returns)\n# Here covariates are rank standardized crosssectionally\ndef prepare_X_y_rank_standardized(dflist, startdate, enddate, filenames, variables):\n# Cut away all time points before the variable date\nselected_times = sorted_times[sorted_times >= startdate]\n# Cut away all time points after the variable enddate\nselected_times = selected_times[selected_times <= enddate]\ndflist_cut = [df.reindex(selected_times) for df in dflist]\n# If a data point needs to be excluded, we delete all info\nfor df in dflist_cut:\ndf.loc[df[\"exclude_training_and_testing\"] == \"True\", [\"daily excess return\", \"close\"]] = np.nan\n# Split up excess returns, rest of data and exclusion mask\nlist_of_excess_returns = [df[\"daily excess return\"] for df in dflist_cut]\ndf_returns = combine_returns(list_of_excess_returns, filenames)\nlist_of_close_prices = [df[\"close\"] for df in dflist_cut]\ndf_close_prices = combine_returns(list_of_close_prices, filenames)\nlist_of_exclusions = [df[\"exclude_testing\"] for df in dflist_cut]\nexclusion_mask = combine_returns(list_of_exclusions, filenames)\nexclusion_mask.replace(np.nan, True, inplace=True)\nlist_of_covariates = [df.loc[:, variables] for df in dflist_cut]\n# Working with a list of dataframes is computationally cumbersome. We change to a 3d numpy-array\ndata_covariates = np.array(list_of_covariates)\n# Replace NAs of covariates with cross-sectional median of other coins\ndata_covariates_nan_median = np.where(np.isnan(data_covariates),\nma.median(ma.array(data_covariates, mask=np.isnan(data_covariates)), axis=0),\ndata_covariates)\n# Rank normalize covariates to (-1, 1)\ncovariates_rank_normalized = 2 * (rankdata(data_covariates_nan_median, axis=0) - 1) \\\n/ (data_covariates.shape[0] - 1) - 1\n# Go back to list of dataframes\nlist_covariates_dfs = [pd.DataFrame(data=covariates_rank_normalized[i, :, :],\ncolumns=variables,\nfor i in np.arange(covariates_rank_normalized.shape[0])]\ncovariates_unstandardized = list_of_covariates\nreturn df_returns, list_covariates_dfs, covariates_unstandardized, df_close_prices, exclusion_mask\ndef create_covariates_nn(num_coins, num_covariates, encoding_dim, list_no_hidden, lambda_reg):\ndict_nn = {}\nhidden = [layers.Dense(no_hidden, activation='relu', kernel_regularizer=l2(lambda_reg)) for no_hidden in\nlist_no_hidden]\nfor i in range(num_coins):\nsubdict = {}\nsubdict[\"input\"] = keras.Input(shape=(num_covariates,))\nsubdict[\"hidden\"] = hidden[0](subdict[\"input\"])\nif len(hidden) > 1:\nfor layer in hidden[1:]:\nsubdict[\"hidden\"] = layer(subdict[\"hidden\"])\nsubdict[\"output\"] = layers.Dense(encoding_dim,\nname=\"betas\" + str(i),\nkernel_regularizer=l2(lambda_reg))(subdict[\"hidden\"])\n# subdict[\"model\"] = keras.Model(inputs = subdict[\"input\"], outputs = subdict[\"output\"])\ndict_nn[\"{}\".format(i)] = subdict\nreturn dict_nn\n# Create the encoder part for the returns\ndef create_returns_encoder(shape_input, encoding_dim, lambda_reg):\ndict_encoder = {}\n# These are our inputs for the factors\ndict_encoder[\"input\"] = keras.Input(shape=(shape_input,))\n# \"encoded\" is the encoded representation of the input\ndict_encoder[\"output\"] = layers.Dense(encoding_dim,\nname=\"Factors\",\nkernel_regularizer=l2(lambda_reg))(dict_encoder[\"input\"])\nreturn dict_encoder\n# Function that links the parallel networks of the covariate part with the output of the returns part\ndef link_both_parts(num_coins, input_dimension, num_covariates, encoding_dim, list_no_hidden, lambda_reg=0.01):\ndict_covars = create_covariates_nn(num_coins, num_covariates, encoding_dim, list_no_hidden, lambda_reg)\ndict_dot_product = {}\nencoder = create_returns_encoder(input_dimension, encoding_dim, lambda_reg)\ndict_dot_product[str(i)] = tf.keras.layers.Dot(axes=1)([dict_covars[str(i)][\"output\"], encoder[\"output\"]])\nconcatted = tf.keras.layers.Concatenate()([dict_dot_product[str(i)] for i in range(num_coins)])\ninput_layers = [dict_covars[str(i)][\"input\"] for i in range(num_coins)]\ninput_layers.append(encoder[\"input\"])\nfull_model = tf.keras.Model(inputs=input_layers,\noutputs=concatted,\nname=\"full_model_{0}_parallel_networks\".format(num_coins))\nreturn full_model\nclass regression_model:\ndef __init__(self, data_path=\"../Aggregate Data v3\", remove_coins=[]):\nself.data_path = data_path\nself.dflist, self.filenames, self.sorted_times = read_files(self.data_path, remove_coins=remove_coins)\nself.variables = [\"new_addresses\", \"active_addresses\", \"bm\", \"volumeto\", \"size\", \"illiq\", \"capm beta\",\n\"capm alpha\", \"ivol\", \"turnover\", \"rvol\", \"bid-ask\",\n\"detrended turnover\", \"standard deviation turnover\", \"rel to high\",\n\"volume shock\", \"r2_1\", \"r13_2\",\n\"r30_14\", \"r180_60\", \"var_5\"]\n# Even though our data starts on 2017-01-01, we delete the first 180 days because we do not have data\n# there for a lot of the features we engineered ourselves\nself.startdate = pd.to_datetime(\"2017-01-01\") + pd.to_timedelta(180, \"day\")\n# Last day where we have data for bitcoin\nself.enddate = pd.to_datetime(\"2022-03-01\")\nself.test_percent = 0.5\nself.validation_percent = 1 / 3\ndef replace_infty(self):\ndflist = self.dflist\nreplaced = []\nfor df in dflist:\nnew = df.replace(to_replace=[np.inf, -np.inf], value=np.nan)\nreplaced.append(new)\nself.dflist = replaced\ndef prepare_rank_standardize_data(self):\nself.returns, self.variables_df_list, self.variables_df_list_unstandardized, self.close_prices, self.exclusion_mask = \\\nprepare_X_y_rank_standardized(self.dflist, self.startdate, self.filenames, self.variables)\nclass cond_auto_model():\ndef __init__(\nself, model_name, data_path=\"../Aggregate Data v3\", list_covar_hidden_neurons=[32],\nlist_factors_hidden_neurons=[5],\nfactor_dim=5, predictiondays=30, remove_coins=[], full_input=True, parallel_runs=10):\nself.variables = [\"new_addresses\", \"active_addresses\", \"bm\", \"volume\", \"standard deviation volume\",\n\"size\", \"illiq\", \"capm beta\", \"max\",\n\"volume shock 30\", \"volume shock 60\", \"r2_1\", \"r13_2\",\nself.list_covar_hidden_neurons = list_covar_hidden_neurons\nself.factor_dim = factor_dim\nself.within_time_limit = True\nself.predictiondays = predictiondays\nself.callback = [keras.callbacks.EarlyStopping(\nmonitor='val_loss',\nmin_delta=0.0001,\nrestore_best_weights=True,\npatience=300)]\nself.epochs = 1500\nself.tuning_trials = 24\nself.number_parallel_models = parallel_runs\nself.list_of_lists_contemporaneous_predictions = []\nself.weights_list = []\nself.full_input = full_input\nself.name = model_name + f\" {factor_dim} factors\"\ndef initialize_X_Y(self, replace_nan_return_with=np.nan,\nregression_model.replace_infty(self)\nregression_model.prepare_rank_standardize_data(self)\nself.model_directory = \"../Models/\" + self.name\nY = self.returns.iloc[1:].copy()\n# Y = Y.reset_index(drop=True)\nself.observed_mask = 1 - np.isnan(Y)\nY = Y.replace(to_replace=np.nan, value=0)\n# Winsorize to remove extreme returns\nfor i in np.arange(len(Y)):\nY.iloc[i, :] = np.array(winsorize(Y.iloc[i, :], limits=[0, 0.05], nan_policy=\"omit\"))\nX = [df.iloc[:-1] for df in self.variables_df_list]\n# X = [x.reset_index(drop=True) for x in X]\nX = [x.replace(to_replace=np.nan, value=replace_nan_covars_with) for x in X]\n# Add intercept\nfor df in X:\ndf[\"Intercept\"] = 1\nself.X = X\nself.initialize_Z()\n# self.calculate_sorted_portfolio_returns()\nsorted_portfolios = self.sorted_portfolios\n# Split up in training and test set\nnumberdays = Y.shape[0]\ntestdays = int(np.ceil(numberdays * self.test_percent))\ntraindays_end = numberdays - testdays\ntraindays_start = 0\nvalidationdays = int(np.ceil(traindays_end * self.validation_percent))\ntraindays_end = traindays_end - validationdays\n# Add constant regressor to the inputs\nY[\"constant\"] = 1\nsorted_portfolios[\"constant\"] = 1\nY_train, Y_valid, Y_test = Y.iloc[: traindays_end], Y.iloc[\ntraindays_end: + validationdays], Y.iloc[\ntraindays_end + validationdays:]\nportfolios_train, = \\\nsorted_portfolios.iloc[: traindays_end], sorted_portfolios.iloc[\ntraindays_end: + validationdays], sorted_portfolios.iloc[\ntraindays_end + validationdays:]\nX_train, X_valid, X_test = [df.iloc[:traindays_end] for df in X], \\\n[df.iloc[traindays_end: traindays_end + validationdays] for df in X], \\\n# Remove the constant intercept from the outputs. We only needed them temporarily because in the inputs we do actually need the constant\ndel Y[\"constant\"]\nif self.full_input:\nX_train.append(Y_train)\nX_valid.append(portfolios_valid)\nself.Y_test_global = Y_test\nself.X, \\\nself.X_train, \\\nself.X_valid_global, \\\nself.X_test, \\\nself.traindays_start, self.validationdays, self.testdays = \\\nX, Y, X_train, X_valid, \\\nX_valid, X_test, traindays_start, validationdays, testdays\n# If we want to use the covariate-sorted portfolio, we need a matrix Z_t for every time point that contains all the\n# covariates of all coins at time t\ndef initialize_Z(self):\nX = np.array(self.X)\nZ = X.swapaxes(0, 1)\nZ = [pd.DataFrame(Z[i, :, :]) for i in np.arange(len(Z))]\nfor df in Z:\ndf.columns = self.X[0].columns\nZ_t = {day: df for day, df in zip(self.X[0].index, Z)}\nself.Z_t = Z_t\nself.reduce_Z_to_observed_values()\n# Reduces the matrices Z by eliminating the rows of coins that are not even observed on the day indexing the row\ndef reduce_Z_to_observed_values(self):\nY = self.Y\nobserved_coins = dict()\nfor day in Y.index:\nseries_of_returns_that_day = Y.loc[day, :]\nobserved_coins[day] = [coin for coin in\nseries_of_returns_that_day.index[~np.isnan(series_of_returns_that_day)]]\n# keep only days one day before returns\nshifted_days = [day - pd.to_timedelta(1, unit=\"day\") for day in Y.index]\nZ_t_reduced = dict()\nfor day_before in shifted_days:\nZ_t_reduced[day_before] = Z_t[day_before]\nfor day_before, day in zip(shifted_days, Y.index):\nZ_t_reduced[day_before] = Z_t_reduced[day_before].loc[observed_coins[day], :]\nself.Z_t = Z_t_reduced\nself.observed_coins = observed_coins\ndef calculate_sorted_portfolio_returns(self):\nportfolio_returns_per_day = {}\nfor day_before, day in zip(Z_t.keys(), Y.index):\nportfolio_returns = np.matmul(np.transpose(Z_t[day_before].replace(np.nan, 0)),\nnp.array(Y.loc[day, :].replace(np.nan, 0)))\nportfolio_returns_per_day[day] = portfolio_returns\nsorted_portfolios = pd.DataFrame.from_dict(portfolio_returns_per_day, orient=\"index\")\nself.sorted_portfolios = sorted_portfolios\ndef populate_covar_sorted_portfolios(self):\nobserved_coins = self.observed_coins\nportfolios_per_day = {}\nZtransposeZ = np.matmul(np.transpose(np.array(Z_t[day_before])), np.array(Z_t[day_before]))\nZtransposeZ_inverse = np.linalg.inv(ZtransposeZ)\nZ_transpose_Z_inverse_Z = np.matmul(ZtransposeZ_inverse, np.transpose(np.array(Z_t[day_before])))\nportfolio_returns = np.matmul(Z_transpose_Z_inverse_Z, Y.loc[day, observed_coins[day]])\nportfolios_per_day[day] = portfolio_returns\nsorted_portfolios = pd.DataFrame.from_dict(portfolios_per_day, orient=\"index\", columns=self.X[0].columns)\ndef shift_X_Y_train_valid_test_forwards(self):\nself.traindays_end = self.traindays_end + self.predictiondays\nif self.testdays > self.predictiondays:\nself.testdays = self.testdays - self.predictiondays\nself.Y_train, = self.Y.iloc[\nself.traindays_start: self.Y.iloc[\nself.traindays_end: + self.validationdays], self.Y.iloc[\nself.traindays_end + self.validationdays:]\nself.X_train, = [df.iloc[self.traindays_start:self.traindays_end] for df in\nself.X], \\\nself.traindays_end: + self.validationdays] for\ndf in self.X], \\\n[df.iloc[self.traindays_end + self.validationdays:] for df in\nself.X]\nself.within_time_limit = False\nprint(\"End of data frames reached.\")\ndef tune_hp(self):\nY_train, Y_valid, variables, factor_dim, list_covar_hidden_neurons = self.Y_train, self.variables, self.factor_dim, self.list_covar_hidden_neurons\nhypermodel = ca_hypermodel(Y_train, X_train, Y_valid, variables, factor_dim, list_covar_hidden_neurons)\nes = self.callback\nepochs = self.epochs\ntrials = self.tuning_trials\nmodel_directory = self.model_directory\ntuner = BayesianOptimization(\nhypermodel,\nobjective='val_loss',\noverwrite=True,\nmax_trials=trials\n)\ntuner.search(X_train, Y_train, epochs=epochs, validation_data=(X_valid, Y_valid), callbacks=es)\nhyperparameters = tuner.get_best_hyperparameters()[0]\nnumber_parallel_models = self.number_parallel_models\nmodel = tuner.hypermodel.build(hyperparameters)\nmodel.save(model_directory + \"/Hypermodel\")\n'''for i, model in enumerate(model_list):\nrandom.seed(i)\nmodel.fit(\nx = X_train,\ncallbacks = es,\nvalidation_data = (X_valid,\n)'''\n# weights = [model.get_weights() for model in model_list]\nself.best_current_hp = hyperparameters\n# self.weights_list.append(weights)\ndef load_hypermodel(self):\ndirectory = self.model_directory\nnumber_parallel = self.number_parallel_models\nmodel_list = [keras.models.load_model(directory + \"/Hypermodel\") for i in np.arange(number_parallel)]\nself.best_current_model_list = model_list\ndef fit_model(self, stepnumber):\nprint(\"Fitting step number \" + str(stepnumber) + \".\")\nX_train, X_valid, Y_train, Y_valid = self.X_train, self.Y_valid\n# Load hypermodel\nfor i, model in enumerate(self.best_current_model_list):\ntraining_history = model.fit(\nx=X_train,\ncallbacks=es,\nvalidation_data=(X_valid, Y_valid)\n)\nmodel.save(model_directory + \"/\" + str(stepnumber) + \"/\" + str(i))\ntry:\nos.makedirs(model_directory + \"/Training History/\" + str(stepnumber))\nexcept:\nprint(\"Directory \" + model_directory + \"/Training History/\" + str(stepnumber) + \" already exists.\")\nwith open(model_directory + \"/Training History/\" + str(stepnumber) + \"/\" + str(i) + \".pkl\", \"wb\") as file:\npickle.dump(training_history.history, file)\n# self.best_current_model_list = current_model_list\ndef predict_next_testperiod_contemporaneous_returns(self):\npredictiondays = self.predictiondays\nX_test = self.X_test\ncurrent_model_list = self.best_current_model_list\ncurrent_X_test = [df.iloc[:predictiondays] for df in X_test]\ncurrent_Y_pred_list = [model.predict(current_X_test) for model in current_model_list]\ncurrent_Y_pred_list = [pd.DataFrame(current_Y_pred, index=current_Y_test.index, columns=current_Y_test.columns)\nfor current_Y_pred in current_Y_pred_list]\nself.list_of_lists_contemporaneous_predictions.append(current_Y_pred_list)\ndef predict_contemporaneous_returns_from_saved(self, models, step, set_var_to_zero):\nif set_var_to_zero != \"none\":\nfor df in X_test[:-1]:\ndf[set_var_to_zero].values[:] = 0\ncurrent_model_list = models\n# Make sure that we have more enough days left to fill a prediction period. If not, shorten prediction period\n# Important for end of dataframe\nif predictiondays > Y_test.shape[0]:\nfor i, df in zip(np.arange(len(current_Y_pred_list)), current_Y_pred_list):\nif set_var_to_zero != \"none\":\ndirectory = self.model_directory + \"/Contemporaneous Predictions Variable Importance/\" + \\\nset_var_to_zero + \"/Step \" + str(step)\nelse:\ndirectory = self.model_directory + \"/Contemporaneous Predictions/Step \" + str(step)\nif not os.path.exists(directory):\nos.makedirs(directory, exist_ok=True)\ndf.to_csv(directory + \"/Parallel Network \" + str(i) + \".csv\")\n# self.list_of_lists_contemporaneous_predictions.append(current_Y_pred_list)\ndef predict_next_testperiod_future_returns(self):\nnum_coins = self.Y_train.shape[1]\nX_train = self.X_train\nX_train = [pd.concat([train, valid]) for (train, valid) in zip(X_train, X_valid)]\nY_train = self.Y_train\nY_train = pd.concat([Y_train, Y_valid])\nif predictiondays > Y_test.shape[0]: predictiondays = Y_test.shape[0]\ncurrent_X = [pd.concat([train, test]) for (train, test) in zip(X_train, X_test)]\n# Model we work with\nmodel_list = self.best_current_model_list\n# Extract the (average) factor predictions from the model\nlayer_name = 'Factors'\nfactors_layer_model_list = [keras.Model(inputs=model.input,\noutputs=model.get_layer(layer_name).output) for model in model_list]\nfactor_predictions_list = [model.predict(current_X) for model in factors_layer_model_list]\nfactor_sample_averages_list = [np.array([factor_predictions[:i].mean(axis=0) for i in\nnp.arange(start=1, stop=factor_predictions.shape[0] + 1)]) for\nfactor_predictions in factor_predictions_list]\n# Extract the beta predictions from the other part of the network\nlayer_names = [\"betas\" + str(i) for i in np.arange(num_coins)]\nbetas_layer_model_list = [[keras.Model(inputs=model.input,\noutputs=model.get_layer(layer_name).output) for\nlayer_name in layer_names] for model in model_list]\nbetas_predictions_list = [[model.predict(current_X) for model in parallel_model] for parallel_model in\nbetas_layer_model_list]\ncurrent_Y_future_pred_list = [np.array(\n[np.multiply(np.array(betas_predictions)[i][-predictiondays:, ],\nfactor_sample_averages[-(predictiondays + 1):-1, ]).sum(axis=1)\nfor i in np.arange(len(betas_predictions))]).transpose()\nfor (factor_sample_averages, betas_predictions) in\ncurrent_Y_future_pred_list = [\npd.DataFrame(current_Y_future_pred, index=current_Y_test.index, columns=current_Y_test.columns)\nfor current_Y_future_pred in current_Y_future_pred_list]\nself.list_of_lists_future_predictions.append(current_Y_future_pred_list)\ndef predict_future_returns_from_saved(self, models, step, factor_averaging_days=\"all\", set_var_to_zero=\"none\"):\ncurrent_X = [pd.concat([train, test]) for (train, test) in zip(X_train, current_X_test)]\n# If we want to test variable importance\nfor df in current_X[:-1]:\nmodel_list = models\nif factor_averaging_days == \"all\":\nfactor_sample_averages_list = [\nnp.array([factor_predictions[:i].mean(axis=0) for i in\nnp.arange(start=1, stop=factor_predictions.shape[0] + 1)]) for factor_predictions in\nfactor_predictions_list]\nnp.array([factor_predictions[np.max([i - factor_averaging_days, 0]):i].mean(axis=0) for i in\ndirectory = self.model_directory + \"/Betas and Factors Predictions Variable Importance/\" + \\\nset_var_to_zero + \"/Factors/Step \" + str(step)\ndirectory = self.model_directory + \"/Betas and Factors Predictions/Factors/Step \" + str(step)\nif not os.path.exists(directory):\nos.makedirs(directory, exist_ok=True)\nfor i, df in zip(np.arange(len(factor_predictions_list)), factor_predictions_list):\nwith open(directory + \"/\" + str(i) + \".pkl\", \"wb\") as file:\npickle.dump(df, file)\nbetas_layer_model_list = [keras.Model(\ninputs=model.input,\noutputs=[\nmodel.get_layer(layer_name).output for layer_name in layer_names]\nfor model in model_list]\nbetas_predictions_list = [model.predict(current_X) for model in betas_layer_model_list]\nset_var_to_zero + \"/Betas/Step \" + str(step)\ndirectory = self.model_directory + \"/Betas and Factors Predictions/Betas/Step \" + str(step)\nfor betas_predictions in betas_predictions_list:\npickle.dump(betas_predictions, file)\nfor i, df in zip(np.arange(len(current_Y_future_pred_list)), current_Y_future_pred_list):\nif set_var_to_zero == \"none\":\nif factor_averaging_days == \"all\":\ndirectory = self.model_directory + \"/Future Predictions/Step \" + str(step)\nelse:\ndirectory = self.model_directory + \"/Future Predictions \" + \\\nstr(factor_averaging_days) + \" days factor average/Step \" + str(step)\ndirectory = self.model_directory + \"/Future Predictions Variable Importance/\" + \\\nset_var_to_zero + \"/Step \" + str(step)\ndirectory = self.model_directory + \"/Future Predictions Variable Importance \" + \\\nstr(factor_averaging_days) + \" days factor average/\" \\\n+ set_var_to_zero + \"/Step \" + str(step)\n# self.list_of_lists_future_predictions.append(current_Y_future_pred_list)\ndef fit_all_models(self, step=0):\nstepnumber = 0\nwhile stepnumber < step:\nself.shift_X_Y_train_valid_test_forwards()\nstepnumber = stepnumber + 1\nprint(stepnumber)\nwhile self.within_time_limit:\nself.fit_model(stepnumber)\ndef reset_X_Y(self):\nself.X_train = self.X_train_global\n# Reset everything\nnumberdays = self.Y.shape[0]\ntest_percent = self.test_percent\ntestdays = int(np.ceil(numberdays * test_percent))\nvalidation_percent = self.validation_percent\nvalidationdays = int(np.ceil(traindays_end * validation_percent))\nself.traindays_start, self.validationdays, self.testdays = 0, traindays_end, validationdays, testdays\ndef load_parallel_models(self, stepnumber):\ncurrent_model_list = [keras.models.load_model(model_directory + \"/\" + str(stepnumber) + \"/\" + str(i)) for i in\nnp.arange(number_parallel)]\nreturn current_model_list\ndef make_contemporaneous_predictions(self, set_var_to_zero=\"none\"):\nself.reset_X_Y()\nprint(\"Make predictions of step number \" + str(stepnumber))\nloaded_models = self.load_parallel_models(stepnumber)\nself.predict_contemporaneous_returns_from_saved(models=loaded_models, step=stepnumber,\nset_var_to_zero=set_var_to_zero)\ndef make_future_predictions(self, set_var_to_zero=\"none\", factor_averaging_days=\"all\"):\nself.predict_future_returns_from_saved(models=loaded_models, step=stepnumber,\nfactor_averaging_days=factor_averaging_days,\ndef update_contemporaneous_Y_predictions(self, set_var_to_zero=\"none\"):\nif set_var_to_zero == \"none\":\ncontemporaneous_predictions_directory = self.model_directory + \"/Contemporaneous Predictions\"\n\"/Contemporaneous Predictions Variable Importance/\" + \\\nset_var_to_zero\nstepnumbers_model = os.listdir(contemporaneous_predictions_directory)\ntry:\nstepnumbers_model = [int(step[5:]) for step in stepnumbers_model]\nstepnumbers_model.sort()\nexcept:\nraise ValueError(\"There seem to be file names in the model directory that cannot be converted to integers.\")\nlist_of_lists_contemporaneous_predictions = [\n[\npd.read_csv(\ncontemporaneous_predictions_directory + \"/Step \" + str(step) + \"/Parallel Network \" + str(\ni) + \".csv\", index_col=\"day\"\n) for i in np.arange(number_parallel)\n] for step in stepnumbers_model\n]\nself.Y_pred_cont_list = [\npd.concat([parallel_predictions[i] for parallel_predictions in list_of_lists_contemporaneous_predictions],\naxis=0)\nfor i in np.arange(number_parallel)]\ndef update_future_Y_predictions(self, factor_averaging_days=\"all\", set_var_to_zero=\"none\"):\nif factor_averaging_days == \"all\":\nfuture_predictions_directory = self.model_directory + \"/Future Predictions\"\nfactor_averaging_days) + \" days factor average\"\nfuture_predictions_directory = self.model_directory + \"/Future Predictions Variable Importance /\" + \\\nset_var_to_zero\nfuture_predictions_directory = self.model_directory + \"/Future Predictions Variable Importance \" + \\\nstr(factor_averaging_days) + \\\nstepnumbers_model = os.listdir(future_predictions_directory)\nlist_of_lists_future_predictions = [\nfuture_predictions_directory + \"/Step \" + str(step) + \"/Parallel Network \" + str(\nself.Y_pred_future_list = [\npd.concat([parallel_predictions[i] for parallel_predictions in list_of_lists_future_predictions],\ndef calculate_contemporaneous_r2(self, masked=True):\nY_test = self.Y_test_global\nY_pred_cont_list = self.Y_pred_cont_list\nmask = 1 - self.exclusion_mask\nmask = np.array(mask.iloc[-self.testdays:, :])\nif not masked:\nmask = mask ** 0\nY_pred_cont_panel = np.array(Y_pred_cont_list)\nY_pred_cont_average = Y_pred_cont_panel.mean(axis=0)\n# Total R^2\n# For every single parallel model\nsum_squared_residuals_list = [(((np.array(Y_test) - np.array(Y_pred_cont)) ** 2)).sum().sum() for Y_pred_cont in\nY_pred_cont_list]\nmasked_sum_squared_residuals_list = [\n((((np.array(Y_test) - np.array(Y_pred_cont)) * mask) ** 2) * mask).sum().sum() for Y_pred_cont in\nY_pred_cont_list]\nsum_squared_returns = ((Y_test ** 2)).sum().sum()\nRsquared_total_list = [1 - sum_squared_residuals / sum_squared_returns\nfor sum_squared_residuals in sum_squared_residuals_list]\nmasked_Rsquared_total_list = [1 - masked_sum_squared_residuals / masked_sum_squared_returns\nfor masked_sum_squared_residuals in masked_sum_squared_residuals_list]\n# For the average\nsum_squared_residuals_average = (((np.array(Y_test) - np.array(Y_pred_cont_average)) ** 2)).sum().sum()\nmasked_sum_squared_residuals_average = (\n((np.array(Y_test) - np.array(masked_Y_pred_cont_average)) ** 2) * mask).sum().sum()\nRsquared_total_average = 1 - sum_squared_residuals_average / sum_squared_returns\nself.masked_Y_test = Y_test * mask\nself.masked_Y_pred_cont_average = masked_Y_pred_cont_average\nself.sum_squared_residuals_list = sum_squared_residuals_list\nself.masked_sum_squared_returns = masked_sum_squared_returns\nself.sum_squared_residuals_average = sum_squared_residuals_average\nself.Y_pred_cont_average = Y_pred_cont_average\nself.Rsquared_total_list = Rsquared_total_list\nself.masked_Rsquared_total_average = masked_Rsquared_total_average\ndef calculate_predictive_r2(self):\nY_pred_future_list = self.Y_pred_future_list\n# Masked: Only predictions where Y_test nonzero are counted. Y_test = 0 is assumed to only be the case when coins are eliminated\nmasked_Y_pred_future_list = [Y_pred_future * mask for Y_pred_future in Y_pred_future_list]\nY_pred_future_panel = np.array(Y_pred_future_list)\nY_pred_future_average = Y_pred_future_panel.mean(axis=0)\n# Predictive R^2\nsum_squared_residuals_predictive_list = [((np.array(Y_test) - np.array(Y_pred_future)) ** 2).sum().sum()\nfor Y_pred_future in Y_pred_future_list]\nsum_squared_returns = (Y_test ** 2).sum().sum()\nRsquared_predictive_list = [1 - sum_squared_residuals_predictive / sum_squared_returns\nfor sum_squared_residuals_predictive in sum_squared_residuals_predictive_list]\nmasked_sum_squared_residuals_predictive_list = [\n(((np.array(Y_test) - np.array(Y_pred_future)) * mask) ** 2).sum().sum()\nfor Y_pred_future in Y_pred_future_list]\nmasked_Y_test = Y_test * mask\nmasked_sum_squared_returns = ((np.array(Y_test) * mask) ** 2).sum().sum()\nmasked_Rsquared_predictive_list = [1 - sum_squared_residuals_predictive / masked_sum_squared_returns\nfor sum_squared_residuals_predictive in\nsum_squared_residuals_average = ((np.array(Y_test) - np.array(Y_pred_future_average)) ** 2).sum().sum()\nRsquared_predictive_average = 1 - sum_squared_residuals_average / sum_squared_returns\nself.masked_Y_test = masked_Y_test\nself.masked_Y_pred_future_list = masked_Y_pred_future_list\nself.Y_pred_future_average = Y_pred_future_average\nself.Rsquared_predictive_list = Rsquared_predictive_list\nself.masked_Rsquared_predictive_average = masked_Rsquared_predictive_average\nclass ca_hypermodel(kt.HyperModel):\ndef __init__(self, Y_train, Y_valid, variables, dim_factors, list_hidden_neurons):\nsuper().__init__(self)\nself.Y_train, self.variables, self.dim_factors, self.list_hidden_neurons = \\\ndef build(self, hp):\n# Build model\nconditional_autoencoder_model = link_both_parts(num_coins=self.Y_train.shape[1],\ninput_dimension=self.X_train[-1].shape[1],\n[0.005, 0.0001]))\n# Optimizer we use\nopt = keras.optimizers.Adam(\nlearning_rate=hp.Choice(\"learning rate\", [0.0001, 0.001])\n# Compile model\nconditional_autoencoder_model.compile(\nloss=\"mean_squared_error\",\noptimizer=opt\nreturn conditional_autoencoder_model\ndef fit(self, hp, model, *args, **kwargs):\n# Fit the Model\nreturn model.fit(\n*args,\nbatch_size=hp.Choice(\"batch size\", [32, 64]),\n**kwargs\n", "output_sequence": "LVU thesis: These notebooks analyzes the Conditional Autoencoder's performance for varying training window lengths."}, {"input_sequence": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport my_functions_v13 as my\nimport importlib\nfull_coins_model = my.cond_auto_model(full_input = True, parallel_runs = 2, factor_dim=number_factors, model_name = \"322 coins with intercept v13 training window \"+str(window_size_training))\n#full_coins_model.make_contemporaneous_predictions()\nfull_coins_model = my.cond_auto_model(full_input = True, factor_dim=number_factors, parallel_runs = 2, model_name = \"322 coins with intercept v13 training window \"+str(window_size_training))\nfull_coins_model.test_percent = 1 - window_size_training\nY_pred_future_panel = np.array(Y_pred_list)\ndef Rsquared(end):\nsum_squared_residuals_predictive_list = [((np.array(Y_test[end - 30:end]) - np.array(Y_pred_future[end - 30:end])) ** 2).sum().sum()\nfor Y_pred_future in Y_pred_list]\nsum_squared_returns = (Y_test[end-30:end] ** 2).sum().sum()\n\nx = np.arange(len(Y_test))\nmasked_Y_pred_future_panel = np.array(masked_Y_pred_list)\nsum_squared_residuals_predictive_list = [((np.array(Y_test[:end]) - np.array(masked_Y_pred_future[:end])) ** 2).sum().sum()\nfor masked_Y_pred_future in masked_Y_pred_list]\nsum_squared_returns = (Y_test[:end] ** 2).sum().sum()\nfull_coins_model.update_contemporaneous_Y_predictions()\nfull_coins_model.update_future_Y_predictions(factor_averaging_days=80)\nfull_coins_model.calculate_contemporaneous_r2()\nY_pred_cont = full_coins_model.masked_Y_pred_cont_average[-1194:]\nprint(\n1 - ((Y_pred_cont - Y_test)**2).sum().sum() / (Y_test**2).sum().sum()\n", "output_sequence": "LVU thesis: These notebooks analyzes the Conditional Autoencoder's performance for varying training window lengths."}, {"input_sequence": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport my_functions_v13 as my\nimport importlib\nfull_coins_model = my.cond_auto_model(full_input = True, parallel_runs = 2, factor_dim=number_factors, model_name = \"322 coins with intercept v13 training window \"+str(window_size_training))\n#full_coins_model.make_contemporaneous_predictions()\nfull_coins_model = my.cond_auto_model(full_input = True, factor_dim=number_factors, parallel_runs = 2, model_name = \"322 coins with intercept v13 training window \"+str(window_size_training))\nfull_coins_model.test_percent = 1 - window_size_training\nY_pred_future_panel = np.array(Y_pred_list)\ndef Rsquared(end):\nsum_squared_residuals_predictive_list = [((np.array(Y_test[end - 30:end]) - np.array(Y_pred_future[end - 30:end])) ** 2).sum().sum()\nfor Y_pred_future in Y_pred_list]\nsum_squared_returns = (Y_test[end-30:end] ** 2).sum().sum()\n\nx = np.arange(len(Y_test))\nmasked_Y_pred_future_panel = np.array(masked_Y_pred_list)\nsum_squared_residuals_predictive_list = [((np.array(Y_test[:end]) - np.array(masked_Y_pred_future[:end])) ** 2).sum().sum()\nfor masked_Y_pred_future in masked_Y_pred_list]\nsum_squared_returns = (Y_test[:end] ** 2).sum().sum()\nfull_coins_model.update_contemporaneous_Y_predictions()\nfull_coins_model.update_future_Y_predictions(factor_averaging_days=80)\nfull_coins_model.calculate_contemporaneous_r2()\nY_pred_cont = full_coins_model.masked_Y_pred_cont_average[-1194:]\nprint(\n1 - ((Y_pred_cont - Y_test)**2).sum().sum() / (Y_test**2).sum().sum()\n", "output_sequence": "LVU thesis: These notebooks analyzes the Conditional Autoencoder's performance for varying training window lengths."}, {"input_sequence": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport my_functions_v13 as my\nimport importlib\nfull_coins_model = my.cond_auto_model(full_input = True, parallel_runs = 2, factor_dim=number_factors, model_name = \"322 coins with intercept v14 training window \"+str(window_size_training))\n#full_coins_model.make_contemporaneous_predictions()\nfull_coins_model = my.cond_auto_model(full_input = True, factor_dim=number_factors, parallel_runs = 2, model_name = \"322 coins with intercept v14 training window \"+str(window_size_training))\nfull_coins_model.test_percent = 1 - window_size_training\nY_pred_future_panel = np.array(Y_pred_list)\ndef Rsquared(end):\nsum_squared_residuals_predictive_list = [((np.array(Y_test[end - 30:end]) - np.array(Y_pred_future[end - 30:end])) ** 2).sum().sum()\nfor Y_pred_future in Y_pred_list]\nsum_squared_returns = (Y_test[end-30:end] ** 2).sum().sum()\n\nx = np.arange(len(Y_test))\nmasked_Y_pred_future_panel = np.array(masked_Y_pred_list)\nsum_squared_residuals_predictive_list = [((np.array(Y_test[:end]) - np.array(masked_Y_pred_future[:end])) ** 2).sum().sum()\nfor masked_Y_pred_future in masked_Y_pred_list]\nsum_squared_returns = (Y_test[:end] ** 2).sum().sum()\nfull_coins_model.update_contemporaneous_Y_predictions()\nfull_coins_model.update_future_Y_predictions(factor_averaging_days=80)\nfull_coins_model.calculate_contemporaneous_r2()\nY_pred_cont = full_coins_model.masked_Y_pred_cont_average[-1194:]\nprint(\n1 - ((Y_pred_cont - Y_test)**2).sum().sum() / (Y_test**2).sum().sum()\n", "output_sequence": "LVU thesis: These notebooks analyzes the Conditional Autoencoder's performance for varying training window lengths."}, {"input_sequence": "import scipy as sp\nimport matplotlib.pyplot as plt\nimport os\nimport pandas as pd\nimport seaborn as sb\nimport missingno as mn\nfrom datetime import datetime, timedelta\nfrom functools import reduce\nimport importlib\nfrom time import time\nplt.style.use(\"classic\")\nimport my_functions_v6 as my\nfrom sklearn import linear_model as lm\nimport statsmodels.api as sm\nimportlib.reload(my)\nprices = dict(\nzip(\n[file[:-4] for file in price_files if file[-4:] == \".csv\"],\n[pd.read_csv(\"../Cryptocompare data/Prices/\" + file, index_col= \"Unnamed: 0\") for file in price_files if file[-4:] == \".csv\"]\n)\n# Calculate daily returns\nfor coin in prices.keys():\nblockchain_data = dict(\n[pd.read_csv(\"../Cryptocompare data/Blockchain Data/\" + file, index_col= \"Unnamed: 0\") for file in blockchain_data_files if file[-4:] == \".csv\"]\nfor key in blockchain_data.keys():\ndf.index = [datetime.fromtimestamp(time) for time in df.time]\ndel df[\"id\"]\n# Combine price and blockchain data\nWe only keep days for which we have both the price and blockchain information.\naggregate_data = {}\nfor key in prices.keys():\nprice = prices[key]\ncovariates = blockchain_data[key]\nprice[\"day\"] = [str(time)[:10] for time in price.timestamp]\ncovariates[\"day\"] = [str(id)[:10] for id in covariates.index]\naggregate = pd.merge(left = price, right = covariates, left_on=\"day\", how = \"inner\")\naggregate.index = pd.to_datetime(aggregate[\"day\"])\ndel aggregate[\"day\"]\n# Keep only after 2017\nfor coin in aggregate_data.keys():\nyield_files = os.listdir(\"../Daily Treasury Par Yield Curve Rates\")\ntreasury_yield_rates = pd.concat(\n[pd.read_csv(\"../Daily Treasury Par Yield Curve Rates/\" + file, index_col= \"Date\") for file in yield_files if file[-4:] == \".csv\"]\ntreasury_yield_rates.head()\n# As we can see, the format of the dates is different from the one we use in our other datasets, so we will equalize it\ntreasury_yield_rates.index = [date[-4:] + \"-\" + date[:2] + \"-\" + date[3:5] for date in list(treasury_yield_rates.index)]\n# On the weekends and some other days there is no data. We just copy the last known value\ndays = treasury_yield_rates.index.to_numpy()\ndays = pd.to_datetime(days)\n# All days between first and last day\nidx = pd.date_range(days[0], days[-1])\n# Reindex\ntreasury_yield_rates.index = pd.to_datetime(treasury_yield_rates.index)\n# Remove invalid coins\n- SMT has buggy supply variable (in the order of 10^56)\n# https://coinmarketcap.com/view/usd-stablecoin/\nUSDStablecoins = [\"USDT\", \"DAI\", \"TUSD\", \"USTC\", \"GUSD\", \"FRAX\", \"LUSD\", \"VAI\", \"CUSD\", \"EOSDT\", \"DOLA\", \"DUSD\", \"TOR\", \"ALUSD\",\ninvalid_coins.extend(USDStablecoins)\n# https://coinmarketcap.com/view/stablecoin/\nStablecoins = [\"USDT\", \"DAI\", \"TUSD\", \"FEI\", \"USTC\", \"GUSD\"]\ninvalid_coins.extend(Stablecoins)\n# https://coinmarketcap.com/view/eur-stablecoin/\nEURStablecoins = [\"EURS\", \"PAR\", \"EEUR\", \"EUROS\"]\ninvalid_coins.extend(EURStablecoins)\n# https://coinmarketcap.com/view/algorithmic-stablecoin/\nAlgoStablecoins = [\"USDD\", \"FEI\", \"USTC\", \"TRIBE\", \"FRAX\", \"USDX\", \"AMPL\", \"CEUR\", \"PAR\", \"MIMATIC\", \"DUSD\", \"TOR\", \"USDS\", \"EUROS\", \"USN\", \"SAI\", \"U8D\", \"YFLUSD\", \"zUSD\"]\ninvalid_coins.extend(AlgoStablecoins)\n# https://coinmarketcap.com/view/tokenized-gold/\nTokenizedGold = [\"PAXG\", \"XAUT\", \"PMGT\", \"DGX\", \"AWG\"]\ninvalid_coins.extend(TokenizedGold)\n# https://coinmarketcap.com/view/centralized-exchange/\nCentralizedExchange = [\"BNB\", \"LEO\", \"FTT\", \"CRO\", \"KCS\", \"OKB\", \"HT\", \"WOO\", \"LN\", \"MX\", \"WRX\", \"ASD\", \"WXT\", \"TKO\", \"LA\", \"BTSE\", \"BF\", \"BIR\", \"PROB\", \"BTR\", \"ONE\", \"SCC\", \"EXM\", \"BIX\", \"WELL\", \"MAX\", \"TCH\", \"PTF\", \"COB\", \"DGTX\", \"BIT\", \"NGC\", \"GDT\", \"BNS\", \"ZMT\", \"BEST\", \"TKX\", \"HTB\", \"BITO\", \"PLA\", \"BST\", \"THR\", \"RES\", \"BWB\", \"OBITS\", \"LKK\", \"BLN\", \"CREDO\", \"XIN\", \"CNX\", \"BCO\", \"DEW\", \"SXDT\", \"LGO\", \"BNK\", \"ZB\", \"TIOX\", \"NET\", \"BQQQ\", \"BHC\", \"ACXT\", \"EQO\", \"OVI\"]\ninvalid_coins.extend(CentralizedExchange)\n# https://coinmarketcap.com/view/derivatives/\nDerivatives = [\"FTT\", \"SNX\", \"UMA\", \"INJ\", \"DYDX\", \"XPRT\", \"PERP\", \"MNGO\", \"RBN\", \"VEGA\", \"CHESS\", \"MIR\", \"DDX\", \"SWTH\", \"MCB\", \"HEGIC\", \"MATTER\", \"FLX\", \"DERI\", \"OOE\", \"GOVI\", \"BNC\", \"HZN\", \"ODDZ\", \"WHITE\", \"HGET\", \"OPIUM\", \"JRT\", \"SAKE\", \"PTF\", \"DOWS\", \"AUC\", \"DGTX\", \"FNX\", \"PREMIA\", \"ETH2X-FLI\", \"GDT\", \"LIEN\", \"STF\", \"GFARM2\", \"SI\", \"DFT\", \"OIL\", \"RISQ\", \"LEV\", \"DEUS\", \"SYN\", \"DEA\", \"FFLY\", \"BTCDOM\", \"YOLO\", \"TBA\"]\ninvalid_coins.extend(Derivatives)\n# https://coinmarketcap.com/view/wrapped-tokens/\nWrappedCoins = [\"WBTC\", \"RENBTC\", \"WNXM\", \"XTM\", \"WOA\", \"RENDOGE\", \"WCK\", \"RUNE\", \"WMATIC\", \"WHT\", \"WWAN\", \"RENZEC\", \"WXMR\", \"WBIND\", \"WLEO\", \"WCCX\", \"WZEC\", \"wCRES\", \"WCELO\", \"WSHIFT\", \"MAUSDC\", \"MAYFI\"]\ninvalid_coins.extend(WrappedCoins)\n# https://coinmarketcap.com/view/options/\nOptionsTokens = [\"HEGIC\", \"MATTER\", \"DERI\", \"ODDZ\", \"HGET\", \"PTF\", \"AUC\", \"FNX\", \"PREMIA\", \"DPX\", \"LYRA\", \"SI\", \"RISQ\"]\ninvalid_coins.extend(OptionsTokens)\n# Remove Coins that have a price of 0 for at least 10% of the observations after having had a nonzero price\nzero_price_coins = []\ncloseprice = aggregate_data[coin][\"close\"]\nfirst_nonzero = (closeprice > 0).idxmax()\nif (closeprice.loc[first_nonzero:last_nonzero] == 0).sum() / len(closeprice.loc[first_nonzero:last_nonzero] == 0) > 0.1:\nzero_price_coins.append(coin)\n# Remove coins where traded volume is 0 in more than 10% of days after having had a nonzero traded volume\nzero_volume_coins = []\nvolume = aggregate_data[coin][\"volumefrom\"]\nfirst_nonzero = (volume > 0).idxmax()\nif (volume.loc[first_nonzero:last_nonzero] == 0).sum() / len(volume.loc[first_nonzero:last_nonzero]) > 0.1:\nzero_volume_coins.append(coin)\nremoved = []\nnotindata = []\nfor coin in invalid_coins:\ntry:\naggregate_data.pop(coin)\nexcept:\nnotindata.append(coin)\nelse:\nremoved.append(coin)\nprint(\"Coins that were removed: \")\nprint(removed)\nprint(\"Coins that were not part of the data: \")\nprint(notindata)\n# Volume\nfor key in aggregate_data.keys():\ndf = aggregate_data[key].copy()\ndf[\"volume\"] = df[\"volumefrom\"] * df[\"close\"]\n# First data fix: remove returns at days when traded volume is 0 or when price is 0\nfor df in aggregate_data.values():\ndf[\"exclude_testing\"] = False\ndf[\"exclude_training_and_testing\"] = np.isnan(df[\"daily excess return\"])\nzero_volume = df[\"volume\"] < 0.1\nzero_volume_next_day = zero_volume.shift(1).replace(np.NaN, False)\ndf.loc[zero_volume, \"daily excess return\"] = 0\ndf.loc[zero_volume, \"exclude_training_and_testing\"] = True\n# Also remove next day because prices on days with zero volume get REALLY weird so we want to exclude returns\n# that are calculated using these prices\ndf.loc[zero_volume_next_day, \"daily excess return\"] = 0\ndf.loc[zero_volume_next_day, \"exclude_training_and_testing\"] = True\n# Same for prices\nzero_price = df[\"close\"] == 0\nzero_price_next_day = zero_price.shift(1).replace(np.NaN, False)\ndf.loc[zero_price, \"daily excess return\"] = 0\ndf.loc[zero_price, \"exclude_training_and_testing\"] = True\ndf.loc[zero_price_next_day, \"daily excess return\"] = 0\ndf.loc[zero_price_next_day, \"exclude_training_and_testing\"] = True\n# Second data fix: remove extreme returns due to change of conversion type / symbol\n# Apply same logic for close prices for the sake of feature engineering\nextreme_days = df[(df[\"daily excess return\"] > 2) | (df[\"daily excess return\"] < 1/3 - 1)].index\ndays_before = df[((df[\"daily excess return\"] > 2) | (df[\"daily excess return\"] < 1/3 - 1)).shift(-1).replace(np.NaN, False)].index\nif len(extreme_days) > 0:\nfor (extreme_day, day_before, day_after) in zip(extreme_days, days_before, days_after):\nif (df.loc[extreme_day, \"conversionType\"] != df.loc[day_before, \"conversionType\"]) \\\n# Fix return\ndf.loc[extreme_day, \"daily excess return\"] = (df.loc[day_before, \"daily excess return\"] +\ndf.loc[day_after, \"daily excess return\"]) / 2\n# Fix price\ndf.loc[extreme_day, \"open\"] = df.loc[day_before, \"open\"] * (1 + df.loc[extreme_day, \"daily excess return\"])\n# Note that a data fix occurred\ndf.loc[extreme_day, \"exclude_testing\"] = True\nelif (df.loc[extreme_day, \"conversionType\"] != df.loc[day_before, \"conversionType\"]) \\\ndf.loc[extreme_day, \"daily excess return\"] = np.sqrt(\ndf.loc[day_after, \"close\"] / df.loc[day_before, \"close\"])\n# new add.\n\"The number of unique addresses that appeared for the first time in a transaction of the native coin in the network. Liu et al. (2021) provide some preliminary evidence on the predictive content of new addresses for cryptocurrency returns.\"\n# active add.\n\"The number of unique addresses that were active in the network either as a sender or receiver. Only addresses that were active in successful transactions are counted. As highlighted by Pagnotta and Buraschi (2018) such statistics approximate the network growth and the adoption base for a given cryptocurrency.\n# bm.\ndf[\"bm\"] = df[\"unique_addresses_all_time\"] / (df[\"current_supply\"] * df[\"close\"])\n# size.\ndf[\"size\"] = df[\"current_supply\"] * df[\"close\"]\n# Check coins that have volumefrom > size\n# Conservative check for \"fake\" volumes\ndf[\"exclude_training_and_testing\"] = (df[\"volume\"]) > df[\"size\"]\naggregate_data[key] = df\n# illiq.\nWe follow Amihud (2002) and calculate a price impact (illiquidity) measure as the ratio between the absolute value of the cumulative intraday returns and the aggregate daily trading volume expressed in \\$.\nfor key in tqdm(aggregate_data.keys()):\ndf[\"illiq\"] = np.nan\nfor id in df.index:\nx, y, z = df[\"close\"].loc[id],\nif x > 0 and y > 0 and z > 0:\ndf.loc[id, \"illiq\"] = np.log(abs(1 - x/y)) / z\n# capm\n# Union of all values in the days column\n# Create empty dataframe for close prices\nclose_prices = pd.DataFrame(index = all_days_sorted, columns = aggregate_data.keys())\n# Fill it with data\nfor coin in tqdm(close_prices.columns):\nindex_where_coin_has_data = aggregate_data[coin].index\nfor index in index_where_coin_has_data:\n# Create empty dataframe for market capitalization\nmarket_capitalization = pd.DataFrame(index = all_days_sorted, columns = aggregate_data.keys())\nmarket_capitalization.loc[index, coin] = aggregate_data[coin].loc[index, \"size\"]\n# Some coins have buggy data. BTC has the highest market capitalization so we can remove coins that supposedly have higher market capitalization\nextreme_market_cap = []\nfor coin in market_capitalization.columns:\nfor coin in extreme_market_cap:\ndel market_capitalization[coin]\n# Calculate sum of market capitalization\ntotal_market_capitalization = market_capitalization.sum(axis = 1)\n# Percentage of importance based on values\nvalue_weights = market_capitalization.divide(total_market_capitalization.replace(0, np.nan), axis = 0)\n# Price of value-weighted market portfolio\nmarket_price = value_weights.multiply(close_prices, axis = 1).sum(axis = 1)\n# Returns of market portfolio\nmarket_returns = market_price.pct_change()\nmarket_returns.index = pd.to_datetime(market_returns.index)\n## capm $\\beta$\n\"Second, and more important, nonsynchronous price movements can have a big impact on short-horizon betas. Lo and MacKinlay (1990) show that small stocks tend to react with a week or more delay to common news, so a daily or weekly beta will miss much of the small-stock covariance with market returns. To mitigate this problem, our tests focus on value-weighted portfolios and exclude NASDAQ stocks. Also, following Dimson (1979), we include both current and lagged market returns in the regressions, estimating beta as the sum of the slopes on all lags (alpha is still just the intercept). For daily returns, we include four lags of market returns, imposing the constraint that lags 2\u20134 have the same slope to reduce the number of parameters:\"\n$R_{i,t} = \\alpha_i + \\beta_{i0}R_{M,t} + \\beta_{i1}R_{M,t-1} + \\beta_{i2}[(R_{M,t-2} + R_{M,t-3} + R_{M,t-4})/3]$\n\"The daily beta is then $\\beta_i = \\beta_{i0} $. (Adding a few more lags doesn\u2019t affect the\n# Suppress runtime warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# Cycle through all the coins\nfor coin in tqdm(aggregate_data.keys()):\n# Create empty column where we will insert data\naggregate_data[coin][\"capm beta\"] = np.nan\n# One regression per rolling window of size 30 days:\nwindowsize = 30\n# How many windows?\nm = len(aggregate_data[coin]) - windowsize + 1\nfor i in np.arange(m):\n# Don't start on first day since there is no return, instead start on day 2\ndays_0_lag = aggregate_data[coin][\"daily excess return\"].iloc[i + 1 : i+31].index\nx0 = excess_market_returns.loc[days_0_lag].reset_index(drop = True)\nX = pd.concat([x0, x1], axis = 1)\nX = sm.add_constant(X)\n# Also start on day 2\ny = aggregate_data[coin][\"daily excess return\"].iloc[i + 1:i+31].reset_index(drop = True)\nif any(1 - np.isnan(y)) and any(1 - np.isnan(X)):\ntry:\nmodel = sm.OLS(y, X, missing='drop')\nexcept:\nbeta = np.nan\nelse:\nresults = model.fit()\nexcept:\nbeta = np.nan\nelse:\nbeta = results.params[0] + results.params[1]\nivol = np.nanstd(y - results.predict(X))\nexcept:\nbeta = np.nan\naggregate_data[coin].loc[days_0_lag[-1], \"capm beta\"] = beta\naggregate_data[\"BTC\"][\"ivol\"].plot()\n# ivol. all clear\nThe standard deviation from the residuals from a CAPM regression calculated based on a 30-day rolling window (see description of the capm \u03b2).\n# turnover.\n# rvol.\nWe follow Yang and Zhang (2000) and calculate the daily realised volatility calculated based on daily OHLC prices.\naggregate_data[coin][\"rvol\"] = np.nan\n# Don't start on first day since we need previous day's prices to normalize, instead start on day 2\n# To normalize the prices we sometimes need information from the current day, sometimes from the previous\n# We shift the indices of the 'previous' variable back after retrieving the information so that yesterday's information is matched to today\nclose_previous = aggregate_data[coin].loc[days_1_lag, \"close\"]\nclose_previous.index = close_previous.index + timedelta(days = 1)\nclose_current = aggregate_data[coin].loc[days_0_lag, \"close\"]\n# Normalize prices\nopen_normalized = np.log(open_current) - np.log(close_previous)\n# First estimator\nV_O = 1 / (windowsize - 1) * ((open_normalized - open_normalized.mean())**2).sum()\n# Second estimator\nV_C = 1 / (windowsize - 1) * ((close_normalized - close_normalized.mean())**2).sum()\n# Third estimator\nV_RS = 1 / windowsize * (high_normalized * (high_normalized - close_normalized) + low_normalized * (low_normalized - close_normalized)).sum()\n# Weighing constant k\n# Authors suggest to set alpha to 1.34 in practice\nalpha = 1.34\nk = (alpha - 1) / (alpha + (windowsize + 1) / (windowsize - 1))\n# Final, weighted estimator\nV = V_O + k * V_C + (1 - k) * V_RS\n# bid-ask.\nhigh_prices = aggregate_data[coin][\"high\"]\naggregate_data[coin][\"S Corwin Schultz interval with previous day\"] = np.nan\n# Corwin and Schultz\nfor id in high_prices.index[1:]:\nhigh_current = high_prices.loc[id]\nlow_previous = low_prices.shift(1).loc[id]\nhigh_over_two_days = np.max([high_current, high_previous])\n# Highs and lows over two consecutive days\ngamma = np.log(high_over_two_days / low_over_two_days) ** 2\n# Single highs and lows of two consecutive days\nbeta = np.sum([np.log(high_current / low_current) ** 2, np.log(high_previous / low_previous) ** 2])\nalpha = (np.sqrt(2 * beta) - np.sqrt(beta) ) / (3 - 2 * np.sqrt(2)) - np.sqrt(gamma / (3 - 2 * np.sqrt(2)))\nS = 2 * (np.exp(alpha) - 1) / (1 + np.exp(alpha))\n# Save only positive estimates (Adjustment C in original paper)\naggregate_data[coin].loc[id, \"S Corwin Schultz interval with previous day\"] = np.max([S,0])\n# Abdi and Ranaldo:\nfor id in close_prices.index[1:]:\n# Log of close prices of current and future day\nlog_close_current = np.log(close_prices.loc[id])\n# Mid ranges\nlog_mid_range_current = (np.log(low_prices.loc[id]) + np.log(high_prices.loc[id])) / 2\n# Actual estimate\ns_t = np.sqrt(np.max([4 * (log_close_current - log_mid_range_current) * (log_close_current - log_mid_range_previous), 0]))\naggregate_data[coin].loc[id, \"S Abdi Ranaldo interval with previous day\"] = s_t\n# These are all estimates over two-day intervals\n# To get daily estimates take the average of the two intervals that intersect on a given day (see internet apendix of original paper by Corwin Schultz)\naggregate_data[coin][\"S Corwin Schultz\"] = aggregate_data[coin][\"S Corwin Schultz interval with previous day\"]\naggregate_data[coin].loc[aggregate_data[coin].index[1:-1], \"S Corwin Schultz\"] = (aggregate_data[coin].loc[aggregate_data[coin].index[1:-1], \"S Corwin Schultz interval with previous day\"] + aggregate_data[coin].loc[aggregate_data[coin].index[1:], \"S Corwin Schultz interval with previous day\"].shift(-1)) / 2\naggregate_data[coin][\"S Abdi Ranaldo\"] = aggregate_data[coin][\"S Abdi Ranaldo interval with previous day\"]\naggregate_data[coin].loc[aggregate_data[coin].index[1:-1], \"S Abdi Ranaldo\"] = (aggregate_data[coin].loc[aggregate_data[coin].index[1:-1], \"S Abdi Ranaldo interval with previous day\"] + aggregate_data[coin].loc[aggregate_data[coin].index[1:], \"S Abdi Ranaldo interval with previous day\"].shift(-1)) / 2\n# Average of both estimators\naggregate_data[coin][\"bid-ask\"] = np.nanmean([aggregate_data[coin][\"S Corwin Schultz\"],aggregate_data[coin][\"S Abdi Ranaldo\"]], axis = 0)\ndel aggregate_data[coin][\"S Corwin Schultz interval with previous day\"]\n# dto. relatively clear (\"coins that have been burned\")\n# Create empty dataframe to save turnovers of all coins\nturnovers = pd.DataFrame(index = all_days_sorted, columns = aggregate_data.keys())\nfor coin in tqdm(turnovers.columns):\nturnovers.loc[index, coin] = aggregate_data[coin].loc[index, \"turnover\"]\n# Turnover of value-weighted market portfolio\n# Market adjusted turnover is individual turnover minus the whole market's turnover\n# The market adjusted turnover is detrended by subtracting the median of the previous 180 days (rolling window approach)\n# So first we calculate this median\npast_median_market_adjusted_turnovers = pd.DataFrame(index = market_adjusted_turnovers.index, columns = market_adjusted_turnovers.columns)\n# 2 cases: we either have at least 180 days before the current time point or we do not\n# Every coin can start at a different date, so we calculate this coin by coin\nfor coin in tqdm(market_adjusted_turnovers.columns):\n# First day where coin has turnover?\nfirstday = market_adjusted_turnovers[coin].notna().idxmax()\n# Last day where coin has turnover?\nlastday = market_adjusted_turnovers[coin][::-1].notna().idxmax()\nwindowrange = 180\n# Case if we have less than 180 observations IN TOTAL\nif len(market_adjusted_turnovers.loc[firstday : lastday, coin]) + 1 <= 180:\n# Now succesively populate next medians\nfor i in np.arange(windowrange):\npast_median_market_adjusted_turnovers.loc[firstday + timedelta(days = int(i)), coin] = np.median(market_adjusted_turnovers.loc[firstday : firstday + timedelta(days = int(i + 1)), coin])\n# Continue if we have more than 180 observations\nif len(market_adjusted_turnovers.loc[firstday : lastday, coin]) > 180:\nfor i in np.arange(windowrange, len(market_adjusted_turnovers.loc[firstday : lastday, coin]) + 1):\npast_median_market_adjusted_turnovers.loc[firstday + timedelta(days = int(i)), coin] = np.median(market_adjusted_turnovers.loc[firstday + timedelta(days = int(i + 1 - 180)) : firstday + timedelta(days = int(i + 1)), coin])\n# Detrended turnover is market adjusted turnover minus its past 180 days' median\n# Save results back\nfor coin in tqdm(detrended_turnover.columns):\n# std to. ???\nThe standard deviation of the residuals from a 30-day rolling window regression of daily turnover on a constant (see Chordia et al., 2001).\nall_days_sorted = my.sort_all_times([aggregate_data[key] for key in aggregate_data.keys()])\n# Create empty dataframe to save standard deviation of turnovers of all coins\nfor coin in tqdm(std_turnovers.columns):\n# Create 30 copies of turnovers of this coin and save as columns of a dataframe. Each column is lagged by an incremental amount of days (0 to 29)\n# Due to rolling window approach of size 30\nlagged_turnovers = pd.DataFrame({i: turnovers[coin].shift(i) for i in np.arange(30)})\n# New column with rowwise standard deviation\nlagged_turnovers[\"std\"] = np.std(lagged_turnovers, axis = 1)\n# Save back to dataframe with all standard deviations for all coins\n# std vol. ???\ndaily_trading_volumes = pd.DataFrame(index = all_days_sorted, columns = aggregate_data.keys())\nfor coin in tqdm(daily_trading_volumes.columns):\ndaily_trading_volumes.loc[index, coin] = aggregate_data[coin].loc[index, \"volume\"]\nfor coin in tqdm(std_volumes.columns):\n# Create 30 copies of trading volumes of this coin and save as columns of a dataframe. Each column is lagged by an incremental amount of days (0 to 29)\nlagged_volumes = pd.DataFrame({i: daily_trading_volumes[coin].shift(i) for i in np.arange(30)})\nlagged_volumes[\"std\"] = np.std(lagged_volumes, axis = 1)\n# rel to high. all clear\n# Create empty dataframe to save close prices of all coins\nclose_prices.loc[index, coin] = aggregate_data[coin].loc[index, \"close\"]\n# Create empty dataframe to save rel to high of all coins\n# Create 90 copies of close prices of this coin and save as columns of a dataframe. Each column is lagged by an incremental amount of days (1 to 90)\n# Since we need maximum of last 90 days\nlagged_close_prices = pd.DataFrame({i: close_prices[coin].shift(i) for i in np.arange(1, 91)})\n# New column with rel to high\nlagged_close_prices[\"rel_to_high\"] = lagged_close_prices[1] / np.max(lagged_close_prices, axis = 1).replace(0, np.nan)\n# Save back to dataframe with all rel to high for all coins\nfor coin in tqdm(rel_to_high.columns):\n# max. all clear\n# Create empty dataframe to save returns of all coins\ndaily_returns = pd.DataFrame(index = all_days_sorted, columns = aggregate_data.keys())\nfor coin in tqdm(daily_returns.columns):\ndaily_returns.loc[index, coin] = aggregate_data[coin].loc[index, \"daily excess return\"]\n# Create 30 copies of excess returns of this coin and save as columns of a dataframe. Each column is lagged by an incremental amount of days (0 to 29)\n# Since we need maximum of last 30 days\nlagged_returns = pd.DataFrame({i: daily_returns[coin].shift(i) for i in np.arange(30)})\nlagged_returns[\"max\"] = np.max(lagged_returns, axis = 1)\n# Save back to dataframe with all max returns for all coins\nfor coin in tqdm(max_return_one_month.columns):\n# vol shock ld. all clear\nfor coin in tqdm(vol_shock.columns):\n# Create 30 copies of trading volumes of this coin and save as columns of a dataframe. Each column is lagged by an incremental amount of days (0 to 30)\nlagged_volumes = pd.DataFrame({i: daily_trading_volumes[coin].shift(i) for i in np.arange(31)})\nlagged_volumes[\"volume_shock\"] = (np.log(lagged_volumes.loc[:, 0].astype(float)) - np.log((np.mean(lagged_volumes.loc[:, 1:30], axis = 1) / 30)).astype(float)) / np.log(np.std(lagged_volumes.loc[:, 1:30], axis = 1).astype(float))\n# Create 30 copies of trading volumes of this coin and save as columns of a dataframe. Each column is lagged by an incremental amount of days (0 to 60)\n# Due to rolling window approach of size 60\nlagged_volumes = pd.DataFrame({i: daily_trading_volumes[coin].shift(i) for i in np.arange(61)})\nlagged_volumes[\"volume_shock\"] = (np.log(lagged_volumes.loc[:, 0].astype(float)) - np.log((np.mean(lagged_volumes.loc[:, 1:60], axis = 1) / 60)).astype(float)) / np.log(np.std(lagged_volumes.loc[:, 1:60], axis = 1).astype(float))\n# Reversals\nVariety of cumulative returns.\n# Create empty dataframes to save various time horizon returns\nr2_1 = pd.DataFrame(index = all_days_sorted, columns = aggregate_data.keys())\n# Create 181 copies of close prices of this coin and save as columns of a dataframe. Each column is lagged by a different amount of days\nlagged_close_prices = pd.DataFrame({i: close_prices[coin].shift(i) for i in [1, 2, 7, 13, 22, 31, 14, 30,60, 180]})\n# New columns with reversals\nlagged_close_prices[\"r2_1\"] = 1 - lagged_close_prices[2] / lagged_close_prices[1]\n# Save back to dataframes with reversals of all coins\nr2_1[coin] = lagged_close_prices[\"r2_1\"]\nfor coin in tqdm(r2_1.columns):\naggregate_data[coin].loc[index, \"r2_1\"] = r2_1.loc[index, coin]\n# VaR(5%). just empirical quantile?\nfor coin in tqdm(var_5.columns):\n# Create 30 copies of excess returns of this coin and save as columns of a dataframe. Each column is lagged by an incremental amount of days (0 to 89)\nlagged_returns = pd.DataFrame({i: daily_returns[coin].shift(i) for i in np.arange(90)}).fillna(np.nan)\nlagged_returns[\"var_5\"] = np.quantile(lagged_returns, q = 0.05, axis = 1)\npath_aggregate_data = \"../Aggregate Data v3/\"\naggregate_data_files = os.listdir(path_aggregate_data)\naggregate_data = dict(\n[file[:-4] for file in aggregate_data_files if file[-4:] == \".csv\"],\n[pd.read_csv(path_aggregate_data + file, index_col=\"day\") for file in aggregate_data_files if file[-4:] == \".csv\"]\n)\n", "output_sequence": "LVU thesis: This notebook calculates the 27 observed asset characteristics from the raw data."}, {"input_sequence": "# Data fixes before feature engineering --> X and Y are already initialized\n# V12: Same as V9, but additional functionality in make_predictions for variable importance analysis\n# V13: Same as V12, but number of factors included in model directory, added intercept to X, fixed the covariates sorted portfolios now (used later for interpretation of latent factors and performance benchmark)\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import rankdata\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.regularizers import l2\nimport keras_tuner as kt\nfrom kerastuner.tuners import Hyperband\nimport tqdm\nfrom kerastuner.tuners import BayesianOptimization\nimport random\nfrom scipy.stats.mstats import winsorize\nimport pdb\nimport pickle\nfrom sklearn.preprocessing import StandardScaler\nimport os\n# Function to read .csv files. Uses only those that have a price column. Returns list of dataframes, their names\n# and the sorted times\ndef read_files(data_path, remove_coins=[]):\nfilenames = os.listdir(data_path)\nfilenames.sort()\n# Select only .csv files\nfilenames = [file for file in filenames if file[-4:] == \".csv\" and file[:-4] not in remove_coins]\n# Read files\ndflist = [pd.read_csv(data_path + \"/\" + file, index_col=\"day\") for file in filenames]\nfor df in dflist: df.index = pd.to_datetime(df.index)\n# Extract all time points in a chronologically sorted manner\nsorted_times = sort_all_times(dflist)\nreturn dflist, filenames, sorted_times\n# Function that takes a list of data frames and returns a chronologically ordered array of all times\n# Times need to be in index\ndef sort_all_times(dflist):\n# Extract list of times from each dataframe\nlist_times = [x.index for x in dflist]\n# List of sets of times from each dataframe\nlist_times_sets = [set(x) for x in list_times]\n# Unite all sets\nall_times = list_times_sets[0]\nfor i in range(len(list_times_sets)):\nall_times = all_times.union(list_times_sets[i])\n# Save as array\nall_times_array = pd.array(all_times)\n# Sort chronologically\nall_times_sorted = pd.array(sorted(all_times_array))\nreturn all_times_sorted\n# Function that takes a list of pandas Series and concatenates them all into one dataframe, where each column is one\n# of the original series. Second argument is the list of names of the columns\ndef combine_returns(list_return_dfs, list_coin_names):\nfor i in range(len(list_return_dfs)):\nif i == 0:\nprices_df = list_return_dfs[i]\nelse:\nprices_df = pd.concat([prices_df, list_return_dfs[i]], axis=1)\nprices_df.columns = [coin[:-4] for coin in list_coin_names]\nreturn prices_df\n# Function that takes as input a list of dataframes and gives as output a list of all column/covariate names\ndef all_columns(dflist):\ncolumns_list = [column for df in dflist for column in df.columns]\ncolumns_set = set(columns_list)\ncolumns_final_list = list(columns_set)\nreturn columns_final_list\n# Function that\n# takes as input:\n# list of dataframes, date as string, minimum number of coins, minimum percentage of observations per\n# variable per coin, list of dataframe names\n# gives as output:\n# List of coin names, List of covariate names, X (returns + covariates), y (returns)\n# Here covariates are rank standardized crosssectionally\ndef prepare_X_y_rank_standardized(dflist, startdate, enddate, filenames, variables):\n# Cut away all time points before the variable date\nselected_times = sorted_times[sorted_times >= startdate]\n# Cut away all time points after the variable enddate\nselected_times = selected_times[selected_times <= enddate]\ndflist_cut = [df.reindex(selected_times) for df in dflist]\n# If a data point needs to be excluded, we delete all info\nfor df in dflist_cut:\ndf.loc[df[\"exclude_training_and_testing\"] == \"True\", [\"daily excess return\", \"close\"]] = np.nan\n# Split up excess returns, rest of data and exclusion mask\nlist_of_excess_returns = [df[\"daily excess return\"] for df in dflist_cut]\ndf_returns = combine_returns(list_of_excess_returns, filenames)\nlist_of_close_prices = [df[\"close\"] for df in dflist_cut]\ndf_close_prices = combine_returns(list_of_close_prices, filenames)\nlist_of_exclusions = [df[\"exclude_testing\"] for df in dflist_cut]\nexclusion_mask = combine_returns(list_of_exclusions, filenames)\nexclusion_mask.replace(np.nan, True, inplace=True)\nlist_of_covariates = [df.loc[:, variables] for df in dflist_cut]\n# Working with a list of dataframes is computationally cumbersome. We change to a 3d numpy-array\ndata_covariates = np.array(list_of_covariates)\n# Replace NAs of covariates with cross-sectional median of other coins\ndata_covariates_nan_median = np.where(np.isnan(data_covariates),\nma.median(ma.array(data_covariates, mask=np.isnan(data_covariates)), axis=0),\ndata_covariates)\n# Rank normalize covariates to (-1, 1)\ncovariates_rank_normalized = 2 * (rankdata(data_covariates_nan_median, axis=0) - 1) \\\n/ (data_covariates.shape[0] - 1) - 1\n# Go back to list of dataframes\nlist_covariates_dfs = [pd.DataFrame(data=covariates_rank_normalized[i, :, :],\ncolumns=variables,\nfor i in np.arange(covariates_rank_normalized.shape[0])]\ncovariates_unstandardized = list_of_covariates\nreturn df_returns, list_covariates_dfs, covariates_unstandardized, df_close_prices, exclusion_mask\ndef create_covariates_nn(num_coins, num_covariates, encoding_dim, list_no_hidden, lambda_reg):\ndict_nn = {}\nhidden = [layers.Dense(no_hidden, activation='relu', kernel_regularizer=l2(lambda_reg)) for no_hidden in\nlist_no_hidden]\nfor i in range(num_coins):\nsubdict = {}\nsubdict[\"input\"] = keras.Input(shape=(num_covariates,))\nsubdict[\"hidden\"] = hidden[0](subdict[\"input\"])\nif len(hidden) > 1:\nfor layer in hidden[1:]:\nsubdict[\"hidden\"] = layer(subdict[\"hidden\"])\nsubdict[\"output\"] = layers.Dense(encoding_dim,\nname=\"betas\" + str(i),\nkernel_regularizer=l2(lambda_reg))(subdict[\"hidden\"])\n# subdict[\"model\"] = keras.Model(inputs = subdict[\"input\"], outputs = subdict[\"output\"])\ndict_nn[\"{}\".format(i)] = subdict\nreturn dict_nn\n# Create the encoder part for the returns\ndef create_returns_encoder(shape_input, encoding_dim, lambda_reg):\ndict_encoder = {}\n# These are our inputs for the factors\ndict_encoder[\"input\"] = keras.Input(shape=(shape_input,))\n# \"encoded\" is the encoded representation of the input\ndict_encoder[\"output\"] = layers.Dense(encoding_dim,\nname=\"Factors\",\nkernel_regularizer=l2(lambda_reg))(dict_encoder[\"input\"])\nreturn dict_encoder\n# Function that links the parallel networks of the covariate part with the output of the returns part\ndef link_both_parts(num_coins, input_dimension, num_covariates, encoding_dim, list_no_hidden, lambda_reg=0.01):\ndict_covars = create_covariates_nn(num_coins, num_covariates, encoding_dim, list_no_hidden, lambda_reg)\ndict_dot_product = {}\nencoder = create_returns_encoder(input_dimension, encoding_dim, lambda_reg)\ndict_dot_product[str(i)] = tf.keras.layers.Dot(axes=1)([dict_covars[str(i)][\"output\"], encoder[\"output\"]])\nconcatted = tf.keras.layers.Concatenate()([dict_dot_product[str(i)] for i in range(num_coins)])\ninput_layers = [dict_covars[str(i)][\"input\"] for i in range(num_coins)]\ninput_layers.append(encoder[\"input\"])\nfull_model = tf.keras.Model(inputs=input_layers,\noutputs=concatted,\nname=\"full_model_{0}_parallel_networks\".format(num_coins))\nreturn full_model\nclass regression_model:\ndef __init__(self, data_path=\"../Aggregate Data v3\", remove_coins=[]):\nself.data_path = data_path\nself.dflist, self.filenames, self.sorted_times = read_files(self.data_path, remove_coins=remove_coins)\nself.variables = [\"new_addresses\", \"active_addresses\", \"bm\", \"volumeto\", \"size\", \"illiq\", \"capm beta\",\n\"capm alpha\", \"ivol\", \"turnover\", \"rvol\", \"bid-ask\",\n\"detrended turnover\", \"standard deviation turnover\", \"rel to high\",\n\"volume shock\", \"r2_1\", \"r13_2\",\n\"r30_14\", \"r180_60\", \"var_5\"]\n# Even though our data starts on 2017-01-01, we delete the first 180 days because we do not have data\n# there for a lot of the features we engineered ourselves\nself.startdate = pd.to_datetime(\"2017-01-01\") + pd.to_timedelta(180, \"day\")\n# Last day where we have data for bitcoin\nself.enddate = pd.to_datetime(\"2022-03-01\")\nself.test_percent = 0.5\nself.validation_percent = 1 / 3\ndef replace_infty(self):\ndflist = self.dflist\nreplaced = []\nfor df in dflist:\nnew = df.replace(to_replace=[np.inf, -np.inf], value=np.nan)\nreplaced.append(new)\nself.dflist = replaced\ndef prepare_rank_standardize_data(self):\nself.returns, self.variables_df_list, self.variables_df_list_unstandardized, self.close_prices, self.exclusion_mask = \\\nprepare_X_y_rank_standardized(self.dflist, self.startdate, self.filenames, self.variables)\nclass cond_auto_model():\ndef __init__(\nself, model_name, data_path=\"../Aggregate Data v3\", list_covar_hidden_neurons=[32],\nlist_factors_hidden_neurons=[5],\nfactor_dim=5, predictiondays=30, remove_coins=[], full_input=True, parallel_runs=10):\nself.variables = [\"new_addresses\", \"active_addresses\", \"bm\", \"volume\", \"standard deviation volume\",\n\"size\", \"illiq\", \"capm beta\", \"max\",\n\"volume shock 30\", \"volume shock 60\", \"r2_1\", \"r13_2\",\nself.list_covar_hidden_neurons = list_covar_hidden_neurons\nself.factor_dim = factor_dim\nself.within_time_limit = True\nself.predictiondays = predictiondays\nself.callback = [keras.callbacks.EarlyStopping(\nmonitor='val_loss',\nmin_delta=0.0001,\nrestore_best_weights=True,\npatience=300)]\nself.epochs = 1500\nself.tuning_trials = 24\nself.number_parallel_models = parallel_runs\nself.list_of_lists_contemporaneous_predictions = []\nself.weights_list = []\nself.full_input = full_input\nself.name = model_name + f\" {factor_dim} factors\"\ndef initialize_X_Y(self, replace_nan_return_with=np.nan,\nregression_model.replace_infty(self)\nregression_model.prepare_rank_standardize_data(self)\nself.model_directory = \"../Models/\" + self.name\nY = self.returns.iloc[1:].copy()\n# Y = Y.reset_index(drop=True)\nself.observed_mask = 1 - np.isnan(Y)\nY = Y.replace(to_replace=np.nan, value=0)\n# Winsorize to remove extreme returns\nfor i in np.arange(len(Y)):\nY.iloc[i, :] = np.array(winsorize(Y.iloc[i, :], limits=[0, 0.05], nan_policy=\"omit\"))\nX = [df.iloc[:-1] for df in self.variables_df_list]\n# X = [x.reset_index(drop=True) for x in X]\nX = [x.replace(to_replace=np.nan, value=replace_nan_covars_with) for x in X]\n# Add intercept\nfor df in X:\ndf[\"Intercept\"] = 1\nself.X = X\nself.initialize_Z()\n# self.calculate_sorted_portfolio_returns()\nsorted_portfolios = self.sorted_portfolios\n# Split up in training and test set\nnumberdays = Y.shape[0]\ntestdays = int(np.ceil(numberdays * self.test_percent))\ntraindays_end = numberdays - testdays\ntraindays_start = 0\nvalidationdays = int(np.ceil(traindays_end * self.validation_percent))\ntraindays_end = traindays_end - validationdays\n# Add constant regressor to the inputs\nY[\"constant\"] = 1\nsorted_portfolios[\"constant\"] = 1\nY_train, Y_valid, Y_test = Y.iloc[: traindays_end], Y.iloc[\ntraindays_end: + validationdays], Y.iloc[\ntraindays_end + validationdays:]\nportfolios_train, = \\\nsorted_portfolios.iloc[: traindays_end], sorted_portfolios.iloc[\ntraindays_end: + validationdays], sorted_portfolios.iloc[\ntraindays_end + validationdays:]\nX_train, X_valid, X_test = [df.iloc[:traindays_end] for df in X], \\\n[df.iloc[traindays_end: traindays_end + validationdays] for df in X], \\\n# Remove the constant intercept from the outputs. We only needed them temporarily because in the inputs we do actually need the constant\ndel Y[\"constant\"]\nif self.full_input:\nX_train.append(Y_train)\nX_valid.append(portfolios_valid)\nself.Y_test_global = Y_test\nself.X, \\\nself.X_train, \\\nself.X_valid_global, \\\nself.X_test, \\\nself.traindays_start, self.validationdays, self.testdays = \\\nX, Y, X_train, X_valid, \\\nX_valid, X_test, traindays_start, validationdays, testdays\n# If we want to use the covariate-sorted portfolio, we need a matrix Z_t for every time point that contains all the\n# covariates of all coins at time t\ndef initialize_Z(self):\nX = np.array(self.X)\nZ = X.swapaxes(0, 1)\nZ = [pd.DataFrame(Z[i, :, :]) for i in np.arange(len(Z))]\nfor df in Z:\ndf.columns = self.X[0].columns\nZ_t = {day: df for day, df in zip(self.X[0].index, Z)}\nself.Z_t = Z_t\nself.reduce_Z_to_observed_values()\n# Reduces the matrices Z by eliminating the rows of coins that are not even observed on the day indexing the row\ndef reduce_Z_to_observed_values(self):\nY = self.Y\nobserved_coins = dict()\nfor day in Y.index:\nseries_of_returns_that_day = Y.loc[day, :]\nobserved_coins[day] = [coin for coin in\nseries_of_returns_that_day.index[~np.isnan(series_of_returns_that_day)]]\n# keep only days one day before returns\nshifted_days = [day - pd.to_timedelta(1, unit=\"day\") for day in Y.index]\nZ_t_reduced = dict()\nfor day_before in shifted_days:\nZ_t_reduced[day_before] = Z_t[day_before]\nfor day_before, day in zip(shifted_days, Y.index):\nZ_t_reduced[day_before] = Z_t_reduced[day_before].loc[observed_coins[day], :]\nself.Z_t = Z_t_reduced\nself.observed_coins = observed_coins\ndef calculate_sorted_portfolio_returns(self):\nportfolio_returns_per_day = {}\nfor day_before, day in zip(Z_t.keys(), Y.index):\nportfolio_returns = np.matmul(np.transpose(Z_t[day_before].replace(np.nan, 0)),\nnp.array(Y.loc[day, :].replace(np.nan, 0)))\nportfolio_returns_per_day[day] = portfolio_returns\nsorted_portfolios = pd.DataFrame.from_dict(portfolio_returns_per_day, orient=\"index\")\nself.sorted_portfolios = sorted_portfolios\ndef populate_covar_sorted_portfolios(self):\nobserved_coins = self.observed_coins\nportfolios_per_day = {}\nZtransposeZ = np.matmul(np.transpose(np.array(Z_t[day_before])), np.array(Z_t[day_before]))\nZtransposeZ_inverse = np.linalg.inv(ZtransposeZ)\nZ_transpose_Z_inverse_Z = np.matmul(ZtransposeZ_inverse, np.transpose(np.array(Z_t[day_before])))\nportfolio_returns = np.matmul(Z_transpose_Z_inverse_Z, Y.loc[day, observed_coins[day]])\nportfolios_per_day[day] = portfolio_returns\nsorted_portfolios = pd.DataFrame.from_dict(portfolios_per_day, orient=\"index\", columns=self.X[0].columns)\ndef shift_X_Y_train_valid_test_forwards(self):\nself.traindays_end = self.traindays_end + self.predictiondays\nif self.testdays > self.predictiondays:\nself.testdays = self.testdays - self.predictiondays\nself.Y_train, = self.Y.iloc[\nself.traindays_start: self.Y.iloc[\nself.traindays_end: + self.validationdays], self.Y.iloc[\nself.traindays_end + self.validationdays:]\nself.X_train, = [df.iloc[self.traindays_start:self.traindays_end] for df in\nself.X], \\\nself.traindays_end: + self.validationdays] for\ndf in self.X], \\\n[df.iloc[self.traindays_end + self.validationdays:] for df in\nself.X]\nself.within_time_limit = False\nprint(\"End of data frames reached.\")\ndef tune_hp(self):\nY_train, Y_valid, variables, factor_dim, list_covar_hidden_neurons = self.Y_train, self.variables, self.factor_dim, self.list_covar_hidden_neurons\nhypermodel = ca_hypermodel(Y_train, X_train, Y_valid, variables, factor_dim, list_covar_hidden_neurons)\nes = self.callback\nepochs = self.epochs\ntrials = self.tuning_trials\nmodel_directory = self.model_directory\ntuner = BayesianOptimization(\nhypermodel,\nobjective='val_loss',\noverwrite=True,\nmax_trials=trials\n)\ntuner.search(X_train, Y_train, epochs=epochs, validation_data=(X_valid, Y_valid), callbacks=es)\nhyperparameters = tuner.get_best_hyperparameters()[0]\nnumber_parallel_models = self.number_parallel_models\nmodel = tuner.hypermodel.build(hyperparameters)\nmodel.save(model_directory + \"/Hypermodel\")\n'''for i, model in enumerate(model_list):\nrandom.seed(i)\nmodel.fit(\nx = X_train,\ncallbacks = es,\nvalidation_data = (X_valid,\n)'''\n# weights = [model.get_weights() for model in model_list]\nself.best_current_hp = hyperparameters\n# self.weights_list.append(weights)\ndef load_hypermodel(self):\ndirectory = self.model_directory\nnumber_parallel = self.number_parallel_models\nmodel_list = [keras.models.load_model(directory + \"/Hypermodel\") for i in np.arange(number_parallel)]\nself.best_current_model_list = model_list\ndef fit_model(self, stepnumber):\nprint(\"Fitting step number \" + str(stepnumber) + \".\")\nX_train, X_valid, Y_train, Y_valid = self.X_train, self.Y_valid\n# Load hypermodel\nfor i, model in enumerate(self.best_current_model_list):\ntraining_history = model.fit(\nx=X_train,\ncallbacks=es,\nvalidation_data=(X_valid, Y_valid)\n)\nmodel.save(model_directory + \"/\" + str(stepnumber) + \"/\" + str(i))\ntry:\nos.makedirs(model_directory + \"/Training History/\" + str(stepnumber))\nexcept:\nprint(\"Directory \" + model_directory + \"/Training History/\" + str(stepnumber) + \" already exists.\")\nwith open(model_directory + \"/Training History/\" + str(stepnumber) + \"/\" + str(i) + \".pkl\", \"wb\") as file:\npickle.dump(training_history.history, file)\n# self.best_current_model_list = current_model_list\ndef predict_next_testperiod_contemporaneous_returns(self):\npredictiondays = self.predictiondays\nX_test = self.X_test\ncurrent_model_list = self.best_current_model_list\ncurrent_X_test = [df.iloc[:predictiondays] for df in X_test]\ncurrent_Y_pred_list = [model.predict(current_X_test) for model in current_model_list]\ncurrent_Y_pred_list = [pd.DataFrame(current_Y_pred, index=current_Y_test.index, columns=current_Y_test.columns)\nfor current_Y_pred in current_Y_pred_list]\nself.list_of_lists_contemporaneous_predictions.append(current_Y_pred_list)\ndef predict_contemporaneous_returns_from_saved(self, models, step, set_var_to_zero):\nif set_var_to_zero != \"none\":\nfor df in X_test[:-1]:\ndf[set_var_to_zero].values[:] = 0\ncurrent_model_list = models\n# Make sure that we have more enough days left to fill a prediction period. If not, shorten prediction period\n# Important for end of dataframe\nif predictiondays > Y_test.shape[0]:\nfor i, df in zip(np.arange(len(current_Y_pred_list)), current_Y_pred_list):\nif set_var_to_zero != \"none\":\ndirectory = self.model_directory + \"/Contemporaneous Predictions Variable Importance/\" + \\\nset_var_to_zero + \"/Step \" + str(step)\nelse:\ndirectory = self.model_directory + \"/Contemporaneous Predictions/Step \" + str(step)\nif not os.path.exists(directory):\nos.makedirs(directory, exist_ok=True)\ndf.to_csv(directory + \"/Parallel Network \" + str(i) + \".csv\")\n# self.list_of_lists_contemporaneous_predictions.append(current_Y_pred_list)\ndef predict_next_testperiod_future_returns(self):\nnum_coins = self.Y_train.shape[1]\nX_train = self.X_train\nX_train = [pd.concat([train, valid]) for (train, valid) in zip(X_train, X_valid)]\nY_train = self.Y_train\nY_train = pd.concat([Y_train, Y_valid])\nif predictiondays > Y_test.shape[0]: predictiondays = Y_test.shape[0]\ncurrent_X = [pd.concat([train, test]) for (train, test) in zip(X_train, X_test)]\n# Model we work with\nmodel_list = self.best_current_model_list\n# Extract the (average) factor predictions from the model\nlayer_name = 'Factors'\nfactors_layer_model_list = [keras.Model(inputs=model.input,\noutputs=model.get_layer(layer_name).output) for model in model_list]\nfactor_predictions_list = [model.predict(current_X) for model in factors_layer_model_list]\nfactor_sample_averages_list = [np.array([factor_predictions[:i].mean(axis=0) for i in\nnp.arange(start=1, stop=factor_predictions.shape[0] + 1)]) for\nfactor_predictions in factor_predictions_list]\n# Extract the beta predictions from the other part of the network\nlayer_names = [\"betas\" + str(i) for i in np.arange(num_coins)]\nbetas_layer_model_list = [[keras.Model(inputs=model.input,\noutputs=model.get_layer(layer_name).output) for\nlayer_name in layer_names] for model in model_list]\nbetas_predictions_list = [[model.predict(current_X) for model in parallel_model] for parallel_model in\nbetas_layer_model_list]\ncurrent_Y_future_pred_list = [np.array(\n[np.multiply(np.array(betas_predictions)[i][-predictiondays:, ],\nfactor_sample_averages[-(predictiondays + 1):-1, ]).sum(axis=1)\nfor i in np.arange(len(betas_predictions))]).transpose()\nfor (factor_sample_averages, betas_predictions) in\ncurrent_Y_future_pred_list = [\npd.DataFrame(current_Y_future_pred, index=current_Y_test.index, columns=current_Y_test.columns)\nfor current_Y_future_pred in current_Y_future_pred_list]\nself.list_of_lists_future_predictions.append(current_Y_future_pred_list)\ndef predict_future_returns_from_saved(self, models, step, factor_averaging_days=\"all\", set_var_to_zero=\"none\"):\ncurrent_X = [pd.concat([train, test]) for (train, test) in zip(X_train, current_X_test)]\n# If we want to test variable importance\nfor df in current_X[:-1]:\nmodel_list = models\nif factor_averaging_days == \"all\":\nfactor_sample_averages_list = [\nnp.array([factor_predictions[:i].mean(axis=0) for i in\nnp.arange(start=1, stop=factor_predictions.shape[0] + 1)]) for factor_predictions in\nfactor_predictions_list]\nnp.array([factor_predictions[np.max([i - factor_averaging_days, 0]):i].mean(axis=0) for i in\ndirectory = self.model_directory + \"/Betas and Factors Predictions Variable Importance/\" + \\\nset_var_to_zero + \"/Factors/Step \" + str(step)\ndirectory = self.model_directory + \"/Betas and Factors Predictions/Factors/Step \" + str(step)\nif not os.path.exists(directory):\nos.makedirs(directory, exist_ok=True)\nfor i, df in zip(np.arange(len(factor_predictions_list)), factor_predictions_list):\nwith open(directory + \"/\" + str(i) + \".pkl\", \"wb\") as file:\npickle.dump(df, file)\nbetas_layer_model_list = [keras.Model(\ninputs=model.input,\noutputs=[\nmodel.get_layer(layer_name).output for layer_name in layer_names]\nfor model in model_list]\nbetas_predictions_list = [model.predict(current_X) for model in betas_layer_model_list]\nset_var_to_zero + \"/Betas/Step \" + str(step)\ndirectory = self.model_directory + \"/Betas and Factors Predictions/Betas/Step \" + str(step)\nfor betas_predictions in betas_predictions_list:\npickle.dump(betas_predictions, file)\nfor i, df in zip(np.arange(len(current_Y_future_pred_list)), current_Y_future_pred_list):\nif set_var_to_zero == \"none\":\nif factor_averaging_days == \"all\":\ndirectory = self.model_directory + \"/Future Predictions/Step \" + str(step)\nelse:\ndirectory = self.model_directory + \"/Future Predictions \" + \\\nstr(factor_averaging_days) + \" days factor average/Step \" + str(step)\ndirectory = self.model_directory + \"/Future Predictions Variable Importance/\" + \\\nset_var_to_zero + \"/Step \" + str(step)\ndirectory = self.model_directory + \"/Future Predictions Variable Importance \" + \\\nstr(factor_averaging_days) + \" days factor average/\" \\\n+ set_var_to_zero + \"/Step \" + str(step)\n# self.list_of_lists_future_predictions.append(current_Y_future_pred_list)\ndef fit_all_models(self, step=0):\nstepnumber = 0\nwhile stepnumber < step:\nself.shift_X_Y_train_valid_test_forwards()\nstepnumber = stepnumber + 1\nprint(stepnumber)\nwhile self.within_time_limit:\nself.fit_model(stepnumber)\ndef reset_X_Y(self):\nself.X_train = self.X_train_global\n# Reset everything\nnumberdays = self.Y.shape[0]\ntest_percent = self.test_percent\ntestdays = int(np.ceil(numberdays * test_percent))\nvalidation_percent = self.validation_percent\nvalidationdays = int(np.ceil(traindays_end * validation_percent))\nself.traindays_start, self.validationdays, self.testdays = 0, traindays_end, validationdays, testdays\ndef load_parallel_models(self, stepnumber):\ncurrent_model_list = [keras.models.load_model(model_directory + \"/\" + str(stepnumber) + \"/\" + str(i)) for i in\nnp.arange(number_parallel)]\nreturn current_model_list\ndef make_contemporaneous_predictions(self, set_var_to_zero=\"none\"):\nself.reset_X_Y()\nprint(\"Make predictions of step number \" + str(stepnumber))\nloaded_models = self.load_parallel_models(stepnumber)\nself.predict_contemporaneous_returns_from_saved(models=loaded_models, step=stepnumber,\nset_var_to_zero=set_var_to_zero)\ndef make_future_predictions(self, set_var_to_zero=\"none\", factor_averaging_days=\"all\"):\nself.predict_future_returns_from_saved(models=loaded_models, step=stepnumber,\nfactor_averaging_days=factor_averaging_days,\ndef update_contemporaneous_Y_predictions(self, set_var_to_zero=\"none\"):\nif set_var_to_zero == \"none\":\ncontemporaneous_predictions_directory = self.model_directory + \"/Contemporaneous Predictions\"\n\"/Contemporaneous Predictions Variable Importance/\" + \\\nset_var_to_zero\nstepnumbers_model = os.listdir(contemporaneous_predictions_directory)\ntry:\nstepnumbers_model = [int(step[5:]) for step in stepnumbers_model]\nstepnumbers_model.sort()\nexcept:\nraise ValueError(\"There seem to be file names in the model directory that cannot be converted to integers.\")\nlist_of_lists_contemporaneous_predictions = [\n[\npd.read_csv(\ncontemporaneous_predictions_directory + \"/Step \" + str(step) + \"/Parallel Network \" + str(\ni) + \".csv\", index_col=\"day\"\n) for i in np.arange(number_parallel)\n] for step in stepnumbers_model\n]\nself.Y_pred_cont_list = [\npd.concat([parallel_predictions[i] for parallel_predictions in list_of_lists_contemporaneous_predictions],\naxis=0)\nfor i in np.arange(number_parallel)]\ndef update_future_Y_predictions(self, factor_averaging_days=\"all\", set_var_to_zero=\"none\"):\nif factor_averaging_days == \"all\":\nfuture_predictions_directory = self.model_directory + \"/Future Predictions\"\nfactor_averaging_days) + \" days factor average\"\nfuture_predictions_directory = self.model_directory + \"/Future Predictions Variable Importance /\" + \\\nset_var_to_zero\nfuture_predictions_directory = self.model_directory + \"/Future Predictions Variable Importance \" + \\\nstr(factor_averaging_days) + \\\nstepnumbers_model = os.listdir(future_predictions_directory)\nlist_of_lists_future_predictions = [\nfuture_predictions_directory + \"/Step \" + str(step) + \"/Parallel Network \" + str(\nself.Y_pred_future_list = [\npd.concat([parallel_predictions[i] for parallel_predictions in list_of_lists_future_predictions],\ndef calculate_contemporaneous_r2(self, masked=True):\nY_test = self.Y_test_global\nY_pred_cont_list = self.Y_pred_cont_list\nmask = 1 - self.exclusion_mask\nmask = np.array(mask.iloc[-self.testdays:, :])\nif not masked:\nmask = mask ** 0\nY_pred_cont_panel = np.array(Y_pred_cont_list)\nY_pred_cont_average = Y_pred_cont_panel.mean(axis=0)\n# Total R^2\n# For every single parallel model\nsum_squared_residuals_list = [(((np.array(Y_test) - np.array(Y_pred_cont)) ** 2)).sum().sum() for Y_pred_cont in\nY_pred_cont_list]\nmasked_sum_squared_residuals_list = [\n((((np.array(Y_test) - np.array(Y_pred_cont)) * mask) ** 2) * mask).sum().sum() for Y_pred_cont in\nY_pred_cont_list]\nsum_squared_returns = ((Y_test ** 2)).sum().sum()\nRsquared_total_list = [1 - sum_squared_residuals / sum_squared_returns\nfor sum_squared_residuals in sum_squared_residuals_list]\nmasked_Rsquared_total_list = [1 - masked_sum_squared_residuals / masked_sum_squared_returns\nfor masked_sum_squared_residuals in masked_sum_squared_residuals_list]\n# For the average\nsum_squared_residuals_average = (((np.array(Y_test) - np.array(Y_pred_cont_average)) ** 2)).sum().sum()\nmasked_sum_squared_residuals_average = (\n((np.array(Y_test) - np.array(masked_Y_pred_cont_average)) ** 2) * mask).sum().sum()\nRsquared_total_average = 1 - sum_squared_residuals_average / sum_squared_returns\nself.masked_Y_test = Y_test * mask\nself.masked_Y_pred_cont_average = masked_Y_pred_cont_average\nself.sum_squared_residuals_list = sum_squared_residuals_list\nself.masked_sum_squared_returns = masked_sum_squared_returns\nself.sum_squared_residuals_average = sum_squared_residuals_average\nself.Y_pred_cont_average = Y_pred_cont_average\nself.Rsquared_total_list = Rsquared_total_list\nself.masked_Rsquared_total_average = masked_Rsquared_total_average\ndef calculate_predictive_r2(self):\nY_pred_future_list = self.Y_pred_future_list\n# Masked: Only predictions where Y_test nonzero are counted. Y_test = 0 is assumed to only be the case when coins are eliminated\nmasked_Y_pred_future_list = [Y_pred_future * mask for Y_pred_future in Y_pred_future_list]\nY_pred_future_panel = np.array(Y_pred_future_list)\nY_pred_future_average = Y_pred_future_panel.mean(axis=0)\n# Predictive R^2\nsum_squared_residuals_predictive_list = [((np.array(Y_test) - np.array(Y_pred_future)) ** 2).sum().sum()\nfor Y_pred_future in Y_pred_future_list]\nsum_squared_returns = (Y_test ** 2).sum().sum()\nRsquared_predictive_list = [1 - sum_squared_residuals_predictive / sum_squared_returns\nfor sum_squared_residuals_predictive in sum_squared_residuals_predictive_list]\nmasked_sum_squared_residuals_predictive_list = [\n(((np.array(Y_test) - np.array(Y_pred_future)) * mask) ** 2).sum().sum()\nfor Y_pred_future in Y_pred_future_list]\nmasked_Y_test = Y_test * mask\nmasked_sum_squared_returns = ((np.array(Y_test) * mask) ** 2).sum().sum()\nmasked_Rsquared_predictive_list = [1 - sum_squared_residuals_predictive / masked_sum_squared_returns\nfor sum_squared_residuals_predictive in\nsum_squared_residuals_average = ((np.array(Y_test) - np.array(Y_pred_future_average)) ** 2).sum().sum()\nRsquared_predictive_average = 1 - sum_squared_residuals_average / sum_squared_returns\nself.masked_Y_test = masked_Y_test\nself.masked_Y_pred_future_list = masked_Y_pred_future_list\nself.Y_pred_future_average = Y_pred_future_average\nself.Rsquared_predictive_list = Rsquared_predictive_list\nself.masked_Rsquared_predictive_average = masked_Rsquared_predictive_average\nclass ca_hypermodel(kt.HyperModel):\ndef __init__(self, Y_train, Y_valid, variables, dim_factors, list_hidden_neurons):\nsuper().__init__(self)\nself.Y_train, self.variables, self.dim_factors, self.list_hidden_neurons = \\\ndef build(self, hp):\n# Build model\nconditional_autoencoder_model = link_both_parts(num_coins=self.Y_train.shape[1],\ninput_dimension=self.X_train[-1].shape[1],\n[0.005, 0.0001]))\n# Optimizer we use\nopt = keras.optimizers.Adam(\nlearning_rate=hp.Choice(\"learning rate\", [0.0001, 0.001])\n# Compile model\nconditional_autoencoder_model.compile(\nloss=\"mean_squared_error\",\noptimizer=opt\nreturn conditional_autoencoder_model\ndef fit(self, hp, model, *args, **kwargs):\n# Fit the Model\nreturn model.fit(\n*args,\nbatch_size=hp.Choice(\"batch size\", [32, 64]),\n**kwargs\n", "output_sequence": "LVU thesis: This notebook calculates the 27 observed asset characteristics from the raw data."}, {"input_sequence": "### Estimating a system of equations: CAPM ###\n## install packages ##\nlibraries = c(\"gmm\", \"quantmod\", \"tidyverse\", \"PerformanceAnalytics\",\"RColorBrewer\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) )\n\nlibrary(\"quantmod\")\nlibrary(\"PerformanceAnalytics\")\nlibrary(\"tidyverse\")\nlibrary(\"gmm\")\n## import data ##\n# wd_current <- getwd()\n# factor_FF3_all <- read.csv(paste(wd_current, \"/F-F_Research_Data_Factors_daily.csv\",sep = \"\"),skip = 4)\nfactor_FF3 <- factor_FF3_all[factor_FF3_all$X>=\"20200101\"&factor_FF3_all$X<=\"20201231\",]\nr_factor_mkt <- factor_FF3$Mkt.RF\n## component of DJIA ##\nlist_DJIA <- c(\"V\",\"CAT\",\"CVX\",\"VZ\",\"TRV\",\"DIS\",\"MRK\",\"MMM\",\"BA\",\"JNJ\",\n\"MCD\",\"AMGN\",\"KO\",\"NKE\",\"DOW\",\"IBM\",\"AXP\",\"HD\",\"WBA\",\"UNH\",\n\"INTC\",\"WMT\",\"HON\",\"AAPL\",\"PG\",\"CRM\",\"MSFT\",\"CSCO\",\"JPM\",\"GS\")\nsetSymbolLookup(s_AAPL=list(name=\"AAPL\",src=\"yahoo\"))\ngetSymbols(\"s_AAPL\", from=\"2020-01-01\",\nr_AAPL <- Return.calculate(S_AAPL[, 6], method = \"log\")\nr_AAPL <- na.fill(r_AAPL,0)\nr_stock_all<-matrix(nrow = length(r_AAPL),ncol = length(list_DJIA))\ncolnames(r_stock_all)<-list_DJIA\nfor (i in 1:30) {\nsetSymbolLookup(s_TEMP=list(name=list_DJIA[i],src=\"yahoo\"))\ngetSymbols(\"s_TEMP\", from=\"2020-01-01\",\nr_TEMP = Return.calculate(S_TEMP[, 6], method = \"log\")\nr_TEMP <- na.fill(r_TEMP,0)\nr_stock_all[,i]<-r_TEMP\n}\n## gmm estimation and test ##\n## unrestricted model ##\nres_1 <- gmm((r_stock_all - r_factor_mkt[-length(r_factor_mkt)])~r_factor_mkt[-length(r_factor_mkt)],\nx = r_factor_mkt[-length(r_factor_mkt)])\ncoef(res_1)\nspecTest(res_1)\n## restricted model ##\nres_2 <- gmm((r_stock_all - r_factor_mkt[-length(r_factor_mkt)])~r_factor_mkt[-length(r_factor_mkt)] - 1,\nx = cbind(1,r_factor_mkt[-length(r_factor_mkt)]))\ncoef(res_2)\nspecTest(res_2)\n## robustness ##\nlist_remove <- c(\"UNH\",\"V\",\"AAPL\",\"JPM\",\"MSFT\")\nr_stock_robust <- r_stock_all[,!list_DJIA %in% list_remove]\nres_robust <- gmm((r_stock_robust - r_factor_mkt[-length(r_factor_mkt)]) ~ r_factor_mkt[-length(r_factor_mkt)] - 1,\nx = cbind(1,r_factor_mkt[-length(r_factor_mkt)]))\nspecTest(res_robust)\nsave.image(file = \"Metis_GMM.RData\")\n## plot time seies of returns ##\ndate_xts <- as.Date(factor_FF3$X,\"%Y%m%d\")\nr_factor_mkt_xts <- xts(r_factor_mkt[-length(r_factor_mkt)], order.by = date_xts[-length(date_xts)])\nn <- 30\nmyCol = c(\"pink1\", \"violet\", \"mediumpurple1\", \"slateblue1\", \"purple\",\n\"turquoise3\", \"skyblue\", \"steelblue\", \"blue2\", \"navyblue\",\n\"orangered\", \"tomato\", \"coral2\", \"palevioletred\", \"red2\",\n\"springgreen2\", \"darkgreen\",\n\"darkolivegreen\", \"tan4\", \"brown\",\n\"grey40\",\n# alternative color scheme\n# myCol1 <- colorRampPalette(c(\"steelblue\", \"#FFFFFF\", \"brown\"))(30)\n# qual_col_pals = brewer.pal.info[brewer.pal.info$category == 'qual',]\n# col_vector = unlist(mapply(brewer.pal, qual_col_pals$maxcolors, rownames(qual_col_pals)))\n# col_plot <- col_vector[seq(2,(2*n+2),by=2)]\n# plot.xts(r_stock_all_xts[\"2020-02\"], col = myCol)\n# addLegend(lty = rep(1,30),col = myCol, ncol = 5)\n# manually plot the time series in different months\npng(paste(\"Metis_tsplot\",as.character(1),\".png\", sep = \"\"), width = 900, height = 600, bg = \"transparent\")\npar(mfrow=c(2,1))\nplot.xts(r_stock_all_xts[\"2020-1\"], col = myCol, main = \"Daily returns of components of Dow Jones Industrial Average\", bg = \"transparent\")\nplot.xts(r_factor_mkt_xts[\"2020-1\"], main = \"Daily risk premium\", bg = \"transparent\")\ndev.off()\n", "output_sequence": "Test Capital Asset Pricing Theory using Generaliyed Method of Moments."}, {"input_sequence": "'''Analysis of one group of data\n\nThis script shows how to\n- Use a t-test for a single mean\n- Use a non-parametric test (Wilcoxon signed rank sum) to check a single mean\n- Compare the values from the t-distribution with those of a normal distribution\n'''\n# Copyright(c) 2015, Thomas Haslwanter. All rights reserved, under the CC BY-SA 4.0 International License\n# Import standard packages\nimport numpy as np\nimport scipy.stats as stats\ndef check_mean():\n'''Data from Altman, check for significance of mean value.\nCompare average daily energy intake (kJ) over 10 days of 11 healthy women, and compare it to the recommended level of 7725 kJ.\n'''\n# Get data from Altman\ninFile = 'altman_91.txt'\ndata = np.genfromtxt(inFile, delimiter=',')\n# Watch out: by default the standard deviation in numpy is calculated with ddof=0, corresponding to 1/N!\nmyMean = np.mean(data)\nmySD = np.std(data, ddof=1) # sample standard deviation\nprint(('Mean and SD: {0:4.2f} and {1:4.2f}'.format(myMean, mySD)))\n# Confidence intervals\ntf = stats.t(len(data)-1)\n# multiplication with np.array[-1,1] is a neat trick to implement \"+/-\"\nci = np.mean(data) + stats.sem(data)*np.array([-1,1])*tf.ppf(0.975)\nprint(('The confidence intervals are {0:4.2f} to {1:4.2f}.'.format(ci[0], ci[1])))\n# Check if there is a significant difference relative to \"checkValue\"\ncheckValue = 7725\n# --- >>> START stats <<< ---\nt, prob = stats.ttest_1samp(data, checkValue)\nif prob < 0.05:\nprint(('{0:4.2f} is significantly different from the mean (p={1:5.3f}).'.format(checkValue, prob)))\n# For not normally distributed data, use the Wilcoxon signed rank sum test\n(rank, pVal) = stats.wilcoxon(data-checkValue)\nif pVal < 0.05:\nissignificant = 'unlikely'\nelse:\nissignificant = 'likely'\n# --- >>> STOP stats <<< ---\n\nprint(('It is ' + issignificant + ' that the value is {0:d}'.format(checkValue)))\nreturn prob # should be 0.018137235176105802\n\ndef compareWithNormal():\n'''This function is supposed to give you an idea how big/small the difference between t- and normal\ndistribution are for realistic calculations.\n# generate the data\nnp.random.seed(12345)\nnormDist = stats.norm(loc=7, scale=3)\ndata = normDist.rvs(100)\ncheckVal = 6.5\n# T-test\nt, tProb = stats.ttest_1samp(data, checkVal)\n# Comparison with corresponding normal distribution\nmmean = np.mean(data)\nmstd = np.std(data, ddof=1)\nnormProb = stats.norm.cdf(checkVal, loc=mmean,\nscale=mstd/np.sqrt(len(data)))*2\n# compare\nprint(('The probability from the t-test is ' + '{0:5.4f}, and from the normal distribution {1:5.4f}'.format(tProb, normProb)))\nreturn normProb # should be 0.054201154690070759\n\nif __name__ == '__main__':\ncheck_mean()\ncompareWithNormal()\n", "output_sequence": "Analysis of one group of data\nThis script shows how to - Use a t-test for a single mean - Use a non-parametric test (Wilcoxon signed rank) to check a single mean - Compare the values from the t-distribution with those of a normal distribution"}, {"input_sequence": "import matplotlib\nmatplotlib.use('TkAgg')\nimport matplotlib.pyplot as plt\nimport numpy as np\n# cf: https://github.com/antoine-eon/olga/tree/master/Algorithme/Algo%20Reseau%20Neuronal%20Regression%20Lineaire%20EX\n# Load the data and create the data matrices X and Y\n# This creates a feature vector X with a column of ones (bias)\n# and a column of car weights.\n# The target vector Y is a column of MPG values for each car.\nX_file = np.genfromtxt('mpg.csv', delimiter=',', skip_header=1)\nN = np.shape(X_file)[0]\nX = np.hstack((np.ones(N).reshape(N, 1), X_file[:, 4].reshape(N, 1)))\nY = X_file[:, 0]\n# Standardization\nX[:, 1] = (X[:, 1]-np.mean(X[:, 1]))/np.std(X[:, 1])\n# Two weights (bias and feature)\nw = np.array([0, 0])\n# Batch gradient descent\n# size eta\nmax_iter = 100\neta = 1e-4\nfor t in range(0, max_iter):\nprint(t)\n# We iterate over each data point for one epoch\ngrad_t = np.array([0., 0.])\nfor i in range(0, N):\nx_i = X[i, :]\nh = np.dot(w, x_i)-y_i\ngrad_t += 2*x_i*h\n# Update the weights\nw = w - eta*grad_t\n\n# Plot the data and best fit line\ntt = np.linspace(np.min(X[:, 1]), np.max(X[:, 1]), 10)\nbf_line = w[0]+w[1]*tt\nplt.plot(X[:, 1], Y, 'kx', tt, bf_line)#, label = t)\n# Plot the data and best fit line\nplt.plot(X[:, 1], Y, 'kx', tt, bf_line, 'r-')\nplt.savefig('figure1.png')\nplt.show()\nprint(\"Weights found:\",w)\n", "output_sequence": "Gradient descent implementation from scratch in order to train an artificial neural network to fit a regression of city-cycle fuel consumption in miles per gallon."}, {"input_sequence": "source(XFGPowerLawEst1)\n# sample period\nstartDate = ymd(20081231)\nDate = startDate %m+% months(c(1:(length(alpha))))\n# store the Power Law estimation results in 'PL'\nPL = cbind(as.Date(Date), alpha, Xmin, KS, P)\n# load fundamental variables\nTVs = as.matrix(read.csv(\"Bitcoin_Data1.csv\", header = T))\nTVs = TVs[match(\"3/1/2009\", TVs[, 1]):match(\"31/12/2013\", TVs[, 1]), ]\nstartDate = ymd(20090103)\ndmy = seq.Date(as.Date(startDate), as.Date(endDate), by = \"1 day\")\nYear = year(dmy)\nym = as.numeric(paste(Mon, Year, sep = \".\"))\nTVs[, 1] = ym\n# convert daily data to monthly data\nDuni = unique(TVs[, 1])\nfor (i in 1:length(Duni)) {\nm = apply(matrix(as.numeric(as.matrix(TVs[(TVs[, 1] == Duni[i]), ])[, -c(1:2)]), ncol = (ncol(TVs) - 2)), 2, sum)\nm = matrix(m, nrow = 1)\n}\n# store Power Law parameters with Transaction Variables in PL.C\nPL.C = cbind(PL, M.TVs)\n# remove the date column\nPL.s = PL.s[, -1]\ncolnames(PL.s)[5:length(colnames(PL.s))] = colnames(TVs)[-c(1:2)]\nPL.s = as.matrix(PL.s)\n# run linear regression days.destroyed, 8\nDays.destroyed.alpha.fit = lm(diff(as.numeric(PL.s[, 8])) ~ D.Alpha)\nDays.destroyed.alpha.table = summary(Days.destroyed.alpha.fit)\n# MB.1\nMB..alpha.fit = lm(diff(as.numeric(PL.s[, 9])) ~ D.Alpha)\nMB..alpha.table = summary(MB..alpha.fit)\n# difficulty, 11\nDifficulty.alpha.fit = lm(diff(as.numeric(PL.s[, 12])) ~ D.Alpha)\nDifficulty.alpha.table = summary(Difficulty.alpha.fit)\n# Hashrate 14\nHashrate.alpha.fit = lm(diff(as.numeric(PL.s[, 15])) ~ D.Alpha)\nHashrate.alpha.table = summary(Hashrate.alpha.fit)\n# Market.cap.usd 15\nMarket.cap.alpha.fit = lm(diff(as.numeric(PL.s[, 16])) ~ D.Alpha)\nMarket.cap.alpha.table = summary(Market.cap.alpha.fit)\n# price.usd 16\nMarket.price.alpha.fit = lm(diff(as.numeric(PL.s[, 17])) ~ D.Alpha)\nMarket.price.alpha.table = summary(Market.price.alpha.fit)\n# miners.Revenue 17\nMiners.Revenue.alpha.fit = lm(diff(as.numeric(PL.s[, 18])) ~ D.Alpha)\nMiners.Revenue.alpha.table = summary(Miners.Revenue.alpha.fit)\n# network.deficit.usd 18\nNetwork.Deficit.alpha.fit = lm(diff(as.numeric(PL.s[, 19])) ~ D.Alpha)\nNetwork.Deficit.alpha.table = summary(Network.Deficit.alpha.fit)\n# No. of Transactions 26\nNo.Transaction.alpha.fit = lm(diff(as.numeric(PL.s[, 26])) ~ D.Alpha)\nNo.Transaction.alpha.table = summary(No.Transaction.alpha.fit)\n# Ratio 27\nRatio.alpha.fit = lm(as.numeric(PL.s[-1, 29]) ~ D.Alpha)\nRatio.alpha.table = summary(Ratio.alpha.fit)\n# Draw Table 1\ntexreg(list(Days.destroyed.alpha.fit,\nMB..alpha.fit,\nMiners.Revenue.alpha.fit,\nstars = c(0.01, 0.05, 0.1),\ncaption.above = TRUE,\ncaption = \"Estimation Results for Bitcoin and Auroracoin\",\ndcolumn = FALSE,\ncustom.model.names = c(\"Days Destroyed\",\n\"MB.1\",\noverride.se = list(Days.destroyed.alpha.table$coef[, 2],\nsource(XFGPowerLawEst2)\nstartDate = ymd(20140228)\nDate = startDate %m+% months(c(1:(length(alpha.2))))\n# using difference data\nD.Alpha = diff(alpha.2)\n# read transaction data\nData = read.csv(\"Auroracoin.csv\",header=T)\n# Regression days.destroyed,\ndays.destroyed.alpha.fita = lm(diff(as.numeric(Data[, 8])) ~ D.Alpha)\ndays.destroyed.alpha.tablea = summary(days.destroyed.alpha.fita)\n# cost.transaction,\ncost.transaction.alpha.fita = lm(diff(as.numeric(Data[, 6])) ~ D.Alpha)\ncost.transaction.alpha.tablea = summary(cost.transaction.alpha.fita)\n# difficulty,\ndifficulty.alpha.fita = lm(diff(as.numeric(Data[, 3])) ~ D.Alpha)\ndifficulty.alpha.tablea = summary(difficulty.alpha.fita)\n# Transaction.volume,usd 12\nTransactionVolumeu.alpha.fita = lm(diff(as.numeric(Data[, 5])) ~ D.Alpha)\nTransactionVolumeu.alpha.tablea = summary(TransactionVolumeu.alpha.fita)\n# Price\nPrice.alpha.fita = lm(diff(as.numeric(Data[, 2])) ~ D.Alpha)\nPrice.alpha.tablea = summary(Price.alpha.fita)\n# No.of.Transactions\nNo.of.Transactions.alpha.fita = lm(diff(as.numeric(Data[, 4])) ~ D.Alpha)\nNo.of.Transactions.alpha.tablea = summary(No.of.Transactions.alpha.fita)\n# Transaction.fees\nTransaction.fees.alpha.fita = lm(diff(as.numeric(Data[, 7])) ~ D.Alpha)\nTransaction.fees.alpha.tablea = summary(Transaction.fees.alpha.fita)\n# Draw Table 2\ntexreg(list(days.destroyed.alpha.fita,\ncost.transaction.alpha.fita,\n\"Cost.transaction\",\noverride.se = list(days.destroyed.alpha.tablea$coef[, 2],\nsource(XFGPowerLawEst3)\n### store the results in 'PL'\n# combine the transaction data with the power law estimation results\n# remove the data column\n# store PL.s in PL.S\nPL.S = PL.s\n# Using periods when powerlaw is well fitted\nPL.s = PL.S[(P > 0.1), ]\n# run linear regression for table 3 days.destroyed, 8\n# MB.1, 9\n# Draw Table 3\nstars = c(0.01, 0.05, 0.1),\ncaption.above = TRUE,\ncaption = \"Estimation Results for Bitcoin and Auroracoin\",\ndcolumn = FALSE,\ncustom.model.names = c(\"Days Destroyed\",\n\"MB.1\",\noverride.se = list(Days.destroyed.alpha.table$coef[, 2],\n# Part 4: Table 4 Using periods when powerlaw is well fitted\nPL.s = PL.S[(P > 0.1), ] # choosing power law periods\nP.value = as.numeric(PL.s[-1, 4])\n# days.destroyed, 8\nDays.destroyed.P.value.fit = lm(diff(as.numeric(PL.s[, 8])) ~ P.value)\nDays.destroyed.P.value.table = summary(Days.destroyed.P.value.fit)\nMB..P.value.fit = lm(diff(as.numeric(PL.s[, 9])) ~ P.value)\nMB..P.value.table = summary(MB..P.value.fit)\nDifficulty.P.value.fit = lm(diff(as.numeric(PL.s[, 12])) ~ P.value)\nDifficulty.P.value.table = summary(Difficulty.P.value.fit)\nHashrate.P.value.fit = lm(diff(as.numeric(PL.s[, 15])) ~ P.value)\nHashrate.P.value.table = summary(Hashrate.P.value.fit)\nMarket.cap.P.value.fit = lm(diff(as.numeric(PL.s[, 16])) ~ P.value)\nMarket.cap.P.value.table = summary(Market.cap.P.value.fit)\nMarket.price.P.value.fit = lm(diff(as.numeric(PL.s[, 17])) ~ P.value)\nMarket.price.P.value.table = summary(Market.price.P.value.fit)\nMiners.Revenue.P.value.fit = lm(diff(as.numeric(PL.s[, 18])) ~ P.value)\nMiners.Revenue.P.value.table = summary(Miners.Revenue.P.value.fit)\nNetwork.Deficit.P.value.fit = lm(diff(as.numeric(PL.s[, 19])) ~ P.value)\nNetwork.Deficit.P.value.table = summary(Network.Deficit.P.value.fit)\nNo.Transaction.P.value.fit = lm(diff(as.numeric(PL.s[, 26])) ~ P.value)\nNo.Transaction.P.value.table = summary(No.Transaction.P.value.fit)\nRatio.P.value.fit = lm(as.numeric(PL.s[-1, 29]) ~ P.value)\nRatio.P.value.table = summary(Ratio.P.value.fit)\n# Draw Table 4\ntexreg(list(Days.destroyed.P.value.fit,\nMB..P.value.fit,\nMiners.Revenue.P.value.fit,\noverride.se = list(Days.destroyed.P.value.table$coef[, 2],\n# Part 5: Table 5 & 6 Using whole periods\nPL.s = PL.S\nPs = as.numeric((as.numeric(PL.s[-1, 4]) > 0.1))\n# run linear regression for table 5 & 6 days.destroyed, 8\nDays.destroyed.Ps.fit = lm(diff(as.numeric(PL.s[, 8])) ~ Ps)\nDays.destroyed.Ps.table = summary(Days.destroyed.Ps.fit)\nMB..Ps.fit = lm(diff(as.numeric(PL.s[, 9])) ~ Ps)\nMB..Ps.table = summary(MB..Ps.fit)\nDifficulty.Ps.fit = lm(diff(as.numeric(PL.s[, 12])) ~ Ps)\nDifficulty.Ps.table = summary(Difficulty.Ps.fit)\nHashrate.Ps.fit = lm(diff(as.numeric(PL.s[, 15])) ~ Ps)\nHashrate.Ps.table = summary(Hashrate.Ps.fit)\nMarket.cap.Ps.fit = lm(diff(as.numeric(PL.s[, 16])) ~ Ps)\nMarket.cap.Ps.table = summary(Market.cap.Ps.fit)\nMarket.price.Ps.fit = lm(diff(as.numeric(PL.s[, 17])) ~ Ps)\nMarket.price.Ps.table = summary(Market.price.Ps.fit)\nMiners.Revenue.Ps.fit = lm(diff(as.numeric(PL.s[, 18])) ~ Ps)\nMiners.Revenue.Ps.table = summary(Miners.Revenue.Ps.fit)\nNetwork.Deficit.Ps.fit = lm(diff(as.numeric(PL.s[, 19])) ~ Ps)\nNetwork.Deficit.Ps.table = summary(Network.Deficit.Ps.fit)\nNo.Transaction.Ps.fit = lm(diff(as.numeric(PL.s[, 26])) ~ Ps)\nNo.Transaction.Ps.table = summary(No.Transaction.Ps.fit)\nRatio.Ps.fit = lm(as.numeric(PL.s[-1, 29]) ~ Ps)\nRatio.Ps.table = summary(Ratio.Ps.fit)\n# Draw Table 5\ntexreg(list(Days.destroyed.Ps.fit,\nMB..Ps.fit,\nDifficulty.Ps.fit,\nMiners.Revenue.Ps.fit,\noverride.se = list(Days.destroyed.Ps.table$coef[, 2],\n# Draw Table 6\n", "output_sequence": "Generates the Latex code for tables of regression results in Risk Analysis of Cryptos as Alternative Asset Class."}, {"input_sequence": "options(stringsAsFactors = FALSE)\nsetwd(\"\")\ndata.df = read.csv(\"term_evolution.csv\")\ndata.df$date = sapply(strsplit(data.df$date, \" \"), \"[\", 1)\ndata.df$date = as.Date(data.df$date, \"%Y-%m-%d\")\ndev.new(width = 8, height = 6)\npar(mar = c(2, 4.1, 2, 2.1), mfrow = c(2, 1))\n# Gox23\nplot(data.df$gox23,\nmain = \"Topic 23\",\nlwd = 2,\ncol = \"black\",\nbty = 'l',\nxaxt = \"n\",\nxlim = c(min(data.df$date),\nylim = c(0, max(data.df$gox23)))\nlines(data.df$date, data.df$gox23, lwd = 1.5)\nbox(lwd = 1.5, bty = 'l', col = \"gray22\")\nmtext(\"p(w = mtgox | k = 23)\",\nside = 2,\ncol = \"gray22\",\nlas = 3)\naxis(side = 2,\nround(data.df$gox23, 2),\ntick = FALSE,\ncex.lab = 1,\nline = -0.8,\ncol.axis = \"gray22\",\nlas = 1)\naxis(side = 1,\ndata.df$date, format(data.df$date, \" %m-%Y \"),\ncex.lab = 0.1,\n# Gox38\nplot(data.df$gox38,\nmain = \"Topic 38\",\nylim = c(0, max(data.df$gox38)))\nlines(data.df$date, data.df$gox38, lwd = 1.5)\nmtext(\"p(w = mtgox | k = 38)\",\nround(data.df$gox38, 1),\n", "output_sequence": "Plots word evolution for the former crypto-currency exchange Mt Gox."}, {"input_sequence": "############################# computation of VaR using Joe copula\nrm(list = ls(all = TRUE))\ngraphics.off()\n# please set working directory setwd('C:/...') #\n# linux/mac os setwd('/Users/...') # windows\nlibraries = c(\"fGarch\", \"foreach\", \"doParallel\", \"copula\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n############################# set up\neps1 = read.csv(\"eps1.csv\")\nsp.126 = read.csv(\"sp126.csv\", header = T)[, -c(1, 2)]\np1 = sp.126[-1, ]\np2 = sp.126[-length(sp.126[, 1]), ]\nl.r = 100 * log(p1/p2)\nr = l.r\nr1 = r[, -c(1)]\ndims = 5\nattach(r2)\nr2 = data.frame(cbind(AVB, EQR, TXN, ADI, LLY))\nM = 100\nVaR = matrix(NA, backtestNr, 4)\nslidingWindowLength = 300\neps = matrix(NA, backtestNr, dim(r2)[2])\ncolnames(eps) = colnames(r2)\neps1 = data.frame()\ndat = eps1\nparaMat = list()\npara = matrix(NA, dims, 4)\nsigma = matrix(NA, backtestNr, dims)\n############################# estimation GARCH parameter\ncl = makeCluster(37)\nregisterDoParallel(cl)\ngetDoParWorkers()\ndat = r2\ndatMat = list()\nfor (i in 1:backtestNr) {\ndatMat[[i]] = dat[c(i:(i + (slidingWindowLength - 1))), 1:dims]\n}\nlengthPara = 4\nlengthEps1 = slidingWindowLength\nobjFun = function(dMat) {\nparaComb = list()\nlibrary(fGarch)\nfor (i in 1:dims) {\nfit = garchFit(~garch(1, 1), data = dMat[[i]], trace = F)\neps1.loop = fit@residuals/fit@sigma.t\npara.loop = fit@fit$coef\nlengthPara = length(para.loop)\nparaComb[[i]] = c(para.loop, sigma.loop, eps1.loop)\n}\nreturn(paraComb)\nresultD = foreach(dMat = datMat) %dopar% objFun(dMat)\nstopCluster(cl)\n############################# estimation of copula parameter\ncl = makeCluster(37)\nregisterDoParallel(cl)\nlengthPara = 4\nlengthEps1 = slidingWindowLength\ntotalLength = lengthPara + lengthEps1 + lengthSigma\nepsComb = list()\nk = pobs(as.data.frame(resultD[[i]]))\nepsComb[[i]] = k[-(1:(lengthPara + lengthEps1)), ]\ndatMatEps = epsComb\nlibrary(copula)\njoe.cop = joeCopula(2, dim = dims)\nfit.mln = fitCopula(joe.cop, dMat, method = \"mpl\")\npara.normal = summary(fit.mln)$coefficient[1]\nreturn(para.normal)\nresultDcopPara = foreach(dMat = datMatEps, .combine = \"rbind\") %dopar%\nobjFun(dMat)\nresultDcopPara\n############################# VaR computation\nspread.real.0 = sp.126\nattach(spread.real.0)\nspread.real = data.frame(cbind(AVB, EQR, TXN, ADI, LLY))\nS = rowSums(spread.real)\nS1 = S[-1]\nS2 = S[-length(S)]\nS3 = S2 - S1\nL.real = S3[301:(300 + backtestNr)]\ncl = makeCluster(37)\nregisterDoParallel(cl)\nlengthPara = 4\nlengthEps1 = slidingWindowLength\ntotalLength = lengthPara + lengthEps1 + lengthSigma\npara.vec = resultDcopPara\nVaR = matrix(NA, backtestNr, 4)\ndatMatIndex = c(1:backtestNr)\nobjFunVaR = function(dMat) {\ni = dMat\npara.normal = para.vec[i]\njoe.cop.2 = joeCopula(para.normal, dim = dims)\nu = rCopula(M, joe.cop.2)\nu = qnorm(u)\nk = as.data.frame(resultD[[i]])\nk1 = k[c(1:lengthPara, lengthPara + lengthEps1, totalLength), ]\nh = sqrt(k1[2, ] + k1[3, ] * k1[lengthPara + 1, ]^2 * k1[lengthPara +\nk2 = matrix(NA, M, dims)\nk3[1, ] = as.matrix(k1[lengthPara + 1, ])\nsig.t = k3[rep(1, M), ]\nR = mu.t + sig.t * u\nspread.real.loop = read.csv(\"sp126.csv\")[, -c(1:2)]\nattach(spread.real.loop)\nspread = data.frame(cbind(AVB, EQR, TXN, ADI, LLY))\ns1 = spread[i + (slidingWindowLength), ]\nst = data.frame(matrix(NA, M, dims))\nst[1, ] = s1\nst = st[rep(1, M), ]\nL.sim = rowSums(st * (exp(0.01 * R) - 1))\nVaRt1005 = quantile(L.sim, 0.05)\nVaRt = c(VaRt1005, VaRt10001)\nreturn(VaRt)\nresultVaR = foreach(dMat = datMatIndex, .combine = \"rbind\") %dopar% objFunVaR(dMat)\nhead(resultVaR)\nVaR = resultVaR\nresultComb = data.frame(L.real, VaR)\n############################# exceeding ratio\nExceeding_Ratio = numeric(4)\nfor (alpha in 2:5) {\nnullVector = rep(0, backtestNr)\nfor (i in 1:length(resultComb[, alpha])) {\nif (resultComb[, 1][i] < resultComb[, alpha][i]) {\nnullVector[i] = 1\n} else {\nnullVector[i] = 0\n}\nExceeding_Ratio[alpha] = sum(nullVector)/backtestNr\nExceeding_Ratio\n############################# plot of VaR and quantile = 0.001\nalpha = 5\nptForPlot = c(min(resultComb[, 1]), min(resultComb[, 2]), min(resultComb[,\n3]), min(resultComb[, 4]), min(resultComb[, 5]))\nlowPt = min(ptForPlot)\nupPt = max(resultComb[, 1])\nPortfolio_Value = seq(lowPt, upPt, length.out = length(resultComb[, 1]))\n1])))\nplot(Time_Index,\nPortfolio_Value,\ncol = \"white\",\npch = 19,\nxlab = \"Time Index\",\nylab = \"Profit and Loss of Portfolio\")\nlines(resultComb[, alpha], col = \"gray\", lwd = 6)\nfor (i in 1:length(resultComb[, alpha])) {\nif (resultComb[, 1][i] < resultComb[, alpha][i]) {\npoints(i, lowPt + 1, col = \"black\", pch = 17, cex = 1.5)\npoints(i, resultComb[, 1][i],\ncol = \"black\",\n} else {\npch = 19,\n", "output_sequence": "Shows VaRs of Joe copula model introduced in the paper. The alphas are chosen eqaul to .001."}, {"input_sequence": "import re\nimport pandas as pd\nimport numpy as np\nimport pickle\nimport os\n#https://stackoverflow.com/questions/2319019/using-regex-to-remove-comments-from-source-files\n#https://github.com/HektorLin/HU-IRTG/blob/master/Partition%20Codes%20and%20Comments.ipynb\ndef remove_comments(string):\npattern = r\"(\\\".*?\\\"|\\'.*?\\')|(/\\*.*?\\*/|//[^\\r\\n]*$)\"\n# first group captures quoted strings (double or single)\n# second group captures comments (//single-line or /* multi-line */)\nregex = re.compile(pattern, re.MULTILINE|re.DOTALL)\ndef _replacer(match):\n# if the 2nd group (capturing comments) is not None,\n# it means we have captured a non-quoted (real) comment string.\nif match.group(2) is not None:\nreturn \"\" # so we will return empty to remove the comment\nelse: # otherwise, we will return the 1st group\nreturn match.group(1) # captured quoted-string\nreturn regex.sub(_replacer, string)\n#following the example above, distinguish group(1): \"\" or'' and group(2,true comments): \\*.*\\ or //\n#return when a true comment is located\ndef leave_only_comments(string):\ncomments = [x[1] for x in regex.findall(string)]\ncomments = ' '.join(comments)\nfiles = os.listdir('sol_source')\nfor file in tqdm(files):\ncontract_hash = file.split('_')[0]\nname = file.split('_')[1].split('.')[0]\nwith open(f'sol_source/{file}', 'r', encoding=\"utf8\") as readf:\ncontract = readf.read()\ndf_temp = pd.DataFrame(data={'hash':contract_hash,\n'name_from_SC': name,\n'SC_no_comments': remove_comments(contract),\ndf = pd.concat([df, df_temp])\ndf_additional_info = pd.read_csv('../CSC_Dapp_scraping/dapps.csv')\n", "output_sequence": "obtaining Dapp smart contracts using API, parsing source code"}, {"input_sequence": "import requests\nimport pandas as pd\nimport numpy as np\nimport sys\n%matplotlib inline\ncontract = pd.read_csv(\"../CSC_Dapp_scraping/dapps.csv\")\n# contants\nwith open(\"token.txt\") as file:\nif not os.path.exists(sol_source):\nfiles_in_sol = os.listdir(f\"./sol_source/\")\nfiles_in_sol = [i.split('_')[0] for i in files_in_sol]\ndef scrape_ether_contract_and_write(address_array, API_Token):\nc = 0\nfor i, address in tqdm(enumerate(address_array)):\nif address in files_in_sol:\ncontinue\nelse:\n# time.sleep(0.01) # we can do 5 GET/POST requests per sec\nurl = f'https://api.etherscan.io/api?module=contract&action=getsourcecode&address={address}&apikey={API_Token}'\nresp = requests.get(url=url)\ndata = resp.json()\ntry:\ncontract_name = data['result'][0]['ContractName']\n# save solidity source code\nwith open(f\"./sol_source/{address}_{contract_name}.sol\", \"w\", encoding=\"utf-8\") as f:\nprint(data['result'][0]['SourceCode'], file=f)\nexcept:\nc += 1\ntime.sleep(0.25)\n\n", "output_sequence": "obtaining Dapp smart contracts using API, parsing source code"}, {"input_sequence": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndf['date'] = pd.to_datetime(df.timestamp)\nfig, ax1 = plt.subplots(figsize=(20,10))\ncolor = '#DB3B28'\nax1.set_xlabel('date')\n#ax1.set_ylabel('New contracts', color=color)\nax1.plot(agg['count'], color=color)\nax1.tick_params(axis='y', labelcolor=color)\nax2 = ax1.twinx() # instantiate a second axes that shares the same x-axis\ncolor = 'tab:blue'\n#ax2.set_ylabel('Total contracts', color=color) # we already handled the x-label with ax1\nax2.plot(cums, color=color)\nax2.tick_params(axis='y', labelcolor=color)\nfig.tight_layout() # otherwise the right y-label is slightly clipped\n#plt.show()\n", "output_sequence": "Plotting contracts over time"}, {"input_sequence": "import os\nimport numpy as np\nimport pandas as pd\nimport re\nimport json\n#from keras.preprocessing.text import Tokenizer\ndef check_json(row):\nif row.startswith(\"{\"):\ntry:\ntmp_row = json.loads(row)\nexcept:\ntmp_row = row\n\nnew_content = []\nfor i in tmp_row:\nnew_content.append(tmp_row[i]['content'])\nnew_content = ' '.join(new_content)\nreturn new_content\nelse:\ndata.source_code.fillna('empty', inplace=True)\n# functions\ndef contract_name_extract(data):\n#extract name\ncontract_name = re.findall('[\\n\\r].*Contract|contract\\s*([^\\n\\r]*)\\s|\\n{', data)\nif len(contract_name)>1:\ncontract_name_string = ' '.join(contract_name)\ncontract_name_string = re.sub(r'[^\\w\\s]','', contract_name_string)\ncontract_name_string = contract_name\nreturn(contract_name_string)\ndef function_name_extract(data):\n#extract function names and join to one string\nfunction_name = re.findall('[\\n\\r].*function\\s*([^\\n\\r\\(]*)', data)\nfunction_name_string = ' '.join(function_name)\nfunction_name_string = re.sub(r'[^\\w\\s]','', function_name_string)\nreturn(function_name_string)\ndef comments_extract(data):\n#extract comments and join to one text\none_line_comments = re.findall(re.compile(\"/\\*.*?\\*/\", re.DOTALL), data) # find all occurance streamed comments (/*COMMENT */) from string\ncomments1 = ' '.join(one_line_comments)\ncomments = re.sub(r'[^\\w\\s]',' ', comments)\ncomments = comments.split()\ncomments = ' '.join(comments)\nreturn(comments)\ndef code_no_punct(data):\ncodes = re.sub(r'[^\\w\\s]',' ', data)\ncodes = codes.split()\ncodes = ' '.join(codes)\ndata['cn_string'] = data.source_code.apply(contract_name_extract)\ndf[['ContractName', 'cn_string']]\ntokenizer = Tokenizer(num_words=40000)\ntokenizer.fit_on_texts(df.source_code)\n", "output_sequence": "Parsing of smart contracts, data frame creation"}, {"input_sequence": "import os\nimport numpy as np\nimport pandas as pd\nfor i, f in enumerate(FILES):\nif i % 1000 == 0 :\nprint(i)\n\ntry:\nwith open(f'../data/sol_source/{f}') as file:\ndata = file.read()\ndf_temp = pd.DataFrame(data = {'address' : f.split('_')[0], 'source_code': data}, index=[0])\ndf = pd.concat([df, df_temp])\nexcept:\npass\n# print(f, ' is empty')\nFILES_IN_SOL = [f.split('_')[0] for f in FILES]\nNOT_LOADED = list(set(FILES_IN_SOL) - set(df.address))\nNOT_LOADED_F = []\nfor file in NOT_LOADED:\nmatching = [s for s in FILES if file in s]\nfor i, f in enumerate(NOT_LOADED_F):\n# try:\nwith open(f'../data/sol_source/{f[0]}', encoding='utf8') as file:\ndata = file.read()\ndf_temp = pd.DataFrame(data = {'address' : f[0].split('_')[0], 'source_code': data}, index=[0])\ndf = pd.concat([df, df_temp])\n# except:\n# pass\n", "output_sequence": "Parsing of smart contracts, data frame creation"}, {"input_sequence": "import requests\nimport pandas as pd\nimport numpy as np\nimport sys\n%matplotlib inline\nimport matplotlib.pyplot as plt\ndef check_json(row):\nif row.startswith(\"{\"):\ntry:\ntmp_row = json.loads(row)\nexcept:\ntmp_row = row\n\nnew_content = []\nfor i in tmp_row:\nnew_content.append(tmp_row[i]['content'])\nnew_content = ' '.join(new_content)\nreturn new_content\nelse:\n# functions\ndef contract_name_extract(data):\n#extract name\ncontract_name = re.findall('[\\n\\r].*Contract|contract\\s*([^\\n\\r]*)\\s{', data)\nif len(contract_name)>1:\ncontract_name_string = ' '.join(contract_name)\ncontract_name_string = re.sub(r'[^\\w\\s]','', contract_name_string)\ncontract_name = contract_name[0]\ncontract_name_string = 'only_1_contract'\nreturn(contract_name, contract_name_string)\ndef function_name_extract(data):\n#extract function names and join to one string\nfunction_name = re.findall('[\\n\\r].*function\\s*([^\\n\\r\\(]*)', data)\nfunction_name_string = ' '.join(function_name)\nfunction_name_string = re.sub(r'[^\\w\\s]','', function_name_string)\nreturn(function_name_string)\ndef comments_extract(data):\n#extract comments and join to one text\none_line_comments = re.findall(re.compile(\"/\\*.*?\\*/\", re.DOTALL), data) # find all occurance streamed comments (/*COMMENT */) from string\ncomments1 = ' '.join(one_line_comments)\ncomments = re.sub(r'[^\\w\\s]',' ', comments)\ncomments = comments.split()\ncomments = ' '.join(comments)\nreturn(comments)\ndef code_no_punct(data):\ncodes = re.sub(r'[^\\w\\s]',' ', data)\ncodes = codes.split()\ncodes = ' '.join(codes)\nfor i, f in enumerate(files):\nif i%1000==0:\nprint(i)\n#try:\nwith open(f'../data/sol_source/{f}') as file:\ndata = file.read()\ncontract_name, contract_name_string = contract_name_extract(data)\ncomments = comments_extract(data)\ncodes_no_p = code_no_punct(data)\ndf_temp = pd.DataFrame(data = {'address' : f.split('_')[0], 'contract_name' : contract_name, 'source_code': data,\n'contract_name_string' : contract_name_string, 'function_names' : function_name_string,\n'comments' : comments, 'code_all_no_punct' : codes_no_p}, index=[0])\ndf = pd.concat([df, df_temp])\n#except:\ncontinue\nprint('Parsing finished')\n", "output_sequence": "Parsing of smart contracts, data frame creation"}, {"input_sequence": "# clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\nbullspreadcall = function(St, K1, T, sigma, r) {\nif (K1 > K2)\nprint(\"K2 must be larger than K1\") else {\nK = c(K1, K2)\n\n# Calculate the terms for the BS option prices\nd1 = (log(St/K) + (r + sigma^2/2) * T)/(sigma * sqrt(T))\nd2 = d1 - sigma * sqrt(T)\n# Set the coordinates\nx = c(0, K1, + K2)\ncal = numeric(2)\n# Calculate to plain vanilla call option prices\nfor (i in 1:2) {\ncal[i] = St * pnorm(d1[i]) - K[i] * exp(-r * T) * pnorm(d2[i])\n}\n# Value of plain vanilla options at time T\ncal_T = cal * exp(r * T)\n# Calculate the payoff at each coordinate\ny1 = c(-cal_T[1], K[2] - K[1] - cal_T[1], K[2] - cal_T[1])\n# Determine the strategy payoff\ny = y1 + y2\n# Plot strategy\nplot(x, y, type = \"l\", lwd = 3, col = \"red\", xlab = \"S_T\", ylab = \"Payoff\",\nxlim = c(0.7 * x[2], 1.4 * x[2]), ylim = c(-1.2 * y[3], 1.2 * y[3]))\ntitle(\"Bull Call Spread\")\n# Plot plain vanilla option payoff profiles\nlines(x, y1, lty = 2)\nlines(x, c(0, 0, 0), lty = 3)\ntext(95, 8.2, \"Short Call\")\n}\n}\nbullspreadcall(92, 100, 1, 0.4, 0.03)\n", "output_sequence": "Plots the combination of a long call and a short call where the exercise price of the short call is higher than the exercise price of the long call, i.e. bull call spread."}, {"input_sequence": "# clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\nx = read.table(\"Coca_cola.txt\") # Stock prices\nx = ts(x) # transform to time series object\n# Returns\nr1 = (x[2:length(x)] - x[1:length(x) - 1])/x[1:length(x) - 1] # Returns\nr2 = log(x[2:length(x)]) - log(x[1:length(x) - 1]) # Log returns\nsplit.screen(c(2, 1))\n# Time series r1\nscreen(1)\nplot(r1, type = \"l\", col = \"blue\", xlab = \"Time (days)\", ylab = \"Returns\")\n# Time series r2\nscreen(2)\nplot(r2, type = \"l\", col = \"blue\", xlab = \"Time (days)\", ylab = \"Log Returns\")\n", "output_sequence": "Plots the time series of daily returns and log returns for Coca-Cola company from 1 January 2002 to 30 November 2004."}, {"input_sequence": "# clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# parameter settings\nS0 = 100 # Closing price of underlying at time t_0\nstrike = seq(85, 115, 1) # range of strike prices\ntau = 0.25 # time to maturity\n# define risk reversal\ni1 = 1\n# define functions\nf1 = function(x) 0.000167 * x^2 - 0.03645 * x + 2.08 # initial\n# Calculation of call option price by BS model\ncall_blsprice = function(S0, strike, r, tau, sigma) {\nd1 = (log(S0/strike) + (r + sigma^2/2) * tau)/(sigma * sqrt(tau))\nd2 = d1 - sigma * sqrt(tau)\ncall_blsprice = S0 * pnorm(d1) - strike * exp(-r * tau) * pnorm(d2)\n}\n# Calculation of put option price by BS model\nput_blsprice = function(S0, strike, r, tau, sigma) {\nput_blsprice = strike * exp(-r * tau) * pnorm(-d2) - S0 * pnorm(-d1)\n# calculate\nivsmileK = f1(strike)\ncall1 = call_blsprice(S0, strike[i1], r, tau, ivsmileK[i1])\nRR1 = round(put1 - call2, 4)\nivsmileK = f2(strike)\ncallS1 = call_blsprice(S0, strike[i1], r, tau, ivsmileK[i1])\nRR2 = round(putS1 - callS2, 4)\nivsmileK = f3(strike)\nRR3 = round(putS1 - callS2, 4)\n# final results\nprint(\"Prices of risk reversal\")\nprint(\" f1 f2 f3\")\nprint(c(RR1, RR2, RR3))\n# difference f1 vs. f2\nplot(strike, f1(strike), type = \"l\", lwd = 2.5, col = \"blue\", ylim = c(0.08, 0.2),\nxlab = \"Strike\", ylab = \"sigma_imp\")\nlines(strike, f2(strike), lwd = 2.5, col = \"blue\", lty = 2)\ntitle(\"f_1 vs. f_2\")\n# difference f1 vs. f3\ndev.new()\nlines(strike, f3(strike), lwd = 2.5, col = \"blue\", lty = 2)\n", "output_sequence": "Compares the prices of the risk reversal functions for the following implied volatility curves, given as a function of strike price: (i) f1(K) = 0.000167 x K^2 - 0.03645 x K + 2.080 (ii) f2(K) = 0.000167 x K^2 - 0.03645 x K + 2.090 (iii)f3(K) = 0.000167 x K^2 - 0.03517 x K + 1.952. The plots compare the functions f2 and f3 to the function f1."}, {"input_sequence": "# Close all plots and clear variables\ngraphics.off()\nrm(list = ls(all = TRUE))\nh = 0.01\nlam = seq(0, 1, h)\nm = 100\nk = seq(0, m, 1)\nL1 = matrix(0, 1, m + 1)\nL2 = L1\n# Gamma pdfs with alpha = (2,4,6) and beta=5.\nfp1 = dgamma(lam, 2, 0.05^(-1))\n# Loss distribution with the Poisson model.\nfor (i in 1:(m + 1)) {\nL1[i] = sum(dpois(k[i], m * lam) * fp1 * h)\n}\nplot(k, L1, type = \"l\", col = \"blue\", lwd = 2, xlab = \"\", ylab = \"\")\nlines(k, L2, col = \"blue\", lwd = 2)\n# Gamma pdfs with alpha = (3,2,10) and beta=(3.33,5,1).\nfp4 = dgamma(lam, 3, 0.0333^(-1))\nL4[i] = sum(dpois(k[i], m * lam) * fp4 * h)\ndev.new()\nplot(k, L4, type = \"l\", xlim = c(0, 30), ylim = c(0, 0.1), col = \"blue\", lwd = 2,\nxlab = \"\", ylab = \"\")\nlines(k, L5, col = \"blue\", lwd = 2)\n", "output_sequence": "Plots the loss distribution in the simplified Poisson model with default probabilities comming from Gamma distribution."}, {"input_sequence": "# clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# parameter settings\nK = 6000\nS1 = 60 # stock price for Allianz\nsigma1 = 0.4249 # volatility for Allianz\nrho = 0.3\ntau = 1\nsigmasq = sigma1^2 + 2 * rho * sigma1 * sigma2 + sigma2^2\nsigma = sqrt(sigmasq)\nd1 = (log(S1 * S2/K) + (2 * r - (sigma1^2 + sigma2^2)/2) * tau)/(sigma * sqrt(tau))\nd2 = d1 + sigma * sqrt(tau)\ncdf1 = pnorm(d1)\n# price of european product call\n(ce = exp((r + sigma1 * sigma2 * rho) * tau) * S1 * S2 * cdf2 - (exp(-r * tau) *\nK * cdf1))\n", "output_sequence": "Calculates the price of a European product call option of Allianz and Munich RE stock prices."}, {"input_sequence": "% clear variables and close windows\nclear all\nclc\nstandalone = 0; % set to 0 to make plots as seen in STF2\n% market volatilities for respective deltas\ndelta = .1:.1:.9;\nmarketvols = [0.1135 0.109 0.10425 0.103 0.10575 0.1105 0.1165];\nspot = 1.215; % spot on July 1, 2004\nrd = 0.02165; % domestic riskless interest rate\ntau = 0.5; % time to expiry\ncp = 1; % call option\n%Calculate strikes for respective deltas\nstrikes = GarmanKohlhagen(spot,delta,marketvols,rd,rf,tau,cp,2);\n%Sample input:\nv0 = 0.01; % initial volatility\nkappa = 1.5; % speed of mean revision of the volatility process\ntheta = 0.015; % long-term mean of volatility process\nvv = 0.2; % volatility\nrho = 0.05; % correlation between the spot price and volatility processes\nlambda = 0;\n% HestonVanillaSmile returns volatility smile\nsmile1 = HestonVanillaSmile(cp,spot,strikes,v0,vv,rd,rf,tau,kappa,theta,lambda,rho);\nif standalone,\nfigure(1);\nelse\nfigure(1);\nsubplot(1,2,1);\nend\nplot(delta*100,smile1*100,'x-',...\ndelta*100,smile3*100,'+:','LineWidth',1);\nif standalone, title('Correlation and the smile'); end\nxlabel ('Delta [%]');\nylabel ('Implied volatility [%]');\nlegend('rho = 0.05', 'rho = -0.15', 'rho = 0.15','Location','North')\nset(gca,'XTick', 10:20:90, 'Ylim', [9 13]);\nfigure(2);\nsubplot(1,2,2);\nplot(delta*100,smile4*100,'x-',...\ndelta*100,smile6*100,'+:','LineWidth',1);\nlegend('rho = 0', 'rho = -0.5', 'rho = 0.5','Location','North')\n", "output_sequence": "Plots the effect of changing the correlation on the shape of the smile. Requires GarmanKohlhagen.m, HestonVanillaSmile.m and HestonVanilla.m function."}, {"input_sequence": "function smile=HestonVanillaSmile(cp,spot,strikes,v0,vv,rd,rf,tau,kappa,theta,lambda,rho)\n%HESTONVANILLASMILE The volatility smile implied by the Heston model.\n% SMILE=HESTONVANILLASMILE(CP,SPOT,STRIKES,V0,VV,RD,RF,TAU,KAPPA,THETA,LAMBDA,RHO)\n% returns a vector of volatilities given a vector of strikes STRIKES,\n% spot price SPOT, initial volatility V0, vol of vol VV, domestic and\n% foreign interest rates RD and RF, time to maturity (in years) TAU, mean\n% reversion KAPPA, long-run mean THETA, market price of risk LAMBDA,\n% correlation RHO and option type CP:\n% CP=1 -> call option, CP=-1 -> put option.\n%\n% Sample use:\n% >> HestonVanillaSmile(1,0.8,0.2:0.2:1,0,0.2,0.05,0.05,1,2,0.04,0,0.5)\n%\n% Reference(s):\n% [1] A.Janek, T.Kluge, R.Weron, U.Wystup (2010) FX smile in the\n% Heston model, see http://ideas.repec.org/p/pra/mprapa/25491.html\n% {Chapter prepared for the 2nd edition of \"Statistical Tools for\n% Springer-Verlag, forthcoming in 2011.}\n% Written by Rafal Weron (2004.07.30)\n% Revised by Agnieszka Janek (2010.07.07, 2010.10.20)\n% Revised by Rafal Weron (2010.12.27)\nnostrikes = length(strikes);\n% Create an array of option values\nP = zeros(1,nostrikes);\n% Create an array of implied volatilities\nIV = zeros(1,nostrikes);\nfor m=1:nostrikes\nlambda = 0;\n% Calculate option prices with respective strikes in the Heston\n% stochastic volatility model with lambda = 0;\nP(m) = HestonVanilla(cp,spot,strikes(m),v0,vv,rd,rf,tau,kappa,theta,lambda,rho);\n% Calculate implied volatilities for options with respective strikes\nIV(m) = ImplVolFX(P(m),spot,strikes(m),rd,rf,tau,cp);\nend\nsmile = IV;\n\n%%%%%%%%%%%%% INTERNALLY USED ROUTINE %%%%%%%%%%%%%\nfunction vol=ImplVolFX(X,S,K,rd,rf,tau,cp)\n%IMPLVOLFX Implied volatility assuming the Garman-Kohlhagen model for a\n%European style FX option.\n% VOL=IMPLVOLFX(X,S,K,RD,RF,TAU,CP) determines implied volatility\n% of a European FX option given its price X, spot price SPOT, strike K,\n% domestic interest rate RD, foreign interest rate RF, time to maturity\n% (in years) TAU and option type CP:\n% CP = 1 --> call option, CP = -1 --> put option\n% Find zero of a function GK(vol) - X\nvol = fzero(@(vol) (GarmanKohlhagen(S, K, vol , rd, tau, cp, 0)-X),0.001,optimset('TolX',1e-8));\n", "output_sequence": "Plots the effect of changing the correlation on the shape of the smile. Requires GarmanKohlhagen.m, HestonVanillaSmile.m and HestonVanilla.m function."}, {"input_sequence": "function opv = GarmanKohlhagen(S,K,vol,rd,rf,tau,cp,task)\n%GARMANKOHLHAGEN European FX option pricing formula.\n% OPV=GARMANKOHLHAGEN(S,K,VOL,RD,RF,TAU,CP,TASK) returns option price,\n% (spot) delta or strike depending on the value of the TASK parameter:\n% TASK = 0 --> option price (default),\n% TASK = 2 --> option strike (given its delta as parameter K),\n% in the Garman and Kohlhagen (1983) option pricing model.\n% The remaining input parameters are: FX spot S, strike/spot delta K,\n% volatility VOL, domestic and foreign riskless interest rates RD and RF\n% (annualized), time to expiry (in years) TAU and option type CP:\n% CP = 1 --> call option (default), CP = -1 --> put option.\n%\n% Sample use:\n% >> C = GarmanKohlhagen(1,1,0.1,0.05,0.03,0.25,1,0)\n%\n% Reference(s):\n% [1] M.B.Garman, S.W.Kohlhagen (1983) Foreign currency option values,\n% Journal of International Money and Finance 2, 231-237.\n% [2] A.Janek, T.Kluge, R.Weron, U.Wystup (2010) FX smile in the\n% Heston model, see http://ideas.repec.org/p/pra/mprapa/25491.html\n% {Chapter prepared for the 2nd edition of \"Statistical Tools for\n% Springer-Verlag, forthcoming in 2011.}\n% Written by Rafal Weron (2004.06.20)\n% Revised by Agnieszka Janek (2010.07.07)\n% Revised by Rafal Weron (2010.11.16, 2010.12.27)\nif (nargin < 6)\nerror ('Wrong number of input arguments.')\nelse\n% Set default values:\nif (nargin < 8)\ntask = 0; %calculate option price\nif (nargin == 6)\ncp = 1; % call option\nend\nend\nd1 = ( log(S./K)+(rd-rf+0.5.*vol.^2).*tau )./ ( vol.*sqrt(tau));\nd2 = d1 - vol.*sqrt(tau);\nswitch task\ncase 1 % calculate spot delta\nopv = cp.*exp(-rf.*tau).*normcdf(cp.*d1,0,1);\ncase 2 % calculate strike given spot delta\nopv = S.*exp(-cp.*norminv(K.*exp(rf*tau),0,1).*vol.*sqrt(tau)+(rd-rf+0.5.*vol.^2).*tau);\notherwise % calculate option price\nopv = cp.*( S.*exp(-rf.*tau).*normcdf(cp.*d1,0,1)-K.*exp(-rd.*tau).*normcdf(cp.*d2,0,1));\n", "output_sequence": "Plots the effect of changing the correlation on the shape of the smile. Requires GarmanKohlhagen.m, HestonVanillaSmile.m and HestonVanilla.m function."}, {"input_sequence": "function P= HestonVanilla(cp,s,k,v0,vv,rd,rf,tau,kappa,theta,lambda,rho)\n%HESTONVANILLA European FX option price in the Heston model.\n% P=HESTONVANILLA(CP,S,K,V0,VV,RD,RF,TAU,KAPPA,THETA,LAMBDA,RHO) returns\n% the price of a European call (CP=1) or put (CP=-1) option given\n% spot price S, strike K, initial volatility V0, vol of vol VV,\n% domestic interest rate RD, foreign interest rate RF, time to maturity\n% (in years) TAU, level of mean reversion KAPPA, long-run variance THETA,\n% market price of volatility risk LAMBDA and correlation RHO.\n\n% Sample use:\n% >> HestonVanilla(1,1.03,1,.01,.02,.05,.03,.25,10,.01,0,.5)\n%\n% Reference(s):\n% [1] H.Albrecher, P.Mayer, W.Schoutens, J.Tistaert (2006) The little\n% [2] A.Janek, T.Kluge, R.Weron, U.Wystup (2010) FX smile in the\n% Heston model, see http://ideas.repec.org/p/pra/mprapa/25491.html\n% {Chapter prepared for the 2nd edition of \"Statistical Tools for\n% Springer-Verlag, forthcoming in 2011.}\n% Written by Agnieszka Janek and Rafal Weron (2010.07.07)\n% Revised by Rafal Weron (2010.10.08, 2010.12.27)\n% Integrate using adaptive Gauss-Kronod quadrature\nP1 = 0.5+1/pi.*quadgk(@(phi) HestonVanillaInt(phi,1,s,k,v0,vv,rd,rf,tau,kappa,theta,lambda,rho),0,inf,'RelTol',1e-8);\n% Calculate Pplus and Pminus\nPplus = (1-cp)/2 + cp.*P1;\n% Calculate option price\nP = cp.*( s.*exp(-rf.*tau).*Pplus - k.*exp(-rd.*tau).*Pminus);\n%%%%%%%%%%%%% INTERNALLY USED ROUTINE %%%%%%%%%%%%%\n\nfunction F=HestonVanillaInt(phi,m,s,k,v0,vv,rd,rf,tau,kappa,theta,lambda,rho)\n%HESTONVANILLAINT Auxiliary function used by HESTONVANILLA.\n% F=HESTONVANILLAINT(phi,m,s,k,v0,vv,rd,rf,tau,kappa,theta,lambda,rho)\n% returns the values of the auxiliary function evaluated at points PHI,\n% given spot price S, strike K, initial volatility V0, vol of vol VV,\n% market price of volatility risk LAMBDA and correlation RHO.\na = kappa.*theta;\nu = [0.5 -0.5];\nb = [kappa + lambda - rho.*vv kappa + lambda];\nx = log(s);\nd = sqrt((1i*rho.*vv.*phi - b(m)).^2 - vv.^2*(2*u(m).*phi.*1i - phi.^2));\ng = (b(m) - rho.*vv.*phi.*1i - d)./(b(m) - rho.*vv.*phi.*1i + d);\nD = (b(m) - rho.*vv.*phi.*1i - d)./(vv.^2).*((1 - exp(-d.*tau))./(1 - g.*exp(-d.*tau)));\nC = (rd - rf).*phi.*1i.*tau + a./(vv.^2).*((b(m) - rho.*vv.*phi.*1i - d).*tau - 2*log((1 - g.*exp(-d.*tau))./(1-g)));\nf = exp(C + D.*v0 + 1i.*phi.*x);\n", "output_sequence": "Plots the effect of changing the correlation on the shape of the smile. Requires GarmanKohlhagen.m, HestonVanillaSmile.m and HestonVanilla.m function."}, {"input_sequence": "function retval=theil(x,n)\n% Converts the given time series into time series of Theil index\n% n - size of moving window\n% x - analysed time series\n%\n% Author: Janusz Mi\u015bkiewicz, email: jamis@ift.uni.wroc.pl\n[n_pocz,k]=size(x);\nrozm=n_pocz-n+1;\nretval=zeros(rozm,k);\nfor i=1:rozm\nsr=mean(x(i:i+n-1,:));\ntemp=x(i:i+n-1,:)./(ones(n,1)*sr);\ntemp=temp.*log(temp);\nretval(i,:)=mean(temp);\nend;\n", "output_sequence": "Presents the statistical properties of the minimum spanning tree as a function of time windows sizes for elements of the chosen WIG 20 companies (close.csv). The Theil index based distance is used. Requires theil.m, mst.m, manh.m to run the program. Note that the {statistical toolbox} from Matlab is required as well."}, {"input_sequence": "rm(list=ls(all=TRUE))\ngraphics.off()\ninstall.packages(\"moments\")\nlibrary(moments)\n##########################################\n########## Subroutine manh(x) ############\nmanh = function(x){\n# Manhattan distence between time series normalised by the time series\n# length.\n# x - time series\n#\nx = as.matrix(x)\nh = nrow(x)\nresult = matrix(0,k,k)\nfor (j in 1:k){\nresult[i,j] = abs(mean(x[,i]-x[,j]))\n}\n}\nretval= result\nreturn(retval)\n###########################################\n############ Subroutine mst(x) ############\nmst = function(x){\n# Algorithm generates minimum spanning tree\n# The rsult is presentes as a set of links between nodes\nn = nrow(x)\ntrue = upper.tri(x)\nx = true*x\nnet = matrix(0,n-1,3)\nonnet = rep(as.integer(0),n)\nklast = 0L\n# check if the matrics is symmetric and positive\nmaxx = max(apply(x,2,max))\nsmax = 10*abs(maxx)\nx[x==0] = smax\nwhile (licz<n-1){\nminx = min(apply(x,2,min))\nd = which(x<=minx,arr.ind=T)\ni = d[,1]\nif (length(i) > 1){\nii = i[1]\nj = 0\n\nif (onnet[i] ==0 & onnet[j] ==0){\nlicz = licz+1L\nnet[licz,1] = i\nklast = klast+1L\nnet[licz,3] = min(x[i,j],x[j,i])\nonnet[i] = 1\nx[i,j] = smax\n}else if (onnet[i]==0 & onnet[j]==1) {\nlicz = licz+1\nklaster[i] = klaster[j]\n}else if (onnet[i] ==1 & onnet[j] ==0) {\nklaster[j] = klaster[i]\n}else if (onnet[i] ==1 & onnet[j] ==1 & klaster[i]==klaster[j]) {\nlicz = licz+1L\nklaster[klaster==klaster[i]]=klaster[j]\nretval = net\nreturn(retval)\n#############################################\n############ Subroutine theil(x) ############\ntheil = function(x,n){\n# Converts the given time series into time series of Theil index\n# n - size of moving window\n# x - analysed time series\nx = as.matrix(x)\nn_pocz = nrow(x)\nk = ncol(x)\nrozm = n_pocz-n+1\nretval = matrix(0,rozm,k)\nfor (i in 1:rozm){\nsr = apply(x[i:(i+n-1),],2,mean)\ntemp = x[i:(i+n-1),]/(matrix(1,n,1)%*%sr)\ntemp = temp*log(temp)\nretval[i,] = apply(temp,2,mean)\n}\n############ Main calculation #############\ndata = read.table(\"gwp.csv\",header=T)\ndata = as.matrix(data)\ndata = abs(diff(log(data))) # abs log return\ndl_szer = nrow(data)\npodmioty= ncol(data)\ndata[data==0] = 0.0000001\nprint(\"The procedure may last several hours since 9025 networks are constructed.\")\n# define matrices\nwynik_wind_mean = matrix(0,96,96)\n# time window loop\nfor (window1 in 5:100){\ntheil_data = theil(data,window1)\nfor (window2 in 5:100){\n\n# moving time window\nwynik = numeric()\nfor (t in 1:(dl_szer - window1-1-window2)){\nwindow_data = theil_data[t:(t+window2),]\nwind_dist = manh(window_data)\nwynik = c(wynik,wind_mst[,3])\nwind_mst = numeric()\n}\nwynik_wind_mean[window1-4,window2-4] = mean(wynik)\nrequire(lattice)\nwireframe(wynik_wind_mean,drape=T,#ticktype=\"detailed\",\nmain=\"mean distance, MST\",\nscales=list(arrows=FALSE,col=\"black\",distance=1,tick.number=8,cex=.7),\nxlab=list(\"T_1\",rot=30,cex=1.2),\ndev.new()\nwireframe(wynik_wind_std,drape=T,#ticktype=\"detailed\",\nmain=\"Std, MST\",\nzlab=list(\"std\",cex=1.1))\nwireframe(wynik_wind_skew,drape=T,#ticktype=\"detailed\",\nmain=\"Skewness, MST\",\nzlab=list(\"skewness\",rot=95,cex=1.1))\nwireframe(wynik_wind_kurt,drape=T,#ticktype=\"detailed\",\nmain=\"Kurtosis, MST\",\nzlab=list(\"kurtosis\",rot=95,cex=1.1))\n", "output_sequence": "Presents the statistical properties of the minimum spanning tree as a function of time windows sizes for elements of the chosen WIG 20 companies (close.csv). The Theil index based distance is used. Requires theil.m, mst.m, manh.m to run the program. Note that the {statistical toolbox} from Matlab is required as well."}, {"input_sequence": "function retval = mst(x)\n% Algorithm generates minimum spanning tree\n% The rsult is presentes as a set of links between nodes\n%\n% Author: Janusz Mi\u015bkiewicz, email: jamis@ift.uni.wroc.pl\n[n,m]=size(x);\nx=triu(x,1);\nnet=zeros(n-1,3);\nklaster=zeros(n,1);\nklast=0;\nlicz=0;\n%check if the matrics is symmetric and positive\nmaxx=max(max(x));\nsmax=10*abs(maxx);\nx(x==0)=smax;\nwhile (licz<n-1)\nminx=min(min(x));\n[i,j]=find(x<=minx);\nif (length(i) > 0)\nii=i(1);\nj=[];\nend;\nif (onnet(i) ==0 && onnet(j) ==0)\nlicz=licz+1;\nnet(licz,1)=i;\nklast=klast+1;\nnet(licz,3)=min(x(i,j),x(j,i));\nonnet(i)=1;\nx(i,j)=smax;\nelseif (onnet(i)==0 && onnet(j)==1)\nklaster(i)=klaster(j);\nelseif (onnet(i) ==1 && onnet(j) ==0)\nklaster(j)=klaster(i);\nelseif (onnet(i) ==1 && onnet(j) ==1 && klaster(i)==klaster(j))\nklaster(klaster==klaster(i))=klaster(j);\nend;\nretval=net;\n", "output_sequence": "Presents the statistical properties of the minimum spanning tree as a function of time windows sizes for elements of the chosen WIG 20 companies (close.csv). The Theil index based distance is used. Requires theil.m, mst.m, manh.m to run the program. Note that the {statistical toolbox} from Matlab is required as well."}, {"input_sequence": "function retval=manh(x)\n% Manhattan distence between time series normalised by the time series\n% length.\n% x - time series\n%\n% Author: Janusz Mi\u015bkiewicz, email: jamis@ift.uni.wroc.pl\n[h,k]=size(x);\nresult=zeros(k);\nfor j=1:k\nresult(i,j)=abs(mean(x(:,i)-x(:,j)));\nend;\nretval= result;\n", "output_sequence": "Presents the statistical properties of the minimum spanning tree as a function of time windows sizes for elements of the chosen WIG 20 companies (close.csv). The Theil index based distance is used. Requires theil.m, mst.m, manh.m to run the program. Note that the {statistical toolbox} from Matlab is required as well."}, {"input_sequence": "clear all\nclose all\nclc\ndata = load('gwp.csv');\ndata = abs(diff(log(data))); % abs log return\n[dl_szer,podmioty] = size(data);\ndata(data==0) = 0.0000001;\n% time window loop\ndisp('The procedure may last several hours since 9025 networks are constructed.')\nfor window1=5:100\ntheil_data = theil(data,window1);\nfor window2=5:100\n% moving time window\nwynik = [];\nfor t=1:(dl_szer - window1-1-window2)\nwindow_data = theil_data(t:(t+window2),:);\nwind_dist = manh(window_data);\nwynik = [wynik;wind_mst(:,3)];\nwind_mst = [];\nend;\nwynik_wind_mean(window1-4,window2-4) = mean(wynik);\nend;\nend;\nsubplot(2,2,1), mesh(wynik_wind_mean,'DisplayName','mean distance, MST');figure(gcf)\nxlabel('T_1');\ntitle('mean distance, MST');\nsubplot(2,2,2),mesh(wynik_wind_std,'DisplayName','Std, MST');figure(gcf)\nzlabel('std');\ntitle('Std, MST');\nsubplot(2,2,3),mesh(wynik_wind_skew,'DisplayName','Skewness, MST');figure(gcf)\nzlabel('skewness');\ntitle('Skewness, MST');\nsubplot(2,2,4),mesh(wynik_wind_kurt,'DisplayName','Kurtosis, MST');figure(gcf)\nzlabel('kurtosis');\n", "output_sequence": "Presents the statistical properties of the minimum spanning tree as a function of time windows sizes for elements of the chosen WIG 20 companies (close.csv). The Theil index based distance is used. Requires theil.m, mst.m, manh.m to run the program. Note that the {statistical toolbox} from Matlab is required as well."}, {"input_sequence": "rm(list = ls(all = TRUE))\ngraphics.off()\n# setwd('C:/...')\n# install.packages('MASS')\nlibrary(MASS)\nu0 <- 5 * 10^10 # initial capital of insurance company (in USD)\nn <- 70\nu1 <- c(seq(0, by = u0/200, length.out = 21), seq(u0/10 + u0/200, by = (u0 - u0/10 - u0/200)/48, length.out = 49))\ntheta1 <- 0.3 # relative safety loading\ndparameters1 <- list(c(0.0584, 0.9416), c(3.59e-10, 7.5088e-09)) # weights (first vector) and exponential parameters (second vector)\n# produces the exact ruin probability in infinite time for insurance collective risk model with mixture of 2 exponentials\n# distribution claims\nruinmix2exps <- function(u, theta, dparameters) {\n# u: initial capital for risk process theta: security loading in insurance collective risk model dparameters: list, composed\n# of 2 vectors containing the parameters of loss distribution, exponential parameters (first vector) and weights (second\n# vector)\np1 <- dparameters[[1]] # exponential parameters\np <- p1[1]/p2[1]/(p1[1]/p2[1] + (1 - p1[1])/p2[2])\npsii <- p2[1] * (1 - p) + p2[2] * p\nr1 <- (psii + theta * sum(p2) - sqrt((psii + theta * sum(p2))^2 - 4 * prod(p2) * theta * (1 + theta)))/(2 * (1 + theta))\ny <- 1/((1 + theta) * (r2 - r1)) * ((psii - r1) * exp(-r1 * u) + (r2 - psii) * exp(-r2 * u)) # ruin probability using the Laplace transform inversion\nreturn(y)\n}\n# the exact ruin probability in infinite time\npsi1 <- ruinmix2exps(u1, theta1, dparameters1)\n# returns the k-th moment (up to fourth) of the mixture of 2 exponentials distribution claims\nmoments <- function(k, dparameters) {\n# k: order of moment to calculate dparameters: list, composed of 2 vectors containing the parameters of loss distribution,\n# weights (first vector) and exponential parameters (second vector)\np1 <- dparameters[[1]] # weights\np2 <- dparameters[[2]] # exponential parameters\nif (k == 1) {\nmk <- sum(p1/p2)\n} else {\nif (k == 2) {\nmk <- 2 * sum(p1/p2^2)\n} else {\nif (k == 3) {\nmk <- 6 * sum(p1/p2^3)\n} else {\nif (k == 4) {\nmk <- 24 * sum(p1/p2^4)\n}\n}\n}\nreturn(mk) # k-th raw moment of the mixture of 2 exponentials distribution claims\n# returns the moment generating function or its k-th derivative (up to third) for mixture of 2 exponentials distribution\n# claims\nmgfs <- function(x, k, dparameters) {\n# x: scalar, n x 1 vector or m x n matrix, argument of the moment generating function k: scalar, integer, 0 =< k <= 3, order\n# of the derivative dparameters: list, composed of 2 vectors containing the parameters of the loss distribution, weights\n# (first vector) and exponential parameters (second vector)\nif (k == 0) {\ny <- sum((p1 * p2)/(p2 - t(x)))\nif (k == 1) {\ny <- sum((p1 * p2)/(p2 - t(x))^2)\nif (k == 2) {\ny <- 2 * sum((p1 * p2)/(p2 - t(x))^3)\nif (k == 3) {\ny <- 6 * sum((p1 * p2)/(p2 - t(x))^4)\nm <- moments(1, dparameters1) # 1st raw moment\n# returns the adjustment coefficient R for mixture of 2 exponentials distribution claims\nadjR <- function(theta, dparameters) {\n# theta: security loading in insurance collective risk model dparameters: list, composed of 2 vectors containing the\n# parameters of the loss distribution, weights (first vector) and exponential parameters (second vector)\nR0 <- min(p2)\nR0 <- c(12 * theta * m/(3 * m2 + sqrt(9 * m2^2 + 24 * m * m3 * theta)), R0)\nR0 <- min(R0)\nerr = 1\nwhile (err > 1e-09) {\nD1 <- 1 + (1 + theta) * m * r - mgfs(r, 0, dparameters1)\nerr <- r\nr <- r - D1/D2\nerr <- abs(err - r)/r\nR <- r\nreturn(R) # adjustment coefficient R\nR <- adjR(theta1, dparameters1) # adjustment coefficient R\nmgfprim <- mgfs(R, 1, dparameters1) # moment generating function\nC <- (theta1 * m)/(mgfprim - m * (1 + theta1))\n# the Cramer-Lundberg approximation\npsi2 <- C * exp(-R * u1)\n# the exponential approximation\npsi3 <- exp(-1 - (2 * m * theta1 * u1 - m2)/sqrt(m2^2 + 4 * theta1 * m * m3/3))\n# the Lundberg approximation\npsi4 <- (1 + (theta1 * u1 - m2/m/2) * (4 * theta1 * m^2 * m3/(3 * m2^3))) * exp(-(2 * m * theta1 * u1)/m2)\ndelta1 <- (2 * m * theta1)/(m2 + ((4 * m * m3/3/m2) - m2) * theta1) # scale parameter of gamma distribution function\n# the Beekman-Bowers approximation\npsi5 <- (1 - pgamma(u1, shape = delta2, scale = 1/delta1))/(1 + theta1)\n# the Renyi approximation\npsi6 <- exp(-(2 * m * theta1 * u1)/(m2 * (1 + theta1)))/(1 + theta1)\n# parameters of the De Vylder approximation\ndelta <- 3 * m2/m3\nbeta <- 9 * m2^3/(2 * (1 + theta1) * m * m3^2)\np <- 3 * m2^2/(2 * (1 + theta1) * m * m3) - 1/(1 + theta1) + 1\n# the De Vylder approximation\npsi7 <- (beta/(p * delta)) * exp(-(delta - beta/p) * u1)\nif (m2 * m4 < 3/2 * m3^2) {\nthetanew <- (theta1 * m * (2 * m3^2 - m2 * m4)/m2^2/m3)\nm2new <- ((m2 * m4 - 2 * m3^2) * (2 * m2 * m4 - 3 * m3^2)/m3^2/m2^2)\n} else {\nthetanew <- 1/2 * theta1/m2^2 * m * (m3 + m2 * m)\nmnew <- m\nm2new <- 1/2/m2 * m * (m3 + m2 * m)\np1 <- mnew^2/(m2new - mnew^2)\ndparametersnew <- list(p1, p2) # gamma parameters\n# returns the k-th moment (up to fourth) of the mixture of 2 gamma distribution claims\nmomentsgam <- function(k, dparameters) {\n# k: order of moment to calculate dparameters: list of scalars, parameters of gamma distribution\np1 <- dparameters[[1]]\nmk <- p1/p2\nmk <- (p1^2 + p1)/(p2^2)\nmk <- (p1^3 + 3 * p1^2 + 2 * p1)/(p2^3)\nreturn(mk) # k-th raw moment of gamma distribution claims\n# returns the moment generating function or its k-th derivative (up to third) for gamma distribution claims\nmgfsg <- function(x, k, dparameters) {\n# x: scalar, argument of the moment generating function k: scalar, integer, 0 =< k <= 3, order of the derivative\n# dparameters: list of scalars, parameters of gamma distribution\ny <- p2^p1/((p2 - x)^p1)\ny <- (p1/p2) * (p2/(p2 - x))^(p1 + 1)\ny <- p1 * (p1 + 1) * (p1 + 2) * p2^p1/(p2 - x)^(p1 + 3)\n# moments for gamma distribution claims\nmg <- momentsgam(1, dparametersnew) # 1st raw moment\n# returns the adjustment coefficient R for gamma distribution claims\n# theta: security loading in insurance collective risk model dparameters: list of scalars, parameters of gamma distribution\nR0 <- 0.99999999 * p2\nR0 <- c(12 * theta * mg/(3 * mg2 + sqrt(9 * mg2^2 + 24 * mg * mg3 * theta)), R0)\nerr <- 1\nD1 <- 1 + (1 + theta) * mg * r - mgfsg(r, 0, dparameters)\nR <- adjR(thetanew, dparametersnew) # adjustment coefficient R for gamma distribution claims\nmgfprim <- mgfsg(R, 1, dparametersnew) # moment generating function for gamma distribution claims\nC <- (thetanew * mg)/(mgfprim - mg * (1 + thetanew))\nCram <- C * exp(-R * u1) # the Cramer-Lundberg approximation for gamma claims\nu1 <- u1 * p2/p1\nb <- 1/p1\nn <- length(u1)\n# the function to be integrated\nexactgamint <- function(x) {\nj <- 1\nwhile (j < n + 1) {\nuj <- u1[j]\nL <- x^(1/b) * exp(-(x + 1) * uj/b)\nM <- (x^(1/b) * (1 + (1 + thetanew) * (x + 1)/b) - cos(pi/b))^2 + sin(pi/b)^2\nj <- j + 1\ny <- L/M\n# integrates exactgamint function using the Simpson's method\nd <- area(exactgamint, 0, 0.001)\nd <- rbind(1, d)\nerr <- 1e-05\nint <- matrix(1, n)\nj <- 1\nwhile (j < n + 1) {\ni <- 2\nwhile (abs((d[i - 1] - d[i])/d[i]) > err) {\nv <- area(exactgamint, i - 1, i)\nd <- rbind(d, (v + d[i]))\ni <- i + 1\nendd <- length(d)\nint[j] <- d[endd]\nj <- j + 1\n# the 4-moment gamma De Vylder approximation\npsi8 <- Cram + as.vector((thetanew * sin(pi/b)/pi/b) * int)\n# the heavy traffic approximation\npsi9 <- exp((-2 * theta1 * m * u1)/m2)\npa <- matrix(dparameters1[[1]]) # 2 x 1 matrix of weights\npaa <- matrix(pa, nrow = 2, ncol = length(u1))\nd <- rowSums(t(paa/pbb * exp(-pb %*% u1)))\nc <- matrix(m, nrow = length(d)) - d\n# the light traffic approximation\npsi10 <- (m - c)/(1 + theta1)/m\nu2 <- (1 - 1/(1 + theta1)) * u1 # capital for the light traffic approximation\npaa <- matrix(pa, nrow = 2, ncol = length(u2))\nd <- rowSums(t(paa/pbb * exp(-pb %*% u2)))\npsil <- (m - c)/(1 + theta1)/m # the new light traffic approximation\n# the heavy-light traffic approximation\npsi11 <- (1/(1 + theta1)^2) * psi9 + (1 - 1/(1 + theta1)) * psil\nu1 <- u1/10^9\n# the relative errors of the approximations\nerr2 <- (psi2 - psi1)/psi1\nplot(u1, psi1, type = \"l\", col = \"red\", ylim = c(0, 0.801), lwd = 3, main = \"\", xlab = \"u (USD billion)\", ylab = expression(psi(u)),\ncex.axis = 1.6, cex.lab = 1.6)\ndev.new()\nplot(u1, err2, type = \"l\", col = \"blue\", ylim = c(-0.301, 0.301), lwd = 3, main = \"\", xlab = \"u (USD billion)\", ylab = expression((psi[i](u) -\npsi(u))/psi(u)), cex.axis = 1.6, cex.lab = 1.6)\nlines(u1, err3, col = \"brown\", lwd = 3, lty = 4)\nplot(u1, err4, type = \"l\", lty = 4, col = \"red\", ylim = c(-1.01, 1.01), lwd = 3, main = \"\", xlab = \"u (USD billion)\", ylab = expression((psi[i](u) -\nlines(u1, err6, col = \"blue\", lwd = 3, lty = 3)\n", "output_sequence": "Produces the relative error of the approximations (with respect to exact method) of ruin probability in infinite time treated as a function of the initial capital for the mixture of 2 exponentials distibution claims."}, {"input_sequence": "% returns the k-th moment (up to fourth) of the mixture of 2 exponentials distribution claims\nfunction [ mk ] = moments( k,dparameters );\n% k: order of moment to calculate\n% dparameters: list, composed of 2 vectors containing the parameters of loss distribution, weights (first vector) and exponential parameters (second vector)\np1=dparameters(:,1); % weights\np2=dparameters(:,2); % exponential parameters\nif (k==1)\nmk=sum(p1./p2);\nelseif (k==2)\nmk=2*sum(p1./(p2.^2));\nelseif (k==3)\nmk=6*sum(p1./(p2.^3));\nelseif (k==4)\nmk=24*sum(p1./(p2.^4));\nelse\ndisp('k chosen too large')\nend\n", "output_sequence": "Produces the ruin probability in infinite time for mixture of 2 exponentials distribution claims given by Lundberg approximation. Needs the \"moments.m\" function."}, {"input_sequence": "clear all;\nclose all,\nclc;\nformat long;\nu=[0;10^9;5*10^9;10^10;2*10^10;5*10^10]; % initial capital of insurance company (in USD)\ntheta=0.3; % relative safety loading\ndparameters1=[0.0584 , 3.5900e-10 ; 0.9416 , 7.5088e-09]; % weights (first column) and exponential parameters (second column)\nm=moments(1,dparameters1); % 1st raw moment\n% the Lundberg approximation for mixture of 2 exponentials claims with beta1=3.5900e-10, beta2=7.5088e-09, alpha=0.0584 and theta=0.3 (u in USD)\n", "output_sequence": "Produces the ruin probability in infinite time for mixture of 2 exponentials distribution claims given by Lundberg approximation. Needs the \"moments.m\" function."}, {"input_sequence": "rm(list = ls(all = TRUE))\n# setwd('C:/...')\n# returns the k-th moment (up to fourth) of the mixture of 2 exponentials distribution claims\nmoments <- function(k, dparameters) {\n# k: order of moment to calculate dparameters: list, composed of 2 vectors containing the parameters of loss distribution,\n# weights (first vector) and exponential parameters (second vector)\np1 <- dparameters[[1]] # weights\np2 <- dparameters[[2]] # exponential parameters\nif (k == 1) {\nmk <- sum(p1/p2)\n} else {\nif (k == 2) {\nmk <- 2 * sum(p1/p2^2)\n} else {\nif (k == 3) {\nmk <- 6 * sum(p1/p2^3)\n} else {\nif (k == 4) {\nmk <- 24 * sum(p1/p2^4)\n}\n}\n}\nreturn(mk) # k-th raw moment of the mixture of 2 exponentials distribution claims\n}\nu <- c(0, 10^9, 5 * 10^9, 10^10, 2 * 10^10, 5 * 10^10) # initial capital of insurance company (in USD)\ntheta <- 0.3 # relative safety loading\ndparameters1 <- list(c(0.0584, 0.9416), c(3.59e-10, 7.5088e-09)) # weights (first vector) and exponential parameters (second vector)\nm <- moments(1, dparameters1) # 1st raw moment\n# the Lundberg approximation for mixture of 2 exponentials claims with \\fbeta1=3.5900e-10, beta2=7.5088e-09, alpha=0.0584\n# and theta=0.3 (u in USD)\npsi <- (1 + (theta * u - m2/m/2) * (4 * theta * m^2 * m3/(3 * m2^3))) * exp(-(2 * m * theta * u)/m2)\n", "output_sequence": "Produces the ruin probability in infinite time for mixture of 2 exponentials distribution claims given by Lundberg approximation. Needs the \"moments.m\" function."}, {"input_sequence": "clear all\n% user input parameters\ndisp('Please input the lowest and the highest strike of options portfolio as: [10,200]') ;\ndisp(' ') ;\npara=input('Options strike range [lower bound, upper bound]=');\nwhile length(para) < 2\ndisp('Not enough input arguments. Please input in 1*2 vector form like [10,200] or [10,200]');\npara=input('Options strike range [lower bound, upper bound]=');\nend\ns1=para(1);\ndisp(' ') ;\ndisp('Please imput the implied volatility, i.e. 0.25 for 25%') ;\npara=input('Implied volatility =');\nv=para;\ndisp('Please input the maturity of a swap (e.g. 1 year = 1, 3 month = 0.25)') ;\npara=input('Maturity of a swap =');\ntau=para;\nr=0; % interest rate\ns=(s2+s1)/2; % defines the underlying price S\nkp=s1:10:(s-1); % defines the range of puts\ncallputspot = blsprice(s, s, r, tau, v); %BS value of the call(put) with strike equal to spot price\nfor i=1:length(kc);\nivc(i)=kc(i)*v/s;\nd1=(log(s/kc(i))+(r+v^2/2)*tau)/(ivc(i)*tau^0.5);\nd2=d1-ivc(i)*tau^0.5;\ncall(i) = s*normcdf(d1)-kc(i)*exp(-r*tau)*normcdf(d2);\ni=i+1;\nfor i=1:length(kp)\nivp(i)=kp(i)*v/s;\nd1=(log(s/kp(i))+(r+ivp(i)^2/2)*tau)/(ivp(i)*tau^0.5);\nd2=d1-ivp(i)*tau^0.5;\nput(i) = kp(i)*exp(-r*tau)*normcdf(-d2)-s*normcdf(-d1);\ni=i+1;\nend\nfp=log(s./kp)+kp./s-1;\nwc(1)=(fc(1)-0)/(kc(1)-s); %weight of fist call(put) (with strike eqial to spot price)\nwcum=wc(1);\nfor j=2:length(kc);\nwc(j)=(fc(j)-fc(j-1))/(kc(j)-kc(j-1))-wcum;\nwcum=wcum+wc(j);\nj=j+1;\nj=numel(kp);\nwcum=(0-fp(j))/(kp(j)-s);\nwhile kp(j)> s1\nwp(j)=(fp(j-1)-fp(j))/(kp(j)-kp(j-1))-wcum;\nwcum=wcum+wp(j);\nj=j-1;\nwp = wp(2:length(wp));\ncall = call(1:(length(call)-1));\nopt=cat(2,put,callputspot,call);\nw=cat(2,wp,wc);\nstrike=((2/tau)*sum(w.*opt))^0.5\nclear x wcum v th tau r para opt n j i fp f discr d2 d1 wp wc kp kc fc\n", "output_sequence": "Calculates the strike of a variance of a given maturity using the potrfolio of options of a given strike range."}, {"input_sequence": "# clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\ninstall.packages(\"MASS\")\nlibrary(MASS)\n# Black-Scholes Function\nBS = function(S, K, Time, r, sig, type) {\nd1 = (log(S/K) + (r + sig^2/2) * Time)/(sig * sqrt(Time))\nd2 = d1 - sig * sqrt(Time)\nif (type == 1) {\nvalue = S * pnorm(d1) - K * exp(-r * Time) * pnorm(d2)\n}\nif (type == 0) {\nvalue = K * exp(-r * Time) * pnorm(-d2) - S * pnorm(-d1)\nreturn(value)\n}\n# Function to find BS Implied Vol using Bisection Method\nblsimpv = function(S, K, Time, r, market, type) {\nsig = 0.2\nsig.up = 1\nsig.down = 0.001\ncount = 0\nerr = BS(S, K, Time, r, sig, type) - market\n# repeat until error is sufficiently small or counter hits 1000\nwhile (abs(err) > 1e-05 && count < 1000) {\nif (err < 0) {\nsig.down = sig\nsig = (sig.up + sig)/2\n} else {\nsig.up = sig\nsig = (sig.down + sig)/2\n}\nerr = BS(S, K, Time, r, sig, type) - market\ncount = count + 1\n# return NA if counter hit 1000\nif (count == 1000) {\nreturn(NA)\n} else {\nreturn(sig)\n# load data\nx = read.table(\"volsurfdata2.dat\")\n# define variables\nx[, 7] = x[, 2]/(x[, 1] * exp(x[, 3] * x[, 4])) # define moneyness\nPrice = x[, 1]\nClass = x[, 6]\nmon = x[, 7]\nn = length(x[, 1])\n# calculate implied volatility\niv = rep(0, n)\nfor (i in 1:n) {\niv[i] = blsimpv(S = Price[i], K = Strike[i], Time = Time[i], r = Rate[i], market = Value[i], type = Class[i])\nstepwidth = c(0.02, (2/52))\nfirstmon = min(x[, 7])\nlengthmon = ceiling((lastmon - firstmon)/stepwidth[1])\nmongrid = seq(firstmon, lastmon, length = c(lengthmon + 1))\n# grid function\nmeshgrid = function(a, b) {\nlist(x = outer(b * 0, a, FUN = \"+\"), y = outer(b, a * 0, FUN = \"+\"))\n# compute grid\ngridone = meshgrid(mongrid, matgrid)\nMON = gridone$x\ngmon = lengthmon + 1L\nuu = dim(x)\nv = uu[1]\n# calculate the implied volatility surface\nbeta = matrix(0, gmat, gmon)\n# kernel regression via fft using quartic kernel\nj = 1L\nwhile (j < gmat + 1L) {\nk = 1L\nwhile (k < gmon + 1L) {\n\ni = 1L\nX = matrix(0, v, 3)\nwhile (i < (v + 1L)) {\nX[i, ] = c(1, x[i, 7] - MON[j, k], x[i, 4] - MAT[j, k])\ni = i + 1\nY = iv\nh = bandwidth\nW = matrix(0, v, v) #Kernel matrix\nu1 = (MON[j, k] - x[i, 7])/h[1]\naa = 15/16 * (1 - u1^2)^2 %*% (abs(u1) <= 1)/h[1]\nW[i, i] = aa %*% bb\ni = i + 1L\nest = ginv(t(X) %*% W %*% X) %*% t(X) %*% W %*% Y\nbeta[j, k] = est[1]\nk = k + 1L\nIV = beta\nxnew = x\n# redefine variables\nPrice = xnew[, 1]\nn = length(xnew[, 1])\n# calculate implied volatility for original options\niv[i] = blsimpv(Price[i], Strike[i], Time[i], Value[i], Class[i])\n# define points\npts = cbind(mon, Time, iv)\n# load required package\nrequire(lattice)\n# plot\nwireframe(IV ~ MON + MAT, drape = T, ticktype = \"detailed\", pts = pts, main = \"\", zlim = c(0.25, 0.6), aspect = c(1, 1), scales = list(arrows = FALSE,\ny = list(labels = round(seq(firstmat, lastmat, length = 6), 2)), x = list(labels = round(seq(firstmon, lastmon, length = 6),\n2)), z = list(labels = round(seq(0.25, max(IV), length = 8), 2))), ylab = list(\"Time to Maturity\", rot = -8), xlab = list(\"Moneyness\",\nrot = 25), zlab = list(\"Implied Volatility\", rot = 94), panel.3d.wireframe = function(x, y, z, xlim, xlim.scaled,\nylim.scaled, pts, drape = drape, ...) {\npanel.3dwire(x, y, z, xlim = xlim, ylim = ylim, zlim = zlim, xlim.scaled = xlim.scaled, = ylim.scaled, = zlim.scaled,\ndrape = TRUE, ...)\n\npanel.3dscatter(pts[, 1], pts[, 2], pts[, 3], xlim = xlim, ylim = ylim, zlim = zlim, xlim.scaled = xlim.scaled, = ylim.scaled,\nzlim.scaled = zlim.scaled, type = \"p\", col = c(4), lwd = 3, cex = 1, pch = c(19), .scale = TRUE, ...)\n}, screen = list(z = 240, x = -70))\n", "output_sequence": "Plots the implied volatility of DAX options on January, 4th 1999, using kernel regression employing a quartic kernel with bandwidth=c(0.05, 0.3) for moneyness and time t maturity respectively."}, {"input_sequence": "function A2 = ADcritPareto(param,x)\n% ADCRITPARETO Anderson-Darling test statistics for the Pareto distribution.\n% A2=ADcritPareto(param,x) returns the value of the Anderson-Darling\n% statistics for the Pareto distribution with given\n% parameters (param) and data from the analyzed sample (x).\nglobal d v w2 a2;\nn = length(x);\nalpha = param(1);\nu = sort(Paretocdf(x,alpha,lambda,1))';\nemp = (1:n) / n;\nDplus = max(emp-u);\nDminus = max(u-emp+1/n);\nD = max([Dplus;Dminus]);\nW2 = sum((u-(1:2:(2*n-1))/(2*n)).^2)+1/(12*n);\nuno0 = u(find(u==0));\nif ~isempty(uno0)\nu(find(u==0))=min(uno0);\nend\nA2 = -n-1/n*sum((1:2:(2*n-1)).*log(u)+((2*n-1):-2:1).*log(1-u));\nd = [d;D];\nw2 = [w2;W2];\n", "output_sequence": "Calculates the Anderson-Darling test statistic for Pareto distribution. The function is required by STFloss08t."}, {"input_sequence": "########################## SUBROUTINES ##########################\nBurrrnd = function(alpha,lambda,tau,n,m){\n#BURRRND Random arrays from Burr distribution.\n# R = BURRRND(ALPHA,LAMBDA,TAU,N,M) returns an M-by-N array of random numbers\n# chosen from the Burr distribution with parameters ALPHA, LAMBDA, TAU.\n#\n# The default values for the parameters ALPHA, LAMBDA, TAU, M, N are\n# 1, 2, 1, respectively.\n# BURRRND uses the inversion method.\nif (missing(m)){\nm = 1\n}\nif (missing(n)){\nn = 1\nif (missing(tau)){\ntau = 2\nif (missing(lambda)){\nlambda = 1\nif (missing(alpha)){\nalpha = 1\nu = matrix(0,n,m)\nfor (i in 1:m){\nu[,i] = (lambda*(runif(n,0,1)^(-1/alpha)-1))^(1/tau)\n}\ny = u\nreturn(y)\n}\nmixexprnd = function(p,beta1,beta2,n,m){\n#MIXEXPRND Random arrays from the mixed exponential distribution.\n# Y = MIXEXPRND(P,BETA1,BETA2,N,M) returns an M-by-N array of random numbers\n# chosen from the mixed exponential distribution with parameters P, BETA1,\n# The default values for A, BETA1, N, M are 0.5, 1, 2, 1,\n# respectively.\n# MIXEXPRND uses the exponential number generator.\nif (missing(p)){\np = 0.5\nif (missing(beta1)){\nbeta1 = 1\nif (missing(beta2)){\nbeta2 = 2\ny = rexp(n*m,rate=(1/beta2))\naux = which(runif(n*m,0,1)<=p)\nif(!missing(aux)){\ny[aux]=rexp(length(aux),1/beta1)\n}\ny=matrix(y,n,m);\nreturn(y)\nsimHPP = function(lambda,T,N){\n# SIMHPP Homogeneous Poisson process.\n# Y = SIMHPP(lambda,T,N) generates N trajectories of the\n# homogeneous Poisson process with intensity LAMBDA. T is the time\n# horizon.\nif(lambda <= 0 || length(lambda)!=1){\nstop(\"simHPP: Lambda must be a positive scalar.\")\n}\nif(T <= 0 || length(T)!=1){\nstop(\"simHPP: T must be a positive scalar.\")\nif(N <= 0 || length(N)!=1){\nstop(\"simHPP: N must be a positive scalar.\")\nEN = rpois(N,lambda*T)\nym = matrix(T,2*max(EN)+2,N)\ny = tmp\ny[,,1] = ym\ny[,,2] = matrix(1,2*max(EN)+2,1)%*%t(EN)\ni=1\nwhile(i<=N){\nif(EN[i]>0){\nttmp = c(sort(T*runif(EN[i])))\ny[1:(2*EN[i]+1),i,1] = c(0,ttmp[ceiling((1:(2*EN[i]))/2)])\n}else{\ny[1,i,1] = 0\n}\ny[1:(2*EN[i]+2),i,2] = c(0,floor((1:(2*EN[i]))/2),EN[i])\ni=i+1\nsimNHPP = function(lambda,parlambda,T,N){\n\n# SIMNHPP Non-homogeneous Poisson process.\n# ---------------------------------------------------------------------\n# Y = SIMNHPP(lambda,parlambda,T,N) generates N trajectories of the\n# non-homogeneous Poisson process with intensity specified by LAMBDA\n# (0 - sine function, 1 - linear function, 2 - sine square function)\n# with paramters in PARLAMBDA. T is the time horizon. The function\n# usues thining method.\n\na = parlambda[1]\nif (lambda==0){\nd = parlambda[3]\nJM = simHPP(a+b,T,N)\n} else if(lambda==1){\nJM = simHPP(a+b*T,T,N)\n} else if (lambda==2){\n}\nrjm = nrow(JM)\nyy = array(0,c(rjm,N,2))\nyy[,,1]= matrix(T,nrow=rjm,ncol=N)\ni=1\nmaxEN=0\nwhile(i<=N){\npom = JM[,i,1][JM[,i,1]<T]\npom = pom[2*(1:(length(pom)/2))]\nR = runif(NROW(pom))\nif (lambda==0){\nlambdat = (a+b*sin(2*pi*(pom+d)))/(a+b)\n} else {\nif (lambda==1){\nlambdat = (a+b*pom)/(a+b*T)\n} else {\nif (lambda==2){\nlambdat = (a+b*sin(2*pi*(pom+d))^2)/(a+b)\n}}}\npom = pom[R<lambdat]\nEN = NROW(pom)\nmaxEN = max(maxEN,EN)\nyy[1:(2*EN+1),i,1] = c(0,rep(pom,each=2))\nyy[2:(2*EN),i,2] = c(floor((1:(2*EN-1))/2))\nyy[(2*EN+1):rjm,i,2] = matrix(EN,nrow=rjm-2*EN,ncol=1)\ni=i+1\nyy = yy[1:(2*maxEN+2),,]\nreturn(yy)\nParetornd = function(alpha,lambda,n,m){\n#PARETORND Random arrays from Pareto distribution.\n# Y = PARETORND(ALPHA,LAMBDA,N,M) returns an M-by-N array of random numbers\n# chosen from the Pareto distribution with parameters ALPHA, LAMBDA.\n# The default values for ALPHA, LAMBDA, N, M 1, respectively.\n# PARETORND uses the inversion method.\nu = matrix(0,n,m)\nfor (i in 1:m){\nu[,i] = lambda*(runif(n,0,1)^(-1/alpha)-1)\ny = u\nsimNHPPALP = function(lambda,parlambda,distrib,params,T,N){\n# generates aggregate loss process driven by the non-homogeneous Poisson process.\n# ---------------------------------------------------------------------\n# y = simNHPPALP(lambda,parlambda,distrib,params,T,N)\n# Output:\n# Parameter: y\n# Definition: 2*max+2 x N x 2 array, generated process - max is the maximum number of jumps for all generated trajectories\n# Input:\n# Parameter: lambda\n# Definition: scalar, intensity function, sine function (lambda=0), linear function (lambda=1) or sine square function (lambda=2)\n# Parameter: parlambda\n# Definition: n x 1 vector, parameters of the intensity function lambda\n# (n=2 for lambda=1, n=3 otherwise)\n# Parameter: distrib\n# Definition: string, claim size distribution\n# Parameter: params\n# Definition: n x 1 vector, parameters of the claim size distribution\n# n = 1 (exponential)\n# n = 2 (gamma, lognormal, Pareto, Weibull)\n# n = 3 (Burr, mixofexps)\n# Parameter: T\n# Definition: scalar, time horizon\n# Parameter: N\n# Definition: scalar, number of trajectories\n\nif(lambda != 0 && lambda != 1 && lambda!=2){\nstop(\"simNHPPALP: Lambda must be either 0,1 or 2.\")\nif(T <= 0 || (length(T))!=1){\nstop(\"simNHPPALP: T must be a positive scalar.\")\nif(N <= 0 || (length(N))!=1){\nstop(\"simNHPPALP: N must be a positive scalar.\")\nif(length(parlambda)!=3 && (lambda)!=1){\nstop(\"simNHPPALP: for lambda 0 or 2, parlambda must be a 3 x 1 vector.\")\nif(length(parlambda)!=2 && (lambda)==1){\nstop(\"simNHPPALP: for lambda 1, parlambda must be a 2 x 1 vector.\")\nif((distrib==\"Burr\" || distrib==\"mixofexps\") && (length(params))!=3){\nstop(\"simNHPPALP: for Burr and mixofexps distributions, params must be a 3 x 1 vector.\")\nif((distrib==\"gamma\" || distrib==\"lognormal\"|| distrib==\"Pareto\" || distrib==\"Weibull\") && (length(params))!=2){\nstop(\"simNHPPALP: for gamma, lognormal, Pareto and Weibull distributions, params must be a 2 x 1 vector.\")\nif(distrib==\"exponential\" && (length(params))!=1){\nstop(\"simNHPPALP: for exponential distribution, params must be a scalar.\")\nif(distrib != \"exponential\" && distrib != \"gamma\" && distrib != \"mixofexps\" && distrib != \"Weibull\" && distrib != \"lognormal\" && distrib !=\"Pareto\" && distrib != \"Burr\"){\nstop(\"simNHPPALP: distribs should be: exponential, gamma, mixofexps, Weibull, lognormal, Pareto or Burr\")\npoisproc = simNHPP(lambda,parlambda,T,N)\nrpp = dim(poisproc)[1]\nlosses = matrix(0,rpp,cpp)\nif (distrib==\"Burr\"){\ni = 1\nif(N==1){\naux = min(as.matrix(which(poisproc[,1]==T))) #[1:i,])\n}else{\naux = min(as.matrix(which(poisproc[,i,1]==T))) #[1:i,])\n}\nif(aux>2){\nlaux = cumsum(Burrrnd(params[1],params[2],params[3],aux/2-1,1))\nlosses[3:aux,i] = laux[ceiling((1:(aux-2))/2)]\nif(aux<rpp){\nlosses[(aux+1):rpp,i] = matrix(laux[length(laux)],rpp-aux,1)\n}else{\nlosses[,i]=rep(0,rpp)\ni=i+1\n}else if(distrib==\"exponential\"){\nif(N==1){\nlaux = cumsum(rexp(aux/2-1,rate=1/params[1]))\nlosses[3:aux,i]=laux[ceiling((1:aux-2)/2)]\nlosses[(aux+1):rpp,i]=matrix(laux[length(laux)],rpp-aux,1)\n}else if(distrib==\"gamma\"){\nlaux = cumsum(rgamma(aux/2-1,shape=params[1],rate=params[2],scale=(1/params[2])))\nlosses[3:aux,i] = laux[ceiling((1:aux-2)/2)]\nlosses[,i] = rep(0,rpp)\n}else if(distrib==\"lognormal\"){\nlaux = cumsum(rlnorm(aux/2-1,meanlog=params[1],sdlog=params[2]))\nlosses[3:aux,i] = laux[ceiling((1:(aux-2))/2)]\n}else if(distrib==\"mixofexps\"){\nlaux = cumsum(mixexprnd(params[3],params[1],params[2],aux/2-1,1))\n}else if(distrib==\"Pareto\"){\nlaux=cumsum(Paretornd(params[1],params[2],aux/2-1,1))\n}else if(distrib==\"Weibull\"){\nlaux=cumsum(rweibull(aux/2-1,scale=params[1]^(-1/params[2]),shape=params[2]))\nif(N==1){\ny = array(0,dim(poisproc))\ny[,1] = poisproc[,1]\n}else{\ny[,,1] = poisproc[,,1]\ny[,,2] = losses\n########################## MAIN PROGRAM ##########################\nBondCoupon = function(Z,C,D,T,r,lambda,parlambda,distr,params,Tmax,N){\nstop(\"BondCoupon: Lambda must be either 0,1 or 2.\")\nif(length(Z) !=1){\nstop(\"BondCoupon: payment at maturity Z needs to be a scalar\")\nif(length(C) !=1){\nstop(\"BondCoupon: coupon payments C needs to be a scalar\")\nif(length(r) !=1){\nstop(\"BondCoupon: discount rate needs to be a scalar\")\nif(length(D)==1){\nstop(\"BondCoupon: threshold level D needs to be a vector \")\nif(length(T)==1){\nstop(\"BondCoupon: time to expiry T needs to be a vector \")\nx = simNHPPALP(lambda,parlambda,distr,params,Tmax,N)\nTl = length(T)\ny = matrix(0,Tl*Dl,3)\ni = 1 #loop (times to maturity)\nwyn = 0\nwhile(i<=Tl){\nwhile(k<=N){\ntraj = cbind(x[,k,1],x[,k,2])\nif (traj[length(traj[which(traj[,1]<=T[i]),1]),2]<=D[j]){\nwyn = wyn+(1-exp(-r*T[i]))/r\nwyn2 = wyn2+1\nwyn = wyn+(1-exp(-r*traj[length(traj[which(traj[,2]<=D[j])]),1]))/r\nk = k + 1\ny[(i-1)*Dl+j,1] = T[i]\ny[(i-1)*Dl+j,3] = C*wyn/N+Z*exp(-r*T[i])*wyn2/N\nwyn = 0\nj = j+1\nj = 1\ni = i + 1\n", "output_sequence": "Computes the price of the coupon-bearing CAT bond for the given claim amount distribution and non-homogeneous Poisson process governing the flow of losses."}, {"input_sequence": "function[y] = BondCoupon(Z,C,D,T,r,lambda,parlambda,distr,params,Tmax,N)\nif(lambda ~= 0 && lambda ~= 1 && lambda~=2)\nerror('BondCoupon: Lambda must be either 0,1 or 2.');\nend\nif(length(Z) ~=1)\nerror('BondCoupon: payment at maturity Z needs to be a scalar');\nif(length(C) ~=1)\nerror('BondCoupon: coupon payments C needs to be a scalar');\nif(length(r) ~=1)\nerror('BondCoupon: discount rate needs to be a scalar');\nif(length(D)==1)\nerror('BondCoupon: threshold level D needs to be a vector');\nif(length(T)==1)\nerror('BondCoupon: time to expiry T needs to be a vector');\nx = simNHPPALP(lambda,parlambda,distr,params,Tmax,N);\nTl = length(T);\ny = zeros(Tl*Dl,3);\ni = 1; %loop (times to maturity)\nwyn = 0;\nwhile(i<=Tl)\nwhile(k<=N)\ntraj = [x(:,k,1),x(:,k,2)];\nif (traj(length(traj(find(traj(:,1)<=T(i)),1)),2)<=D(j))\nwyn = wyn+(1-exp(-r*T(i)))/r;\nwyn2 = wyn2+1;\nelse\nwyn = wyn+(1-exp(-r*traj(length(traj(find(traj(:,2)<=D(j)))),1)))/r;\nend\nk = k + 1;\nend\ny((i-1)*Dl+j,1) = T(i);\ny((i-1)*Dl+j,3) = C*wyn/N+Z*exp(-r*T(i))*wyn2/N;\nwyn = 0;\nj = j + 1;\nend\nj = 1;\ni = i + 1;\n\nend\n", "output_sequence": "Computes the price of the coupon-bearing CAT bond for the given claim amount distribution and non-homogeneous Poisson process governing the flow of losses."}, {"input_sequence": "function [D,V,W2,A2]=cvln(n,m,mu,sigma,method,numberofiterations)\n% CVLN Simulated values of the EDF statistics for the lognormal distribution.\n% [D,V,W2,A2]=CVLN(N,M,MU,SIGMA,METHOD,NUMBERIFITERATIONS) returns\n% simulated values of the EDF statistics for the lognormal distribution\n% with a given parameters MU, SIGMA. It uses M simulated samples\n% of length N and the chosen method of estimation (METHOD=0 - method\n% of moments, METHOD=1 - maximum likelihood, METHOD=2 - A2 statistic\n% minimalization). NUMBEROFITERATIONS is the number of iterations\n% in estimation, if needed.\nu=zeros(n,m);\nx=lognrnd(mu,sigma,n,m);\nfor i=1:m\n[db,delik,demin,vb,velik,vemin,w2b,w2elik,w2emin,a2b,a2elik,a2emin,mu,sigma,mulik,sigmalik,mumin,sigmamin,caution]=estln(x(:,i),method,numberofiterations);\nif(method==1)\nmur=mulik;\nsigmar=sigmalik;\nelse if(method==2)\nmur=mumin;\nsigmar=sigmamin;\nelse\nmur=mu;\nsigmar=sigma;\nend\nu(:,i)=sort(logncdf(x(:,i),mur,sigmar));\nemp(:,i)=(1:n)/n;\nend\nDplus=max(emp-u);\nDminus=max(u-emp+1/n);\nD=max([Dplus;Dminus]);\nW2(i)=sum((u(:,i)-((1:2:(2*n-1))/(2*n))').^2)+1/(12*n);\nA2(i)=-n-1/n*sum((1:2:(2*n-1))'.*log(u(:,i))+((2*n-1):-2:1)'.*log(1-u(:,i)));\n", "output_sequence": "Simulates values of the EDF statistics for the lognormal distribution for given values of MU and SIGMA. Required by STFloss08t."}, {"input_sequence": "function res=samplemef(data, xaxis)\n% SAMPLEMEF Sample mean excess function.\n% RES = SAMPLEMEF (DATA, XAXIS) returns the value of the sample\n% mean excess function for the vector DATA in the points from vector XAXIS.\nsorteddata = sort (data);\ndataLength =length(sorteddata);\nresLength = length(xaxis);\nres = xaxis;\ni=0;\nvaluesOnTheLeft = 0;\nsmef = mean (sorteddata(1:dataLength));\nwhile ( i < resLength )\ni = i + 1;\nwhile (sorteddata(valuesOnTheLeft+1) < xaxis(i))\nvaluesOnTheLeft = valuesOnTheLeft + 1;\nend\nsmef = mean (sorteddata(valuesOnTheLeft+1:dataLength));\nres(i) = smef - xaxis(i);\nend\n", "output_sequence": "Returns the value of the sample mean excess function for the vector DATA in the points from vector XAXIS. A function required by STFloss08."}, {"input_sequence": "function y=tstabpdf3(x,alpha,sigma,lambda,beta);\nif nargin<5\nbeta=0;\nend;\nN=length(x);\nif (alpha>0)&&(alpha<=2)&&(alpha~=1)&&(lambda>0)&&(sigma>0)&&(beta<=1)&&(beta>=-1)\nxmax=max(abs(x))+1;\n[xp,yp]=tstabpdf(alpha,sigma,lambda,beta,xmax);\n\n% y=zeros(N,1);\n% for i=1:N\n% ind=sum(x(i)>xp);\n% y(i)=yp(ind)+(yp(ind+1)-yp(ind))/(xp(ind+1)-xp(ind))*(x(i)-xp(ind));\n% end;\ny=max(interp1q(xp',yp',x),eps);\nelse\ny=NaN(N,1);\n", "output_sequence": "Calculates distribution of a stable distribution. Auxiliary function, required by STFstab03.m and STFstab04.m function."}, {"input_sequence": "function [D,V,W2,A2]=cvBurr(n,m,alpha,lambda,tau,method,numberofiterations)\n% CVBURR Simulated values of the EDF statistics for the Burr distribution.\n% [D,V,W2,A2]=CVBURR(N,M,ALPHA,LAMBDA,TAU,METHOD,NUMBERIFITERATIONS)\n% returns simulated values of the EDF statistics for the Burr distribution\n% with given parameters ALPHA, LAMBDA, TAU. It uses M simulated samples\n% of length N and the chosen method of estimation (METHOD=1 - maximum\n% likelihood, METHOD=2 - A2 statistics minimalization).\n% NUMBEROFITERATIONS is the number of iterations in estimation, if needed.\nu=zeros(n,m);\nx=Burrrnd(alpha,lambda,tau,n,m);\nfor i=1:m\n[db,de,vb,ve,w2b,w2e,a2b,a2e,alpha,lambda,tau,alphamin,lambdamin,taumin,likconv]=estBurr(x(:,i),method,numberofiterations);\nif(method==1)\nal=alpha;\nta=tau;\nend\nif(method==2)\nal=alphamin;\nu(:,i)=sort(Burrcdf(x(:,i),al,la,ta,1));\nemp(:,i)=(1:n)/n;\nend\nDplus=max(emp-u);\nDminus=max(u-emp+1/n);\nD=max([Dplus;Dminus]);\nfor i=1:m\nW2(i)=sum((u(:,i)-((1:2:(2*n-1))/(2*n))').^2)+1/(12*n);\nA2(i)=-n-1/n*sum((1:2:(2*n-1))'.*log(u(:,i))+((2*n-1):-2:1)'.*log(1-u(:,i)));\n", "output_sequence": "Simulates values of the EDF statistics for the Burr distribution. Required by STFloss08t"}, {"input_sequence": "import numpy as np\nfrom PythonTsa.True_acf import Tacf_pacf_fig\nimport matplotlib.pyplot as plt\nar1 = np.array([1, 0, -0.36])\nTacf_pacf_fig(ar = ar1, ma = [1], both = True, lag = 20)\nplt.savefig('pyTSA_SARMA_fig5-6.png', dpi = 1200,\nbbox_inches ='tight', transparent = True, legend = None);\nma1 = np.array([1,0,0,0,0.46])\nTacf_pacf_fig(ar = [1], ma = ma1, both = True, lag = 20)\nplt.savefig('pyTSA_SARMA_fig5-7.png', dpi = 1200,\nTacf_pacf_fig(ar = ar1, ma = ma1, both = True, lag = 20)\nplt.savefig('pyTSA_SARMA_fig5-8.png', dpi = 1200,\n", "output_sequence": "This Quantlet simulates ARMA(2,2) - autoregressive moving average process and draws the true ACF and PACF"}, {"input_sequence": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom PythonTsa.plot_acf_pacf import acf_pacf_fig\nfrom PythonTsa.LjungBoxtest import plot_LB_pvalue\nIBM = pd.read_csv('IBM.csv', header = 0)\nIBM['Date'] = pd.to_datetime(IBM['Date'])\nIBM.index = IBM['Date']\nIBMclose = IBM.Close\nLIBMclose = np.log(IBMclose)\nLIBMclose.plot()\nplt.xticks(rotation = 15)\nplt.savefig('pyTSA_TrendIBM_fig9-5.png', dpi = 1200, bbox_inches ='tight',\ntransparent = True, legend = None); plt.show()\ndLIBMclose = LIBMclose.diff(1).dropna()\ndLIBMclose.plot()\nplt.savefig('pyTSA_TrendIBM_fig9-6.png', dpi = 1200, bbox_inches ='tight',\nacf_pacf_fig(dLIBMclose, both = False, lag = 20)\nplt.savefig('pyTSA_TrendIBM_fig9-7.png', dpi = 1200, bbox_inches ='tight',\nplot_LB_pvalue(dLIBMclose, noestimatedcoef = 0, nolags = 30)\nplt.savefig('pyTSA_TrendIBM_fig9-8.png', dpi = 1200, bbox_inches ='tight',\n", "output_sequence": "This Quantlet plots monthly time series of returns of Procter and Gamble from 1961 to 2016 and  their ACF and PACF (Example, 2.4 Figures 2.8-2.9 in the book)"}, {"input_sequence": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom pandas.plotting import lag_plot\nurl= 'http://s3.amazonaws.com/assets.datacamp.com/course/dasi/present.txt'\nbirth= pd.read_table(url, sep=' ')\nbirth.to_csv('Noboyngirl.csv')\nbirth_year = birth.set_index('year')\nbirth_year.plot(); plt.show()\nboy,= plt.plot(birth_year['boys'],'b-', label='boys')\nplt.legend(handles=[boy, girl]);\nplt.savefig('pyTSA_USBoysGirls_Fig1-21.png', dpi = 1200, bbox_inches='tight',\ntransparent = True); plt.show()\nfig = plt.figure()\nbirth_year[:30].plot(kind='bar', color=['black','grey'], ax=fig.add_subplot(211))\nplt.show()\nlag_plot(birth_year['boys'], ax=fig.add_subplot(211))\nplt.title('Lag plot of the boys')\nlag_plot(birth_year['girls'], ax=fig.add_subplot(212))\nplt.title('Lag plot of the girls')\nplt.savefig('pyTSA_USBoysGirls_Fig1-22.png', dpi = 1200, bbox_inches='tight',\ntransparent = True, legend = None);\n", "output_sequence": "This Quantlet produces time series plot, the bar charts and lag plots to vizualize a data of the numbers of boys and girls born in USA per year from 1940 to 2002."}, {"input_sequence": "import pandas as pd\n#import numpy as np\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nfrom PythonTsa.plot_acf_pacf import acf_pacf_fig\nfrom PythonTsa.LjungBoxtest import plot_LB_pvalue\nfrom arch import arch_model\nfrom PythonTsa.Selecting_arma import choose_arma\nfrom statsmodels.graphics.api import qqplot\nibm = pd.read_csv('ibmlogret.csv', header = 0)\nlogret = ibm['logreturn']\nlogret.index = ibm['date']\nlogret.plot()\nplt.savefig('pyTSA_ReturnsIBM_fig6-29.png', dpi = 1200,\nbbox_inches ='tight', transparent = True, legend = None); plt.show()\nsm.tsa.kpss(logret, regression = 'c', nlags = 'auto')\nacf_pacf_fig(logret, both = True, lag = 36)\nplt.savefig('pyTSA_ReturnsIBM_fig6-30.png', dpi = 1200,\nplot_LB_pvalue(logret, noestimatedcoef = 0, nolags = 36)\nplt.savefig('pyTSA_ReturnsIBM_fig6-31.png', dpi = 1200,\nbbox_inches ='tight', transparent = True, legend = None)\nchoose_arma(logret, max_p = 2, max_q = 2, ctrl = 1.01)\narma = sm.tsa.ARMA(logret, order = (0, 1)).fit(trend = 'nc')\nprint(arma.summary())\narmaresid = arma.resid\nplot_LB_pvalue(armaresid, noestimatedcoef = 1, nolags = 36)\nplt.savefig('pyTSA_ReturnsIBM_fig6-32.png', dpi = 1200,\nbbox_inches ='tight', transparent = True, legend = None)\nplot_LB_pvalue(armaresid**2, noestimatedcoef = 0, nolags = 36)\nplt.savefig('pyTSA_ReturnsIBM_fig6-33.png', dpi = 1200,\ngarch = arch_model(armaresid, p = 1, q = 1, mean = 'Zero').fit(disp = 'off')\nprint(garch.summary())\negarch = arch_model(armaresid, p = 0, o = 1, q = 1, mean = 'Zero',\nvol = 'EGARCH').fit(disp = 'off')\nprint(egarch.summary())\negarchresid = egarch.std_resid\nplot_LB_pvalue(egarchresid, noestimatedcoef = 0, nolags = 36)\nplt.savefig('pyTSA_ReturnsIBM_fig6-34.png', dpi = 1200,\nplot_LB_pvalue(egarchresid**2, noestimatedcoef = 0, nolags = 36)\nplt.savefig('pyTSA_ReturnsIBM_fig6-35.png', dpi = 1200,\nqqplot(egarchresid, line = 'q', fit = True)\nplt.savefig('pyTSA_ReturnsIBM_fig6-36.png', dpi = 1200,\n", "output_sequence": "This Quantlet plots monthly time series of returns of Procter and Gamble from 1961 to 2016 and  their ACF and PACF (Example, 2.4 Figures 2.8-2.9 in the book)"}, {"input_sequence": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom PythonTsa.plot_acf_pacf import acf_pacf_fig\nfrom statsmodels.tsa.stattools import kpss\nx = pd.read_csv('gdpquarterlychina1992.1-2017.4.csv', header = 0)\ndates = pd.date_range(start = '1992', periods = len(x), freq = 'Q')\nx.index = dates; x = pd.Series(x['GDP'])\ndx = x.diff(4) # seasonal differencing\ndx = dx.dropna()\ndx.plot(marker = 'o', ms = 3) # ms means marker size\nplt.savefig('pyTSA_GDPChinaDediff_fig3-4.png', dpi = 1200,\nbbox_inches ='tight', transparent = True, legend = None); plt.show()\nd1dx = dx.diff(1)\nd1dx = d1dx.dropna()\nd1dx.plot(marker = 'o', ms = 3);\nplt.savefig('pyTSA_GDPChinaDediff_fig3-5.png', dpi = 1200,\nacf_pacf_fig(d1dx, both = False, lag = 44)\nplt.savefig('pyTSA_GDPChinaDediff_fig3-6.png', dpi = 1200,\nkpss(d1dx, regression = 'c')\n#InterpolationWarning: p-value is greater than the indicated p-value\n#(0.20253789040706957, 0.1, 12,\n", "output_sequence": "This Quantlet produces and plot time series of seasonally and firstly differenced Chinese quarterly GDP and its ACF for the period from 1992 to 2017."}, {"input_sequence": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PythonTsa.plot_acf_pacf import acf_pacf_fig\nbitcoin = pd.read_excel('BitcoinPrice17-6-23-18-6-22.xlsx', header = 0)\ndat = pd.date_range('2017 06 23', periods = len(bitcoin),freq = 'D')\nbitcoin.index = dat\nprice = bitcoin['ClosingP']\nprice.plot(); plt.title('Bitcoin Price 2017.6.23 2018.6.22')\nplt.ylabel('Price in USD');\nplt.savefig('pyTSA_BTC_Fig1-1.png', dpi = 1200,\nbbox_inches ='tight', transparent = True); plt.show()\nacf_pacf_fig(price, lag = 25)\nplt.savefig('pyTSA_BTC_Fig1-14.png', dpi = 1200,\nbbox_inches ='tight', transparent = True, legend = None); plt.show()\nlogp = np.log(price)\nlogp.plot(); plt.title('Logarithm of the Bitcoin Price')\nplt.ylabel('log(rice)')\nplt.savefig('pyTSA_BTC_Fig1-15.png', dpi = 1200,\nacf_pacf_fig(logp, lag = 25)\nplt.savefig('pyTSA_BTC_Fig1-16.png', dpi = 1200,\ndlogp = logp.diff(1)\ndlogp = dlogp.dropna() #delete \"NaN\"\ndlogp.plot()\nplt.title('Difference of Logarithm of the Bitcoin Price')\nplt.savefig('pyTSA_BTC_Fig1-17.png', dpi = 1200,\nacf_pacf_fig(dlogp, lag = 25); plt.savefig('pyTSA_BTC_Fig1-3.png', dpi = 1200,\nbbox_inches ='tight', transparent = True)\nplt.savefig('pyTSA_BTC_Fig1-18.png', dpi = 1200,\n", "output_sequence": "This Quantlet produces plots of closing daily prices, log-returns and their ACF for Time series of BTC daily prices from June 23, 2017 to June 22, 2018."}, {"input_sequence": "%% Spanning tests - Condacts all Mean-variance tests from Kan, Zhou (2012)\nW_all = [];\nFtest_all = [];\nfor n=1:size(CC_RET_wins,2)\n[W,LR,LM,pw,plr,plm] = span(IND_RET(end-261:end,:), CC_RET_wins(end-261:end,n));\n[Ftest,Ftest1,Ftest2,pval,pval1,pval2,alpha,delta] = stepdown(IND_RET(end-261:end,:), CC_RET_wins(end-261:end,n));\nW_all = [W_all , W ];\nplr_all = [plr_all, plr ];\nFtest_all = [Ftest_all, Ftest ];\npval_all = [pval_all, pval ];\ndelta_all = [delta_all, delta ];\nend\n[W,LR,LM,pw,plr,plm] = span(IND_RET, CC_RET_wins(:,pval_all<0.05)); %Display only significant results\n[Ftest,Ftest1,Ftest2,pval,pval1,pval2,alpha,delta] = stepdown(IND_RET, CC_RET_wins(:,pval_all<0.05));\n%% Save results to tex tables\ninput.data = [W,LR,LM,Ftest,Ftest1,Ftest2;pw,plr,plm,pval,pval1,pval2]';\ninput.tableColLabels = {'Test-stat', 'P-value'};\ninput.tableRowLabels = {'Wald', 'LR', 'LM','F-test', 'F-test1',\ninput.transposeTable = 0;\ninput.dataFormatMode = 'column';\ninput.tableColumnAlignment = 'r';\ninput.tableBorders = 0;\ninput.tableCaption = strcat('Diversification measures: rebalancing_', rebal_freq, '_liquidity constraint_',liquidity_const);\ninput.makeCompleteLatexDocument = 0;\nlatex = latexTable(input);\n%% Save results to tex tables Spanning tests for all cryptos separately\ninput.data = [Ftest_all;Ftest1_all;Ftest2_all;pval_all;pval1_all;pval2_all]';\ninput.tableColLabels = {'F-Test', 'F-Test1','F-Test2', 'P-value', 1', 'P-value 2'};\ninput.tableRowLabels = CC_TICK;\ninput.dataFormat = {'%.2f'};\n%% Tests of Sharpe and CEQ difference significance according Wolf, Ledoit (2008)\nRET_ALL = [IND_RET(end-size(CC_IND_RET_EW,1)+1:end,4), IND_RET_EW, ...\nCC_IND_RET_EW, CC_IND_RET_MAXSHARPE, ...\nCC_IND_RET_ALL_COMB_NAIVE,\nSTR_COMB = nchoosek(1:size(RET_ALL, 2),\nPVAL = [];\nPVAL_Bootstrap = [];\nPVALPW_CEQ_Bootstrap = [];\nbstar = 5; %length of block for bootstrap inference\nM = 1000; %number of resamplings\ngamma = 1; %risk-averse\nfor n = 1: size(STR_COMB, 1)\n[se, pval,sepw,pvalpw] = sharpeHACnoOut(RET_ALL(:,STR_COMB (n,1)),RET_ALL(:,STR_COMB (n,2)));\nPVAL = [PVAL; pval];\nend\nSTR_NAME = {'INDEX','IND-EW', 'EW', 'MV - max ret', ...\n'MV - max Sharpe','Min Var','ERC',...\n'MCVaR - min risk', 'MD', 'COMB NAIVE', 'COMB'}\ninput2.data = [PVAL, PVAL_CEQ];\ninput2.tableColLabels = {'P-Value Sharpe', 'P-Value CEQ'};\ninput2.tableRowLabels = {};\nfor n = 1:size(PVAL_CEQ,1)\ninput2.tableRowLabels(n) = strcat(STR_NAME(STR_COMB(n,1)),' - ',STR_NAME(STR_COMB(n,2)));\ninput2.transposeTable = 0;\ninput2.dataFormatMode = 'column';\ninput2.dataNanString = '-';\ninput2.tableColumnAlignment = 'r';\ninput2.tableBorders = 0;\ninput2.tableCaption = strcat('P-value_', rebal_freq, '_liquidity constraint_',liquidity_const);\ninput2.makeCompleteLatexDocument = 0;\nlatex = latexTable(input2);\n%% T-stat results to tex tables\n[H,P,CI,STATS] = ttest(repmat(CC_IND_RET_EW,[1,8]),[CC_IND_RET_MAXRET, CC_IND_RET_MAXSHARPE, CC_IND_RET_MINVAR_CVAR, CC_IND_RET_MD, CC_IND_RET_ALL_COMB_NAIVE,CC_IND_RET_ALL_COMB_CEQ]);\nSTR_NAME = { 'RR - Max Ret', ...\n'MV -S','MinVar','ERC',...\n'MinCVaR', 'MD', 'COMB NA\\\"IVE', 'COMB'}\ninput2.data = [STATS.tstat;P]';\ninput2.tableColLabels = {'t-stat', 'P-Value'};\ninput2.tableRowLabels = STR_NAME;\ninput2.dataFormat = {'%.2f'};\n", "output_sequence": "Implements mean-variance spanning tests from Kan, Zhou (2012), tests from (Wolf, Ledoit 2008) to check significance of difference for CEQ and Sharpe ratios for 9 portfolio strategies with CC"}, {"input_sequence": "%\n% This Matlab program computes the three spanning tests\n% Wald, LR, and LM test statistics as well as their\n% exact p-values.\n% Input:\n% R1: TxK matrix of returns on benchmark assets\n% Output:\n% W: Wald test statistic\n% LR: Likelihood ratio statistic\n% LM: Lagrange Multiplier\n% pw: p-value of Wald test\n% plr: p-value of likelihood ratio test\n% plm: p-value of Lagrange multiplier test\n%\nfunction [W,LR,LM,pw,plr,plm] = span(R1,R2)\n[T,K] = size(R1);\nN = size(R2,2);\nmu1 = mean(R1)';\nV11i = inv(cov(R1,1));\na1 = mu1'*V11i*mu1;\nc1 = sum(sum(V11i));\nG = [1+a1 b1; c1];\n%\n% Compute \\hat\\alpha and \\hat\\delta\nA = [1 zeros(1,K); 0 -ones(1,K)];\nX = [ones(T,1) R1];\nB = X\\R2;\nTheta = A*B-C;\ne = R2-X*B;\nSigma = cov(e,1);\nH = Theta*inv(Sigma)*Theta';\nlam = eig(H*inv(G));\n% Compute the three test statistics\nUi = prod(1+lam);\nW = T*sum(lam);\nLR = T*log(Ui);\nLM = T*sum(lam./(1+lam));\n% Compute the p-values\nif nargout>3\nif N==1\npw = 1-fcdf((T-K-1)*W/(2*T),2,T-K-1);\nplr = pw;\nelse\npw = 1-wald(W/T,N,T-K-N+1);\nFtest = (T-K-N)*(sqrt(Ui)-1)/N;\nplr = 1-fcdf(Ftest,2*N,2*(T-K-N));\nplm = 1-pillai2(LM/T,N,T-K-N+1);\nend\n\n", "output_sequence": "Implements mean-variance spanning tests from Kan, Zhou (2012), tests from (Wolf, Ledoit 2008) to check significance of difference for CEQ and Sharpe ratios for 9 portfolio strategies with CC"}, {"input_sequence": "%\n% This Matlab program computes the step-down spanning test.\n% Input:\n% R1: TxK matrix of returns on benchmark assets\n% Output:\n% Ftest: Joint test of alpha=0_N, delta=0_N\n% Ftest2: Test of delta=0_N conditional on alpha=0_N\n% pval: p-value of Ftest\n% alpha: sample estimate of alpha\n%\nfunction [Ftest,Ftest1,Ftest2,pval,pval1,pval2,alpha,delta] = stepdown(R1,R2)\n[T,K] = size(R1);\nN = size(R2,2);\nmu1 = mean(R1)';\nV11i = inv(cov(R1,1));\na1 = mu1'*V11i*mu1;\nc1 = sum(sum(V11i));\nd1 = a1*c1-b1^2;\nG = [1+a1 b1; c1];\nR = [R1 R2];\nmu = mean(R)';\nVi = inv(cov(R,1));\na = mu'*Vi*mu;\nc = sum(sum(Vi));\nd = a*c-b^2;\n%\n% Compute \\hat\\alpha and \\hat\\delta\nA = [1 zeros(1,K); 0 -ones(1,K)];\nX = [ones(T,1) R1];\nB = X\\R2;\nTheta = A*B-C;\ne = R2-X*B;\nSigma = cov(e,1);\nH = Theta*inv(Sigma)*Theta';\nlam = eig(H*inv(G));\n% Compute the three test statistics\nUi = prod(1+lam);\nif N==1\nFtest = (T-K-1)*(Ui-1)/2;\nelse\nFtest = (T-K-N)*(sqrt(Ui)-1)/N;\nend\nFtest1 = (T-K-N)/N*(a-a1)/(1+a1);\nFtest2 = (T-K-N+1)/N*((c+d)*(1+a1)/((c1+d1)*(1+a))-1);\n% Compute the p-values\nif nargout>3\nif N==1\npval = 1-fcdf(Ftest,2,T-K-1);\nelse\npval = 1-fcdf(Ftest,2*N,2*(T-K-1));\nend\npval1 = 1-fcdf(Ftest1,N,T-K-N);\nend\nif nargout>6\nalpha = Theta(1,:)';\n", "output_sequence": "Implements mean-variance spanning tests from Kan, Zhou (2012), tests from (Wolf, Ledoit 2008) to check significance of difference for CEQ and Sharpe ratios for 9 portfolio strategies with CC"}, {"input_sequence": "%-------------------------------------------------------------------------------\n% Book: Tail Event Driven Asset Allocation\n%-------------------------------------------------------------------------------\n% Quantlet: latexTable\n% Description: latexTable is a function that generates a LaTeX table from a given\n% MATLAB input containing numeric values. The LaTeX code is printed in the\n% command window for quick copy&paste and given back as a cell array. Features:\n% data formats, pivoting, column/row headers.\n% Usage: none\n% Inputs: input.data - data for latex table\n% optional fields\n% Output: latex cell array containing LaTex code\n% Keywords: latex table\n% Author: Eli Duenisch, 26/07/2014\n% See also:\n% Datafile: none\n%\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n%%% Copyright (c) 2015, Eli Duenisch\n%%% All rights reserved.\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n% Redistribution and use in source and binary forms, with or without\n% modification, are permitted provided that the following conditions are\n% met:\n% * Redistributions of source code must retain the above copyright\n% notice, this list of conditions and the following disclaimer.\n% * Redistributions in binary form must reproduce the above copyright\n% notice, this list of conditions and the following disclaimer in\n% the documentation and/or other materials provided with the distribution\n% THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n% AND EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n% IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n% ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE\n% LIABLE FOR ANY DIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n% CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n% SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n% INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n% CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n% ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n% POSSIBILITY OF SUCH DAMAGE.\nfunction latex = latexTable(input)\n%\n% Example and explanation of the input struct fields:\n% numeric values you want to tabulate:\n% this field has to be a matrix or MATLAB table datatype\n% missing values have to be NaN\n% in this example we use an array\n% input.data = [1.12345 ...\n% 7.12345 NaN 9.12345; ...\n% 10.12345\n% Optional fields (if not set default values will be used):\n% Set column labels (use empty string for no label):\n% input.tableColLabels = {'col1','col2','col3'};\n% Set row labels (use empty string for no label):\n% input.tableRowLabels = {'row1','row2','','row4'};\n% Switch transposing/pivoting your table:\n% input.transposeTable = 0;\n% Determine whether input.dataFormat is applied column or row based:\n% input.dataFormatMode = 'column'; % use 'column' or 'row'. if not set 'colum' is used\n% Formatting-string to set the precision of the table values:\n% For using different formats in different rows use a cell array like\n% {myFormatString1,numberOfValues1,myFormatString2,numberOfValues2, ... }\n% where myFormatString_ are formatting-strings and numberOfValues_ are the\n% number of table columns or rows that the preceding formatting-string applies.\n% Please make sure the sum of numberOfValues_ matches the number of columns or\n% rows in input.tableData!\n% input.dataFormat = {'%.3f'}; % uses three digit precision floating point for all data values\n% input.dataFormat = {'%.3f',2,'%.1f',1}; % three digits precision for first two columns, one digit for the last\n% Define how NaN values in input.tableData should be printed in the LaTex table:\n% input.dataNanString = '-';\n% Column alignment in Latex table ('l'=left-justified, 'c'=centered,'r'=right-justified):\n% input.tableColumnAlignment = 'c';\n% Switch table borders on/off:\n% input.tableBorders = 1;\n% LaTex table caption:\n% input.tableCaption = 'MyTableCaption';\n% LaTex table label:\n% input.tableLabel = 'MyTableLabel';\n% Switch to generate a complete LaTex document or just a table:\n% input.makeCompleteLatexDocument = 1;\n% Now call the function to generate LaTex code:\n% latex = latexTable(input);\n%%%%%%%%%%%%%%%%%%%%%%%%%% Default settings %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n% These settings are used if the corresponding optional inputs are not given.\n% Pivoting of the input data swithced off per default:\nif ~isfield(input,'transposeTable'),\ninput.transposeTable = 0;\nend\n% Default mode for applying input.tableDataFormat:\nif ~isfield(input,'dataFormatMode'),\ninput.dataFormatMode = 'column';\n% Sets the default display format of numeric values in the LaTeX table to '%.4f'\n% (4 digits floating point precision).\nif ~isfield(input,'dataFormat'),\ninput.dataFormat = {'%.4f'};\n% Define what should happen with NaN values in input.tableData:\nif ~isfield(input,'nanString'),\ninput.dataNanString = '-';\n% Specify the alignment of the columns:\n% 'l' for left-justified, 'c' for centered, 'r' for right-justified\nif ~isfield(input,'tableColumnAlignment'),\ninput.tableColumnAlignment = 'c';\n% Specify whether the table has borders:\n% 0 for no borders, 1 for borders\nif ~isfield(input,'tableBorders'),\ninput.tableBorders = 1;\n% Other optional fields:\nif ~isfield(input,'tableCaption'),\ninput.tableCaption = 'MyTableCaption';\nif ~isfield(input,'tableLabel'),\ninput.tableLabel = 'MyTableLabel';\nif ~isfield(input,'makeCompleteLatexDocument'),\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n% process table datatype\nif isa(input.data,'table')\ninput.tableColLabels = input.data.Properties.RowNames';\ninput.data = table2array(input.data)';\n% get size of data\nnumberDataRows = size(input.data,1);\n% obtain cell array for the table data and labels\ncolLabelsExist = isfield(input,'tableColLabels');\ncellSize = [numberDataRows+colLabelsExist,numberDataCols+rowLabelsExist];\nC = cell(cellSize);\nC(1+colLabelsExist:end,1+rowLabelsExist:end) = num2cell(input.data);\nif rowLabelsExist\nC(1+colLabelsExist:end,1)=input.tableRowLabels';\nif colLabelsExist\nC(1,1+rowLabelsExist:end)=input.tableColLabels;\n% obtain cell array for the format\nlengthDataFormat = length(input.dataFormat);\nif lengthDataFormat==1\ntmp = repmat(input.dataFormat(1),numberDataRows,numberDataCols);\nelse\ndataFormatList={};\nfor i=1:2:lengthDataFormat\ndataFormatList(end+1:end+input.dataFormat{i+1},1) = repmat(input.dataFormat(i),input.dataFormat{i+1},1);\nend\nif strcmp(input.dataFormatMode,'column')\ntmp = repmat(dataFormatList',numberDataRows,1);\nif strcmp(input.dataFormatMode,'row')\ntmp = repmat(dataFormatList,1,numberDataCols);\nif ~isequal(size(tmp),size(input.data))\nerror(['Please check your values in input.dataFormat:'...\n'The sum of the numbers of fields must match the number of columns OR rows '...\n'(depending on input.dataFormatMode)!']);\ndataFormatArray = cell(cellSize);\ndataFormatArray(1+colLabelsExist:end,1+rowLabelsExist:end) = tmp;\n% transpose table (if this switched on)\nif input.transposeTable\nC = C';\ndataFormatArray = dataFormatArray';\n% make table header lines:\nhLine = '\\hline';\nif input.tableBorders\nheader = ['\\begin{tabular}{|',repmat([input.tableColumnAlignment,'|'],1,size(C,2)),'}'];\nlatex = {'\\begin{table}[H!]';'\\centering';header};\n% generate table\nfor i=1:size(C,1)\nif input.tableBorders\nlatex(end+1) = {hLine};\nrowStr = '';\nfor j=1:size(C,2)\ndataValue = C{i,j};\nif isnan(dataValue)\ndataValue = input.dataNanString;\nelseif isnumeric(dataValue)\ndataValue = num2str(dataValue,dataFormatArray{i,j});\nend\nif j==1\nrowStr = dataValue;\nelse\nrowStr = [rowStr,' & ',dataValue];\nlatex(end+1) = {[rowStr,' \\\\']};\n% make table footer lines:\nfooter = {'\\end{tabular}';['\\caption{',input.tableCaption,'}']; ...\n['\\label{table:',input.tableLabel,'}'];'\\end{table}'};\nlatex = [latex;{hLine};footer];\n% add code if a complete latex document should be created:\nif input.makeCompleteLatexDocument\nlatexHeader = {'\\documentclass[a4paper,10pt]{article}';'\\begin{document}'};\nlatexFooter = {'\\end{document}'};\nlatex = [latexHeader;latex;latexFooter];\n% print latex code to console:\ndisp(char(latex));\n", "output_sequence": "Implements mean-variance spanning tests from Kan, Zhou (2012), tests from (Wolf, Ledoit 2008) to check significance of difference for CEQ and Sharpe ratios for 9 portfolio strategies with CC"}, {"input_sequence": "# Clear all variables\nrm(list = ls(all = TRUE))\ngraphics.off()\n# Install and load packages\nlibraries = c(\"lsa\", \"ggplot2\", \"plyr\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n(obj.names = load(\"TDM_Q3D3LSA.RData\", .GlobalEnv))\n# LSA space creation\nsystem.time( LSA_space <- lsa(m_a) )\nsummary(LSA_space)\n# Tests and experiments with SVD\nU = LSA_space$tk\ndim(m_a)\nlength(sv)\n# Check that the rows of the right matrix of SVD (rows of t(V) = Docs Coefficients to a PC) are normalized to 1\nrowSums(t(V)^2)\n# Check that columnns of U are normalized to 1\ncolSums(U^2)\n#------------------------------------------------\n# Plot the highest singular values having a total sum of 50%: a default parametrization given by LSA_space <- lsa(m_a)\n# Effect of the singular values i.e. their weights\nx_axis = 1:length(LSA_space$sk)\n#qp <- qplot(x_axis, LSA_space$sk, xlab = \"\", ylab = \"singular values\")\nqp <- qplot(x_axis, LSA_space$sk, xlab = \"The highest singular values having a total sum of 50%\", ylab = \"singular values\")\nqp + theme(axis.text = element_text(size = 17), axis.title = element_text(size = 20))\n# TDK - Histogram of the matrix values in TDM (basis VSM)\n# convert the TDM to a vector\nm_v = as.vector(m_a)\nsummary(m_v)\n# Sparsity of TDM (basis VSM)\nlength(m_v[m_v > 0])\nlength(m_v)\nlength(m_v[m_v > 0]) / length(m_v)\n# collect all data in a data frame for ggplot representation\ndatTDM <- data.frame(\nMatrices = factor(rep(c(\"tdm\"), each = length(m_v))),\nValues = c(m_v)\n)\ncdatTDM <- ddply(datTDM, \"Matrices\", summarise, Values.mean=mean(Values),\ncdatTDM\n# ggplot\ndev.new(width = 16, height = 13)\nggplot(datTDM, aes(x = Values, fill = Matrices)) + coord_cartesian(xlim = c(0,1), ylim = c(0,20)) +\ngeom_histogram(binwidth = .0001, alpha = .9, position = \"identity\", colour = \"red\") +\ngeom_vline(data = cdatTDM, aes(xintercept = Values.mean), linetype = \"dashed\", size = 2, colour = \"darkred\") +\ntheme(axis.text = element_text(size = 35), axis.title = element_text(size = 40)) +\ntheme(legend.title = element_text(size = 20, face = \"bold\"), legend.text = element_text(size=20))\n", "output_sequence": "Plots the highest singular values as provided by the LSA process for the given term document matrix TDM of the Quantlets. Additionally, the histogram of the matrix values in the TDM is plotted via ggplot2 package. The high sparsity of the TDM becomes evident. The mean value is indicated via a dashed vertical line."}, {"input_sequence": "# Clear all variables\nrm(list = ls(all = TRUE))\ngraphics.off()\n# Install and load packages\nlibraries = c(\"lsa\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n(obj.names = load(\"TDM_Q3D3LSA.RData\", .GlobalEnv))\n# LSA space creation\nsystem.time( LSA_space <- lsa(m_a) )\nsummary(LSA_space)\n# ------------------------------------------------\n# LSA Extended / Interpretation\nLSA.PC.rotation = function(LSA_space, auto_rotation = TRUE, n = 10, top_terms = 5, weight_round = 2){\npc_labels = c()\nsv_weights = c()\n\nU = LSA_space$tk\nfor (i in 1:n) {\nPC = U[,i]\n\nif (auto_rotation) {\nif (sum(PC[PC > 0]) + sum(PC[PC < 0]) >= 0) {\nres = PC[PC > 0]\n} else {\nres = -PC[PC < 0]\n}\n} else {\nres = PC[PC > 0]\n}\nres = sort(res, decreasing = TRUE)[1:top_terms] * sv[i]\nres = round(res, weight_round)\npc_labels = c(pc_labels, list(res))\nsv_weights = c(sv_weights, SV_weight)\n}\n#return(pc_labels)\nreturn(list(PC_labels = pc_labels, SV_weights = sv_weights))\n}\nLSA_PC = LSA.PC.rotation(LSA_space)\nLSA_PC = LSA.PC.rotation(LSA_space, auto_rotation = F)\nstr(LSA_PC)\nLSA_PC$PC_labels\nLSA_PC$SV_weights\n", "output_sequence": "The function \"LSA.PC.rotation\" determines the proper sign / rotation of the LSA components (semantic space principal components) and extracts the top words (top_terms) of each component (PC) for the given LSA space which is created from the term document matrix TDM of the Quantlets by means of the SVD process. The (positive and accordingly negative) part is chosen from those terms, where the biggest subtotal is concentrated since the singular vectors are unique up to scalar multiples of modulus one, in the case of real matrices +/- 1. Finally, the top words with the highest weights are taken as \"prototypes\" for the semantic space principal components\u2019 topics (PC topics). That way the function \"LSA.PC.rotation\" allows the determination of possible labels for the LSA PC\u2019s. The function requires as input the LSA space which is provided via the function \"lsa\" (from the R package lsa). The remaining parameters of \"LSA.PC.rotation\" are optional allowing finer control of the output (number of PC\u2019s, terms/topic etc.)"}, {"input_sequence": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Fri Nov 4 14:29:59 2022\n@author: domenicomaffei\nfrom PIL import Image\nfrom numpy.fft import fft,ifft\nimport numpy as np\n# Open the image by using Python Imaging Library(PIL)\nimage_before=Image.open('Laufenburg.png')\n# Decoding and encoding image to float number\nimage_int=np.fromstring(image_before.tobytes(), dtype=np.int8)\n# Processing Fourier transform\nfft_transformed=fft(image_int)\n# Filter the lower frequency, i.e. employ a high pass\nfft_transformed=np.where(np.absolute(fft_transformed) < 9e4,0,fft_transformed)\n# Inverse Fourier transform\nfft_transformed=ifft(fft_transformed)\n# Keep the real part\nfft_transformed=np.int8(np.real(fft_transformed))\n# Output the image\nimage_output=Image.frombytes(image_before.mode, image_before.size, fft_transformed)\nimage_output.show()\n", "output_sequence": "Creating a (retro) filter using Fourier Transformation for a picture of Laufenburg (Baden) old town"}, {"input_sequence": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Fri Nov 4 16:12:10 2022\n@author: wenenlim\nFourier Transformation\nfrom PIL import Image\nfrom numpy.fft import fft,ifft\nimport numpy as np\n# Open the image by using Python Imaging Library(PIL)\nimage_before=Image.open('stgallen_view.jpg')\n# Decoding and encoding image to float number\nimage_int=np.fromstring(image_before.tobytes(), dtype=np.int8)\n# Processing Fourier transform\nfft_transformed=fft(image_int)\n# Filter the lower frequency, i.e. employ a high pass\nfft_transformed=np.where(np.absolute(fft_transformed) < 9e4,0,fft_transformed)\n# Inverse Fourier transform\nfft_transformed=ifft(fft_transformed)\n# Keep the real part\nfft_transformed=np.int8(np.real(fft_transformed))\n# Output the image\nimage_output=Image.frombytes(image_before.mode, image_before.size, fft_transformed)\nimage_output.show()\n", "output_sequence": "Creating a filter using Fourier Transformation for a picture of St Gallen"}, {"input_sequence": "import pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport numpy as np\nfrom dl_portfolio.data import load_data\nfrom dl_portfolio.backtest import plot_perf, backtest_stats, get_target_vol_other_weights\nimport pickle\nimport os, sys\nfrom sklearn.neighbors import KernelDensity\nimport seaborn as sns\nfrom sklearn import metrics as skmetrics\nfrom dl_portfolio.sensitivity import plot_sensitivity\ndef plot_final_backtest_performance(returns, benchmark, save=False, save_path=None, hedge=False):\nplt.figure(figsize=(20,10))\nplt.plot((returns['equal'] + 1).cumprod() - 1,\nlabel=\"equal\",\ncolor=\"royalblue\"\n)\nplt.plot((returns['equal_class'] + 1).cumprod() - 1,\nlabel=\"equal_class\",\ncolor=\"navy\"\nplt.plot((returns['markowitz'] + 1).cumprod() - 1, label=\"markowitz\", color=\"darkslategray\")\n\nplt.plot((returns['hrp'] + 1).cumprod() - 1, label=\"hrp\", color=\"red\")\nif hedge:\nplt.plot((returns['aerp_hedge'] + 1).cumprod() - 1, label=\"aerp-hedge\", color=\"lightcoral\")\nplt.plot((benchmark.loc[returns.index, 'SP500'] * 0.05 / (benchmark.loc[returns.index, 'SP500'].std() * np.sqrt(252)) + 1).cumprod() - 1,\nlabel='SP500',\ncolor='purple')\nplt.plot((benchmark.loc[returns.index, 'Russel2000'] * 0.05 / (benchmark.loc[returns.index, 'Russel2000'].std() * np.sqrt(252)) + 1).cumprod() - 1,\nlabel='Russel2000',\ncolor='fuchsia')\nplt.plot((benchmark.loc[returns.index, 'EuroStoxx50'] * 0.05 / (benchmark.loc[returns.index, 'EuroStoxx50'].std() * np.sqrt(252)) + 1).cumprod() - 1,\nlabel='EuroStoxx50',\ncolor='violet')\nplt.plot(np.cumprod(returns['ae_rp_c'] + 1) - np.cumprod(returns['ae_rp_c'] + 1), linestyle= '--', c=\"lightgrey\")\nif save:\nplt.savefig(save_path, bbox_inches='tight', transparent=True)\nelse:\nplt.legend()\n\ndef plot_target_pred_proba(target, proba, max_xticks, save_path=None, figsize=(20,10)):\nnrow = proba.shape[-1]\nfig, axs = plt.subplots(nrow, 1, figsize=figsize, sharex=True)\nfor i, f in enumerate(proba.columns):\npred = proba[[f]].copy()\ntrue = (target.loc[pred.index, [f]] == 0).astype(int)\naxs[i].imshow(true.T.values,\nextent = [0, len(true)-1, 0, 1],\naspect=\"auto\",\ninterpolation=\"nearest\")\naxs[i].plot(pred.values, color=\"red\")\naxs[i].set_ylim(0, 1)\nxtickslabels = list(true.index)\nxticks = list(range(0, len(true), // max_xticks + 1))\nxtickslabels = np.array(xtickslabels)[xticks].tolist()\nxtickslabels = [l.strftime(\"%Y-%m-%d\") for l in xtickslabels]\naxs[i].set_xticks(xticks)\naxs[i].set_xticklabels(xtickslabels, rotation=45)\nif i == 0:\naxs[i].set_yticks([0, 1])\nelse:\naxs[i].set_yticks([0])\nplt.subplots_adjust(hspace=0)\nif save_path is not None:\ndef classification_metrics(cv_signal: dict, target):\nstats = {}\nfor factor in target.columns:\nrecall = []\nfor cv in cv_signal:\ncv_dates = cv_signal[cv].index\ntrue = (target.loc[cv_dates, factor] == 0).astype(int)\nf1.append(skmetrics.f1_score(true, pred, labels=[0,1], average=\"weighted\"))\ncm = skmetrics.confusion_matrix(true, pred, labels=[0,1])\nstats[factor] = {\"f1\": f1, \"precision\": \"recall\": \"tpr\": tpr}\nreturn stats\ndef load_all_backtest_result(ae_dir, nmf_dir, dataset, markowitz_dir, m=0):\n# Load Market budget\nif dataset == 'dataset1':\nmarket_budget = pd.read_csv(\"data/market_budget_dataset1.csv\", index_col=0)\ncryptos = [\"BTC\", \"DASH\", \"ETH\", \"XRP\"]\nmarket_budget = pd.concat([market_budget, pd.DataFrame(np.array([[\"crypto\", 1]] * len(cryptos)),\nindex=cryptos,\nmarket_budget[\"rc\"] = market_budget[\"rc\"].astype(int)\nelif dataset == \"dataset2\":\nmarket_budget = pd.read_csv('data/market_budget_dataset2.csv', index_col=0)\nmarket_budget['rc'] = market_budget['rc'].astype(int)\nraise NotImplementedError()\n# Load markowitz and robust M results\nGMV_robust_weights = pd.read_csv(f\"{markowitz_dir}/weights_GMV_robust.csv\", index_col = 0)\nGMV_robust_weights.index = pd.to_datetime(GMV_robust_weights.index)\nmarkowitz_weights = pd.read_csv(f\"{markowitz_dir}/weights_MeanVar_long.csv\", index_col = 0)\nmarkowitz_weights.index = pd.to_datetime(markowitz_weights.index)\n# Leverage\nleverage = pd.read_csv(f'{ae_dir}/leverage.csv', index_col = 0)\npyrobustm, l = get_target_vol_other_weights(f\"GMV_robust_{dataset}\")\nleverage[\"GMV_robust\"] = l['other']\npyrobustm.columns = ['GMV_robust']\nmarkowitz, l = get_target_vol_other_weights(f\"MeanVar_{dataset}\")\nleverage[\"markowitz\"] = l['other']\nmarkowitz.columns = ['markowitz']\nport_weights = pickle.load(open(f'{ae_dir}/portfolios_weights.p', 'rb'))\nfor k in hedge_port_weights:\nport_weights[k + \"_hedge\"] = hedge_port_weights[k]\nport_weights[\"markowitz\"] = markowitz_weights.loc[port_weights[\"ae_rp_c\"].index]\nnmf_port_weights = pickle.load(open(f'{nmf_dir}/portfolios_weights.p', 'rb'))\nfor k in nmf_port_weights.keys():\nkey = k.replace(\"ae\", \"nmf\")\nport_weights[key] = nmf_port_weights[k]\nann_perf = pd.read_csv(f'{ae_dir}/portfolios_returns.csv', index_col = 0)\nann_perf.index = pd.to_datetime(ann_perf.index)\nann_perf['GMV_robust'] = pyrobustm['GMV_robust']\nann_perf['markowitz'] = markowitz['markowitz']\nhedged_perf = pd.read_csv(f'{ae_dir}/portfolios_returns_hedged_{m}.csv', index_col = 0)\nhedged_perf.index = pd.to_datetime(hedged_perf.index)\nhedged_perf.columns = [c +\"_hedge\" for c in hedged_perf.columns]\nann_perf = pd.concat([ann_perf, hedged_perf], axis = 1)\nassert np.sum(ann_perf.isna().sum()) == 0, print(ann_perf.isna().sum())\nnmf_perf = pd.read_csv(f'{nmf_dir}/portfolios_returns.csv', index_col = 0)\nnmf_perf.index = pd.to_datetime(nmf_perf.index)\nnmf_perf.columns = [c.replace('ae', 'nmf') for c in nmf_perf.columns]\ncommon_cols = [c for c in nmf_perf.columns if \"nmf\" not in c]\nnmf_perf.drop(common_cols, 1, inplace = True)\nann_perf = pd.concat([ann_perf, nmf_perf], 1)\nstats = backtest_stats(ann_perf, port_weights, period=252, market_budget=market_budget, sspw_tto=False)\nleverage_hedge = pd.read_csv(f'{ae_dir}/leverage_hedged_0.csv', index_col = 0)\nleverage_hedge.columns = [c +\"_hedge\" for c in leverage_hedge.columns]\nleverage = pd.concat([leverage, leverage_hedge], 1)\nreturn ann_perf, port_weights, leverage, stats\nSAVE = False\nsavedir = \"paper_result\"\nif SAVE:\nif not os.path.isdir(savedir):\n# Load tail events data\ntrain_activation1 = pd.DataFrame()\nfor cv in range(22):\nt = pd.read_csv(f\"activationProba/data/dataset1/{cv}/train_linear_activation.csv\", index_col = 0)\nt.index = pd.to_datetime(t.index)\nif cv > 0:\nlast_date = train_activation1.index[-1]\nt = t[t.index > last_date]\ntrain_activation1 = pd.concat([train_activation1, t])\nactivation1 = pd.DataFrame()\nt = pd.read_csv(f\"activationProba/data/dataset1/{cv}/test_linear_activation.csv\", index_col = 0)\nactivation1 = pd.concat([activation1, t])\nactivation1.index = pd.to_datetime(activation1.index)\ntarget1 = (activation1 > 0).astype(int)\ntrain_activation2 = pd.DataFrame()\nt = pd.read_csv(f\"activationProba/data/dataset2/{cv}/train_linear_activation.csv\", index_col = 0)\nlast_date = train_activation2.index[-1]\ntrain_activation2 = pd.concat([train_activation2, t])\nactivation2 = pd.DataFrame()\nfor cv in range(177):\nt = pd.read_csv(f\"activationProba/data/dataset2/{cv}/test_linear_activation.csv\", index_col = 0)\nactivation2 = pd.concat([activation2, t])\nactivation2.index = pd.to_datetime(activation2.index)\ntarget2 = (activation2 > 0).astype(int)\nperf_ae_dir_1 = \"./performance/test_final_models/ae/dataset1_20220322_150317\"\nmarkowitz_dir1 = \"final_models/run_7_global_bond_dl_portfolio_20220122_151211\"\nperf_ae_dir_2 = \"./performance/test_final_models/ae/dataset2_20220323_152418\"\nmarkowitz_dir2 = \"final_models/run_6_multiasset_traditional_dl_portfolio_20211206_173539\"\ngarch_dir_1 = \"activationProba/output/dataset1/final\"\nclusters1 = pickle.load(open(f\"{perf_ae_dir_1}/cluster_assignment.p\", \"rb\"))\nstrats = list(signals1.keys())\nclusters2 = pickle.load(open(f\"{perf_ae_dir_2}/cluster_assignment.p\", \"rb\"))\npred_probas1 = pd.read_csv(f\"{garch_dir_1}/activation_probas.csv\",\nindex_col=0)\npred_probas1.index = pd.to_datetime([pd.to_datetime(d).date() for d in pred_probas1.index])\npred_probas2 = pd.read_csv(f\"{garch_dir_2}/activation_probas.csv\",\npred_probas2.index = pd.to_datetime([pd.to_datetime(d).date() for d in pred_probas2.index])\n# Handle R renaming of columns...\npred_probas1.columns = [c.replace(\".\", \"-\") for c in pred_probas1.columns]\npred_probas1 = pred_probas1[activation1.columns] # to be safe\ntest_target1 = target1.loc[pred_probas1.index]\nsave_path=f'{savedir}/activation_map.png'\nelse:\nsave_path = None\nplot_sensitivity((train_activation1 <= 0).astype(int),\nfigsize = (20,2),\nmax_xticks=20,\nsave_path=save_path)\n_ = plt.yticks(range(4), [\"Bond\", \"Stock\", \"Forex\", \"Crypto\"])\nfig, axs = plt.subplots(1, 2, figsize=(15,5))\ntemp = pd.concat([train_activation1, activation1])\nlabels1 = list(temp.columns)\naxs[0].plot((temp - temp.mean())/temp.std(), alpha=0.6)\ntemp = pd.concat([train_activation2, activation2])\nlabels2 = list(temp.columns)\naxs[1].plot((temp - temp.mean())/temp.std(), alpha=0.6)\nplt.subplots_adjust(wspace = .1)\n\nif not SAVE:\naxs[0].legend(labels1)\n\nbenchmark, _ = load_data(dataset='dataset2')\nbenchmark = benchmark.pct_change().dropna()\nbenchmark = benchmark * 0.05 / (benchmark.std() * np.sqrt(252))\nann_perf1, port_weights1, leverage1, stats1 = load_all_backtest_result(perf_ae_dir_1,\nperf_nmf_dir_1,\ndates1 = ann_perf1.index\nann_perf2, port_weights2, leverage2, stats2 = load_all_backtest_result(perf_ae_dir_2,\nperf_nmf_dir_2,\nsave = SAVE\nif save:\nsave_path = f\"{savedir}/performance_all_inc_hedge_dataset1.png\"\nplot_final_backtest_performance(ann_perf1, benchmark, save=save, save_path=save_path, hedge=True)\nORDER = [\"equal\", \"equal_class\", \"SP500\", \"hrp\", \"nmfrp\", \"aerp_hedge\", \"ae_rp_c\",\n\"nmf_rp_c\", \"ae_rp_c_hedge\", \"hcaa\", \"nmfaa\", \"aeaa_hedge\"]\nXTICKLABELS = [\"Equal\", \"Equal\\nclass\", \"SP500\", \"HRP\", \"NMFRP\", \"AERP-H\",\n\"AERC-W\", \"HCAA\", \"NMFAA\", \"AEAA-H\"]\n# stats1.loc[['SP500', 'Russel2000', 'EuroStoxx50'], 'TTO'] = np.nan\npstats = stats1.drop([\"Russel2000\", \"EuroStoxx50\", \"aeerc\", \"nmferc\"])\npstats = pstats.loc[ORDER, :]\nmetrics = [c for c in list(pstats.columns) if c not in ['CR']]\nfig, axs = plt.subplots(len(metrics), 1, figsize = (14,14), sharex = True)\ni = 0\nfor i, c in enumerate(metrics):\nif c == \"Return\":\ncmap = \"bwr\"\nelif c == \"Volatility\":\ncmap=\"bwr\"\nif c in ['Return', 'VaR-5%', 'Volatility', 'MDD', 'CEQ', 'TTO']:\nyticklabel = c + ' (%)'\ntemp = pstats.loc[:, [c]].T\nmin_ = min(temp.loc[c])\nif min_ < 0:\ncenter = 0\ncenter=min(temp.loc[c]) - np.std(temp.loc[c])\nsns.heatmap(temp,\ncmap='bwr',\nvmin=min(temp.loc[c]) - np.std(temp.loc[c]),\nannot=True,\nfmt='.2f' if c != \"minTRL\" else '.1g',\nannot_kws={'color': 'black', 'size': 'x-large', 'ha': 'center'},\nyticklabels = [yticklabel],\ncbar=False,\nax=axs[i])\naxs[i].tick_params(axis='y', labelrotation = 0, labelsize=15)\nplt.subplots_adjust(hspace = .000)\nnrow, ncol = 2,3\nfig, axs = plt.subplots(1, ncol, figsize=(20,5), sharey=True,\nfor i, strat in enumerate([s for s in strats if s != \"aeerc\"]):\naxs[i].plot(np.cumprod(ann_perf1[strat] + 1) - 1)\naxs[i].plot(np.cumprod(ann_perf1[f\"{strat}_hedge\"] + 1) - np.cumprod(ann_perf1[strat] + 1))\naxs[i].plot(ann_perf1[strat] - ann_perf1[strat], linestyle=\"--\", color=\"lightgrey\")\naxs[i].tick_params(axis='x', labelrotation = 45, labelsize=12)\nif not SAVE:\naxs[i].legend([\"original\", \"hedged\", \"excess return\"])\nplt.subplots_adjust(wspace = .05)\nplt.savefig(f'{savedir}/perf_vs_hedge_excess_ret_dataset1.png', bbox_inches='tight', transparent=True)\nplt.show()\nsave_path = f\"{savedir}/performance_all_inc_hedge_dataset2.png\"\nplot_final_backtest_performance(ann_perf2, benchmark, save=save, save_path=save_path, hedge=True)\n# dataset1_stats.loc[['SP500', 'Russel2000', 'EuroStoxx50'], 'TTO'] = np.nan\npstats = stats2.drop([\"Russel2000\", \"EuroStoxx50\", \"aeerc\", \"nmferc\"])\naxs[i].plot(np.cumprod(ann_perf2[strat] + 1) - 1)\naxs[i].plot(np.cumprod(ann_perf2[f\"{strat}_hedge\"] + 1) - np.cumprod(ann_perf2[strat] + 1))\naxs[i].plot(ann_perf2[strat] - ann_perf2[strat], linestyle=\"--\", color=\"lightgrey\")\nplt.savefig(f'{savedir}/perf_vs_hedge_excess_ret_dataset2.png', bbox_inches='tight', transparent=True)\n## Classification metrics dataset 1\nwarnings.filterwarnings(action='ignore')\nclass_metrics1 = {}\nstrat = list(signals1.keys())[0]\nclass_metrics1 = classification_metrics(signals1[strat], target1)\nfactors = list(class_metrics1.keys())\nfig, axs = plt.subplots(1, len(factors), figsize=(10, 4), sharey=True)\nfor i,f in enumerate(factors):\naxs[i].boxplot(class_metrics1[f].values(), showmeans=True)\nif SAVE:\naxs[i].set_xticks([])\naxs[i].tick_params(axis='x', labelrotation = 45, labelsize=12)\nplt.savefig(f'{savedir}/class_metrics_hedged_aerp_w_dataset1.png', bbox_inches='tight',\ntransparent=True)\nstrat = \"ae_rp_c\"\nplt.figure(figsize=(5,5))\nplt.plot([0, 1], [0, 1], color=\"lightgrey\", linestyle=\"--\")\nroc_auc1 = {}\nfactors = list(pred_probas1.keys())\nfor f in factors:\npred = pred_probas1[f].dropna()\ntrue = (target1.loc[pred.index, f] == 0).astype(int)\nfpr, _ = skmetrics.roc_curve(true, pred)\nroc_auc1[f] = skmetrics.auc(fpr, tpr)\nplt.plot(fpr, tpr, label=f)\nplt.xlim([0.0, 1.0])\nplt.savefig(f'{savedir}/roc_curve_aerp_w_dataset1.png', bbox_inches='tight',\nplt.legend()\nprint(\"AUC\")\nroc_auc1 = pd.DataFrame(pd.Series(roc_auc1), columns=[\"dataset1\"])\nprint(roc_auc1)\nplt.figure(figsize=(4,8))\nsns.heatmap(roc_auc1.T,\ncmap='bwr',\ncenter=min(roc_auc1.values.reshape(-1)) - np.std(roc_auc1.values.reshape(-1)),\nannot=True,\nannot_kws={'color': 'black', 'size': 'x-large', 'ha': 'center'},\ncbar=False)\nplt.yticks([])\nplt.tick_params(axis='x', labelrotation = 45, labelsize=12)\nplt.savefig(f'{savedir}/auc_aerp_w_dataset1.png', bbox_inches='tight',\nplt.tick_params(axis='y', labelrotation = 0, labelsize=12)\n## Classification metrics dataset 2\nclass_metrics2 = {}\nstrat = list(signals2.keys())[0]\nclass_metrics2 = classification_metrics(signals2[strat], target2)\nfactors = list(class_metrics2.keys())\naxs[i].boxplot(class_metrics2[f].values(), showmeans=True)\nplt.savefig(f'{savedir}/class_metrics_hedged_aerp_w_dataset2.png', bbox_inches='tight',\nroc_auc2 = {}\nfactors = list(pred_probas2.keys())\npred = pred_probas2[f].dropna()\ntrue = (target2.loc[pred.index, f] == 0).astype(int)\nroc_auc2[f] = skmetrics.auc(fpr, tpr)\nplt.savefig(f'{savedir}/roc_curve_aerp_w_dataset2.png', bbox_inches='tight',\nroc_auc2 = pd.DataFrame(pd.Series(roc_auc2), columns=[\"dataset2\"])\nprint(roc_auc2)\nsns.heatmap(roc_auc2.T,\ncenter=min(roc_auc2.values.reshape(-1))-\nplt.savefig(f'{savedir}/auc_aerp_w_dataset2.png', bbox_inches='tight',\nroc_auc = pd.concat([roc_auc1[\"dataset1\"], roc_auc2[\"dataset2\"]])\nroc_auc = pd.DataFrame(roc_auc, columns=[\"AUC\"]) * 100\npred1 = pd.DataFrame()\nfor cv in signals1[\"aerp\"]:\npred1 = pd.concat([pred1, signals1[\"aerp\"][cv]])\npred1 = pred1[target1.columns]\npred2 = pd.DataFrame()\nfor cv in signals2[\"aerp\"]:\npred2 = pd.concat([pred2, signals2[\"aerp\"][cv]])\npred2 = pred2[target2.columns]\nexceedance1 = pd.DataFrame(index = [\"true\", \"pred\"],\ncolumns = pd.MultiIndex.from_product([[\"dataset1\"], list(test_target1.columns)]))\nexceedance1.loc[\"true\", pd.IndexSlice[\"dataset1\"]] = ((test_target1 == 0).sum()/len(test_target1)).values\nexceedance2 = pd.DataFrame(index = [\"true\", \"pred\"],\ncolumns = pd.MultiIndex.from_product([[\"dataset2\"], list(test_target2.columns)]))\nexceedance2.loc[\"true\", pd.IndexSlice[\"dataset2\"]] = ((test_target2 == 0).sum()/len(test_target2)).values\nexceedance = pd.concat([exceedance1, exceedance2], 1) * 100\nexcess = exceedance.diff().dropna().droplevel(0,1).astype(float)\nexcess = excess.T\nexcess.columns = [\"Excess\\nExceedance\"]\nexceedance = pd.concat([exceedance1.loc[\"true\"], exceedance2.loc[\"true\"]]).droplevel(0,0)\nexceedance = pd.DataFrame(exceedance).astype(float) * 100\nexceedance.columns = [\"Exceedance\"]\nstats = pd.concat([roc_auc, exceedance, excess], 1)\nfig, axs = plt.subplots(len(stats.columns), 1, sharex = True, figsize = (7,2))\nfor i, stat in enumerate(list(stats.columns)):\nprint(stat)\ntemp=stats[[stat]].T\nif stat == \"AUC\":\nfmt = \".0f\"\ncenter=0,\nvmin=min(temp.values.reshape(-1))-\nfmt=fmt,\nax=axs[i])\naxs[i].tick_params(axis='x', labelrotation = 75, labelsize=15)\naxs[i].set_xticklabels([\"Bond\", \"Stocks\", \"Forex\", \"Crypto\", \"US Stocks\",\n\"Other Stocks\", \"Commodities\", \"US Bonds\", \"FR Bonds\"])\n\nsave_path = f'{savedir}/pred_proba_dataset1.png'\nplot_target_pred_proba(target1,\npred_probas1,\nsave_path=save_path,\nsave_path=f'{savedir}/pred_proba_dataset2.png',\nplot_target_pred_proba(target2,\npred_probas2,\nsave_path=f'{savedir}/pred_proba_dataset2.png',\n", "output_sequence": "We provide results for \"Does non-linear factorization of financial returns help build better portfolio?\", Spilak, WK H\u00e4rdle (2022). Please refer to github Wiki for a detailed description on how to use the code!"}, {"input_sequence": "import os\nimport pandas as pd\nfrom dl_portfolio.data import load_data\nfrom dl_portfolio.utils import get_linear_encoder\nfrom dl_portfolio.logger import LOGGER\ndef create_linear_features(base_dir):\nsys.path.append(base_dir)\nimport ae_config as config\n# Load test results\ndata, assets = load_data(dataset=config.dataset)\nos.makedirs(f\"activationProba/data/{config.dataset}\")\ntest_lin_activation = pd.DataFrame()\ncvs = list(config.data_specs.keys())\nfor cv in cvs:\nLOGGER.info(f\"Saving to: 'activationProba/data/{config.dataset}/{cv}'\")\n_, train_act = get_linear_encoder(config, 'train', data, assets, base_dir, cv)\ntrain_act = pd.concat([train_act, val_act])\ntrain_act.to_csv(f\"activationProba/data/{config.dataset}/{cv}/train_linear_activation.csv\")\ntest_lin_activation = pd.concat([test_lin_activation, test_act])\n_, train_lin_activation = get_linear_encoder(config, 'train', data, assets, base_dir, 0)\nlin_act = pd.concat([train_lin_activation, val_lin_activation,\nlin_act.to_csv(f\"activationProba/data/{config.dataset}/linear_activation.csv\")\nif __name__ == \"__main__\":\nimport argparse\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--base_dir\",\ntype=str,\nhelp=\"Directory with ae model logs, ex: 'final_models/ae/dataset1/m_0_dataset1_nbb_resample_bl_60_seed_0_1647953383912806'\")\nargs = parser.parse_args()\nLOGGER.info(f\"Create linear activation for model {args.base_dir}\")\ncreate_linear_features(args.base_dir)\nLOGGER.info(\"Done\")\n", "output_sequence": "We provide results for \"Does non-linear factorization of financial returns help build better portfolio?\", Spilak, WK H\u00e4rdle (2022). Please refer to github Wiki for a detailed description on how to use the code!"}, {"input_sequence": "import os\nimport pandas as pd\nimport pickle, json\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom dl_portfolio.data import load_data\nfrom dl_portfolio.utils import load_result, load_result_wrapper\nimport sys\nimport itertools\nfrom sklearn.neighbors import KernelDensity\nfrom scipy.stats import pearsonr, kendalltau\nimport warnings\ndef plt_features(features, start, end, dataset, savepath=None):\nif dataset == \"dataset1\":\nnrow, ncol = 2,2\nfigsize=(12,10)\nelif dataset == \"dataset2\":\nnrow, ncol = 2,3\nfigsize=(18,10)\n\nfig, axs = plt.subplots(nrow, ncol, figsize=figsize, sharex=True,\nfor i, c in enumerate(features[\"ae\"].columns):\ntemp = features[\"ae\"].loc[start:end]\nif i % ncol == 0:\nelse:\nrow = 0\ncol = i % ncol\nmin_ = np.min(temp[c]) - 1\nX_plot = np.linspace(min_, max_, 1000)[:, np.newaxis]\nkde = KernelDensity(kernel='gaussian', bandwidth=0.4).fit(np.random.normal(size=1000).reshape(-1,1))\nnorm_log_dens = kde.score_samples(X_plot)\naxs[row, col].plot(X_plot[:, 0], np.exp(norm_log_dens), label='normal', color=\"green\")\nkde = KernelDensity(kernel='gaussian', bandwidth=0.4).fit(temp[c].values.reshape(-1,1))\nlog_dens = kde.score_samples(X_plot)\n_ = axs[row, col].hist(temp[c], bins = 50, density=True, alpha=0.25,\nhisttype='stepfilled', edgecolor='none', color=\"blue\")\naxs[row, col].plot(X_plot[:, 0], np.exp(log_dens), label='ae', color=\"blue\")\nif savepath is None:\naxs[row, col].legend()\ntemp = features[\"nmf\"].loc[start:end]\n_ = axs[row, col].hist(temp[c], bins = 50, density=True, alpha=0.25,\nhisttype='stepfilled', edgecolor='none', color=\"purple\")\naxs[row, col].plot(X_plot[:, 0], np.exp(log_dens), label='nmf', color=\"purple\")\naxs[row, col].set_xlim(-10,10)\nif dataset == \"dataset2\":\naxs[1,2].remove()\nplt.subplots_adjust(hspace = .1)\nif savepath:\nplt.savefig(savepath, bbox_inches='tight', transparent=True)\ndef plot_factor_corr(components, c1, assets, t =0.05, components2 = None, show=False, unit_circle=False,\ntitle=None, savepath=None, ax = None):\n\nif ax is None:\n(fig, ax) = plt.subplots(figsize=(20, 20))\nax.plot([-1,1], [0, 0], '--', c='black')\nfor i in range(0, len(assets)):\nif components[c1, i] > t or components[c2, i] > t:\nax.arrow(0,\n0, # Start the arrow at the origin\ncomponents[c1, i], #0 for PC1\n# width = 0.01, # float, default: 0.001\nhead_width=0.025,\ncolor='b')\nax.text(components[c1, i] + 0.02,\nassets[i],\n\nif components2 is not None:\nfor i in range(0, len(assets)):\nif components2[c1, i] > t or components2[c2, i] > t:\nax.arrow(0,\n0, # Start the arrow at the origin,\ncomponents2[c1, i], #0 for PC1\n# width = 0.01, # float, default: 0.001\nhead_width=0.025,\nax.text(components2[c1, i] + 0.02,\nassets[i],\nif unit_circle:\nan = np.linspace(0, 2 * np.pi, 100)\nax.plot(np.cos(an), np.sin(an)) # Add a unit circle for scale\nax.axis('equal')\nelse:\nax.xlim([-0.05, 1])\nif title:\nax.set_title(title)\nax.set_title('Variable factor map')\nax.set_xlabel(f'dim {c1}')\nax.set(adjustable='box', aspect='equal')\nplt.savefig(savepath, bbox_inches='tight', transparent=False)\nif show:\nplt.show()\ndef get_corr_factor_asset(test_data, features, corr=\"pearson\"):\nif corr == \"pearson\":\ncorr_measure = pearsonr\nelif corr == \"spearman\":\ncorr_measure = spearmanr\nelif corr == \"kendall\":\ncorr_measure = kendalltau\nraise NotImplementedError()\ncorr_asset_factor = pd.DataFrame(index = test_data.columns, columns = features.columns)\nfor c in features.columns:\ncorr_asset_factor.loc[a, c], p_value = corr_measure(test_data[a], features[c])\nif p_value >= 0.05:\ncorr_asset_factor.loc[a, c] = 0\nreturn corr_asset_factor\ndef intrepretation_plot(test_data, features, dataset, labels, corr=\"pearson\", savepath=None):\npairs = list(itertools.combinations(labels, 2))\nind_pairs = list(itertools.combinations(list(range(len(labels))), 2))\ncorr_ae = get_corr_factor_asset(test_data, features[\"ae\"], corr=corr)\nfigsize = (15,10)\nnrow=2\nh_space=0.1\nncol=5\nfigsize = (20,10)\nh_space=0.\nfig, axs = plt.subplots(nrow, ncol, figsize = figsize, sharex=True,\ncol = 0\nfor i, dims in enumerate(ind_pairs):\nrow +=1\nif i >= ncol:\ncol = i - ncol\nelse:\ncol = i\ndim1=dims[0]\nname1=pairs[i][0]\nplot_factor_corr(corr_ae.values.T, dim1, assets, t=0, components2=corr_nmf.T.values,\ntitle = f\"{name2} vs {name1}\",\nunit_circle=True, savepath=None, ax = axs[row,col])\nfig.subplots_adjust(hspace=h_space, wspace=w_space)\nif savepath is not None:\nSAVE = True\nsavedir = \"paper_result\"\nif SAVE:\nif not os.path.isdir(savedir):\ntest_set = 'test'\nif DATASET == 'dataset1':\nLABELS = [\"Bond\", \"Stock\", \"Forex\", \"Crypto\"]\nae_base_dir = \"final_models/ae/dataset1/m_0_dataset1_nbb_resample_bl_60_seed_0_1647953383912806\"\nnmf_base_dir = \"final_models/nmf/dataset1/m_0_seed_7_20220322_122627\"\nperf_ae_dir = \"./performance/test_final_models/ae/dataset1_20220322_150317\"\nelif DATASET == 'dataset2':\nLABELS = [\"US Stocks\", \"Other Stocks\", \"Commodities\", \"US Bond\", \"FR Bond\"]\nae_base_dir = \"./final_models/ae/dataset2/m_4_dataset2_nbb_resample_bl_60_seed_0_1647957514122232\"\nnmf_base_dir = \"./final_models/nmf/dataset2/m_0_seed_4_20220322_123053\"\nperf_ae_dir = \"./performance/test_final_models/ae/dataset2_20220323_152418\"\nelse:\nraise NotImplementedError()\n# Load evaluation\nae_test_eval = json.load(open(f'{perf_ae_dir}/evaluation.json', 'r'))\nae_port_weight = pickle.load(open(f\"{perf_ae_dir}/portfolios_weights.p\", \"rb\"))\n# Load results\nsys.path.append(ae_base_dir)\nimport ae_config\nsys.path.append(nmf_base_dir)\nimport nmf_config\ntest_data = pd.DataFrame()\nprediction = {\"ae\": pd.DataFrame(), \"nmf\": pd.DataFrame()}\nembedding = {}\ndata, assets = load_data(dataset=ae_config.dataset)\nwarnings.filterwarnings(action='ignore')\ntest_data, prediction[\"ae\"], features[\"ae\"], residuals[\"ae\"], embedding[\"ae\"], relu_activation= load_result_wrapper(ae_config,\ntest_set,\n_, prediction[\"nmf\"], features[\"nmf\"], embedding[\"nmf\"], _ = load_result_wrapper(nmf_config,\n# Factors\nfig, axs = plt.subplots(2, 1, figsize=(15,1.5), sharex=True)\nr2 = pd.DataFrame(ae_test_eval['model']['r2'].values(), index = ae_test_eval['model']['r2'].keys()).T\nif DATASET == \"dataset1\":\nvmin = np.min(r2.values) - np.std(r2.values)\ncenter = 0\nvmin=np.min(r2.values)\ncenter=np.min(r2.values) - np.std(r2.values)\nsns.heatmap(r2,\nvmin=vmin,\ncenter=center,\nvmax=1,\nannot=True,\nyticklabels=False,\nfmt='.2f',\nannot_kws={'color': 'black', 'size': 'large', 'ha': 'center'},\ncmap='bwr',\nax=axs[0]) # cmap='Reds')\nr2 = pd.DataFrame(nmf_test_eval['model']['r2'].values(), index = nmf_test_eval['model']['r2'].keys()).T\nax=axs[1]) # cmap='Reds')\nplt.subplots_adjust(hspace = .0)\naxs[1].tick_params(axis='x', labelrotation = 45, labelsize=10)\nae_loadings = np.array([decoding[\"ae\"][k].values for k in decoding[\"ae\"].keys()])\nae_avg_loadings = pd.DataFrame(ae_loadings.mean(0), index=assets)\nw = decoding[\"nmf\"][0].copy()\nw = w /np.sqrt(np.sum(w**2))\n(w**2).sum()\nnmf_loadings = np.array([decoding[\"nmf\"][k].values for k in decoding[\"nmf\"].keys()])\nnmf_avg_loadings = pd.DataFrame(nmf_loadings.mean(0), index=assets)\nfigsize=(10,4)\nelif DATASET == \"dataset2\":\nfigsize=(10,6)\nfig, axs = plt.subplots(2, 1, figsize=figsize, sharex=True,\ncbar_ax = fig.add_axes([.91, .3, .4])\nsns.heatmap(ae_avg_loadings.T, vmin=0, vmax=1, ax=axs[0], cbar=False)\nplt.subplots_adjust(hspace = 0.05)\nsavepath = f'{savedir}/corr_asset_factor_{DATASET}.png'\nsavepath = None\nintrepretation_plot(test_data,\nfeatures,\nsavepath=savepath,\nsavepath=f'{savedir}/factor_distribution_{DATASET}.png'\nstart = features[\"ae\"].index[0]\nplt_features(features, start, end, DATASET, savepath=savepath)\n", "output_sequence": "We provide results for \"Does non-linear factorization of financial returns help build better portfolio?\", Spilak, WK H\u00e4rdle (2022). Please refer to github Wiki for a detailed description on how to use the code!"}, {"input_sequence": "import pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport numpy as np\nfrom dl_portfolio.data import load_data\nfrom dl_portfolio.backtest import plot_perf, backtest_stats, get_target_vol_other_weights\nimport pickle\nimport os, sys\nfrom sklearn.neighbors import KernelDensity\nimport seaborn as sns\ndef plot_weights(weights, savepath=None, x_step=2):\nstrat_to_plot = ['hrp', 'aerp', 'ae_rp_c', 'aeaa', 'nmfrp',\nnrow, ncol = 2,4\nfig, axs = plt.subplots(nrow, ncol, figsize=(18, 5), sharex=True,\nLABELS = [str(d.date()) for d in weights['hrp'].index]\nNUM_COLORS = len(weights[strat_to_plot[0]].columns)\ncm = plt.get_cmap('gist_rainbow')\nrow = -1\nfor i, strat in enumerate(strat_to_plot):\nprint(strat)\ncol = i % ncol\nif col == 0:\nrow = row + 1\nprint(row, col)\naxs[row, col].set_prop_cycle(color=[cm(1. * i / NUM_COLORS) for i in range(NUM_COLORS)])\n\"\"\"if strat == 'equal_class':\ndata = pd.DataFrame(np.repeat(weights['equal_class'].values, len(LABELS), axis=0),\ncolumns = weights['hrp'].columns,\nelse:\ndata = weights[strat]\"\"\"\nfor j, c in enumerate(list(weights[strat].columns)):\naxs[row, col].bar(LABELS, weights[strat][c], label=c, width=1, align='edge',\nbottom=weights[strat].iloc[:, :j].sum(1))\naxs[row, col].tick_params(axis='x', labelrotation=45)\n_ = axs[row, col].set_xticks(list(range(0, len(weights[strat].index), x_step)))\nplt.subplots_adjust(hspace = .1)\nif save:\nplt.savefig(savepath, bbox_inches='tight', transparent=True)\ndef plot_final_backtest_performance(returns, benchmark, save=False, save_path=None, hedge=False):\nplt.figure(figsize=(20,10))\nplt.plot((returns['equal'] + 1).cumprod() - 1,\nlabel=\"equal\",\ncolor=\"royalblue\"\n)\nplt.plot((returns['equal_class'] + 1).cumprod() - 1,\nlabel=\"equal_class\",\ncolor=\"navy\"\nplt.plot((returns['markowitz'] + 1).cumprod() - 1, label=\"markowitz\", color=\"darkslategray\")\n\nplt.plot((returns['hrp'] + 1).cumprod() - 1, label=\"hrp\", color=\"red\")\nif hedge:\nplt.plot((returns['aerp_hedge'] + 1).cumprod() - 1, label=\"aerp-hedge\", color=\"lightcoral\")\nplt.plot((benchmark.loc[returns.index, 'SP500'] * 0.05 / (benchmark.loc[returns.index, 'SP500'].std() * np.sqrt(252)) + 1).cumprod() - 1,\nlabel='SP500',\ncolor='purple')\nplt.plot((benchmark.loc[returns.index, 'Russel2000'] * 0.05 / (benchmark.loc[returns.index, 'Russel2000'].std() * np.sqrt(252)) + 1).cumprod() - 1,\nlabel='Russel2000',\ncolor='fuchsia')\nplt.plot((benchmark.loc[returns.index, 'EuroStoxx50'] * 0.05 / (benchmark.loc[returns.index, 'EuroStoxx50'].std() * np.sqrt(252)) + 1).cumprod() - 1,\nlabel='EuroStoxx50',\ncolor='violet')\nplt.plot(np.cumprod(returns['ae_rp_c'] + 1) - np.cumprod(returns['ae_rp_c'] + 1), linestyle= '--', c=\"lightgrey\")\nplt.savefig(save_path, bbox_inches='tight', transparent=True)\nelse:\nplt.legend()\n\ndef load_all_backtest_result(ae_dir, nmf_dir, markowitz_dir, dataset):\n# Load Market budget\nif dataset == 'dataset1':\nmarket_budget = pd.read_csv('data/market_budget_dataset1.csv', index_col=0)\ncryptos = ['BTC', 'DASH', 'ETH', 'XRP']\nmarket_budget = pd.concat([market_budget, pd.DataFrame(np.array([['crypto', 1]] * len(cryptos)),\nindex=cryptos,\n# market_budget = market_budget.drop('CRIX')\nmarket_budget['rc'] = market_budget['rc'].astype(int)\nelif dataset == 'dataset2':\nmarket_budget = pd.read_csv('data/market_budget_dataset2.csv', index_col=0)\nraise NotImplementedError()\n# Load markowitz and robust M results\nGMV_robust_weights = pd.read_csv(f\"{markowitz_dir}/weights_GMV_robust.csv\", index_col = 0)\nGMV_robust_weights.index = pd.to_datetime(GMV_robust_weights.index)\nmarkowitz_weights = pd.read_csv(f\"{markowitz_dir}/weights_MeanVar_long.csv\", index_col = 0)\nmarkowitz_weights.index = pd.to_datetime(markowitz_weights.index)\n# Leverage\nleverage = pd.read_csv(f'{ae_dir}/leverage.csv', index_col = 0)\npyrobustm, l = get_target_vol_other_weights(f\"GMV_robust_{dataset}\")\nleverage[\"GMV_robust\"] = l['other']\npyrobustm.columns = ['GMV_robust']\nmarkowitz, l = get_target_vol_other_weights(f\"MeanVar_{dataset}\")\nleverage[\"markowitz\"] = l['other']\nmarkowitz.columns = ['markowitz']\nport_weights = pickle.load(open(f'{ae_dir}/portfolios_weights.p', 'rb'))\nfor k in nmf_port_weights.keys():\nkey = k.replace(\"ae\", \"nmf\")\nport_weights[key] = nmf_port_weights[k]\nport_weights[\"markowitz\"] = markowitz_weights.loc[port_weights[\"ae_rp_c\"].index]\nann_perf = pd.read_csv(f'{ae_dir}/portfolios_returns.csv', index_col = 0)\nann_perf.index = pd.to_datetime(ann_perf.index)\nann_perf['GMV_robust'] = pyrobustm['GMV_robust']\nann_perf['markowitz'] = markowitz['markowitz']\nnmf_perf = pd.read_csv(f'{nmf_dir}/portfolios_returns.csv', index_col = 0)\nnmf_perf.index = pd.to_datetime(nmf_perf.index)\nnmf_perf.columns = [c.replace('ae', 'nmf') for c in nmf_perf.columns]\ncommon_cols = [c for c in nmf_perf.columns if \"nmf\" not in c]\nnmf_perf.drop(common_cols, 1, inplace = True)\nann_perf = pd.concat([ann_perf, nmf_perf], 1)\nstats = backtest_stats(ann_perf, port_weights, period=252, market_budget=market_budget)\nSAVE=False\nsave=SAVE\nsavedir = \"paper_result\"\nif save:\nif not os.path.isdir(savedir):\n\nbenchmark, _ = load_data(dataset='dataset2')\nbenchmark = benchmark.pct_change().dropna()\nbenchmark = benchmark * 0.05 / (benchmark.std() * np.sqrt(252))\nperf_ae_dir_1 = \"./performance/test_final_models/ae/dataset1_20220322_150317\"\nmarkowitz_dir1 = \"final_models/run_7_global_bond_dl_portfolio_20220122_151211\"\nperf_ae_dir_2 = \"./performance/test_final_models/ae/dataset2_20220323_152418\"\nmarkowitz_dir2 = \"final_models/run_6_multiasset_traditional_dl_portfolio_20211206_173539\"\nperf1, port_weights1, leverage1, stats1 = load_all_backtest_result(perf_ae_dir_1,\nperf_nmf_dir_1,\ndates1 = perf1.index\nperf2, port_weights2, leverage2, stats2 = load_all_backtest_result(perf_ae_dir_2,\nperf_nmf_dir_2,\nsave = SAVE\nsave_path = f\"{savedir}/performance_all_dataset1.png\"\nplot_final_backtest_performance(perf1, benchmark, save=save, save_path=save_path)\nORDER = [\"equal\", \"equal_class\", \"SP500\", \"hrp\", \"nmfrp\", \"ae_rp_c\",\n\"nmf_rp_c\", \"hcaa\", \"nmfaa\", \"markowitz\", \"GMV_robust\"]\nXTICKLABELS = [\"Equal\", \"Equal\\nclass\", \"SP500\", \"HRP\", \"NMFRP\",\n\"AERC-W\", \"HCAA\", \"NMFAA\",\n\"Markowitz\", \"Robust-M\"]\nstats1.loc[['SP500', 'Russel2000', 'EuroStoxx50'], 'TTO'] = np.nan\npstats = stats1.drop([\"Russel2000\", \"EuroStoxx50\", \"aeerc\", \"nmferc\"])\npstats = pstats.loc[ORDER, :]\nmetrics = [c for c in list(pstats.columns) if c not in ['CR', 'Skewness', 'Excess kurtosis']]\nfig, axs = plt.subplots(len(metrics), 1, figsize = (14,14), sharex = True)\ni = 0\nfor i, c in enumerate(metrics):\nif c == \"Return\":\ncmap = \"bwr\"\nelif c == \"Volatility\":\ncmap=\"bwr\"\nif c in ['Return', 'VaR-5%', 'Volatility', 'MDD', 'CEQ', 'TTO']:\nyticklabel = c + ' (%)'\ntemp = pstats.loc[:, [c]].T\nmin_ = min(temp.loc[c])\nif min_ < 0:\ncenter = 0\ncenter=min(temp.loc[c]) - np.std(temp.loc[c])\nsns.heatmap(temp,\ncmap='bwr',\nvmin=min(temp.loc[c]) - np.std(temp.loc[c]),\nannot=True,\nfmt='.2f' if c != \"minTRL\" else '.1g',\nannot_kws={'color': 'black', 'size': 'x-large', 'ha': 'center'},\nyticklabels = [yticklabel],\ncbar=False,\nax=axs[i])\naxs[i].tick_params(axis='y', labelrotation = 0, labelsize=15)\nplt.subplots_adjust(hspace = .000)\nweights = port_weights1.copy()\nsavepath = f'{savedir}/weights_barplot_dataset1.png'\nelse:\nsavepath = None\n# Dataset 2\nsave_path = f\"{savedir}/performance_all_dataset2.png\"\nplot_final_backtest_performance(perf2, benchmark, save=save, save_path=save_path)\nstats2.loc[['SP500', 'Russel2000', 'EuroStoxx50'], 'TTO'] = np.nan\npstats = stats2.drop([\"Russel2000\", \"EuroStoxx50\", \"aeerc\", \"nmferc\"])\nfig, axs = plt.subplots(len(metrics), 1, figsize = (12,12), sharex = True)\nfmt='.2f' if c != \"minTRL\" else '.0f',\nplt.savefig(f'{savedir}/stats_heatmap_dataset2.png', bbox_inches='tight', transparent=True)\n\nweights = port_weights2.copy()\nsavepath = f'{savedir}/weights_barplot_dataset2.png'\n", "output_sequence": "We provide results for \"Does non-linear factorization of financial returns help build better portfolio?\", Spilak, WK H\u00e4rdle (2022). Please refer to github Wiki for a detailed description on how to use the code!"}, {"input_sequence": "import os\nimport pickle\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom joblib import Parallel, delayed\nfrom dl_portfolio.data import load_data\nfrom dl_portfolio.hedge import hedged_portfolio_weights_wrapper\nfrom dl_portfolio.backtest import cv_portfolio_perf_df\nfrom dl_portfolio.logger import LOGGER\nfrom dl_portfolio.constant import METHODS_MAPPING, AVAILABLE_METHODS\nDATA_BASE_DIR_1 = \"./activationProba/data/dataset1\"\nPERF_DIR_1 = \"./performance/test_final_models/ae/dataset1_20220322_150317\"\nDATA_BASE_DIR_2 = \"./activationProba/data/dataset2\"\nPERF_DIR_2 = \"./performance/test_final_models/ae/dataset2_20220323_152418\"\nif __name__ == \"__main__\":\nimport argparse\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--dataset\",\ndefault=None,\nhelp=\"Dataset name: dataset1 or dataset2\")\nparser.add_argument(\"--method\",\ndefault=\"calibrated_exceedance\",\nhelp=\"Method to compute optimal threshold\")\nparser.add_argument(\"--n_jobs\",\ndefault=os.cpu_count() - 1,\ntype=int,\nhelp=\"Number of parallel jobs\")\nparser.add_argument(\"--save\",\naction='store_true',\nhelp=\"Save results\")\nparser.add_argument(\"--show\",\nhelp=\"Show performance\")\nargs = parser.parse_args()\nassert args.method in AVAILABLE_METHODS, args.method\n# ------------------------------------------------ input ------------------------------------------------\n# dataset1\ndataset = args.dataset\n# -------------------------------------------------------------------------------------------------------\n# Get necessary data\nif dataset == \"dataset1\":\nLOGGER.info(\"Run for dataset1\")\n# Define paths\ndata_base_dir = DATA_BASE_DIR_1\nperf_dir = PERF_DIR_1\n# Load data\ndata, assets = load_data(dataset=\"dataset1\")\nmarket_budget = pd.read_csv(\"data/market_budget_dataset1.csv\", index_col=0)\ncryptos = [\"BTC\", \"DASH\", \"ETH\", \"XRP\"]\nmarket_budget = pd.concat([market_budget, pd.DataFrame(np.array([[\"crypto\", 1]] * len(cryptos)),\nindex=cryptos,\nmarket_budget[\"rc\"] = market_budget[\"rc\"].astype(int)\nelif dataset == \"dataset2\":\nLOGGER.info(\"Run for dataset2\")\ndata_base_dir = DATA_BASE_DIR_2\nperf_dir = PERF_DIR_2\ndata, assets = load_data(dataset=\"dataset2\")\nmarket_budget = pd.read_csv('data/market_budget_dataset2.csv', index_col=0)\nmarket_budget['rc'] = market_budget['rc'].astype(int)\nelse:\nraise NotImplementedError(dataset)\nport_weights = pickle.load(open(f\"{perf_dir}/portfolios_weights.p\", \"rb\"))\nlinear_activation = pd.read_csv(f\"./activationProba/data/{dataset}/linear_activation.csv\", index_col=0)\nlinear_activation.index = pd.to_datetime(linear_activation.index)\ntarget = (linear_activation <= 0).astype(int)\nreturns = data.pct_change(1).dropna()\ncluster_assignment = pickle.load(open(f\"{perf_dir}/cluster_assignment.p\", \"rb\"))\n# Reorder series\nfor cv in cluster_assignment:\ncluster_assignment[cv] = cluster_assignment[cv].loc[assets]\nstrats = [s for s in list(port_weights.keys()) if \"ae\" in s and s != \"aeerc\"]\ncv_folds = list(cluster_assignment.keys())\nLOGGER.info(f\"Method for optimal threshold is: {args.method}\")\nif args.n_jobs > 1:\nLOGGER.info(f\"Compute weights with {args.n_jobs} jobs...\")\nwith Parallel(n_jobs=args.n_jobs) as _parallel_pool:\ncv_results = _parallel_pool(\ndelayed(hedged_portfolio_weights_wrapper)(cv, returns, cluster_assignment[cv], f\"{garch_base_dir}/{cv}\",\nf\"{data_base_dir}/{cv}\", port_weights, strats=strats,\nmethod=args.method)\nfor cv in cv_folds\n)\ncv_results = {cv: res for (cv, res) in cv_results}\nLOGGER.info(f\"n_jobs = 1: compute weights sequentially...\")\ncv_results = {}\nfor cv in cv_folds:\n_, cv_results[cv] = hedged_portfolio_weights_wrapper(cv, returns, cluster_assignment[cv],\nf\"{garch_base_dir}/{cv}\",\nport_weights, strats=strats, method=args.method)\nLOGGER.info(\"Done.\")\n# Now parse cv portfolio weights and train weights\nLOGGER.info(\"Portfolio returns...\")\ncv_portfolio = {\ncv: {\n\"returns\": cv_results[cv][\"returns\"],\n\"port\": cv_results[cv][\"port\"][port] for port in strats\n# if port not in [\"equal\", \"equal_classs\"]\n}\n} for cv in cv_folds\n}\ntrain_weights = {\nstrat: port_weights[strat].iloc[cv] for strat in list(port_weights.keys())\n# Get portfolio returns\nport_perf, leverage = cv_portfolio_perf_df(cv_portfolio, portfolios=strats, train_weights=train_weights)\n# Format final results\nport_returns = pd.DataFrame()\nfor p in strats:\nport_returns[p] = port_perf[p]['total'].iloc[:, 0]\nnew_port_weights = {\nstrat: {cv: cv_results[cv][\"port\"][strat] for cv in cv_results} for strat in strats\nsignals = {\nstrat: {cv: cv_results[cv][\"signal\"][strat] for cv in cv_results} for strat in strats\nif args.save:\nLOGGER.info('Saving results... ')\nport_returns.to_csv(f\"{perf_dir}/portfolios_returns_hedged_{METHODS_MAPPING[args.method]}.csv\")\npickle.dump(signals, open(f\"{perf_dir}/hedging_signals_{METHODS_MAPPING[args.method]}.p\", \"wb\"))\npickle.dump(new_port_weights,\nopen(f\"{perf_dir}/portfolios_weights_hedged_{METHODS_MAPPING[args.method]}.p\", \"wb\"))\nif args.show:\nLOGGER.info('Show performance... ')\nor_port_perf = pd.read_csv(f\"{perf_dir}/portfolios_returns.csv\",\nindex_col=0)\nor_port_perf.index = pd.to_datetime(or_port_perf.index)\nfor strat in strats:\nplt.figure(figsize=(20, 10))\nplt.plot(np.cumsum(or_port_perf[strat]), label=\"or\")\nplt.title(strat)\n", "output_sequence": "We provide results for \"Does non-linear factorization of financial returns help build better portfolio?\", Spilak, WK H\u00e4rdle (2022). Please refer to github Wiki for a detailed description on how to use the code!"}, {"input_sequence": "import datetime as dt\nimport logging\nimport os\nimport pickle\nimport sys\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn import metrics, preprocessing\nfrom dl_portfolio.backtest import bar_plot_weights, backtest_stats, plot_perf, get_ts_weights, get_cv_results, \\\nget_dl_average_weights, cv_portfolio_perf_df\nfrom dl_portfolio.cluster import get_cluster_labels, consensus_matrix, rand_score_permutation, \\\nassign_cluster_from_consmat\nfrom dl_portfolio.evaluate import average_prediction,\nfrom dl_portfolio.logger import LOGGER\nfrom dl_portfolio.constant import BASE_FACTOR_ORDER_DATASET2,\nPORTFOLIOS = ['equal', 'equal_class', 'aerp', 'ae_rp_c', 'aeaa', 'kmaa']\nif __name__ == \"__main__\":\nimport argparse, json\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--base_dir\",\ntype=str,\nhelp=\"Experiments dir\")\nparser.add_argument(\"--model_type\",\ndefault='ae',\nparser.add_argument(\"--test_set\",\ndefault='val',\nhelp=\"val or test\")\nparser.add_argument(\"--n_jobs\",\ndefault=2 * os.cpu_count(),\ntype=int,\nhelp=\"Number of parallel jobs\")\nparser.add_argument(\"--window\",\ndefault=250,\nhelp=\"Window size for portfolio optimisation\")\nparser.add_argument(\"--show\",\naction='store_true',\nhelp=\"Show plots\")\nparser.add_argument(\"--save\",\nhelp=\"Save results\")\nparser.add_argument(\"--legend\",\nhelp=\"Add legend to plots\")\nparser.add_argument(\"-v\",\n\"--verbose\",\naction=\"store_const\",\ndefault=logging.WARNING)\nparser.add_argument('-d',\n'--debug',\nhelp=\"Debugging statements\",\nconst=logging.DEBUG,\nargs = parser.parse_args()\nlogging.basicConfig(level=args.loglevel)\nLOGGER.setLevel(args.loglevel)\nmeta = vars(args)\nif args.save:\nsave_dir = f\"performance/{args.test_set}_{args.base_dir}\" + '_' + dt.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\nif not os.path.isdir(save_dir):\nLOGGER.info(f\"Saving result to {save_dir}\")\n# os.makedirs(f\"{save_dir}/cv_plots/\")\nmeta['save_dir'] = save_dir\njson.dump(meta, open(f\"{save_dir}/meta.json\", \"w\"))\nEVALUATION = {'model': {}, 'cluster': {}}\nLOGGER.info(\"Loading data...\")\n# Load paths\nmodels = os.listdir(args.base_dir)\npaths = [f\"{args.base_dir}/{d}\" for d in models if os.path.isdir(f\"{args.base_dir}/{d}\") and d[0] != \".\"]\nn_folds = os.listdir(paths[0])\nn_folds = sum([d.isdigit() for d in n_folds])\nsys.path.append(paths[0])\nif args.model_type == \"ae\":\nimport ae_config as config\nassert \"ae\" in config.model_type\nelif args.model_type == \"nmf\":\nimport nmf_config as config\nassert \"nmf\" in config.model_type\nelse:\nraise ValueError(f\"model_type '{args.model_type}' is not implemented. Shoule be 'ae' or 'kmeans' or 'nmf'\")\n# Load Market budget\nif config.dataset == 'dataset1':\nmarket_budget = pd.read_csv('data/market_budget_dataset1.csv', index_col=0)\ncryptos = ['BTC', 'DASH', 'ETH', 'XRP']\nmarket_budget = pd.concat([market_budget, pd.DataFrame(np.array([['crypto', 1]] * len(cryptos)),\nindex=cryptos,\n# market_budget = market_budget.drop('CRIX')\nmarket_budget['rc'] = market_budget['rc'].astype(int)\nelif config.dataset == 'dataset2':\nmarket_budget = pd.read_csv('data/market_budget_dataset2.csv', index_col=0)\nraise NotImplementedError(config.dataset)\nif config.dataset == \"dataset1\":\nCLUSTER_NAMES = BASE_FACTOR_ORDER_DATASET1\nelif config.dataset == \"dataset2\":\nCLUSTER_NAMES = BASE_FACTOR_ORDER_DATASET2\nraise NotImplementedError()\nLOGGER.info(\"Main loop to get results and portfolio weights...\")\n# Main loop to get results\ncv_results = {}\ntrain_cov = {}\nport_perf = {}\nfor i, path in enumerate(paths):\nLOGGER.info(len(paths) - i)\nif i == 0:\nportfolios = PORTFOLIOS\nelse:\nportfolios = [p for p in PORTFOLIOS if 'ae' in p]\ncv_results[i] = get_cv_results(path,\nargs.test_set,\nLOGGER.info(\"Done.\")\nLOGGER.info(\"Backtest weights...\")\n# Get average weights for AE portfolio across runs\nport_weights = get_dl_average_weights(cv_results)\n# Build dictionary for cv_portfolio_perf\ncv_returns = {}\nfor cv in cv_results[0]:\ncv_returns[cv] = cv_results[0][cv]['returns'].copy()\nfor port in PORTFOLIOS:\nif port not in ['equal', 'equal_class'] and 'ae' not in port:\nweights = pd.DataFrame(cv_results[0][cv]['port'][port]).T\nweights.index = [date]\nport_weights[cv][port] = weights\nport_weights_df = {}\nfor port in port_weights[0]:\nport_weights_df[port] = {}\nfor cv in cv_results[0]:\ndates = cv_returns[cv].index\ncv_weights = port_weights[cv][port]\ncv_weights = pd.DataFrame(np.repeat(cv_weights.values, len(dates), axis=0),\nindex=dates, columns=cv_weights.columns)\ncv_weights = cv_weights[cv_returns[cv].columns]\nport_weights_df[port][cv] = cv_weights\ncv_portfolio_df = {\ncv: {\n'returns': cv_returns[cv],\n'train_returns': cv_results[0][cv]['train_returns'],\n'port': port_weights_df[port][cv] for port in port_weights_df\n# if port not in ['equal', 'equal_classs']\n}\n} for cv in cv_returns\n}\nport_perf, leverage = cv_portfolio_perf_df(cv_portfolio_df, portfolios=PORTFOLIOS, volatility_target=0.05,\nmarket_budget=market_budget)\nK = cv_results[i][0]['loading'].shape[-1]\nCV_DATES = [str(cv_results[0][cv]['returns'].index[0].date()) for cv in range(n_folds)]\nASSETS = list(cv_results[i][0]['returns'].columns)\n# Get portfolio weights time series\n# port_weights = {}\n# for p in PORTFOLIOS:\n# if p not in ['equal', 'equal_class']:\n# port_weights[p] = get_ts_weights(cv_results, port=p)\nport_weights = get_ts_weights(port_weights)\n# Get average perf across runs\nann_perf = pd.DataFrame()\nfor p in PORTFOLIOS:\nann_perf[p] = port_perf[p]['total'].iloc[:, 0]\nLOGGER.info(\"Saving backtest performance and plots...\")\nLOGGER.info('Saving performance... ')\nann_perf.to_csv(f\"{save_dir}/portfolios_returns.csv\")\npickle.dump(port_weights, open(f\"{save_dir}/portfolios_weights.p\", \"wb\"))\nplot_perf(ann_perf, strategies=PORTFOLIOS,\nsave_path=f\"{save_dir}/performance_all.png\",\nshow=args.show, legend=args.legend)\nplot_perf(ann_perf, strategies=[p for p in PORTFOLIOS if p not in ['aerp', 'aeerc']],\nsave_path=f\"{save_dir}/performance_ae_rp_c_vs_all.png\",\nif 'hrp' in PORTFOLIOS:\nplot_perf(ann_perf, strategies=['hrp', 'aerp'], save_path=f\"{save_dir}/performance_hrp_aerp.png\",\nshow=args.show, legend=args.legend)\nplot_perf(ann_perf, strategies=['hrp', 'ae_rp_c'],\nsave_path=f\"{save_dir}/performance_hrp_aeerc_cluster.png\",\nif 'hcaa' in PORTFOLIOS:\nplot_perf(ann_perf, strategies=['hcaa', 'aeerc'], save_path=f\"{save_dir}/performance_hcaa_aeerc.png\",\nplot_perf(ann_perf, strategies=['hcaa', 'ae_rp_c'],\nsave_path=f\"{save_dir}/performance_hcaa_aeerc_cluster.png\",\nif 'markowitz' in PORTFOLIOS:\nplot_perf(ann_perf, strategies=['markowitz', 'ae_rp_c'],\nsave_path=f\"{save_dir}/performance_markowitz_aeerc_cluster.png\",\nif 'shrink_markowitz' in PORTFOLIOS:\nbar_plot_weights(port_weights['shrink_markowitz'], save_path=f\"{save_dir}/weights_shrink_markowitz.png\",\nshow=args.show)\nbar_plot_weights(port_weights['markowitz'], save_path=f\"{save_dir}/weights_markowitz.png\", show=args.show)\nlegend=args.legend)\nbar_plot_weights(port_weights['hrp'], save_path=f\"{save_dir}/weights_hrp.png\", show=args.show,\nlegend=args.legend)\nbar_plot_weights(port_weights['aeerc'], save_path=f\"{save_dir}/weights_aeerc.png\", show=args.show,\nplot_perf(ann_perf, strategies=PORTFOLIOS, show=args.show, legend=args.legend)\nbar_plot_weights(port_weights['hrp'], show=args.show)\nplot_perf(ann_perf, strategies=['hcaa', 'aeerc'], show=args.show, legend=args.legend)\nbar_plot_weights(port_weights['hcaa'], show=args.show)\n# Plot excess return\nif 'hrp' in PORTFOLIOS:\nplt.figure(figsize=(20, 10))\nplt.plot(np.cumprod(ann_perf['aerp'] + 1) - np.cumprod(ann_perf['hrp'] + 1))\nif args.save:\nplt.savefig(f\"{save_dir}/excess_performance_hrp_aerp.png\", bbox_inches='tight', transparent=True)\nplt.plot(np.cumprod(ann_perf['ae_rp_c'] + 1) - np.cumprod(ann_perf['hrp'] + 1))\nplt.savefig(f\"{save_dir}/excess_performance_hrp_aeerc_cluster.png\", bbox_inches='tight', transparent=True)\nif 'hcaa' in PORTFOLIOS:\nplt.plot(np.cumprod(ann_perf['aeerc'] + 1) - np.cumprod(ann_perf['hcaa'] + 1))\nplt.savefig(f\"{save_dir}/excess_performance_hcaa_aeerc.png\", bbox_inches='tight', transparent=True)\nplt.plot(np.cumprod(ann_perf['ae_rp_c'] + 1) - np.cumprod(ann_perf['hcaa'] + 1))\nplt.savefig(f\"{save_dir}/excess_performance_hcaa_aeerc_cluster.png\", bbox_inches='tight', transparent=True)\nif 'markowitz' in PORTFOLIOS:\nplt.plot(np.cumprod(ann_perf['ae_rp_c'] + 1) - np.cumprod(ann_perf['markowitz'] + 1))\nplt.savefig(f\"{save_dir}/excess_performance_markowitz_aeerc_cluster.png\", bbox_inches='tight',\ntransparent=True)\n# Plot one cv weight\n# CV = 0\n# plt.figure(figsize=(14, 7))\n# plt.bar(ASSETS, port_weights['hrp'].iloc[CV].values, label='hrp')\n# plt.legend()\n# plt.ylim([0, 0.9])\n# x = plt.xticks(rotation=45)\n# if args.save:\n# plt.savefig(f\"{save_dir}/weights_hrp_aerp.png\", bbox_inches='tight', transparent=True)\n#\n# plt.bar(ASSETS, port_weights['ae_rp_c'].iloc[CV].values, label='ae_rp_c')\n# plt.savefig(f\"{save_dir}/weights_hrp_aeerc_cluster.png\", bbox_inches='tight', transparent=True)\n# plt.bar(ASSETS, port_weights['hcaa'].iloc[CV].values, label='hcaa')\n# plt.savefig(f\"{save_dir}/weights_hcaa_aeerc.png\", bbox_inches='tight', transparent=True)\n# Get statistics\nstats = backtest_stats(ann_perf, port_weights, period=250, format=True, market_budget=market_budget)\nstats.to_csv(f\"{save_dir}/backtest_stats.csv\")\nLOGGER.info(stats.to_string())\nLOGGER.info(\"Done with backtest.\")\n##########################\n# Model evaluation\n# Average prediction across runs for each cv\nLOGGER.info(\"Starting with evaluation...\")\nreturns, scaled_returns, pred, scaled_pred = average_prediction_cv(cv_results)\nLOGGER.info(\"Prediction metric\")\n# Compute pred metric\ntotal_rmse = []\nfor cv in returns.keys():\n# scaler = preprocessing.MinMaxScaler(feature_range=(-1, 1))\n# scaler.fit(returns[cv])\n# same_ret = pd.DataFrame(scaler.transform(returns[cv]), index=returns.index, columns=returns.columns)\ntotal_rmse.append(float(np.sqrt(np.mean(np.mean((returns[cv] - pred[cv]).values ** 2, axis=-1)))))\ntotal_r2.append(metrics.r2_score(returns[cv], pred[cv], multioutput='uniform_average'))\nEVALUATION['model']['cv_total_rmse'] = total_rmse\n# Average prediction across runs\nreturns, scaled_returns, pred, scaled_pred = average_prediction(cv_results)\nscaler = preprocessing.MinMaxScaler(feature_range=(-1, 1))\nsame_ret = pd.DataFrame(scaler.fit_transform(returns), index=returns.index, columns=returns.columns)\nEVALUATION['model']['scaled_rmse'] = np.sqrt(np.mean((same_ret - same_pred) ** 2)).to_dict()\nEVALUATION['model']['total_rmse'] = float(np.sqrt(np.mean(np.mean((returns - pred).values ** 2, axis=-1))))\nEVALUATION['model']['r2'] = {a: metrics.r2_score(returns[a], pred[a]) for a in\nreturns.columns}\nEVALUATION['model']['total_r2'] = metrics.r2_score(returns, pred, multioutput='uniform_average')\nif False:\n# loading analysis\n# loading over cv folds\nLOGGER.info(\"CV loadings plots\")\np = 0\nn_cv = len(cv_results[p])\nn_cols = 6\nn_rows = n_cv // n_cols + 1\nfigsize = (15, int(n_rows * 6))\nfig, axs = plt.subplots(n_rows, n_cols, figsize=figsize, sharex=True,\ncbar_ax = fig.add_axes([.91, .3, .4])\nrow = -1\nfor cv in cv_results[p]:\nloading = cv_results[p][cv]['loading'].copy()\nif cv % n_cols == 0:\nsns.heatmap(loading,\nax=axs[row, col],\ncbar_ax=None if cv else cbar_ax, cmap='Reds')\ndate = str(cv_results[p][cv]['returns'].index[0].date())\naxs[row, col].set_title(date)\ncol += 1\nfig.tight_layout(rect=[0, 0, .9, 1])\nplt.savefig(f\"{save_dir}/cv_loading_weights.png\", bbox_inches='tight', transparent=True)\nif args.show:\nplt.close()\nLOGGER.info(\"Done.\")\n# Correlation\nLOGGER.info(\"Correlation...\")\navg_cv_corr = []\nfor cv in range(n_folds):\ncv_corr = []\nfor i in cv_results.keys():\ncorr = cv_results[i][cv]['test_features'].corr().values\ncorr = corr[np.triu_indices(len(corr), k=1)]\ncv_corr.append(corr)\ncv_corr = np.array(cv_corr)\ncv_corr = cv_corr.mean(0)\navg_cv_corr.append(cv_corr)\navg_cv_corr = np.array(avg_cv_corr)\navg_cv_corr = np.mean(avg_cv_corr, axis=1).tolist()\nEVALUATION['cluster']['corr'] = {}\nEVALUATION['cluster']['corr']['cv'] = avg_cv_corr\n# Ex factor correlation cv = 0\ncorr_0 = cv_results[i][0]['test_features'].corr()\nsns.heatmap(corr_0,\ncmap='bwr',\nplt.savefig(f\"{save_dir}/corr_factors_heatmap_0.png\", bbox_inches='tight', transparent=True)\nif args.show:\nplt.show()\nplt.close()\n# Ex pred correlation cv = 0\ncorr_0 = cv_results[i][0]['test_pred'].corr()\nplt.figure(figsize=(10, 10))\nplt.savefig(f\"{save_dir}/corr_pred_heatmap_0.png\", bbox_inches='tight', transparent=True)\nmy_cmap = plt.get_cmap(\"bwr\")\nrescale = lambda y: (y - np.min(y)) / (np.max(y) - np.min(y))\nplt.figure(figsize=(10, 6))\nplt.bar(range(len(avg_cv_corr)), avg_cv_corr, color=my_cmap(rescale(avg_cv_corr)), width=0.5)\nif len(avg_cv_corr) > 22:\nxticks = range(0, len(avg_cv_corr), 6)\nxticks_labels = np.array(CV_DATES)[xticks].tolist()\n_ = plt.xticks(xticks, xticks_labels, rotation=45)\n_ = plt.ylim([-1, 1])\nplt.savefig(f\"{save_dir}/avg_corr.png\", bbox_inches='tight', transparent=True)\nLOGGER.info(\"Cluster analysis...\")\n# Cluster analysis\nLOGGER.info(\"Get cluster labels...\")\ncv_labels = {}\ncv_labels[cv] = {}\nfor i in cv_results:\nc, cv_labels[cv][i] = get_cluster_labels(cv_results[i][cv]['embedding'])\nLOGGER.info(\"Compute Rand Index...\")\nEVALUATION['cluster']['rand_index'] = {}\nn_runs = len(cv_results)\ncv_rand = {}\ncv_rand[cv] = rand_score_permutation(cv_labels[cv])\nLOGGER.info(\"Rand Index heatmap...\")\n# Plot heatmap\ntrii = np.triu_indices(n_runs, k=1)\nEVALUATION['cluster']['rand_index']['cv'] = [np.mean(cv_rand[cv][trii]) for cv in cv_rand]\n# Plot heatmap of average rand\navg_rand = np.zeros_like(cv_rand[0])\nfor cv in cv_rand:\ntriu = np.triu(cv_rand[cv], k=1)\navg_rand = avg_rand + triu\navg_rand = avg_rand / len(cv_rand)\nmean = np.mean(avg_rand[trii])\nEVALUATION['cluster']['rand_index']['mean'] = mean\nsns.heatmap(avg_rand, vmin=0, vmax=1)\nplt.title(f\"Rand index\\nMean: {mean.round(2)}, Std: {std.round(2)}\")\nplt.savefig(f\"{save_dir}/rand_avg.png\", bbox_inches='tight', transparent=True)\nLOGGER.info(\"Consensus matrix...\")\n# Consensus matrix\nassets = cv_labels[cv][0]['label'].index\navg_cons_mat = pd.DataFrame(0, columns=assets, index=assets)\ncluster_assignment = {}\nfor cv in cv_labels:\ncons_mat = consensus_matrix(cv_labels[cv], reorder=True, method=\"single\")\ncluster_assignment[cv] = assign_cluster_from_consmat(cons_mat, CLUSTER_NAMES, t=0)\nif cv == 0:\norder0 = cons_mat.index\navg_cons_mat = avg_cons_mat.loc[order0, :]\navg_cons_mat += cons_mat\navg_cons_mat = avg_cons_mat / len(cv_labels)\nsns.heatmap(avg_cons_mat, square=True)\nplt.savefig(f\"{save_dir}/avg_cons_mat.png\", bbox_inches='tight', transparent=True)\nLOGGER.info(\"Saving final results...\")\n# Save final result\npickle.dump(cluster_assignment, open(f\"{save_dir}/cluster_assignment.p\", \"wb\"))\njson.dump(EVALUATION, open(f\"{save_dir}/evaluation.json\", \"w\"))\n", "output_sequence": "We provide results for \"Does non-linear factorization of financial returns help build better portfolio?\", Spilak, WK H\u00e4rdle (2022). Please refer to github Wiki for a detailed description on how to use the code!"}, {"input_sequence": "from setuptools import setup, find_packages\nsetup(\nname='dl_portfolio',\nversion='0.0.0',\nurl='https://github.com/BrunoSpilak/dl-portfolio.git',\nauthor='Bruno Spilak',\nauthor_email='bruno.spilak@gmail.com',\ndependency_links=[],\npython_requires='~=3.8',\ninstall_requires=[\n\"numpy>=1.19.2\",\n\"tensorflow==2.4.0\",\n\"tensorflow-probability==0.12.2\",\n\"jax[cpu]\",\n\"matplotlib==3.2.2\",\n\"seaborn==0.11.1\",\n\"pyportfolioopt==1.4.1\",\n\"riskparityportfolio==0.2\",\n\"joblib==1.0.0\",\n\"scikit-learn==0.24.0\",\n\"fastcluster==1.2.6\",\n],\nsetup_requires=['pytest-runner'],\nzip_safe=False,\npackages=find_packages()\n)\n", "output_sequence": "We provide results for \"Does non-linear factorization of financial returns help build better portfolio?\", Spilak, WK H\u00e4rdle (2022). Please refer to github Wiki for a detailed description on how to use the code!"}, {"input_sequence": "rm(list = ls(all = TRUE))\ngraphics.off()\n# setwd('C:/...')\n# install.packages('MASS')\nlibrary(MASS)\nu0 <- 5 * 10^10 # initial capital of insurance company (in USD)\nn <- 70\nu1 <- c(seq(0, by = u0/200, length.out = 21), seq(u0/10 + u0/200, by = (u0 - u0/10 - u0/200)/48, length.out = 49))\ntheta1 <- 0.3 # relative safety loading\ndparameters1 <- list(c(0.0584, 0.9416), c(3.59e-10, 7.5088e-09)) # weights (first vector) and exponential parameters (second vector)\n# produces the exact ruin probability in infinite time for insurance collective risk model with mixture of 2 exponentials\n# distribution claims\nruinmix2exps <- function(u, theta, dparameters) {\n# u: initial capital for risk process theta: security loading in insurance collective risk model dparameters: list, composed\n# of 2 vectors containing the parameters of loss distribution, exponential parameters (first vector) and weights (second\n# vector)\np1 <- dparameters[[1]] # exponential parameters\np <- p1[1]/p2[1]/(p1[1]/p2[1] + (1 - p1[1])/p2[2])\npsii <- p2[1] * (1 - p) + p2[2] * p\nr1 <- (psii + theta * sum(p2) - sqrt((psii + theta * sum(p2))^2 - 4 * prod(p2) * theta * (1 + theta)))/(2 * (1 + theta))\ny <- 1/((1 + theta) * (r2 - r1)) * ((psii - r1) * exp(-r1 * u) + (r2 - psii) * exp(-r2 * u)) # ruin probability using the Laplace transform inversion\nreturn(y)\n}\n# the exact ruin probability in infinite time\npsi1 <- ruinmix2exps(u1, theta1, dparameters1)\n# returns the k-th moment (up to fourth) of the mixture of 2 exponentials distribution claims\nmoments <- function(k, dparameters) {\n# k: order of moment to calculate dparameters: list, composed of 2 vectors containing the parameters of loss distribution,\n# weights (first vector) and exponential parameters (second vector)\np1 <- dparameters[[1]] # weights\np2 <- dparameters[[2]] # exponential parameters\nif (k == 1) {\nmk <- sum(p1/p2)\n} else {\nif (k == 2) {\nmk <- 2 * sum(p1/p2^2)\n} else {\nif (k == 3) {\nmk <- 6 * sum(p1/p2^3)\n} else {\nif (k == 4) {\nmk <- 24 * sum(p1/p2^4)\n}\n}\n}\nreturn(mk) # k-th raw moment of the mixture of 2 exponentials distribution claims\n# returns the moment generating function or its k-th derivative (up to third) for mixture of 2 exponentials distribution\n# claims\nmgfs <- function(x, k, dparameters) {\n# x: scalar, n x 1 vector or m x n matrix, argument of the moment generating function k: scalar, integer, 0 =< k <= 3, order\n# of the derivative dparameters: list, composed of 2 vectors containing the parameters of the loss distribution, weights\n# (first vector) and exponential parameters (second vector)\nif (k == 0) {\ny <- sum((p1 * p2)/(p2 - t(x)))\nif (k == 1) {\ny <- sum((p1 * p2)/(p2 - t(x))^2)\nif (k == 2) {\ny <- 2 * sum((p1 * p2)/(p2 - t(x))^3)\nif (k == 3) {\ny <- 6 * sum((p1 * p2)/(p2 - t(x))^4)\nm <- moments(1, dparameters1) # 1st raw moment\n# returns the adjustment coefficient R for mixture of 2 exponentials distribution claims\nadjR <- function(theta, dparameters) {\n# theta: security loading in insurance collective risk model dparameters: list, composed of 2 vectors containing the\n# parameters of the loss distribution, weights (first vector) and exponential parameters (second vector)\nR0 <- min(p2)\nR0 <- c(12 * theta * m/(3 * m2 + sqrt(9 * m2^2 + 24 * m * m3 * theta)), R0)\nR0 <- min(R0)\nerr = 1\nwhile (err > 1e-09) {\nD1 <- 1 + (1 + theta) * m * r - mgfs(r, 0, dparameters1)\nerr <- r\nr <- r - D1/D2\nerr <- abs(err - r)/r\nR <- r\nreturn(R) # adjustment coefficient R\nR <- adjR(theta1, dparameters1) # adjustment coefficient R\nmgfprim <- mgfs(R, 1, dparameters1) # moment generating function\nC <- (theta1 * m)/(mgfprim - m * (1 + theta1))\n# the Cramer-Lundberg approximation\npsi2 <- C * exp(-R * u1)\n# the exponential approximation\npsi3 <- exp(-1 - (2 * m * theta1 * u1 - m2)/sqrt(m2^2 + 4 * theta1 * m * m3/3))\n# the Lundberg approximation\npsi4 <- (1 + (theta1 * u1 - m2/m/2) * (4 * theta1 * m^2 * m3/(3 * m2^3))) * exp(-(2 * m * theta1 * u1)/m2)\ndelta1 <- (2 * m * theta1)/(m2 + ((4 * m * m3/3/m2) - m2) * theta1) # scale parameter of gamma distribution function\n# the Beekman-Bowers approximation\npsi5 <- (1 - pgamma(u1, shape = delta2, scale = 1/delta1))/(1 + theta1)\n# the Renyi approximation\npsi6 <- exp(-(2 * m * theta1 * u1)/(m2 * (1 + theta1)))/(1 + theta1)\n# parameters of the De Vylder approximation\ndelta <- 3 * m2/m3\nbeta <- 9 * m2^3/(2 * (1 + theta1) * m * m3^2)\np <- 3 * m2^2/(2 * (1 + theta1) * m * m3) - 1/(1 + theta1) + 1\n# the De Vylder approximation\npsi7 <- (beta/(p * delta)) * exp(-(delta - beta/p) * u1)\nif (m2 * m4 < 3/2 * m3^2) {\nthetanew <- (theta1 * m * (2 * m3^2 - m2 * m4)/m2^2/m3)\nm2new <- ((m2 * m4 - 2 * m3^2) * (2 * m2 * m4 - 3 * m3^2)/m3^2/m2^2)\n} else {\nthetanew <- 1/2 * theta1/m2^2 * m * (m3 + m2 * m)\nmnew <- m\nm2new <- 1/2/m2 * m * (m3 + m2 * m)\np1 <- mnew^2/(m2new - mnew^2)\ndparametersnew <- list(p1, p2) # gamma parameters\n# returns the k-th moment (up to fourth) of the mixture of 2 gamma distribution claims\nmomentsgam <- function(k, dparameters) {\n# k: order of moment to calculate dparameters: list of scalars, parameters of gamma distribution\np1 <- dparameters[[1]]\nmk <- p1/p2\nmk <- (p1^2 + p1)/(p2^2)\nmk <- (p1^3 + 3 * p1^2 + 2 * p1)/(p2^3)\nreturn(mk) # k-th raw moment of gamma distribution claims\n# returns the moment generating function or its k-th derivative (up to third) for gamma distribution claims\nmgfsg <- function(x, k, dparameters) {\n# x: scalar, argument of the moment generating function k: scalar, integer, 0 =< k <= 3, order of the derivative\n# dparameters: list of scalars, parameters of gamma distribution\ny <- p2^p1/((p2 - x)^p1)\ny <- (p1/p2) * (p2/(p2 - x))^(p1 + 1)\ny <- p1 * (p1 + 1) * (p1 + 2) * p2^p1/(p2 - x)^(p1 + 3)\n# moments for gamma distribution claims\nmg <- momentsgam(1, dparametersnew) # 1st raw moment\n# returns the adjustment coefficient R for gamma distribution claims\n# theta: security loading in insurance collective risk model dparameters: list of scalars, parameters of gamma distribution\nR0 <- 0.99999999 * p2\nR0 <- c(12 * theta * mg/(3 * mg2 + sqrt(9 * mg2^2 + 24 * mg * mg3 * theta)), R0)\nerr <- 1\nD1 <- 1 + (1 + theta) * mg * r - mgfsg(r, 0, dparameters)\nR <- adjR(thetanew, dparametersnew) # adjustment coefficient R for gamma distribution claims\nmgfprim <- mgfsg(R, 1, dparametersnew) # moment generating function for gamma distribution claims\nC <- (thetanew * mg)/(mgfprim - mg * (1 + thetanew))\nCram <- C * exp(-R * u1) # the Cramer-Lundberg approximation for gamma claims\nu1 <- u1 * p2/p1\nb <- 1/p1\nn <- length(u1)\n# the function to be integrated\nexactgamint <- function(x) {\nj <- 1\nwhile (j < n + 1) {\nuj <- u1[j]\nL <- x^(1/b) * exp(-(x + 1) * uj/b)\nM <- (x^(1/b) * (1 + (1 + thetanew) * (x + 1)/b) - cos(pi/b))^2 + sin(pi/b)^2\nj <- j + 1\ny <- L/M\n# integrates exactgamint function using the Simpson's method\nd <- area(exactgamint, 0, 0.001)\nd <- rbind(1, d)\nerr <- 1e-05\nint <- matrix(1, n)\nj <- 1\nwhile (j < n + 1) {\ni <- 2\nwhile (abs((d[i - 1] - d[i])/d[i]) > err) {\nv <- area(exactgamint, i - 1, i)\nd <- rbind(d, (v + d[i]))\ni <- i + 1\nendd <- length(d)\nint[j] <- d[endd]\nj <- j + 1\n# the 4-moment gamma De Vylder approximation\npsi8 <- Cram + as.vector((thetanew * sin(pi/b)/pi/b) * int)\n# the heavy traffic approximation\npsi9 <- exp((-2 * theta1 * m * u1)/m2)\npa <- matrix(dparameters1[[1]]) # 2 x 1 matrix of weights\npaa <- matrix(pa, nrow = 2, ncol = length(u1))\nd <- rowSums(t(paa/pbb * exp(-pb %*% u1)))\nc <- matrix(m, nrow = length(d)) - d\n# the light traffic approximation\npsi10 <- (m - c)/(1 + theta1)/m\nu2 <- (1 - 1/(1 + theta1)) * u1 # capital for the light traffic approximation\npaa <- matrix(pa, nrow = 2, ncol = length(u2))\nd <- rowSums(t(paa/pbb * exp(-pb %*% u2)))\npsil <- (m - c)/(1 + theta1)/m # the new light traffic approximation\n# the heavy-light traffic approximation\npsi11 <- (1/(1 + theta1)^2) * psi9 + (1 - 1/(1 + theta1)) * psil\nu1 <- u1/10^9\n# the relative errors of the approximations\nerr2 <- (psi2 - psi1)/psi1\nplot(u1, psi1, type = \"l\", col = \"red\", ylim = c(0, 0.801), lwd = 3, main = \"\", xlab = \"u (USD billion)\", ylab = expression(psi(u)),\ncex.axis = 1.6, cex.lab = 1.6)\ndev.new()\nplot(u1, err2, type = \"l\", col = \"blue\", ylim = c(-0.301, 0.301), lwd = 3, main = \"\", xlab = \"u (USD billion)\", ylab = expression((psi[i](u) -\npsi(u))/psi(u)), cex.axis = 1.6, cex.lab = 1.6)\nlines(u1, err3, col = \"brown\", lwd = 3, lty = 4)\nplot(u1, err4, type = \"l\", lty = 4, col = \"red\", ylim = c(-1.01, 1.01), lwd = 3, main = \"\", xlab = \"u (USD billion)\", ylab = expression((psi[i](u) -\nlines(u1, err6, col = \"blue\", lwd = 3, lty = 3)\n", "output_sequence": "Produces the relative error of the approximations (with respect to exact method) of ruin probability in infinite time treated as a function of the initial capital for the mixture of 2 exponentials distibution claims."}, {"input_sequence": "# clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\ninstall.packages(\"MASS\")\nlibrary(MASS)\n# Black-Scholes Function\nBS = function(S, K, Time, r, sig, type) {\nd1 = (log(S/K) + (r + sig^2/2) * Time)/(sig * sqrt(Time))\nd2 = d1 - sig * sqrt(Time)\nif (type == 1) {\nvalue = S * pnorm(d1) - K * exp(-r * Time) * pnorm(d2)\n}\nif (type == 0) {\nvalue = K * exp(-r * Time) * pnorm(-d2) - S * pnorm(-d1)\nreturn(value)\n}\n# Function to find BS Implied Vol using Bisection Method\nblsimpv = function(S, K, Time, r, market, type) {\nsig = 0.2\nsig.up = 1\nsig.down = 0.001\ncount = 0\nerr = BS(S, K, Time, r, sig, type) - market\n# repeat until error is sufficiently small or counter hits 1000\nwhile (abs(err) > 1e-05 && count < 1000) {\nif (err < 0) {\nsig.down = sig\nsig = (sig.up + sig)/2\n} else {\nsig.up = sig\nsig = (sig.down + sig)/2\n}\nerr = BS(S, K, Time, r, sig, type) - market\ncount = count + 1\n# return NA if counter hit 1000\nif (count == 1000) {\nreturn(NA)\n} else {\nreturn(sig)\n# load data\nx = read.table(\"volsurfdata2.dat\")\n# define variables\nx[, 7] = x[, 2]/(x[, 1] * exp(x[, 3] * x[, 4])) # define moneyness\nPrice = x[, 1]\nClass = x[, 6]\nmon = x[, 7]\nn = length(x[, 1])\n# calculate implied volatility\niv = rep(0, n)\nfor (i in 1:n) {\niv[i] = blsimpv(S = Price[i], K = Strike[i], Time = Time[i], r = Rate[i], market = Value[i], type = Class[i])\nstepwidth = c(0.02, (2/52))\nfirstmon = min(x[, 7])\nlengthmon = ceiling((lastmon - firstmon)/stepwidth[1])\nmongrid = seq(firstmon, lastmon, length = c(lengthmon + 1))\n# grid function\nmeshgrid = function(a, b) {\nlist(x = outer(b * 0, a, FUN = \"+\"), y = outer(b, a * 0, FUN = \"+\"))\n# compute grid\ngridone = meshgrid(mongrid, matgrid)\nMON = gridone$x\ngmon = lengthmon + 1L\nuu = dim(x)\nv = uu[1]\n# calculate the implied volatility surface\nbeta = matrix(0, gmat, gmon)\n# kernel regression via fft using quartic kernel\nj = 1L\nwhile (j < gmat + 1L) {\nk = 1L\nwhile (k < gmon + 1L) {\n\ni = 1L\nX = matrix(0, v, 3)\nwhile (i < (v + 1L)) {\nX[i, ] = c(1, x[i, 7] - MON[j, k], x[i, 4] - MAT[j, k])\ni = i + 1\nY = iv\nh = bandwidth\nW = matrix(0, v, v) #Kernel matrix\nu1 = (MON[j, k] - x[i, 7])/h[1]\naa = 15/16 * (1 - u1^2)^2 %*% (abs(u1) <= 1)/h[1]\nW[i, i] = aa %*% bb\ni = i + 1L\nest = ginv(t(X) %*% W %*% X) %*% t(X) %*% W %*% Y\nbeta[j, k] = est[1]\nk = k + 1L\nIV = beta\nxnew = x\n# redefine variables\nPrice = xnew[, 1]\nn = length(xnew[, 1])\n# calculate implied volatility for original options\niv[i] = blsimpv(Price[i], Strike[i], Time[i], Value[i], Class[i])\n# define points\npts = cbind(mon, Time, iv)\n# load required package\nrequire(lattice)\n# plot\nwireframe(IV ~ MON + MAT, drape = T, ticktype = \"detailed\", pts = pts, main = \"\", zlim = c(0.25, 0.6), aspect = c(1, 1), scales = list(arrows = FALSE,\ny = list(labels = round(seq(firstmat, lastmat, length = 6), 2)), x = list(labels = round(seq(firstmon, lastmon, length = 6),\n2)), z = list(labels = round(seq(0.25, max(IV), length = 8), 2))), ylab = list(\"Time to Maturity\", rot = -8), xlab = list(\"Moneyness\",\nrot = 25), zlab = list(\"Implied Volatility\", rot = 94), panel.3d.wireframe = function(x, y, z, xlim, xlim.scaled,\nylim.scaled, pts, drape = drape, ...) {\npanel.3dwire(x, y, z, xlim = xlim, ylim = ylim, zlim = zlim, xlim.scaled = xlim.scaled, = ylim.scaled, = zlim.scaled,\ndrape = TRUE, ...)\n\npanel.3dscatter(pts[, 1], pts[, 2], pts[, 3], xlim = xlim, ylim = ylim, zlim = zlim, xlim.scaled = xlim.scaled, = ylim.scaled,\nzlim.scaled = zlim.scaled, type = \"p\", col = c(4), lwd = 3, cex = 1, pch = c(19), .scale = TRUE, ...)\n}, screen = list(z = 240, x = -70))\n", "output_sequence": "Plots the implied volatility of DAX options on January, 4th 1999, using kernel regression employing a quartic kernel with bandwidth=c(0.05, 0.3) for moneyness and time t maturity respectively."}, {"input_sequence": "% returns the k-th moment (up to fourth) of the mixture of 2 exponentials distribution claims\nfunction [ mk ] = moments( k,dparameters );\n% k: order of moment to calculate\n% dparameters: list, composed of 2 vectors containing the parameters of loss distribution, weights (first vector) and exponential parameters (second vector)\np1=dparameters(:,1); % weights\np2=dparameters(:,2); % exponential parameters\nif (k==1)\nmk=sum(p1./p2);\nelseif (k==2)\nmk=2*sum(p1./(p2.^2));\nelseif (k==3)\nmk=6*sum(p1./(p2.^3));\nelseif (k==4)\nmk=24*sum(p1./(p2.^4));\nelse\ndisp('k chosen too large')\nend\n", "output_sequence": "Produces the ruin probability in infinite time for mixture of 2 exponentials distribution claims given by Lundberg approximation. Needs the \"moments.m\" function."}, {"input_sequence": "clear all;\nclose all,\nclc;\nformat long;\nu=[0;10^9;5*10^9;10^10;2*10^10;5*10^10]; % initial capital of insurance company (in USD)\ntheta=0.3; % relative safety loading\ndparameters1=[0.0584 , 3.5900e-10 ; 0.9416 , 7.5088e-09]; % weights (first column) and exponential parameters (second column)\nm=moments(1,dparameters1); % 1st raw moment\n% the Lundberg approximation for mixture of 2 exponentials claims with beta1=3.5900e-10, beta2=7.5088e-09, alpha=0.0584 and theta=0.3 (u in USD)\n", "output_sequence": "Produces the ruin probability in infinite time for mixture of 2 exponentials distribution claims given by Lundberg approximation. Needs the \"moments.m\" function."}, {"input_sequence": "rm(list = ls(all = TRUE))\n# setwd('C:/...')\n# returns the k-th moment (up to fourth) of the mixture of 2 exponentials distribution claims\nmoments <- function(k, dparameters) {\n# k: order of moment to calculate dparameters: list, composed of 2 vectors containing the parameters of loss distribution,\n# weights (first vector) and exponential parameters (second vector)\np1 <- dparameters[[1]] # weights\np2 <- dparameters[[2]] # exponential parameters\nif (k == 1) {\nmk <- sum(p1/p2)\n} else {\nif (k == 2) {\nmk <- 2 * sum(p1/p2^2)\n} else {\nif (k == 3) {\nmk <- 6 * sum(p1/p2^3)\n} else {\nif (k == 4) {\nmk <- 24 * sum(p1/p2^4)\n}\n}\n}\nreturn(mk) # k-th raw moment of the mixture of 2 exponentials distribution claims\n}\nu <- c(0, 10^9, 5 * 10^9, 10^10, 2 * 10^10, 5 * 10^10) # initial capital of insurance company (in USD)\ntheta <- 0.3 # relative safety loading\ndparameters1 <- list(c(0.0584, 0.9416), c(3.59e-10, 7.5088e-09)) # weights (first vector) and exponential parameters (second vector)\nm <- moments(1, dparameters1) # 1st raw moment\n# the Lundberg approximation for mixture of 2 exponentials claims with \\fbeta1=3.5900e-10, beta2=7.5088e-09, alpha=0.0584\n# and theta=0.3 (u in USD)\npsi <- (1 + (theta * u - m2/m/2) * (4 * theta * m^2 * m3/(3 * m2^3))) * exp(-(2 * m * theta * u)/m2)\n", "output_sequence": "Produces the ruin probability in infinite time for mixture of 2 exponentials distribution claims given by Lundberg approximation. Needs the \"moments.m\" function."}, {"input_sequence": "clear all\n% user input parameters\ndisp('Please input the lowest and the highest strike of options portfolio as: [10,200]') ;\ndisp(' ') ;\npara=input('Options strike range [lower bound, upper bound]=');\nwhile length(para) < 2\ndisp('Not enough input arguments. Please input in 1*2 vector form like [10,200] or [10,200]');\npara=input('Options strike range [lower bound, upper bound]=');\nend\ns1=para(1);\ndisp(' ') ;\ndisp('Please imput the implied volatility, i.e. 0.25 for 25%') ;\npara=input('Implied volatility =');\nv=para;\ndisp('Please input the maturity of a swap (e.g. 1 year = 1, 3 month = 0.25)') ;\npara=input('Maturity of a swap =');\ntau=para;\nr=0; % interest rate\ns=(s2+s1)/2; % defines the underlying price S\nkp=s1:10:(s-1); % defines the range of puts\ncallputspot = blsprice(s, s, r, tau, v); %BS value of the call(put) with strike equal to spot price\nfor i=1:length(kc);\nivc(i)=kc(i)*v/s;\nd1=(log(s/kc(i))+(r+v^2/2)*tau)/(ivc(i)*tau^0.5);\nd2=d1-ivc(i)*tau^0.5;\ncall(i) = s*normcdf(d1)-kc(i)*exp(-r*tau)*normcdf(d2);\ni=i+1;\nfor i=1:length(kp)\nivp(i)=kp(i)*v/s;\nd1=(log(s/kp(i))+(r+ivp(i)^2/2)*tau)/(ivp(i)*tau^0.5);\nd2=d1-ivp(i)*tau^0.5;\nput(i) = kp(i)*exp(-r*tau)*normcdf(-d2)-s*normcdf(-d1);\ni=i+1;\nend\nfp=log(s./kp)+kp./s-1;\nwc(1)=(fc(1)-0)/(kc(1)-s); %weight of fist call(put) (with strike eqial to spot price)\nwcum=wc(1);\nfor j=2:length(kc);\nwc(j)=(fc(j)-fc(j-1))/(kc(j)-kc(j-1))-wcum;\nwcum=wcum+wc(j);\nj=j+1;\nj=numel(kp);\nwcum=(0-fp(j))/(kp(j)-s);\nwhile kp(j)> s1\nwp(j)=(fp(j-1)-fp(j))/(kp(j)-kp(j-1))-wcum;\nwcum=wcum+wp(j);\nj=j-1;\nwp = wp(2:length(wp));\ncall = call(1:(length(call)-1));\nopt=cat(2,put,callputspot,call);\nw=cat(2,wp,wc);\nstrike=((2/tau)*sum(w.*opt))^0.5\nclear x wcum v th tau r para opt n j i fp f discr d2 d1 wp wc kp kc fc\n", "output_sequence": "Calculates the strike of a variance of a given maturity using the potrfolio of options of a given strike range."}, {"input_sequence": "function retval=theil(x,n)\n% Converts the given time series into time series of Theil index\n% n - size of moving window\n% x - analysed time series\n%\n% Author: Janusz Mi\u015bkiewicz, email: jamis@ift.uni.wroc.pl\n[n_pocz,k]=size(x);\nrozm=n_pocz-n+1;\nretval=zeros(rozm,k);\nfor i=1:rozm\nsr=mean(x(i:i+n-1,:));\ntemp=x(i:i+n-1,:)./(ones(n,1)*sr);\ntemp=temp.*log(temp);\nretval(i,:)=mean(temp);\nend;\n", "output_sequence": "Presents the statistical properties of the minimum spanning tree as a function of time windows sizes for elements of the chosen WIG 20 companies (close.csv). The Theil index based distance is used. Requires theil.m, mst.m, manh.m to run the program. Note that the {statistical toolbox} from Matlab is required as well."}, {"input_sequence": "rm(list=ls(all=TRUE))\ngraphics.off()\ninstall.packages(\"moments\")\nlibrary(moments)\n##########################################\n########## Subroutine manh(x) ############\nmanh = function(x){\n# Manhattan distence between time series normalised by the time series\n# length.\n# x - time series\n#\nx = as.matrix(x)\nh = nrow(x)\nresult = matrix(0,k,k)\nfor (j in 1:k){\nresult[i,j] = abs(mean(x[,i]-x[,j]))\n}\n}\nretval= result\nreturn(retval)\n###########################################\n############ Subroutine mst(x) ############\nmst = function(x){\n# Algorithm generates minimum spanning tree\n# The rsult is presentes as a set of links between nodes\nn = nrow(x)\ntrue = upper.tri(x)\nx = true*x\nnet = matrix(0,n-1,3)\nonnet = rep(as.integer(0),n)\nklast = 0L\n# check if the matrics is symmetric and positive\nmaxx = max(apply(x,2,max))\nsmax = 10*abs(maxx)\nx[x==0] = smax\nwhile (licz<n-1){\nminx = min(apply(x,2,min))\nd = which(x<=minx,arr.ind=T)\ni = d[,1]\nif (length(i) > 1){\nii = i[1]\nj = 0\n\nif (onnet[i] ==0 & onnet[j] ==0){\nlicz = licz+1L\nnet[licz,1] = i\nklast = klast+1L\nnet[licz,3] = min(x[i,j],x[j,i])\nonnet[i] = 1\nx[i,j] = smax\n}else if (onnet[i]==0 & onnet[j]==1) {\nlicz = licz+1\nklaster[i] = klaster[j]\n}else if (onnet[i] ==1 & onnet[j] ==0) {\nklaster[j] = klaster[i]\n}else if (onnet[i] ==1 & onnet[j] ==1 & klaster[i]==klaster[j]) {\nlicz = licz+1L\nklaster[klaster==klaster[i]]=klaster[j]\nretval = net\nreturn(retval)\n#############################################\n############ Subroutine theil(x) ############\ntheil = function(x,n){\n# Converts the given time series into time series of Theil index\n# n - size of moving window\n# x - analysed time series\nx = as.matrix(x)\nn_pocz = nrow(x)\nk = ncol(x)\nrozm = n_pocz-n+1\nretval = matrix(0,rozm,k)\nfor (i in 1:rozm){\nsr = apply(x[i:(i+n-1),],2,mean)\ntemp = x[i:(i+n-1),]/(matrix(1,n,1)%*%sr)\ntemp = temp*log(temp)\nretval[i,] = apply(temp,2,mean)\n}\n############ Main calculation #############\ndata = read.table(\"gwp.csv\",header=T)\ndata = as.matrix(data)\ndata = abs(diff(log(data))) # abs log return\ndl_szer = nrow(data)\npodmioty= ncol(data)\ndata[data==0] = 0.0000001\nprint(\"The procedure may last several hours since 9025 networks are constructed.\")\n# define matrices\nwynik_wind_mean = matrix(0,96,96)\n# time window loop\nfor (window1 in 5:100){\ntheil_data = theil(data,window1)\nfor (window2 in 5:100){\n\n# moving time window\nwynik = numeric()\nfor (t in 1:(dl_szer - window1-1-window2)){\nwindow_data = theil_data[t:(t+window2),]\nwind_dist = manh(window_data)\nwynik = c(wynik,wind_mst[,3])\nwind_mst = numeric()\n}\nwynik_wind_mean[window1-4,window2-4] = mean(wynik)\nrequire(lattice)\nwireframe(wynik_wind_mean,drape=T,#ticktype=\"detailed\",\nmain=\"mean distance, MST\",\nscales=list(arrows=FALSE,col=\"black\",distance=1,tick.number=8,cex=.7),\nxlab=list(\"T_1\",rot=30,cex=1.2),\ndev.new()\nwireframe(wynik_wind_std,drape=T,#ticktype=\"detailed\",\nmain=\"Std, MST\",\nzlab=list(\"std\",cex=1.1))\nwireframe(wynik_wind_skew,drape=T,#ticktype=\"detailed\",\nmain=\"Skewness, MST\",\nzlab=list(\"skewness\",rot=95,cex=1.1))\nwireframe(wynik_wind_kurt,drape=T,#ticktype=\"detailed\",\nmain=\"Kurtosis, MST\",\nzlab=list(\"kurtosis\",rot=95,cex=1.1))\n", "output_sequence": "Presents the statistical properties of the minimum spanning tree as a function of time windows sizes for elements of the chosen WIG 20 companies (close.csv). The Theil index based distance is used. Requires theil.m, mst.m, manh.m to run the program. Note that the {statistical toolbox} from Matlab is required as well."}, {"input_sequence": "function retval=manh(x)\n% Manhattan distence between time series normalised by the time series\n% length.\n% x - time series\n%\n% Author: Janusz Mi\u015bkiewicz, email: jamis@ift.uni.wroc.pl\n[h,k]=size(x);\nresult=zeros(k);\nfor j=1:k\nresult(i,j)=abs(mean(x(:,i)-x(:,j)));\nend;\nretval= result;\n", "output_sequence": "Presents the statistical properties of the minimum spanning tree as a function of time windows sizes for elements of the chosen WIG 20 companies (close.csv). The Theil index based distance is used. Requires theil.m, mst.m, manh.m to run the program. Note that the {statistical toolbox} from Matlab is required as well."}, {"input_sequence": "clear all\nclose all\nclc\ndata = load('gwp.csv');\ndata = abs(diff(log(data))); % abs log return\n[dl_szer,podmioty] = size(data);\ndata(data==0) = 0.0000001;\n% time window loop\ndisp('The procedure may last several hours since 9025 networks are constructed.')\nfor window1=5:100\ntheil_data = theil(data,window1);\nfor window2=5:100\n% moving time window\nwynik = [];\nfor t=1:(dl_szer - window1-1-window2)\nwindow_data = theil_data(t:(t+window2),:);\nwind_dist = manh(window_data);\nwynik = [wynik;wind_mst(:,3)];\nwind_mst = [];\nend;\nwynik_wind_mean(window1-4,window2-4) = mean(wynik);\nend;\nend;\nsubplot(2,2,1), mesh(wynik_wind_mean,'DisplayName','mean distance, MST');figure(gcf)\nxlabel('T_1');\ntitle('mean distance, MST');\nsubplot(2,2,2),mesh(wynik_wind_std,'DisplayName','Std, MST');figure(gcf)\nzlabel('std');\ntitle('Std, MST');\nsubplot(2,2,3),mesh(wynik_wind_skew,'DisplayName','Skewness, MST');figure(gcf)\nzlabel('skewness');\ntitle('Skewness, MST');\nsubplot(2,2,4),mesh(wynik_wind_kurt,'DisplayName','Kurtosis, MST');figure(gcf)\nzlabel('kurtosis');\n", "output_sequence": "Presents the statistical properties of the minimum spanning tree as a function of time windows sizes for elements of the chosen WIG 20 companies (close.csv). The Theil index based distance is used. Requires theil.m, mst.m, manh.m to run the program. Note that the {statistical toolbox} from Matlab is required as well."}, {"input_sequence": "function retval = mst(x)\n% Algorithm generates minimum spanning tree\n% The rsult is presentes as a set of links between nodes\n%\n% Author: Janusz Mi\u015bkiewicz, email: jamis@ift.uni.wroc.pl\n[n,m]=size(x);\nx=triu(x,1);\nnet=zeros(n-1,3);\nklaster=zeros(n,1);\nklast=0;\nlicz=0;\n%check if the matrics is symmetric and positive\nmaxx=max(max(x));\nsmax=10*abs(maxx);\nx(x==0)=smax;\nwhile (licz<n-1)\nminx=min(min(x));\n[i,j]=find(x<=minx);\nif (length(i) > 0)\nii=i(1);\nj=[];\nend;\nif (onnet(i) ==0 && onnet(j) ==0)\nlicz=licz+1;\nnet(licz,1)=i;\nklast=klast+1;\nnet(licz,3)=min(x(i,j),x(j,i));\nonnet(i)=1;\nx(i,j)=smax;\nelseif (onnet(i)==0 && onnet(j)==1)\nklaster(i)=klaster(j);\nelseif (onnet(i) ==1 && onnet(j) ==0)\nklaster(j)=klaster(i);\nelseif (onnet(i) ==1 && onnet(j) ==1 && klaster(i)==klaster(j))\nklaster(klaster==klaster(i))=klaster(j);\nend;\nretval=net;\n", "output_sequence": "Presents the statistical properties of the minimum spanning tree as a function of time windows sizes for elements of the chosen WIG 20 companies (close.csv). The Theil index based distance is used. Requires theil.m, mst.m, manh.m to run the program. Note that the {statistical toolbox} from Matlab is required as well."}, {"input_sequence": "function smile=HestonVanillaSmile(cp,spot,strikes,v0,vv,rd,rf,tau,kappa,theta,lambda,rho)\n%HESTONVANILLASMILE The volatility smile implied by the Heston model.\n% SMILE=HESTONVANILLASMILE(CP,SPOT,STRIKES,V0,VV,RD,RF,TAU,KAPPA,THETA,LAMBDA,RHO)\n% returns a vector of volatilities given a vector of strikes STRIKES,\n% spot price SPOT, initial volatility V0, vol of vol VV, domestic and\n% foreign interest rates RD and RF, time to maturity (in years) TAU, mean\n% reversion KAPPA, long-run mean THETA, market price of risk LAMBDA,\n% correlation RHO and option type CP:\n% CP=1 -> call option, CP=-1 -> put option.\n%\n% Sample use:\n% >> HestonVanillaSmile(1,0.8,0.2:0.2:1,0,0.2,0.05,0.05,1,2,0.04,0,0.5)\n%\n% Reference(s):\n% [1] A.Janek, T.Kluge, R.Weron, U.Wystup (2010) FX smile in the\n% Heston model, see http://ideas.repec.org/p/pra/mprapa/25491.html\n% {Chapter prepared for the 2nd edition of \"Statistical Tools for\n% Springer-Verlag, forthcoming in 2011.}\n% Written by Rafal Weron (2004.07.30)\n% Revised by Agnieszka Janek (2010.07.07, 2010.10.20)\n% Revised by Rafal Weron (2010.12.27)\nnostrikes = length(strikes);\n% Create an array of option values\nP = zeros(1,nostrikes);\n% Create an array of implied volatilities\nIV = zeros(1,nostrikes);\nfor m=1:nostrikes\nlambda = 0;\n% Calculate option prices with respective strikes in the Heston\n% stochastic volatility model with lambda = 0;\nP(m) = HestonVanilla(cp,spot,strikes(m),v0,vv,rd,rf,tau,kappa,theta,lambda,rho);\n% Calculate implied volatilities for options with respective strikes\nIV(m) = ImplVolFX(P(m),spot,strikes(m),rd,rf,tau,cp);\nend\nsmile = IV;\n\n%%%%%%%%%%%%% INTERNALLY USED ROUTINE %%%%%%%%%%%%%\nfunction vol=ImplVolFX(X,S,K,rd,rf,tau,cp)\n%IMPLVOLFX Implied volatility assuming the Garman-Kohlhagen model for a\n%European style FX option.\n% VOL=IMPLVOLFX(X,S,K,RD,RF,TAU,CP) determines implied volatility\n% of a European FX option given its price X, spot price SPOT, strike K,\n% domestic interest rate RD, foreign interest rate RF, time to maturity\n% (in years) TAU and option type CP:\n% CP = 1 --> call option, CP = -1 --> put option\n% Find zero of a function GK(vol) - X\nvol = fzero(@(vol) (GarmanKohlhagen(S, K, vol , rd, tau, cp, 0)-X),0.001,optimset('TolX',1e-8));\n", "output_sequence": "Plots the effect of changing the correlation on the shape of the smile. Requires GarmanKohlhagen.m, HestonVanillaSmile.m and HestonVanilla.m function."}, {"input_sequence": "function P= HestonVanilla(cp,s,k,v0,vv,rd,rf,tau,kappa,theta,lambda,rho)\n%HESTONVANILLA European FX option price in the Heston model.\n% P=HESTONVANILLA(CP,S,K,V0,VV,RD,RF,TAU,KAPPA,THETA,LAMBDA,RHO) returns\n% the price of a European call (CP=1) or put (CP=-1) option given\n% spot price S, strike K, initial volatility V0, vol of vol VV,\n% domestic interest rate RD, foreign interest rate RF, time to maturity\n% (in years) TAU, level of mean reversion KAPPA, long-run variance THETA,\n% market price of volatility risk LAMBDA and correlation RHO.\n\n% Sample use:\n% >> HestonVanilla(1,1.03,1,.01,.02,.05,.03,.25,10,.01,0,.5)\n%\n% Reference(s):\n% [1] H.Albrecher, P.Mayer, W.Schoutens, J.Tistaert (2006) The little\n% [2] A.Janek, T.Kluge, R.Weron, U.Wystup (2010) FX smile in the\n% Heston model, see http://ideas.repec.org/p/pra/mprapa/25491.html\n% {Chapter prepared for the 2nd edition of \"Statistical Tools for\n% Springer-Verlag, forthcoming in 2011.}\n% Written by Agnieszka Janek and Rafal Weron (2010.07.07)\n% Revised by Rafal Weron (2010.10.08, 2010.12.27)\n% Integrate using adaptive Gauss-Kronod quadrature\nP1 = 0.5+1/pi.*quadgk(@(phi) HestonVanillaInt(phi,1,s,k,v0,vv,rd,rf,tau,kappa,theta,lambda,rho),0,inf,'RelTol',1e-8);\n% Calculate Pplus and Pminus\nPplus = (1-cp)/2 + cp.*P1;\n% Calculate option price\nP = cp.*( s.*exp(-rf.*tau).*Pplus - k.*exp(-rd.*tau).*Pminus);\n%%%%%%%%%%%%% INTERNALLY USED ROUTINE %%%%%%%%%%%%%\n\nfunction F=HestonVanillaInt(phi,m,s,k,v0,vv,rd,rf,tau,kappa,theta,lambda,rho)\n%HESTONVANILLAINT Auxiliary function used by HESTONVANILLA.\n% F=HESTONVANILLAINT(phi,m,s,k,v0,vv,rd,rf,tau,kappa,theta,lambda,rho)\n% returns the values of the auxiliary function evaluated at points PHI,\n% given spot price S, strike K, initial volatility V0, vol of vol VV,\n% market price of volatility risk LAMBDA and correlation RHO.\na = kappa.*theta;\nu = [0.5 -0.5];\nb = [kappa + lambda - rho.*vv kappa + lambda];\nx = log(s);\nd = sqrt((1i*rho.*vv.*phi - b(m)).^2 - vv.^2*(2*u(m).*phi.*1i - phi.^2));\ng = (b(m) - rho.*vv.*phi.*1i - d)./(b(m) - rho.*vv.*phi.*1i + d);\nD = (b(m) - rho.*vv.*phi.*1i - d)./(vv.^2).*((1 - exp(-d.*tau))./(1 - g.*exp(-d.*tau)));\nC = (rd - rf).*phi.*1i.*tau + a./(vv.^2).*((b(m) - rho.*vv.*phi.*1i - d).*tau - 2*log((1 - g.*exp(-d.*tau))./(1-g)));\nf = exp(C + D.*v0 + 1i.*phi.*x);\n", "output_sequence": "Plots the effect of changing the correlation on the shape of the smile. Requires GarmanKohlhagen.m, HestonVanillaSmile.m and HestonVanilla.m function."}, {"input_sequence": "% clear variables and close windows\nclear all\nclc\nstandalone = 0; % set to 0 to make plots as seen in STF2\n% market volatilities for respective deltas\ndelta = .1:.1:.9;\nmarketvols = [0.1135 0.109 0.10425 0.103 0.10575 0.1105 0.1165];\nspot = 1.215; % spot on July 1, 2004\nrd = 0.02165; % domestic riskless interest rate\ntau = 0.5; % time to expiry\ncp = 1; % call option\n%Calculate strikes for respective deltas\nstrikes = GarmanKohlhagen(spot,delta,marketvols,rd,rf,tau,cp,2);\n%Sample input:\nv0 = 0.01; % initial volatility\nkappa = 1.5; % speed of mean revision of the volatility process\ntheta = 0.015; % long-term mean of volatility process\nvv = 0.2; % volatility\nrho = 0.05; % correlation between the spot price and volatility processes\nlambda = 0;\n% HestonVanillaSmile returns volatility smile\nsmile1 = HestonVanillaSmile(cp,spot,strikes,v0,vv,rd,rf,tau,kappa,theta,lambda,rho);\nif standalone,\nfigure(1);\nelse\nfigure(1);\nsubplot(1,2,1);\nend\nplot(delta*100,smile1*100,'x-',...\ndelta*100,smile3*100,'+:','LineWidth',1);\nif standalone, title('Correlation and the smile'); end\nxlabel ('Delta [%]');\nylabel ('Implied volatility [%]');\nlegend('rho = 0.05', 'rho = -0.15', 'rho = 0.15','Location','North')\nset(gca,'XTick', 10:20:90, 'Ylim', [9 13]);\nfigure(2);\nsubplot(1,2,2);\nplot(delta*100,smile4*100,'x-',...\ndelta*100,smile6*100,'+:','LineWidth',1);\nlegend('rho = 0', 'rho = -0.5', 'rho = 0.5','Location','North')\n", "output_sequence": "Plots the effect of changing the correlation on the shape of the smile. Requires GarmanKohlhagen.m, HestonVanillaSmile.m and HestonVanilla.m function."}, {"input_sequence": "function opv = GarmanKohlhagen(S,K,vol,rd,rf,tau,cp,task)\n%GARMANKOHLHAGEN European FX option pricing formula.\n% OPV=GARMANKOHLHAGEN(S,K,VOL,RD,RF,TAU,CP,TASK) returns option price,\n% (spot) delta or strike depending on the value of the TASK parameter:\n% TASK = 0 --> option price (default),\n% TASK = 2 --> option strike (given its delta as parameter K),\n% in the Garman and Kohlhagen (1983) option pricing model.\n% The remaining input parameters are: FX spot S, strike/spot delta K,\n% volatility VOL, domestic and foreign riskless interest rates RD and RF\n% (annualized), time to expiry (in years) TAU and option type CP:\n% CP = 1 --> call option (default), CP = -1 --> put option.\n%\n% Sample use:\n% >> C = GarmanKohlhagen(1,1,0.1,0.05,0.03,0.25,1,0)\n%\n% Reference(s):\n% [1] M.B.Garman, S.W.Kohlhagen (1983) Foreign currency option values,\n% Journal of International Money and Finance 2, 231-237.\n% [2] A.Janek, T.Kluge, R.Weron, U.Wystup (2010) FX smile in the\n% Heston model, see http://ideas.repec.org/p/pra/mprapa/25491.html\n% {Chapter prepared for the 2nd edition of \"Statistical Tools for\n% Springer-Verlag, forthcoming in 2011.}\n% Written by Rafal Weron (2004.06.20)\n% Revised by Agnieszka Janek (2010.07.07)\n% Revised by Rafal Weron (2010.11.16, 2010.12.27)\nif (nargin < 6)\nerror ('Wrong number of input arguments.')\nelse\n% Set default values:\nif (nargin < 8)\ntask = 0; %calculate option price\nif (nargin == 6)\ncp = 1; % call option\nend\nend\nd1 = ( log(S./K)+(rd-rf+0.5.*vol.^2).*tau )./ ( vol.*sqrt(tau));\nd2 = d1 - vol.*sqrt(tau);\nswitch task\ncase 1 % calculate spot delta\nopv = cp.*exp(-rf.*tau).*normcdf(cp.*d1,0,1);\ncase 2 % calculate strike given spot delta\nopv = S.*exp(-cp.*norminv(K.*exp(rf*tau),0,1).*vol.*sqrt(tau)+(rd-rf+0.5.*vol.^2).*tau);\notherwise % calculate option price\nopv = cp.*( S.*exp(-rf.*tau).*normcdf(cp.*d1,0,1)-K.*exp(-rd.*tau).*normcdf(cp.*d2,0,1));\n", "output_sequence": "Plots the effect of changing the correlation on the shape of the smile. Requires GarmanKohlhagen.m, HestonVanillaSmile.m and HestonVanilla.m function."}, {"input_sequence": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Mon May 27 14:49:33 2019\n@author: ms\nimport os\nimport json_tricks\nimport numpy as np\nimport matplotlib.pyplot as plt\n# %%\nmodel = 'params_N'\npath_save = '.'\nerrors = dict()\nfor ds in ['train', 'valid', 'test']:\nwith open(os.path.join('../LOBDeepPP_MSE_computation_T',\nf'preds_T_errors_{ds}.json')) as json_file:\ntmp = json_tricks.load(json_file)\nerrors.update({ds: tmp})\n# %% Extracting MSE from errors\nmse = {k0: v1['mse'] for k1, v1 in v0.items()}\nfor k0, v0 in errors.items()}\n# %% MSE for ask and bid for train, test and validation data\nax = plt.subplot(111)\nlinestyle = {'train': '-', 'test': '--', \"valid\": '-.'}\nfor ds, _ in mse.items():\nres = np.empty([0, 3])\nfor k, v in sorted(mse[ds].items(),\nkey=lambda k: int(k[0].replace(model, '')[:-1])):\nL = int(k.replace(model, '')[:-1])\nv = v.mean(axis=0)\ntmp = np.array([L, v[0], v[1]]).reshape([1, 3])\nres = np.concatenate((res, tmp), axis=0)\nax.plot(res[:, 0], list(res[:, 1]**1), c='red',\nlinestyle=linestyle[ds], label=f'ask {ds}')\nax.plot(res[:, 0], list(res[:, 2]**1), c='green',\nlinestyle=linestyle[ds], label=f'bid {ds}'\n)\nplt.xlabel('Time lags $T$')\nplt.xticks(res[:, 0])\nplt.ylabel('MSE')\n# ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\nplt.savefig(f'{path_save}/mse_ol.pdf', bbox_inches='tight', dpi=300)\nplt.show()\n# %% MSE vs prediction horizon with order levels for ask and bid seperated\nfor ds in mse.keys():\nax = plt.subplot(111)\nlev = int(k.replace(model, '')[:-1])\nax.plot(range(1, 31), (v.mean(axis=1)), label=lev)\nplt.xlabel('Prediction horizon $h$')\n# plt.ylim([0.005, 0.01])\nplt.ylabel(f'MSE ({ds if ds != \"valid\" else \"validation\"})')\nax.legend(loc='center left', bbox_to_anchor=(1, 0.5),\ntitle='Time lags $T$')\n# plt.ylim([0.0035, 0.02])\nplt.savefig(f'{path_save}/ph_mse_ol_{ds}.pdf',\nbbox_inches='tight', dpi=300)\n# plt.ylim([0.0035, 0.01])\n# plt.savefig(f'{path_save}/ph_mse_ol_{ds}_zoom.pdf',\n# bbox_inches='tight', dpi=300)\nplt.show()\nfor bid in [True, False]:\nax = plt.subplot(111)\nfor k, v in sorted(mse[ds].items(),\nkey=lambda k: int(k[0].replace(model, '')[:-1])):\nlev = int(k.replace(model, '')[:-1])\nax.plot(range(1, 31), (v[:, int(bid)]), label=lev)\nplt.xlabel('Prediction horizon $h$')\nplt.ylabel(f'MSE ({ds if ds != \"valid\" else \"validation\"})')\nax.legend(loc='center left', bbox_to_anchor=(1, 0.5),\ntitle='Time lags $T$')\nif ds == 'test':\nplt.ylim([0.0035, 0.014])\nplt.savefig(f'{path_save}/ph_mse_ol_{ds}_{\"bid\" if bid else \"ask\"}.pdf',\nbbox_inches='tight', dpi=300)\n# plt.ylim([0.0035, 0.01])\n# plt.savefig(f'{path_save}/ph_mse_ol_{ds}_{\"bid\" if bid else \"ask\"}_zoom.pdf',\n# bbox_inches='tight', dpi=300)\nplt.show()\n", "output_sequence": "Joint Limit order book ask and bid price predition for multiple predicton horizons with a neural networks. This script creates plots meant for analysis of the fixed order book level application for training, validation and test dataset."}, {"input_sequence": "#---------------------------------------------------------------------------\n#Author: Youjuan Li and Ji Zhu, University of Michigan (jizhu@umich.edu)\n#---------------------------------------------------------------------------\n#######################################################################\n### All rights are reserved by the authors.\n### Authors: Youjuan Li, Ji Zhu, University of Michigan (jizhu@umich.edu)\n### Date: 07/01/2006\n#Compute the entire L1-norm Path\n#Plot the Fitted Coefficients vs. s\nqrL1 <- function(x, y, a, max.steps,eps1=10^(-10), eps2=10^(-6), trace=T) {\n### y in Real set\n### a_th quantile\n### eps1: numerical\n### eps2: lambda limit\n### residual: y-f\n### indL: index\n### lambda save the -lambde values for all steps\ncall <- match.call()\n### To guarantee distinct y's even for tied data\ntmpy <- unique(y)\nif (length(tmpy) < length(y)) {\ndif <- min(diff(sort(tmpy)))\ny <- y + rnorm(length(y), 0, dif/100)\n}\n### Can also transfer x to x=h(x),arbitrary basis functions\n\nif(trace)\n#cat(\"LASSO sequencen\")\n\nn <- dim(x)[1]\nmaxvars <- min(m, n - 1)\nindm <- seq(m)\nindR <- indE <- NULL\nindL <- seq(n)\n###z <- y*x\nini <- qrL1Ini(x, y, a)\nindV <- ini$indV\nu <- ini$u\nresidual <- ini$residual\nif(missing(max.steps))\nmax.steps <- n * maxvars\n###first column is for s=0, 2nd column is for the 1st event\nbeta <- matrix(0, max.steps + 1, m)\nCgacv <- double(max.steps + 1)\nfit <- matrix(0, max.steps + 1, n)\nlamb.the <- rep(0, max.steps + 1)\nElbow.list <- as.list(seq(max.steps+1))\ntheta.g <- matrix(0, max.steps + 1, n)\nbeta0[1] <- ini$quant\nlambda[1] <- ini$lambda\nElbow.list[[1]] <- NULL\n### Calculate criteria SIC and GACV\nfit[1,] <- beta0[1]\ncheckf[1] <- pf(fit[1,], y, a)\ntrHat <- 0\nCgacv[1] <- checkf[1] / (n-trHat)\nCsic[1] <- log(checkf[1]/n) + (log(n)/(2*n))*trHat\ntheta.g[1,][indL] <- -(1-a)\n#cat(\"Initial variables:\", indV, \"n\")\ndrop <- F # to do with \"lasso\"\nk <- 0\n### Main loop for path\nwhile(k < max.steps) {\nk <- k + 1\n#cat(\"step:\",k,\"n\")\n### Check how far to go in u\n### Consider a point hits elbow\ngam <- u0 + x[-indE,indV,drop=F] %*% u\ndelta1 <- residual[-indE]/gam\nif (length(delta1[delta1<=0])==length(delta1)){\ndelta <- Inf\n} else {\ndelta.add <- delta1[delta1>0]\ndelta <- min(delta.add[delta.add > eps1],na.rm=T)\n}\n####For situation that beta may leave from V when k>1\nif(k > 1) {\ndelta2 <- -beta[k, indV]/u\nif (length(delta2[delta2<=0])==length(delta2)){\ntmpz.remove <- Inf\n} else {\ntmpz <- delta2[delta2>0]\ntmpz.remove <- min(tmpz[tmpz > eps1], na.rm=T)\n}\nif (tmpz.remove < delta) {\ndrop <- T\ndelta <- tmpz.remove\ndrop <- F\n}\n#cat(\"Move distance:\", delta, \"n\")\nif (delta == Inf) {\n#cat(\"Path stops here.n\")\nbreak\nsdistance[k+1] <- sdistance[k]+delta\nif (drop == T) {\ntmpdelta <- delta2[delta2 > eps1]\nj1 <- tmpind[which.min(tmpdelta)]\nj2 <- which(indV == j1)\n#cat(\"Var:\", j1, \"removedn\")\n\n### next point hits elbow when the path go in u, denote it as istar\ntmpind <- indn[-indE]\ntmpdelta <- delta1[delta1 > eps1]\nistar <- tmpind[which.min(tmpdelta)]\n#cat(\"Obs:\", istar, \"hits elbow \")\n#if(match(istar, indL, FALSE))\n#cat(\"from left.n\")\n#else\n#cat(\"from right.n\")\n###Update parameters\nbeta[k+1,] <- beta[k,]\nbeta[k+1,indV] <- beta[k+1,indV] + delta * u\nresidual[-indE] <- residual[-indE] - delta*gam\nElbow.list[[k+1]] <- indE\n### Calculate criteria SIC and GACV\nfit[k+1,] <- beta0[k+1] + beta[k+1,] %*% t(x)\ncheckf[k+1] <- pf(fit[k+1,], y, a)\ntrHat <- length(V.list[[k+1]])\nCgacv[k+1] <- checkf[k+1] / (n-trHat)\nCsic[k+1] <- log(checkf[k+1]/n) + (log(n)/(2*n))*trHat\nif (length(indV) != length(indE))\nwarning(\"No. var != No. obs at elbow.n\")\nif (sum(indL)+sum(indR) == 1 && drop == F) {\nlambda[k] <- 0\n#cat(\"No point on left or right.n\")\n### check which event occurs\n### Add a new variable to indV, we have already known istar hitting elbow\n### lambdvar and lambdaobs are -lambda\nif (length(indV) == m){\nlambdavar <- Inf\ninactive <- indm[ - indV ]\ntmpE <- indE\nif (drop == T){\nindV <- indV[ - j2 ]\ntmpE <- c(tmpE, istar)\nif(match(istar, indL, FALSE)){\ntmpL<-setdiff(indL,istar)\n} else {\ntmpR<-setdiff(indR,istar)\n}\ntmp <- rbind(c(0, sign(beta[k+1, indV])), cbind(1, x[tmpE, indV]))\ntmpb <- c(1, rep(0, length(indV)+1))\ntmplvar <- length(tmpb)\nuvar <- matrix(0, nrow=tmplvar, ncol=length(inactive))\nlambdavar <- rep(Inf, length(inactive))\n###Start j loop\nfor (j in 1:length(inactive)) {\njstar <- inactive[j]\ntmpA1 <- cbind(tmp, c(1, x[tmpE,jstar]))\ntmpqr <- qr(tmpA1)\nif (tmpqr$rank < tmplvar)\ntmplam1 <- Inf\nelse {\ntmpu1 <- qr.solve(tmpqr, tmpb)\nif (tmpu1[tmplvar] > 0) {\ntmpV <- c(indV, jstar)\ntmplam1 <- (1-a)*sum(cbind(rep(1,length(tmpL)), x[tmpL,tmpV,drop=F]) %*% tmpu1)-a*sum(cbind(rep(1,length(tmpR)), x[tmpR,tmpV,drop=F]) %*% tmpu1)\n}\ntmplam1 <- Inf\ntmpA2 <- cbind(tmp, c(-1, x[tmpE,jstar]))\ntmpqr <- qr(tmpA2)\ntmplam2 <- Inf\ntmpu2 <- qr.solve(tmpqr, tmpb)\nif (tmpu2[tmplvar] < 0) {\ntmplam2 <- (1-a)*sum(cbind(rep(1,length(tmpL)), x[tmpL,tmpV,drop=F]) %*% tmpu2)-a*sum(cbind(rep(1,length(tmpR)), x[tmpR,tmpV,drop=F]) %*% tmpu2)\ntmplam2 <- Inf\nif (tmplam1 == Inf && tmplam2 == Inf)\nlambdavar[j] <- Inf\nelse if (tmplam1 < tmplam2) {\nlambdavar[j] <- tmplam1\nelse if (tmplam1 > tmplam2) {\nlambdavar[j] <- tmplam2\nwarning(\"tmplam1 == tmplam2 n\")\n#cat(\"Tie in variable:\", jstar, \"n\")\n###end of j loop\n### Remove an observation from indE\ntmp <- rbind(c(0, sign(beta[k+1, indV])), cbind(1, x[tmpE, indV]))\ntmpb <- c(1, rep(0, length(indV)))\ntmplobs <- length(tmpb)\nuobs <- matrix(0, nrow=length(indV)+1, ncol=length(tmpE))\nlambdaobs <- rep(0, length(tmpE))\n###begin i loop\nfor (i in 1:length(tmpE)) {\ntmpA <- tmp[-(i+1),]\ntmpqr <- qr(tmpA)\nif (tmpqr$rank < tmplobs) {\ntmplam <- Inf\ntmpu <- qr.solve(tmpqr, tmpb)\nuobs[,i] <- tmpu\ntmplam <- (1-a)*sum(cbind(rep(1,length(tmpL)), x[tmpL,indV,drop=F]) %*% tmpu)-a*sum(cbind(rep(1,length(tmpR)), x[tmpR,indV,drop=F]) %*% tmpu)\ntmpi <- tmpE[i]\ntmpyf <- c(1, x[tmpi,indV,drop=F]) %*% tmpu\nif (tmpyf > 0) {\n###y_i move to Left\ntmplam <- tmplam + (1-a)*tmpyf\nleftobs[i] <- T\n###y_i move to Right\ntmplam <- tmplam - a*tmpyf\nlambdaobs[i] <- tmplam\n##end i loop\n### Compare var and obs\nlam1 <- min(lambdavar)\nif (lam1 < lam2 && lam1 < 0) {\n### one variable not in |V| is added into |V|\ntmpj <- which.min(lambdavar)\njstar <- inactive[tmpj]\nindV <- c(indV, jstar)\n### If drop==T, on change for E\nif (drop == F) {\nindE <- c(indE, istar)\nif(match(istar, indL, FALSE))\nindL <- setdiff(indL,istar)\nelse\nindR <- setdiff(indR,istar)\nu0 <- uvar[1,tmpj]\nu <- uvar[2:tmplvar,tmpj]\nlambda[k+1] <- lam1\n#cat(\"Variable:\", jstar, \"addedn\")\n} else if (lam2 < lam1 && lam2 < 0) {\n### remove a point from indE\n## istar stay on E, no change for V\n} else{\ntmpi <- which.min(lambdaobs)\n### update E\nistar <- tmpE[tmpi]\nu0 <- uobs[1,tmpi]\nu <- uobs[2:tmplobs,tmpi]\nlambda[k+1] <- lam2\n#cat(\"Observation:\", istar,\"removed \")\nif (leftobs[tmpi] == F) {\n#cat(\"to right.n\")\nindR <- c(indR,istar)\n#cat(\"to left.n\")\nindL <- c(indL,istar)\nindE <- tmpE[-tmpi]\n#cat(\"lam1:\", lam1, \"n\")\n#cat(\"No descent!n\")\n}\nif (abs(lambda[k+1]) < eps2) {\n#cat(\"Descent too small.n\")\ndrop <- F\n}\n###end of main loop for path\nbeta <- beta[seq(k+1),]\nlambda <- lambda[seq(k+1)]\nobject <- list(call = call, beta0 = beta0, = beta, Elbow = Elbow.list[seq(k+1)], V = V.list[seq(k+1)], lambda=lambda, s=sdistance[seq(k+1)], Csic=Csic[seq(k+1)],\nobject\n}\nqrL1Ini <- function(x, y, a, eps=10^(-10)) {\n### y in Real set\n### > eps is defined as nonzero\n### lambda is \"-lambda\"\nn <- dim(x)[1]\nyr <- sort(y)\n### Two cases\nquant <- yr[ floor(n*a)+1 ]\nindex <- match(quant, y)\nindE <- index\nindR <- seq(y)[y > y[index]]\nindV <- NULL\nbeta0<-quant\nbeta <- rep(0,m)\n###current f=beta0\nresidual <- y-beta0\ninactive <- indm\ntmpE <- indE\ntmpb <- c(1,0)\ntmplvar <- length(tmpb)\nuvar <- matrix(0, nrow=tmplvar, ncol=length(inactive))\nlambdavar <- rep(Inf, length(inactive))\n###beginning of loop j\nfor (j in 1:length(inactive)) {\njstar <- inactive[j]\ntmpA1 <- cbind(c(0,1), c(1, x[tmpE,jstar]))\ntmpqr <- qr(tmpA1)\nif (tmpqr$rank < tmplvar)\ntmplam1 <- Inf\nelse {\ntmpu1 <- qr.solve(tmpqr, tmpb)\nif (tmpu1[tmplvar] > 0) {\ntmpV <- jstar\ntmplam1 <- (1-a)*sum(cbind(rep(1,length(tmpL)), x[tmpL,tmpV,drop=F]) %*% tmpu1)-a*sum(cbind(rep(1,length(tmpR)), x[tmpR,tmpV,drop=F]) %*% tmpu1)\n} else tmplam1 <- Inf\ntmpA2 <- cbind(c(0,1), c(-1, x[tmpE,jstar]))\ntmpqr <- qr(tmpA2)\ntmplam2 <- Inf\nelse { tmpu2 <- qr.solve(tmpqr, tmpb)\nif (tmpu2[tmplvar] < 0) {\ntmpV <- jstar\ntmplam2 <- (1-a)*sum(cbind(rep(1,length(tmpL)), x[tmpL,tmpV,drop=F]) %*% tmpu2)-a*sum(cbind(rep(1,length(tmpR)), x[tmpR,tmpV,drop=F]) %*% tmpu2)\n}\nelse tmplam2 <- Inf\nif (tmplam1 == Inf && tmplam2 == Inf)\nlambdavar[j] <- Inf\nelse if (tmplam1 < tmplam2) {\nlambdavar[j] <- tmplam1\nelse if (tmplam1 > tmplam2) {\nlambdavar[j] <- tmplam2\nelse {\nwarning(\"tmplam1 == tmplam2 n\")\n#cat(\"Tie in variable:\", jstar, \"n\")\n###end of loop j\nlambda <- min(lambdavar)\ntmpj <- which.min(lambdavar)\njstar <- inactive[tmpj]\nindV <- jstar\nu0 <- uvar[1,tmpj]\nu <- uvar[2:tmplvar,tmpj]\n####################Not updat right now\n###Compute how far to go in u\n#gamL <- (1-a)*( u0 + x[indL,indV,drop=F] %*% u)\n#delta1L <- residual[indL]/gamL\n#delta1 <- c(delta1L,delta1R)\n#delta <- min(delta1[delta1 > eps], na.rm=T)\n#cat(\"Move distance:\", delta, \"n\")\n#beta[indV] <- delta * u\n#beta0 <- beta0 + delta * u0\n#residual[indL] <- residual[indL] - delta*gamL\n#residual [indE] <- 0\n########################\nreturn(list(beta=beta, beta0=beta0, u=u, u0=u0, quant=quant,lambda = lambda,\nindV=indV, residual=residual))\nqrL1.predict <- function(obj, x, y=NULL) {\n### obj: output of svmL1\n### y in {1,-1}\nN <- length(y)\nf <- obj$beta %*% t(x) + obj$beta0\npredict <- sign(f)\nif (is.null(y))\nreturn(list(f=f, error=NULL))\nerror <- apply(apply(predict, 1, FUN=\"!=\", y), 2, sum)/N\nreturn(list(f=f, error=error))\n###this defines the check function\npf <- function(beta0, y0, tau) {\ntmp <- y0 - beta0\nsum(tau*tmp[tmp>0]) - sum((1-tau)*tmp[tmp<=0])\n##Get path of coefficients\n\"plot.L1qr\" <-\nfunction(x, breaks = TRUE, eps = 1e-10, ...)\n{\nobject <- x\ncoef1 <- object$beta ### Get rid of many zero coefficients\n##coef1 <- scale(coef1, FALSE, 1/object$normx)\n##coef1[,4066] <- 0\nc1 <- drop(rep(1, nrow(coef1)) %*% abs(coef1))\nnonzeros <- c1 > eps\ncnums <- seq(nonzeros)[nonzeros]\ncoef1 <- coef1[, nonzeros]\ns1 <- apply(abs(coef1), 1, sum)\ns1 <- s1/max(s1)\n#xname<-\"|beta|/max|beta|\"\nxname<-\"s\"\nmatplot(s1, coef1, xlab = xname, ..., type = \"b\", lty=rep(1,dim(coef1)[2]),lwd=2,\npch = \"*\", ylab = \"Coefficients\", cex.lab=2, cex.axis=1.5)\n#title(\"Quantile Lasso Path\",line=2.5)\nabline(h = 0, lty = 3)\naxis(4, at = coef1[nrow(coef1), ], label = paste(cnums\n), cex = 2, cex.lab=2, cex.axis=1.5, adj = 1)\nif(breaks) {\n#axis(3, at = s1, labels = paste(seq(s1)-1),cex=.8)\nabline(v = s1,lwd=0.5,col=\"grey\")\ninvisible()\n", "output_sequence": "L1-norm quantile regression is applied"}, {"input_sequence": "# ---------------------------------------------------------------------\n# Paper: P. Burdejova and W.K. H\u00e4rdle\n# \"Dynamics of Tail Event Curves\"\n# ---------------------------------------------------------------------\n# Quantlet: DYTEC_temperature\n# Description: Do the simulation for DYTEC algorithm\n# Author: Petra Burdejova\nlibrary(expectreg)\nlibrary(fda)\nlibrary(matrixcalc)\nlibrary(gglasso)\nlibrary(doParallel)\nlibrary(foreach)\ninstall.packages(\"vars\")\nlibrary(\"vars\")\nwgt_fun <- function(tau,basis,mtx_hat,y_expl){\nwgt_vec <- y_expl < (basis %*% mtx_hat)\nwgt_vec <- abs(wgt_vec - tau)\nreturn(wgt_vec)\n}\nmy_tau=0.5\n#stat=1\nfor (stat in 1:1){\ndata0 = t(temp_list_stations[[stat]])\ndata0= #[1:20,30:150]\ndata0_exp = t(exp_list_stations[[stat]])\ndata0_exp= #[1:20,30:150]\nfcast_dytec = dytec_fcast(data0)\nfcast_var = var_fcast(data0_exp)\n} # end of for cycle stat\n#\n# plot(data0[53,])\n# lines(data0_exp[53,], col=\"red\")\n# lines(fcast_dytec)\nlines(unlist(fcast_dytec_sm$values), col=\"blue\")\ncl<-makeCluster(2)\nregisterDoParallel(cl)\nsim_output <-\nforeach (stat=c(1:159), .packages=c('fda','gglasso','expectreg','matrixcalc','vars')) %dopar% {\ndata0 = t(temp_list_stations[[stat]])\ndata0= #[1:20,30:150]\ndata0_exp = t(exp_list_stations[[stat]])\ndata0_exp= #[1:20,30:150]\nfcast_dytec = dytec_fcast(data0)\nfcast_dytec_sm <- expectreg.ls(fcast_dytec~rb(c(1:365),type=\"pspline\"), smooth=\"aic\",expectile=0.5)\nfcast_dytec_sm_disc <- unlist(fcast_dytec_sm$values)\nfcast_var = var_fcast(data0_exp)\ncompare_fcast(fcast_dytec_sm_disc,fcast_var,data0_exp[53,])\n}\nstopCluster(cl)\ncompare_fcast <- function(dytec_fun, var_fun, exp_fun){\np=length(exp_fun)\nmse_dytec <- sum(abs(exp_fun-dytec_fun)^2)/p\nreturn(list(mse_dytec,mse_var) )\n# mse_var <- sum(abs(data0_exp[53,]-fcast_var)^2)/365\n# mse_var\n# mse_dytec\nvar_fcast <- function(data_exp){\nd1=dim(data_exp)[1]\npca <- prcomp(data_exp[1:(d1-1),])\ncomponents<- pca$rotation[,1:3]\n#dim(components)\nprojection <- pca$x[,1:3]\n#dim(projection)\nmean <- apply(data_exp[1:(d1-1),], 2,mean)\n# var na y tj. projection\nscores_var <- VAR(projection, p = 3)\nscores_f <-(predict(scores_var,n.ahead=1, ci=0))\n# forecast(a_pred, h=1)$mean }\nLOC<-c(\"PC1\",\"PC2\",\"PC3\")\nscores_fcast <- sapply(scores_f $fcst[LOC], function (k) k[ , 1])\ne_fcast_var <- components%*% scores_fcast\nreturn(e_fcast_var+mean)\n} # end of var_fcast function\ndytec_fcast <- function(data){\nd1=dim(data)[1]\nnbas_time=round(d1+1)\n\n#--- create time basis\ntime_basis <- dytec_create_time_basis(d1,nbas_time)\ntime_basis_matrix <- time_basis[[1]]\n# if pca used, then nbas_space could change\nnbas_time <- time_basis[[2]]\nspace_basis <- dytec_create_space_basis(data,opt=1,nbas_space)\nspace_basis_matrix <- space_basis[[1]]\n# delete last year for modelling\nx_vec <- kronecker(t(space_basis_matrix), time_basis_matrix[1:d1-1,]) # delete last year for modelling\n#dim(x_vec) # 200 x 1400\ndata_vec <- vec(data[1:d1-1,]) # delete last year for modelling\n#dim(data_vec)\n#--- minimize for mean\nm1 <- gglasso(x=x_vec, y=data_vec, intercept=FALSE)\n#dim(m1$beta)\nbeta_hat_mtx <- matrix(m1$beta[,100],nrow=nbas_time, ncol=nbas_space)\nJ=dim(x_vec)[2]\n#my_tau=0.8\nwgt_old<- wgt_fun(my_tau,x_vec,m1$beta[,100], data_vec)\nw_iter_k <- 0\nmin_change <- length(wgt_old)\nrepeat {\nw_iter_k=\nprint(paste(\"tau=\", my_tau ,\"iter:\",w_iter_k))\nwgt_x_vec <- x_vec\nfor ( j in 1:J) {wgt_x_vec[,j] <- wgt_old * x_vec[,j] }\nmk <- gglasso(x= wgt_x_vec , y= wgt_old * data_vec, intercept=FALSE)\nbeta_k <- mk$beta[,100]\nwgt_new <- wgt_fun(my_tau, x_vec, beta_k, data_vec )\nprint(paste(\"diff.w.:\",sum(wgt_old != wgt_new)))\n# avoid cycling\nif (sum(wgt_old != wgt_new)>=min_change){break}\nif (sum(wgt_old != wgt_new)<min_change) {min_change <- sum(wgt_old != wgt_new)}\nwgt_old <- wgt_new\n} # end of repeat\nbeta_hat_mtx_k <- matrix(beta_k,nrow=nbas_time, ncol=nbas_space)\ny_fit <- (time_basis_matrix%*% beta_hat_mtx_k %*% space_basis_matrix)\nreturn(y_fit[d1,]) #return just forecast (made with d1-1 curves)\n} # end of dytec fcast\n", "output_sequence": "This is the application of  DYTEC algorithms to the temperature data"}, {"input_sequence": "# ---------------------------------------------------------------------\n# Paper: P. Burdejova and W.K. H\u00e4rdle\n# \"Dynamics of Tail Event Curves\"\n# ---------------------------------------------------------------------\n# Quantlet: DYTEC_hurricanes\n# Description: Apply DYTEC algorithm for hurricanes data\n# Author: Petra Burdejova\ndim(year_data_nex_mtx)\ndata = year_data_nex_mtx[16:65,801:1200]\nd1=dim(data)[1]\nnbas_time=round((d1+1)/5)\n\n#--- create time basis\ntime_basis <- dytec_create_time_basis(d1,nbas_time)\ntime_basis_matrix <- as.matrix(time_basis[[1]])\ndim(time_basis_matrix)\nmatplot(time_basis_matrix, type=\"l\")\nnbas_time <- time_basis[[2]]\nspace_basis <- dytec_create_space_basis(data,opt=1,nbas_space)\nspace_basis_matrix <- space_basis[[1]]\n# if pca used, then nbas_space could change\nnbas_space <- space_basis[[2]]\nspace_basis_matrix <- t(space_basis_matrix)\ndim(space_basis_matrix)\n#--- create space basis\nl <- prcomp(data)$sdev\ncumsum( l*l/ sum(l*l))\npcs <- prcomp(data)$rotation[,1:20]\nspace_basis_matrix <- pcs\nnbas_space=20\n# delete last year for modelling\nx_vec <- kronecker(t(space_basis_matrix), time_basis_matrix[1:d1-1,]) # delete last year for modelling\ndim(x_vec) # 200 x 1400\ndata_vec <- vec(data_used[1:d1-1,]) # delete last year for modelling\ndim(data_vec)\n#--- minimize for mean\nm1 <- gglasso(x=x_vec, y=data_vec, intercept=FALSE)\n#dim(m1$beta)\nbeta_hat_mtx <- matrix(m1$beta[,100],nrow=nbas_time, ncol=nbas_space)\n\nwgt_fun <- function(tau,basis,mtx_hat,y_expl){\nwgt_vec <- y_expl < (basis %*% mtx_hat)\nwgt_vec <- abs(wgt_vec - tau)\nreturn(wgt_vec)\n}\nJ=dim(x_vec)[2]\nmy_tau=0.8\nwgt_old<- wgt_fun(my_tau,x_vec,m1$beta[,100], data_vec)\nw_iter_k <- 0\nmin_change <- length(wgt_old)\nrepeat {\nw_iter_k=\nprint(paste(\"tau=\", my_tau ,\"iter:\",w_iter_k))\nwgt_x_vec <- x_vec\nfor ( j in 1:J) {wgt_x_vec[,j] <- wgt_old * x_vec[,j] }\nmk <- gglasso(x= wgt_x_vec , y= wgt_old * data_vec, intercept=FALSE)\nbeta_k <- mk$beta[,100]\nwgt_new <- wgt_fun(my_tau, x_vec, beta_k, data_vec )\nprint(paste(\"diff.w.:\",sum(wgt_old != wgt_new)))\n# avoid cycling\nif (sum(wgt_old != wgt_new)>=min_change){break}\nif (sum(wgt_old != wgt_new)<min_change) {min_change <- sum(wgt_old != wgt_new)}\nwgt_old <- wgt_new\n} # end of repeat\nbeta_hat_mtx_k <- matrix(beta_k,nrow=nbas_time, ncol=nbas_space)\ny_fit <- (time_basis_matrix%*% beta_hat_mtx_k %*% space_basis_matrix)\nreturn(y_fit[d1,]) #return just forecast (made with d1-1 curves)\n} # end of dytec fcast\nmyImagePlot(beta_hat_mtx)\n", "output_sequence": "This is the application of  DYTEC algorithms to the hurricanes data"}, {"input_sequence": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import svm\n#Read data\nspiral = np.loadtxt(\"spiral.txt\")\nx, y = spiral[:, :2], spiral[:, 2]\ndef plot_svm_decision(X, y, model_class, **model_params):\n#Fit model\nmodel = model_class(**model_params)\nmodel.fit(X, y)\n\n#Define grid\nh = .001\nx_min, x_max = x[:, 0].min() - 0.2, x[:, 0].max() + 0.2\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\nnp.arange(y_min, y_max, h))\n#Prediction on grid\nZ = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n#Contour + scatter plot\nplt.contourf(xx, yy, Z, alpha=0.4, cmap='coolwarm')\nplt.scatter(X[:, 0], X[:, 1], c=y, alpha=0.4, cmap='coolwarm')\nplt.gca().set_aspect('equal', adjustable='box')\nplt.savefig('spiral.png', transparent=True, dpi=200)\nreturn plt\nplot_svm_decision(x,y,svm.SVC,C=100,kernel='rbf',gamma=20)\n", "output_sequence": "Determines and plots the decision boundaries of a SVM classifier with RBF kernel for spiral data"}, {"input_sequence": "# clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# load data\nUSTF = read.table(\"XFGUSTF.dat\")\n# first observation point\nxmin = 1\n# last observation point\nxmax = nrow(USTF)\n# observation dummy\nx = (xmin:xmax)\n# plot of treasuries\nz1 = plot(x, USTF[xmin:xmax, 4], type = \"l\", col = \"black\", lwd = 1.5, xlab = \"Day\", ylab = \"Yield in %\", axes = FALSE, frame = TRUE,\nylim = c(2.5, 8), main = \"US Treasury Yields (3M, 1Y, 10Y)\") # 3 month yield\naxis(side = 1, at = seq(0, 2000, 500), labels = seq(0, 2000, 500))\nz2 = lines(x, USTF[xmin:xmax, 6], col = \"green\", lwd = 1.5) # 1 year yield\n", "output_sequence": "Plots US Treasury Yields (3M, 1Y, 2Y, 5Y, 10Y) from 0 to 2000 days"}, {"input_sequence": "compl = function (re, im){ # Complex array generation\nif(missing(re)){\nstop(\"compl: no composed object for real part\")\n}\nif(missing(im)){\nim = 0*(re<=Inf)\nif(nrow(matrix(re))!=nrow(matrix(im))){\nstop(\"compl: dim(re)<>dim(im)\")\nz = list()\nz$re = re\nreturn(z)\n}\ncmul = function(x, y){ # Complex multiplication\nre = x$re*y$re - x$im*y$im\nz = list()\ncexp = function(x){ # Complex exponential\nre = exp(x$re) * cos(x$im)\ncsub = function(x, y){# Complex subtraction two arrays of complex numbers\nre = x$re - y$re\ncdiv = function(x, y) { # Complex division\nw = y$re^2 + y$im^2\nre = (x$re*y$re + x$im*y$im) / w\nz$im = im\ncln = function(x){ # Complex natural logarithm\nre = log(x$re^2+x$im^2) / 2\nim = atan2(x$im, x$re)\ncreal = function(z){ # Returning real part\nre = z$re\nreturn(re)\nStandardNormalCharf = function(t,l){ # Standard Normal Characteristic Function\ns2 = l$sigma^2\ntmp = compl(-0.5*s2*t$re,-0.5*s2*t$im + l$mu)\nr = cexp(cmul(tmp,t))\nreturn(r)\ngFourierInversion = function(N,K,dt,t0,x0,charf,l){ # generic function for density approximation\n\n# 1. form the grids:\ndx = (2*pi)/(N*dt)\nt = (0:(K-1))*dt\nif (t0 != 0) {\nt = t + dt/2\nt = compl(t,t*0)\nx = x0 + (0:(N-1))*dx\n# 2. do the FFT:\ntmp = charf(t,l)\nphi = matrix(1,N,2)\nphi[1:K,1] = tmp$re\ntmp = x0 * dt *(0:(N-1))\nphi = cmul(compl(phi[,1],phi[,2]),cexp(compl(tmp*0,-tmp)))\nphi = cbind(phi$re,phi$im)\nphitmp = complex(real=phi[,1],imaginary=phi[,2])\nninvfft = length(phitmp)\ny = fft(phitmp,inverse=T)#ninvfft\ny = compl(Re(y),Im(y))\nif (t0 != 0){\ntmp = x*dt/2\ny = cmul(y,cexp(compl(tmp*0,-tmp)))\n# 3. rescale:\nif (t0 == 0){\nr = dt * ( creal(y) - 0.5*creal(charf(list(re=0,im=0),l)) )/ pi\n}else{\nr = dt * creal(y)/pi\n}\nVaRcgfDG = function(t,par){ # cumulant generating function (cgf) for the\n# class of quadratic forms of Gaussian vectors.\n\ns = compl(par$theta*t$re, par$theta*t$im)\ni = 1\nm = length(par$lambda)\nwhile (i <= m){\n# 1-lambda*t:\nomlt = compl(1 - par$lambda[i] * t$re, -par$lambda[i] * t$im)\ntmp = cmul(t,t)\ntmp = compl(par$delta[i]^2 * tmp$re, par$delta[i]^2 * tmp$im)\ntmp = csub(tmp,cln(omlt))\ns = compl(s$re + 0.5*tmp$re, s$im + 0.5*tmp$im)\ni = i+1\nreturn(s)\nVaRcharfDG = function(t,par){# computes the characteristic function for the\n# class of quadratic forms of Gaussian vectors.\nt = compl(-t$im,t$re) # 1i*t\nr = cexp(VaRcgfDG(t,par))\nVaRcharfDGF2 = function(t, l){ # Fourier transform of an approximating Gaussian cdf\nmu = l$theta + 0.5*sum(l$lambda)\ns2 = sum(l$delta^2 + 0.5*l$lambda^2)\ntmp = compl(-0.5*s2*t$re,-0.5*s2*t$im + mu)\ntmp = cexp(cmul(tmp,t))\ntmp = csub(VaRcharfDG(t,l),tmp)\ntmp = cdiv(tmp,t)\nr = compl(-tmp$im,tmp$re)\nr$re= replace(r$re,r$re==\"NaN\",0)\nVaRcorrfDGF2 = function(x,l){ # cdf of normal approximation\nr = pnorm((x-mu)/sqrt(s2))\nreturn(r)\nVaRcdfDG = function(l,N,K,dt){ # approximates the cdf for the class of quadratic forms\n# Gaussian vectors\ndx = 2*pi/(N*dt)\ny = gFourierInversion(N,K,dt,dt/2,x0,VaRcharfDGF2,l)\nx = x0 + (0:(N-1))*dx\ny = y + VaRcorrfDGF2(x,l)\ny[x<=0] = 0 # correct for misspecification\nr = list(x=x,y=y)\ncdf2quant = function(a,l){ # compute the quantile from a CDF on a given grid\n# robust version:\nx = l$x\nleft = 1\nright = length(y)\n# deal with extreme cases:\nif( a <= y[left] ){\nq = x[left]\n}else if( a >= y[right] ){\nq = x[right]\n}else{\nidx = 1:length(y)\nleft = max(idx[which(y <= a)])\nif ( left < right ){\nw = (y[right]-a)/(y[right]-y[left])\nq = w*x[left] + (1-w)*x[right]\n}else{\nq = 0.5*(x[left]+x[right])\n}\nreturn(q)\n#################### MAIN FUNCTION ####################\nVaRqDG = function(a,par,N,K,dt){\nr = VaRcdfDG(par,N,K,dt)\nq = cdf2quant(a,r)\nreturn(q)\n##################### TEST FUNCTION ######################\ntheta = 0\ndelta = c(0)\nlambda = c(1.4142)\npar = list(theta=theta,delta=delta,lambda=lambda)\nVaRqDG(a=0.95,par=par,N=512,K=512,dt=0.1)\nqnorm(0.95)\n", "output_sequence": "Computes the a-quantile for the class of quadratic forms of Gaussian vectors uses Fourier inversion to approximate the cumulative distribution function (CDF)."}, {"input_sequence": "compl = function(re, im) {\n# Complex array generation\nif (missing(re)) {\nstop(\"compl: no composed object for real part\")\n}\nif (missing(im)) {\nim = 0 * (re <= Inf)\nif (nrow(matrix(re)) != nrow(matrix(im))) {\nstop(\"compl: dim(re)<>dim(im)\")\nz = list()\nz$re = re\nreturn(z)\n}\ncexp = function(x) {\n# Complex exponential\nre = exp(x$re) * cos(x$im)\ncmul = function(x, y) {\n# Complex multiplication\nre = x$re * y$re - x$im * y$im\ncsub = function(x, y) {\n# Complex subtraction two arrays of complex numbers\nre = x$re - y$re\ncdiv = function(x, y) {\n# Complex division\nw = y$re^2 + y$im^2\nre = (x$re * y$re + x$im * y$im)/w\ncln = function(x) {\n# Complex natural logarithm\nre = log(x$re^2 + x$im^2)/2\nim = atan2(x$im, x$re)\nVaRcgfDG = function(t, par) {\n# cumulant generating function (cgf) for the class of quadratic forms of Gaussian vectors.\n\ns = compl(par$theta * t$re, par$theta * t$im)\ni = 1\nm = length(par$lambda)\nwhile (i <= m) {\n# 1-lambda*t:\nomlt = compl(1 - par$lambda[i] * t$re, -par$lambda[i] * t$im)\ntmp = cmul(t, t)\ntmp = compl(par$delta[i]^2 * tmp$re, par$delta[i]^2 * tmp$im)\ntmp = csub(tmp, cln(omlt))\ns = compl(s$re + 0.5 * tmp$re, s$im + 0.5 * tmp$im)\ni = i + 1\nreturn(s)\nVaRcharfDG = function(t, par) {\n# computes the characteristic function for the class of quadratic forms of Gaussian vectors.\nt = compl(-t$im, t$re) # 1i*t\nr = cexp(VaRcgfDG(t, par))\n############################ Main Program ############################\nVaRcharfDGF2 = function(t, l) {\nmu = l$theta + 0.5 * sum(l$lambda)\ns2 = sum(l$delta^2 + 0.5 * l$lambda^2)\ntmp = compl(-0.5 * s2 * t$re, -0.5 * s2 * t$im + mu)\ntmp = cexp(cmul(tmp, t))\ntmp = csub(VaRcharfDG(t, l), tmp)\ntmp = cdiv(tmp, t)\nr = compl(-tmp$im, tmp$re)\nr$re = replace(r$re, r$re == \"NaN\", 0)\nreturn(r)\n\n", "output_sequence": "Computes the a-quantile for the class of quadratic forms of Gaussian vectors uses Fourier inversion to approximate the cumulative distribution function (CDF)."}, {"input_sequence": "##################### SUBROUTINES ####################\ncompl = function (re, im){ # Complex array generation\nif(missing(re)){\nstop(\"compl: no composed object for real part\")\n}\nif(missing(im)){\nim = 0*(re<=Inf)\nif(nrow(matrix(re))!=nrow(matrix(im))){\nstop(\"compl: dim(re)<>dim(im)\")\nz = list()\nz$re = re\nreturn(z)\n}\ncmul = function(x, y){ # Complex multiplication\nre = x$re*y$re - x$im*y$im\nz$im = im\n\ncexp = function(x){ # Complex exponential\nre = exp(x$re) * cos(x$im)\ncsub = function(x, y){# Complex subtraction two arrays of complex numbers\nre = x$re - y$re\nz = list()\nz$re = re\nreturn(z)\ncdiv = function(x, y) { # Complex division\nw = y$re^2 + y$im^2\nre = (x$re*y$re + x$im*y$im) / w\ncln = function(x){ # Complex natural logarithm\nre = log(x$re^2+x$im^2) / 2\nim = atan2(x$im, x$re)\ncreal = function(z){ # Returning real part\nre = z$re\nreturn(re)\nStandardNormalCharf = function(t,l){ # Standard Normal Characteristic Function\ns2 = l$sigma^2\ntmp = compl(-0.5*s2*t$re,-0.5*s2*t$im + l$mu)\nr = cexp(cmul(tmp,t))\nreturn(r)\ngFourierInversion = function(N,K,dt,t0,x0,charf,l){ # generic function for density approximation\n# 1. form the grids:\ndx = (2*pi)/(N*dt)\nt = (0:(K-1))*dt\nif (t0 != 0) {\nt = t + dt/2\nt = compl(t,t*0)\nx = x0 + (0:(N-1))*dx\n# 2. do the FFT:\ntmp = charf(t,l)\nphi = matrix(1,N,2)\nphi[1:K,1] = tmp$re\ntmp = x0 * dt *(0:(N-1))\nphi = cmul(compl(phi[,1],phi[,2]),cexp(compl(tmp*0,-tmp)))\nphi = cbind(phi$re,phi$im)\nphitmp = complex(real=phi[,1],imaginary=phi[,2])\nninvfft = length(phitmp)\ny = fft(phitmp,inverse=T)#ninvfft\ny = compl(Re(y),Im(y))\nif (t0 != 0){\ntmp = x*dt/2\ny = cmul(y,cexp(compl(tmp*0,-tmp)))\n# 3. rescale:\nif (t0 == 0){\nr = dt * ( creal(y) - 0.5*creal(charf(list(re=0,im=0),l)) )/ pi\n}else{\nr = dt * creal(y)/pi\nVaRcgfDG = function(t,par){ # cumulant generating function (cgf) for the\n# class of quadratic forms of Gaussian vectors.\ns = compl(par$theta*t$re, par$theta*t$im)\ni = 1\nm = length(par$lambda)\nwhile (i <= m){\n# 1-lambda*t:\nomlt = compl(1 - par$lambda[i] * t$re, -par$lambda[i] * t$im)\ntmp = cmul(t,t)\ntmp = compl(par$delta[i]^2 * tmp$re, par$delta[i]^2 * tmp$im)\ntmp = csub(tmp,cln(omlt))\ns = compl(s$re + 0.5*tmp$re, s$im + 0.5*tmp$im)\ni = i+1\nreturn(s)\nVaRcharfDG = function(t,par){# computes the characteristic function for the\nt = compl(-t$im,t$re) # 1i*t\nr = cexp(VaRcgfDG(t,par))\nVaRcharfDGF2 = function(t, l){ # Fourier transform of an approximating Gaussian cdf\nmu = l$theta + 0.5*sum(l$lambda)\ns2 = sum(l$delta^2 + 0.5*l$lambda^2)\ntmp = compl(-0.5*s2*t$re,-0.5*s2*t$im + mu)\ntmp = cexp(cmul(tmp,t))\ntmp = csub(VaRcharfDG(t,l),tmp)\ntmp = cdiv(tmp,t)\nr = compl(-tmp$im,tmp$re)\nr$re = replace(r$re,r$re==\"NaN\",0)\nVaRcorrfDGF2 = function(x,l){ # cdf of normal approximation\nmu = l$theta + 0.5*sum(l$lambda)\ns2 = sum(l$delta^2 + 0.5*l$lambda^2)\nr = pnorm((x-mu)/sqrt(s2))\nVaRcdfDG = function(l,N,K,dt){ # approximates the cdf for the class of quadratic forms\n# Gaussian vectors\ndx = 2*pi/(N*dt)\ny = gFourierInversion(N,K,dt,dt/2,x0,VaRcharfDGF2,l)\nx = x0 + (0:(N-1))*dx\ny = y + VaRcorrfDGF2(x,l)\ny[x<=0] = 0 # correct for misspecification\nr = list(x=x,y=y)\ncdf2quant = function(a,l){ # compute the quantile from a CDF on a given grid\n# robust version:\nx = l$x\nleft = 1\nright = length(y)\n# deal with extreme cases:\nif( a <= y[left] ){\nq = x[left]\n}else if( a >= y[right] ){\nq = x[right]\n}else{\nidx = 1:length(y)\nleft = max(idx[which(y <= a)])\nif ( left < right ){\nw = (y[right]-a)/(y[right]-y[left])\nq = w*x[left] + (1-w)*x[right]\n}else{\nq = 0.5*(x[left]+x[right])\n}\nreturn(q)\nVaRqDG = function(a,par,N,K,dt){\nr = VaRcdfDG(par,N,K,dt)\nq = cdf2quant(a,r)\nreturn(q)\n#################### MAIN FUNCTIONS ####################\nXFGqDGtest = function(n,N,dt){\n# di = createdisplay(1,2)\nlambdav = -sqrt(2) + (0:(n-1))*2*sqrt(2)/(n-1)\nq = lambdav\nwhile ( i <= n ){\nq[i] = VaRqDG(0.01,XFGdg11par(lambdav[i]),N,N,dt)\nz = cbind(lambdav,q)\nplot(z,type=\"l\",col=\"blue3\",lwd=2,xlab=\"X\",ylab=\"Y\")\nphi = -pi/2 + (0:(n-1))*pi/(n-1)\nq = phi\nwhile (i <= n){\nq[i] = VaRqDG(0.01,XFGdg12par(phi[i]),N,N,dt)\nz = cbind(phi,q)\ndev.new()\nXFGdg11par = function(la,de,th){\nlambda = la\nif (missing(de)){\ndelta = c(sqrt(max(c(0,1-lambda^2/2))))\ndelta = de\nif (missing(th)){\ntheta = -0.5*lambda\nSigma = diag(rep(1,1))\nGamma = lambda*diag(c(1))\nDelta = delta\nreturn(list(Sigma=Sigma,Gamma=Gamma,Delta=Delta,theta=theta,lambda=lambda,delta=delta))\nXFGdg12par = function(phi){ #XFG\nsr2 = sqrt(2)\nphi = phi + pi/4\nlambda = sr2* c(sin(phi),-cos(phi))\ndelta = c(0,0)\ntheta = -0.5 * sum(lambda)\nSigma = diag(rep(1,2))\nGamma = lambda*diag(c(1))\nDelta = delta\nreturn(list(Sigma=Sigma,Gamma=Gamma,Delta=Delta,theta=theta,lambda=lambda,delta=delta))\nXFGqDGtest(n=33,N=1024,dt=0.2)\n", "output_sequence": "XFGqDGtest plots the 1%-quantile from the two one-parametric sub-families of the class of quadratic forms of Gaussian vectors, as defined by the functions XFGdg11par and XFGdg12par"}, {"input_sequence": "import os\nfrom selenium import webdriver\nfrom selenium.webdriver.common.keys import Keys\nfrom bs4 import BeautifulSoup\nfrom PIL import Image\nimport requests\nimport datetime\nimport re\nimport urllib\nfrom urllib.request import Request, urlopen\nimport pandas as pd\nimport numpy as np\nurl = \"https://www.larvalabs.com/cryptopunks/bids\"\nweb_r = requests.get(url)\nwebsoup = BeautifulSoup(web_r.text, \"html.parser\")\n#req = Request(url,headers=hdr)\n#web_r = urlopen(req)\nfinal_df = pd.DataFrame()\nfor titlesoup in websoup.findAll(\"div\",{\"class\":\"col-md-1 col-sm-2 punk-image-container-dense\"}):\nfor element in titlesoup.findAll(\"a\"):\nfinal_df= final_df.append([element.get(\"title\")])\n", "output_sequence": "Clustering 9999 cryptopunks into 200 clusters based on Hierarchical Clustering"}, {"input_sequence": "# for loading/processing the images\nfrom keras.preprocessing.image import load_img\nfrom keras.applications.vgg16 import preprocess_input\n# models\nfrom keras.applications.vgg16 import VGG16\nfrom keras.models import Model\n# clustering and dimension reduction\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\n# for everything else\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randint\nimport pandas as pd\nimport pickle\npath = r\"C:\\Users\\Acer\\Git\\deda_punks\\Crypto_punks\\bids\"\n# change the working directory to the path where the images are located\nos.chdir(path)\n# this list holds all the image filename\npunks = []\n# creates a ScandirIterator aliased as files\nwith os.scandir(path) as files:\n# loops through each file in the directory\nfor file in files:\nif file.name.endswith('.png'):\n# adds only the image files to the flowers list\n# load model\nmodel = VGG16()\n# remove the output layer\nfor i in range(len(model.layers)):\nlayer = model.layers[i]\n\nif 'conv' not in layer.name:\ncontinue\nmodel2 = VGG16()\nmodel2 = Model(inputs=model2.inputs, outputs=model2.layers[1].output)\nimg = np.array(img)\n# reshape the data for the model reshape(num_of_samples, dim 1, dim 2, channels)\nreshaped_img = img.reshape(1,224,224,3)\n# prepare image for model\nimgx = preprocess_input(reshaped_img)\n# get the feature vector\nsquare = 8\nix = 1\nfor _ in range(square):\nax = plt.subplot(square, square, ix)\nax.set_xticks([])\nplt.imshow(features_1[0,:,:,ix-1])\nix +=1\nmodel2 = Model(inputs=model2.inputs, outputs=model2.layers[17].output)\ndef extract_features(file, model):\n# load the image as a 224x224 array\nimg = load_img(file, target_size=(224,224))\n# convert from 'PIL.Image.Image' to numpy array\nimg = np.array(img)\n# reshape the data for the model reshape(num_of_samples, dim 1, dim 2, channels)\nreshaped_img = img.reshape(1,224,224,3)\n# prepare image for model\nimgx = preprocess_input(reshaped_img)\n# get the feature vector\nfeatures = model.predict(imgx, use_multiprocessing=True)\ndata = {}\np = r\"C:\\Users\\Acer\\Git\\deda_punks\\punks_features_.pkl\"\n# lop through each image in the dataset\nfor punk in tqdm.tqdm(punks):\n# try to extract the features and update the dictionary\ntry:\nfeat = extract_features(punk,model)\ndata[punk] = feat\n# if something fails, save the extracted features as a pickle file (optional)\nexcept:\nwith open(p,'wb') as file:\nfrom matplotlib import pyplot as plt\nplt.imshow(img2[:,:,0], interpolation=None)\n#plt.show()\nplt.imsave(r'C:\\Users\\Acer\\Git\\deda_punks\\R__.png', img2[:,:,0])\nplt.imshow(img2[:,:,1], interpolation=None)\nplt.imsave(r'C:\\Users\\Acer\\Git\\deda_punks\\G__.png', img2[:,:,1])\nplt.imshow(img2[:,:,2], interpolation=None)\nplt.imsave(r'C:\\Users\\Acer\\Git\\deda_punks\\B__.png', img2[:,:,2])\n# get a list of the filenames\nfilenames = np.array(list(data.keys()))\n# get a list of just the features\nfeat = np.array(list(data.values()))\nprint(feat.shape)\n# reshape so that there are 210 samples of 4096 vectors\nfeat = feat.reshape(-1,4096)\npca = PCA(n_components=150, random_state=22)\npca.fit(feat)\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')\nplt.savefig(r\"C:\\Users\\Acer\\Git\\deda_punks\\PCA.png\")\nclustered = AgglomerativeClustering(n_clusters=200, linkage = \"average\")\nclustered = SpectralClustering(n_clusters=200)\nclustered = KMeans(n_clusters=200, n_jobs=-1)\n# holds the cluster id and the images { id: [images] }\ndef post_process_clustering (model):\ngroups = {}\nfor file, cluster in zip(filenames,model.labels_):\nif cluster not in groups.keys():\ngroups[cluster] = []\nelse:\nfrom PIL import Image\nfor i in range(len(groups)):\n#print(\"Group\", i)\nfor mini_punk in groups[i]:\nimg = Image.open(r\"C:\\Users\\Acer\\Git\\deda_punks\\Crypto_punks\\bids\\\\\"+mini_punk)\n#display(img)\nif mini_punk == \"Punk 0310.png\":\ndisplay(img)\nprint(\"Group\", i)\nfor mini_punk in groups[i-1]:\nprint(\"Group\", i-1)\nfor mini_punk in groups[new_group]:\nimg = Image.open(r\"C:\\Users\\Acer\\Git\\deda_punks\\Crypto_punks\\bids\\\\\"+mini_punk)\n", "output_sequence": "Clustering 9999 cryptopunks into 200 clusters based on Hierarchical Clustering"}, {"input_sequence": "import os\nimport math\n\ndef initColors():\nos.system(\"cls\")\ndef findLargestElement(rows, cols, lengthArray, matrix):\n# Loop through each row\nfor i in range(rows):\n# Loop through each column\nfor j in range(cols):\nlengthArray.append(len(str(matrix[i][j])))\n# Sort the length matrix so that we can find the element with the longest length\nlengthArray.sort()\n# Store that length\nlargestElementLength = lengthArray[-1]\nreturn largestElementLength\ndef createMatrix(rows, cols, matrixToWorkOn, matrix):\nfor i in range(rows):\n# Append a row to matrixToWorkOn for each row in the matrix passed in\nmatrixToWorkOn.append([])\n# Add a each column of the current row (in string form) to matrixToWorkOn\nmatrixToWorkOn[i].append(str(matrix[i][j]))\ndef makeRows(rows, cols, largestElementLength, rowLength, matrixToWorkOn, finalTable, color):\nfor i in range(rows):\n# Initialize the row that will we work on currently as a blank string\ncurrentRow = \"\"\n# Loop trhough each column\n# If we are using colors then do the same thing but as without (below)\nif ((color != None) and (j == 0 or i == 0)):\n# Only add color if it is in the first column or first row\ncurrentEl = \" + \"\\033[38;2;\" + str(color[0]) + \";\" + str(color[1]) + \";\" + str(color[2]) +\"m\" + matrixToWorkOn[i][j] + \"\\033[0m\"\n# If we are not using colors (or j != 0 or i != 0) just add a space and the element that should be in that position to a variable which will store the current element to work on\nelse:\ncurrentEl = \" + matrixToWorkOn[i][j]\n# If the raw element is less than the largest length of a raw element (raw element is just the unformatted element passed in)\nif (largestElementLength != len(matrixToWorkOn[i][j])):\n# If we are using colors then add the amount of spaces that is equal to the difference of the largest element length and the current element (minus the length that is added for the color)\n# * The plus two here comes from the one space we would normally need and the fact that we need to account for a space that tbe current element already has\nif (color != None):\nif (j == 0 or i == 0):\ncurrentEl = currentEl + \" * (largestElementLength - (len(currentEl) - len(\"\\033[38;2;\" + str(color[0]) + \";\" + str(color[1]) + \";\" + str(color[2]) + \"m\" + \"\\033[0m\")) + 2) + \"|\"\n# If it is not the first column or first row than it doesn't need to subtract the color length\nelse:\ncurrentEl = currentEl + \" * (largestElementLength - len(currentEl) + 2) + \"|\"\n# If we are not using color just do the same thing as above when we were using colors for when the row or column is not the first each time\nelse:\ncurrentEl = currentEl + \" * (largestElementLength - len(currentEl) + 2) + \"|\"\n# If the raw element length us equal to the largest length of a raw element then we don't need to add extra spaces\ncurrentEl = currentEl + \" + \"|\"\n# Now add the current element to the row that we are working on\ncurrentRow += currentEl\n# When the entire row that we were working on is done add it as a row to the final table that we will print\nfinalTable.append(\"|\" + currentRow)\n# If we are using color then length of each row (each row will end up being the same length) equals to the length of the last row (again each row will end up being the same length) minus the length the color will inevitably add if we are using colors\nif (color != None):\nrowLength = len(currentRow) - len(\"\\033[38;2;\" + str(color[0]) + \";\" + str(color[1]) + \";\" + str(color[2]) + \"m\" + \"\\033[0m\")\n# Otherwise (we are not using colors) the length of each row will be equal to the length of the last row (each row will end up being the same length)\nelse:\nrowLength = len(currentRow)\n\nreturn rowLength\ndef createWrappingRows(rowLength, finalTable):\n# Here we deal with the rows that will go on the top and bottom of the table (look like -> +--------------+), we start by initializing an empty string\nwrappingRows = \"\"\n# Then for the length of each row minus one (have to account for the plus that comes at the end, not minus two because rowLength doesn't include the | at the beginning) we add a -\nfor i in range(rowLength - 1):\nwrappingRows += \"-\"\n# Add a plus at the beginning\nwrappingRows = \"+\" + wrappingRows\n# Add a plus at the end\nwrappingRows += \"+\"\n# Add the two wrapping rows\nfinalTable.insert(0, wrappingRows)\ndef createRowUnderFields(largestElementLength, cols, finalTable):\n# Initialize the row that will be created\nrowUnderFields = \"\"\n# Loop through each column\nfor j in range(cols):\n# For each column add a plus\ncurrentElUnderField = \"+\"\n# Then add an amount of -'s equal to the length of largest raw element and 2 for the 2 spaces that will be either side the element\ncurrentElUnderField = currentElUnderField + \"-\" * (largestElementLength + 2)\n# Then add the current element (there will be one for each column) to the final row that will be under the fields\nrowUnderFields += currentElUnderField\n# Add a final plus at the end of the row\nrowUnderFields += \"+\"\n# Insert this row under the first row\nfinalTable.insert(2, rowUnderFields)\ndef printRowsInTable(finalTable):\n# For each row - print it\nfor row in finalTable:\nprint(row)\ndef printTable(matrix, useFieldNames=False, color=None):\n# Rows equal amount of lists inside greater list\nrows = len(matrix)\n# Cols equal amount of elements inside each list\ncols = len(matrix[0])\n# This is the array to sort the length of each element\nlengthArray = []\n# This is the variable to store the vakye of the largest length of any element\nlargestElementLength = None\n#This is the variable that will store the length of each row\nrowLength = None\n# This is the matrix that we will work with throughout this program (main difference between matrix passed in and this matrix is that the matrix that is passed in doesn't always have elements which are all strings)\nmatrixToWorkOn = []\n#This the list in which each row will be one of the final table to be printed\nfinalTable = []\nlargestElementLength = findLargestElement(rows, cols, lengthArray, matrix)\ncreateMatrix(rows, cols, matrixToWorkOn, matrix)\nrowLength = makeRows(rows, cols, largestElementLength, rowLength, matrixToWorkOn, finalTable, color)\ncreateWrappingRows(rowLength, finalTable)\nif (useFieldNames):\ncreateRowUnderFields(largestElementLength, cols, finalTable)\nprintRowsInTable(finalTable)\n", "output_sequence": "Clustering 9999 cryptopunks into 200 clusters based on Hierarchical Clustering"}, {"input_sequence": "<b>Content</b>\n<p>\nThis anonymized data set is a transaction graph collected from the Bitcoin blockchain. A node in the graph represents a transaction, an edge can be viewed as a flow of Bitcoins between one transaction and the other. Each node has 166 features and has been labeled as being created by a \"licit\", or \"unknown\" entity.\n</p>\n<b>Nodes and edges</b>\nThe graph is made of 203,769 nodes and 234,355 edges. Two percent (4,545) of the nodes are labelled class1 (illicit). Twenty-one percent (42,019) are labelled class2 (licit). The remaining transactions are not labelled with regard to licit versus illicit.\n<b>Features</b>\nThere are 166 features associated with each node. Due to intellectual property issues, we cannot provide an exact description of all the features in the dataset. There is a time step associated to each node, representing a measure of the time when a transaction was broadcasted to the Bitcoin network. The time steps, running from 1 to 49, are evenly spaced with an interval of about two weeks. Each time step contains a single connected component of transactions that appeared on the blockchain within less than three hours between each other; are no edges connecting the different time steps.\nThe first 94 features represent local information about the transaction \u2013 including the time step described above, number of inputs/outputs, transaction fee, output volume and aggregated figures such as average BTC received (spent) by the inputs/outputs and average number of incoming (outgoing) transactions associated with the inputs/outputs. The remaining 72 features are aggregated features, obtained using transaction information one-hop backward/forward from the center node - giving the maximum, standard deviation and correlation coefficients of the neighbour transactions for the same information data (number of inputs/outputs, transaction fee, etc.).\n<b>License</b>\n<p>Summary</p>\n<ul>\n<li>Licit: exchanges, wallet providers, miners, licit services, etc.\n<li>Ilicit: scams, malware, terrorist, organization, ransomware, Ponzi shcemes, etc. A given transaction is licit if the entity that generated it was licit.\n<li>Nodes and Edges: 203,769 node transactions and 234,355 directed edge payments flows. 2% are ilicit (Class 1), 21% are licit (Class 2)\n<li>Features: Each node has associated 166 features. 94 represent local information (timestep, number of inputs/outputs, transaction fee, output volume and aggregated figures) and 72 features represent aggregated features (obtained by aggregating transaction information such as maximum, standard deviation, correlation coefficients of neighbor transactions).\n<li>Temporal Information: A time step is associated with each node, representing an stimated of the time when the transaction is confirmed. There are 49 distinct timesteps evenly spaced with an interval of 2 weeks.\n<li>elliptic_txs_edgelist.csv contains graph edges information;\n<li>elliptic_txs_classes.csv contains information about legality of transactions;\n# Create a global variable to idicate whether the notebook is run in Colab\nimport sys\nIN_COLAB = 'google.colab' in sys.modules\n# Configure variables pointing to directories and stored files\nif IN_COLAB:\nfrom google.colab import drive\ndrive.mount('/content/gdrive')\nos.chdir('/content/gdrive/My Drive/DEDA')\nprint(os.getcwd())\n!ls\nelse:\nprint('Please select your local path (place datasets in same folder like notebook)')\nos.chdir(r\"C:\\Users\\YourPath\")\nfrom pathlib import Path\nimport math\nimport pandas as pd\nimport numpy as np\nimport pickle\nfrom pprint import pprint\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\npd.options.mode.chained_assignment = None # removes warning messages\n#pd.set_option('display.max_rows', None)\n# Setting seed for results to be re-producable\nmain_classes = pd.read_csv('elliptic_txs_classes.csv')\n#change class unknown to 3\ngroup_class = main_classes.groupby('class').count()\n# renaming columns\n# merge with classes\nselected_ids = main_combined.loc[(main_combined['class'] != 3), 'txId']\nmain_edges_selected = main_edgelist.loc[main_edgelist['txId1'].isin(selected_ids)]\n# Merge Class and features\nmain_combined_selected = pd.merge(main_classes_selected, main_features_selected )\nX = main_combined_selected.drop(columns=['txId', 'class', 'time step']) # drop class, text id and time step\ny = main_combined_selected[['class']]\n# in this case, class 2 corresponds to licit transactions, we chang this to 0 as our interest is the ilicit transactions\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, roc_curve, f1_score, auc\nrfc = RandomForestClassifier().fit(X_train,y_train)\nprec,rec,f1,num = precision_recall_fscore_support(y_test, rfc_predict)\nprint(\"Random Forest Classifier\")\nprint(\"Precision:%.3f \\nRecall:%.3f \\nF1 Score:%.3f\"%(prec[1],rec[1],f1[1]))\nprint(\"=== Confusion Matrix ===\")\nprint(confusion_matrix(y_test, rfc_predict))\nprint('\\n')\nprint(\"=== Classification Report ===\")\nprint(classification_report(y_test, rfc_predict))\nprint(\"=== AUC Score ===\")\nrfc_predict = rfc.predict_proba(X_test)[:,1]\nprint(metrics.roc_auc_score(y_test,rfc_predict))\n# Hyperparameter tuning for all models was not performed in this notebook due to time constraint\nfrom sklearn.model_selection import RandomizedSearchCV\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 1500, num = 4)]\n# Number of features to consider at every split\nmax_features = [5, 10, 15]\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 100, num = 10)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [5, 10, 30]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Create the random grid\nrandom_grid = {'n_estimators':\n'max_features':\n'min_samples_split':\n%%time\n###### WARNING: this might take a really long time depending on your CPU\n# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrfc_tuning = RandomForestClassifier()\n# Random search of parameters, using 3 fold cross validation,\n# search across 50 different combinations, and use all available cores\nrfc_tuning_random = RandomizedSearchCV(estimator = rfc_tuning, param_distributions = random_grid, n_iter = 50, cv = 3, verbose=1, random_state=666, n_jobs = -1)\n# Fit the random search model\nrfc_tuning_random.fit(X_train,y_train)\nrfc_tuning_random_predict = rfc_tuning_random.predict(X_test)\nprec,rec,f1,num = precision_recall_fscore_support(y_test, rfc_tuning_random_predict)\nxgb_model = XGBClassifier()\nxgb_model.fit(X_train, y_train)\n# XGBoost AUC score shows a different number from sklearn's roc_auc_score, find out why: https://github.com/dmlc/xgboost/issues/2064\n# A small clarification of the above: https://discuss.analyticsvidhya.com/t/what-is-the-difference-between-predict-and-predict-proba/67376/3\nprec,rec,f1,num = precision_recall_fscore_support(y_test, xgb_predict)\nprint(\"XGBoost Classifier\")\nprint(confusion_matrix(y_test, xgb_predict))\nxgb_predict = xgb_model.predict_proba(X_test)[:,1]\n# After finding the best parameters (which was not done in this notebook due to time constraints)\n# Aguide to do that can be found here: https://blog.cambridgespark.com/hyperparameter-tuning-in-xgboost-4ff9100a3b2f\n# For now, we will pretend to have found the optimal parameters and set them manually.\nxgb_tuned_model = XGBClassifier(silent=False,\nscale_pos_weight=1,\nlearning_rate=0.01,\ncolsample_bytree = 0.5,\nobjective='binary:logistic',\nn_estimators=1000,\neval_set = [(X_train, y_train), (X_test, y_test)]\neval_metric = [\"auc\"]\nprec,rec,f1,num = precision_recall_fscore_support(y_test, xgb_tuned_model_predict)\nprint(confusion_matrix(y_test, xgb_tuned_model_predict))\nxgb_tuned_model_predict = xgb_tuned_model.predict_proba(X_test)[:,1]\n#from tensorflow import keras # instead of importing the whole framework, we can import individual modules that we need\nfrom keras.models import Sequential\n#regularization\nfrom keras.layers import Dropout\n# define and fit the model\ndef get_model(X_train, y_train):\n# define model\nnn_model = Sequential()\nnn_model.add(Dense(150, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.01, l2=0.01), bias_regularizer=regularizers.l1_l2(l1=0.01, l2=0.01), input_shape=(165,)))\nnn_model.add(Dropout(0.20))\nnn_model.add(Dense(50, activation='relu'))\n# compile model\nnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n# fit model\nnn_model.fit(X_train, y_train, epochs=10, batch_size=10, verbose=1)\n# predict probabilities for test set\nyhat_probs = nn_model.predict(X_test, verbose=0)\n# predict crisp classes for test set\nyhat_classes = nn_model.predict_classes(X_test, verbose=0)\n# reduce to 1d array\nyhat_probs = yhat_probs[:, 0]\nyhat_classes = yhat_classes[:, 0]\n\n# accuracy: (tp + tn) / (p + n)\naccuracy = accuracy_score(y_test, yhat_classes)\nprint('Accuracy: %f' % accuracy)\n# precision tp / (tp + fp)\nprecision = precision_score(y_test, yhat_classes)\nprint('Precision: %f' % precision)\n# recall: tp / (tp + fn)\nrecall = recall_score(y_test, yhat_classes)\nprint('Recall: %f' % recall)\n# f1: 2 tp / (2 tp + fp + fn)\nf1 = f1_score(y_test, yhat_classes)\n# RF and XGBoost\nclassifiers = [rfc, xgb_tuned_model]\nax = plt.gca()\nfor i in classifiers:\n# Neural Network\nfalse_positive_rate, recall, thresholds = roc_curve(y_test, predictions_NN_prob)\nroc_auc = auc(false_positive_rate, recall)\nplt.figure()\nplt.title('Receiver Operating Characteristic (ROC)')\nplt.plot(false_positive_rate, recall, 'b', label = 'AUC = %0.3f' %roc_auc)\nplt.legend(loc='lower right')\nplt.plot([0,1], [0,1], 'r--')\nplt.xlim([0.0,1.0])\nplt.ylabel('Recall')\nplt.xlabel('Fall-out (1-Specificity)')\nsources\n<li>https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74<li>https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/random_forest_explained/.ipynb_checkpoints/Improving%20Random%20Forest%20Part%202-checkpoint.ipynb\n<li>https://www.kaggle.com/artgor/elliptic-data-eda\n<li>https://www.kaggle.com/smlopezza/elliptic-data-set-eda-graphs-random-forest\n<li> https://www.kaggle.com/georsara1/95-auc-score-in-train-sample-with-neural-nets</li>\n<li>https://towardsdatascience.com/fine-tuning-xgboost-in-python-like-a-boss-b4543ed8b1e</li>\n</ul>\n", "output_sequence": "Working on the Elliptic labeled Bitcoin transaction dataset, an attempt is made to detect illicit activities using different Machine Learning models"}, {"input_sequence": "# %% Setup\nimport os\nimport pandas as pd\nimport numpy as np\nimport time\nfrom bs4 import BeautifulSoup\nimport urllib.request\nfrom pdf2image import convert_from_path\nimport cv2\nimport xlsxwriter # required dependency\nimport pytesseract\n# Notes - OCR installation:\n# 1. tesseract must to be installed <-- download from https://github.com/UB-Mannheim/tesseract/wiki\n# 2. german training data must be added during installation\n# (can also manually be downloaded from https://github.com/tesseract-ocr/tessdata/blob/master/deu.traineddata\n# and be manually moved in folder: C:\\ProgramData\\Anaconda3\\envs\\*env_name*\\Library\\bin\\tessdata)\n# Set working directory to file location\nabspath = os.path.abspath(__file__)\ndname = os.path.dirname(abspath)\nos.chdir(dname)\n# %% Main\ndef main():\n# ***** 1) Scrape PDFs *****\n# parse the SNB website\nparser = 'html.parser' # or 'lxml' (preferred) or 'html5lib', if installed\nmain_url = 'https://www.snb.ch'\nresp = urllib.request.urlopen(main_url + '/en/iabout/monpol/id/monpol_current#')\nsoup = BeautifulSoup(resp, parser, from_encoding=resp.info().get_param('charset'))\n# extract all links\nlinks = []\nfor link in soup.find_all('a', href=True):\nlinks.append(link['href'])\n# only keep links with /en/mmr/reference/pre_\nlinks_lagebeurteilung = [main_url + s for s in links if \"/en/mmr/reference/pre_\" in s]\n# specify names of files\nnames = []\nfor i in links_lagebeurteilung:\ntest = str(i.split('/')[-1]) # only take the last part of url for the name\ntest = str(test.split('.')[-3]) # remove the points\ntest = test + '.pdf' # add pdf ending to name\nnames.append(test)\n# specifiy download path\npdf_dir = \"SNB_Lagebeurteilungen/\"\n# download the file to specific path\ndef downloadFile(url, fileName):\nwith open(pdf_dir + fileName, \"wb\") as file:\nresponse = requests.get(url)\nfile.write(response.content)\nfor idx, url in enumerate(links_lagebeurteilung):\nprint(\"downloading {}\".format(names[idx]))\ndownloadFile(url, names[idx])\n# ***** 2) Read PDFs (~1h runtime) *****\nstart = time.time()\n# i) specify cols for final df\ncolumns = ['date', 'text', 'pagenr', 'filename']\ndata = []\n# ii) loop through pdf files\nfor i, fname in enumerate(os.listdir(pdf_dir)):\nprint(\"Extracting text from '{}' ({}/{})\".format(fname, i + 1, len(os.listdir(pdf_dir))))\npath = pdf_dir + fname\nimgs = convert_from_path(path)\n# iii) loop page-images and apply OCR\nfor i, img in enumerate(imgs):\n# scale images to increase OCR accuracy\nheight, width = img.size\nimg = np.array(img)\nimg = cv2.resize(img, (3 * width, 3 * height), interpolation=cv2.INTER_LINEAR)\nimg = cv2.bitwise_not(img)\ntext = pytesseract.image_to_string(img, lang='deu') # requires installation as specified at start of file\ndate_str = fname[4:12] # cut 8 digit date\ndate = datetime.datetime.strptime(date_str, '%Y%m%d')\ndata.append([date, text, i, fname])\n# save final df\ndf = pd.DataFrame(data, columns=columns)\ndf.to_excel('articles_raw.xlsx', engine='xlsxwriter')\nprint(\n\"Saved text in DataFrame. Elapsed time: {}\".format(time.strftime(\"%Mm %Ss\", time.gmtime(time.time() - start))))\n# %% Run file\nif __name__ == '__main__':\nmain()\n", "output_sequence": "Scraping all the Swiss National Bank Monetary Policy Assessments from 2000 to 2020. Reading in the pdf files and save as excel file"}, {"input_sequence": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n#%%\nimport bs4 as bs\nimport datetime as dt\nimport os\nimport pandas as pd\nimport pandas_datareader.data as web\nimport pickle\nimport requests\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport pandas_datareader as dr\nfrom pandas_datareader import data\nfrom datetime import datetime\nimport cvxopt as opt\nfrom cvxopt import blas, solvers\nfrom scipy import ndimage\nfrom scipy.stats import kurtosis, skew\nfrom scipy.cluster import hierarchy\nfrom scipy.spatial import distance_matrix\nfrom sklearn import manifold, datasets\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.datasets.samples_generator import make_blobs\n#pip install yellowbrick\nimport yellowbrick\n# set working path\nos.getcwd()\nos.chdir('C:\\Temp\\SDA\\SDA_2020_St_Gallen\\SDA_2020_St_Gallen\\SDA_2020_St_Gallen_Clustering_Financial_Performance') # please set your own working path\n# define object that can collect component stocks of S&P500\ndef save_sp500_tickers():\nresp = requests.get('http://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\nsoup = bs.BeautifulSoup(resp.text, 'lxml')\ntable = soup.find('table', {'class': 'wikitable sortable'})\ntickers = []\nfor row in table.findAll('tr')[1:]:\nticker = row.findAll('td')[0].text\nticker = ticker[:-1]\nif \".\" in ticker:\nticker = ticker.replace('.','-')\nprint('ticker replaced to', ticker)\n\ntickers.append(ticker)\nsector = row.findAll('td')[3].text\nif \"\\n\" in sector:\nsector = sector.replace('\\n','')\nprint('sector replaced to', sector)\nsectors.append(sector)\nwith open(\"sp500tickers.pickle\",\"wb\") as f:\npickle.dump(tickers,f)\nreturn tickers, sectors\ntickers,sectors = save_sp500_tickers()\n# define object that can collect data from yahoo finance\ndef get_data_from_yahoo(reload_sp500=False):\nif reload_sp500:\ntickers = save_sp500_tickers()\nelse:\nwith open(\"sp500tickers.pickle\", \"rb\") as f:\ntickers = pickle.load(f)\nif not os.path.exists('stock_dfs'):\nstart = datetime(2020, 1, 20)\n\nfor ticker in tickers:\n# just in case your connection breaks, we'd like to save our progress!\nif not os.path.exists('stock_dfs/{}.csv'.format(ticker)):\ndf = web.DataReader(ticker, 'yahoo', start, end)\ndf.reset_index(inplace=True)\n#df = df.drop(\"Symbol\", axis=1)\ndf.to_csv('stock_dfs/{}.csv'.format(ticker))\nelse:\nprint('Already have {}'.format(ticker))\nget_data_from_yahoo()\n# Compile data into one sheet\ndef compile_data():\nwith open(\"sp500tickers.pickle\", \"rb\") as f:\ntickers = pickle.load(f)\nmain_df = pd.DataFrame()\nfor count, ticker in enumerate(tickers):\ndf = pd.read_csv('stock_dfs/{}.csv'.format(ticker))\ndf.set_index('Date', inplace=True)\ndf.rename(columns={'Adj Close': ticker}, inplace=True)\ndf.drop(['Open', 'High', 'Low', 'Close', 'Volume'], 1, inplace=True)\nif main_df.empty:\nmain_df = main_df.join(df, how='outer')\nif count % 10 == 0:\nprint(count)\nprint(main_df.head())\nmain_df.to_csv('sp500_joined_closes.csv')\ncompile_data()\n# Compute returns and drop the columns with zero values\ndf = pd.read_csv('sp500_joined_closes.csv')\ndf.set_index('Date', inplace = True)\n#look if there are values missing\nprint(df.isnull().sum())\n#drop the stock if more than 5 prices are missing, otherwise replace the missing values with the values in the previous row\ndf=df.dropna(axis=1,thresh=5)\ndf=df.fillna(axis=1, method='ffill')\ndf\nprices = df\nprices\n# Calculate the log returns\nlog_r = np.log(prices / prices.shift(1))\nlog_r = log_r.drop(axis = 0, index = ['2020-01-21'])\nlog_r\n# Compute the annualised returns\nannual_r = log_r.mean() * 252\nannual_r.name = 'annual return log'\nannual_r\n\n# Calculate the covariance matrix\ncov_matrix = log_r.cov() * 252\ncov_matrix\n# Calculate the volatility\nvar = log_r.var() * 252\nStd = np.sqrt(var)\nStd.name = 'Std'\n# Calculate Skewness and Kurtosis\nindex=annual_r.index\nSkewness=pd.DataFrame(skew(log_r))\nKurtosis=pd.DataFrame(kurtosis(log_r))\nSkewness.set_index(index, inplace=True)\nSkewness.columns=['Skewness']\nKurtosis.columns=['Kurtosis']\n# Compile the dataset we need\npd_sectors = pd.Series(sectors, index = tickers)\npd_sectors.name = 'sector'\ndataset = pd.concat([pd_sectors, annual_r, Std, Skewness, Kurtosis], axis = 1)\ndataset.drop(['CARR','LUMN' ,'OTIS' ,'VNT','TSLA', 'VIAC'], inplace=True)\ndataset.reset_index(level=0, inplace=True)\ndataset.columns=['Company', 'sector', 'annual_return_log', 'Std','Skewness','Kurtosis']\n# save the dataset\ndataset.to_csv('sp500_dataset.csv')\n# Prepare Dataset\npdf=pd.read_csv('sp500_dataset.csv')\nindustry= pdf.groupby(['sector'])['annual_return_log', 'Std' ].mean()\nindustry = industry.reset_index()\nfeatureset = industry[['annual_return_log', 'Std']]\nfeature_mtx = featureset.values\nfeature500=pdf[['annual_return_log','Std']]\nfeature500_mtx=feature500.values\n######### Part 1: Hierarchical Clustering\n######## Plot1\nimport scipy\nleng = feature_mtx.shape[0]\nD = scipy.zeros([leng,leng])\nfor i in range(leng):\nD[i,j] = scipy.spatial.distance.euclidean(feature_mtx[i], feature_mtx[j])\nimport pylab\nimport scipy.cluster.hierarchy\nZ = hierarchy.linkage(D, 'complete')\nfrom scipy.cluster.hierarchy import fcluster\nmax_d = 5\nclusters = fcluster(Z, max_d, criterion='distance')\nclusters\n#plot the denrogram\nfig = pylab.figure(figsize=(9, 12))\ndef llf(id):\nreturn '[%s]' % ( (str(industry['sector'][id])) )\ndendro = hierarchy.dendrogram(Z, leaf_label_func=llf, leaf_rotation=0, leaf_font_size =12, orientation = 'right')\nplt.savefig('plot 1.png', bbox_inches = 'tight')\nplt.show()\n######## Part 2.1: agglomerative clustering\ndist_matrix = distance_matrix(feature500_mtx,feature500_mtx)\nagglom = AgglomerativeClustering(n_clusters = 4, linkage = 'complete')\nagglom.fit(feature500_mtx)\nagglom.labels_\npdf['cluster_'] = agglom.labels_\npdf.head()\nimport matplotlib.cm as cm\nn_clusters = max(agglom.labels_)+1\ncolors = cm.rainbow(np.linspace(0, 1, n_clusters))\ncluster_labels = list(range(0, n_clusters))\nplt.figure(figsize=(40,45))\nfor color, label in zip(colors, cluster_labels):\nsubset = pdf[pdf.cluster_ == label]\nfor i in subset.index:\nplt.text(subset.Std[i], subset.annual_return_log[i], str(subset['Company'][i]), rotation=25)\nplt.scatter(subset.Std, subset.annual_return_log, s= 50, c=color, label='cluster'+str(label),alpha=0.5)\nplt.legend()\nplt.title('stock return')\nplt.xlabel('standard deviation')\n####### Part 2.2: agglomerative clustering by industry\ndf2= pdf.groupby(['cluster_', 'sector'])['annual_return_log', 'Std' ].mean()\nplt.figure(figsize=(20,12))\nsubset = df2.loc[(label,),]\nplt.text(subset.loc[i][1], str(i), rotation=20)\nplt.scatter(subset.Std, subset.annual_return_log, s=20, c=color, label='cluster'+str(label))\n##### Part 3: K-mean set-up\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom mpl_toolkits.mplot3d import Axes3D\nfeature=pdf[['annual_return_log','Skewness','Kurtosis']]\nfeature_mtx2=feature.values\n#Normalisation\nfrom sklearn.preprocessing import StandardScaler\nfeature_mtx2 = StandardScaler().fit_transform(feature_mtx2)\n##### Part 3.1: K-mean clustering 3D\nclusterNum = 8\nk_means = KMeans(init = \"k-means++\", n_clusters = clusterNum, n_init = 12)\nk_means.fit(feature_mtx2)\nlabels = k_means.labels_\nfig = plt.figure(1, figsize=(8, 6))\nplt.clf()\nax = Axes3D(fig, rect=[0, 0, .95, 1], elev=20, azim=160)\nplt.cla()\nax.set_xlabel('Kurtosis')\nax.set_zlabel('Annual_return')\nax.scatter(feature_mtx2[:, 2], feature_mtx2[:, 1], feature_mtx2[:, 0], c= labels.astype(np.float))\n##### Part 3.2: K-mean clustering 3D\nax = Axes3D(fig, rect=[0, 0, .95, 1], elev=20, azim=100)\n#%% choosing the optimal k (This part needs to be done separately, because it eliminates the cluster colors)\nfrom sklearn.cluster import KMeans\nfrom yellowbrick.cluster import KElbowVisualizer\nmodel = KMeans()\nvisualizer = KElbowVisualizer(model, k=(4,12))\nvisualizer.fit(feature_mtx) # Fit the data to the visualizer\nvisualizer.show()\n", "output_sequence": "Scrape S&P 500 component stocks from the start of pandemic and Cluster their performance using different clusering methods"}, {"input_sequence": "install.packages(\"remotes\")\nremotes::install_github(\"vdorie/bartCause\")\n# Fits a collection of treatment and response models using the Bayesian Additive\n# Regression Trees (BART) algorithm, producing estimates of treatment effects.\nCausal_BART <- function(df_aux,df_main,covariates){\nfit_bart <- bartc(df_aux$y,df_aux$d,df_aux[,covariates],keepTrees = TRUE)\nscore_bart <- predict(fit_bart,newdata = df_main,type=\"icate\")\nscore_bart_m <- apply(score_bart,2,mean)\nite.lb <- score_bart_m - 2 * ite.sd\ncate_CBART <- as.data.frame(cbind(score_bart_m,ite.lb,ite.ub))\ncolnames(cate_CBART) <- c(\"pred\",\"X5.\",\"X95.\")\nreturn(cate_CBART)\n}\n", "output_sequence": "Causal Bayesian additive regression tree method to estimate the conditional average treatment effect (CATE) via a variety of machine learning (ML) methods."}, {"input_sequence": "install.packages(\"xgboost\", repos=c(\"http://dmlc.ml/drat/\", getOption(\"repos\")), type=\"source\")\nvec.pac= c(\"SuperLearner\", \"gbm\", \"glmnet\",\"ranger\",\"caret\")\nlapply(vec.pac, require, character.only = TRUE)\n#Learner Library:\nlearners <- c( \"SL.glmnet\",\"SL.xgboost\", \"SL.ranger\",\"SL.nnet\")\n#CV Control for the SuperLearner\ncontrol <- SuperLearner.CV.control(V=5)\nR_learner <- function(data,covariates,learners){\ndata$ID <- c(1:nrow(data))\npseudo_all <- matrix(NA,nrow(data),2)\nID_pseudo <- 1:nrow(data)\npseudo_all <- cbind(pseudo_all,ID_pseudo)\n##### # 5-fold sample splitting\n# Sample splitting\nset.seed(1234)\nfolds <- createFolds(data$d,k=5)\nfor(f in 1:(length(folds))){\n\nif(f == 1){\ndata1 <- data[c(folds[[5]],folds[[2]],folds[[3]],folds[[4]]),]\ndf_main <- data[folds[[1]],]\n}\nif(f == 2){\ndata1 <- data[c(folds[[1]],folds[[5]],folds[[3]],folds[[4]]),]\ndf_main <- data[folds[[2]],]\nif(f == 3){\ndata1 <- data[c(folds[[1]],folds[[2]],folds[[5]],folds[[4]]),]\ndf_main <- data[folds[[3]],]\nif(f == 4){\ndata1 <- data[c(folds[[1]],folds[[2]],folds[[3]],folds[[5]]),]\ndf_main <- data[folds[[4]],]\nif(f == 5){\ndata1 <- data[c(folds[[1]],folds[[2]],folds[[3]],folds[[4]]),]\ndf_main <- data[folds[[5]],]\ndf_aux <- data1\n## R-learner\n# Train a classification model to get the propensity scores\np_mod <- SuperLearner(Y = df_aux$d, X = df_aux[,covariates], newX = df_main[,covariates], SL.library = learners,\nverbose = FALSE, method = \"method.NNLS\", family = binomial(),cvControl = control)\np_hat <- p_mod$SL.predict\np_hat = ifelse(p_hat<0.025, 0.025, ifelse(p_hat>.975,.975, p_hat)) # Overlap bounding\n# Train a regression model\nm_mod <- SuperLearner(Y = df_aux$y, X = df_aux[,covariates], newX = df_main[,covariates], SL.library = learners,\nverbose = FALSE, method = \"method.NNLS\",cvControl = control)\nm_hat <- m_mod$SL.predict\n# Apply the R-learner (residual-on-residual approach)\ny_tilde = df_main$y - m_hat\npseudo_outcome = y_tilde/w_tilde\nweights = w_tilde^2\n## Collect all pseudo outcomes\npseudo_all[,1][df_main$ID] <- pseudo_outcome\n}\n# Cross-fit version\nres_combined_r <- matrix(NA,nrow(data),5)\n# loop\nfor(l in 1:10){\nset <- seq(from=1, to=nrow(data)+1,by=(nrow(data)/10))\nset\nif(l<=5){\nr_mod_cf <- SuperLearner(Y = pseudo_all[(set[l]:set[l+1]-1),2],X=data[(set[l]:set[l+1]-1),covariates],\nnewX = data[(set[6]:set[11]-1),covariates], SL.library = learners,\nverbose = FALSE, method = \"method.NNLS\",obsWeights = pseudo_all[(set[l]:set[l+1]-1),3],cvControl = control)\n\nscore_r_1_cf <- r_mod_cf$SL.predict\nres_combined_r[(set[6]:set[11]-1),l] <- score_r_1_cf\n}\nif(l>5){\nr_mod_cf <- SuperLearner(Y =pseudo_all[(set[l]:set[l+1]-1),2],X=data[(set[l]:set[l+1]-1),covariates],\nnewX =data[(set[1]:set[6]-1),covariates], SL.library = learners,\nscore_r_0_cf <- r_mod_cf$SL.predict\nres_combined_r[(set[1]:set[6]-1),(l-5)] <- score_r_0_cf\nscore_R_oob <- rowMeans(res_combined_r)\nreturn(score_R_oob)\n", "output_sequence": "R-learner (or orthogonal-learner) method to estimate the conditional average treatment effect (CATE) via a variety of machine learning (ML) methods."}, {"input_sequence": "# Clear memory\nrm(list = ls())\n#install.packages(\"ggplot2\")\nlibrary(ggplot2)\n#\n# constructor of the class alphaStable ----------------------------------------\nalphaStable = function(alpha){\n\n# time step in grid\ntimeStep = 10000\n# fixed time grid\nt = seq(0, T, by = 1/timeStep)\n# number of time points\nn = length(t)\n# n independent uniformely distributed rv\ngamma = runif(n, min = -pi/2, max = pi/2)\n# n independent standart exponential rv\nW = rexp(n, rate = 1)\ndeltaX = (1/timeStep)^(1/alpha)*sin(alpha)*gamma/((cos(gamma))^(1/alpha))*\n(cos((1-alpha)*gamma)/W)^((1-alpha)/alpha)\n# initialization of the vector that should contain values of\n# process at the given set of time points\nX = rep(0, length(t))\nfor (i in 1:length(t)){\n\nsumDeltaX = 0\nfor (j in 1:i){\nsumDeltaX = sumDeltaX + deltaX[j]\n}\nX[i] = sumDeltaX\n}\nalpha_stable_data = list(time=t, process=X, alpha=alpha)\nclass(alpha_stable_data) = \"alphaStable\"\nreturn(alpha_stable_data)\n}\n# object of the class alphaStable\ninstance1 = alphaStable(1)\n# plot method for the class alphaStable ----------------------------------------\nplot.alphaStable = function(instance_alphaStable){\n# properties of the process to be plotted\nprocess = instance_alphaStable$process\ndfPlot = data.frame(time, process)\n# construct name of the file to save the plot (based on the given instance)\nparameter = instance_alphaStable$alpha\nname_of_plot =paste0(\"process\", \"Stable\", as.character(parameter), \".pdf\")\n# plotting procedure\nggplot(dfPlot, aes(time, process)) + geom_line(size=0.3) +\nxlab(\"time\") +\nggtitle(\"alpha stable process \") +\nggsave(name_of_plot, width = 6, height =4)\nplot(instance1)\n# density of the log returns in exponential Levy model -------------------------\n# reserve the name of the function, and use UseMethod command to tell R to\n# search for the correct function\nreturnsDensity = function(someClass) {\nUseMethod(\"returnsDensity\", someClass)\nreturnsDensity.alphaStable = function(instance_alphaStable){\n# log returns of the process\nlog_returns = diff(instance_alphaStable$process, lag=1)\nname_of_plot =paste0(\"retDen\",\"Stable\", as.character(parameter), \".pdf\")\n# density of the log returns\npdf(name_of_plot, width = 6, height = 4)\nplot(density(log_returns,kernel=\"gaussian\"), xlim=c(-0.1,0.1),\ncol = \"blue3\", main = \"\", xlab = \"log returns\", ylab = \"Density\")\ngrid()\ndev.off()\n#\nreturnsDensity(instance1)\n# plot of the log returns ------------------------------------------------------\nreturnsPlot = function(someClass) {\nUseMethod(\"returnsPlot\", someClass)\nreturnsPlot.alphaStable = function(instance_alphaStable){\nname_of_plot =paste0(\"retPlot\",\"Stable\", as.character(parameter), \".pdf\")\n# plot of log returns\npdf(name_of_plot, width = 6, height =4)\nplot(log_returns, type = \"l\", xlab = \"\", ylab = \"log returns\",\nmain = \"\")\n", "output_sequence": "Simulates an alpha stable L\u00e9vy process and plots the simulated data, log returns and the density of log returns. "}, {"input_sequence": "#Close windows and clear variables\ngraphics.off()\nrm(list = ls(all=TRUE))\n# install and load packages\nlibraries = c(\"stats\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# read market factor and firm characteristics\nfffactors = read.csv(\"F-F_3Factors.csv\", header = TRUE, sep = \",\", dec = \".\")\nrownames(fffactors) = fffactors[,1]\nfffactors[,1] = fffactors[,2]+fffactors[,5]\nfactorvol = matrix(0,177,3)\nfor (i in 1:177){\nfactorvol[i,j] = sd(fffactors[((3*i-2):(3*i+33)),j])\n}\nfactorvol = as.data.frame(factorvol)\ntotalconn = read.csv(\"conn_mw.csv\", header = TRUE, sep = \",\", dec = \".\")\nrownames(totalconn) = totalconn[,1]\ntotalconn = totalconn[,-1]\nfor (i in 1:3){\nlm_reg = lm(clustering[,i]~ factorvol[,1]+factorvol[,2]+factorvol[,3])\nsummary(lm_reg)\nclustering = read.csv(\"cc_mw.csv\", header = TRUE, sep = \",\", dec = \".\")\nclustering = clustering[,-1]\nrownames(clustering) = rownames(totalconn)\nfor (j in 1:12){\nlm_reg = lm(clustering[,j]~ factorvol[,1]+factorvol[,2]+factorvol[,3])\n", "output_sequence": "Regress the moving-window connectedness and clustering coefficients of industry network on volatilities of Fama-French three risk factors"}, {"input_sequence": "# clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# install and load packages\nlibraries = c(\"tseries\", \"fGarch\", \"rugarch\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n\n#Monte Carlo Study\n# k: Number of repetitions\nk = 1000\nn = c(100, 250, 500, 1000)\n#model parameters:\nalpha0 = 0.9\n#Simulation:\nfor (i in 1:length(n)){\nalpha = rep(0, k)\nfor (l in 1:k){\n#Simulate ARCH(1) time series\nts_sim = garchSim(spec = garchSpec(model = list(alpha = alpha0, beta = 0, omega = omega0)), n = n[i])\n\n#fit model to QMLE-method\nfit = try(garchFit(~ garch(1,0), data = ts_sim, cond.dist = \"QMLE\", trace = FALSE))\nif (l < k){\nalpha[l] = fit@fit$par[3]}\nassign(paste0(\"alpha\", n[i]), alpha)\n}\n}\n#Avg. deviation from true parameter\ndev = function(x, alpha0){\nx = as.vector(x)\ny = sqrt(sum((x - alpha0)^2)*k^-1)\nreturn(y)\n}\n# Percentage of alpha estimates >= 1\nperc = function(x){\nx = as.vector(x)\nx = round(x, 4)\ny = sum(x == 1 | x > 1)\nreturn(y/length(x))\n}\n#Merge results:\nn.obs = rbind(n[1], n[2], n[4])\naverage = rbind(mean(alpha100),\ndeviation = rbind(dev(alpha100, alpha0),dev(alpha250, alpha0))\nleq.one = rbind(perc(alpha100),\n#Create Table & save output\nResults = cbind(n.obs, average, deviation, leq.one)\nResults = data.frame(round(Results, 3))\nnames(Results) = c(\"n.obs\", \"average\", \"deviation\", \"leq 1\")\nwrite.table(Results, file = \"MC_Summary.csv\", sep = \",\", dec = \".\", row.names = FALSE)\nwrite.table(cbind(alpha100, alpha250, alpha1000), file = \"MC_parameters.csv\", sep = \",\", dec = \".\", row.names = FALSE)\n", "output_sequence": "Monte Carlo Simulation of ARCH(1)-process with 1000 repetitions. Estimates alpha-parameter with Quasi Maximum Likelihood method. Shows summary statistics for different sample sizes: 1) mean of all estimates; 2) average deviation from true alpha; 3) percentage of parameters estimated equal or lager than one. Default parameter settings: alpha = 0, omega = 0.2"}, {"input_sequence": "# clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# load data\nUSTF = read.table(\"XFGUSTF.dat\")\n# first observation point\nxmin = 1\n# last observation point\nxmax = nrow(USTF)\n# observation dummy\nx = (xmin:xmax)\n# plot of treasuries\nz1 = plot(x, USTF[xmin:xmax, 4], type = \"l\", col = \"black\", lwd = 1.5, xlab = \"Day\", ylab = \"Yield in %\", axes = FALSE, frame = TRUE,\nylim = c(2.5, 8), main = \"US Treasury Yields (3M, 1Y, 10Y)\") # 3 month yield\naxis(side = 1, at = seq(0, 2000, 500), labels = seq(0, 2000, 500))\nz2 = lines(x, USTF[xmin:xmax, 6], col = \"green\", lwd = 1.5) # 1 year yield\n", "output_sequence": "Plots US Treasury Yields (3M, 1Y, 2Y, 5Y, 10Y) from 0 to 2000 days"}, {"input_sequence": "compl = function (re, im){ # Complex array generation\nif(missing(re)){\nstop(\"compl: no composed object for real part\")\n}\nif(missing(im)){\nim = 0*(re<=Inf)\nif(nrow(matrix(re))!=nrow(matrix(im))){\nstop(\"compl: dim(re)<>dim(im)\")\nz = list()\nz$re = re\nreturn(z)\n}\ncmul = function(x, y){ # Complex multiplication\nre = x$re*y$re - x$im*y$im\nz = list()\ncexp = function(x){ # Complex exponential\nre = exp(x$re) * cos(x$im)\ncsub = function(x, y){# Complex subtraction two arrays of complex numbers\nre = x$re - y$re\ncdiv = function(x, y) { # Complex division\nw = y$re^2 + y$im^2\nre = (x$re*y$re + x$im*y$im) / w\nz$im = im\ncln = function(x){ # Complex natural logarithm\nre = log(x$re^2+x$im^2) / 2\nim = atan2(x$im, x$re)\ncreal = function(z){ # Returning real part\nre = z$re\nreturn(re)\nStandardNormalCharf = function(t,l){ # Standard Normal Characteristic Function\ns2 = l$sigma^2\ntmp = compl(-0.5*s2*t$re,-0.5*s2*t$im + l$mu)\nr = cexp(cmul(tmp,t))\nreturn(r)\ngFourierInversion = function(N,K,dt,t0,x0,charf,l){ # generic function for density approximation\n\n# 1. form the grids:\ndx = (2*pi)/(N*dt)\nt = (0:(K-1))*dt\nif (t0 != 0) {\nt = t + dt/2\nt = compl(t,t*0)\nx = x0 + (0:(N-1))*dx\n# 2. do the FFT:\ntmp = charf(t,l)\nphi = matrix(1,N,2)\nphi[1:K,1] = tmp$re\ntmp = x0 * dt *(0:(N-1))\nphi = cmul(compl(phi[,1],phi[,2]),cexp(compl(tmp*0,-tmp)))\nphi = cbind(phi$re,phi$im)\nphitmp = complex(real=phi[,1],imaginary=phi[,2])\nninvfft = length(phitmp)\ny = fft(phitmp,inverse=T)#ninvfft\ny = compl(Re(y),Im(y))\nif (t0 != 0){\ntmp = x*dt/2\ny = cmul(y,cexp(compl(tmp*0,-tmp)))\n# 3. rescale:\nif (t0 == 0){\nr = dt * ( creal(y) - 0.5*creal(charf(list(re=0,im=0),l)) )/ pi\n}else{\nr = dt * creal(y)/pi\n}\nVaRcgfDG = function(t,par){ # cumulant generating function (cgf) for the\n# class of quadratic forms of Gaussian vectors.\n\ns = compl(par$theta*t$re, par$theta*t$im)\ni = 1\nm = length(par$lambda)\nwhile (i <= m){\n# 1-lambda*t:\nomlt = compl(1 - par$lambda[i] * t$re, -par$lambda[i] * t$im)\ntmp = cmul(t,t)\ntmp = compl(par$delta[i]^2 * tmp$re, par$delta[i]^2 * tmp$im)\ntmp = csub(tmp,cln(omlt))\ns = compl(s$re + 0.5*tmp$re, s$im + 0.5*tmp$im)\ni = i+1\nreturn(s)\nVaRcharfDG = function(t,par){# computes the characteristic function for the\n# class of quadratic forms of Gaussian vectors.\nt = compl(-t$im,t$re) # 1i*t\nr = cexp(VaRcgfDG(t,par))\nVaRcharfDGF2 = function(t, l){ # Fourier transform of an approximating Gaussian cdf\nmu = l$theta + 0.5*sum(l$lambda)\ns2 = sum(l$delta^2 + 0.5*l$lambda^2)\ntmp = compl(-0.5*s2*t$re,-0.5*s2*t$im + mu)\ntmp = cexp(cmul(tmp,t))\ntmp = csub(VaRcharfDG(t,l),tmp)\ntmp = cdiv(tmp,t)\nr = compl(-tmp$im,tmp$re)\nr$re= replace(r$re,r$re==\"NaN\",0)\nVaRcorrfDGF2 = function(x,l){ # cdf of normal approximation\nr = pnorm((x-mu)/sqrt(s2))\nreturn(r)\nVaRcdfDG = function(l,N,K,dt){ # approximates the cdf for the class of quadratic forms\n# Gaussian vectors\ndx = 2*pi/(N*dt)\ny = gFourierInversion(N,K,dt,dt/2,x0,VaRcharfDGF2,l)\nx = x0 + (0:(N-1))*dx\ny = y + VaRcorrfDGF2(x,l)\ny[x<=0] = 0 # correct for misspecification\nr = list(x=x,y=y)\ncdf2quant = function(a,l){ # compute the quantile from a CDF on a given grid\n# robust version:\nx = l$x\nleft = 1\nright = length(y)\n# deal with extreme cases:\nif( a <= y[left] ){\nq = x[left]\n}else if( a >= y[right] ){\nq = x[right]\n}else{\nidx = 1:length(y)\nleft = max(idx[which(y <= a)])\nif ( left < right ){\nw = (y[right]-a)/(y[right]-y[left])\nq = w*x[left] + (1-w)*x[right]\n}else{\nq = 0.5*(x[left]+x[right])\n}\nreturn(q)\n#################### MAIN FUNCTION ####################\nVaRqDG = function(a,par,N,K,dt){\nr = VaRcdfDG(par,N,K,dt)\nq = cdf2quant(a,r)\nreturn(q)\n##################### TEST FUNCTION ######################\ntheta = 0\ndelta = c(0)\nlambda = c(1.4142)\npar = list(theta=theta,delta=delta,lambda=lambda)\nVaRqDG(a=0.95,par=par,N=512,K=512,dt=0.1)\nqnorm(0.95)\n", "output_sequence": "Computes the a-quantile for the class of quadratic forms of Gaussian vectors uses Fourier inversion to approximate the cumulative distribution function (CDF)."}, {"input_sequence": "##################### SUBROUTINES ####################\ncompl = function (re, im){ # Complex array generation\nif(missing(re)){\nstop(\"compl: no composed object for real part\")\n}\nif(missing(im)){\nim = 0*(re<=Inf)\nif(nrow(matrix(re))!=nrow(matrix(im))){\nstop(\"compl: dim(re)<>dim(im)\")\nz = list()\nz$re = re\nreturn(z)\n}\ncmul = function(x, y){ # Complex multiplication\nre = x$re*y$re - x$im*y$im\nz$im = im\n\ncexp = function(x){ # Complex exponential\nre = exp(x$re) * cos(x$im)\ncsub = function(x, y){# Complex subtraction two arrays of complex numbers\nre = x$re - y$re\nz = list()\nz$re = re\nreturn(z)\ncdiv = function(x, y) { # Complex division\nw = y$re^2 + y$im^2\nre = (x$re*y$re + x$im*y$im) / w\ncln = function(x){ # Complex natural logarithm\nre = log(x$re^2+x$im^2) / 2\nim = atan2(x$im, x$re)\ncreal = function(z){ # Returning real part\nre = z$re\nreturn(re)\nStandardNormalCharf = function(t,l){ # Standard Normal Characteristic Function\ns2 = l$sigma^2\ntmp = compl(-0.5*s2*t$re,-0.5*s2*t$im + l$mu)\nr = cexp(cmul(tmp,t))\nreturn(r)\ngFourierInversion = function(N,K,dt,t0,x0,charf,l){ # generic function for density approximation\n# 1. form the grids:\ndx = (2*pi)/(N*dt)\nt = (0:(K-1))*dt\nif (t0 != 0) {\nt = t + dt/2\nt = compl(t,t*0)\nx = x0 + (0:(N-1))*dx\n# 2. do the FFT:\ntmp = charf(t,l)\nphi = matrix(1,N,2)\nphi[1:K,1] = tmp$re\ntmp = x0 * dt *(0:(N-1))\nphi = cmul(compl(phi[,1],phi[,2]),cexp(compl(tmp*0,-tmp)))\nphi = cbind(phi$re,phi$im)\nphitmp = complex(real=phi[,1],imaginary=phi[,2])\nninvfft = length(phitmp)\ny = fft(phitmp,inverse=T)#ninvfft\ny = compl(Re(y),Im(y))\nif (t0 != 0){\ntmp = x*dt/2\ny = cmul(y,cexp(compl(tmp*0,-tmp)))\n# 3. rescale:\nif (t0 == 0){\nr = dt * ( creal(y) - 0.5*creal(charf(list(re=0,im=0),l)) )/ pi\n}else{\nr = dt * creal(y)/pi\nVaRcgfDG = function(t,par){ # cumulant generating function (cgf) for the\n# class of quadratic forms of Gaussian vectors.\ns = compl(par$theta*t$re, par$theta*t$im)\ni = 1\nm = length(par$lambda)\nwhile (i <= m){\n# 1-lambda*t:\nomlt = compl(1 - par$lambda[i] * t$re, -par$lambda[i] * t$im)\ntmp = cmul(t,t)\ntmp = compl(par$delta[i]^2 * tmp$re, par$delta[i]^2 * tmp$im)\ntmp = csub(tmp,cln(omlt))\ns = compl(s$re + 0.5*tmp$re, s$im + 0.5*tmp$im)\ni = i+1\nreturn(s)\nVaRcharfDG = function(t,par){# computes the characteristic function for the\nt = compl(-t$im,t$re) # 1i*t\nr = cexp(VaRcgfDG(t,par))\nVaRcharfDGF2 = function(t, l){ # Fourier transform of an approximating Gaussian cdf\nmu = l$theta + 0.5*sum(l$lambda)\ns2 = sum(l$delta^2 + 0.5*l$lambda^2)\ntmp = compl(-0.5*s2*t$re,-0.5*s2*t$im + mu)\ntmp = cexp(cmul(tmp,t))\ntmp = csub(VaRcharfDG(t,l),tmp)\ntmp = cdiv(tmp,t)\nr = compl(-tmp$im,tmp$re)\nr$re = replace(r$re,r$re==\"NaN\",0)\nVaRcorrfDGF2 = function(x,l){ # cdf of normal approximation\nmu = l$theta + 0.5*sum(l$lambda)\ns2 = sum(l$delta^2 + 0.5*l$lambda^2)\nr = pnorm((x-mu)/sqrt(s2))\nVaRcdfDG = function(l,N,K,dt){ # approximates the cdf for the class of quadratic forms\n# Gaussian vectors\ndx = 2*pi/(N*dt)\ny = gFourierInversion(N,K,dt,dt/2,x0,VaRcharfDGF2,l)\nx = x0 + (0:(N-1))*dx\ny = y + VaRcorrfDGF2(x,l)\ny[x<=0] = 0 # correct for misspecification\nr = list(x=x,y=y)\ncdf2quant = function(a,l){ # compute the quantile from a CDF on a given grid\n# robust version:\nx = l$x\nleft = 1\nright = length(y)\n# deal with extreme cases:\nif( a <= y[left] ){\nq = x[left]\n}else if( a >= y[right] ){\nq = x[right]\n}else{\nidx = 1:length(y)\nleft = max(idx[which(y <= a)])\nif ( left < right ){\nw = (y[right]-a)/(y[right]-y[left])\nq = w*x[left] + (1-w)*x[right]\n}else{\nq = 0.5*(x[left]+x[right])\n}\nreturn(q)\nVaRqDG = function(a,par,N,K,dt){\nr = VaRcdfDG(par,N,K,dt)\nq = cdf2quant(a,r)\nreturn(q)\n#################### MAIN FUNCTIONS ####################\nXFGqDGtest = function(n,N,dt){\n# di = createdisplay(1,2)\nlambdav = -sqrt(2) + (0:(n-1))*2*sqrt(2)/(n-1)\nq = lambdav\nwhile ( i <= n ){\nq[i] = VaRqDG(0.01,XFGdg11par(lambdav[i]),N,N,dt)\nz = cbind(lambdav,q)\nplot(z,type=\"l\",col=\"blue3\",lwd=2,xlab=\"X\",ylab=\"Y\")\nphi = -pi/2 + (0:(n-1))*pi/(n-1)\nq = phi\nwhile (i <= n){\nq[i] = VaRqDG(0.01,XFGdg12par(phi[i]),N,N,dt)\nz = cbind(phi,q)\ndev.new()\nXFGdg11par = function(la,de,th){\nlambda = la\nif (missing(de)){\ndelta = c(sqrt(max(c(0,1-lambda^2/2))))\ndelta = de\nif (missing(th)){\ntheta = -0.5*lambda\nSigma = diag(rep(1,1))\nGamma = lambda*diag(c(1))\nDelta = delta\nreturn(list(Sigma=Sigma,Gamma=Gamma,Delta=Delta,theta=theta,lambda=lambda,delta=delta))\nXFGdg12par = function(phi){ #XFG\nsr2 = sqrt(2)\nphi = phi + pi/4\nlambda = sr2* c(sin(phi),-cos(phi))\ndelta = c(0,0)\ntheta = -0.5 * sum(lambda)\nSigma = diag(rep(1,2))\nGamma = lambda*diag(c(1))\nDelta = delta\nreturn(list(Sigma=Sigma,Gamma=Gamma,Delta=Delta,theta=theta,lambda=lambda,delta=delta))\nXFGqDGtest(n=33,N=1024,dt=0.2)\n", "output_sequence": "Pots the 1%-quantile from the two one-parametric sub-families of the class of quadratic forms of Gaussian vectors, as defined by the functions XFGdg11par and XFGdg12par"}, {"input_sequence": "% -----------------------------------------------------------------\n% Library smoother\n% -----------------------------------------------------------------\n% See_also regxci regest regxbwsel lpregxest\n% Macro regxest\n% Keywords kernel smoothing, kernel regression,\n% Nadaraya-Watson estimator, nonparametric regression\n% Description computes the Nadaraya-Watson estimator for\n% univariate regression.\n% Notes This function does an exact computation, i.e.,\n% requires O(n^2) operations for estimating the\n% regression function on all observations. For\n% exploratory purposes, the quantlet \"regest\" is\n% recommended, which uses the faster WARPing method.\n% Reference Haerdle, W. (1990). Applied Nonparametric Regression.\n% Econometric Society Monographs, No. 19. Cambridge\n% University Press.\n% Usage mh = regxest(x {,h {,K} })\n% Input\n% Parameter x\n% Definition n x 2 matrix, the data. The first column contains the\n% independent and the second column the\n% dependent variable.\n% Parameter h\n% Definition optional scalar, bandwidth. If not given, 20% of the\n% range of x[,1] is used as default.\n% Parameter K\n% Definition optional string, kernel function on [-1,1] or Gaussian\n% kernel \"gau\". If not given, the Quartic kernel\n% \"qua\" is used as default.\n% Parameter v\n% Definition optional m x 1 vector, values of the independent variable on\n% which to compute the regression. If not given,\n% the (sorted) x is used as default.\n% Output\n% Parameter mh\n% Definition n x 2 or m x 2 matrix, the first column represents the\n% sorted first column of x or the sorted v and the\n% second column contains the regression estimate\n% on the values of the first column.\nfunction[mh]=regxest(x,h,K)\n[n,m]=size(x);\nif m ~= 2\ndisp('regxest: data matrix must contain 2 columns');\ndisp('Please input the data matrix again');\ndat = input('x=');\nend\n;\neh = exist('h');\nif (eh == 0)\n%h=(max(x(:,1))-min(x(:,1)))/5;\nh = 2.42*std(x(:,1))*n^(-0.2);\neK = exist('K');\nif (eK == 0)\nK=1; %quartic kernel\nelse\nif K == 0 || K >4\ndisp('Type of the kernel must be \"1\", \"3\" or \"4\"');\ndisp('The quartic kernel will be used');\nK = 1;\nend\ntmp=sortrows(x)\nx=tmp(:,1);\n%if (exist(v)==0)\n% v=x\n% else\n% v=sort(v)\n% endif\n% require nw (Nadaraya-Watson) function / toolbox\nfor i = 1:n\ns(i,1)= nw(x(i),x,y,h,K)./nw(x(i),x,ones(n,1),h,K);\nend\n%for i = 1:n % Gaussian kernel\n% s(i,1)= nw(x(i),x,y,5*h,K)./nw(x(i),x,ones(n,1),5*h,K)\n%end\n", "output_sequence": "XFGIBT01 Outputs implied binomial trees computed by DK and BC algorithm of stock prices, transition probabilities and Arrow-Debreu prices, respectively, with a parabolic implied volatility (see IBTimpliedvola). Require IBTblackscholes.m, IBTdk.m, IBTbc.m, IBTimpliedvola.m, IBTresort.m, IBTcrr.m"}, {"input_sequence": "%% CODE\ndisp('Please input Price of Underlying Asset s0, Riskless Interest Rate per Year r');\ndisp('Time to Expiration (Years) t, Number of steps n');\ndisp('as: [100, 0.03, 5, 5]');\ndisp(' ') ;\npara=input('[s0, r, t, n]=');\nwhile length(para) < 4\ndisp('Not enough input arguments. Please input in 1*4 vector form like [100, 0.03, 5, 5]');\ndisp(' ') ;\npara=input('[s0, r, t, n]=');\nend\ns0=para(1); % Stock price\nr=para(2); % Riskless interest rate\n[St, AD, P, LV] = IBTdk(s0,r,t,n,[]); % DK algorithm with parabolic implied volatility\n%% Resorting\nresort the matrices\nStree = IBTresort(St);\nADtree = IBTresort(AD);\n'implied stock tree D&K'\ndisp(Stree);\n'implied stock tree B&C'\ndisp(Stree2);\n'transition probability tree D&K'\ndisp(Ptree);\n'transition probability tree B&C'\ndisp(Ptree2);\n'arrow debreu price tree D&K'\ndisp(ADtree);\n'arrow debreu price tree B&C'\n", "output_sequence": "XFGIBT01 Outputs implied binomial trees computed by DK and BC algorithm of stock prices, transition probabilities and Arrow-Debreu prices, respectively, with a parabolic implied volatility (see IBTimpliedvola). Require IBTblackscholes.m, IBTdk.m, IBTbc.m, IBTimpliedvola.m, IBTresort.m, IBTcrr.m"}, {"input_sequence": "import os\nimport numpy as np\nimport pandas as pd\n#2. Get the file\nif os.name=='posix':\n\n# GOOGLE COLAB SETUP\n# Load the Drive helper and mount\nfrom google.colab import drive\n# This will prompt for authorization.\ndrive.mount('/content/drive')\n#3. Read file as panda dataframe\ntrain = pd.read_csv(f'{data_path}/train_cleaned_no_punkt.csv')\ntrain['mal'] = train[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum(axis=1) >= 1\ntrain.comment_text.fillna(\"empty\", inplace=True)\ntest_labelled['mal'] = test_labelled[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum(axis=1) >= 1\ntest_labelled.comment_text.fillna(\"empty\", inplace=True)\n# CHANGE TRAIN AND TEST, MIX TO GET SIMILAR DISTRIBUTION\nfrom sklearn.model_selection import train_test_split\nrs=42\nX_train1, X_test1, y_train1, y_test1 = train_test_split(train.drop('mal', axis=1), train.mal, stratify=train.mal, test_size=0.29, random_state=rs )\nX = np.concatenate((X_train1.comment_text, X_train2.comment_text))\ny = np.concatenate((y_train1, y_train2))\nX_test = np.concatenate((X_test1.comment_text, X_test2.comment_text))\ndef toxicity(text, dict_toxic):\nexample_score = 0\nfor word in text['text'].split():\nif word in dict_toxic:\nexample_score += 1\nelse:\ncontinue\nfrom tqdm import tqdm\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nscaler.fit(pd.DataFrame(test_preds, columns=['preds']))\nfrom sklearn.metrics import roc_auc_score\nfrom tensorflow.keras import backend as K\nfrom sklearn.metrics import roc_auc_score, average_precision_score\nfor thr in [0.001]:\nthreshold = thr\nprobs_class = test_preds_scaled.copy()\nprobs_class[probs_class >= threshold] = 1\nauc_f = average_precision_score(y_test, test_preds_scaled)\nf_score = f1_score(y_test, probs_class)\ntrain = pd.read_csv(f'{data_path}/facebook_cleaned_no_punkt.csv')\ntrain.drop(['id', 'agr'], axis=1, inplace=True)\ntrain.text.fillna(\"empty\", inplace=True)\nfrom sklearn.model_selection import train_test_split\nX, X_test, y, y_test = train_test_split(train.text, train.mal, stratify=train.mal, test_size=0.29, random_state=rs )\nX = np.array(X)\nX_test = np.array(X_test)\ntest_preds = pd.DataFrame(X_test, columns=['text']).progress_apply(lambda x: toxicity(x, sw_words), axis=1)\ntrain.drop(['count', 'hate_speech', 'offensive_language', 'neither', 'class'], axis=1, inplace=True)\ntrain.tweet.fillna(\"empty\", inplace=True)\ntest_preds = pd.DataFrame(X_test).rename(columns={'tweet':'text'}).progress_apply(lambda x: toxicity(x, sw_words), axis=1)\nfor thr in [0.1]:\n", "output_sequence": "Antisocial Online behaivor detection. This part is dedicated to lexicon-based approaches. The list is taken from http://www.bannedwordlist.com/"}, {"input_sequence": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport os\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split, KFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nimport random\nfrom sklearn import metrics\nfrom collections import Counter\nimport argparse\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import roc_auc_score, average_precision_score, f1_score, precision_score, recall_score\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import ParameterGrid\nif os.name=='posix':\n# GOOGLE COLAB SETUP\n# Load the Drive helper and mount\nfrom google.colab import drive\n# This will prompt for authorization.\ndrive.mount('/content/drive')\n\n#2. Get the file\ndata_path = 'drive/My Drive/Colab Notebooks/adaptHAN/AOBDL_code/data'\n#3. Read file as panda dataframe\ntrain = pd.read_csv(f'{data_path}/facebook_cleaned_no_punkt.csv')\nelse:\n#2. Get the file\ndata_path = '../data'\ncodes_path = '../AOBDL_TML'\ntrain.drop(['id', 'agr'], axis=1, inplace=True)\nrs=42\nfrom sklearn.model_selection import train_test_split\nX, X_test, y, y_test = train_test_split(train.text, train.mal, stratify=train.mal, test_size=0.29, random_state=rs )\nX = np.array(X)\nX_test = np.array(X_test)\nkf = StratifiedKFold(n_splits=5, random_state=rs)\nauc = []\nfscore_ = []\nc = 0\nword_vectorizer = TfidfVectorizer(\nsublinear_tf = True,\nstrip_accents = 'unicode',\nanalyzer = 'word',\ntoken_pattern = r'\\w{1,}',\n#stop_words = 'english',\nngram_range = (1, 1),\n#auc_pr = 0.093434375824395\nc = 0\nC_parameter = np.arange(0.1, 1.1, 0.1)\n# use best C\nfor c_p in C_parameter:\nfor c, (train_index, val_index) in enumerate(kf.split(X, y)):\nX_train, X_val = X[train_index],\nword_vectorizer.fit(X_train)\ntrain_word_features = word_vectorizer.transform(X_train)\ny_train = y_train.astype('int')\nclassifier = LogisticRegression(C=c_p, solver='sag', random_state=rs)\nclassifier.fit(train_word_features, y_train)\nprobs = classifier.predict_proba(val_word_features)[:,1]\nauc_roc = roc_auc_score(y_val, probs)\n\nthreshold = 0.4\n#for threshold in np.arange(0.1, 1, 0.1):\nprobs_class = probs.copy()\nprobs_class[probs_class >= threshold] = 1\nprecision = precision_score(y_val, probs_class)\nprint(f' {threshold} fold {c} precision {round(precision, 3)} recall {round(recall, 3)} fscore {round(fscore,3)}')\nif len(C_parameter)==1:\n# print performance\nprint(f'---------------------------------------------')\nprint(f'FOLD {c}: AUC PR-C = {round(auc_pr, 3)}, AUC ROC = {round(auc_roc, 3)}')\nprint(f'')\nauc.append(auc_pr)\nif len(C_parameter)!=1:\nprint(f'PARAMETER C = {c_p}')\n# print performance\nprint(f'-----------------------------------------------')\nprint(f'CV average: AUC PR-C = {round(np.array(auc).mean(), 3)}, AUC ROC = {round(np.array(roc).mean(), 3)}, FSCORE = {round(np.array(fscore_).mean(), 3)}')\nprint(f'')\nprint(f'CV std: AUC PR-C = {round(np.array(auc).std(), 3)}, AUC ROC = {round(np.array(roc).std(), 3)}, FSCORE = {round(np.array(fscore_).std(), 3)}')\n# TRAIN ON WHOLE DAATA AND PREDICT ON TEST\nword_vectorizer.fit(X)\ntrain_word_features = word_vectorizer.transform(X)\nclassifier = LogisticRegression(C=1, solver='sag', random_state=rs)\nclassifier.fit(train_word_features, y)\nprobs = classifier.predict_proba(test_word_features)[:,1]\nauc_roc = roc_auc_score(y_test, probs)\n# print performance\nprint(f'-----------------------------------------')\nprint(f'TEST: AUC PR-C = {round(auc_pr, 4)}, AUC ROC = {round(auc_roc, 4)}')\nprint(f'')\nthreshold = 0.4\nprobs_class = probs.copy()\nprobs_class[probs_class >= threshold] = 1\nprecision = precision_score(y_test, probs_class)\nprint(f' {threshold} precision {round(precision, 3)} recall {round(recall, 3)} fscore {round(fscore,3)}')\nstop_words = 'english',\nfor c, (train_index, val_index) in enumerate(kf.split(X, y)):\nclassifier = RandomForestClassifier(n_estimators=600, max_depth=None, max_features='auto',\nmin_samples_split=2, verbose = True, n_jobs=20, random_state=rs)\n# print performance\nprint(f'---------------------------------------------')\nprint(f'FOLD {c}: AUC PR-C = {round(auc_pr, 3)}, AUC ROC = {round(auc_roc, 3)}')\nprint(f'')\nprint(f'-----------------------------------------------')\nprint(f'CV average: AUC PR-C = {round(np.array(auc).mean(), 3)}, AUC ROC = {round(np.array(roc).mean(), 3)}, FSCORE = {round(np.array(fscore_).mean(), 3)}')\nclassifier = RandomForestClassifier(n_estimators=600, max_depth=None, max_features='auto',\nprint(f'TEST: AUC PR-C = {round(auc_pr, 3)}, AUC ROC = {round(auc_roc, 3)}')\npenalty = ['l2', 'l1']\nalpha = [0.000001, 0.001, 0.1]\nmax_iter = [1000, 15000]\n# best parameters\npenalty = ['l2']\nalpha = [0.0001]\nmax_iter = [1000]\nfor p in penalty:\nfor a in alpha:\nfor i in max_iter:\nauc = []\nfscore_ = []\nc = 0\nprint(f'-------------')\nprint(f'penalty {p}')\n\nclassifier = SGDClassifier(n_jobs=20, random_state=rs, loss='log', shuffle=False,\npenalty=p, alpha=a, max_iter=i)\nclassifier = SGDClassifier(n_jobs=20, random_state=rs, loss='log', shuffle=False,\npenalty=penalty[0], alpha=alpha[0], max_iter=max_iter[0])\n# muner of rounds\nmax_rounds = 600\nstopping = 600\nverbose = 200\n# LGB parameters\nlgb_params = {\n'boosting_type': 'gbdt',\n'objective': 'binary',\n'metrics': 'binary_logloss',\n'bagging_fraction': 0.9,\n'lambda_l1': 0.1,\n'min_split_gain': 0.01,\n'min_child_weight': 2,\n'silent': True,\n'verbosity': 100,\n'learning_rate': 0.1,\n'max_depth': 7,\n'scale_pos_weight': 1,\n'n_estimators': max_rounds,\n'nthread' : 20,\n'random_state': rs,\nclassifier = lgb.LGBMClassifier(**lgb_params)\nthreshold = 0.3\nclassifier = lgb.LGBMClassifier(**lgb_params)\nthreshold = 0.3\n", "output_sequence": "Antisocial Online behaivor detection. Contains traditional machine learning methods: ridge regression, SVM, random forest, lightGBM"}, {"input_sequence": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport os\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split, KFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nimport random\nfrom sklearn import metrics\nfrom collections import Counter\nimport argparse\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import roc_auc_score, average_precision_score, f1_score, precision_score, recall_score\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import ParameterGrid\nif os.name=='posix':\n# GOOGLE COLAB SETUP\n# Load the Drive helper and mount\nfrom google.colab import drive\n# This will prompt for authorization.\ndrive.mount('/content/drive')\n\n#2. Get the file\ndata_path = 'drive/My Drive/Colab Notebooks/adaptHAN/AOBDL_code/data'\n#3. Read file as panda dataframe\ntrain = pd.read_csv(f'{data_path}/formspring_cleaned_no_punkt.csv')\nelse:\n#2. Get the file\ndata_path = '../data'\ncodes_path = '../AOBDL_TML'\nrs=42\nfrom sklearn.model_selection import train_test_split\nX, X_test, y, y_test = train_test_split(train.text, train.mal, stratify=train.mal, test_size=0.29, random_state=rs )\nX = np.array(X)\nX_test = np.array(X_test)\nkf = StratifiedKFold(n_splits=5, random_state=rs)\nauc = []\nfscore_ = []\nc = 0\nword_vectorizer = TfidfVectorizer(\nsublinear_tf = True,\nstrip_accents = 'unicode',\nanalyzer = 'word',\ntoken_pattern = r'\\w{1,}',\n#stop_words = 'english',\nngram_range = (1, 1),\n#auc_pr = 0.093434375824395\nc = 0\nC_parameter = np.arange(0.1, 1.1, 0.1)\n# use best C\nfor c_p in C_parameter:\nfor c, (train_index, val_index) in enumerate(kf.split(X, y)):\nX_train, X_val = X[train_index],\nword_vectorizer.fit(X_train)\ntrain_word_features = word_vectorizer.transform(X_train)\ny_train = y_train.astype('int')\nclassifier = LogisticRegression(C=c_p, solver='sag', random_state=rs)\nclassifier.fit(train_word_features, y_train)\nprobs = classifier.predict_proba(val_word_features)[:,1]\nauc_roc = roc_auc_score(y_val, probs)\n\nthreshold = 0.14\n#for threshold in np.arange(0.1, 0.2, 0.01):\nprobs_class = probs.copy()\nprobs_class[probs_class >= threshold] = 1\nprecision = precision_score(y_val, probs_class)\nprint(f' {threshold} fold {c} precision {round(precision, 3)} recall {round(recall, 3)} fscore {round(fscore,3)}')\nif len(C_parameter)==1:\n# print performance\nprint(f'---------------------------------------------')\nprint(f'FOLD {c}: AUC PR-C = {round(auc_pr, 3)}, AUC ROC = {round(auc_roc, 3)}')\nprint(f'')\nauc.append(auc_pr)\nif len(C_parameter)!=1:\nprint(f'PARAMETER C = {c_p}')\n# print performance\nprint(f'-----------------------------------------------')\nprint(f'CV average: AUC PR-C = {round(np.array(auc).mean(), 3)}, AUC ROC = {round(np.array(roc).mean(), 3)}, FSCORE = {round(np.array(fscore_).mean(), 3)}')\nprint(f'')\nprint(f'CV std: AUC PR-C = {round(np.array(auc).std(), 3)}, AUC ROC = {round(np.array(roc).std(), 3)}, FSCORE = {round(np.array(fscore_).std(), 3)}')\n# TRAIN ON WHOLE DAATA AND PREDICT ON TEST\nword_vectorizer.fit(X)\ntrain_word_features = word_vectorizer.transform(X)\nclassifier = LogisticRegression(C=1, solver='sag', random_state=rs)\nclassifier.fit(train_word_features, y)\nprobs = classifier.predict_proba(test_word_features)[:,1]\nauc_roc = roc_auc_score(y_test, probs)\n# print performance\nprint(f'-----------------------------------------')\nprint(f'TEST: AUC PR-C = {round(auc_pr, 4)}, AUC ROC = {round(auc_roc, 4)}')\nprint(f'')\nthreshold = 0.14\nprobs_class = probs.copy()\nprobs_class[probs_class >= threshold] = 1\nprecision = precision_score(y_test, probs_class)\nprint(f' {threshold} precision {round(precision, 3)} recall {round(recall, 3)} fscore {round(fscore,3)}')\nstop_words = 'english',\nfor c, (train_index, val_index) in enumerate(kf.split(X, y)):\nclassifier = RandomForestClassifier(n_estimators=600, max_depth=None, max_features='auto',\nmin_samples_split=2, verbose = True, n_jobs=20, random_state=rs)\n# print performance\nprint(f'---------------------------------------------')\nprint(f'FOLD {c}: AUC PR-C = {round(auc_pr, 3)}, AUC ROC = {round(auc_roc, 3)}')\nprint(f'')\nthreshold = 0.2\nprint(f'-----------------------------------------------')\nprint(f'CV average: AUC PR-C = {round(np.array(auc).mean(), 3)}, AUC ROC = {round(np.array(roc).mean(), 3)}, FSCORE = {round(np.array(fscore_).mean(), 3)}')\nclassifier = RandomForestClassifier(n_estimators=600, max_depth=None, max_features='auto',\nprint(f'TEST: AUC PR-C = {round(auc_pr, 3)}, AUC ROC = {round(auc_roc, 3)}')\nthreshold = 0.2\npenalty = ['l2', 'l1']\nalpha = [0.000001, 0.001, 0.1]\nmax_iter = [1000, 15000]\n# best parameters\npenalty = ['l2']\nalpha = [0.0001]\nmax_iter = [1000]\nfor p in penalty:\nfor a in alpha:\nfor i in max_iter:\nauc = []\nfscore_ = []\nc = 0\nprint(f'-------------')\nprint(f'penalty {p}')\n\nclassifier = SGDClassifier(n_jobs=20, random_state=rs, loss='log', shuffle=False,\npenalty=p, alpha=a, max_iter=i)\nclassifier = SGDClassifier(n_jobs=20, random_state=rs, loss='log', shuffle=False,\npenalty=penalty[0], alpha=alpha[0], max_iter=max_iter[0])\n# muner of rounds\nmax_rounds = 600\nstopping = 600\nverbose = 200\n# LGB parameters\nlgb_params = {\n'boosting_type': 'gbdt',\n'objective': 'binary',\n'metrics': 'binary_logloss',\n'bagging_fraction': 0.9,\n'lambda_l1': 0.1,\n'min_split_gain': 0.01,\n'min_child_weight': 2,\n'silent': True,\n'verbosity': 100,\n'learning_rate': 0.1,\n'max_depth': 7,\n'scale_pos_weight': 1,\n'n_estimators': max_rounds,\n'nthread' : 20,\n'random_state': rs,\nclassifier = lgb.LGBMClassifier(**lgb_params)\n", "output_sequence": "Antisocial Online behaivor detection. Contains traditional machine learning methods: ridge regression, SVM, random forest, lightGBM"}, {"input_sequence": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport os\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split, KFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nimport random\nfrom sklearn import metrics\nfrom collections import Counter\nimport argparse\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import roc_auc_score, average_precision_score, f1_score, precision_score, recall_score\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import ParameterGrid\nif os.name=='posix':\n# GOOGLE COLAB SETUP\n# Load the Drive helper and mount\nfrom google.colab import drive\n# This will prompt for authorization.\ndrive.mount('/content/drive')\n\n#2. Get the file\ndata_path = 'drive/My Drive/Colab Notebooks/adaptHAN/AOBDL_code/data'\n#3. Read file as panda dataframe\ntrain = pd.read_csv(f'{data_path}/train_cleaned_no_punkt.csv')\nelse:\n#2. Get the file\ndata_path = '../data'\ncodes_path = '../AOBDL_TML'\ntrain['mal'] = train[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum(axis=1) >= 1\ntrain.comment_text.fillna(\"empty\", inplace=True)\ntrain = train.drop_duplicates(subset=['comment_text', 'mal'])\ntest_labelled['mal'] = test_labelled[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum(axis=1) >= 1\ntest_labelled.comment_text.fillna(\"empty\", inplace=True)\ntest_labelled = test_labelled.drop_duplicates(subset=['comment_text'])\ntest_unlabelled.comment_text.fillna(\"empty\", inplace=True)\nrs=42\ndef strat_split(strat=False):\nglobal rs\nif strat:\nfrom sklearn.model_selection import train_test_split\nX_train1, X_test1, y_train1, y_test1 = train_test_split(train.drop('mal', axis=1), train.mal, stratify=train.mal, test_size=0.29, random_state=rs )\nX = np.concatenate((X_train1.comment_text, X_train2.comment_text))\ny_test = np.concatenate((y_test1, y_test2))\nelse:\nX = np.concatenate((train.comment_text, test_labelled.comment_text))\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nX = shuffle(X, random_state=rs)\nX, X_test, y, y_test = train_test_split(X, y, stratify=y, test_size=0.3, random_state=rs )\n\nreturn X, X_test, y, y_test\nkf = StratifiedKFold(n_splits=5, random_state=rs)\nauc = []\nfscore_ = []\nc = 0\nword_vectorizer = TfidfVectorizer(\nsublinear_tf = True,\nstrip_accents = 'unicode',\nanalyzer = 'word',\ntoken_pattern = r'\\w{1,}',\nstop_words = 'english',\nngram_range = (1, 1),\nmax_features = 40000)\n#auc_pr = 0.093434375824395\nc = 0\nC_parameter = np.arange(0.1, 1, 0.1)\n# use best C\nfor c_p in C_parameter:\nfor c, (train_index, val_index) in enumerate(kf.split(X, y)):\nX_train, X_val = X[train_index],\nword_vectorizer.fit(X_train)\ntrain_word_features = word_vectorizer.transform(X_train)\ny_train = y_train.astype('int')\nclassifier = LogisticRegression(C=c_p, solver='sag', random_state=rs)\nclassifier.fit(train_word_features, y_train)\nprobs = classifier.predict_proba(val_word_features)[:,1]\nauc_roc = roc_auc_score(y_val, probs)\n\nthreshold = 0.3\nprobs_class = probs.copy()\nprobs_class[probs_class >= threshold] = 1\nprecision = precision_score(y_val, probs_class)\nprint(f' {threshold} fold {c} precision {round(precision, 3)} recall {round(recall, 3)} fscore {round(fscore,3)}')\nif len(C_parameter)==1:\n# print performance\nprint(f'---------------------------------------------')\nprint(f'FOLD {c}: AUC PR-C = {round(auc_pr, 3)}, AUC ROC = {round(auc_roc, 3)}')\nprint(f'')\nauc.append(auc_pr)\nif len(C_parameter)!=1:\nprint(f'PARAMETER C = {c_p}')\n# print performance\nprint(f'-----------------------------------------------')\nprint(f'CV average: AUC PR-C = {round(np.array(auc).mean(), 3)}, AUC ROC = {round(np.array(roc).mean(), 3)}, FSCORE = {round(np.array(fscore_).mean(), 3)}')\nprint(f'')\n# print performance\nprint(f'CV std: AUC PR-C = {round(np.array(auc).std(), 3)}, AUC ROC = {round(np.array(roc).std(), 3)}, FSCORE = {round(np.array(fscore_).std(), 3)}')\n# TRAIN ON WHOLE DAATA AND PREDICT ON TEST\nword_vectorizer.fit(X)\ntrain_word_features = word_vectorizer.transform(X)\nclassifier = LogisticRegression(C=1, solver='sag', random_state=rs)\nclassifier.fit(train_word_features, y)\nprobs = classifier.predict_proba(test_word_features)[:,1]\nauc_roc = roc_auc_score(y_test, probs)\n# print performance\nprint(f'-----------------------------------------')\nprint(f'TEST: AUC PR-C = {round(auc_pr, 4)}, AUC ROC = {round(auc_roc, 4)}')\nprint(f'')\nthreshold = 0.3\nprobs_class = probs.copy()\nprobs_class[probs_class >= threshold] = 1\nprecision = precision_score(y_test, probs_class)\nprint(f' {threshold} precision {round(precision, 3)} recall {round(recall, 3)} fscore {round(fscore,3)}')\nfor c, (train_index, val_index) in enumerate(kf.split(X, y)):\nclassifier = RandomForestClassifier(n_estimators=600, max_depth=None, max_features='auto',\nmin_samples_split=2, verbose = True, n_jobs=20, random_state=rs)\n# print performance\nprint(f'---------------------------------------------')\nprint(f'FOLD {c}: AUC PR-C = {round(auc_pr, 3)}, AUC ROC = {round(auc_roc, 3)}')\nprint(f'')\nprint(f'-----------------------------------------------')\nprint(f'CV average: AUC PR-C = {round(np.array(auc).mean(), 3)}, AUC ROC = {round(np.array(roc).mean(), 3)}, FSCORE = {round(np.array(fscore_).mean(), 3)}')\n# print performance\nprint(f'CV std: AUC PR-C = {round(np.array(auc).std(), 3)}, AUC ROC = {round(np.array(roc).std(), 3)}, FSCORE = {round(np.array(fscore_).std(), 3)}')\nclassifier = RandomForestClassifier(n_estimators=600, max_depth=None, max_features='auto',\nprint(f'TEST: AUC PR-C = {round(auc_pr, 3)}, AUC ROC = {round(auc_roc, 3)}')\npenalty = ['l2', 'l1']\nalpha = [0.000001, 0.001, 0.1]\nmax_iter = [1000, 15000]\n# best parameters\npenalty = ['l1']\nalpha = [0.00001]\nmax_iter = [1000]\nfor p in penalty:\nfor a in alpha:\nfor i in max_iter:\nauc = []\nfscore_ = []\nc = 0\nprint(f'-------------')\nprint(f'penalty {p}')\nclassifier = SGDClassifier(n_jobs=20, random_state=rs, loss='log', shuffle=False,\npenalty=p, alpha=a, max_iter=i)\nthreshold = 0.1\nclassifier = SGDClassifier(n_jobs=20, random_state=rs, loss='log', shuffle=False,\npenalty=penalty[0], alpha=alpha[0], max_iter=max_iter[0])\nthreshold = 0.1\n# muner of rounds\nmax_rounds = 600\nstopping = 600\nverbose = 200\n# LGB parameters\nlgb_params = {\n'boosting_type': 'gbdt',\n'objective': 'binary',\n'metrics': 'binary_logloss',\n'bagging_fraction': 0.9,\n'lambda_l1': 0.1,\n'min_split_gain': 0.01,\n'min_child_weight': 2,\n'silent': True,\n'verbosity': 100,\n'learning_rate': 0.1,\n'max_depth': 7,\n'scale_pos_weight': 1,\n'n_estimators': max_rounds,\n'nthread' : 20,\n'random_state': rs,\nclassifier = lgb.LGBMClassifier(**lgb_params)\n", "output_sequence": "Antisocial Online behaivor detection. Contains traditional machine learning methods: ridge regression, SVM, random forest, lightGBM"}, {"input_sequence": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport os\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split, KFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nimport random\nfrom sklearn import metrics\nfrom collections import Counter\nimport argparse\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import roc_auc_score, average_precision_score, f1_score, precision_score, recall_score\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import ParameterGrid\nif os.name=='posix':\n# GOOGLE COLAB SETUP\n# Load the Drive helper and mount\nfrom google.colab import drive\n# This will prompt for authorization.\ndrive.mount('/content/drive')\n\n#2. Get the file\ndata_path = 'drive/My Drive/Colab Notebooks/adaptHAN/AOBDL_code/data'\n#3. Read file as panda dataframe\ntrain = pd.read_csv(f'{data_path}/twitter_cleaned_no_punkt.csv')\nelse:\n#2. Get the file\ndata_path = '../data'\ncodes_path = '../AOBDL_TML'\ntrain.drop(['count', 'hate_speech', 'offensive_language', 'neither', 'class'], axis=1, inplace=True)\ntrain.tweet.fillna(\"empty\", inplace=True)\nrs=42\nfrom sklearn.model_selection import train_test_split\nkf = StratifiedKFold(n_splits=5, random_state=rs)\nauc = []\nfscore_ = []\nc = 0\nword_vectorizer = TfidfVectorizer(\nsublinear_tf = True,\nstrip_accents = 'unicode',\nanalyzer = 'word',\ntoken_pattern = r'\\w{1,}',\n#stop_words = 'english',\nngram_range = (1, 1),\n#auc_pr = 0.093434375824395\nc = 0\nC_parameter = np.arange(0.1, 1.1, 0.1)\n# use best C\nfor c_p in C_parameter:\nfor c, (train_index, val_index) in enumerate(kf.split(X, y)):\nX_train, X_val = X.iloc[train_index],\nword_vectorizer.fit(X_train)\ntrain_word_features = word_vectorizer.transform(X_train)\ny_train = y_train.astype('int')\nclassifier = LogisticRegression(C=c_p, solver='sag', random_state=rs)\nclassifier.fit(train_word_features, y_train)\nprobs = classifier.predict_proba(val_word_features)[:,1]\nauc_roc = roc_auc_score(y_val, probs)\n\nthreshold = 0.7\n#for threshold in np.arange(0.1, 1, 0.1):\nprobs_class = probs.copy()\nprobs_class[probs_class >= threshold] = 1\nprecision = precision_score(y_val, probs_class)\nprint(f' {threshold} fold {c} precision {round(precision, 3)} recall {round(recall, 3)} fscore {round(fscore,3)}')\nif len(C_parameter)==1:\n# print performance\nprint(f'---------------------------------------------')\nprint(f'FOLD {c}: AUC PR-C = {round(auc_pr, 3)}, AUC ROC = {round(auc_roc, 3)}')\nprint(f'')\nauc.append(auc_pr)\nif len(C_parameter)!=1:\nprint(f'PARAMETER C = {c_p}')\n# print performance\nprint(f'-----------------------------------------------')\nprint(f'CV average: AUC PR-C = {round(np.array(auc).mean(), 3)}, AUC ROC = {round(np.array(roc).mean(), 3)}, FSCORE = {round(np.array(fscore_).mean(), 3)}')\nprint(f'')\nprint(f'CV std: AUC PR-C = {round(np.array(auc).std(), 3)}, AUC ROC = {round(np.array(roc).std(), 3)}, FSCORE = {round(np.array(fscore_).std(), 3)}')\n# TRAIN ON WHOLE DAATA AND PREDICT ON TEST\nword_vectorizer.fit(X)\ntrain_word_features = word_vectorizer.transform(X)\nclassifier = LogisticRegression(C=1, solver='sag', random_state=rs)\nclassifier.fit(train_word_features, y)\nprobs = classifier.predict_proba(test_word_features)[:,1]\nauc_roc = roc_auc_score(y_test, probs)\n# print performance\nprint(f'-----------------------------------------')\nprint(f'TEST: AUC PR-C = {round(auc_pr, 4)}, AUC ROC = {round(auc_roc, 4)}')\nprint(f'')\nthreshold = 0.7\nprobs_class = probs.copy()\nprobs_class[probs_class >= threshold] = 1\nprecision = precision_score(y_test, probs_class)\nprint(f' {threshold} precision {round(precision, 3)} recall {round(recall, 3)} fscore {round(fscore,3)}')\nstop_words = 'english',\nfor c, (train_index, val_index) in enumerate(kf.split(X, y)):\nclassifier = RandomForestClassifier(n_estimators=600, max_depth=None, max_features='auto',\nmin_samples_split=2, verbose = True, n_jobs=20, random_state=rs)\n# print performance\nprint(f'---------------------------------------------')\nprint(f'FOLD {c}: AUC PR-C = {round(auc_pr, 3)}, AUC ROC = {round(auc_roc, 3)}')\nprint(f'')\nprint(f'-----------------------------------------------')\nprint(f'CV average: AUC PR-C = {round(np.array(auc).mean(), 3)}, AUC ROC = {round(np.array(roc).mean(), 3)}, FSCORE = {round(np.array(fscore_).mean(), 3)}')\nclassifier = RandomForestClassifier(n_estimators=600, max_depth=None, max_features='auto',\nprint(f'TEST: AUC PR-C = {round(auc_pr, 3)}, AUC ROC = {round(auc_roc, 3)}')\npenalty = ['l2', 'l1']\nalpha = [0.000001, 0.001, 0.1]\nmax_iter = [1000, 15000]\n# best parameters\npenalty = ['l1']\nalpha = [1e-05]\nmax_iter = [1000]\nfor p in penalty:\nfor a in alpha:\nfor i in max_iter:\nauc = []\nfscore_ = []\nc = 0\nprint(f'-------------')\nprint(f'penalty {p}')\n\nclassifier = SGDClassifier(n_jobs=20, random_state=rs, loss='log', shuffle=False,\npenalty=p, alpha=a, max_iter=i)\nclassifier = SGDClassifier(n_jobs=20, random_state=rs, loss='log', shuffle=False,\npenalty=penalty[0], alpha=alpha[0], max_iter=max_iter[0])\n# muner of rounds\nmax_rounds = 600\nstopping = 600\nverbose = 200\n# LGB parameters\nlgb_params = {\n'boosting_type': 'gbdt',\n'objective': 'binary',\n'metrics': 'binary_logloss',\n'bagging_fraction': 0.9,\n'lambda_l1': 0.1,\n'min_split_gain': 0.01,\n'min_child_weight': 2,\n'silent': True,\n'verbosity': 100,\n'learning_rate': 0.1,\n'max_depth': 7,\n'scale_pos_weight': 1,\n'n_estimators': max_rounds,\n'nthread' : 20,\n'random_state': rs,\nclassifier = lgb.LGBMClassifier(**lgb_params)\nthreshold = 0.5\nclassifier = lgb.LGBMClassifier(**lgb_params)\nthreshold = 0.5\n", "output_sequence": "Antisocial Online behaivor detection. Contains traditional machine learning methods: ridge regression, SVM, random forest, lightGBM"}, {"input_sequence": "#!/usr/bin/python\n# -*- coding: utf-8 -*-\n################################################################################\n# Computes distances of sentences generated based on several techniques\n# 1. Average aggregation of word2vec word vectors + euclidean distance\n# 3. Word Movers' Dinstances of sentences using word2vec word vectors\n# 3. doc2vec sentence vectors + euclidean distance\n# input: - wordModel: the gensim word2vec model\n# - completeVersions: a dictionary with key: date, value: list of sentences\n# output: folder 'output' with files called METHOD_METRIC.txt for each distance\n# calculation method and each metric (mean, sd, iqr)\n# imports\nimport os\nimport gensim\nfrom gensim import corpora, models\nimport logging\nimport pickle\nimport multiprocessing\nimport math\nfrom nltk.corpus import stopwords\nfrom collections import defaultdict\nimport itertools\nfrom scipy.spatial import distance\nimport numpy as np\nimport warnings\nfrom tqdm import tqdm\nfrom functions import *\n# functions in functions.py\n# main\n# load paragraphs with dates and sentences\ncwd = os.getcwd()\nbd = os.path.normpath(os.path.join(cwd, \"..\"))\nwith open(os.path.join(bd, 'Regulatory_Complexity_Preprocessing', 'completeVersions'),'rb') as g:\nversions = pickle.load(g)\n# prepare for multithreaded processing\nnumCores = int(math.floor(multiprocessing.cpu_count()))\npool = multiprocessing.Pool(processes=numCores)\n# create directory for output measures\ndirectory = os.path.join(os.getcwd(), 'output')\nif not os.path.exists(directory):\nos.mkdir(directory)\nfor method in ['average', 'tfidf', 'wmd', 'doc2vec']:\nprint 'Current method: ', method\n# load neccessary data\nif (method == 'average') or (method == 'tfidf') or (method == 'wmd'):\n# load gensim model\nmodel = gensim.models.Word2Vec.load(os.path.join(bd, 'Regulatory_Complexity_Word_Vectors', 'wordModel'))\n# load sentences for dictionary\nwith open(os.path.join(bd, 'Regulatory_Complexity_Preprocessing', 'modelData'), 'rb') as f:\ndata = pickle.load(f)\nelse:\nmodel = gensim.models.Doc2Vec.load(os.path.join(bd, 'Regulatory_Complexity_Sentence_Vectors', 'sentenceModel'))\n# load stopwords for wmd\nif method == 'wmd':\nfrequency = defaultdict(int)\nfor s in data:\nfor token in s:\nfrequency[token] += 1\nstop = set(stopwords.words('german'))\n# calculate distances\nvMeans = []\nfor key in tqdm(sorted(versions)):\ndate = key\nsentences = versions[key]\nif method == 'wmd':\nsentences = wmdPrep(sentences, frequency, stop)\nsentences = [s for s in sentences if s]\nx = len(sentences)\nindices = [(i,j) for i,j in itertools.product(range(x), range(x)) if (i != j) and (i < j)]\ninputs = [(i[0], i[1], sentences, model) for i in indices]\ndistList = pool.map(wmdDist, inputs)\ndists = [d for d in distList if d != float('inf')]\nelse:\nif method == 'average':\ninputs = [(s, model) for s in sentences]\nsentVecs = pool.map(avAggreg,inputs)\nsentVecs = [s for s in sentVecs if s]\nelif method == 'tfidf':\ndictionary = corpora.Dictionary(sentences)\ncorpus = [dictionary.doc2bow(s) for s in sentences]\ntfidf = models.TfidfModel(corpus)\ncorpus_tfidf = tfidf[corpus]\ndicCorpus = {dictionary.get(id): value for doc in corpus_tfidf for id, value in doc}\ninputs =[(s, model, dicCorpus) for s in sentences]\nsentVecs = pool.map(tfidfAggreg,inputs)\nsentVecs = [s for s in sentVecs if s]\nelif method == 'doc2vec':\nsentVecs = pool.map(getVectors, inputs)\nx = len(sentVecs)\nindices = [(i,j) for i,j in itertools.product(range(x), range(x)) if (i != j) and (i < j)]\ninputs = [(i[0], i[1], sentVecs) for i in indices]\ndists = pool.map(distances,inputs)\n# dispersion measures per version\ntry:\nmean = np.mean(dists)\nexcept RuntimeWarning:\nmean = 0\nstd = np.std(dists)\nstd = 0\nq1 = np.percentile(dists, 25)\niqr = q3 - q1\nexcept:\niqr = 0\nvMeans.append((date, mean))\n# save data\noutputs = [vMeans, vStds, vIQRs]\nnames = ['_means', '_stds', '_iqrs']\nfor i, output in enumerate(outputs):\nwith open(os.path.join(directory, method + names[i] + '.txt'), 'w') as f:\nfor item in output:\nw = str(item[0]) + ',' + str(item[1]) + '\\n'\nf.write(w)\n", "output_sequence": "Computes distances between all sentences in a given version of the GBA and aggregates them to a single complexity measure."}, {"input_sequence": "from scipy.spatial import distance\ndef avAggreg(inputs):\nsentence, model = inputs\nwVecs = []\nfor w in sentence:\ntry:\nwVec = model[w]\nwVecs.append(wVec)\nexcept:\ncontinue\nl = len(wVecs)\navVec = [sum(i)/l for i in zip(*wVecs)]\nreturn avVec\ndef tfidfAggreg(inputs):\nsentence, model, dicCorpus = inputs\nwwVecs = []\nwWeight = dicCorpus[w]\nwwVec = [i*wWeight for i in wVec]\nwwVecs.append(wwVec)\ntfidfVector = [sum(i) for i in zip(*wwVecs)]\nreturn tfidfVector\ndef distances(inputs):\ni, j, sentVecs = inputs\ndist = distance.euclidean(sentVecs[i], sentVecs[j])\nreturn dist\ndef newDistances(inputs):\ni, j, newSentVecs, inputs\ndist = distance.euclidean(newSentVecs[i], oldSentVecs[j])\ndef wmdPrep(paragraph, frequency, stop):\nsentences = []\nfor s in paragraph:\nsent = [w for w in s if w not in stop]\nsent = [token for token in sent if frequency[token] > 2]\nsentences.append(sent)\nreturn sentences\ndef wmdDist(inputs):\ni, j, sentences, model = inputs\ndist = model.wmdistance(sentences[i], sentences[j])\ndef getVectors(inputs):\nlabel = \"_\".join(sentence)\nvector = model.docvecs[label]\nreturn vector\n", "output_sequence": "Computes distances between all sentences in a given version of the GBA and aggregates them to a single complexity measure."}, {"input_sequence": "#!/usr/bin/python\n# -*- coding: utf-8 -*-\n################################################################################\n# HTM Scraper\nimport os\nimport urllib2\nfrom bs4 import BeautifulSoup\nimport pickle\nimport random\n# open proxy list\nwith open('proxies.txt') as fp:\nips = [i.strip().split(':')[0] for i in fp.readlines()]\n# create directory for pages\ndirectory = os.path.join(os.getcwd(), 'pages')\nif not os.path.exists(directory):\nos.mkdir(directory)\n# scrape pages\nc = 1\nfor url in ['http://www.buzer.de/gesetz/962/l.htm',\n# read main page\nhtml = urllib2.urlopen(url).read()\n# convert it to a BeautifulSoup object\nsoup = BeautifulSoup(html)\n# retrieve all of the anchor tags and save relevant pages\ntags = soup('a')\nfor tag in tags:\nlink = tag.get('href', None)\nif link and ('/962/a' in link):\nc += 1\nwhile True:\nip = random.choice(ips)\nproxy_handler = urllib2.ProxyHandler({'http': ip})\nopener = urllib2.build_opener(proxy_handler)\nopener.addheaders = [('User-agent', 'Mozilla/5.0')]\nurllib2.install_opener(opener)\nreq = urllib2.Request('http://www.buzer.de' + link)\nresponse = urllib2.urlopen(req, None, 5)\nhtml = response.read()\nif not 'KWG' in html:\ncontinue\n# check for expired proxies and delete them\nexcept urllib2.HTTPError:\nips.remove(ip)\nexcept Exception:\nname = link.split('/')[-1]\nwith open(os.path.join(directory, name), 'w') as g:\npickle.dump(html, g)\nif c % 10 == 0:\nprint 'Number of pages downloaded: ', c\nprint 'Number of active proxies left: ', len(ips)\n", "output_sequence": "Scrapes the webpage http://www.buzer.de that contains all changes to the German Banking Act since 2006."}, {"input_sequence": "rm(list = ls(all = TRUE))\n#install.packages(\"HAC\")\nlibrary(HAC)\ndata(finData)\nX = finData # read data set\neps = X\n# do plot\npairs(eps, pch = 20)\n", "output_sequence": "COPhac4firmsscatter gives pairwise scatterplots from ARMA-GARCH residuals provided from the HAC package, including Chevron Corporation (CVX), Exxon Mobil Corporation (XOM), Royal Dutch Shell (RDSA) and Total (FP) covering n = 283 observations from 2011-02-02 to 2012-03-19."}, {"input_sequence": "rm(list = ls(all = TRUE))\n# please install these packages if necessary\n# install.packages(\"fGarch\")\nlibrary(fGarch)\n#setwd(\"C:/...\") # please change your working directory\nfile.name = \"COPapp1residual.csv\"\nmain.names = as.matrix(read.csv(file.name, header = FALSE))\nX = read.csv(file.name, header = FALSE) # read data set\nrownames(X) = X[, 1]\nR = apply(log(X[, -1]), 2, diff) # compute log-returns\nX = X[, -1]\nwhere.put = which(diff(as.numeric(format(as.Date(rownames(X), \"%d.%m.%Y\"),\n\"%Y%m\"))) != 0)\nlabels = format(as.Date(rownames(X), \"%d.%m.%Y\"), \"%b %Y\")\neps = matrix(nrow = dim(R)[1], ncol = dim(R)[2])\n# estimate parameters of an ARMA(1,1)+GARCH(1,1) model\nif(exists(\"params\")){rm(params)}\nfor(i in 1:dim(R)[2]){\nfit = garchFit(~arma(1, 0) + garch(1, 1), data = R[, i], trace = F)\neps[,i] = fit@residuals / fit@sigma.t # compute the residuals\nif(!exists(\"params\")){params = c(fit@fit$coef,\n\"BL\" = Box.test(eps[, i],\ntype = \"Ljung-Box\", lag = 12)$p.value,\nelse {params = rbind(params, c(fit@fit$coef,\nBox.test(eps[, i], type = \"Ljung-Box\",\nlag = 12)$p.value,\nks.test(eps[, i], \"pnorm\")$p.value))}\nparams = rbind(params, c(fit@fit$matcoef[, 2], NA, NA))\n}\n# do plot\nlayout(matrix(1:9, nrow=3, byrow = T))\npar(mar=c(1, 2, 1))\nplot(0, 0, axes = FALSE, frame = TRUE, col = \"white\", xlab = \"\", ylab = \"\")\ntext(0, 0, \"APPL\", cex = 2)\npar(mar=c(1, 1.5, 2, 1.5), pty=\"m\")\nplot(eps[, 2], eps[, 1], pch = 19, xlab = \"\", ylab = \"\", axes = F,\nframe = T); axis(3)\n\npar(mar=c(1, 1, 2, 2),pty=\"m\")\nplot(eps[,3], eps[,1], pch = 19, xlab = \"\", ylab = \"\", axes = F,\nframe = T); axis(3);\npar(mar=c(1.5, 2, 1.5, 1), pty=\"m\")\nplot(edf(eps[, 1], F), edf(eps[, 2], F), pch = 19, xlab = \"\",\nylab = \"\", axes = F, frame = T); axis(2)\npar(mar=c(1.5, 1.5, pty=\"m\")\nplot(0, 0, axes = FALSE, frame = TRUE, col = \"white\", xlab = \"\",\nylab = \"\")\ntext(0, 0, \"HP\", cex = 2)\npar(mar=c(1.5, 1, 1.5, 2), pty=\"m\")\nplot(eps[, 3], eps[, 2], pch = 19, xlab = \"\", ylab = \"\", axes = F,\nframe = T); axis(4)\npar(mar=c(2, 2, 1, 1), pty=\"m\")\nplot(edf(eps[, 1], F), edf(eps[, 3], F), pch = 19, xlab = \"\",\nylab = \"\", axes = F, frame = T); axis(1);\npar(mar=c(2, 1.5, 1, 1.5), pty=\"m\")\nplot(edf(eps[, 2], F), edf(eps[, 3], F), pch = 19, xlab = \"\",\nylab = \"\", axes = F, frame = T); axis(1)\npar(mar=c(2, 1, 2), pty=\"m\")\ntext(0, 0, \"MSFT\", cex = 2)\n", "output_sequence": "COPapp1residual shows pairwise scatter plots of based on AR(1)-GARCH(1,1) fitted residuals. The 3 upper triangular plots show the pairwise residuals scatter points. The 3 lower triangular plots show the scatter points computed from empirical cdf of the residuals. Companies Apple (AAPL), the Hewlett Packard (HP) and Microsoft (MSFT) are contained."}, {"input_sequence": "# replace the path of the working directory if necessary\n# setwd(\"C:/R\")\nd = read.csv(\"dax140624.csv\")\nDateInput = as.Date(d[, 1])\nnumOfDate = as.numeric(as.Date(c(DateInput)))\nnewDF = data.frame(d, numOfDate)\nsortNewDF = newDF[order(newDF[, 3]), ]\nnewDF2 = data.frame(sortNewDF, seq(1, length(newDF[, 3]),\nlength.out = length(newDF[, 3])))\ndateNum1 = as.numeric(as.Date(c(\"1986-01-02\", \"1987-01-02\",\n\"1988-01-04\",\n\"1994-01-03\",\n\"1996-01-02\",\n\"1997-01-02\",\n\"1999-01-04\",\n\"2003-01-02\",\n\"2004-01-02\",\n\"2006-01-02\",\nAt = c(newDF2[which(newDF2[, 3] == as.numeric(as.Date(c(\"1986-01-02\")))), 4],\ndateC = c(\"1986\", \"\", \"1988\", \"\", \"1990\",\n\"\", \"1992\", \"\", \"1994\",\n\"2000\", \"\", \"2002\", \"\",\nPt = newDF[, 2]\nP1 = Pt[-length(Pt)]\nP2 = Pt[-1]\nDAXreturn = log(P1/P2)\nDAXreturn = data.frame(DAXreturn, length(DAXreturn):1)\nDAXreturn = DAXreturn[order(DAXreturn[, 2]), ]\nDAXreturn = DAXreturn[, 1]*100\n# do plot\ndev.new(width = 13, height = 7)\nplot(seq(1, length(DAXreturn), length.out = length(DAXreturn)),\nxlab = \"\", ylab = \"\",\nmain = expression(\"DAX Returns (\"*r[t] == log(P[t] / P[t-1])*\")\"),\ncol = \"White\",\naxes = FALSE, xaxt = \"n\", yaxt = \"n\")\nbox()\naxis(1, at = At, labels = F)\naxis(2, at = c(-10, -5, 0, 5, 10))\nmtext(expression(r[t]), side = 2, line = 2.7, at = seq(min(DAXreturn),\nmax(DAXreturn), length.out = 13)[7], font = 2, cex = 1)\nmtext(\"Date\", side = 1, line = 2.7, at = 2800, font = 2, cex = 1)\ntext(cex = 1, x = At - 10, y = -16.3, dateC, xpd = TRUE)\n# time series plot\nlines(1:length(DAXreturn), DAXreturn, pch = 1, lty = 1,\ncol = \"blue\", lwd = 1)\n", "output_sequence": "COPdaxreturn gives a DAX returns' time series plot with a window from 1986-01-03 to 2008-12-30."}, {"input_sequence": "# replace the path of the working directory if necessary\n# setwd(\"C:/R\")\nd = read.csv(\"COPdax140624.csv\") # pls download the pertinent data set.\nDateInput = as.Date(d[, 1])\nnumOfDate = as.numeric(as.Date(c(DateInput)))\nnewDF = data.frame(d, numOfDate)\nsortNewDF = newDF[order(newDF[, 3]), ]\nnewDF2 = data.frame(sortNewDF, seq(1, length(newDF[, 3]),\nlength.out = length(newDF[, 3])))\nPt = newDF[, 2]\nP1 = Pt[ - length(Pt)]\nDAXreturn = log(P1 / P2)\nDAXreturn = data.frame(DAXreturn, length(DAXreturn):1)\nDAXreturn = DAXreturn[order(DAXreturn[, 2]), ]\nDAXreturn = DAXreturn[, 1] * 100\nr = DAXreturn[- which(DAXreturn >= 6.5|DAXreturn <= - 6.5)]\n# do plot\nhist(r, prob = TRUE, 12, main = \"Histogram of DAX Returns\",\ncol = \"Blue\", freq = F, breaks = 24,\nxlab = \"DAX Returns\")\n", "output_sequence": "COPdaxreturn gives a DAX returns' time series plot with a window from 1986-01-03 to 2008-12-30."}, {"input_sequence": "\"\"\" Equivalence between T-Test and Statistical Model \"\"\"\n# author: Thomas Haslwanter, date: Sept-2021\n# Import standard packages\nimport numpy as np\nimport scipy.stats as stats\nimport pandas as pd\nimport statsmodels.formula.api as sm\n# Generate normally distributed data around 'reference'\nnp.random.seed(123)\nreference = 5\ndata = reference + np.random.randn(100)\n# t-test\n(t, pVal) = stats.ttest_1samp(data, reference)\nprint('The probability that the sample mean is different ' +\nf'than {reference} is {pVal:5.3f}.')\n# Equivalent linear model\ndf = pd.DataFrame({'data': data-reference})\nresult = sm.ols(formula='data ~ 1', data=df).fit()\nprint(result.summary())\n", "output_sequence": "Analysis of one group of data\nThis script shows how to - Use a t-test for a single mean - Use a non-parametric test (Wilcoxon signed rank) to check a single mean - Compare the values from the t-distribution with those of a normal distribution"}, {"input_sequence": "\"\"\"Analysis of one group of data\nThis script shows how to\n- Use a t-test for a single mean\n- Use a non-parametric test (Wilcoxon signed rank sum) to check a single mean\n- Compare the values from t-distribution with those of a normal distribution\n\"\"\"\n# author: Thomas Haslwanter, date: Sept-2021\n# Import standard packages\nimport numpy as np\nimport scipy.stats as stats\nimport pingouin as pg\nfrom pprint import pprint\ndef check_mean() -> float:\n\"\"\"Data from Altman, check for significance of mean value.\nCompare average daily energy intake (kJ) over 10 days of 11 healthy women,\nand compare it to the recommended level of 7725 kJ.\nReturns\n-------\np-value : just for testing the function\n\"\"\"\n# Get data from Altman\ninFile = 'altman_91.txt'\ndata = np.genfromtxt(inFile, delimiter=',')\n# Watch out: by default the standard deviation in numpy is calculated with\n# ddof=0, corresponding to 1/N!\nmyMean = np.mean(data)\nmySD = np.std(data, ddof=1) # sample standard deviation\nprint(('Mean and SD: {0:4.2f} and {1:4.2f}'.format(myMean, mySD)))\n# Confidence intervals\ntf = stats.t(len(data)-1)\n# multiplication with np.array[-1,1] is a neat trick to implement \"+/-\"\nci = np.mean(data) + stats.sem(data)*np.array([-1,1])*tf.ppf(0.975)\nprint(('The confidence intervals are {0:4.2f} to {1:4.2f}.'.format(ci[0], ci[1])))\n# Check if there is a significant difference relative to \"checkValue\"\ncheckValue = 7725\n# --- >>> START stats <<< ---\nt, prob = stats.ttest_1samp(data, checkValue)\nif prob < 0.05:\nprint(f'{checkValue:4.2f} is significantly different '+\nf'from the mean (p={prob:5.3f}).')\n# For not normally distributed data, use the Wilcoxon signed rank sum test\n(rank, pVal) = stats.wilcoxon(data-checkValue)\nif pVal < 0.05:\nissignificant = 'unlikely'\nelse:\nissignificant = 'likely'\n# --- >>> STOP stats <<< ---\n\nprint(f'It is {issignificant} that the value is {checkValue:d}')\nreturn prob # should be 0.018137235176105802\n\ndef explain_power() -> None:\n\"\"\" Reproduce most of the parameters from pingouin's 'ttest' \"\"\"\n# generate the data\nnp.random.seed(12345)\nn = 100\ndata = stats.norm(7,3).rvs(n)\n# analysis parameters\nc = 6.5\nalpha = 0.05\n# standard parameters\nmean = np.mean(data)\nsem = stats.sem(data)\nstd = np.std(data, ddof=1)\ndof = n-1\nh1 = stats.t(df=len(data)-1, loc=mean, scale=sem)\n# reproduce the results of the pingouin T-test\nresults = {}\nresults['dof'] = dof\nresults['t_val'] = (mean-c)/sem\nresults['d'] = (mean-c)/np.std(data, ddof=1)\ntc = stats.t(dof).isf(alpha/2)\nresults['p_val'] = stats.t(dof).sf(results['t_val'])*2\nresults['ci'] = h1.ppf([alpha/2, 1-alpha/2])\n# power-calculation\nnct = stats.nct(df=dof, nc=(mean-c)/sem)\nresults['power'] = nct.sf(tc) + nct.cdf(-tc)\npprint(results)\ndef compareWithNormal():\n\"\"\" This function is supposed to give you an idea how big/small the\ndifference between t- and normal distribution are for realistic\ncalculations.\nnormDist = stats.norm(loc=7, scale=3)\ndata = normDist.rvs(100)\ncheckVal = 6.5\n# T-test\nt, tProb = stats.ttest_1samp(data, checkVal)\n# Comparison with corresponding normal distribution\nmmean = np.mean(data)\nmstd = np.std(data, ddof=1)\nnormProb = stats.norm.cdf(checkVal, loc=mmean,\nscale=mstd/np.sqrt(len(data)))*2\n# compare\nprint(f'The probability from the t-test is ' + '{tProb:5.4f}, ' +\nf'and from the normal distribution {normProb:5.4f}')\nreturn normProb # should be 0.054201154690070759\n\nif __name__ == '__main__':\ncheck_mean()\nexplain_power()\ncompareWithNormal()\n", "output_sequence": "Analysis of one group of data\nThis script shows how to - Use a t-test for a single mean - Use a non-parametric test (Wilcoxon signed rank) to check a single mean - Compare the values from the t-distribution with those of a normal distribution"}, {"input_sequence": "%% Figure\nclear all; clc;\nload results_network_dynamic.mat\nfirmnum = 10;\nrolling = 260;\nN = 2088;\nhorizon = 12;\nplott = 1;\nusnum = 5;\n%% Level\ngroup_net_Level_to = [mean(squeeze(mean(net_Level(usnum + 1: firmnum, ...\n1 : usnum, :), 1)), 1);\nmean(squeeze(mean(net_Level(1 : usnum, ...\nusnum + 1: firmnum, :), 1)), 1)]';\nperiods = size(group_net_Level_to, 1);\ngroup_net_Level_in_us = zeros(periods, 1);\nfor i = 1:usnum\nif i ~= j\ngroup_net_Level_in_us = group_net_Level_in_us + squeeze(net_Level(i, j, :));\nend\nend\nend\np = usnum;\ngroup_net_Level_in_us = group_net_Level_in_us./p;\ngroup_net_Level_in_eu = zeros(periods, 1);\nfor i = usnum + 1 : firmnum\ngroup_net_Level_in_eu = group_net_Level_in_eu + squeeze(net_Level(i, j, :));\np = firmnum - usnum;\ngroup_net_Level_in_eu = group_net_Level_in_eu./p;\ngroup_net_Level_in = [group_net_Level_in_us\nfigure\nsubplot(3, 1, 1)\ntm = 1 : 1: N;\nplot(tm(rolling:N), group_net_Level(:, 1), 'b--', 'linewidth', 1.4);\nhold on\nplot(tm(rolling:N), group_net_Level(:, 2), 'r-', 'linewidth', 1.4);\nDay = {'2008', '2016'};\nset(gca, 'xtick', [1 263 524 785 1045 1306 1567 1828 2088]);\nset(gca, 'xticklabel', Day);\nxlim([1, N])\nylim([min(min(group_net_Level(:, 1:2))) - 0.5, max(max(group_net_Level(:, 1:2))) + 0.5])\nh1 = ylabel('');\nh = title('Level factor');\nset([h, h1], 'FontName', 'Times New Roman', 'FontSize', 16)\n%% Slope\ngroup_net_Slope_to = [mean(squeeze(mean(net_Slope(usnum + 1: firmnum, ...\n1 : usnum, :), 1)), 1);\nmean(squeeze(mean(net_Slope(1 : usnum, ...\nusnum + 1: firmnum, :), 1)), 1)]';\nperiods = size(group_net_Slope_to, 1);\ngroup_net_Slope_in_us = zeros(periods, 1);\ngroup_net_Slope_in_us = group_net_Slope_in_us + squeeze(net_Slope(i, j, :));\np = usnum;\ngroup_net_Slope_in_us = group_net_Slope_in_us./p;\ngroup_net_Slope_in_eu = zeros(periods, 1);\ngroup_net_Slope_in_eu = group_net_Slope_in_eu + squeeze(net_Slope(i, j, :));\np = firmnum - usnum;\ngroup_net_Slope_in_eu = group_net_Slope_in_eu./p;\ngroup_net_Slope_in = [group_net_Slope_in_us\nsubplot(3, 1, 2)\nplot(tm(rolling:N), group_net_Slope(:, 1), 'b--', 'linewidth', 1.4);\nylim([min(min(group_net_Slope(:, 1:2))) - 0.5, max(max(group_net_Slope(:, 1:2))) + 0.5])\nh = title('Slope factor');\n%% Curva\ngroup_net_Curva_to = [mean(squeeze(mean(net_Curva(usnum + 1: firmnum, ...\nmean(squeeze(mean(net_Curva(1 : usnum, ...\nperiods = size(group_net_Curva_to, 1);\ngroup_net_Curva_in_us = zeros(periods, 1);\nfor i = 1 : usnum\ngroup_net_Curva_in_us = group_net_Curva_in_us + squeeze(net_Curva(i, j, :));\ngroup_net_Curva_in_us = group_net_Curva_in_us./p;\ngroup_net_Curva_in_eu = zeros(periods, 1);\ngroup_net_Curva_in_eu = group_net_Curva_in_eu + squeeze(net_Curva(i, j, :));\ngroup_net_Curva_in_eu = group_net_Curva_in_eu./p;\ngroup_net_Curva_in = [group_net_Curva_in_us\nsubplot(3, 1, 3)\nplot(tm(rolling:N), group_net_Curva(:, 1), 'b--', 'linewidth', 1.4);\nylim([min(min(group_net_Curva(:, 1:2))) - 0.5, max(max(group_net_Curva(:, 1:2))) + 0.5])\nh = title('Curvature factor');\nxlabel('Year');\nsaveas(gcf, 'network_US_EU', 'png');\n", "output_sequence": "Group the dynamics of connectedness measures in US and European banks for level, slope, curvature factor respectively."}, {"input_sequence": "# load libraries\nlibrary(R.matlab)\nman=c(1,2,3,4,5,6,7,8,9,11,13,14,16,17,18,19,21,23,24)\n# vector of risk attitudes (0...1 scale)\nriskatt=readMat(file.path(dataDir,\"Risk_Att.mat\"))\nrisk_att <- as.vector(riskatt$Risk.Att[,2])\n#plot risk attitudes\nrisk_att2 <- data.frame(cbind(man,risk_att))\nrisk_att2 <- risk_att2[order(-risk_att),]\n#pdf(file = file.path(figDir,\"risk_attitudes.pdf\"),paper=\"special\",height=5,width=9)\nplot(risk_att2[,2], axes=FALSE, xlab=\"Individual ID\", ylab=\"Risk attitude\", ylim=c(-0.1,1.2), col=\"blue\", pch=16, lwd=5, cex=1.5)\nsegments(1:19,rep(-0.25,19), x1 = 1:19, y1 = risk_att2[,2]-0.02, lty=2, col=\"grey\", lwd=2)\naxis(1,lwd=1,cex.axis=0.8,at=1:19,lab=risk_att2[,1])\naxis(1,lwd=1,cex.axis=0.8,at=1:19,lab=risk_att2[,1]) # repeat to make bold\naxis(2, ylim=c(-0.1,1.2))\nbox()\n#dev.off()\n# load data\nload(\"/Users/PetraB/Github/PEC/output_voxel_data.RData\")\n# 2st coordinate = tau level\n# 2st coordinate = type of output (1=1st scores, 2=2nd scores, 3= 1st component, 4 = 2nd component)\n# 3rd coordinate = individual\n# 4th coordinate = brain area\n# stimuluses\nstim_list <- matrix (nrow=256, ncol=19)\nfor (j in 1:19){\nfile <- paste0(100+man[j],\"_output.txt\")\ntemp_data <- read.table(file.path(dataDir,file))\nstim_list[,j] <- temp_data[,4]\n}\nstim_list = round(stim_list/1000)\nfor (i in c(1:256)) {\nif (stim_list[i,j] == 0){stim_list[i,j] = max(stim_list[i,j-1],stim_list[i,j+1])}\n}}\nstim_mtx = matrix ( nrow= 19, ncol = 6)\ncolnames(stim_mtx) <- c (\"PC1-aINS-L\", \"PC1-DMPFC\", \"PC2-aINS-L\", \"PC2-DMPFC\")\nrownames(stim_mtx) <- paste0(\"Ind_\",man)\nstimuluses = rbind(stim_list, stim_list+1,stim_list+2)\nresults = rep(0,19)\nfitall <- vector(mode='list', length=19)\nfor (tau in c(1:19)){\nfor (j in 1:19){\nfor (brainpart_id in 1:3) {\nprint(tau,j, brainpart_id)\nstim_mtx[j,brainpart_id] = mean(output[[tau]][[1]][[j]][[brainpart_id]][stimuluses[,j]])\n}\nresults[tau] <- (summary(lm (risk_att ~ stim_mtx[,1:6]))$r.squared)\nfitall[[tau]] <- lm (risk_att ~ stim_mtx[,1:6])$fitted.values\n} # of tau\n#linear regresion for specific! tau=0.6\n#pdf(file = file.path(figDir,\"pec_fmri_fitted_riskatt.pdf\"),paper=\"special\",height=6,width=6)\nfit <- unlist(fitall[12])\nfin_model <-lm( risk_att ~ fit )\nplot( fit, risk_att,cex=2.5, xlab=\"Fitted value\", ylab=\"Risk attitude\", cex.lab=1.3, main=\"\", ylim=c(-0.2,1.2),xlim=c(-0.25,0.9))\nabline(fin_model, lwd=2, col=\"blue\")\nnewx <- seq(min(fit), 0.9, 0.01)\nlm_int <- predict(fin_model, newdata=data.frame(fit=newx), interval=\"confidence\", level=0.99)\nlines(newx, lm_int[,2], lty=2, lwd=3, col=\"blue\")\nfor (j in 1:19){text(fit[j], risk_att[j],man[j])}\n#plot loadings no1 and no16(ie19)\npar(mfrow=c(1,1))\npdf(file = file.path(figDir,\"loadings_ind1_area1.pdf\"),paper=\"special\",height=3,width=9)\nplot(output2[[0.6*20]][[1]][[1]][[1]], type=\"l\", xlab=\"time\", ylab=\"\", ylim=c(-1500,1500))\ndev.off()\npdf(file = file.path(figDir,\"loadings_ind19_area1.pdf\"),paper=\"special\",height=3,width=9)\nplot(output2[[0.6*20]][[1]][[16]][[1]], type=\"l\", xlab=\"time\", ylab=\"\", ylim=c(-1500,1500))\n# plot all R^2\nplot(results, ylim=c(0,0.7), xlab=\"tau\", ylab=\"expression(R^2)\", type=\"l\", xaxt=\"n\")\npoints(results, ylim=c(0,0.7))\naxis(1, at=c(1,10,19), lab=c(0.05,0.5,0.95))\n", "output_sequence": "This is the application of  PrincipalExpectile algorithms to the fMRI data"}, {"input_sequence": "# ------------------------------------------------------------------------------\n# Quantlet No. 2 : VaR and CVaR Quantlet\n#\n# This quantlet calculates both the Value at Risk and Conditional Value at Risk\n# using three different methods, namely Historical Simulation, Analytical Models\n# and Monte Carlo Simulation.\n# ------------------------------------------------------------------------------\n\n# Clear all variables\nrm(list = ls(all = TRUE))\ngraphics.off()\n# Source Data Preparation Quantlet\nsource(\"Quantlet1/Data_Preparation_Quantlet.R\")\n# Install and load packages\nlibraries = c(\"ggplot2\", \"reshape2\", \"stats\", \"e1071\")\nlapply(libraries, function(x)\nif (!(x %in% installed.packages())) {\n}\n)\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# -----------------------------------------------------------------------------\n# Setting of parameters\nalpha = 0.95 # Confidence level\ninput.pf = stock_data$Returns # Data input: vector or dataframe\n# with one column\nweight = 0.995 # Weighting of observations\nDoF = 4 # Degrees of freedom\nstart.stockPrize = 9000 # Start stock prize for Geometric\n# Brownian Motion (GBM) in MC simulation\nMC.size = 10000 # Sample size of Monte Carlo sample\nnum.randWalk = 100 # Number of random walks per path for\n# GBM\n# Calculation of VaR and CVaR using Historical Simulation\n# with & without weighting\n# Define function for calculating VaR and CVaR using Historical Simulation\n# Arguments: a (confidence level), input.pf (historical portfolio data -\n# log-returns), risk.measure (VaR or CVaR), lambda (weight of observations)\n# Output: Vector containing VaR or CVaR with and without weighting\nHist_Sim = function(a = 0.95, input.pf, risk.measure, lambda = 0.995) {\n\n# Print error message and stop calculation if input.pf is not\n# specified\nif (missing(input.pf)) {\nstop(\"Need to specify input.pf for calculations.\")\n\n# Convert vector to dataframe\n} else if (is.vector(input.pf)) {\ninput.pf = as.data.frame(input.pf)\n# correctly specified\nif (dim(input.pf)[2] > 1) {\nstop(\"input.pf has to be a vector or dataframe with one column\")\n# Rename column with returns\ncolnames(input.pf) = \"Returns\"\nif (!is.numeric(lambda) | lambda <= 0 | lambda >= 1) {\n# Print error message and stop calculation if lambda is not\n# correctly specified\nstop(\"Need to specifiy 0 < lambda < 1 for calculations.\")\n} else if (!is.numeric(a) | a <= 0 | a >= 1) {\n# Print error message and stop calculation if a is not correctly\n# specified\nstop(\"Need to specifiy 0 < a < 1 for calculations.\")\n} else if (a > 0.5) {\n# Transform a for calculations\na = 1 - a\n# Print error message and stop calculation if risk.measure is not\nif (missing(risk.measure)) {\nstop(\"Need to specify risk.measure ('VaR' or 'CVaR') for\ncalculations.\")\n\n# Calculate VaR using Historical Simulation\n} else if (risk.measure == \"VaR\") {\n# Calculate VaR using Historical Simulation with equal weights for\n# each observation\nVaR_hist = quantile(input.pf$Returns, a, names = FALSE)\n# Calculate VaR using historical simulation with higher weights for\n# more recent observations\nw_input.pf = input.pf\n# Calculate respective weights for observations\nw_input.pf$Weights = (lambda^(nrow(w_input.pf) - (1:nrow(w_input.pf))) *\n(1 - lambda)) / (1 - lambda^nrow(w_input.pf))\n# Order observations with respect to log returns\nw_input.pf = w_input.pf[order(w_input.pf$Returns), ]\n# Sum weights until required percentile of distribution is reached:\n# Respective log return is VaR\nw_VaR_hist = min(w_input.pf$Returns[cumsum(w_input.pf$Weights) > a])\nreturn(c(VaR_hist, w_VaR_hist))\n# Calculate CVaR using Historical Simulation\n} else if (risk.measure == \"CVaR\") {\n# Calculate VaR using Historical Simulation\nVaR_vec = Hist_Sim(a = a, input.pf = input.pf, risk.measure = \"VaR\",\nlambda = lambda)\n# Calculate CVaR using Historical Simulation with equal weights for\nCVaR_hist = mean(input.pf$Returns[input.pf$Returns <= VaR_vec[1]])\n# Calculate VaR using Historical Simulation with higher weights for\nw_CVaR_hist = mean(input.pf$Returns[input.pf$Returns <= VaR_vec[2]])\nreturn(c(CVaR_hist, w_CVaR_hist))\n} else {\n}\n# Calculation of VaR and CVaR using analytical models\n# Define function for calculating VaR and CVaR using analytical models\n# log returns), risk.measure (VaR or CVaR), df (degrees of freedom)\n# Output: Vector containing VaR or CVaR under normal and Student's t-distribution\n# and Cornish-Fisher Expansion\nAnalyt_Mod = function(a = 0.95, input.pf, risk.measure, df) {\nstop(\"Need to specify input.pf for calculations.\")\ninput.pf = as.data.frame(input.pf)\nstop(\"input.pf has to be a vector or dataframe with one column\")\nif (missing(df) | !is.numeric(df) | df <= 0) {\n# Print error message and stop calculation if df is not specified\nstop(\"Need to specify df > 0 for calculations.\")\n# Estimate standard deviation and mean from historical data\nsd_pf = sd(input.pf$Returns)\n\n# Print error message and stop calculation if risk.measure is not specified\n# Calculate VaR using analytical models\n# Calculate VaR under assumption of normal distribution\nn_VaR = qnorm(a, 0, 1) * sd_pf + mean_pf # VaR\n# Calculate VaR under assumption of Student's t-distribution\nt_VaR = qt(a, df) * sd_pf * sqrt((df - 2) / df) + mean_pf\n# Calculate VaR with Cornish-Fisher Expansion\nz = qnorm(a, 0, 1)\nskew = skewness(input.pf$Returns)\ncf_VaR = mean_pf + (z + (1 / 6) * (z^2 - 1) * skew +\n(1 / 24) * (z^3 - 3 * z) * (kurt - 3) -\nreturn(c(n_VaR, t_VaR, cf_VaR))\n# Calculate CVaR using analytical models\nVaR_vec = Analyt_Mod(a = a, input.pf = input.pf, risk.measure = \"VaR\",\ndf = df)\n# Calculate CVaR under assumption of normal distribution\nn_CVaR = integrate(function(x) x * dnorm(x, mean_pf, sd_pf),\n-Inf, VaR_vec[1])$value / a\n# Calculate CVaR under assumption of Student's t-distribution\nt_CVaR = - mean_pf - sd_pf * sqrt((df - 2) / df) * dt(qt(a, df), / a *\n(df + qt(a, df)^2) / (df - 1)\n# Calculate CVaR with Cornish-Fisher Expansion\ncf_CVaR = integrate(function(x) (mean_pf + (qnorm(x, 0, 1) + (1 / 6)\n* (qnorm(x, 0, 1)^2 - 1) * skew + (1 / 24) * (qnorm(x, 0, 1)^3\nqnorm(x, 0, 1)^3 - 5 * qnorm(x, 0, 1)) * skew^2) *\nsd_pf), 0, a)$value / a\nreturn(c(n_CVaR, t_CVaR, cf_CVaR))\n# Print error message and stop calculation if risk.measure is not correctly\n# Calculation of VaR and CVaR using Monte Carlo Simulation\n# Arguments: a (confidence level), input.pf (historical portfolio data - log\n# returns), risk.measure (VaR or CVaR), S0 (start stock prize for Geometric\n# Brownian motion (GBM)), sample.size (sample size of simulated sample),\n# num.randWalk (number of random walks within one simulated path in GBM),\n# df (degrees of freedom)\n# Output: Vector containing VaR or CVaR for assumption of normal and of\n# Student's t-distribution\nMonteCarlo = function(a = 0.05, input.pf = NA, risk.measure = NA, S0 = 9000,\nsample.size = 10000, num.randWalk = 100, df) {\n} else if (!is.numeric(S0) | S0 <= 0) {\n# Print error message and stop calculation if a or df are\n# not numeric\nstop(\"S0 has to be a numeric value > 0 for calculations.\")\n} else if (!is.numeric(sample.size) | sample.size <= 0) {\n# Print error message and stop calculation if S0, sample.size\n# or num.randWalk are negative or equal to zero\nstop(\"sample.size has to be a numeric value > 0 for calculations.\")\n} else if (!is.numeric(num.randWalk) | num.randWalk <= 0) {\n# Print error message and stop calculation if df is not correctly\nstop(\"num.randWalk has to be a numeric value > 0 for calculations.\")\n# Determine mean and standard deviation of given data\nmean_pf = mean(input.pf$Returns)\nsd_pf = sqrt(var(input.pf$Returns))\n# Create a matrix of standard normally distributed random numbers\nZ.rv = matrix(rnorm((num.randWalk - 1) * sample.size, 0, 1),\nnrow = num.randWalk - 1)\n# Specify deltat, start stock prize and define matrix for GBM stock prizes\ndt = 1 / num.randWalk\nstart.val = S0\nstock.data = matrix(nrow = num.randWalk, ncol = sample.size)\nstock.data[1, ] = start.val\n# Simulate stock prizes using Geometric Brownian motion\nfor (i in 1:(num.randWalk - 1)) {\nstock.data[i + 1, ] = stock.data[i, ] * exp(sd_pf * sqrt(dt) * Z.rv[i, ]\n+ mean_pf * dt)\n# Save only last row of the data set for calculation of risk measures\n# and transform stock prizes to log-returns\nMC_data_norm = log(stock.data[nrow(stock.data), ]) - log(S0)\n# Simulate data with Student's t-distribution\nMC_data_t = rt(sample.size, df) * sd_pf * sqrt((df - 2) / df) + mean_pf\n# Print error message and stop calculation if risk.measure is not specified\nif (missing(risk.measure)) {\nstop(\"Need to specify risk.measure ('VaR' or 'CVaR') for\ncalculations.\")\n# Calculation of VaR\n} else if (risk.measure == \"VaR\") {\nMC_VaR_norm = quantile(sort(MC_data_norm), a, names = FALSE)\nreturn(list(\"MC_data_norm\" = MC_data_norm, = MC_VaR_norm,\n# Calculation of CVaR\n} else if (risk.measure == \"CVaR\") {\nVaR_norm = quantile(sort(MC_data_norm), a, names = FALSE)\nMC_CVaR_norm = mean(MC_data_norm[MC_data_norm <= VaR_norm])\nreturn(list(\"MC_data_norm\" = MC_data_norm, \"MC_CVaR_norm\" = MC_CVaR_norm,\n# Print error message and stop calculation if risk.measure is not correctly\n# specified\n} else {\ncalculations.\")\n}\n# Function Calls\n# Run Hist_Sim function for VaR and CVaR\nHist.VaR = Hist_Sim(a = alpha, input.pf = input.pf, risk.measure = \"VaR\",\nlambda = weight)\nHist.CVaR = Hist_Sim(a = alpha, input.pf = input.pf, risk.measure = \"CVaR\",\n# Run Analyt_Mod function for VaR and CVaR\nAnalyt.VaR = Analyt_Mod(a = alpha, input.pf = input.pf, risk.measure = \"VaR\",\ndf = DoF)\nAnalyt.CVaR = Analyt_Mod(a = alpha, input.pf = input.pf, risk.measure = \"CVaR\",\n# Run MonteCarlo function for VaR and CVaR\nset.seed(333)\nMC.VaR = MonteCarlo(a = alpha, input.pf = input.pf, risk.measure = \"VaR\",\nS0 = start.stockPrize, sample.size = MC.size,\nnum.randWalk = num.randWalk, df = DoF)\nMC.CVaR = MonteCarlo(a = alpha, input.pf = input.pf, risk.measure = \"CVaR\",\n", "output_sequence": "Calculates Value at Risk and Conditional Value at Risk using three different methods, namely Historical Simulation, Analytical Models and Monte Carlo Simulation. In Historical Simulation VaR and CVaR are calculated using equally weighted as well as differently weighted returns. As Analytical models normal distribution, Student's t-distribution and Cornish-Fisher Expansion were implemented to compute VaR and CVaR. In Monte Carlo Simulation the calculation of VaR and CVaR is based on the generation of normally distributed random log-returns using Geometric Brownian Motion or alternatively on the generation of Student's t-distributed random log-returns."}, {"input_sequence": "# -----------------------------------------------------------------------------\n# Quantlet No. 4 : Portfolio Optimization Quantlet\n#\n# This quantlet calculates the efficient frontier of risky assets incl.\n# a risk free asset and a Value-at-Risk (VaR) or Conditional Value-at-\n# Risk (CVaR) constraint. This model assumes an underlying normal\n# distribution. Further information can be found in Termpaper and its\n# sources\n# -----------------------------------------------------------------------------\n# Clear all variables\nrm(list = ls(all = TRUE))\ngraphics.off()\n# Install and load packages\nlibraries = c(\"ggplot2\", \"tseries\", \"corrplot\", \"reshape2\", \"chron\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# ------------------------ 2. Data --------------------------------------------\nsource('Quantlet1/Data_Preparation_Quantlet.R')\n# ----------------------- 3. Parameter ----------------------------------------\nrf = 0.005 # Risk-free rate\na = 0.995 # Confidence level\ncv = 0.25 # The threshold for the additional risk measure applied\nlc = \"CVaR\" # The additional risk measure applied for optimizing the PF\nt.cost = 0.01 # Transaction costs for rebalancing the portfolio\n# -------------------- 4. Functions -------------------------------------------\n# Cal. the arithmetic return for a portfolio incl. a risk free asset\n# Input returns need to be arithmetic\nPFER = function(data = NA, weights, rfr = 0) {\nif (length(weights) != ncol(data)) {\nstop(\"Error: Dimensions issue\")\n} else {\nPFer = (1 - sum(weights)) * rfr * (nrow(data) / 260) +\n(sapply(1:ncol(data), function(x) prod(1 +\ndata[, x])) - 1) %*% weights\nreturn(PFer)\n}\n}\n# This function calculates the portfolio's standard deviation\nPFSD = function(data, weights) {\n} else {\nsig2 = t(weights) %*% (cov(data) * nrow(data)) %*% weights\nreturn(sqrt(sig2))\n}\n# Calculating the minimum variance portfolio (no risk free asset)\nwmv = function(data = NA) {\nE = cov(data) * nrow(data)\ne = rep(1, length(E[1, ]))\ntest = try(solve(E), silent = T) # test invertability of E\nif (is(test, \"try-error\")) {\nstop(\"cov(data) cannot be inverted\")\nw = (solve(E) %*% e) / (as.numeric(t(e) %*% solve(E) %*% e))\nwmv.er = PFER(data, w)\nr = list(w, as.numeric(wmv.er),\nnames(r) = c(\"Portfolio Weights:\", \"Expected Return:\",\n\"Standard Deviation:\")\nreturn(r)\n# Calculate the efficient portfolio incl. a risk free asset given a wished mean\nwts = function(wmu = NA, rfr = 0.005, data = NA, smean = sapply(1:ncol(data),\nfunction(x) prod(1 + data[, x])) - 1) {\ne = rep(1, length(smean))\nw1 = as.numeric((wmu - rf) / (t(smean - rf * e) %*% solve(E) %*%\n(smean - rf * e)))\nw2 = solve(E) %*% (smean - rf * e)\nwts = w1 * w2\nwts.er = PFER(data, wts, rf)\nrf.ratio = 1 - sum(wts)\nr = list(wts, as.numeric(wts.er), rf.ratio)\n\"Standard Deviation:\", \"Risk-Free Ratio:\")\n# Calculation of the tangent portfolio\n# Input returns need to be arithmetic\nwtpf = function(data = NA, rfr = 0.005,\nsmean = sapply(1:ncol(data),\nfunction(x) prod(1 + data[, x])) - 1) {\nw1 = solve(E) %*% (smean - rfr * e)\nw2 = as.numeric(1 / (t(e) %*% solve(E) %*% (smean - rfr * e)))\nwt = w1 * w2\nwt.er = PFER(data, wt, rfr)\nrf.ratio = 1 - sum(wt)\nr = list(wt, as.numeric(wt.er), rf.ratio)\n# Efficient Frontier without a risk free asset\nwef = function(wmu = NA, data = NA,\nsmean = sapply(1:ncol(data),\nfunction(x) prod(1 + data[, x])) - 1) {\nM = cbind(smean, e)\nB = t(M) %*% solve(E) %*% M\nmp = c(wmu, 1)\nw = solve(E) %*% M %*% solve(B) %*% mp\nw.er = PFER(data, w)\nr = list(w, as.numeric(w.er),\n# Value-at-Risk-Constraint\nVaR.C = function(sig = NA, alpha = 0.999, V = NA) {\nif (alpha <= 0.5) {\nreturn(print(\"Error: Alpha has to be set between 0.5 and 1\"))\nVC = -1 * qnorm(1 - alpha) * sig - V\nreturn(VC)\n# Conditional-Value-at-Risk-Constraint\nCVaR.C = function(sig = NA, alpha = 0.999, CV = NA) {\nCVARC = -1 * (integrate(function(x) x * dnorm(x),\n-Inf, qnorm(1 - alpha))$value /\n(1 - alpha)) * sig - CV\nreturn(CVARC)\n# Intersection between eff. frontier incl. a risk free asset\n# and linear constraint\nLCon.IS = function(LC = \"CVaR\", data = NA, rfr = 0.005, a = 0.995,\ncv = 0.25) {\nif (LC == \"CVaR\") {\ncvar = matrix(c(-1 * (integrate(function(x) x * dnorm(x),\n-Inf, qnorm(1 - a))$value /\n(1 - a)), -1, -(0.05 - 0.03) /\n(as.numeric(wts(0.05, rfr, data)[3]) -\nbyrow = TRUE, nrow = 2)\ncvar.sol = c(cv, rfr)\ncvar.is = solve(cvar, cvar.sol)\nnames(cvar.is) = c(\"SD\", \"ER\")\nreturn(cvar.is)\nif (LC == \"VaR\") {\nvar = matrix(c(-1 * qnorm(1 - a), -1, -(0.05 - 0.03) /\n(as.numeric(wts(0.05, rfr, data)[3]) -\nbyrow = TRUE, nrow = 2)\nvar.sol = c(cv, rfr)\nvar.is = solve(var, var.sol)\nnames(var.is) = c(\"SD\", \"ER\")\nreturn(var.is)\nprint(\"Error: Parameter LC must be VaR or CVaR\")\n# Optimizes a given portfolio\nPF.Opt = function(Start.PF.mu = NA, data = NA, risk.m = \"VaR\", rfr = 0.005,\na = 0.995, cv = 0.25) {\nif (!is.numeric(Start.PF.mu)) {\nstop(\"Error: Start.PF.mu must be numeric\")\nopf = wts(wmu = Start.PF.mu, rfr, data = data)\n# Optimize portfolio without linear constraint\nif (opf[2] < 0) {\nopf = wts(wmu = PFER(data, weights = -1 * unlist(opf[1],\nuse.names = FALSE),\nrfr), data = data)\n}\n# Check linear constraint\nif (LCon.IS(LC = risk.m, data = data, rfr = rfr, a = a, cv = cv)[1] >= 0) {\nas.numeric(opf[3])) {\nopf = wts(wmu = LCon.IS(LC = risk.m, data = data, rfr = rfr, a = a,\ncv = cv)[2], rfr, data = data)\n}\nreturn(opf)\n# Calculates a portfolio's value and the new weights over time\nPF_NLV_W = function(start.weights = NA, data = NA,\nrfr = NA, reb = TRUE, t.cost = 0.01) {\n\nif (length(start.weights) != ncol(data)) {\nrf.ratio = (1 - sum(as.numeric(start.weights)))\n\nif (reb == TRUE) {\nrebalanceBins = ifelse(length(which(grepl(\"01-02\", rownames(data)))) == 0 |\nwhich(grepl(\"01-02\", rownames(data))) %in% c(1, 2),\n} else {\nrebalanceBins = 0\ndata = as.matrix(data)\npval = vector() # Vector of portfolios NLV\npval.p[1] = sum(start.weights) # Initial risky asset portfolio NLV\npfrt = vector() # Vector of portfolio returns\nweight = list() # List containing weights over time\nweights = start.weights / sum(start.weights)\nweight[[1]] = weights # Initial portfolio weights in percent\nweights.m = list() # Weights in the means of invested money\nweights.m[[1]] = start.weights # Initial portfolio weights in money\nfor (i in 1:nrow(data)) {\npfrt[i] = data[i, ] %*% weights # stock portfolio returns\npval.p[i + 1] = pval.p[i] * (1 + pfrt[i])\npval[i + 1] = pval.p[i + 1] + rf.ratio * (1 + rfr * ((i + 1) / 260))\n\nif (i %in% rebalanceBins) {\npval.p[i + 1] = pval.p[i + 1] * (1 - t.cost)\nrf.ratio * (1 + rfr * ((i + 1) / 260)) *\n(1 - t.cost)\nweights = start.weights / sum(start.weights)\nweights.m[[i + 1]] = pval.p[i + 1] * weights\n} else {\nweights.m[[i + 1]] = weights.m[[i]] * (1 + data[i, ])\nweights = weights.m[[i + 1]] / pval.p[i + 1]\nweight[[i + 1]] = weights\ndw = t(data.frame(matrix(unlist(weight),\nnrow = length(unlist(weight[1])))))\nrownames(dw) = c(as.character(as.Date(rownames(data)[1]) - 1),\nrownames(data))\ncolnames(dw) = colnames(data)\nret = list(pval, dw)\nnames(ret) = c(\"NLV\", \"Weights\")\nreturn(ret)\n# Calculate the Sharpe-Ratio of a Strategy\n# Uses a time series of strategies NLV to calculate Sharpe-Ratio\nSharpe = function(ts = NA,rfr = 0) {\nif (!is.numeric(rfr)) {\nstop(\"Error: rfr needs to be numeric\")\n} else if (!is.vector(ts,mode=\"numeric\")) {\nstop(\"Error: ts needs to be a numeric vector\")\nmean = (prod(1 + (diff(ts) / ts[-length(ts)])) - 1)\nsd = sd(diff(ts) / ts[-length(ts)]) * sqrt(length(ts))\nreturn((mean - rfr) / sd)\n# ----------------------- 5. Data Seperation ----------------------------------\n# Extract the first year, e.g. 2013\nFirst_Year = min(unique(format(as.Date(rownames(stock_data$Stock_Returns)),\n\"%Y\")))\n# Save returns of the first year in data.frame\nReturn_First_Year = exp(stock_data$Stock_Returns[format(as.Date(\nrownames(stock_data$Stock_Returns)), \"%Y\") == First_Year, ]) - 1\n# Return of the other years\nReturn_Other = exp(stock_data$Stock_Returns[!format(as.Date(\n# Initial portfolio weights\nwMPF = stock_data$Weights[1, ]\n# ----------------------- 6. Calculations -------------------------------------\n# Calculate the efficient frontier incl. a risk free asset\nw.rf.eff.front = lapply(seq(-1, 1, 0.001),\nfunction(x) wts(x,\ndata = Return_First_Year,\ner.rf.eff.front = as.numeric(sapply(seq(-1, 1, 0.001),\nfunction(x) wts(x,\ndata = Return_First_Year,\nsd.rf.eff.front = as.numeric(sapply(seq(-1, 1, 0.001),\nrfr = rf)[3]))\n# Calculate the efficient frontier without a risk free asset\ner.eff.front = as.numeric(sapply(seq(-1, 1, 0.001),\nfunction(x) wef(x,\ndata = Return_First_Year)[2]))\nsd.eff.front = as.numeric(sapply(seq(-1, 1, 0.001),\ndata = Return_First_Year)[3]))\n# Calculate the Minimum-Variance-Portfolio\nMVP = data.frame(SD = wmv(Return_First_Year)[3],\ncolnames(MVP) = c(\"SD\", \"ER\")\n# Calculate the Tangent-Portfolio\nTP = data.frame(SD = wtpf(Return_First_Year, rfr = rf)[3],\ncolnames(TP) = c(\"SD\", \"ER\")\n# Calculate MPF\nMPF = data.frame(SD = PFSD(Return_First_Year, wMPF),\nER = PFER(Return_First_Year, wMPF, rfr = rf))\ncolnames(MPF) = c(\"SD\", \"ER\")\n# Calculate VaR-constraint\nVAR = data.frame(SD = seq(0,\nas.numeric(wts(1, rf, Return_First_Year)[[3]]),\n0.001),\nER = as.numeric(sapply(seq(0,\nas.numeric(wts(1,\nReturn_First_Year)[[3]]),\n0.001),\nfunction(x) VaR.C(sig = x, alpha = a, V = cv))))\ncolnames(VAR) = c(\"SD\", \"ER\")\n# Calculate CVaR-constraint\nCVAR = data.frame(SD = seq(0,\nrf,\nfunction(x) CVaR.C(sig = x, alpha = a, CV = cv))))\n# ----------------------- 7. Optimize given portfolio -------------------------\n# Optimal portfolio\nOPF = data.frame(SD = PF.Opt(Start.PF.mu = as.numeric(MPF[2]),\ndata = Return_First_Year,\nrisk.m = lc, rfr = rf, a = a, cv = cv)[3],\nER = PF.Opt(Start.PF.mu = as.numeric(MPF[2]),\ndata = Return_First_Year,\nrisk.m = lc, rfr = rf, a = a, cv = cv)[2])\ncolnames(OPF) = c(\"SD\", \"ER\")\n# 8. Calculate portfolio over time, prepare graphics & calculate Sharpe-Ratio\n# Portfolio without optimization and without rebalancing\nNormPF = PF_NLV_W(start.weights = wMPF,\ndata = Return_Other, rfr = rf,\nreb = FALSE, t.cost = 0)\n# Prepare data for Weights Plot\nNormPF_1 = melt(NormPF$Weights)\ncolnames(NormPF_1) = c(\"Date\", \"Symbol\", \"Weights\")\nNormPF_1$Date = as.Date(NormPF_1$Date)\n# Portfolio without optimization and with annual rebalancing\nRegularPF = PF_NLV_W(start.weights = wMPF, data = Return_Other,\nrfr = rf, reb = TRUE, t.cost = t.cost)\n# Prepare data for Weights plot\nRegularPF_1 = melt(RegularPF$Weights)\ncolnames(RegularPF_1) = c(\"Date\", \"Symbol\", \"Weights\")\nRegularPF_1$Date = as.Date(RegularPF_1$Date)\n# Portfolio with optimization and with annual rebalancing\nOptPF = PF_NLV_W(start.weights =\nas.numeric(unlist(PF.Opt(Start.PF.mu = as.numeric(MPF[2]),\ndata = Return_First_Year,\ndata = Return_Other, rfr = rf, reb = TRUE, t.cost = t.cost)\nOptPF_1 = melt(OptPF$Weights)\ncolnames(OptPF_1) = c(\"Date\", \"Symbol\", \"Weights\")\nOptPF_1$Date = as.Date(OptPF_1$Date)\n# Combine strategies into one data frame\nOverall.Val = data.frame(\"NormPF\" = NormPF$NLV,\n\"RegularPF\" = RegularPF$NLV,\n# Calculate ER, SD & Sharpe Ratio of the three strategies\nsapply(list(OptPF$NLV, NormPF$NLV, RegularPF$NLV),\nfunction(x) (prod(1 + (diff(x) / x[-length(x)])) - 1)) # ER\nfunction(x) sd(diff(x) / x[-length(x)]) * sqrt(length(x))) # SD\nfunction(x) Sharpe(ts = x, rfr = rf)) # Sharpe-Ratio\n", "output_sequence": "Optimizes a given portfolio, calculates the efficient frontier and calculates the portfolios net liquidation value over time"}, {"input_sequence": "# -----------------------------------------------------------------------------\n# Quantlet No. 5 : Plot Quantlet\n#\n# This quantlet covers the respective plots of the quantlets 2 to 4, namely the\n# VaR & CVaR quantlet, the Testing quantlet and the Portfolio Optimization\n# quantlet. All plots are generated by means of the R function 'ggplot' of the\n# corresponding R package 'ggplot2'. Please refer to the termpaper for a more\n# detailed explanation of the code.\n# -----------------------------------------------------------------------------\n# Source Testing Quantlet (automatically sources VaR & CVaR and Data Prep.\n# Quantlets)\n# Takes about 16 minutes\nsource(\"Quantlet3/Testing_Quantlet.R\")\n# Install and load packages\nlibraries = c(\"ggplot2\", \"metRology\", \"grid\", \"gridExtra\")\nlapply(libraries, function(x)\nif (!(x %in% installed.packages())) {\ninstall.packages(x)\n}\n)\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# 1. Plots for VaR & CVaR Quantlet\n# Specifiy mean and standard deviation of given log-returns\nsd_pf = sd(input.pf$Returns)\n# ---------------------------------------------------------------------------\n# 1.1 Plots for Analytical VaR and CVaR (normal and Student's t-distribution)\n# Define function for plotting analytical densities (normal and Student's t)\n# with corresponding VaRs and CVaRs\n# Arguments: distr (type of distribution - 'normal' or 't'), val.VaR\n# (calculated VaR for specified distribution), val.CVaR\n# (calculated CVaR for specified distribution), pos.labelVaR and pos.labelCVaR\n# (position factor for VaR and CVaR labels, df (degrees of freedom),\n# plot.title (default = \"\")\n# Output: No explicit output; plot is automatically displayed\nAnalyt.plot = function(distr, val.VaR, pos.labelVaR,\ndf = NA, plot.title = \"\") {\nif (missing(distr) | !(distr %in% c(\"normal\", \"t\"))) {\nstop(\"Need to specify distr = 'normal' or 't' for plotting\")\n} else if (missing(val.VaR) | !is.numeric(val.VaR)) {\nstop(\"Need to specify val.VaR as a numeric value for plotting\")\n} else if (missing(val.CVaR) | !is.numeric(val.CVaR)) {\nstop(\"Need to specify val.CVaR as a numeric value for plotting\")\n} else if (distr == \"t\" & (missing(df) | df <= 0)) {\nstop(\"Need to specify df > 0 for t-distribution\")\n} else {\nx = seq(-0.07, 0.07, length = 500) # x-coordinates\nif (distr == \"normal\") {\n# y-coordinates for normal distribution\ny = dnorm(x, mean_pf, sd_pf) # y-coordinates of given x\n} else {\n# y-coordinates for non-standard Student's t-distribution\ny = dt.scaled(x, df, mean_pf, sd_pf * sqrt((df - 2) / df))\n}\n# Create dataframe with calulated x- and y-coordinates\ndat = data.frame(x, y)\n\nggplot(data = dat, mapping = aes(x = x, y = y)) +\ngeom_line() + # Connect points with line\nxlab(\"Daily Log-Returns\") + # Add x-axis label\nggtitle(label = plot.title) + # Add plot title\n# Add vertical lines indicating VaR and CVaR\ngeom_segment(aes(x = val.VaR, y = 0, xend = val.VaR, yend = y.VaR)) +\n# Add corresponding VaR and CVaR as labels\nannotate(\"text\", x = val.VaR * pos.labelVaR, y = y.VaR,\nlabel = paste(\"VaR =\", round(val.VaR, 3)), size = 4) +\nannotate(\"text\", x = val.CVaR * pos.labelCVaR, y = y.CVaR,\nlabel = paste(\"CVaR =\", round(val.CVaR, 3)), size = 4) +\n# Create yellow area under the curve from -Inf to VaR\ngeom_area(mapping = aes(x = ifelse(x <= val.VaR, x, val.VaR), y = y),\nfill = \"yellow\", alpha = 0.3) +\n# Scale the whole plot between 0 and the greatest y-value\nscale_y_continuous(limits = c(0, max(dat$y))) +\n# Adjust plot format\ntheme(text = element_text(size = 13, color = \"black\"),\nplot.title = element_text(hjust = 0.5),\naxis.text.y = element_blank(), # Delete axis values of y-axis\npanel.background = element_rect(fill = \"white\", colour = NA),\naxis.line.x = element_line(color = \"black\",\narrow = arrow(length = unit(0.25, \"cm\"))),\naxis.line.y = element_line(color = \"black\",\narrow = arrow(length = unit(0.25, \"cm\"))))\n}\n# Analytical VaR and CVaR under normal and t-distribution\n# Save Plot into a pdf\npdf(\"Quantlet5/Analytical VaR & CVaR under normal and t-distribution.pdf\",\nwidth = 10, height = 5)\n# Specifications for the function call of Analyt.plot\ndistr = list(\"normal\", \"t\")\nVaR = list(Analyt.VaR[1],\npos.VaR = c(2.15, 2.25)\ndf = list(NA, 4)\nplot.title = c(\"Normal distribution\", \"Student's t-distribution\")\n# The Analyt.plot function is called two times and the resulting plots are arranged\n# in a matrix with two columns and one row\ndo.call(grid.arrange, c(lapply(1:length(distr), function(x)\nAnalyt.plot(distr[[x]], VaR[[x]], pos.VaR[x],\npos.CVaR[x], df[[x]], plot.title[x])), ncol = 2))\ndev.off()\n# -------------------------------------------------------------------------------\n# 1.2 Plots for VaR and CVaR of Monte Carlo Simulation and Historical Simulation\n# Define function for plotting empirical densities (historical and Monte Carlo)\n# Arguments: dat (historical log-returns or MC simulated log-returns), val.VaR\n# (calculated VaR for specified data), val.CVaR\n# (calculated CVaR for specified data), pos.labelVaR and pos.labelCVaR\n# (position factor for VaR and CVaR labels, plot.title (default = \"\")\nEmp.plot = function(dat, val.VaR, pos.labelVaR,\nplot.title = \"\") {\nif (missing(dat)) {\nstop(\"Need to specify the data of log-returns for plotting\")\n} else if (missing(val.VaR) | !is.numeric(val.VaR)) {\nstop(\"Need to specify val.VaR as a numeric value for plotting\")\n} else if (missing(val.CVaR) | !is.numeric(val.CVaR)) {\nstop(\"Need to specify val.CVaR as a numeric value for plotting\")\n# Estimate the density of the given data using the\n# Kernel density estimator with gaussian kernel function\ndens = density(dat)\ndat.plot = data.frame(x = dens$x, y = dens$y)\n# approxfun() interpolates all points between the given\n# observations of dat.plot\nfunc.dens = approxfun(dens)\n\nggplot(data = dat.plot, mapping = aes(x = x, y = y)) +\ngeom_line() + # Connect points with line\nxlab(\"Daily Log-Returns\") + # Add x-axis label\nggtitle(label = plot.title) + # Add plot title\n# Add vertical lines indicating VaR and CVaR\ngeom_segment(aes(x = val.VaR, y = 0, xend = val.VaR,\nyend = func.dens(val.VaR))) +\ngeom_segment(aes(x = val.CVaR, y = 0, xend = val.CVaR,\nyend = func.dens(val.CVaR))) +\n# Add corresponding VaR and CVaR as labels\nannotate(\"text\", x = val.VaR * pos.labelVaR, y = func.dens(val.VaR),\nlabel = paste(\"VaR =\", round(val.VaR, 3)), size = 4) +\nannotate(\"text\", x = val.CVaR * pos.labelCVaR, y = func.dens(val.CVaR),\nlabel = paste(\"CVaR =\", round(val.CVaR, 3)), size = 4) +\n# Create yellow area under the curve from -Inf to VaR\ngeom_area(mapping = aes(x = ifelse(x <= val.VaR, x, val.VaR), y = y),\nfill = \"yellow\", alpha = 0.3) +\n# Scale the whole plot between 0 and the greatest y-value\nscale_y_continuous(limits = c(0, max(dat.plot$y))) +\n# Adjust plot format\ntheme(text = element_text(size = 13, color = \"black\"),\nplot.title = element_text(hjust = 0.5),\naxis.text.y = element_blank(), # Delete axis values of y-axis\npanel.background = element_rect(fill = \"white\", colour = NA),\naxis.line.x = element_line(color = \"black\",\narrow = arrow(length = unit(0.25, \"cm\"))),\naxis.line.y = element_line(color = \"black\",\narrow = arrow(length = unit(0.25, \"cm\"))))\n# Monte Carlo VaR and CVaR under normal and t-distribution\npdf(\"Quantlet5/MC VaR & CVaR under normal and t-distribution.pdf\",\n# Specifications for the function call of Emp.plot\ndat = list(MC.VaR$MC_data_norm, MC.VaR$MC_data_t)\nVaR = list(MC.VaR[[2]], MC.VaR[[4]])\npos.VaR = c(1.65, 3.1)\n# The Emp.plot function is called two times and the resulting plots are arranged\ndo.call(grid.arrange, c(lapply(1:length(dat), function(x)\nEmp.plot(dat[[x]], VaR[[x]], pos.VaR[x],\npos.CVaR[x], plot.title[x])), ncol = 2))\n# Unweighted and weighted log-returns and corresponding historical\n# VaRs and CVaRs\npdf(\"Quantlet5/Historical VaR & CVaR with equally and differently weighted returns.pdf\",\ndat = list(input.pf$Returns,\nVaR = list(Hist.VaR[1], Hist.VaR[2])\npos.VaR = c(2, 2)\npos.CVaR = c(1.85, 1.85)\nplot.title = c(\"Equally weighted returns\", \"Differently weighted returns\")\n# The Emp.plot function is called again two times and the resulting\n# plots are arranged in a matrix with two columns and one row\ndo.call(grid.arrange, c(lapply(1:length(dat), function(x) Emp.plot(dat[[x]],\nVaR[[x]], pos.VaR[x], plot.title[x])), ncol = 2))\n# 2. Plots for Testing Quantlet\n# --------------------------------------------------\n# 2.1 Plots for visualization of distribution tests\nset.seed(333)\n# Define function for plotting two distributions in order to compare them\n# Arguments: data1 (first data to compare), name_distrbution1 (name of first\n# second distribution)\nplot.distribution = function(data1, name_distrbution1, data2, name_distrbution2) {\nif (missing(data1) | missing(data2)) {\nstop(\"Need to specify both data1 and data2 for plotting\")\n} else if (missing(name_distrbution1) | missing(name_distrbution2)) {\nstop(\"Need to specify the distribution names of both\ndistributions for plotting\")\ndf = data.frame(x = c(data1, data2), Cum.Distrib. = c(rep(name_distrbution1,\nlength(data1)), rep(name_distrbution2, length(data2))))\nggplot(df, aes(x, colour = Cum.Distrib.)) + stat_ecdf() +\nxlab(\"Quantile\") + # Add x-axis label\ntheme(legend.justification = c(1, 0.2), legend.position = c(1, 0.2),\ntext = element_text(size = 13, color = \"black\"),\n# Create one matrix containing all plots resulting from\n# plot.distribution function and save it to a pdf\npdf(\"Quantlet5/Distribution.pdf\", width = 10, height = 10)\ndata1 = list(hist, hist, mc_data_norm,\nnames(data1) = c(\"Hist. data\", \"Hist. data\",\n\"MC data (normal)\", \"MC data (normal)\",\ndata2 = list(rnorm(100 * length(hist), sd(hist)),\nrt(length(hist), 4) * sd(hist) / sqrt(2) + mean(hist),\nrnorm(length(mc_data_norm),\nsd(mc_data_norm)),\nrt(length(mc_data_norm), 4) * sd(mc_data_norm) / sqrt(2) +\nmean(mc_data_norm),\nrnorm(length(mc_data_t), mean(mc_data_t),\nmean(mc_data_t), hist,\nnames(data2) = c(\"Normal\", \"Student\u00b4s t\", \"Normal\", \"Student\u00b4s t\",\n\"Normal\", \"Student\u00b4s t\", \"Hist. data\", \"Hist. data\")\ndo.call(grid.arrange, c(lapply(1:length(data1), function(x)\nplot.distribution(data1[[x]],names(data1)[x], data2[[x]],\nnames(data2)[x])), ncol = 2))\n# --------------------------------------------------------------------------\n# 2.2 Plots for visualization of VaR tests\n# Define function for plotting results of the cTestData function (line 371\n# of Testing quantlet) for the Kupiec Test (VaRTest)\n# Arguments: dat (output data of cTestData function), VaR (vector of VaRs\n# resulting from cTestData function), plot.title (default = \"\")\nplot.VaRTest = function(dat, VaR, plot.title = \"\") {\nif (missing(dat) | missing(VaR)) {\nstop(\"Need to specify both dat and VaR for plotting\")\nplot = ggplot(dat, aes(x = Date, y = PnL))\nplot + geom_bar(stat = \"identity\") + # Create a bar plot\nylab(\"Daily Log-Returns (hist.)\") + # Add y-axis label\nggtitle(label = plot.title) + # Add plot title\n# Add daily estimated VaR as red points to the plot\ngeom_point(aes(x = Date, y = VaR), size = 2, shape = 45,\ncolor = \"red\") +\n# Adjust plot format\ntheme(text = element_text(size = 13, color = \"black\"),\nplot.title = element_text(hjust = 0.5),\naxis.text.x = element_blank(),\naxis.line.y = element_line(color = \"black\"))\n# Create matrix containing plots concerning MC Simulation\n# resulting from plot.VaRTest function and save it to a pdf\npdf(\"Quantlet5/VaRTest_MC.pdf\", width = 6, height = 10)\ndata = list(VarTest.dat.mcnorm$Test.data,\nplot.title = c(\"Monte Carlo Sim. (Normal distribution)\",\n\"Monte Carlo Sim. (Student\u00b4s t-distribution)\")\ndo.call(grid.arrange, c(lapply(1:length(data), function(x)\nplot.VaRTest(data[[x]], VaR[[x]], plot.title[x])), ncol = 1))\n# Create matrix containing plots concerning Historical Simulation\npdf(\"Quantlet5/VaRTest_hist.pdf\", width = 6, height = 10)\ndata = list(VarTest.dat.hist$Test.data,\nplot.title = c(\"Historical Simulation (unweighted)\",\n# Create matrix containing plots for Analytical Models\npdf(\"Quantlet5/VaRTest_ana.pdf\", width = 6, height = 10)\ndata = list(VarTest.dat.ananorm$Test.data,\nVarTest.dat.anacf$Test.data)\nVaR = list(VarTest.dat.ananorm$Test.data$VAR,\nVarTest.dat.anacf$Test.data$VAR)\nplot.title = c(\"Analytical model (Normal distribution)\",\n\"Analytical model (Student\u00b4s t-distribution)\",\n# ------------------------------------------------------------------------\n# 2.3 Plots for visualization of CVaR tests\n# of Testing quantlet) for the McNeil & Frey Test (ESTest)\n# resulting from cTestData function), CVaR (vector of CVaRs\nplot.ESTest = function(dat, VaR, plot.title = \"\") {\nif (missing(dat) | missing(VaR) | missing(CVaR)) {\nstop(\"Need to specify dat, VaR and CVaR for plotting\")\n# Keep only those observations where the estimated VaR is exceeded\ndat_ex = dat[VaR > PnL, ]\n# Plot log-returns and corresponding estimated CVaR (red) where the\n# estimated VaR was exceeded\nplot = ggplot(dat_ex, aes(x = Date_ex, y = PnL_ex))\nplot + geom_bar(stat = \"identity\") + # Create a bar plot\nxlab(\"Days with VaR exceedances\") + # Add x-axis label\nylab(\"Daily Log-Returns for exceeded VaRs\") + # Add y-axis label\nggtitle(label = plot.title) + # Add plot title\n# Add daily estimated CVaR as red lines to the plot\ngeom_point(aes(x = Date_ex, y = CVaR_ex), size = 5,\nshape = 45, color = \"red\") +\npanel.background = element_rect(fill = \"white\", colour = NA),\n# Create matrix containing plots concerning MC Simulation\n# resulting from plot.ESTest function and save it to a pdf\npdf(\"Quantlet5/CVaRTest_MC.pdf\", width = 6, height = 10)\nCVaR = list(VarTest.dat.mcnorm$Test.data$CVAR,\nplot.title = c(\"Monte Carlo model (Normal distribution)\",\n\"Monte Carlo model (Student\u00b4s t-distribution)\")\nplot.ESTest(data[[x]], VaR[[x]],\nplot.title[x])), ncol = 1))\npdf(\"Quantlet5/CVaRTest_hist.pdf\", width = 6, height = 10)\nCVaR = list(VarTest.dat.hist$Test.data$CVAR,\npdf(\"Quantlet5/CVaRTest_ana.pdf\", width = 6, height = 10)\nCVaR = list(VarTest.dat.ananorm$Test.data$CVAR,\nVarTest.dat.anacf$Test.data$CVAR)\n\"Analytical model (Student\u00b4s t distribution)\",\n# 3. Plots for Portfolio Optimization Quantlet\nsource(\"Quantlet4/PortfolioOptimization.R\")\n# Define function for plotting the mu-sigma diagram incl. efficient frontiers\n# for a given portfolio\n# Argument: risk.measure (VaR or CVaR)\n# Output: plot of gererated mu-sigma diagram incl. efficient frontiers\nplot.PortOpt = function(risk.measure = \"VaR\") {\nplot = ggplot(data.frame(ER = er.rf.eff.front, SD = sd.rf.eff.front),\naes(SD, ER)) +\n# eff.frontier incl. risk free asset\ngeom_path(aes(SD, ER, colour = ER >= rf, linetype = ER >= rf,\nsize = ER >= 0, alpha = ER >= 0)) +\n# eff.frontier excl. risk free asset\ngeom_path(aes(x = SD, y = ER,\ncol = ER >= wmv(Return_First_Year)[2],\ndata.frame(ER = er.eff.front, SD = sd.eff.front)) +\n# Add the optimized portfolio and label it\ngeom_point(aes(x = SD, y = ER), data = OPF) +\ngeom_label(label = \"Opt. PF\", aes(x = SD, y = ER, hjust = .5,\nvjust = -0.15), data = OPF) +\n# Add the initial portfolio and label it\ngeom_point(aes(x = SD, y = ER), data = MPF) +\ngeom_label(label =\"Start PF\", aes(x = SD, y = ER, hjust = -0.05,\nvjust = 0.5), data = MPF) +\n# Formats the conditional linetype and colour\nscale_colour_manual(name = \"wmv(Return_First_Year)[2]\",\nvalues = setNames(c(\"darkgreen\",\n\"firebrick1\"), c(T, F))) +\nscale_linetype_manual(name = \"ER >= wmv(Return_First_Year)[2]\",\nvalues = setNames(c(1, 2), c(T, F))) +\nscale_size_manual(name = \"ER >= wmv(Return_First_Year)[2]\",\nvalues = setNames(c(1.2, 0.5), c(T, F))) +\nscale_alpha_manual(name = \"ER >= wmv(Return_First_Year)[2]\",\nvalues = setNames(c(1, 0.5), c(T, F))) +\n# Adjust plot format\ntheme(legend.position = \"none\", panel.background = element_blank(),\naxis.line.x = element_line(color = \"black\",\narrow = arrow(length = unit(0.25,\naxis.line.y = element_line(color = \"black\",\n\"cm\")))) +\nxlab(\"Standard Deviation\") + ylab(\"Expected Return\") +\n# Define the displayed are of the x and y axis\nscale_x_continuous(limits = c(0, 0.5)) +\n# Add addiotional linear risk constraint\nif (risk.measure == \"VaR\") {\nplot = plot + geom_path(aes(x = SD, y = ER), data.frame(ER = VAR[2],\nSD = VAR[1])) + # Var.C\ngeom_label(label = \"VaR Constraint\", aes(x = SD, y = ER,\nhjust = -0.05, vjust = 1),\ndata = data.frame(ER = -cv, SD = 0))\n} else if (risk.measure == \"CVaR\") {\nplot = plot + geom_path(aes(x = SD, y = ER), data.frame(ER = CVAR[2],\nSD = CVAR[1])) + # CVar.C\ngeom_label(label = \"CVaR Constraint\", aes(x = SD, y = ER,\nhjust = -0.05, vjust = 1),\n} else {\nstop(\"Need to specify risk.measure ('VaR' or 'CVaR').\")\nreturn(plot)\n# Create mu-sigma diagram incl. efficient frontiers for given portfolio\n# resulting from plot.PortOpt function and save it to a pdf\npdf(\"Quantlet5/Portfolio_Optimization_Plot.pdf\", width = 7, height = 5)\nplot.PortOpt(risk.measure = lc) #lc = 'CVaR' (see Portfolio Optimization quantlet)\n# Plot development of portfolio weights\n# Define function for plotting the development of portfolio weights for\n# given portfolio\n# Argument: x (a data.frame containing the Dates, Symbols and Weights),\n# y (vector containing the Strategies Name)\nplot.Weights = function(x, y) {\nif (missing(x) | colnames(x)[1] != \"Date\" |\ncolnames(x)[2] != \"Symbol\" | colnames(x)[3] != \"Weights\") {\nstop(\"Need to specify a dataset for x with colums Date, Symbol and Weights\")\nggplot(x, aes(x = Date, y = Weights, fill = Symbol, width = 3)) +\ngeom_bar(stat = \"identity\") +\ntheme(panel.background = element_blank(),\narrow = arrow(length = unit(0.25, \"cm\"))),\naxis.text.y = element_text(size = 6),\nlegend.key.size = unit(0.4, \"cm\")) +\nxlab(\"Date\") +\nylab(paste(\"Weights of\", gsub(\"_1\", \"\", y, perl = TRUE)))\n# Create matrix containing plots of the development of portfolio weights\n# resulting from plot.Weights function and save it to a pdf\npdf(\"Quantlet5/Portfolio_Weights.pdf\", width = 7, height = 5)\nstrategies = list(NormPF_1, RegularPF_1, OptPF_1)\nnames(strategies) = c(\"NormPF_1\", \"RegularPF_1\", \"OptPF_1\")\ndo.call(grid.arrange, c(lapply(1:length(strategies), function(x)\nplot.Weights(strategies[[x]], names(strategies)[x])), ncol = 1))\n# Plot portfolio value over time and save it to a pdf\ndate = RegularPF_1$Date[RegularPF_1$Symbol == stocks[1]]\nOverall.Val = melt(Overall.Val)\nOverall.Val$date = c(date, date)\n# Using Overall.Val generate Plot containing all Strategies to display over time\npdf(\"Quantlet5/Portfolio_value_overTime.pdf\", width = 7, height = 5)\nggplot(Overall.Val, aes(x = date, y = value, colour = variable)) +\ntheme(panel.background = element_blank(),\naxis.line.x = element_line(color = \"black\",\narrow = arrow(length = unit(0.25, \"cm\"))),\naxis.line.y = element_line(color = \"black\",\narrow = arrow(length = unit(0.25, \"cm\")))) +\nxlab(\"Date\") + ylab(\"Portfolios Net Liquidation Value\") +\ngeom_line() + guides(colour = guide_legend(title = \"Strategy\"))\n", "output_sequence": "This quantlet covers all plots for the three quantlets VaR_and_CVaR_Quantlet, Testing_Quantlet and PortfolioOptimization. For VaR_and_CVaR_Quantlet it plots different distributions (normal, Student's t and empirical distributions) with corresponding VaR and CVaR calculated within VaR_and_CVaR_Quantlet. For the Testing_Quantlet it plots two different distributions in order to compare these distributions as well as plotting the results of the Kupiec Test for the VaR and McNeal & Frey test for the CVaR. For the PortfolioOptimization Quantlet, it plots the portfolios weights and let liquidation value over time as well as the efficient frontiers."}, {"input_sequence": "################################## Replace Time Variables #####################################################3 This script\n################################## provides function to plot a variable against the house price\nlibraries = c(\"data.table\", \"ggplot2\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\nprice_per_factor_box <- function(factor, factor_name) {\nsold_per_x <- data.frame(factor, train$SalePrice)\ncolnames(sold_per_x) <- c(factor_name, \"SalePrice\")\n# create boxplot\np <- ggplot(sold_per_x) + geom_boxplot(aes(x = factor, y = SalePrice, group = factor, fill = factor))\n# add title\np <- p + ggtitle(paste(factor_name, \"SalePrice\", sep = \" vs. \"))\n# add colours\np <- p + theme(legend.position = \"none\") + xlab(factor_name)\nreturn(p)\n}\nprice_per_factor_plot <- function(factor, factor_name) {\n# create scatterplot\np <- ggplot(sold_per_x) + geom_point(aes(x = factor, y = SalePrice))\n# add mean and confidence intervall\np <- p + geom_smooth(aes(factor))\np <- p + ggtitle(paste(factor_name, \"SalePrice\", sep = \" vs. \")) + xlab(factor_name)\n# function to do a boxplot for a parameter\nbox_hyperparameter <- function(results, parameter, parameter_name) {\ngrid <- sort(unique(parameter))\np <- ggplot(results) + geom_boxplot(aes(x = parameter, y = rmse, group = parameter, fill = parameter))\np <- p + ggtitle(paste(parameter_name, \"RMSE of log y\", sep = \" vs. \"))\np <- p + theme(legend.position = \"none\") + xlab(parameter_name) + scale_x_discrete(limits = c(grid))\n# function to plot two different parameters in a headmap according to their rmse results\nhyperparameter_heatmap <- function(results, parameter1, name1, {\ngrid1 <- sort(unique(parameter1))\np <- ggplot(results, aes(parameter1, parameter2)) + geom_raster(aes(fill = rmse), interpolate = F)\np <- p + xlab(name1) + ylab(name2)\np <- p + scale_x_discrete(limits = c(grid1)) + scale_y_discrete(limits = c(grid2))\n# TODO:correlation plot of all variables\ncor_plot <- function(x, title) {\ncorrgram(x, order = NULL, lower.panel = panel.shade, upper.panel = panel.pie, text.panel = panel.txt,\nmain = title)\n", "output_sequence": "Tune parameters of the xgboost model using cross validation. The optimal parameter will be used to foreast house price."}, {"input_sequence": "############ Stochastic Gradient Boosting training using package 'xgboost' #############\n### setwd('F:/PHD/IRTG/courses/SPL/Quantlet/xgb_tuning')\nrm(list = ls())\ngraphics.off()\nlibraries = c(\"caret\", \"xgboost\", \"Matrix\", \"ggplot2\", \"Rmisc\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\nsource(\"performanceMetrics.R\") # to get performance metrics\nsource(\"visulizations.R\")\nload(\"basic_processing.RData\")\n# get preprocessed data\ntrain = basic_data$traint\ny = train$y\ntrain$y = NULL\nX_com = rbind(train, test)\n# save result path (change according to experiment here: input date is from quick\n# preprocessing function)\nresult_path = \"output/xgb_basic\"\n########## perform repeated nested cv set cv parameter\nrepetitions = 5 # repetitions of cv\nk_folds = 5 # folds in the cv loop\n# create Grid for GridSearch to tune hyperparameter Tree specific Parameters: maxnodes:\n# longest path of a single tree (decreased performance) colsample_bytree: variable considered\n# at each split subsample: size of the bagging bootstrap sample\nnrounds_fixed = 1000 # number of trees: no need for tuning since early.stopping is possible\neta_fixed = 0.025 # learning rate (fixed for now)\ntreeSpecificGrid = expand.grid(max_depth = seq(10, 16, 2), gamma = seq(0, 6, 2), subsample = seq(0.4,\n0.8, 0.2), colsample_bytree = seq(0.6, 1, 0.2))\n# samplesize could be inspected as well\nnumOfParameter = ncol(treeSpecificGrid)\n# create a matrix to with the GridSearch parameters and their RMSE\nparameter_names = c(\"max_depth\", \"gamma\", \"subsample\", \"colsample_bytree\")\n# set up empty matrices to be filled with rmse and best_paramer (here only lambda)\n### start nested repeated nested cv loop Repetition outer loop create empty vector/matrix to\n### save best results\nparameters = matrix(0, nrow = nrow(treeSpecificGrid), ncol = numOfParameter + 1)\ncolnames(parameters) = c(\"rmse\", parameter_names)\nresult_list = lapply(seq_len(repetitions), function(X) parameters)\n# start repetition loop\nfor (t in 1:repetitions) {\n# draw random integers for the k-folds\nfolds = sample(rep(1:k_folds, length.out = nrow(train)))\n# create empty vector/matrix to save best results\ntuning_results = cbind(rep(0, nrow(treeSpecificGrid)),\ncolnames(tuning_results) = c(\"rmse\", parameter_names)\n# start crossvalidation loop\nfor (k in 1:k_folds) {\n# split into training and validation set\nindexValidation = which(folds == k)\ntraining = train[-indexValidation, ]\n\n# convert for data into a format xgb.train can handle\ndtrain = xgb.DMatrix(data = as.matrix(training), label = y_training)\nwatchlist = list(eval = dvalidation, train = dtrain)\n# start GridSearch loop\nfor (i in 1:nrow(treeSpecificGrid)) {\n# determine arbitrary xgboost parameters in a list\nxgb_paramters = list(eta = eta_fixed, max.depth = treeSpecificGrid$max_depth[i], gamma = treeSpecificGrid$gamma[i],\ncolsample_bytree = treeSpecificGrid$colsample_bytree[i], subsample = treeSpecificGrid$subsample[i],\neval_metric = \"rmse\", maximize = FALSE)\n# fit the xgboost\nxgbFit = xgb.train(params = xgb_paramters, data = dtrain, booster = \"gbtree\", nround = nrounds_fixed,\nverbose = 1, early.stop.round = 50, objective = \"reg:linear\", watchlist = watchlist)\n# predict SalePrice\nyhat = predict(xgbFit, newdata = dvalidation)\n# fill the first column of this matrix with the rmse results (of the log outputs)\nvalidation_error = rmse_log(y_validation, yhat)\ntuning_results[i, 1] = validation_error\n# save all training results as csv file (fold_k_reptetion_t)\nwrite.csv(tuning_results, file = paste(result_path, t, k, \".csv\", sep = \"_\"))\n}\n}\nresult_list[[t]] = tuning_results\n}\n# print result list\n### read the results and do the plots\nload_data = function(wd = \"output/\") {\nsetwd(wd)\nfiles = list.files()\ntables = lapply(files, read.csv)\ndf = do.call(rbind, tables)\nreturn(df)\nresults = load_data()\n# plot result per parameter\nmax_depth = box_hyperparameter(results, results$max_depth, \"Max Depth\")\ncolbytree = box_hyperparameter(results, results$colsample_bytree, \"Column by Tree\")\nmultiplot(max_depth, gamma, subsample, colbytree, cols = 2)\np1 = hyperparameter_heatmap(results, results$max_depth, results$gamma, \"Max Depth\", \"Gamma\")\np2 = hyperparameter_heatmap(results, results$subsample, results$colsample_bytree, \"Bootstrap Sample\",\n\"Variables by Tree\")\np3 = hyperparameter_heatmap(results, results$max_depth, results$colsample_bytree, \"Max Depth\",\nmultiplot(p1, p2, cols = 3)\n", "output_sequence": "Tune parameters of the xgboost model using cross validation. The optimal parameter will be used to foreast house price."}, {"input_sequence": "#################### Performance Metrics ##############################\n# calculates normal mse input: y - actual labels of the validation set yhat - predicted label\n# by the model --> output of predict(modelFit, newdata = validation)) output - Mean Squared\n# Error\nmse = function(y, yhat) {\nreturn(sum(y - yhat)^2/length(y))\n}\n# calculates normal rmse input: y - actual labels of the validation set yhat - predicted label\n# by the model --> output of predict(modelFit, newdata = validation)) output - Root Mean\n# Squared Error\nrmse = function(y, yhat) {\nreturn(sqrt(mse(y, yhat)))\n# calculates rmse of the log data (Kaggle evaluation measure) input: y - actual labels of the\n# validation set yhat - predicted label by the model --> output of predict(modelFit, newdata =\n# validation)) output - Root Mean Squared Error of log(y) and log(yhat) --> comparable to\n# Kaggle Leaderboard Scores!\nrmse_log = function(y, yhat) {\nreturn(rmse(log(y), log(yhat)))\n", "output_sequence": "Tune parameters of the xgboost model using cross validation. The optimal parameter will be used to foreast house price."}, {"input_sequence": "# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Wed Apr 3 08:34:12 2019\n@author: Anna Pacher, Kathrin Spendier\n# importing the necessery modules\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nimport csv\nfrom os import path\nimport numpy as np\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\n# Please change the working directory to your path!\nroot_path = os.getcwd()\n#import english stopwords (these are words that will be excluded from the wordcloud,\n#for instance articles and prepositions)\nstopwords = stopwords.words('english')\n# file object is created\nfile_ob = open(path.join(root_path, 'cryptocurrencies_wordcloud.txt'), encoding='utf-8')\n\n# reader object is created\nreader_ob = csv.reader(file_ob)\n# contents of reader object is stored .\n# data is stored in list of list format.\nreader_contents = list(reader_ob)\n# empty string is declare\ntext = \"\"\n# iterating through list of rows\nfor row in reader_contents :\n\n# iterating through words in the row\nfor word in row :\n# concatenate the words\ntext = text + \" + word\n\n\n# Mask\nstar = np.array(Image.open(path.join(root_path, \"star.jpeg\")))\n#########################################\n# remove stopwords from WordCloud, show 200 words in the wordcloud .\nwordcloud = WordCloud(width=480, height=480, max_words=200,\nstopwords=stopwords,\nmask= star,\nmode='RGBA',background_color=None ).generate(text)\n# plot the WordCloud image\nplt.figure()\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.margins(x=0, y=0)\nplt.show()\n# store to file\nwordcloud.to_file(\"bitcoin_wordcloud.png\")\n", "output_sequence": "Creates a sentiment analysis of posts related to crypto currencies. The data for the analysis is taken from a crypto currency forum and is from the years 2013 to 2019. For the analysis the vader package is used."}, {"input_sequence": "\ufeff# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Thu Apr 25 12:40:11 2019\n@author: Anna Pacher, Kathrin Spendier\nIn the following skript a sentiment analysis is performed.\nThe text for analysis is taken from: https://cryptocurrencytalk.com,\nwhich is a forum for crypto currencies.\n#!pip install vaderSentiment\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\nanalyser = SentimentIntensityAnalyzer()\ndef sentiment_analyzer_scores(sentence):\nscore = analyser.polarity_scores(sentence)\nprint(\"{:-<40} {}\".format(sentence, str(score)))\n\n\n#2013\nsentiment_analyzer_scores(\"BYTECOIN DYING? One of the large pools at http://pool.bytecoin.in doesn't seem to be working anymore. I wonder if this is the end of Bytecoin. I have a couple hundred BTEs so I hope it does go somewhere. I also read that the exchange went offline or is under repair... I have no idea. The failure of the coin to make it to an exchange considering the fact that it's older then CNC and BTB which both made it to exchanges, makes me wonder.\")\n#2014\nsentiment_analyzer_scores(\"Hi Community. Today is a day that I realized that banks are for themselves, and not for the people. 3 years ago there was a major earthquake in my city where many people died. My family was lucky enough to have survived this disaster with, what we considered, only a small matter for a severally damaged house After all this time we have struggled without insurance to get a pay out to fix our, un-weatherproof, unhealthy house and we finally succeeded! BUT - only to have our bank refuse to release the money to use because they don't believe it is sufficient to repair the damage and want to protect their interest in our property This is why Bitcoin and crypto-currency are the way of the future. It takes the power out of the hands of the big corporation banks and back into the hands of the people!!! If anyone would like to show their support to a fellow Crypto-miner, please click the link and like my post. Social media has already started working, but the more interest shown, the more the bank will work to resolve our problem. Don't forget to mention Bitcoins in any comments to remind banks that their days are numbered Smiley All we want is a warm dry house this winter the kids deserve it and I don't think we can do another one without walking away from out home. You can start saying fuck the government by saying bye-bye to your friends & stop using facebook.\")\n#2015\nsentiment_analyzer_scores(\"Trying to find a place to spend your treasure trove of Bitcoin can be difficult. Many sites claim to accept Bitcoin but in actuality only accept it for certain items. To help you I have assembled a list of websites that provide lists of merchants who accept Bitcoin. I hope that you find this helpful. This is just a short list of websites that I found. As for individual merchants who accept Bitcoin I know that thecryptostore.com accepts Bitcoin for everything and does so through Bitpay. Definitely checkout these sites, they are very helpful. I believe Newegg.com but I'm not sure? There is a bunch of hosting places which accept Bitcoin. People are waking up and realizing that accepting Bitcoin is a smart move. I think a lot of business think that they will be shorted if they accept Bitcoin and the price drops. What they don't realize is that some payment providers let them instantly exchange their BTC for fiat. I am so glad that today I have a list of the options that I can use when trying to make the bitcoin purchases. Sometimes being confined to one source is never good and hence one need to explore and see how the rest of the platforms are doing. I will for sure find time and visit the above market place and see which one will best work out for me especially in this year when everything is expected to grow further.\")\n#2016\nsentiment_analyzer_scores(\"Hey Everyone !!! I have about $10,000 that I would like to invest in 2017 on Crypto Currencies. I have looked and researched about different types of Crypto currencies such as bitcoin, LiteCoin etc for a while but I do not know what to invest in. Can someone give me some advice on a smart way to invest $ 10,000 on crypto currencies with the best short term and long term options? How safe/risky is it? What kind of a return could I get? Any good strategies on how to go about doing this? Thanks !!!! That's tough, there are risks with any coin you invest in. Honestly, if I had 10K to invest, I'd throw it into one of the gambling sites with an invest feature. The house always makes a nice return. If you're not looking to keep the coins long-term and you're not a day trader, I'd go down the gambling invest route. As with anything financial / gaming there are risks. Know your risks and only invest the amount you can lose. Investing in cryptocurrencies is an extremely high risk venture. That is evident in the extreme volatility of most of the coin values relative to the US Dollar. Also many new coins simply disappear from the market as interest fluctuates amongst miners who get on the bandwagon of a new issue, hoping to see their minted coins rise in value, but then abandon it for the next one with apparently more promise. Investing in cryptocurrencies is high risk does offer the potential of high returns, and a good strategy would be to portion investment in the top five coins in ratio to volumes traded, thereby helping to ensure liquidity. But investing in small amounts of a few hundred dollars a week so as to spread volatility of prices over a long period. Pretty much in the way you would invest in a share portfolio. In the end, from an investment point of view, you need to be able to get your money converted back into fiat currencies to spend, so the important value to watch the cryptocurrency to fiat US Dollar value, or the Euro value. My own preference is to mine an interesting coin, whilst investing about \u20ac50 a week in the same coin on the exchanges. Thus helping to sustain the coin in the market.\")\n#2017\nsentiment_analyzer_scores(\"Bitcoin - is the future freedom from banks? Is it possible? How do you think? In my opinion bitcoin and cryptocurrency in general will co - exist with todays banks. Why? Simply because it's not that easy to do the switch bearing in mind that banks are multi-billionare structures and they won't allowed something else to replace it easily. Plus you will always have people like my friend : I can't trust the trustees system, when I lose my keys I lose everything, while with banks I am safe and I know that my money can't be lost... Even my country guarantee some kind of money to me if bank fail , for some people it is just easier to let others, banks, to do all the things for them as hey don't want to think about it. First step would be to replace PayPal and other e-wallets so that cryptocurrency become used on a major scale for online transfers. As for me, it s hard for now to say for sure if bitcoin become a freedom for banks. It have some advantages and disadvantages, but in my opinion, bitcoin will become a real future for over world.\")\n#2018\nsentiment_analyzer_scores(\"The previous exchanges report published last month provided a meaningful insight on the boom of crypto exchanges. New exchanges were launched, and the peak in demand for cryptocurrencies during December even lead to users being blocked from signing up. Cryptocurrency exchanges like the rest of the crypto ecosystem have seen their fair share of ups and downs over the past months, and this report aims to help illustrate the bigger picture. The falling average price of Bitcoin and overall bearish response in the market has a negative impact on the exchanges. Over the course of the big dips in the market, the total trading volume has decreased. Users seem to be spending less time on exchanges per session, and this may be because people invest in a smaller variety of coins or because they are becoming more sophisticated and only investing in particular coins. This data coupled with the fact that more users see fewer coins on each exchange can further add to the observation above. As seen in the table below some of the largest exchanges including Coinbase, Gdax and Bittrex have seen a decrease in traffic growth. Users from the US still account for the majority of traffic on most exchanges. The information presented below is based on both public and private information derived from proprietary tools and used to estimate the analytics of each website. There was a minimal lack of real active data from countries including China, Russia, Turkey, Korea, and Japan (yahoo). Users that connect to exchanges through a VPN connection could have an effect on the accuracy of this data. China\u2019s attempts to entirely ban any type of cryptocurrency trading on domestic and foreign trading sites have had an impact on web traffic from Chinese users. Since the ban of all exchanges between fiat currency and cryptocurrencies in China during September, authorities have not been satisfied with the results of these measures and have since taken further actions to ban all token trades and ICO related activities.\")\n#2019\nsentiment_analyzer_scores(\"How do you think cryptocurrency craze will be viewed a 100 years from now? Do you think it will be looked at as an investment bubble or a digital revolution? It depends on us. Crypto market survived some sort of katharsis in 2018, whne the Bitcoin bubble burst. Now is the time to make crypto more known for the rest of the world if we want this idea to survive. Hopefully, there are more and more crypto which are gaining public attention (like, for example, FuturoCoin and its partnership with Formule One team, first in history of cryptocurrencies: https://newsroom.futurocoin.com/lets-start-the-season/) so crypto should soon be familiar for people who don\u2019t hear about it before. I've heard before about that F1 sponsorship deal, cause I was writing and article about cryptoworld - sportsworld sponsorship deals. I've heard that they are planning a fork soon, is this true? I couldn't find any official info anywhere. Anyway when it comes to such predicitons it's really difficult to guess, but i want to play that game. I say that 100 years forward, cryptocurrencies are going to be looked as an old, but revolutionary mean of payment, that started a different chapter in economy of our world. I imagine that people (if they are going to be alive as a civilization) will also see cryptocurrencies as a scam - by that I mean something like very unstable, very risky. Well in the current situation I can say that blockchain will survive for sure, it is a really useful technology and probably one of the best right now. But cryptocurrencies seem to lose their position, there are need changes, now crypto is not that trustworthy\")\n# plot the scores over time\nimport matplotlib as mlp\nyear = [i for i in range(2013,2020)]\nnegativ =[0.084, 0.101, 0.035, 0.074]\nneutral = [0.872, 0.727, 0.866, 0.803]\npositiv = [0.045, 0.172, 0.23, 0.188, 0.063, 0.123]\ncompound = [-0.3091, 0.9624, 0.7707, -0.5451, 0.9121]\n# label for x-axis\nplt.xlabel('Years')\n# Label for y-axis\nplt.ylabel('Scores')\n# set ranges for axes manually\n# Syntax: plt.axis([xmin, xmax, ymin, ymax])\nplt.axis([2013, 2019, -1, 1.2])\n# plot the different lines:\nplt.plot(year, negativ, linestyle = '-', color='red', label = \"negativ\")\n#show legend\nplt.legend(loc = \"center left\", frameon=True, bbox_to_anchor=(1, 0.5))\n# show grid\nplt.grid(True)\n# add title\nplt.title(\"Sentiment Analysis\", fontsize = 18)\n# save diagram\nplt.savefig(\"sentimentanalysis_linegraph.png\", dpi = 100, bbox_inches='tight')\n# Diagramm anzeigen:\nplt.show()\n", "output_sequence": "Creates a sentiment analysis of posts related to crypto currencies. The data for the analysis is taken from a crypto currency forum and is from the years 2013 to 2019. For the analysis the vader package is used."}, {"input_sequence": "# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Thu Apr 25 14:19:15 2019\n@author: Anna Pacher, Kathrin Spendier\nIn the following script the two packages textblob and vader are analyzed.\nBoth packages are used for sentiment analysis.\nfrom textblob import TextBlob\n# you may need to install textblob with\n# !pip install textblob\nanalysis = TextBlob(\"Bitcoins are super cool\")\nanalysis.sentiment\n#Out[1]: Sentiment(polarity=0.3416666666666667, subjectivity=0.6583333333333333)\nanalysis1 = TextBlob(\"Bitcoins are super cool!!!\")\nanalysis1.sentiment\n#Out[2]: Sentiment(polarity=0.5084635416666666, subjectivity=0.6583333333333333)\n# ! improves the polarity rate\nanalysis2 = TextBlob(\"Bitcoins are super cool :) !!!\")\nanalysis2.sentiment\n#Out[3]: Sentiment(polarity=0.5532986111111111, subjectivity=0.7722222222222221)\n# :) improves the polarity rate\nanalysis3 = TextBlob(\"Bitcoins are SUPER COOL !!!\")\nanalysis3.sentiment\n#Out[4]: Sentiment(polarity=0.5084635416666666, subjectivity=0.6583333333333333)\n# upper case letters make no difference\n#For German texts this package does not work:\nanalysis4 = TextBlob(\"Bitcoins sind nicht super\")\nanalysis4.sentiment\n#Out[4]: Sentiment(polarity=0.3333333333333333, subjectivity=0.6666666666666666)\n#Does not interpret slang words:\nanalysis5 = TextBlob(\"Bitcoin sux\")\nanalysis5.sentiment\n#Out[5]: Sentiment(polarity=0.0, subjectivity=0.0)\nanalysis6 = TextBlob(\"Bitcoin kinda sux\")\nanalysis6.sentiment\n#Out[6]: Sentiment(polarity=0.0, subjectivity=0.0)\n#Emojis are not implemented:\nanalysis7 = TextBlob(\"Bitcoins make me \ud83d\ude0a\")\nanalysis7.sentiment\n#Out[7]: Sentiment(polarity=0.0, subjectivity=0.0)\nanalysis8 = TextBlob(\"Bitcoins make me \ud83d\ude2d\")\nanalysis8.sentiment\n#Out[8]: Sentiment(polarity=0.0, subjectivity=0.0)\n#Some acronyms are recognized:\nanalysis9 = TextBlob(\"LOL\")\nanalysis9.sentiment\n#Out[9]: Sentiment(polarity=0.8, subjectivity=0.7)\nanalysis10 = TextBlob(\"XOXO\")\nanalysis10.sentiment\n#Out[10]: Sentiment(polarity=0.0, subjectivity=0.0)\n#####################################################################\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n# you may need to install vaderSentiment with\n# !pip install vaderSentiment\nanalyser = SentimentIntensityAnalyzer()\ndef sentiment_analyzer_scores(sentence):\nscore = analyser.polarity_scores(sentence)\nprint(\"{:-<40} {}\".format(sentence, str(score)))\nsentiment_analyzer_scores(\"Bitcoins are super cool\")\n#Bitcoins are super cool----------------- {'neg': 0.0, 'neu': 0.244, 'pos': 0.756, 'compound': 0.7351}\nsentiment_analyzer_scores(\"Bitcoins are super cool!!!\")\n#Bitcoins are super cool!!!-------------- {'neg': 0.0, 'neu': 0.22, 'pos': 0.78, 'compound': 0.795}\n# ! improves the sentiment score\nsentiment_analyzer_scores(\"Bitcoins are super cool :) !!!\")\n#Bitcoins are super cool :) !!!---------- {'neg': 0.0, 'neu': 0.229, 'pos': 0.771, 'compound': 0.8772}\n# :) & ! leads to a very high compound score\nsentiment_analyzer_scores(\"Bitcoins are SUPER COOL!!!\")\n#Bitcoins are SUPER COOL!!!-------------- {'neg': 0.0, 'neu': 0.19, 'pos': 0.81, 'compound': 0.8605}\n# capital letters improve the sentiment score\n#For German texts this packages does not work:\nsentiment_analyzer_scores(\"Bitcoins sind nicht super\")\n#Bitcoins sind nicht super--------------- {'neg': 0.0, 'neu': 0.435, 'pos': 0.565, 'compound': 0.5994}\n#Successfully interprets slang words:\nsentiment_analyzer_scores(\"Bitcoin sux\")\n#Bitcoin sux----------------------------- {'neg': 0.714, 'neu': 0.286, 'pos': 0.0, 'compound': -0.3612}\nsentiment_analyzer_scores(\"Bitcoin kinda sux\")\n#Bitcoin kinda sux----------------------- {'neg': 0.525, 'neu': 0.475, 'pos': 0.0, 'compound': -0.2975}\n#Emojis are implemented (recognizes UTF-8 encoded emojis)\nsentiment_analyzer_scores(\"Bitcoins make me \ud83d\ude0a\")\n#Bitcoins make me \ud83d\ude0a---------------------- {'neg': 0.0, 'neu': 0.5, 'pos': 0.5, 'compound': 0.7184}\nsentiment_analyzer_scores(\"Bitcoins make me \ud83d\ude2d\")\n#Bitcoins make me \ud83d\ude2d---------------------- {'neg': 0.383, 'neu': 0.617, 'pos': 0.0, 'compound': -0.4767}\n#Acronyms are recognized:\nsentiment_analyzer_scores(\"LOL\")\n#LOL------------------------------------- {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.4215}\nsentiment_analyzer_scores(\"XOXO\")\n#XOXO------------------------------------ {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.6124\n", "output_sequence": "Creates a sentiment analysis of posts related to crypto currencies. The data for the analysis is taken from a crypto currency forum and is from the years 2013 to 2019. For the analysis the vader package is used."}, {"input_sequence": "# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Tue May 28 15:00:05 2019\n@author: Patrick Plum\nDescription: This program is the master program to the slides and should be run as it calls all other scripts\n############################### Import other scripts and variables ##########################################\nimport pandas as pd\nimport ReadData\nfrom ReadData import crix\nfrom ReadData import dates_of_first\nfrom ReadData import logreturn_at_rebalance\nfrom ReadData import crix_constituents\nimport WordCloudCRIX\nimport PlotNumberofConst\nimport DailyLogReturns\nfrom DailyLogReturns import mean_logreturn_at_first\nimport Inference\nfrom Inference import ttest\n######################################## Summary ############################################################\n#daily returns of first of the month have about 2.5*expected value and roughly 40% of the variance\ncomp_mean = mean_logreturn_at_first/mean_logreturn_others\nratios= pd.DataFrame([['ratio of means',comp_mean],['ratio of variances',comp_var]])\ntestdata=[['t', ttest[0],ttest[1]],['F', Ftest[0],Ftest[1]],['Shapiro-Wilk`s for 1st of month', SWtest_first[0],SWtest_first[1]],['Shapiro-Wilk`s for 1st of other days',SWtest_others[0],SWtest_others[1]],['Brown-Forsythe', BFtest[0],BFtest[1]],['Mann-Whitney-U', MWUtest[0],MWUtest[1]]]\ntests=pd.DataFrame(testdata, columns=['test','statistic','p-value'])\nprint(ratios)\n", "output_sequence": "An Analysis of CRX daily log returns - are there higher returns at lower risk on index rebalancing days."}, {"input_sequence": "# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Tue May 28 15:19:48 2019\n@author: Patrick Plum\nDescription: This script plots daily log returns of the CRIX and provides first summary statistics for the three classes of dates\n- called by: CRIXRebalancingMaster.py - pls run the latter\n############################# Import Modules and data #######################################\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom __main__ import dates_of_first\nfrom __main__ import logreturn_at_rebalance\n######### statistical inference on influence of index rebalancing ##########################\n# plotting\nfig=plt.figure(figsize=(40,12))\naxes=fig.add_axes([0.1,0.1,0.8,0.8])\nplt.grid(b='TRUE', which='major', color='#666666', linestyle='-',alpha=0.7)\naxes.set_facecolor('white')\naxes.set_aspect(aspect=800)\n# ...dates of others\ny1=logreturn_others\naxes.plot(x1,y1,marker='',color='blue',alpha=0.7)\n# ...dates of index amount rebalancing = 1st of each 3rd month\nx3 = dates_of_ind_rebalance\ny3 = logreturn_at_rebalance\naxes.plot(x3,y3,'rx',mew=9, ms=30)\n# ...dates of usual index change = 1st of a month\nx2 = dates_of_first\ny2 = logreturn_at_first\naxes.plot(x2,y2,'g+',mew=9, ms=30)\naxes.set_ylim([-0.25,0.25])\nmyFmt = mdates.DateFormatter('%Y-%m-%d')\naxes.xaxis.set_major_formatter(myFmt)\nfig.autofmt_xdate()\naxes.tick_params(axis='both', which='major', labelsize=30)\nplt.savefig('DailyLogReturns.png')\n############## Summary of mean and variances of daily log returns #########################\n#mean\nmean_logreturn_at_first=np.mean(logreturn_at_first)\n#variance\nvar_logreturn_at_first=np.var(logreturn_at_first)\n", "output_sequence": "An Analysis of CRX daily log returns - are there higher returns at lower risk on index rebalancing days."}, {"input_sequence": "# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Tue May 28 14:33:06 2019\n@author: Patrick Plum\nDescription: This script creates a wordcloud of the relative appearance of all constituent cryptocurrencies of the CRIX in the time window from 2014/08/01 until 2019/04/01\n- called by: CRIXRebalancingMaster.py - pls run the latter\n############################### Import Modules #############################################\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nimport csv\nfrom PIL import Image\nplt.style.use('ggplot')\n####### Wordcloud for relative frequency of appearance of a cryptocurrency in CRIX #######\n# make bag of words of CRIX members\nfile_ob = open('index_members.csv',encoding=\"utf-8\")\n\n# reader object is created\nreader_ob = csv.reader(file_ob)\n# contents of reader object is stored .\n# data is stored in list of list format.\nreader_contents = list(reader_ob)\n# initialize text\ntext = \"\"\n# iterating through list of rows\nfor row in reader_contents :\n# iterating through words in the row\nfor word in row :\n# concatenate the words\ntext = text + \" + word\n\n#lowercase all words to exclude redundancies\ntext = text.upper()\n#Image as mask for the wordcloud\nwave_mask = np.array(Image.open('Dollar.png'))\n#generating the wordcloud\nwordcloud = WordCloud(width=800, height=1600, background_color=\"white\",colormap=\"Accent\"\n, collocations = False, contour_width=3, contour_color='firebrick',mask=wave_mask,\nrelative_scaling=0.2, max_words=2000).generate(text)\n\n# plot WordCloud image\nplt.figure(figsize=(40,40),facecolor = 'white', edgecolor='blue')\nplt.imshow(wordcloud,interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.margins(x=0, y=0)\nplt.show()\nwordcloud.to_file(\"Wordcloud_CRIX_Frequency.png\")\n", "output_sequence": "An Analysis of CRX daily log returns - are there higher returns at lower risk on index rebalancing days."}, {"input_sequence": "# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Tue May 28 15:13:44 2019\n@author: Patrick Plum\nDescription: This script plots the number of CRIX constituents over time\n- called by: CRIXRebalancingMaster.py - pls run the latter\n############################### Import Modules #############################################\nimport matplotlib.pyplot as plt\nfrom __main__ import dates_of_ind_rebalance\nfrom __main__ import crix_constituents\n##################### Amount of currencies at a certain time preiod ########################\n#amount of currencies in each period\nn = []\nfor i in dates_of_first:\nn.append(len(crix_constituents.loc[i]))\n#plotting number of constituents over time\nfig=plt.figure(figsize=(18,12))\naxes=fig.add_axes([0.1,0.1,0.8,0.8])\naxes.tick_params(axis='both', which='major', labelsize=20)\naxes.set_aspect(aspect=9)\nx=dates_of_first\ny=n\naxes.plot(x,y,'go',mew=3,ms=15)\nplt.grid(b='TRUE', which='both', axis='x', color='#666666', linestyle='-',alpha=0.7)\naxes.set_facecolor('white')\nplt.xticks(dates_of_ind_rebalance[:-1])\nmyFmt = mdates.DateFormatter('%Y-%m-%d')\naxes.xaxis.set_major_formatter(myFmt)\nfig.autofmt_xdate()\nplt.savefig('Nr_of_Cryptos.png')\n############################################################################################\n", "output_sequence": "An Analysis of CRX daily log returns - are there higher returns at lower risk on index rebalancing days."}, {"input_sequence": "# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Tue May 28 15:03:37 2019\n@author: Patrick Plum\nDescription: This script just reads data from CRIX and processes this data for the use in the other scripts\n- called by: CRIXRebalancingMaster.py - pls run the latter\n############################### Import Modules ###########################################\nimport pandas as pd\nimport numpy as np\nimport json\n############################### Read in data #############################################\n# read crix_constituents file\ncrix_constituents = pd.read_csv('index_members.csv', parse_dates=True,index_col=[1])\n# all constituents\ncrix_constituents_all= np.unique(crix_constituents.values.flatten())\nwith open('crix1.json', 'r') as myfile:\ncrix=myfile.read()\n\n# read all CRIX data\ncrix = json.loads(crix)\ncrix = pd.DataFrame(crix)\n############################### Pre-process for further use ###############################\n# dates of index shift (=first of each month)\ndates_of_first = np.unique(crix_constituents.index)\n#converting to a more convenient data type\ndates_of_first = pd.to_datetime(dates_of_first)\n# convert dates to dateformat\ncrix['date'] = pd.to_datetime(crix['date'])\n# calculation of logreturns and adding to the dataframe\ncrix['logreturn'] = np.log(crix['price']).diff()\n# rebalancing of amount of constituents every 3 months\ndates_of_ind_rebalance = []\ni=0\nwhile i<len(dates_of_first):\ndates_of_ind_rebalance.append(dates_of_first[i])\ni=i+3\n# last month out of rule (see Nr_of_Cryptos.png)\ndates_of_ind_rebalance.append(dates_of_first[i-1])\n# get indices of monthly turns\nind=[]\n#...and of the rebalancing months\nind2=[]\n# go for the logreturns of these\nfor i in range(len(dates_of_first)):\nind.append(crix.loc[crix['date'] == dates_of_first[i]].index[0])\nfor i in range(len(dates_of_ind_rebalance)):\nind2.append(crix.loc[crix['date'] == dates_of_ind_rebalance[i]].index[0])\nlogreturn_at_first = crix.logreturn.loc[ind]\n# get other indices\nind_others = list(range(0,len(crix)))\nind_others = list(set(ind_others)-set(ind))\nind_others.remove(0)\n#... and get the logreturns of all other dates\nlogreturn_others=crix.logreturn[ind_others]\ndates_of_others = crix.date[ind_others]\n", "output_sequence": "An Analysis of CRX daily log returns - are there higher returns at lower risk on index rebalancing days."}, {"input_sequence": "# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Tue May 28 15:32:05 2019\n@author: Patrick Plum\nDescription: This script compares 1st-day-of-a-month daily log returns with those of 1st-day-of-every-3rd-month (rebalancing date of number of constituents of the CRIX) and in particular with those of all other days\nIt performs parametric tests for testing difference in variance and mean as well as non-parametric tests.\nMeanwhile, it also tests for normality\n- called by: CRIXRebalancingMaster.py - pls run the latter\n############################# Import Modules and data #######################################\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy\nimport statsmodels.graphics.gofplots as sp\nfrom __main__ import logreturn_at_first\n#########################for normal data, paramteric tests ###################################\nvar_logreturn_at_first=np.var(logreturn_at_first)\n# F-Test\nF=var_logreturn_at_first/var_logreturn_others #teststatistic, F-distributed (=comp_var)\ndf1 = len(logreturn_at_first) - 1 #degrees of freedom1\nFtest_pvalue=scipy.stats.f.cdf(F,df1,df2)\nFtest=(F,Ftest_pvalue,df1,df2)\n# reject, if small -> clearly reject\n# t-test for checking, if means are significantly different - assumption: normality, variance equal or not according to previus result\nif Ftest_pvalue>0.05:\nttest=ws.ttest_ind(logreturn_at_first,logreturn_others,alternative='larger',usevar='equal')\nelse:\nttest=ws.ttest_ind(logreturn_at_first,logreturn_others,alternative='larger',usevar='unequal')\n#################################### Testing for normality #################################\n# QQPLots\n\nfig = plt.figure()\nax1 = fig.add_subplot(1, 1, 1)\nfig1=sp.qqplot(logreturn_others, line='45',fit='TRUE',color='blue',alpha=0.5,marker='o',ax=ax1)\nax2 = fig.add_subplot(1, 1, 1)\nfig2=sp.qqplot(logreturn_at_first, line='45',fit='TRUE',color='green',marker='+',ms=10,ax=ax2)\nplt.axis('scaled')\nplt.xticks(np.arange(-6, 7, 2))\nplt.ylim(-7,7)\nplt.grid(b=True, which='major', color='#666666', linestyle='-')\nax=plt.gca()\nax.set_facecolor('white')\nplt.gcf()\nfig.tight_layout()\nplt.savefig('QQPlots.png')\n# Shapiro Wilk's test\nSWtest_first=scipy.stats.shapiro(logreturn_at_first)\n# If p-value (= 2nd entry is small, reject normality)\n##################################for non-normal: non-paramteric test ######################\n#only account for values within 2 standard deviations ->95% center ->cut 2.5% on each side\nBFtest=tuple(scipy.stats.levene(logreturn_others,logreturn_at_first,center='trimmed',proportiontocut=0.025))\n# Mann-Whitney-U Test for hypothesis test if distributions are similar\nMWUtest=tuple(scipy.stats.mannwhitneyu(logreturn_at_first,logreturn_others,alternative='greater'))\n############################################################################################\n", "output_sequence": "An Analysis of CRX daily log returns - are there higher returns at lower risk on index rebalancing days."}, {"input_sequence": "# please use \"Esc\" key to jump out the run of Shiny app\n# clear history and close windows\n# rm(list=ls(all=TRUE))\ngraphics.off()\n\n# General settings\nlibraries = c(\"shiny\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# please set working directory setwd('C:/...')\n# windows setwd('/Users/...') #\n# mac os setwd('~/...') # linux\n##############################################################################\n# Default parameters\nmmstat.getValues = function(local, ...) {\nret <<- list(...)\nfor (name in names(ret)) {\nif (is.null(ret[[name]]) || (length(ret[[name]]) == 0)) {\nif (is.null(local[[name]])) {\nstopif(is.null(mmstat$UI[[name]]$value) &&\npaste0(\"mmstat.getValues: no default value(s) for \\\"\", name, \"\\\"\"))\nif (is.null(mmstat$UI[[name]]$value)) {\nret[[name]] = mmstat$UI[[name]]$selected\n} else {\nret[[name]] = mmstat$UI[[name]]$value\n}\n} else {\nret[[name]] = local[[name]]\n}\nif (mmstat$UI[[name]]$call == \"mmstat.sliderInput\") {\nif ((compareVersion(mmstat$shiny, \"0.11\") >= 0) &&\n!is.null(mmstat$UI[[name]]$ticks)) {\nret[[name]] = ret[[name]] + 1\n}\nret\n}\n# Datasets and variables\nmmstat.getDataNames = function(...) {\nis.binary = function(v) {\nx = unique(v)\nlength(x) - sum(is.na(x)) == 2L\n}\n\nfiles = list(...)\nif (length(files) == 0)\nreturn(names(mmstat$dataset)[1])\ndataset = vector(\"list\", length(files))\nnames(dataset) = files\nmmstat$dataset <<- dataset\nfor (i in seq(files)) {\nfile = files[[i]]\nif (is.null(mmstat$dataset$file)) {\ndata = readRDS(paste0(file, \".rds\"))\nif (class(data) == \"data.frame\") {\nallvars = names(data)\nmmstat$dataset[[file]] <<- list(data = data,\nallvars = allvars,\nnumvars = allvars[sapply(data, is.numeric)],\nif (class(data) == \"ts\") {\nallvars = colnames(data)\ngettext(names(mmstat$dataset), \"name\")\nmmstat.getVarNames = function(dataname, vartype, which = NULL) {\nvars = NULL\nif (vartype == \"numeric\")\nvars = \"numvars\"\nif (vartype == \"ordered\")\nvars = \"ordvars\"\nif (vartype == \"factor\")\nvars = \"facvars\"\nif (vartype == \"binary\")\nvars = \"binvars\"\nstopif(is.null(vars),\n\"mmstat.getVarNames: Variable type \\\"%s\\\" unknown\")\nif (is.null(which))\nreturn(gettext(mmstat$dataset[[dataname]][[vars]], \"name\"))\nif (length(which == 1))\nreturn(gettext(mmstat$dataset[[dataname]][[vars]][which]))\nmmstat.getDatasets = function(...) {\nfiles = list(...)\ndataset = vector(\"list\", length(files))\nnames(dataset) = files\nmmstat$dataset <<- dataset\nfor (i in seq(files)) {\nfile = files[[i]]\ndata = readRDS(paste0(file, \".rds\"))\nif (class(data) == \"data.frame\") {\nallvars = names(data)\nmmstat$dataset[[file]] <<- list(data = data,\nallvars = allvars,\nnumvars = allvars[sapply(data, is.numeric)],\nif (class(data) == \"ts\") {\nallvars = colnames(data)\nnumvars = allvars)\nnames(dataset)[1]\nmmstat.attrVar = function(var, type, index = NULL) {\nret = var\nif (is.null(index))\nindex = 1:length(var$values)\nelse {\nret$values = var$values[index]\nret$n = length(index)\nif (type == \"numvars\") {\nif (is.null(var$jitter))\nret$jitter = runif(length(index)) else\nret$jitter = var$jitter[index]\nret$mean = mean(var$values[index], na.rm = T)\nret$quart = quantile(var$values[index], c(0.25, 0.75), na.rm = T)\nret$max = max(var$values[index], na.rm = T)\nif ((type == \"binvars\") || (type == \"ordvars\") || (type == \"facvars\")) {\nret$tab = table(var$values[index], useNA = \"ifany\")\nret$prop = prop.table(ret$tab)\nmmstat.getDatasetNames = function() {\ngettext(mmstat$dataset[[name]]$allvars, \"name\")\nmmstat.getVar = function(dataname = NULL, varname = NULL, vartype = NULL, na.action = na.omit) {\nmatch.name = function(needle, haystack) {\nif (is.null(needle))\nreturn(NA)\nreturn(match(needle, haystack))\n# which variable type do we need\nif (is.null(vartype))\nvartype = mmstat$vartype\ndeftype = c(\"numvars\", \"binvars\", \"numeric\",\n\"binary\", \"ordered\", \"factor\")\ntype = pmatch(vartype, deftype)\nstopif(is.na(type), paste0(\"mmstat.getVar: Unknown variable type: \", vartype))\ntype = deftype[type]\nif (type == \"numeric\")\ntype = \"ordvars\"\nif (type == \"factor\")\ntype = \"binvars\"\n# get dataset\npos = match.name(dataname, names(mmstat$dataset))\nif (is.na(pos))\npos = 1 # assume that all datasets contain\ndataname = names(mmstat$dataset)[pos]\nif (is.null(mmstat$dataset[[dataname]]))\nstop(sprintf(gettext(\"Data file '%s' is missing\"), dataname))\n# get variable\npos = match.name(varname, mmstat$dataset[[dataname]][[type]])\npos = 1\n# assume that every datasets contains a variable of correct type\nvarname = mmstat$dataset[[dataname]][[type]][pos]\n# get var values\ndataset = mmstat$dataset[[dataname]]$data\nif (class(dataset) == \"data.frame\")\nvalues = na.action(dataset[[varname]])\nif (class(dataset) == \"ts\")\nvalues = na.action(dataset[, varname])\nvar = list(values = values, n = length(values), name = varname,\nsub = paste(gettext(\"Dataset:\"), gettext(dataname)),\nxlab = gettext(varname),\nvar = mmstat.attrVar(var, type)\nvar\nmmstat.dec = function(x, ord = NULL) {\norder = order(x)\ndf = diff(x[order])\n# take only positive differences\ndf = min(df[df > 0])\nif (!is.null(ord))\norder = order(order[ord])\nlist(decimal = max(0, ceiling(-log10(df))), order = order)\nmmstat.axis = function(side, range, at, labels, ...) {\nat = pretty(range)\ndec = mmstat.dec(at)\naxis(side, at = at, labels = sprintf(\"%.*f\", dec$decimal, at), ...)\nmmstat.baraxis = function(side, range, at, labels, ...) {\npos = 1 + pretty(range)\naxis(side, at = at[pos], labels = labels[pos], ...)\nmmstat.merge = function(range1, range2) {\nreturn(c(min(range1, range2), max(range1, range2)))\nmmstat.range = function(...) {\nranges = list(...)\nisn = sapply(ranges, is.numeric)\nif (all(isn)) {\nmins = sapply(ranges, min)\n} else {\nmins = maxs = NA\nreturn(c(min(mins), max(maxs)))\nmmstat.round.up = function(x, digits = 0) {\nxr = round(x, digits)\ntf = (xr < x)\nxr[tf] = xr[tf] + 10^(-digits)\nxr\nmmstat.round.down = function(x, digits = 0) {\ntf = (xr > x)\nxr[tf] = xr[tf] - 10^(-digits)\nmmstat.pos = function(minmax, pos) {\nmin(minmax) + diff(minmax) * pos\nmmstat.ticks = function(nin, nmin = 3, tin = 11) {\nnmax = nin\nnt = tin - 1\nrepeat {\nn = nmin * exp((0:nt)/nt * log(nmax/nmin))\npow = 10^trunc(log10(n))\nssd[pow == 1] = 0\nssd[(ssd > 2) & (ssd < 8)] = 5\nfsd[ssd > 7] = fsd[ssd > 7] + 1\nnret = fsd * pow + ssd * pow/10\nif (nret[nt + 1] > nmax)\nnret[nt + 1] = nret[nt + 1] - pow[nt + 1]/2\nif (length(unique(nret)) == nt + 1)\nreturn(nret)\nnt = nt - 1\nmmstat.math = function(txt) {\ndollar = strsplit(txt, \"&\", fixed = T)[[1]]\nif (length(dollar) < 2)\nreturn(txt)\nres = paste0(\"expression(paste(\\\"\", dollar[1], \"\\\"\")\nfor (i in 2:length(dollar)) {\npercent = strsplit(dollar[i], \";\", fixed = T)[[1]]\nlp = length(percent)\nif (lp == 1)\nres = paste0(res, \",\\\"\", percent[1], \"\\\"\")\nelse {\nif (lp > 2)\npercent[2] = paste(percent[2:lp], sep = \";\")\nres = paste0(res, \",\", percent[1], \",\\\"\", percent[2], \"\\\"\")\nres = paste0(res, \"))\")\neval(parse(text = res))\nis.ASCII = function(txt) {\nall(charToRaw(txt) <= as.raw(127))\nstopif = function(cond, txt) {\nif (cond) stop(txt)\nmmstat.html = function(file, ...) {\nstopif(!file.exists(file), sprintf(\"File '%s' does not exist\", file))\nhtml = paste0(readLines(file), collapse = \"\")\nstopif(!is.ASCII(html), sprintf(\"File '%s' contains non-ASCII symbols\", file))\nargs = list(...)\ncond = sapply(args, length)\nstopif(!all(cond), paste(\"mmstat.html - Zero length arguments:\",\npaste(names(args)[!cond],\ncollapse = \", \")))\nif (length(args)) {\nstopif(any(sapply(args, is.null)),\n\"One or more arguments contain a NULL\")\nargs$fmt = html\nhtml = do.call(\"sprintf\", args)\nreturn(html)\nmmstat.plotTestRegions = function(crit, xlim, cex, close = F, col = \"black\",\nlabel = NULL, pos = 1) {\nlines(xlim, c(ylim[1], col = col)\nif (close) {\nlines(c(xlim[1], xlim[1]), ylim, col = col)\ncu = max(crit[1], xlim[1])\nif (crit[1] >= xlim[1]) {\nlines(c(cu, cu), ylim, col = col)\ntext((cu + xlim[1])/2, mean(ylim), mmstat.math(\"\\\"&H[1];\\\"\"),\ncex = cex, col = col)\nco = min(crit[2], xlim[2])\nif (crit[2] <= xlim[2]) {\nlines(c(co, co), ylim, col = col)\ntext((co + xlim[2])/2, mean(ylim), mmstat.math(\"\\\"&H[1];\\\"\"),\ntext((co + cu)/2, mean(ylim), mmstat.math(\"\\\"&H[0];\\\"\"), cex = cex, col = col)\nif (!is.null(text)) {\nif (pos == 2)\ntext(xlim[1], mmstat.pos(ylim, -0.25), label, col = col, cex = cex, pos = 4)\nif (pos == 4)\ntext(xlim[2], mmstat.pos(ylim, -0.25), label, col = col, cex = cex, pos = 2)\nmmstat.htest = function(...) {\naddnames = function(txt1, txt2) {\nif (length(txt1)) {\ncont = txt2 %in% txt1\nret = c(txt1, txt2[!cont])\n} else {\nret = txt2\nret\nhtest = list(method = list(attr = NULL, names = \"\", fmt = \"%s\", lines = 0),\nalternative = list(attr = NULL,\nnames = \"Alternative:\",\nnull.value = list(attr = \"names\",\nnames = vector(\"character\", 0),\ndata.name = list(attr = NULL, names = \"Data:\",\nfmt = \"%.4f\",\nconf.int = list(attr = \"conf.level\",\nlines = 1),\nstatistic = list(attr = \"names\",\nfmt = \"%.0f\",\np.value = list(attr = NULL,\nnames = \"p-value:\",\ntests = list(...)\nnhtest = names(htest)\nnrow = vector(\"numeric\", length(htest))\nlines = 0\nfor (j in seq(nhtest)) {\nname = nhtest[j]\nattr = htest[[nhtest[j]]]$attr\nif (!is.null(attr)) {\n# find all names\nfor (i in seq(tests)) {\nhtest[[name]]$names = addnames(htest[[name]]$names,\nattr(tests[[i]][[name]], attr))\n# grab all values\nnrow[j] = length(htest[[name]]$names)\nhtest[[name]]$tab = matrix(\"\", nrow = nrow[j], ncol = length(tests))\nfor (i in seq(tests)) {\ntelem = tests[[i]][[name]]\nif (!is.null(telem)) {\nhtest[[name]]$tab[1, i] = sprintf(htest[[name]]$fmt, telem)\n} else if (attr == \"conf.level\") {\nhtest[[name]]$tab[match(as.character(attr(telem, attr)),\nhtest[[name]]$names), i] = paste0(\"[\", round(telem[1], 4), \",\nround(telem[2], 4), \"]\")\n} else {\nhtest[[name]]$names), i] = sprintf(htest[[name]]$fmt, telem)\n}\nif (attr == \"conf.level\") {\nhtest[[name]]$names = sprintf(\"%.1f%% CI\", 100 *\nas.numeric(htest[[name]]$names))\nlines = lines + htest[[name]]$lines\ntab = matrix(\"\", nrow = sum(nrow) + lines, ncol = 1 + length(tests))\npos = 1\nname = nhtest[j]\nlen = length(htest[[name]]$names)\ntab[pos:(pos + len - 1), 1] = htest[[name]]$names\ntab[pos:(pos + len - 1),\n2:(1 + length(tests))] = htest[[name]]$tab\npos = pos + len + htest[[name]]$lines\nmaxlen = apply(nchar(tab), 2, max)\nfor (j in seq(tab)) {\nif (j <= nrow(tab))\ntab[j] = sprintf(\"%-*s\", maxlen[1 + ((j - 1) %/% nrow(tab))], tab[j])\npaste(apply(tab, 1, paste, collapse = \" \"), collapse = \"n\")\nmmstat.sliderInput = function(...) {\niapply = function(l) {\nret = l\nif (is.list(l)) {\nif (!is.null(ret$name) && (ret$name == \"input\")) {\nret$attribs[[\"data-values\"]] = ticks\n} else {\nfor (i in seq(ret)) ret[[i]] = iapply(ret[[i]])\nif ((compareVersion(mmstat$shiny, \"0.11\") >= 0) &&\n!is.null(args$ticks) && length(args$ticks)) {\nticks = paste0(args$ticks, collapse = \",\")\nargs$ticks = T\nhtml = do.call(\"sliderInput\", args)\nhtml = iapply(html)\nhtml\nmmstat.ui.elem = function(inputId, type, ...) {\nfound = F\nelem = list(...)\nelem$inputId = inputId\nelem$type = type\nshinytypes = c(\"actionButton\",\n\"checkboxInput\",\nmmstattypes = c(\"sampleSize\",\n\"drawSample\",\n\"confidenceLevel\",\npos = pmatch(type, shinytypes)\nif (!is.na(pos)) {\nfound = T\nif (elem$type == \"actionButton\") {\nif (is.null(elem$value))\nelem$value = 0\nelem$call = shinytypes[pos]\npos = pmatch(type, mmstattypes)\nif (elem$type == \"sampleSize\") {\nif (is.null(elem$label))\nelem$label = gettext(\"Sample size (n)\")\nelem$call = \"mmstat.sliderInput\"\nelem$step = 1\nelem$value = as.numeric(compareVersion(mmstat$shiny, \"0.11\") < 0)\nif (elem$type == \"drawSample\") {\nelem$call = \"actionButton\"\nelem$label = gettext(\"Draw sample\")\nif (elem$type == \"testHypotheses\") {\nelem$call = \"radioButtons\"\nelem$label = gettext(\"Choose test type\")\nelem$choices = gettext(c(\"two.sided\", \"less\", \"greater\"), \"name\")\nif (is.null(elem$value))\nelem$value = \"two.sided\"\nif (elem$type == \"significance\") {\nelem$call = \"mmstat.sliderInput\"\nif (is.null(elem$ticks))\nelem$ticks = c(0.1, 0.25, 1, 2, 5, 10, 20)\nelem$label = HTML(gettext(\"Select significance\nlevel (&alpha;)\"))\nelem$step = 1\nelem$max = length(elem$ticks)\nelem$value = 6 - as.numeric(compareVersion(mmstat$shiny, \"0.11\") >= 0)\nif (elem$type == \"confidenceLevel\") {\nelem$ticks = c(80, 85, 90, 99.5, 99.9)\nif (is.null(elem$label))\nelem$label = HTML(gettext(\"Select confidence level (1-&alpha;)\"))\nelem$step = 1\nelem$max = length(elem$ticks)\nelem$value = 4 - as.numeric(compareVersion(mmstat$shiny, \"0.11\") >= 0)\nif (elem$type == \"dataSet\") {\nelem$call = \"selectInput\"\nelem$label = gettext(\"Select a data set\")\nif (is.null(elem$choices))\nelem$choices = mmstat.getDataNames(gsub(\".rds$\", \"\",\nlist.files(pattern = \"*.rds\")))\nelem$value = mmstat.getDataNames()\nif (elem$type == \"variable1\") {\nelem$label = gettext(\"Select a variable\")\nelem$choices = mmstat.getVarNames(1, elem$vartype)\nif (elem$type == \"variableN\") {\nelem$call = \"selectInput\"\nelem$multiple = T\nelem$label = gettext(\"Select variable(s)\")\nif (elem$type == \"fontSize\") {\nelem$label = gettext(\"Font size\")\nif (is.null(elem$min))\nelem$min = 1\nif (is.null(elem$max))\nelem$max = 1.5\nif (is.null(elem$step))\nelem$step = 0.05\nelem$value = elem$min\nif (elem$type == \"speedSlider\") {\nelem$call = \"mmstat.sliderInput\"\nelem$label = list(NULL)\nelem$min = 0\nstopif(!found, sprintf(\"mmstat.ui.elem: Type \\\"%s\\\" unknown\", type))\nif (is.null(elem$update))\nelem$update = paste0(\"update\", ucfirst(elem$call))\nmmstat$UI[[inputId]] <<- elem\nmmstat.ui.call = function(inputId, ...) {\nelem = mmstat$UI[[inputId]]\nwhat = elem$call\nargs = list(...)\nfor (name in names(elem)) {\nif (is.null(args[[name]]))\nargs[[name]] = elem[[name]]\nargs$call = NULL\nif ((what == \"selectInput\") || (what == \"checkboxGroupInput\"))\nargs$value = NULL\ndo.call(what, args)\nmmstat.ui.update = function(inputId, ...) {\nwhat = elem$update\nif ((what == \"updateSelectInput\") ||\n(what == \"updateCheckboxGroupInput\"))\nmmstat.lang = function(deflang = NULL) {\npof = list.files(pattern = \"*.po$\")\nif (is.null(deflang)) {\nlang = sapply(strsplit(pof, \".\", fixed = T),\nfunction(elem) {elem[1]})\npat = paste0(\"_\", lang, \"$\")\npath = getwd()\npath = strsplit(path, \"/\", fixed = T)[[1]]\npos = -1\nfor (i in seq(pat)) {\np = grep(pat[i], path)\nif (length(p)) {\np = max(p)\nif (p > pos) {\nlind = i\nlind = match(paste0(deflang, \".po\"), pof)\nif (is.na(lind))\nlind = (-1)\nmsgfn = ifelse(lind > 0, pof[lind], \"default.po\")\nmmstat.warn(!file.exists(msgfn), sprintf(\"File '%s' does not exist\", msgfn))\nmsg = paste(readLines(msgfn), collapse = \" \")\nmsgid = regmatches(msg, gregexpr(\"msgid\\\\s*\\\".*?\\\"\", msg))\ntmp = strsplit(msgid[[1]], \"\\\"\")\nmsgid = sapply(tmp, function(vec) {\npaste0(vec[2:length(vec)])\n})\nmsgstr = regmatches(msg, gregexpr(\"msgstr\\\\s*\\\".*?\\\"\", msg))\ntmp = strsplit(msgstr[[1]], \"\\\"\")\nmsgstr = sapply(tmp, function(vec) {\nmmstat$messages <<- list(id = msgid, str = msgstr)\nhtmlTable = function(tab, vars = NULL, lines = NULL, cex = 1) {\nhtml = sprintf(\"<table style=\\\"text-align:right;width:100%%;\nfont-size:%i%%;\\\">\",\nas.integer(100 * cex))\nif (length(vars) == 2)\nhtml = paste0(html,\nsprintf(\"<tr><td colspan=\\\"%i\\\">\",1 + ncol(tab)),\nsprintf(gettext(\"Columns: %s\"), gettext(vars[1])),\n\"</td></tr>\")\nwidth = as.integer(100/(1 + ncol(tab)))\nhtml = paste0(html, sprintf(\"<TR><TD style=\\\"width:%i%%\\\"></TD>\",\nwidth))\nhtml = paste0(html, paste0(sprintf(\"<TD style=\\\"width:%i%%;\nbackground-color:#AAAAAA\\\"><b>\",\nwidth),\nsprintf(\"%s\", colnames(tab)),\nhtml = paste0(html, \"</TR>\")\nalign = ifelse(is.na(suppressWarnings(as.numeric(rownames(tab)))),\n\"left\", \"right\")\nfor (i in seq(nrow(tab))) {\nhtml = paste0(html, sprintf(\"<TR><TD style=\\\"text-align:%s;\nbackground-color:\n#AAAAAA\\\"><b>%s</b></TD>\",\nalign[i], rownames(tab)[i]))\nif (i%%2 == 0) {\nhtml = paste0(html, paste0(\"<TD style=\n\\\"background-color:\nsprintf(\"%i\", tab[i, ]),\nhtml = paste0(html, paste0(\"<TD>\", sprintf(\"%i\", tab[i, ]),\nhtml = paste0(html, \"</TR>\")\nhtml = paste0(html, sprintf(\"<tr><td colspan=\\\"%i\\\"\nstyle=\\\"text-align:left;\\\">\",\n1 + ncol(tab)),\nsprintf(gettext(\"Rows: %s\"), vars[2]),\n\"</td></tr>\",\nsprintf(\"<tr><td colspan=\\\"%i\\\"><hr></td></tr>\",\n1 + ncol(tab)))\nfor (i in seq(lines)) html = paste0(html,\nsprintf(\"<tr><td colspan=\n\\\"%i\\\" style=\n\\\"text-align:left;\\\">\",\nlines[i], \"</td></tr>\")\nhtml = paste0(html, \"</table>\")\nucfirst = function(txt) {\nreturn(paste0(toupper(substr(txt, 1, 1)), substring(txt, 2)))\ngettext = function(msg, utype = \"vector\") {\ntype = pmatch(utype, c(\"name\", \"numeric\"))\nif (is.na(type)) {\nret = paste0(ifelse(mmstat$debug > 2, \"?\", \"\"), msg)\npos = match(msg, mmstat$messages$id)\nind = (1:length(pos))[!is.na(pos)]\nret[ind] = mmstat$messages$str[pos[ind]]\n} else if (type == 1) {\nret = as.list(msg)\nnames(ret) = gettext(msg)\n} else if (type == 2) {\nret = as.list(seq(msg))\nreturn(ret)\n# Logging\nmmstat.getLog = function(session) {\nif (!mmstat$debug)\nreturn(\"\")\ninvalidateLater(100, session)\npaste0(\"<hr>\", paste(mmstat$log, collapse = \"<br>\"))\nmmstat.log = function(txt) {\nif (mmstat$debug > 1)\nmmstat$log <<- cbind(sprintf(\"%s (Info): %s\",\ndate(), txt),\nmmstat.warn = function(cond, txt) {\nif ((mmstat$debug > 0) && cond)\nmmstat$log <<- cbind(sprintf(\"%s (Warn): %s\",\nmmstat.input = function(input) {\nif (mmstat$debug > 1) {\nni = names(input)\nfor (i in seq(ni)) {\nout = capture.output(input[[ni[i]]])\nout[1] = paste0(ni[i], \": \", out[1])\nfor (j in seq(out))\nmmstat$log <<- cbind(sprintf(\"%s (Input): %s\",\ndate(), out[j]), mmstat$log)\n# Main\nmmstat = list(debug = 1,\nshiny = sessionInfo()$otherPkgs$shiny$Version,\ncol = list(daquamarine = \"#1B9E77\",\ndorange = \"#D95F02\",\n1, 2.5, 5, 10, 20),\nUI = vector(\"list\", 0))\nmmstat.log(\"Starting app\")\n", "output_sequence": "Shows the conditional probability that the dice is either fair or loaded given the number of rolled sixes (X) in the upper panel. The lower panel shows a bar plot of the conditional probability to roll a specific number of sixes given a fair/loaded dice. The user can interactively choose (1) the number of rolls, (2) the number of rolled sixes for the upper panel and (3) the probability to roll a six with the loaded dice."}, {"input_sequence": "# ------------------------------------------------------------------------------\n# Name of Quantlet: MMSTATdice_game\n# ------------------------------------------------------------------------------\n# Published in: MMSTAT\n# Description: Shows the conditional probability that the dice is either fair or loaded\n# given the number of rolled sixes (X) in the upper panel.\n# The lower panel shows a bar plot of the conditional probability to roll\n# a specific number of sixes given a fair/loaded dice.\n# The user can interactively choose (1) the number of rolls,\n# (2) the number of rolled sixes for the upper panel and (3) the probability\n# to roll a six with the loaded dice.\n# Keywords: conditional distribution, plot, data visualization,\n# visualization, cdf, interactive, estimation, parameter,\n# parametric\n# Usage: MMSTAThelper_function\n# Output: Interactive shiny application\n# Example: Sets the number of rolls equal to 20, the number of sixes is set to 1\n# and the probability for six with loaded dice is set to 0.66.\n# See also: BCS_Hist1, MVAcondnorm, COPdaxnormhist\n# MMSTATtime_series_1, MMSTATlinreg, MMSTATconfmean,\n# MMSTATconfi_sigma, MMSTATassociation, MMSTAThelper_function\n# Author : Sigbert Klinke\n# Code Editor: Yafei Xu\n\n# please use \"Esc\" key to jump out of the Shiny app\nrm(list = ls(all = TRUE))\ngraphics.off()\n# please set working directory setwd('C:/...')\n# setwd('~/...') # linux/mac os\n# setwd('/Users/...') # windows\nsource(\"MMSTAThelper_function.r\")\n############################### SUBROUTINES ##################################\nmmstat.ui.elem('rolls', 'sliderInput',\nlabel = gettext(\"Number of rolls:\"),\nmin = 1,\nmmstat.ui.elem(\"prob\", 'sliderInput',\nlabel = gettext(\"Probability for six with loaded dice:\"),\nmin = 0,\nvalue = .16666*2)\nmmstat.ui.elem(\"cex\", 'fontSize')\nmmstat.ui.elem(\"sixes\", 'sliderInput',\nlabel = gettext(\"Number of sixes (X):\"),\nmax = 3,\nddbinom = function (x, size, prob) {\nif (prob <= 0) {\nreturn (as.numeric(x == 0))\n}\nif (prob >= 1) {\nreturn (as.numeric(x == size))\ndbinom(x, size, prob)\n}\nserver = shinyServer(function(input, output, session) {\n\noutput$rollsUI = renderUI({ mmstat.ui.call('rolls') })\noutput$distPlot = renderPlot({\ninp = mmstat.getValues(NULL,\ncex = input$cex,\nt = 0:inp$rolls\nw0 = dbinom(t, inp$rolls, 1/6)\nmp = barplot(rbind(w1,w0),\nmain = gettext(\"P(Number of sixes | Dice type)\"),\nylim = c(0,1),\ncol = c(mmstat$col[[2]],\nbeside = T,\ncex.axis = inp$cex,\nlegend(\"topright\", gettext(c(\"loaded dice (W=1)\", \"fair dice (W=0)\")),\ncex = inp$cex,\nfill = c(mmstat$col[[2]],\nmp = colMeans(mp)\naxis(1, at = mp, labels=sprintf(\"%.0f\", t), cex.axis = inp$cex)\nbox()\n})\noutput$formula = renderText({\nsixes = input$sixes,\nt = 0:inp$rolls\nw0 = dbinom(t, inp$rolls, 1/6)\np1 = w1[1 + inp$sixes] / (w1[1 + inp$sixes] + w0[1 + inp$sixes])\n\npaste0(sprintf('<table style=\"font-size:%.0f%%\"><tr style=\"color:%s\"><td>',\n90*inp$cex, mmstat$col[[2]]),\nsprintf('P(W=1|X=%.0f) = ', inp$sixes),\n'</td><td align = \"center\">',\nsprintf(' (P(X=%.0f|W=1)*P(W=1)) / (P(X=%.0f|W=0)*P(W=0)+P(X=%.0f|W=1)*P(W=1))',\ninp$sixes,\n'</td><td> = </td><td align = \"center\">',\nsprintf(' (%.3f*0.5) / (%.3f*0.5+%.3f*0.5)',\nw1[1+inp$sixes],\nsprintf(' %0.3f', p1),\n'</td></tr><tr><td><br><br></td></tr>',\nsprintf('<tr style=\"color:%s\"><td>', mmstat$col[[1]]),\nsprintf('P(W=0|X=%.0f) = ', inp$sixes),\n'</td><td align=\"center\">',\nsprintf(' (P(X=%.0f|W=0)*P(W=0)) / (P(X=%.0f|W=0)*P(W=0)+P(X=%.0f|W=1)*P(W=1))',\n'</td><td> = </td><td align=\"center\">',\nw0[1+inp$sixes],\nsprintf(' %0.3f', p0),\n'</td></tr><tr><td><br><br></td></tr></table>')\noutput$logText = renderText({\nmmstat.getLog(session)\n})\n### ui #######################################################################\nui = shinyUI(fluidPage(\ndiv(class = \"navbar navbar-static-top\",\ndiv(class = \"navbar-inner\",\nfluidRow(column(4, div(class = \"brand pull-left\",\ngettext(\"Dice rolling\"))),\ncolumn(2, checkboxInput(\"showgame\",\ngettext(\"Game parameter\"), TRUE)),\ncolumn(2, checkboxInput(\"showprob\",\ngettext(\"Probability\"), TRUE)),\ncolumn(2, checkboxInput(\"showoptions\",\ngettext(\"Options\"), FALSE))))),\nsidebarLayout(\nconditionalPanel(\ncondition = 'input.showgame',\nuiOutput(\"rollsUI\"),\nbr(),\nuiOutput(\"sixesUI\")\n),\ncondition = 'input.showprob',\nuiOutput(\"probUI\")\ncondition = 'input.showoptions',\nhr(),\nuiOutput(\"cexUI\")\n)\nmainPanel(htmlOutput(\"formula\"),\nplotOutput(\"distPlot\"))),\nhtmlOutput(\"logText\")\n))\n### shinyApp #################################################################\nshinyApp(ui = ui, server = server)\n", "output_sequence": "Shows the conditional probability that the dice is either fair or loaded given the number of rolled sixes (X) in the upper panel. The lower panel shows a bar plot of the conditional probability to roll a specific number of sixes given a fair/loaded dice. The user can interactively choose (1) the number of rolls, (2) the number of rolled sixes for the upper panel and (3) the probability to roll a six with the loaded dice."}, {"input_sequence": "%% Clearing all variables\nclear all; clc;\n%% Figure settings\nfonttype = 'Times New Roman';\nfontsize = 16;\nfontsize_axes_top = 10;\npapersize = [20 10];\nquantile_linewidth = 1.4;\n% data input\nmerge = readtable('ARRdata.dat','Delimiter',';');\n% selecting the Google Scholar citations over 2007 till 2014\nsubstrmatch = @(x,y) ~cellfun(@isempty,strfind(y,x));\nfindmatching = @(x,y) y(substrmatch(x,y));\nx = sort(findmatching('gs_citation_20',merge.Properties.VariableNames));\nx = x(2:size(x,2));\n%% Data selecting (GS citations over 2008 till 2015)\nTF = ismissing(merge(:,x));\nz = table2array(merge(~any(TF,2),x));\n%% Standardizing of values\ny = (z-(ones(size(z,1),1)*min(z)))./(ones(size(z,1),1)*(max(z)-min(z)+(max(z)==min(z))));\nlabel = {'2008','2009','2010','2011','2012','2013','2014','2015'};\ny = y(~(y(:,4)>0.5&y(:,5)<0.5),:);\n%% Creating figure\nfigure1 = figure('Visible','on','PaperPosition',[0 0 papersize],'PaperSize',papersize);\nparallelcoords(y,'linewidth',0.01,'label', label,'Color',[138/255 15/255 20/255])\nset(gca,'FontSize',fontsize_axes,'FontName',fonttype,'YTickLabel',{'0','1'},'YTick',[0 1]);\nylabel('Citations','FontSize',fontsize,'FontName',fonttype);\n% computing and ploting of quartiles\ny2=quantile(y,[0.25 0.5 0.75]);\nline(1:length(label),transpose(y2),'linewidth',quantile_linewidth,'Color','k','LineStyle','--');\nbox on;\n% creating a second x-axes on top of the plot where the maximum values are noted\nax1 = gca;\nax2 = axes('Position',ax1.Position,...\n'FontSize',fontsize_axes_top,...\n'XAxisLocation','top',...\n'XTick',(ax1.XTick-1)/(length(ax1.XTick)-1),...\n'XTickLabel',max(z),...\n'Color','none');\n%% Saving figure\n", "output_sequence": "Creates a parallel coordinates plot for GS citation for the period from 2008 till 2015 with quartiles"}, {"input_sequence": "# clear variables\nrm(list=ls(all=TRUE))\n# load library\n# Install Packages\nlibraries = c(\"vcd\")\nlapply(libraries, function(x) if (!(x %in% installed.packages()))\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# setting (font, color) for output, depending on choice\nfont = \"serif\" #Helvetica\"\nres = 300\ngrey.colors = c(\"white\",\nrgb( 80, 95,108,alpha=108*1.0,maxColorValue = 108),\n)\n# load data\ndata = read.csv2(\"ARRdata.csv\", sep = \";\", dec = \",\", header = T, stringsAsFactors = FALSE)\n# Computation of frquency table\nfreq = data.frame()\ntab = c(\"Yes\",\"No\")\nfor (i.hb in tab) {\nif (i.hb == \"Yes\") {\nhb = !is.na(data$hb_commonscore)\n} else {\nhb = T\n}\nif (i.rp == \"Yes\") {\nrp = !is.na(data$rp_rank)\nrp = T\nif (i.gs == \"Yes\") {\ngs = !is.na(data$gs_author)\ngs = T\ni = dim(freq)[1] + 1\nfreq[i, \"HB\"] = i.hb\nif (i.hb == \"Yes\" | i.rp == \"Yes\") {\ntmp = dim(data[hb & rp & gs, ])[1]\ntmp = ifelse(i.gs == \"Yes\", 10000, 0)\nfreq[i, \"Freq\"] = tmp\n}\n}\nfreq = freq[order(freq[,1],freq[,2],freq[,3],decreasing = F),]\ncolgr = 1+(freq$HB==\"Yes\")+(freq$RP==\"Yes\")+(freq$GS==\"Yes\")\nfreq$color = grey.colors[colgr]\nm = freq\nfont = \"serif\" # Font Times\nwidth = 24 # width of plot pdf\nfontsize_big = 44 # font size of the main titles\nmarg = 9 # margin\nfontsize_cells = 30 # font size of cell labels\n# mosaic plot of age groups against Ranking groups\npng(file = \"ARRmosage.png\", width = width, height = height, units = \"in\", res = res, family = font)\nmosaic(~HB + RP + GS, data = m, gp = gpar(fill = m$color), shade = TRUE, direction = c(\"v\",\"h\", \"h\"),\nzero_size = 0, labeling = labeling_values,\ngp_text = gpar(fontsize = fontsize_cells,fontface = 2, col = grey.colors[2]),\nlabeling_args = list(gp_labels = gpar(fontsize = fontsize_small, fontface = 1, fontfamily = font),\nkeep_aspect_ratio = FALSE, margins = unit(c(marg, marg, 1, marg), \"lines\"), split_vertical = F)\n# gp: coloring according to number of merged rankings\ndev.off()\n", "output_sequence": "Produces the mosaic plot of number of researchers, when merging of Handelsblatt (HB), RePEc (RP) and Google Scholar (GS) rankings takes place"}, {"input_sequence": "function ARR_scaage_boxplot_function(data,papersize,label,fontsize,fontsize_axes,fonttype,filename,x_label,y_label,ages)\nfigure1 = figure('Visible','on',...\n'PaperPosition',[0 0 papersize],...\nif strcmp(y_label,'RP')\ndata(:,2) = -data(:,2);\nend\nage_vector = repmat(0,size(data,1),1);\nfor i=1:(length(ages)-1)\ntmp =(data(:,1)>ages(i)).*(data(:,1)<=ages(i+1));\ntmp_age(:,i) =logical(tmp);\nage_vector =age_vector+tmp_age(:,i)*i;\nm(i) =mean(data(tmp_age(:,i),2));\nhold on\nm = m(unique(age_vector));\nfor l_i = 1:(length(unique(age_vector)))\na = 0.6+l_i-1;\nline([a b],[m(l_i) m(l_i)],'Color','k','LineStyle',':','LineWidth',1.2);\nboxplot(data(:,2),age_vector,'Symbol','.','labels',label(unique(age_vector)),'widths',0.8);\nset(gca,'FontSize',fontsize_axes,'FontName',fonttype,...\n'YTickLabel',{600:-200:0},'YTick',[sort(-(0:200:600))]...\n);\nelse\nset(gca,'FontSize',fontsize_axes,'FontName',fonttype);\nhold off\nset(figure1,'Position',[0 0 1 1]);\nprint(figure1,'-dpng','-r400',filename);\n", "output_sequence": "Creates boxplots of the main scores of Handelsblatt (HB), RePEc (RP) and Google Scholar (GS) rankings over the age intervalls: <35, 36-40, 41-45, 46-50, 51-55, 56-60, 61-65, 66-70, 71>"}, {"input_sequence": "%% Clearing all variables\nclear all; clc;\n%% Image settings\nfonttype = 'Helvetica';\nfontsize = 24;\npapersize = [15 10];\n%% Data input\nsubage = readtable('ARRdatage.csv','Delimiter',';');\n%% Data selection\nsubagehb = table2array(subage(:, 1:2));\n%% Data\nages = [0 35 40 45 50 55 60 65 70 999];\nlabel = {'<36' '36-40' '41-45' '46-50' '51-55' '56-60' '61-65' '66-70' '70>'};\n%% Creating figures with the function\nARRboxage_fun(subagehb,papersize,label,fontsize,fontsize_axes,fonttype,'ARRboxage_HB','Age','HB',ages)\n", "output_sequence": "Creates boxplots of the main scores of Handelsblatt (HB), RePEc (RP) and Google Scholar (GS) rankings over the age intervalls: <35, 36-40, 41-45, 46-50, 51-55, 56-60, 61-65, 66-70, 71>"}, {"input_sequence": "# ------------------------------------------------------------------------------ Book: SFS\n# theoretical (red line) and empirical (blue) Mean excess function e(u) of the Frechet distribution with alpha = 2\n# ------------------------------------------------------------------------------ Usage: -\n# ------------------------------------------------------------------------------ Output: Plot of mean excess function for\n# Frechet distriubtion. ------------------------------------------------------------------------------ Example: -\n# Close all plots and clear variables\ngraphics.off()\nrm(list = ls(all = TRUE))\n# Install and load library\ninstall.packages(\"matlab\")\nlibrary(matlab)\n# Simulate Frechet Distribution\nnsim = 1000\nurv = runif(nsim, min = 0, max = 1)\nones = matrix(1, nsim, 1)\nrv = ones/((-log(urv))^(1/(2 * ones)))\nk = 999\nrv = sort(rv, decreasing = TRUE)\nMEF = matrix(, , )\nt = rv[1:k + 1] #t must be >0\n# Calculate the empirical mean excess function\nfor (i in 1:length(t)) {\ny = rv[rv > t[i]]\nMEF[i] = mean(y - t[i])\n}\nplot(t[3:k + 1], t[3:k + 1], type = \"l\", col = \"red\", lwd = 2, xlab = c(\"u\"), ylab = c(\"e(u)\"))\npoints(t[3:k + 1], MEF[3:k + 1], col = \"blue\", pch = 23, bg = \"blue\")\ntitle(\"Plot Mean Excess Function\")\n", "output_sequence": "Plots the theoretical and empirical mean excess function e(u) of the Frechet distribution with alpha = 2."}, {"input_sequence": "% ---------------------------------------------------------------------\n% Book: SFS\n% ---------------------------------------------------------------------\n% Quantlet: SFS_mef_frechet\n% Description: SFS_mef_frechet plots the theoretical (red line) and empirical\n% (blue) Mean excess function e(u) of the Frech\ufffdt\n% distribution with alpha = 2\n% Usage: SFS_mef_frechet\n% Inputs: none\n% Output: Plot of mean excess function for Frechet distriubtion.\n% Example: -\n% Author: Barbara Choros\n%\n% Simulate Frechet Distribution\nclear\nclc\nclose all\nnsim = 1000;\nurv = rand(nsim,1);\nrv = ones(nsim,1)./((-log(urv)).^(1./2*ones(nsim,1)));\nk = 999;\n\n% Calculate the empirical mean excess function\nrv = sort(rv,'descend');\nt = rv(1:k+1);%t must be >0\nfor i=1:length(t)\ny = rv(find(rv > t(i)));\nMEF(i) = mean(y-t(i));\nend\nplot(t(3:k+1),MEF(3:k+1),'.','MarkerSize',10);\nhold on;\nplot(t(3:k+1),t(3:k+1),'LineWidth',2,'Color','r');\ntitle('Plot Mean Excess Function');\nxlabel('u');\n", "output_sequence": "Plots the theoretical and empirical mean excess function e(u) of the Frechet distribution with alpha = 2."}, {"input_sequence": "%Author: Maria Osipenko\nfunction [Call Put] = blspricevec(S,K,r,sigma,tau)\n%Black Scholes formula for vector inputs\nif tau == 0\nt = 1;\nelse\nt = 0;\nend\ny = (log(S./K)+(r-sigma.^2/2).*tau)/(sigma.*sqrt(tau)+t);\ncdfn = normcdf(y+sigma.*sqrt(tau));\n\nif t == 0\nt_l = 1;\nend\nCall = S.*(cdfn.*t_l+t)-K.*exp(-r.*tau).*normcdf(y).*t_l+t;\n", "output_sequence": "Black Scholes formula for vector inputs, MatLab function needed for SFSstoploss."}, {"input_sequence": "!git clone https://github.com/josemiotto/pylevy\n# navigate to atalaia directory\n%cd pylevy\n# get modifications made on the repo\n!git pull origin master\n# install packages requirements\n#!pip install -r requirements.txt\n# install package\n!python setup.py install\n# Import packages\nimport numpy as np\nimport pandas as pd\nimport scipy.stats as st\nfrom scipy.stats import norm, gumbel_l\nimport sklearn as skl\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.cluster import KMeans, AgglomerativeClustering\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import metrics\nfrom sklearn.metrics.cluster import adjusted_rand_score\nfrom sklearn.decomposition import PCA, FactorAnalysis\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom nolds import hurst_rs\nimport random\nimport seaborn as sb\nimport levy\nimport factor_analyzer as fa\nimport pypfopt\n#Features\nfeatures = ['Sharpe', 'std', 'skew', 'kurt', 'VaR0.05', 'ES', 'beta', 'VaR0.95', 'EU', 'autocor', 'Hurst', 'stab_a',\nnum_feat = len(features) #number of features\n#monthly update\nnum_updates=118 #2360:20=118\n#Feature Matrix\nlogreturns = pd.read_csv('logreturns_stocks.csv', index_col=0)\ndates_rightorder = logreturns.index.values\nnum_stocks = logreturns.shape[1] #number of stocks (including market average)\nfor k in range(num_updates):\n#rolling window of logreturns and logreturns_np\nlogr = logreturns.iloc[k*20:250+k*20,:]\n#Features\nData_mon[:,0,k] = logr.mean(axis=0)/logr.std(axis=0)\nData_mon[:,5,k] = logr[logr < Data_mon[:,4,k]].mean(axis=0)\nData_mon[:,7,k] = logr.quantile(q=0.95, axis=0)\nData_mon[:,8,k] = logr[logr > Data_mon[:,7,k]].mean(axis=0)\n#CAPM beta\nr_market = logr_np.transpose()[:,num_stocks-1].reshape(250,1)\nfor j in range(num_stocks):\nX = logr_np.transpose()[:,j].reshape(250,1)\nreg = LinearRegression().fit(X, r_market)\nData_mon[j,6,k]=reg.coef_\nfor i in range(num_stocks):\nseries = logr.iloc[:,i]\n#autocorrelation coefficient\nData_mon[i,9,k]= series.autocorr()\n#Hurst\nData_mon[i,10,k] = hurst_rs(series) #using nolds package\n#fit levy stable distribution\nlevystab = levy.fit_levy(series)\n#alpha stable\nData_mon[i,11,k] = levystab[0].get()[0]\n#gamma stable\nfor k in range(Data_mon.shape[2]):\nData = Data_mon[:,:,k]\nScData = StandardScaler().fit_transform(Data[:num_stocks -1,1:]) # Data ohne EURO bzw Sharpe und ohne market_avg\n\nfig, ax = plt.subplots(figsize=(10,10))\nsb.heatmap(data=pd.DataFrame(ScData, columns=features[1:]).corr(), annot=True, cmap=\"vlag\")\nax.set_title('Correlation matrix')\nplace = '/content/drive/MyDrive/VideosMA/StocksCorrMat/plot' + str(k+100) + '.png'\n#Goal: find best k stocks with k is number of clusters\n#number of clusters\nnum_clust = [3,4,5,6,7,8,9,10,15,20]\nmodel_names = ['km03_PC2',\n'km03_FA2',\n]\n#Assigned clusters Matrix: for every model and every stock and every time: to which cluster belongs the stock?\n#without market average\n#For evaluation\n#Silhouette coefficient: between -1 (bad) and 1 (good). Close to zero: overlapping clusters\nSil_coef = np.zeros((len(model_names), num_updates))\n#Calinski-Harabasz Index = Variance Ratio Criterion: higher better\nVarRatC = np.zeros((len(model_names), num_updates))\n#Davies-Bouldin-Index: close to zero is best\nScData = StandardScaler().fit_transform(Data[:num_stocks-1,1:]) # Data without Sharpe Ratio and without market_avg\n#Perform PCA\npca2 = PCA(2)\nY_2 = pca2.fit_transform(ScData)\nPrinComp = pd.DataFrame(pca3.components_).transpose()\nPrinComp.index = features[1:]\n#Perform FA with varimax\nfac2_vari = FactorAnalysis(n_components=2, rotation = 'varimax')#FA with 2 factors and Varimax rotation\nF_2V = fac2_vari.fit_transform(ScData)\n#Plots for Video\nfig, axes = plt.subplots(1, 3, figsize=(15,5))\naxes[0].set_title('PCA')\nsb.heatmap(ax=axes[0], data=PrinComp, annot=True, cmap=\"vlag\", vmin=-1, cbar=False)\naxes[1].set_title('FA with 2 factors with Varimax')\nsb.heatmap(ax=axes[1], data=pd.DataFrame(fac2_vari.components_, columns=features[1:]).transpose(), annot =True, cmap=\"vlag\", vmin=-1, cbar=False)\naxes[2].set_title('FA with 3 factors with Varimax')\nplace = '/content/drive/MyDrive/VideosMA/StocksFactors/plot' + str(k+100) + '.png'\nfig.savefig(place)\nInput_Data = [Y_2, Y_3, F_2V, F_3V]\n#Clustering\nmodel_num = 0 #for selecting column\nfor c in num_clust:\nfor Input in Input_Data:\nkmeans = KMeans(n_clusters=c).fit(Input)\nlabels = kmeans.labels_\n#Ass_Clusters[:,model_num,k] = kmeans.predict(Input)\n#Evaluation\nSil_coef[model_num, k] = metrics.silhouette_score(Input, labels)\n\nmodel_num = model_num+1\nlabels = AgglomerativeClustering(n_clusters=c).fit_predict(Input)\nnum_models = Ass_Clusters.shape[1]\n#One Weight Matrix for every Asset-Allocation = for every Portfolio\n#Ass_Clusters.shape = 961, 24, 118\n#Data_mon.shape = 961, 10, 118\nfor m in range(num_models):\ny_true = Ass_Clusters[:,m,k]\nSharpe = Data_mon[:num_stocks-1,0,k]\nnc = max(y_true)+1 #number of clusters, +1 because Clusters are named 0,1,2,3...\nfor j in range(int(nc)):\nif (y_true == j).sum() >=5: #only clusters with 5 or more elements are considered\ncopy_Sharpe = np.copy(Sharpe) #we need copy because see wrong code below\ncopy_Sharpe[y_true!=j] = -100000 #no Sharpe is below -100000\nidxmax = copy_Sharpe.argmax() #Choose stock with maximum Sharpe from each cluster\n#Weights of equal weight strategy\nWeights_EW = np.zeros((num_stocks-1, num_updates, num_models))\nfor n in range(num_models):\n#Weights of Max Sharpe portfolio (\"Markowitz\")\nWeights_MV = np.zeros((num_stocks-1, num_updates, num_models))\nfor m in range(num_models):\n#first: get relevant part of logreturns_np.\n#logreturns_np.shape = (961, 2610)\n#append False because logreturns_np includes market_avg, but Weights_ones doesn't\nrel_logr = logreturns_np[np.append(Weights_ones[:,k,m]==1, False), (k*20):(250+k*20)]\nexpected_returns = rel_logr.mean(axis=1)\n\ncov_matrix = np.cov(rel_logr)\nef = EfficientFrontier(expected_returns, cov_matrix)\nmv = ef.min_volatility()\nms = ef.nonconvex_objective(\npypfopt.objective_functions.sharpe_ratio,\nobjective_args=(ef.expected_returns, ef.cov_matrix),\nweights_sum_to_one=True,\n)\nWeights_MV[Weights_ones[:,k,m]==1, k, m]= np.fromiter(mv.values(), dtype=float)\nnum_clustermodels = 8 #(because of: 4 Inputs x 2 (kmeans, agg))\n#for km_PC2', 'ac_PC3', 'km_FA2',\n#We have already calculated the optimal weights for each strategy\n#Saved in Weights_MV,\n#each has dimensions: (num_stocks-1, num_updates, num_models)\n#just select weights from these according to criterions\n#Criterions saved in Sil_coef, VarRatC, Dav_Bould with dim=(num_models, num_updates)\n#optimal number of clusters: save here argmax, i.e. [0,1,2] instead of [3,5,10]\noptclustnum_sil = np.zeros((num_updates, num_clustermodels))\n#Weights for criteria and strategies\n#Equal weights\nWeights_optclust_sil_eq = np.zeros((num_stocks-1, num_updates, num_clustermodels))\n#Mean Variance\nWeights_optclust_sil_mv = np.zeros((num_stocks-1, num_updates, num_clustermodels))\n#Max Sharpe\nWeights_optclust_sil_ms = np.zeros((num_stocks-1, num_updates, num_clustermodels))\nfor i in range(num_clustermodels): #for km_PC2', 'ac_PC3', 'km_FA2',\nran = range(i,num_models,8) #range, such that we choose indices of all numbers of clusters i.e.[3,5,10] for each of the above models\n#that means: ran =[0,8,16,24...], ran= [1,9,17,25,...],...,ran=[7,15,23,31,...]\nsil_am = Sil_coef[ran,k].argmax()\noptclustnum_sil[k,i] = sil_am\n#Weights\nWeights_optclust_sil_eq[:,k,i]= Weights_EW[:,k,ran[sil_am]]\noptclustnum_sil_true = optclustnum_sil + 3\noptclustnum_sil_true[np.where(optclustnum_sil_true==11)] = 15\noptclustnum_db_true = optclustnum_db + 3\noptclustnum_db_true[np.where(optclustnum_db_true==11)] = 15\noptclustnum_vrc_true = optclustnum_vrc + 3\noptclustnum_vrc_true[np.where(optclustnum_vrc_true==11)] = 15\ndates_updates = dates_rightorder\ndates_updates = dates_updates[np.array(range(118))*20+250]\n#New figure\nfontsize=25\nplt.rc('font', size=fontsize) #controls default text size\nplt.rc('axes', titlesize=fontsize) #fontsize of the title\nplt.rc('xtick', labelsize=20) #fontsize of the x tick labels\nplt.rc('legend', fontsize=fontsize) #fontsize of the legend\nfig, ax = plt.subplots(8,1, figsize=(15,25))\nplt.tight_layout(h_pad=3.0)\nax[0].set_title('K-Means with PC2')\nax[3].set_title('Agg. Clustering with PC3')\nfor k in range(8):\nax[k].plot(dates_updates, optclustnum_sil_true[:,k], 'o')\n#ax[0].legend(['Silhouette', 'Variance Ratio', 'Davies Bouldin'], bbox_to_anchor=(1,1))\nweights_allequal = np.ones(960)*(1/960)\nCPW_MarkAvg = np.zeros(2361)\nCPW_MarkAvg[0]=1\nTS_MarkAvg = np.zeros(2360)\nfor t in range(2360):\nnp_in_MarkAvg = np.inner(weights_allequal,logreturns_np[:num_stocks-1,249+t+1])\nTS_MarkAvg = np_in_MarkAvg\n#Cumulative Portfolio Wealth\nCPW_EW = np.zeros((2361, num_models)) #For equal-weights strategy\nCPW_MS = np.zeros((2361, num_models))\nCPW_EW[0,:] = 1 #Initial CPW is 1\n#Timeseries of sums of weighted logreturns\nTS_EW = np.zeros((2360, num_models))\nfor m in range(num_models):\nfor j in range(20):\nt = k*20+j #i.e. t from 0 to 117*20+19 = 2359. Note: CPW[0]=1\nnp_in_EW = np.inner(Weights_EW[:,k,m],logreturns_np[:num_stocks-1,249+t+1])\nTS_EW[t,m] = np_in_EW\nCPW_EW[t+1,m] = CPW_EW[t,m] + np_in_EW\nnp_in_MV = np.inner(Weights_MV[:,k,m],logreturns_np[:num_stocks-1,249+t+1])\nTS_MV[t,m] = np_in_MV\nCPW_MV[t+1,m] = CPW_MV[t,m] + np_in_MV\nnp_in_MS = np.inner(Weights_MS[:,k,m], logreturns_np[:num_stocks-1, 249+t+1])\nTS_MS[t,m] = np_in_MS\nCPW_MS[t+1,m] = CPW_MS[t,m] + np_in_MS\nCPW_opt_clust = np.zeros((2361, #timesteps\nnum_clustermodels, #km/agg and PC23/FA23: 8\n3, #sil, vrc, db\nCPW_opt_clust[0,:,:,:] = 1 #Initial CPW is 1\nTS_opt_clust = np.zeros((2360, num_clustermodels, 3, 3))\nfor m in range(num_clustermodels):\nt = k*20+j\n#0,0 = sil, EW\nnp_in = np.inner(Weights_optclust_sil_eq[:,k,m],logreturns_np[:num_stocks-1,249+t+1])\nTS_opt_clust[t,m,0,0] = np_in\nCPW_opt_clust[t+1,m,0,0] = CPW_opt_clust[t,m,0,0] + np_in\n#1,0 = vrc, EW\nnp_in = np.inner(Weights_optclust_vrc_eq[:,k,m],logreturns_np[:num_stocks-1,249+t+1])\nTS_opt_clust[t,m,1,0] = np_in\nCPW_opt_clust[t+1,m,1,0] = CPW_opt_clust[t,m,1,0] + np_in\n#2,0 = db, EW\nnp_in = np.inner(Weights_optclust_db_eq[:,k,m],logreturns_np[:num_stocks-1,249+t+1])\nTS_opt_clust[t,m,2,0] = np_in\nCPW_opt_clust[t+1,m,2,0] = CPW_opt_clust[t,m,2,0] + np_in\n#0,1 = sil, MV\nnp_in = np.inner(Weights_optclust_sil_mv[:,k,m],logreturns_np[:num_stocks-1,249+t+1])\nTS_opt_clust[t,m,0,1] = np_in\nCPW_opt_clust[t+1,m,0,1] = CPW_opt_clust[t,m,0,1] + np_in\n#1,1 = vrc, MV\nnp_in = np.inner(Weights_optclust_vrc_mv[:,k,m],logreturns_np[:num_stocks-1,249+t+1])\nTS_opt_clust[t,m,1,1] = np_in\nCPW_opt_clust[t+1,m,1,1] = CPW_opt_clust[t,m,1,1] + np_in\n#2,1 = db, MV\nnp_in = np.inner(Weights_optclust_db_mv[:,k,m],logreturns_np[:num_stocks-1,249+t+1])\nTS_opt_clust[t,m,2,1] = np_in\nCPW_opt_clust[t+1,m,2,1] = CPW_opt_clust[t,m,2,1] + np_in\n#0,2 = sil, MS\nnp_in = np.inner(Weights_optclust_sil_ms[:,k,m],logreturns_np[:num_stocks-1,249+t+1])\nTS_opt_clust[t,m,0,2] = np_in\nCPW_opt_clust[t+1,m,0,2] = CPW_opt_clust[t,m,0,2] + np_in\n#1,2 = vrc, MS\nnp_in = np.inner(Weights_optclust_vrc_ms[:,k,m],logreturns_np[:num_stocks-1,249+t+1])\nTS_opt_clust[t,m,1,2] = np_in\nCPW_opt_clust[t+1,m,1,2] = CPW_opt_clust[t,m,1,2] + np_in\n#2,2 = db, MS\nnp_in = np.inner(Weights_optclust_db_ms[:,k,m],logreturns_np[:num_stocks-1,249+t+1])\nTS_opt_clust[t,m,2,2] = np_in\nCPW_opt_clust[t+1,m,2,2] = CPW_opt_clust[t,m,2,2] + np_in\ndef WeightedLogret(WeightMat, logret):\nW = np.zeros((960,2360,WeightMat.shape[2]))\nfor k in range(118):\nW[:,k*20+j,:]=WeightMat[:,k,:]\nWL = np.zeros((960,2360,WeightMat.shape[2]))\nfor k in range(WeightMat.shape[2]):\nWL[:,:,k]=W[:,:,k]*logreturns_np[:num_stocks-1, 250:]\nWL_EW = WeightedLogret(Weights_EW, logreturns_np)\nWL_optclust_db_eq = WeightedLogret(Weights_optclust_db_eq, logreturns_np)\ndef PDI(WL):\nPDI = np.zeros(WL.shape[2])\nfor k in range(WL.shape[2]):\npca=skl.decomposition.PCA() #PCA-Input: X.shape: n_samples, n_features, i.e. here: 2359, 960\npca.fit(WL[:,:,k].transpose())\nPDI[k]=sum(pca.explained_variance_/sum(pca.explained_variance_) *range(1,961))*2-1\nPDI_WL_EW = PDI(WL_EW)\nPDI_WL_db_eq =PDI(WL_optclust_db_eq)\n#CPW to dataframe\ndf_CPW_EW = pd.DataFrame(CPW_EW, columns = model_names, index=dates_rightorder[250:])\ndf_CPW_EW = df_CPW_EW.join(df_CPW_MarkAv)\n#Overview\n#New Figure\nfig, ax = plt.subplots(3,3, figsize=(20,15), sharey=True)\nax[0,0].set_title('Silhouette Coef, 1/n Portfolio')\nax[0,0].plot(df_CPW_EW.loc[:, ['mark_avg']])\nax[0,0].plot(pd.DatetimeIndex(dates_rightorder[250:]), CPW_opt_clust[:,k,0,0])\nax[1,0].set_title('Variance Ratio Crit, 1/n Portfolio')\nax[1,0].plot(df_CPW_EW.loc[:, ['mark_avg']])\nax[1,0].plot(dates_rightorder[250:], CPW_opt_clust[:,k,1,0])\nax[2,0].set_title('Davies Bouldin, 1/n Portfolio')\nax[2,0].plot(df_CPW_EW.loc[:, ['mark_avg']])\nax[2,0].plot(dates_rightorder[250:], CPW_opt_clust[:,k,2,0])\nax[0,1].set_title('Silhouette Coef, MV')\nax[0,1].plot(df_CPW_EW.loc[:, ['mark_avg']])\nax[0,1].plot(dates_rightorder[250:], CPW_opt_clust[:,k,0,1])\nax[1,1].set_title('Variance Ratio Crit, MV')\nax[1,1].plot(df_CPW_EW.loc[:, ['mark_avg']])\nax[1,1].plot(dates_rightorder[250:], CPW_opt_clust[:,k,1,1])\nax[2,1].set_title('Davies Bouldin, MV')\nax[2,1].plot(df_CPW_EW.loc[:, ['mark_avg']])\nax[2,1].plot(dates_rightorder[250:], CPW_opt_clust[:,k,2,1])\nax[0,2].set_title('Silhouette Coef, MS')\nax[0,2].plot(df_CPW_EW.loc[:, ['mark_avg']])\nax[0,2].plot(dates_rightorder[250:], CPW_opt_clust[:,k,0,2])\nax[1,2].set_title('Variance Ratio Crit, MS')\nax[1,2].plot(df_CPW_EW.loc[:, ['mark_avg']])\nax[1,2].plot(dates_rightorder[250:], CPW_opt_clust[:,k,1,2])\nax[2,2].set_title('Davies Bouldin, MS')\nax[2,2].plot(df_CPW_EW.loc[:, ['mark_avg']])\nax[2,2].plot(dates_rightorder[250:], CPW_opt_clust[:,k,2,2])\nfor k in [0,1,2]:\nax[0,k].set_xticks(ax[0,k].get_xticks()[::4])\nax[2,0].legend(['mark_avg', 'KM with PC2', 'AC with PC2', 'KM with PC3', 'AC with PC3',\n'KM with FA2', 'AC with FA2', 'KM with FA3', 'AC with FA3'], ncol=3,\n#EW\nfrom cycler import cycler\ncustom_cycler = (cycler(color=['c', 'm', 'g']))\nfig, ax = plt.subplots(2,3, figsize=(20,10), sharey=True)\nfig.suptitle('1/n Portfolio')\nax[0,0].set_prop_cycle(custom_cycler)\nax[0,0].set_title('Silhouette Coef, K-Means')\nax[0,0].plot(df_CPW_EW.loc[:, ['mark_avg']], color='k')\nfor k in [0,2,4,6]:\nax[0,0].plot(dates_rightorder[250:], CPW_opt_clust[:,k,0,0])\n#ax[0,0].legend(['market avg', 'PC2','PC3','FA2','FA3'])\nax[1,0].set_title('Silhouette Coef, Agg. Cluster')\nax[1,0].plot(df_CPW_EW.loc[:, ['mark_avg']], color='k')\nfor k in [1,3,5,7]:\nax[1,0].plot(dates_rightorder[250:], CPW_opt_clust[:,k,0,0])\nax[0,1].set_title('Variance Ratio Crit., K-Means')\nax[0,1].plot(df_CPW_EW.loc[:, ['mark_avg']], color='k')\nax[0,1].plot(dates_rightorder[250:], CPW_opt_clust[:,k,1,0])\nax[1,1].set_title('Variance Ratio Crit., Agg. Cluster')\nax[1,1].plot(df_CPW_EW.loc[:, ['mark_avg']], color='k')\nax[1,1].plot(dates_rightorder[250:], CPW_opt_clust[:,k,1,0])\nax[0,2].set_title('Davies Bouldin, K-Means')\nax[0,2].plot(df_CPW_EW.loc[:, ['mark_avg']], color='k')\nax[0,2].plot(dates_rightorder[250:], CPW_opt_clust[:,k,2,0])\nax[1,2].set_title('Davies Bouldin, Agg. Cluster')\nax[1,2].plot(df_CPW_EW.loc[:, ['mark_avg']], color='k')\nax[1,2].plot(dates_rightorder[250:], CPW_opt_clust[:,k,2,0])\nax[0,k].set_xticks(ax[0,k].get_xticks()[::3])\nax[1,0].legend(['market avg', 'PC2','PC3','FA2','FA3'], ncol=5,\n#MV\nfig.suptitle('Minimum Variance Portfolio')\nax[0,0].plot(dates_rightorder[250:], CPW_opt_clust[:,k,0,1])\n#MS\nfig.suptitle('Maximum Sharpe Portfolio')\nax[0,0].plot(dates_rightorder[250:], CPW_opt_clust[:,k,0,2])\nfig, ax = plt.subplots(2,4, figsize=(20,10), sharey=True)\nax[0,0].set_title('KM with PC2')\nax[0,0].plot(df_CPW_EW.loc[:, ['km03_PC2',\n'km07_PC2',\n#ax[0,0].legend(['3 clusters', '4 clusters', '5 clusters', '6 clusters', '7 clusters',\n# '8 clusters', '9 clusters', '10 clusters', '15 clusters', '20 clusters', 'mark_avg'])\nax[0,1].set_title('KM with PC3')\nax[0,1].plot(df_CPW_EW.loc[:, ['km03_PC3',\n'km07_PC3',\nax[0,2].set_title('KM with FA2')\nax[0,2].plot(df_CPW_EW.loc[:, ['km03_FA2',\n'km07_FA2',\nax[0,3].set_title('KM with FA3')\nax[0,3].plot(df_CPW_EW.loc[:, ['km03_FA3',\n'km07_FA3',\nax[1,0].set_title('AC with PC2')\nax[1,0].plot(df_CPW_EW.loc[:, ['ac03_PC2',\n'ac07_PC2',\nax[1,1].set_title('AC with PC3')\nax[1,1].plot(df_CPW_EW.loc[:, ['ac03_PC3',\n'ac07_PC3',\nax[1,2].set_title('AC with FA2')\nax[1,2].plot(df_CPW_EW.loc[:, ['ac03_FA2',\n'ac07_FA2',\nax[1,3].set_title('AC with FA3')\nax[1,3].plot(df_CPW_EW.loc[:, ['ac03_FA3',\n'ac07_FA3',\nfor k in [0,1,2,3]:\nax[1,0].legend(['3 clusters', '4 clusters', '5 clusters', '6 clusters', '7 clusters',\n'8 clusters', '9 clusters', '10 clusters', '15 clusters', '20 clusters', 'mark_avg'],\nax[0,0].plot(df_CPW_MV.loc[:, ['km03_PC2',\n# '8 clusters', '9 clusters', '10 clusters', '15 clusters', '20 clusters', 'mark_avg'])\nax[0,1].plot(df_CPW_MV.loc[:, ['km03_PC3',\nax[0,0].set_title('KM with PC2, 1/n Portfolio')\nax[0,0].plot(df_CPW_EW.loc[:, ['km03_PC2', 'mark_avg']])\nax[0,0].legend(['3 clusters', '5 clusters', '10 clusters', 'mark_avg'])\nax[0,1].set_title('KM with PC3, 1/n Portfolio')\nax[0,1].plot(df_CPW_EW.loc[:, ['km03_PC3', 'mark_avg']])\nax[0,2].set_title('KM with FA2, 1/n Portfolio')\nax[0,2].plot(df_CPW_EW.loc[:, ['km03_FA2', 'mark_avg']])\nax[0,3].set_title('KM with FA3, 1/n Portfolio')\nax[0,3].plot(df_CPW_EW.loc[:, ['km03_FA3', 'mark_avg']])\nax[1,0].set_title('AC with PC2, 1/n Portfolio')\nax[1,0].plot(df_CPW_EW.loc[:, ['ac03_PC2', 'mark_avg']])\nax[1,1].set_title('AC with PC3, 1/n Portfolio')\nax[1,1].plot(df_CPW_EW.loc[:, ['ac03_PC3', 'mark_avg']])\nax[1,2].set_title('AC with FA2, 1/n Portfolio')\nax[1,2].plot(df_CPW_EW.loc[:, ['ac03_FA2', 'mark_avg']])\nax[1,3].set_title('AC with FA3, 1/n Portfolio')\nax[0,0].set_title('KM with PC2, Min volatility Portfolio')\nax[0,0].plot(df_CPW_MV.loc[:, ['km03_PC2', 'mark_avg']])\nax[0,1].set_title('KM with PC3, Min volatility Portfolio')\nax[0,1].plot(df_CPW_MV.loc[:, ['km03_PC3', 'mark_avg']])\nax[0,2].set_title('KM with FA2, Min volatility Portfolio')\nax[0,2].plot(df_CPW_MV.loc[:, ['km03_FA2', 'mark_avg']])\nax[0,3].set_title('KM with FA3, Min volatility Portfolio')\nax[0,3].plot(df_CPW_MV.loc[:, ['km03_FA3', 'mark_avg']])\nax[1,0].set_title('AC with PC2, Min volatility Portfolio')\nax[1,0].plot(df_CPW_MV.loc[:, ['ac03_PC2', 'mark_avg']])\nax[1,1].set_title('AC with PC3, Min volatility Portfolio')\nax[1,1].plot(df_CPW_MV.loc[:, ['ac03_PC3', 'mark_avg']])\nax[1,2].set_title('AC with FA2, Min volatility Portfolio')\nax[1,2].plot(df_CPW_MV.loc[:, ['ac03_FA2', 'mark_avg']])\nax[1,3].set_title('AC with FA3, Min volatility Portfolio')\nax[0,0].plot(df_CPW_MS.loc[:, ['km03_PC2', 'mark_avg']])\nax[0,1].set_title('KM with PC3, Max Sharpe Portfolio')\nax[0,1].plot(df_CPW_MS.loc[:, ['km03_PC3', 'mark_avg']])\nax[0,2].set_title('KM with FA2, Max Sharpe Portfolio')\nax[0,2].plot(df_CPW_MS.loc[:, ['km03_FA2', 'mark_avg']])\nax[0,3].set_title('KM with FA3, Max Sharpe Portfolio')\nax[0,3].plot(df_CPW_MS.loc[:, ['km03_FA3', 'mark_avg']])\nax[1,0].set_title('AC with PC2, Max Sharpe Portfolio')\nax[1,0].plot(df_CPW_MS.loc[:, ['ac03_PC2', 'mark_avg']])\nax[1,1].set_title('AC with PC3, Max Sharpe Portfolio')\nax[1,1].plot(df_CPW_MS.loc[:, ['ac03_PC3', 'mark_avg']])\nax[1,2].set_title('AC with FA2, Max Sharpe Portfolio')\nax[1,2].plot(df_CPW_MS.loc[:, ['ac03_FA2', 'mark_avg']])\nax[1,3].set_title('AC with FA3, Max Sharpe Portfolio')\nSharpe_EW = TS_EW.mean(axis=0)/TS_EW.std(axis=0)\n#Certainty Equivalent: mean- variance*0.5*lambda, lambda fuer Risikoaversion, we take lambda=1\nCerE_EW = TS_EW.mean(axis=0)-0.5*TS_EW.var(axis=0)\n#Adjusted Sharpe Ratio\nASR_EW = Sharpe_EW*(1+scipy.stats.skew(TS_EW, axis=0)*Sharpe_EW/6 - scipy.stats.kurtosis(TS_EW, axis=0)*Sharpe_EW*Sharpe_EW/24 )\n", "output_sequence": "Empirical results for stock data. Clustering based on risk measuring variables, PCA or Factor Analysis. Select best asset from each cluster and calculate Portfolio Allocation"}, {"input_sequence": "###################################################\n#Chorpoleth Map of GDP EU and US\n###################################################\n#http://www.r-bloggers.com/plotting-maps-with-r/\n\nlibrary(\"sp\",\"maps\")\nlibrary(\"RColorBrewer\")\nlibrary(\"rworldmap\")\nlibrary(\"GISTools\")\nlibrary(\"diagram\")\ndir= \"C:/Users/Helen/Desktop/FinalVersion-SSPL\"\nsetwd(dir)\nGDP = read.table(\"GDPCountry.txt\", header=TRUE,\nsep = \"\\t\")\nGDP = na.omit(GDP)\n#naming columnes\ncolnames(GDP) = c('country', 2000:2014)\n#saving the blue colour pallete as a picture\npng('BlueShades.png')\npar(mfrow=c(2,1))\ndisplay.brewer.pal(9, \"Blues\")\ndev.off()\n# producing the breaks for the GDP groups\nbrks = c(2000, 4000, 12000, 16000,24000,\n26000,28000,30000,64000)\n# grouping our GDP data\nGDPcut =cut(GDP$'2000', brks)\n#creating a dataframe out of the countries and their GDP group\nd =data.frame(country=GDP$country, value=GDPcut)\nn =joinCountryData2Map(d, joinCode=\"NAME\",\nnameJoinColumn=\"country\")\npar(mfrow=c(1,1))\npng(\"ChoreoBlue.png\")\n#creating choreopleth map\nmapCountryData(n, nameColumnToPlot=\"value\",\nmapTitle=\"US/EUR Currency Exchange Rate\",\nxlim=c(-120, 25), ylim=c(20, 60),\naddLegend=FALSE,\ncolourPalette=brewer.pal(9,'Blues'),\noceanCol=\"lightblue\",\nmissingCountryCol=\"grey\")\n####legend#####\n#legend(\"bottomleft\", \"Legend\",cex=0.5,pt.cex=1)\n#legend(\"topleft\", legend = levels(cut(GDP$'2000',\n# brks)), fill = cols, title = \"N. stations\")\n###### make Animation Choreopleth map (same procedure as before)\n#for one choreopleth map\n#defining GDP breaks\nbrks = c(2000, 4000, 12000, 16000,24000,26000,\n28000,30000,64000)\n#function creates choreopleth map for a year\nfirstChoreo = function(m = 2) {\ntexto=paste(\"Choreo\",m,\".png\",sep=\"\")\npng(texto)\nGDPcut=cut(GDP[,m], brks)\nd =data.frame(country=GDP$country, value=GDPcut)\nn =joinCountryData2Map(d, joinCode=\"NAME\",\nnameJoinColumn=\"country\")\nmapCountryData(n, nameColumnToPlot=\"value\",\nmapTitle=paste(\"US/EUR Currency Exchange Rate in the year\",m+1998),\nxlim=c(-120, 25), ylim=c(50, 55),\naddLegend=FALSE,\noceanCol=\"lightblue\",\nmissingCountryCol=\"grey\")\n}\n#function calls the first function and pauses between the images\nsecondChoreo = function() {\nlapply(seq(2, length(GDP), by = 1), function(i) {\ni=2\nfirstChoreo(i)\nanimation::ani.pause()\n})\n#saving the sequence of pictures as a video\nsaveVideo(secondChoreo(), interval = 1, outdir = dir,\nffmpeg = \"C:/ffmpeg/bin/ffmpeg.exe\",\nvideo.name = \"ChoreoplethMap.mp4\")\n", "output_sequence": "produces Choreopleth Map of the European countries and the US according to GDP data for the years 2000-2014."}, {"input_sequence": "import sys\nimport torch\nfrom torch import nn, optim\nfrom torch.autograd.variable import Variable\nfrom torch import manual_seed as tset_seed\nfrom torchvision import transforms, datasets\nimport torch.distributions as tdist\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom tqdm import tqdm\n\nnp.set_printoptions(precision=4)\nsample_size = 1000\ntset_seed(42)\nclass DiscriminatorNet(torch.nn.Module):\n\"\"\"\nA three hidden-layer discriminative neural network\ndef __init__(self):\nsuper(DiscriminatorNet, self).__init__()#has the same properties as the torch.nn.module\nn_features = sample_size\nn_out = 1\nlayer_neurons=30\nself.hidden0 = nn.Sequential(#Modules will be added to it in the order they are passed in the constructor\nnn.Linear(n_features, layer_neurons),\nnn.LeakyReLU(0.2),\nnn.Dropout(0.1)\n)\nself.hidden1 = nn.Sequential(\nnn.Linear(layer_neurons, layer_neurons),\nself.hidden2 = nn.Sequential(\ntorch.nn.Linear(layer_neurons, n_out),\ntorch.nn.Sigmoid()\nself.requires_grad=True\nself.bias = tdist.Normal(0,0.1)\ndef forward(self, x):\nx = self.hidden0(x)\nreturn x\nclass GeneratorNet(torch.nn.Module):\nA three hidden-layer generative neural network\nsuper(GeneratorNet, self).__init__()\nn_features = 100\nn_out = sample_size\nself.hidden0 = nn.Sequential(\nnn.LeakyReLU(0.1)\nself.hidden1 = nn.Sequential(\n\nnn.Linear(layer_neurons, n_out),\nnn.Tanh()\n# is used when net is setup\n# Noise\ndef noise(size):\nn = Variable(tdist.Normal(0,1).sample((1,size)))\nreturn n\ndef ones_target(size):\n'''\nTensor containing ones, with shape = size\ndata = Variable(torch.ones(size))\nreturn data\ndef zeros_target(size):\nTensor containing zeros, with shape = size\ndata = Variable(torch.zeros(1,size))\ndef real_data_target(size):\n'''\nTensor containing ones, with shape = size\ndata = Variable(torch.ones(size, 1))\nreturn(data)\ndef fake_data_target(size):\nTensor containing zeros, with shape = size\ndata = Variable(torch.zeros(size, 1))\ndef pretrain_discriminator(optimizer,real_data):\noptimizer.zero_grad()\nprediction_real = discriminator(real_data)\nerror_real = loss(prediction_real, ones_target(1))\n\nerror_real.backward()\n#print(error_real.grad)\noptimizer.step()\nreturn(error_real)\ndef train_discriminator(optimizer, real_data, fake_data):\nN = real_data.size(0)\n# Reset gradients\noptimizer.zero_grad()\n# 1.1 Train on Real Data\nprediction_real = discriminator(real_data)\n\n# Calculate error and backpropagate\nerror_real = loss(prediction_real, ones_target(1))\nerror_real.backward()\ngrad = [p.grad for p in list(discriminator.parameters())]\n# 1.2 Train on Fake Data\nprediction_fake = discriminator(fake_data)\nerror_fake = loss(prediction_fake, zeros_target(1))\nerror_fake.backward()\n# 1.3 Update weights with gradients\noptimizer.step()\n# Return error and predictions for real and fake inputs\nreturn error_real + error_fake, prediction_real,\ndef train_generator(optimizer, fake_data):\n# 2. Train Generator\n# Reset gradients\n# Sample noise and generate fake data\nprediction = discriminator(fake_data)\n# Calculate error and backpropagate\nerror = loss(prediction, real_data_target(prediction.size(0)))\nerror.backward()\n# Update weights with gradients\n# Return error\ndiscriminator = DiscriminatorNet()\ngenerator = GeneratorNet()\n# Optimizers\nd_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002)\ndef lr_decay(epoch):\nif epoch <= 2000:\nreturn 0.0004\nif epoch <= 4000:\nreturn 0.0002\n# Loss function\nloss = nn.BCELoss()\n# Number of steps to apply to the discriminator\nd_steps = 1 # In Goodfellow et. al 2014 this variable is assigned to 1\n# Number of epochs\nnum_epochs = 4001\npretrain_d = 500\nd_errors=[]\np=2\nq=1\npopulation = tdist.Gamma(p,q)\ndef population_sample(n):\ntset_seed(42)\nreturn population.sample(n)\nprint('Pretrain Discriminator')\nfor pre in range(pretrain_d):\nreal_data = population_sample((sample_size,))\npretrain_discriminator(d_optimizer,real_data)\npic=0\nfor epoch in range(num_epochs):\nreal_data = np.array(population.sample((sample_size,)).data.tolist())\nreal_data = torch.tensor(2*(real_data-real_data.min())/(real_data.max()-real_data.min())-1).type(torch.FloatTensor)\n# 1. Train Discriminator\n# Generate fake data\nfake_data = generator(noise(100)).detach()#detach means do not compute gradients\n# Train D\nd_error, d_pred_real, = train_discriminator(d_optimizer,real_data, fake_data)\nfake_data = generator(noise(100))\n# Train G\ng_error = train_generator(g_optimizer, fake_data)\n#\nd_errors.append(d_error.item())\nif epoch%100 == 0:\nprint('Epoch: {:5d} , D-Loss: {:.4f}, G-Loss: {:.4f}'.format(epoch, np.round(d_error.data,2),\nres_fake=generator(noise(100)).detach().data.tolist()\nres_real=population_sample((sample_size,)).data.tolist()\nres_fake=np.array(res_fake).flatten()\nres_real=2*(res_real-res_real.min())/(res_real.max()-res_real.min())-1\nplt.hist(res_real,bins=31,edgecolor='black',density=True,alpha=.2)\nplt.ylim(0,4)\nplt.title('GAN Gamma({},{}) Approximation: Epoch: {:4d}'.format(p,q,epoch))\nplt.savefig('GAN_Gamma_Hist_{}_{}_{}.png'.format(p,q,pic))\nplt.close()\npic+=1\ng_optimizer.param_groups[0][\"lr\"] = lr_decay(epoch)\ng_errors = pd.Series(g_errors)\ng_errors.rolling(10)\nd_errors = pd.Series(d_errors)\nd_errors.rolling(10)\nfig, ax = plt.subplots(2,figsize=(11,8))\nax[0].plot(g_errors,color='blue')\nax[0].legend('G_Error')\nax[0].set_xlabel('epoch')\nax[1].plot(d_errors,color='orange')\nax[1].legend('D_Error')\nax[1].set_xlabel('epoch')\n#fig.suptitle('Generator and Discriminator Error',fontsize=12)\nplt.tight_layout()\nplt.savefig('GD_Error.png')\nplt.show()\n", "output_sequence": "Training a GAN on dataset of 1000 samples of a Gamma distribution. Input of the generator is a std. normal distribution, the GAN is trained for 4000 epochs. Every 100 epochs histograms of generated and training data are plotted."}, {"input_sequence": "import matplotlib.pyplot as plt\nfrom scipy import stats\nimport numpy as np\nx = np.linspace(-4, 8, 2000)\ny = stats.gamma.pdf(x, a=2, loc=0, scale=1)\nplt.plot(x, y, label='Gamma(2,1)')\ny = stats.norm.pdf(x, 0)\nplt.plot(x, y, label='std. Gaussian')\nplt.ylabel('pdf')\nplt.savefig(fname='dist_gamma_gaussian.pdf', transparent=True)\n", "output_sequence": "Training a GAN on dataset of 1000 samples of a Gamma distribution. Input of the generator is a std. normal distribution, the GAN is trained for 4000 epochs. Every 100 epochs histograms of generated and training data are plotted."}, {"input_sequence": "\nfunction MSRpdf_cop_Frank\n[u,v] = meshgrid(0:0.02:1);\np = 2;\n\ngcopuly = -p.*(exp(-p.*(u+v+1)) - exp(-p.*(u+v)))./...\n((exp(-p) + exp(-p.*(u+v)) - exp(-p.*u) - exp(-p.*v)).^2);\ngrid on\nmesh(u, v, gcopuly)\nxlabel('X');\n", "output_sequence": "Produces simple plot of the Frank copula density with parameter p = 2."}, {"input_sequence": "rm(list = ls(all = TRUE))\n#setwd(\"C:/...\")\n\n#install.packages(\"lattice\")\nlibrary(lattice)\nu = seq(0, 1, by = 0.02)\nv = u\nm = length(u)\nU = matrix(rep(u, each = n), nrow = n)\np = 2\ngcopuly = -p*(exp(-p*(U+V+1)) - exp(-p*(U+V)))/ ((exp(-p) + exp(-p*(U+V)) - exp(-p*U) - exp(-p*V))^2)\ns = expand.grid(u = u, v = v)\nwireframe(gcopuly ~ u*v, s, shade = TRUE, xlab = \"X\", ylab = \"Y\", zlab = \"Z\",\n", "output_sequence": "Produces simple plot of the Frank copula density with parameter p = 2."}, {"input_sequence": "rm(list = ls(all = TRUE))\n#setwd(\"C:/...\")\n# input parameters\nnn = 18\na = 1366\nb = 150889\nseed = 1234567\nn = 10000\n# main computation\nyy = 1:nn\nyy[1] = seed\ni = 2\nwhile (i <= nn) {\nyy[i] = (a * yy[i - 1] + b)%%M\ni = i + 1\n}\nyy = yy/M\ny = y1[1:18]\ni = 19\nwhile (i <= n + 18) {\nzeta = y[i - 17] - y[i - 5]\nzeta = (zeta + 1) * (zeta < 0) + (zeta * (zeta >= 0))\ny[i] = zeta\ni = i + 1\ny = y[19:(n + 18)]\n# output\nplot(y[1:(n - 2)], y[2:(n - 1)], pch = 20, cex = 0.5,\n", "output_sequence": "Generates numbers coming from a uniform distribution. The function uses Fibonacci method."}, {"input_sequence": "% input parameters\nnn = 18;\na = 1366;\nb = 150889;\nseed = 1234567;\nn = 10000;\n\n% main computation\nyy(1) = seed;\ni = 2;\nwhile(i <= nn)\nyy(i) = mod((a*yy(i - 1) + b), M);\ni = i + 1;\nend\nyy = yy/M;\ny = y1(1:18);\ni = 19;\nwhile (i <= n + 18)\nzeta = y(i - 17) - y(i - 5);\nzeta = (zeta + 1).*(zeta < 0) + zeta.*(zeta >= 0);\ny(i) = zeta;\ny = y(19:n+18);\n% output\nscatter(y(1:n-2), y(2:n-1), 'b', '.')\nxlabel('U_i_-_1')\n", "output_sequence": "Generates numbers coming from a uniform distribution. The function uses Fibonacci method."}, {"input_sequence": "function [r,v] = MSRtcopulaparam(x,y)\nT = length(x);\nz = [x';y'];\nN = size(z,1);\nu = unif_var(z);\nRcml = zeros(N,N);\n\nfor i = 1:N-1\nfor j = i+1:N\nRcml(i,j) = corr(u(i,:)', u(j,:)', 'type', 'Kendall');\nRcml(j,i) = Rcml(i,j);\nend;\nRcml(i,i) = 1;\nend;\nRcml(N,N) = 1;\nRcml = sin(pi/2.*Rcml);\nr = Rcml(1,2);\nv = 3;\nloglike1 = log_dens_t(v, Rcml, u);\nv = v + 1;\nloglike2 = log_dens_t(v, Rcml, u);\nwhile loglike1 < loglike2\nv = v+1;\nloglike2 = log_dens_t(v, Rcml, u);\nif v == 30 break; end;\nv = v-1;\nfunction s = unif_var(x)\nN = size(x,1);\nfor i = 1:N\nfor j = 1:T\ndys(i,j) = sum(x(i,:)<= x(i,j));\ns = dys./(T+1);\nfunction s = log_dens_t(v,R,u)\nsi = size(u);\ny = tinv(u,v);\ng = T*log(gamma((v+n)/2)/gamma(v/2))-T*n*log(gamma((v+1)/2)/gamma(v/2))-T/2*log(abs(det(R)));\na = sum(log(1+y.^2./v));\nb = sum(a);\nfor i = 1:T\nc(i) = log(1+(y(:,i)'*inv(R)*y(:,i))./v);\nend\nc = sum(c);\n", "output_sequence": "Computes number of degrees of freedom and correlation parameter rho for t-Student copula."}, {"input_sequence": "clc;\nclose all\na = load('Bay9906_close_2kPoints.txt', '-ascii');\nd = a + b + c;\nx = log(d(1:end-1))-log(d(2:end));%negative log-returns\nn = 100;\nzb = sort(x);\ntheta = zb(end-n);\nz = zb(end-n+1:end)-theta;\nparams = gpfit(z);\nK = params(1)\nt = (1:n)/(n+1);\ny = gpinv(t, K, sigma);\nhold on\nplot(y, y, 'r', 'LineWidth', 2)\nscatter(z, y, '.')\ntitle('QQ plot, Generalized Pareto Distribution')\nhold off\n%---------------------------------------------------------------------\nfigure\ny = gpcdf(z, K, sigma);\nscatter(y, t, '.')\ntitle('PP plot, Generalized Pareto Distribution')\n", "output_sequence": "Estimates the parameters of Generalized Pareto Distribution for the negative log-returns of portfolio (Bayer, BMW, Siemens), time period: from 1992-01-01 to 2006-09-21, and produces QQ-plot and PP-plot."}, {"input_sequence": "rm(list = ls(all = TRUE))\n#setwd(\"C:/....\")\n#install.packages(\"QRM\")\nlibrary(QRM)\na = read.table('Bay9906_close_2kPoints.txt')\nd = a + b + c\nn1 = dim(d)[1]\nx = log(d[1:n1-1, ]/d[2:n1, ]) #negative log-returns\nn = 100\ngpd = fit.GPD(x, nextremes = n, information = \"observed\")\nt = (1:n)/(n+1)\ny1 = qGPD(t, gpd$par.ests[1],\n\ngpdt = sort(gpd$data) - gpd$threshold\ny2 = pGPD(gpdt, gpd$par.ests[1],\n\nplot(gpdt, y1, col = \"blue\", pch = 21, bg = \"blue\", ylim = c(0, 0.06), xlab = \"\",\nylab = \"\", main = \"QQ plot, Generalized Pareto Distribution\")\nlines(y1, y1, type = \"l\", col = \"red\", lwd = 2)\nplot(y2, t, col = \"blue\", pch = 21, bg = \"blue\", xlab = \"\", ylab = \"\",\nmain = \"PP plot, Generalized Pareto Distribution\")\nlines(y2, y2, type = \"l\", col = \"red\", lwd = 2)\n", "output_sequence": "Estimates the parameters of Generalized Pareto Distribution for the negative log-returns of portfolio (Bayer, BMW, Siemens), time period: from 1992-01-01 to 2006-09-21, and produces QQ-plot and PP-plot."}, {"input_sequence": "optimization = function(theta, kt, kt.reference) {\nt = time(kt)\nt.reference = time(kt.reference)\n\n### loss function\nloss = function(theta, t, kt, t.reference, {\n## assume theta[1]>0, if not, take absolute values\ntheta1 = abs(theta[1])\n\nif (theta[1] < 0 | theta[3] < 0)\nwarning(\"theta1 or theta3 <0, abs value is used\")\nsm.t = (t.reference - theta2)/theta3 # time adjustment\n## common domain for kt and time-adjusted kt.reference\ntmin = max(min(t), min(sm.t))\ni0 = which(t >= tmin & t <= tmax) # index for common domain\nif (length(i0) > 0) {\nt0 = t[i0]\n## smooth interpolation of shifted kt.reference on common grid\ndref = data.frame(sm.t = sm.t, kt.reference = kt.reference)\n# sm: sm.regression with optimal smoothing parameter\nh.optimal3 = h.select(sm.t, kt.reference)\nsm = sm.regression(sm.t, kt.reference, h = h.optimal3, eval.points = t0, model = \"none\", poly.index = 1, display = \"none\")\nmu = theta1 * sm$estimate\n## mean squared error at common grid points\nmse = mean((kt[i0] - mu)^2) # mse of kt and the modelled one\n} else {\nmse = 1e+09\n}\nreturn(mse)\n}\n### parameter estimation with nonlinear optimization\nconv = 1\ntheta0 = theta\nwhile (conv != 0) {\n# check convergence\nout = optim(theta0, loss, gr = NULL, t, kt, t.reference, control = list(maxit = 1000)) # optimization\nconv = out$convergence\ntheta0 = out$par\n### constraint on theta2 (time shift parameter)\nif (out$par[2] >= -200 & out$par[2] <= 200)\ntemp.par = out$par else temp.par = theta\nresult = c(temp.par, out$value, out$convergence)\n### check results on graph\ntheta = result[1:3]\ntheta1 = abs(theta[1])\nsm.t = (t.reference - theta2)/theta3 # time adjustment\n## common grid for kt and shifted kt.reference\ntmin = max(min(t), min(sm.t))\ni0 = which(t >= tmin & t <= tmax)\nt0 = t[i0]\n## shifted curves (kt.hat)\ndref = data.frame(sm.t = sm.t, kt.reference = kt.reference)\n# sm: sm.regression with optimal smoothing parameter\nh.optimal4 = h.select(sm.t, kt.reference)\nsm = sm.regression(sm.t, kt.reference, h = h.optimal4, eval.points = sm.t, model = \"none\", poly.index = 1, display = \"none\")\nmu = theta1 * sm$estimate\nmu = ts(mu, start = sm.t[1], frequency = theta3)\nh.optimal5 = h.select(sm.t, kt.reference)\nsm0 = sm.regression(sm.t, kt.reference, h = h.optimal5, eval.points = t0, model = \"none\", poly.index = 1, display = \"none\")\nmu0 = theta1 * sm0$estimate\nmu0 = ts(mu0, start = t0[1], frequency = 1)\nreturn(list(result, mu, tmin, tmax))\n}\n", "output_sequence": "Optimizes shape variation parameters theta based on kt and reference curve to update new kt, and will be called in MuPoMo_main_twopop and MuPoMo_main_multipop."}, {"input_sequence": "function IntFor = qra(y,X,tau)\n%QRA Perform Quantile Regression Averaging\n% QRA(Y,X,TAU) returns an interval forecast at confidence level\n% (1-tau)*100% obtained using QUANTILE REGRESSION AVERAGING (QRA) [1].\n% In the quantile regression model Y is an independent variable in and\n% variable of interest to have prediction intervals constructed, X is a\n% matrix of individual forecasts corresponding to values of Y, i.e. real\n% observation and their forecasts for a particular time have the same\n% indices. 1-TAU is the confidence level. The forecasts is computed for H\n% steps ahead, where H is difference between X's and Y's lengths (has to\n% be positive integer).\n%\n% [1] J. Nowotarski, R. Weron (2014) Computing electricity spot price\n% prediction intervals using quantile regression and forecast averaging,\n% Computational Statistics, forthcoming. Working paper version available\n% from: http://ideas.repec.org/p/wuu/wpaper/hsc1312.html.\n% Written by Jakub Nowotarski and Rafal Weron (2014.07.30)\n% Preliminaries\nN = size(X,2);\nT = length(y);\nif length(X)-T<1\nerror('X matrix must contain predictions of future y`s values.')\nend\n% lower bound\nw = quantreg(X(1:T,:),y,tau/2);\nif N>1\nIntFor(:,1) = X(T+1:end,:)*w;\nelse\nIntFor(:,1) = [ones(length(X)-T,1) X(T+1:end,:)]*w;\n% upper bound\nw = quantreg(X(1:T,:),y,1-tau/2);\nIntFor(:,2) = X(T+1:end,:)*w;\nIntFor(:,2) = [ones(length(X)-T,1) X(T+1:end,:)]*w;\nfunction [p,stats]=quantreg(x,y,tau,order,Nboot);\n% Quantile Regression\n%\n% USAGE: [p,stats]=quantreg(x,y,tau[,order,nboot]);\n% INPUTS:\n% x,y: data that is fitted. (x and y should be columns)\n% Note: that if x is a matrix with several columns then multiple\n% linear regression is used and the \"order\" argument is not used.\n% tau: quantile used in regression.\n% order: polynomial order. (default=1)\n% (negative orders are interpreted as zero intercept)\n% nboot: number of bootstrap surrogates used in statistical inference.(default=200)\n% stats is a structure with the following fields:\n% .pse: standard error on p. (not independent)\n% .pboot: the bootstrapped polynomial coefficients.\n% .yfitci: 95% confidence interval on \"polyval(p,x)\" or \"x*p\"\n% [If no output arguments are specified then code will attempt to make a default test figure\n% for convenience, which may not be appropriate for your data (especially if x is not sorted).]\n% Note: uses bootstrap on residuals for statistical inference. (see help bootstrp)\n% check also: http://www.econ.uiuc.edu/~roger/research/intro/rq.pdf\n% EXAMPLE:\n% x=(1:1000)';\n% y=randn(size(x)).*(1+x/300)+(x/300).^2;\n% [p,stats]=quantreg(x,y,.9,2);\n% plot(x,y,x,polyval(p,x),x,stats.yfitci,'k:')\n% legend('data','2nd order 90th percentile fit','95% confidence interval','location','best')\n% For references on the method check e.g. and refs therein:\n% http://www.econ.uiuc.edu/~roger/research/rq/QRJEP.pdf\n% Copyright (C) 2008, Aslak Grinsted\n% This software may be used, copied, or redistributed as long as it is not\n% sold and this copyright notice is reproduced on each copy made. This\n% routine is provided as is without any express or implied warranties\n% whatsoever.\nif nargin<3\nerror('Not enough input arguments.');\nif nargin<4, order=[]; end\nif (tau<=0)|(tau>=1),\nerror('the percentile (tau) must be between 0 and 1.')\nif size(x,1)~=size(y,1)\nerror('length of x and y must be the same.');\nif numel(y)~=size(y,1)\nerror('y must be a column vector.')\nif size(x,2)==1\nif isempty(order)\norder=1;\nend\n%Construct Vandermonde matrix.\nif order>0\nx(:,order+1)=1;\nelse\norder=abs(order);\nx(:,order)=x(:,1); %flipped LR instead of\nfor ii=order-1:-1:1\nx(:,ii)=x(:,order).*x(:,ii+1);\nelseif isempty(order)\norder=1; %used for default plotting\nerror('Can not use multi-column x and specify an order argument.');\npmean=x\\y; %Start with an OLS regression\nrho=@(r)sum(abs(r-(r<0).*r/tau));\np=fminsearch(@(p)rho(y-x*p),pmean);\nif nargout==0\n[xx,six]=sortrows(x(:,order));\nplot(xx,y(six),'.',x(six,order),x(six,:)*p,'r.-')\nlegend('data',sprintf('quantreg-fit ptile=%.0f%%',tau*100),'location','best')\nclear p\nreturn\nend\nif nargout>1\n%calculate confidence intervals using bootstrap on residuals\n\nyfit=x*p;\nresid=y-yfit;\nstats.pboot=bootstrp(Nboot,@(bootr)fminsearch(@(p)rho(yfit+bootr-x*p),p)', resid);\nstats.pse=std(stats.pboot);\nqq=zeros(size(x,1),Nboot);\nfor ii=1:Nboot\nqq(:,ii)=x*stats.pboot(ii,:)';\nstats.yfitci=prctile(qq',[2.5 97.5])';\n", "output_sequence": "Calculates a series of day ahead interval forecasts of electricity spot price. The QRA method uses a set of individual point forecasts and the output is a pair of quantile forecasts. Based on: J. Nowotarski, R. Weron (2015) Computing electricity spot price prediction intervals using quantile regression and forecast averaging, Computational Statistics 30(3), 791-803 (doi: 10.1007/s00180-014-0523-0). "}, {"input_sequence": "load indFor.mat\nalpha = 0.1; % tau=1-alpha is confidence level, forecast interval: [tau/2,1-tau/2]\nNdays = 7;\nstartd = 1; % beginnng of the calibration period\nfor d=1:Ndays\nfor h=1:24\nyh = log(data((d-1)*24+startd-1+h:24:(d-1)*24+endd,3));\n% QRA - [lower bound, upper bound]\nPI((d-1)*24+h,:) = exp(qra(yh,Xh,alpha));\nend\nend\n% real price during forecast period\ny = data(endd+1:endd+Ndays*24,3);\n% calculate coverage\nI = PI(:,1)<y & y<PI(:,2);\n", "output_sequence": "Calculates a series of day ahead interval forecasts of electricity spot price. The QRA method uses a set of individual point forecasts and the output is a pair of quantile forecasts. Based on: J. Nowotarski, R. Weron (2015) Computing electricity spot price prediction intervals using quantile regression and forecast averaging, Computational Statistics 30(3), 791-803 (doi: 10.1007/s00180-014-0523-0). "}, {"input_sequence": "import numpy as np\ndef calc_parameters(T, N, sigma, r, div):\n\"\"\"\nCalculates the dependent parameters of the Binomial Tree (CRR)\ninput:\nT : time to maturity\nN : number of steps of the tree\nsigma : volatility\nr : interest rate 0.05 = 5%\ndiv : cuntinuous dividend 0.03 = 3%\noutput:\ndt : size of time step\nu : factor of upwards movement of stock\nq : risk-neutral probability\nb : cost of carry\ndt = T/N\nu = np.exp(sigma*np.sqrt(dt))\nd = 1/u\nb = r-div\nq = 1/2 + 1/2 * (b - 1/2 * sigma**2)*np.sqrt(dt)/sigma # P(up movement)\nreturn(dt, u, d, q, b)\ndef set_factor(t,t_div,div):\nDetermines the factor of dividend with which the stock is multiplied\nt : current time\ndiv : discrete dividend 0.03 = 3%\nt_div : time point on which dividend is issued\noutput:\nfactor with which the stock price is multiplied\nif t>t_div :\nfactor = 1-div\nelif t<=t_div:\nfactor = 1\nreturn factor\ndef calc_price(S0, K, u, d, N, r, dt, q, disc_div, option):\nUses Backpropergation to calculate the option price of an European or\nAmerican option, saves the stock and option prices of the tree\nS0, K, u, d, N, r, dt, q: parameters of the Binomial Tree (CRR) Model\nas in function calc_parameters\ndisc_div : discrete dividend 0.03 = 3%\ndisc_div_t : time point when dividend is issued\noption : 'Call', 'Put'\nasset_values : The asset values from t=T to t=0\ntime_idx_values : Time step indices to the values above\nprice : price of the option\n# calculate the values at maturity T\n# factor: 1-div for t > t_div, else: 1\nfactor = set_factor(T, disc_div_t,\nasset_values = factor * S0*(u**np.arange(N,-1,-1))*(d**np.arange(0,N+1,1))\nif option == 'Call':\noption_values = (np.maximum((asset_values-K),0)).tolist()\nelif option == 'Put':\noption_values = (np.maximum((K-asset_values),0)).tolist()\nasset_values = asset_values.tolist()\n#Using the recursion formula for pricing in the CRR model:\nfor n in np.arange(N-1,-1,-1): # from (T-dt, T-2*dt, ...., dt, 0)\nfactor = set_factor(n*dt, disc_div_t,\nasset_val_temp = factor*(S0*(u**np.arange(n,-1,-1))*\n(d**np.arange(0,n+1,1)))\noption_val_temp = (np.exp(-1*r*dt)\n* (q*np.array(option_values[-(n+2):-1])\nasset_values += asset_val_temp.tolist()\n\nprice = option_values[-1]\nreturn asset_values, option_values, price\n####### MAIN ################\nS0 = 230 # current stock price\nT = 0.50 # time to maturity\nsigma = 0.25 # volatility\nr = 0.04545 # interest rate\ndiv = 0 # continuous dividend\ndisc_div, = (0.01, 0.15) # (div, t). after t: S(t)=(1-div_1)*S(t)\nN = 5 # steps in tree\noption = 'Call'\n# calculate price\ndt, u, d, q, b = calc_parameters(T, N, sigma, r, div)\nasset_values, option_values, price = calc_price(S0, K, u, d, N,\nr, dt, q,\nprint(price)\n", "output_sequence": "Computes European/American option prices using a binomial tree for assets with dividends as a percentage of the stock price amount."}, {"input_sequence": "# clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# input parameters\nprint(\"Please input Price of Underlying Asset s0, Exercise Price k, Domestic Interest Rate per Year i,\")\nprint(\"Volatility per Year sig, Time to Expiration (Years) t, Number of steps n, type\")\nprint(\"yields vector [s0, k, i, sig, t, n, type]=\")\nprint(\"after 1. input the following: 230 0.04545 0.25 5 1\")\nprint(\"then press enter two times\")\npara = scan()\nwhile (length(para) < 7) {\nprint(\"Not enough input arguments. Please input in 1*7 vector form like 230 0.04545 0.25 5 1\")\nprint(\" \")\nprint(\"[s0, k, i, sig, t, n, type]=\")\npara = scan()\n}\ns0 = para[1] # Stock price\ni = para[3] # Rate of interest\nsig = para[4] # Volatility\nt = para[5] # Time to expiration\nn = para[6] # Number of intervals\ntype = para[7] # 0 is American/1 is European\n# Check conditions\nif (s0 <= 0) {\nprint(\"SFEBiTree: Price of Underlying Asset should be positive! Please input again. s0=\")\ns0 = scan()\nif (k < 0) {\nprint(\"SFEBiTree: Exercise price couldnot be negative! Please input again. k=\")\nk = scan()\nif (sig < 0) {\nprint(\"SFEBiTree: Volatility should be positive! Please input again. sig=\")\nsig = scan()\nif (t <= 0) {\nprint(\"SFEBiTree: Time to expiration should be positive! Please input again. t=\")\nt = scan()\nif (n < 1) {\nprint(\"SFEBiTree: Number of steps should be at least equal to 1! Please input again. n=\")\nn = scan()\nprint(\" \")\nprint(\"Please input option choice (1 for call, 0 for put) flag, Number of pay outs nodiv, time point of dividend payoff tdiv\")\nprint(\"dividend rate in percentage of stock price as: [1 1 0.15 0.01]\")\nprint(\"[flag, nodiv tdiv ]=\")\npara2 = scan()\nwhile (length(para2) < (2 * para2[2] + 2)) {\nprint(\"Not enough input arguments. Please input in 1*(2+2*nodiv) vector form like [1 1 0.15 0.01]\")\nprint(\"[flag nodiv tdiv ]=\")\npara2 = scan()\nflag = para2[1] # 1 for call, 2 for put option choice\nnodiv = para2[2] # Times of dividend payoff\npdiv = para2[(nodiv + 3):(nodiv + nodiv + 2)] # Dividend in currency units\nif (t < max(tdiv)) {\nprint(\"SFEBiTree: Payoff shall happend before expiration! Please input tdiv again as [0.25 0.5]. tdiv=\")\ntdiv = scan()\nif (sum(pdiv) < 0) {\nprint(\"SFEBiTree: Dividend must be nonnegative! Please input pdiv again as [1 1]. pdiv=\")\npdiv = scan()\n# Main computation\ndt = t/n # Interval of step\nu = exp(sig * sqrt(dt)) # Up movement parameter u\nb = i # Costs of carry\np = 0.5 + 0.5 * (b - sig^2/2) * sqrt(dt)/sig # Probability of up movement\ntdivn = floor(tdiv/t * n - 1e-04) + 1\ns = matrix(1, n + 1, n + 1) * s0\nun = rep(1, n + 1) - 1\nun[n + 1] = 1\ndm = t(un)\num = matrix(0, 0, n + 1)\nj = 1\nwhile (j < n + 1) {\nd1 = cbind(t(rep(1, n - j) - 1), t((rep(1, j + 1) * d)^(seq(1, j + 1) - 1)))\ndm = rbind(dm, d1) # Down movement dynamics\nu1 = cbind(t(rep(1, n - j) - 1), t((rep(1, j + 1) * u)^((seq(j, 0)))))\num = rbind(um, u1) # Up movement dynamics\nj = j + 1\num = t(rbind(un, um))\ndm = t(dm)\ns = s[1, 1] * um * dm # stock price development\nj = 1\nwhile (j <= nodiv) {\ns[, (tdivn[j] + 1):(n + 1)] = s[, (tdivn[j] + 1):(n + 1)] * (1 - pdiv[j])\nj = j + 1\nStock_Price = s\ns = s[nrow(s):1, ]\nprint(Stock_Price) # Rearangement\nopt = matrix(0, nrow(s),\n# Option is an American call\nif ((flag == 1) && (type == 0)) {\nopt[, n + 1] = pmax(s[, n + 1] - k, 0) # Determine option values from prices\nloopv = seq(n, 1)\nfor (j in loopv) {\nl = seq(1, j)\n# Probable option values discounted back one time step\ndiscopt = ((1 - p) * opt[l, j + 1] + p * opt[l + 1, j + 1]) * exp(-b *\ndt)\n# Option value is max of current price - X or discopt\nopt[, j] = rbind(t(t(pmax(s[l, j] - k, discopt))), t(t(rep(0, n + 1 - j))))\n}\nAmerican_Call_Price = opt[nrow(opt):1, ]\nprint(American_Call_Price)\nprint(\"The price of the option at time t_0 is\")\nprint(American_Call_Price[n + 1, 1])\nif ((flag == 1) && (type == 1)) {\n# Option is a European call\n# Option value\nopt[, j] = rbind(t(t(discopt)), t(t(rep(0, n + 1 - j))))\n}\nEuropean_Call_Price = opt[nrow(opt):1, ]\nprint(European_Call_Price)\nif ((flag == 0) && (type == 0)) {\n# Option is an American put\nopt[, n + 1] = pmax(k - s[, n + 1], 0) # Determine option values from prices\n# Option value is max of X - current price or discopt\nopt[, j] = rbind(t(t(pmax(k - s[l, j], discopt))), t(t(rep(0, n + 1 - j))))\nAmerican_Put_Price = opt[nrow(opt):1, ]\nprint(American_Put_Price)\nif ((flag == 0) && (type == 1)) {\n# Option is a European put\nEuropean_Put_Price = opt[nrow(opt):1, ]\nprint(European_Put_Price)\n", "output_sequence": "Computes European/American option prices using a binomial tree for assets with dividends as a percentage of the stock price amount."}, {"input_sequence": "clear;\nclc;\nclose all;\n%% User inputs parameters\ndisp('Please input Price of Underlying Asset s0, Exercise Price k, Domestic Interest Rate per Year i');\ndisp('Volatility per Year sig, Time to Expiration (Years) t, Number of steps n, type');\ndisp('as: [230, 210, 0.04545, 0.25, 5, 1] or [1.50, 0.08, 0.2, 0.33, 6, 1]');\ndisp(' ');\npara = input('[s0, k, i, sig, t, n, type]=');\nwhile length(para) < 7\ndisp('Not enough input arguments. Please input in 1*7 vector form like [230, 210, 0.04545, 0.25, 5 ,1] or [1.50, 0.08, 0.2, 0.33, 6, 1]');\ndisp(' ');\npara = input('[s0, k, i, sig, t, n, type]=');\nend\ns0 = para(1); % Stock price\nn = para(6); % Number of intervals\ntype = para(7); % 0 is American/1 is European\n%Check conditions\nif s0<=0\ndisp('SFEBiTree: Price of Underlying Asset should be positive! Please input again')\ns0=input('s0=');\nif k<0\ndisp('SFEBiTree: Exercise price couldnot be negative! Please input again')\nk=input('k=');\nif sig<0\ndisp('SFEBiTree: Volatility should be positive! Please input again')\nsig=input('sig=');\nif t<=0\ndisp('SFEBiTree: Time to expiration should be positive! Please input again')\nt=input('t=');\nif n<1\ndisp('SFEBiTree: Number of steps should be at least equal to 1! Please input again')\nn=input('n=');\ndisp('Please input option choice, Number of pay outs, time point of dividend payoff, ');\ndisp('dividend rate in percentage of stock price as: [1, 0.15, 0.01]');\npara2 = input('[flag, nodiv tdiv ]=');\nwhile length(para2) < (2*para2(2)+2)\ndisp('Not enough input arguments. Please input in 1*(2+2*nodiv) vector form like [1, 0.15, 0.01]');\npara2=input('[flag, nodiv, pdiv ]=');\nflag = para2(1); % 1 for call, 2 for put option choice\nnodiv = para2(2); % Times of dividend payoff\npdiv = para2((nodiv+3):(nodiv+nodiv+2)); % Dividends as a percentage of the stock price amount\nif t<max(tdiv)\ndisp('SFEBiTree: Payoff shall happend before expiration! Please input tdiv again as (1*ndiv) vector < t')\ntdiv = input('tdiv=');\nif pdiv<0\ndisp('SFEBiTree: Dividend must be nonnegative! Please input pdiv again as (1*ndiv) vector')\npdiv = input('pdiv=');\n%% Main computation\n\ndt = t/n; % Interval of step\nu = exp(sig*sqrt(dt)); % Up movement parameter u\nb = i; % Costs of carry\np = 0.5+0.5*(b-sig^2/2)*sqrt(dt)/sig; % Probability of up movement\ntdivn = floor(tdiv/t*n-0.0001)+1;\ns = ones(n+1,n+1)*s0;\nun(n+1,1) = 1;\ndm = un';\nwhile j<n+1\nd1 = [ones(1,n-j)-1 (ones(1,j+1)*d).^((1:j+1)-1)];\ndm = [dm; d1]; % Down movement dynamics\nu1 = [ones(1,n-j)-1 (ones(1,j+1)*u).^((j:-1:0))];\num = [um; u1]; % Up movement dynamics\nj = j+1;\num = [un';um]';\ndm = dm';\ns = s(1,1).*um.*dm; % Stock price development\nj = 1;\nwhile (j<=nodiv)\ns(:,(tdivn(j)+1):(n+1))=s(:,(tdivn(j)+1):(n+1))*(1-pdiv(j));\nj = j+1;\nStock_Price = s;\ndisp('Stock_Price')\ndisp(s)\ns = flipud(s); % Rearrangement\nopt = zeros(size(s));\n%% Option is a american call\nif flag == 1 & type==0\nopt(:,n+1) = max(s(:,n+1)-k,0); % Determine option values from prices\nfor j = n:-1:1;\nl = 1:j;\n% Probable option values discounted back one time step\ndiscopt = ((1-p)*opt(l,j+1)+p*opt(l+1,j+1))*exp(-b*dt);\n% Option value is max of current price - X or discopt\nopt(:,j) = [max(s(1:j,j)-k,discopt);zeros(n+1-j,1)];\nend\nAmerican_Call_Price = flipud(opt)\ndisp('The price of the option at time t_0 is')\ndisp(American_Call_Price(n+1,1))\n\n%% Option is a european call\nelseif flag == 1 & type==1\nopt(:,n+1) = max(s(:,n+1)-k,0); % Determine option values from prices\n% Option value\nopt(:,j) = [discopt;zeros(n+1-j,1)];\nEuropean_Call_Price = flipud(opt)\ndisp(' ') ;\ndisp(European_Call_Price(n+1,1))\n%% Option is an american put\nelseif flag == 0 & type==0\nopt(:,n+1) = max(k-s(:,n+1),0); % Determine option values from prices\nfor j = n:-1:1\n% Option value is max of X - current price or discopt\nopt(:,j) = [max(k-s(1:j,j),discopt);zeros(n+1-j,1)];\nAmerican_Put_Price = flipud(opt)\ndisp(American_Put_Price(n+1,1))\n%% Option is a european put\nelseif flag == 0 & type==1\nEuropean_Put_Price = flipud(opt)\ndisp(European_Put_Price(n+1,1))\n", "output_sequence": "Computes European/American option prices using a binomial tree for assets with dividends as a percentage of the stock price amount."}, {"input_sequence": "# clear all variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# set working directory\n# setwd(\"C:/...\")\n# install and load packages\nlibraries = c(\"ismev\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# data import\nData = read.csv(\"2004-2014_dax_ftse.csv\")\n# Date variable as variable of class Date\nData$Date = as.Date(Data$Date, \"%Y-%m-%d\")\nh = 250 # size of moving window\nx = Data$BAYER + Data$BMW + Data$SIEMENS + Data$VOLKSWAGEN\nx = diff(x) # returns\nminus_x = -x\np = 0.95 # quantile for the Value at Risk\nq = 0.1\nObs = length(x)\n# function ----\nvar_pot = function(y,h,p,q){\n# Inputs:\n# y - vector of returns\n# p - quantile for at which Value at Risk should be estimated\n# h - size of the window\n# q - scalar, e.g. 0.1\nN = floor(h*q)\nys = sort(y,decreasing = TRUE)\nu = ys[N+1]\nz = y[y>u]-u\nparams = gpd.fit(z, threshold = 1 - p)\nksi = params$mle[2]\nvar = u + beta/ksi*((h/N*(1-p))^(-ksi)-1)\nout = c(var=var,ksi=ksi,beta=beta,u=u)\n}\n# Value at Risk ----\n# preallocation\nresults = data.frame(var=rep(NaN,Obs-h), ksi=rep(NaN,Obs-h),\nbeta=rep(NaN,Obs-h),\nfor(i in 1:(Obs-h)){\ny = minus_x[i:(i+h-1)]\nresults[i,] = var_pot(y,h,p,q)\n# Plot the shape, scale and treshold parameter.\nylim = c(min(min(results[, -1]))-1, max(max(results[, -1]))+1)\nwindows()\nplot(Data$Date[(h+2):length(Data$Date)],results$beta, xlab = \"\",\nylab = \"\", col = \"blue\", type = \"l\", lwd = 2, ylim = ylim)\nlines(Data$Date[(h+2):length(Data$Date)],results$ksi, col = \"red\", lwd = 2)\nlegend(\"topleft\", c(\"Scale Parameter\", \"Shape Parameter\", \"Threshold\"),\nlwd = c(2, 2, 2), col = c(\"blue\", \"red\", \"magenta\"))\ntitle(\"Parameters in Peaks Over Threshold Model\")\n", "output_sequence": "Plots the parameters estimated for calculating Value-at-Risk with Peaks Over Treshold model. These parameters were estimated with a moving window of size 250 for the portfolio composed by Bayer, BMW, siemens and Volkswagen."}, {"input_sequence": "%% clear all variables and console\nclear\nclc\n%% close windows\nclose all\n%% data import\nformatSpec = '%{yyyy-MM-dd}D%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f';\nData = readtable('2004-2014_dax_ftse.csv','Delimiter',',', 'Format',formatSpec);\nData = Data(:,{'Date','BAYER','BMW', 'SIEMENS', 'VOLKSWAGEN'});\nh = 250; % size of moving window\nx = Data.BAYER + Data.BMW + Data.SIEMENS + Data.VOLKSWAGEN;\nx = diff(x); % returns\nminus_x = -x;\np = 0.95; % quantile for the Value at Risk\nq = 0.1;\nObs = length(x);\n%% Value at Risk\n% preallocation\nvar = NaN(1,Obs-h);\nksi = var;\nfor i=1:(Obs-h)\ny = minus_x(i:(i+h-1));\n[var(i),ksi(i),beta(i),u(i)] = var_pot(y,h,p,q);\nend\n%% plot\nplot(Data.Date(h+2:end),beta)\nhold on\nplot(Data.Date(h+2:end),ksi,'Color','red')\nlegend('Scale Parameter','Shape Parameter','Threshold','FontSize',16,'FontWeight','Bold','Location','NorthWest')\ntitle('Parameters in Peaks Over Threshold Model','FontSize',16,'FontWeight','Bold')\n", "output_sequence": "Plots the parameters estimated for calculating Value-at-Risk with Peaks Over Treshold model. These parameters were estimated with a moving window of size 250 for the portfolio composed by Bayer, BMW, siemens and Volkswagen."}, {"input_sequence": "# clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# simulation routine\n# simGBM returns a vector of a sample trajectory of Geometric Brownian Motion on the time interval [0,N]\nsimGBM = function(n, x0, mu, sigma, delta, no, method) {\n# Set default value\nif (missing(method)) {\nmethod = 0 # direct integration\n}\n# Generate normal random numbers if not provided in 'no'\nif (missing(no)) {\nno = rnorm(ceiling(n/delta), 0, 1)\n# Check whether length of 'no' is appropriate\n} else {\nno = no\nif (length(no) != ceiling(n/delta)) {\nstop(\"Error: length(no) <> n/delta\")\n}\nif (method == 1) {\n# Euler scheme\nx = x0 * cumprod(1 + mu * delta + sigma * delta^0.5 * no)\n} else if (method == 2) {\n# Milstein scheme\nx = x0 * cumprod(1 + mu * delta + sigma * delta^0.5 * no + 0.5 * sigma^2 *\ndelta * (no^2 - 1))\n} else if (method == 3) {\n# 2nd order Milstein scheme\ndelta * (no^2 - 1) + mu * sigma * no * (delta^1.5) + 0.5 * (mu^2) * (delta^2))\n# Direct integration\nx = x0 * exp(cumsum((mu - 0.5 * sigma^2) * delta + sigma * delta^0.5 * no))\n# Add starting value\nx = c(x0, x)\n}\nset.seed(100)\n# parameter settings\nstep = 100 # number of steps of the motion\nk = 5 # number of trajectories\nmu = 0.84\nsig = 0.02\npath = matrix(0, step + 1, k)\nfor (i in 1:k) {\npath[, i] = simGBM(1, mu, sig, sqrt(0.1), 1/100, rnorm(step, 0, 1),\n# plot\nplot(path[, 1], col = 1, lwd = 2, ylab = \"\", xlab = \"\", type = \"l\", ylim = c(min(path),\nmax(path)), sub = paste(\"drift =\", mu, \", volatility =\", sig))\ntitle(paste(\"Simulation of Geometric Brownian Motion\"))\nfor (i in 2:k) {\nlines(path[, i], col = i, lwd = 2)\n}\n", "output_sequence": "Simulates and plots a path of a geometric Brownian motion (GBM) using 4 different methods: direct integration, Euler scheme, Milstein scheme, second order Milstein scheme."}, {"input_sequence": "# clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# install and load packages\nlibraries = c(\"copula\", \"fGarch\", \"fBasics\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# Main computation\ndataset = read.table(\"GumHAC_VaR_PL_w250_n1000_s2500.txt\", skip = 1)\nbHAC = T\nquantile.one = 0.05\nquantiles.points = c(5, 1, 0.5, 0.1)/100\nPL = as.vector(dataset[, dim(dataset)[2] - 1])\ndataset.date.ind = which(c(1, diff(dataset[, dim(dataset)[2]])) == 1)\nfirst.date = dataset[, dim(dataset)[2]][1]\ndataset.date.labels = first.date:(first.date + length(dataset.date.ind) - 1)\nq = 1\n# copula.name = 'HAC Gumbel' # uncomment this line and comment next line to plot VaR - HAC Gumbel Copula\ncopula.name = \"Gumbel 3D\" # plot VaR - Gumbel 3D Copula\nVaR.v = as.vector(dataset[, q])\nplot(VaR.v, col = \"yellow\", type = \"l\", ylim = c(min(dataset[, c(q, dim(dataset)[2] -\n1)]), max(dataset[, c(q, dim(dataset)[2] - 1)])), lwd = 2, xlab = \"time\", ylab = \"P&L\",\nmain = paste(\"VaR -\", copula.name, \"Copula\"), axes = F, frame = T, cex.main = 2,\ncex.lab = 1.5)\nlines(as.vector(dataset[, 2]), col = \"green3\")\nif (length(PL[PL > VaR.v]) != 0) points(which(PL == pmax(PL, VaR.v)), PL[PL > VaR.v],\ncol = \"black\", pch = 19, cex = 0.5)\nif (length(PL[PL < VaR.v]) != 0) text(which(PL == pmin(PL, VaR.v)), PL[PL < VaR.v],\n\"+\", col = \"red3\", pch = 19, cex = 1.5)\naxis(1, dataset.date.ind, cex.axis = 1.5)\ny.labels = c(round(seq(min(dataset[, c(q, dim(dataset)[2] - 1)]), max(dataset[, c(q,\ndim(dataset)[2] - 1)]), length = 7) * 100)/100, 0)\naxis(2, y.labels, cex.axis = 1.5)\n", "output_sequence": "According to the VaR methodology the profit and loss (P&L) is plotted against the time. The dots represent the empirical P&L stated by the data. The four curves under the dots correspond to the lower 5% (yellow), 1% (green), 0.5% (blue) and 0.1% (dark yellow) quantiles of the a estimated Gumbel-copula."}, {"input_sequence": "rm(list = ls(all = TRUE))\ngraphics.off()\n# install and load packages\nlibraries = c(\"TeachingDemos\", \"lattice\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# parameter settings\nn = 10000 # sample size\na = 2^16 + 3\nseed = 1298324 # makes sure that the same numbers are generated each time the quantlet is executed\ny = NULL\n# main computation\ny[1] = seed\ni = 2\nwhile (i <= n) {\ny[i] = (a * y[i - 1])%%M\ni = i + 1\n}\ny = y/M\n# output\nrotate.cloud(y[3:n] ~ y[1:(n - 2)] + y[2:(n - 1)], xlab = \"\", ylab = \"\", zlab = \"\",\ntype = \"p\", scales = list(arrows = FALSE, col = \"black\", distance = 1,\ntick.number = 8, cex = 0.7, x = list(labels = round(seq(0, 1, length = 10),\n1)), y = list(labels = round(seq(0, 1, length = 10), z = list(labels =\n", "output_sequence": "Produces 3d plot of hyperplanes of pseudo-random uniform numbers for given values of sample size and seed"}, {"input_sequence": "clear\nclc\nclose all\n% Input parameters\nn = 10000; % sample size\nseed = 1298324; % makes sure that the same numbers are generated each time\n% the quantlet is executed\n% Main computation\ny(1) = seed;\na = 2^16 + 3;\nM = 2^31;\nwhile(i<=n)\ny(i) = mod((a*y(i-1)),M);\ni = i+1;\nend\ny = y/M;\n% Output\n", "output_sequence": "Produces 3d plot of hyperplanes of pseudo-random uniform numbers for given values of sample size and seed"}, {"input_sequence": "# clear all variables\nrm(list=ls(all=TRUE))\n# set working directory\n#setwd(\"C:/...\")\n# install and load packages\nlibraries = c(\"data.table\", \"tseries\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\ninstall.packages(\"POT\")\nlibrary(POT)\n# load the data\ndataset = fread(\"2004-2014_dax_ftse.csv\", select = c(\"Date\", \"BAYER\", \"BMW\", \"SIEMENS\", \"VOLKSWAGEN\"))\ndataset = as.data.frame(dataset)\n# Portfolio\nd = dataset$BAYER + dataset$BMW + dataset$SIEMENS + dataset$VOLKSWAGEN\nn1 = length(d) # length of portfolio\nx = log(d[1:n1-1]/d[2:n1]) #negative log-returns\ngpd = fitgpd(x,quantile(x, 0.95),est=\"mle\") #\nn = gpd$nat\nthr = gpd$threshold\nscale= gpd$param[1]\ndata = gpd$data\nexc = gpd$exceed\nt = (1:n)/(n+1)\ny1 = qgpd(t,scale=scale,shape=shape)\n\ngpdt = sort(exc)-thr\ny2 = pgpd(gpdt,scale=scale,shape=shape)\nplot(y2,t,col=\"blue\",pch=15,bg=\"blue\",xlab=\"\",ylab=\"\",main=\"PP plot, Generalized Pareto Distribution\")\n", "output_sequence": "SFEtailGPareto_pp estimates the parameters of Generalized Pareto Distribution for the negative log-returns of portfolio (Bayer, BMW, Siemens)"}, {"input_sequence": "%% clear all variables\nclc\nclose all\n\n%% data import\nformatSpec = '%{yyyy-MM-dd}D%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f';\ndataset = readtable('2004-2014_dax_ftse.csv','Delimiter',',', 'Format',formatSpec);\n%% Portfolio\nd = dataset.BAYER + dataset.BMW + dataset.SIEMENS + dataset.VOLKSWAGEN;\nx = log(d(1:end-1))-log(d(2:end));%negative log-returns\nn = 100;\nzb = sort(x);\n%% estimates of the parameters for the two-parameter generalized Pareto\ntheta = zb(end-n);\nz = zb(end-n+1:end)-theta;\nparams = gpfit(z);\nK = params(1);\nt = (1:n)/(n+1);\n%% plot\nfigure\ny = gpcdf(z,K,sigma);\nhold on\nplot(y,y,'r','LineWidth',2)\nscatter(y,t,'o','filled','LineWidth',3)\nt = [0:0.2:1];\nset(gca,'YTick',t)\ntitle('PP plot, Generalized Pareto Distribution','FontSize',16,'FontWeight','Bold')\nhold off\nbox on\nset(gca,'FontSize',16,'LineWidth',1.6,'FontWeight','bold');\n", "output_sequence": "SFEtailGPareto_pp estimates the parameters of Generalized Pareto Distribution for the negative log-returns of portfolio (Bayer, BMW, Siemens)"}, {"input_sequence": "# clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# install and load packages\nlibraries = c(\"fGarch\", \"tseries\", \"pracma\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# load data\nDS = read.table(\"FSE_LSE.dat\")\nD = DS[, 1] # date\nend = nrow(s)\nr = s[-1, ] - s[1:(end - 1), ] # r(t)\nn = nrow(r) # sample size\nrdax = r[, 1] # DAX returns\n# Nonparametric volatility estimation, DAX\nend1 = nrow(r)\ny = cbind(r[1:(end1 - 1), r[-1, 1])\nyy = cbind(y[, 1], y[, 2]^2)\nhm = 0.04 # bandwidth\nX = y[, 1]\np = 1\n# compute quadratik kernel\nquadk = function(x) {\n# Usage : y = quadk(x); Returns: y = (15/16).*(x.*x.<1).*(1- x.*x).^2 ) ;\nI = as.numeric(x * x < 1)\nx = x * I\ny = (15/16) * I * (1 - x * x)^2\nreturn(y)\n}\nlpregest = function(X, Y, p, h) {\nn = length(X)\nx = seq(min(X), max(X), - min(X))/(100)) # grid\nm = length(x)\nbhat = matrix(0, p + 1, m)\nfor (i in 1:m) {\ndm = matrix(1, n, 1)\ndd = NULL\nxx = X - x[i]\nif (p > 0) {\nfor (j in 1:p) {\ndm = cbind(dm, (xx)^j)\n}\nw = diag(quadk(xx/h)/h)\nmh = solve(t(dm) %*% w %*% dm) %*% t(dm) %*% w\nbhat[, i] = mh %*% Y\n}\nreturn(list(bhat, x))\nfirst = lpregest(y[, 1], y[, 2], 1, hm) # estimate conditional mean function\nm1h = first[[1]]\nsecond = lpregest(yy[, 1], yy[, 2], 1, hs) # estimate conditional second moment\nm2h = second[[1]]\nsh = rbind(yg, m2h[1, ] - m1h[1, ]^2) # conditional variance\nm1hx = interp1(yg, m1h[1, ], y[, 1]) # interpolate mean\n# plot\npar(mfrow = c(2, 1))\nplot(shx_DAX^0.5, type = \"l\", col = \"blue3\", lwd = 0.7, xlab = \"\", ylab = \"\", frame = T,\naxes = F, ylim = c(0.01, 0.033))\naxis(1, seq(0, 3000, 500), seq(0, 3000, 500))\ntitle(\"DAX\")\n# Nonparametric volatility estimation, FTSE 100\ny = cbind(r[1:(end1 - 1), 22], r[2:end1, 22])\nyy = cbind(y[, 1], y[, 2]^2)\nhm = 0.04 # bandwidth\nfirst = lpregest(y[, 1], y[, 2], 1, hm) # estimate conditional mean function\nm1h = first[[1]]\nsecond = lpregest(yy[, 1], yy[, 2], 1, hs) # estimate conditional second moment\nm2h = second[[1]]\nsh = rbind(yg, m2h[1, ] - m1h[1, ]^2) # conditional variance\nm1hx = interp1(yg, m1h[1, ], y[, 1]) # interpolate mean\nplot(shx_FTSE^0.5, type = \"l\", col = \"blue3\", lwd = 0.7, xlab = \"Time, t\", ylab = \"\",\nframe = T, axes = F, ylim = c(0.01, 0.033))\ntitle(\"FTSE 100\")\n", "output_sequence": "Reads the date, DAX index values, stock prices of 20 largest companies at Frankfurt Stock Exchange (FSE), FTSE 100 index values and stock prices of 20 largest companies at London Stock Exchange (LSE) and estimates the volatility of the DAX and FTSE 100 daily return processes from 1998 to 2007."}, {"input_sequence": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom arch import arch_model\nfrom arch.univariate import GARCH\nfrom statsmodels.tsa.arima_model import ARMA\nimport datetime\nnp.set_printoptions(suppress=True)\nds = pd.read_table(\"FSE_LSE.dat\",header=None)\ndef log_returns(df):\nlogs=np.log((df.pct_change()+1).dropna())\nlogs=pd.DataFrame(logs)\nreturn(logs)\nS = ds.iloc[:,1:42] #stocks\nr = log_returns(S) #log returns\nD = ds.iloc[:,0] #date\nn = r.shape[0] #observations\nt = np.arange(0,n) #time steps\nrdax = r.iloc[:,1] #dax returns\ny1 = r.iloc[np.arange(r.shape[0]-1),0].values.reshape((-1,1))\ny = np.concatenate([y1,y2],axis=1)\nyy = np.concatenate([y[:,0].reshape((-1,1)),(y[:,1]**2).reshape((-1,1))],axis=1)\nhm = 0.04 # bandwidth\nX = y[:,0]\np = 1\ndef quadk(x):#compute quadratic kernel\nI =np.array([(x*x)<1]).astype(int)#true:1, false:0\nx=x*I\ny=I*((1-x*x)**2)*(15)/(16)\nreturn(y)\ndef lpregest(X,Y,p,h):\n\nn = X.shape[0]\nx = np.arange(np.min(X),np.max(X),(np.max(X)-np.min(X))/(100))\nm = x.shape[0]\nbhat = []\nfor i in range(m):\ndm = np.ones((n,1))\nxx = X-x[i]\n\nif p>0:\nfor j in range(1,p+1):\ndm=np.concatenate([dm,np.array((xx)**j).reshape((-1,1))],axis=1).astype(float)\n\nval = np.array([quadk(arg)/h for arg in xx/h])\nw = np.zeros((val.shape[0],val.shape[0]))\nnp.fill_diagonal(w,val)\nmh = np.linalg.solve(np.transpose(dm).dot(w).dot(dm),np.identity(np.transpose(dm).shape[0])).dot(np.transpose(dm)).dot(w)\nbhat.append(mh.dot(Y))\nreturn(np.array(bhat),np.array(x))\n#estimate conditional first moment\nm1h,yg = lpregest(y[:,0],y[:,1],1,hm)\n#estimate conditional second moment\nm2h,yg = lpregest(yy[:,0], yy[:,1], 1, hs)\n#conditional variance\nsh = np.concatenate([np.transpose(yg).reshape((1,-1)),np.transpose(m2h[:,0]-m1h[:,0]**2).reshape((1,-1))],axis=0)\n#interpolation\nm1hx = np.interp(y[:,1],yg,m1h[:,0])\n############################# FTSE ########################\ny1 = r.iloc[np.arange(r.shape[0]-1),21].values.reshape((-1,1))\nshx_FTSE = np.interp(y[:,1],yg,sh[1,:])\nfig = plt.figure(figsize=(14,6))\nax = fig.add_subplot(2, 1, 1)\nax.plot(shx_DAX**.5,label='DAX 30')\nax.set_xlabel('t')\nax.title\nplt.legend()\nax = fig.add_subplot(2, 1, 2)\nax.plot(shx_FTSE**.5,label='FTSE 100')\nplt.suptitle('Nonparametric volatility estimation - DAX 30 and FTSE 100')\nplt.savefig('SFEvolnoparest_py.png')\n", "output_sequence": "Reads the date, DAX index values, stock prices of 20 largest companies at Frankfurt Stock Exchange (FSE), FTSE 100 index values and stock prices of 20 largest companies at London Stock Exchange (LSE) and estimates the volatility of the DAX and FTSE 100 daily return processes from 1998 to 2007."}, {"input_sequence": "function [bhat x]=lpregest(X, Y, p, h)\nn = max(size(X));\nx = (min(X) : (max(X) - min(X))/(100) : max(X))';\ngridm = max(size(x));\nbhat = zeros(p + 1, gridm);\nfor i = 1 : gridm\ndm = ones(n, 1);\nxx = X - x(i);\nif p>0\nfor j = 1 : p\ndm = [dm (xx).^j];\nend;\nw = diag(quadk(xx./h)./h);\nmh = inv(dm'*w*dm)*dm'*w;\nbhat(:, i) = mh*Y;\nend\n", "output_sequence": "Reads the date, DAX index values, stock prices of 20 largest companies at Frankfurt Stock Exchange (FSE), FTSE 100 index values and stock prices of 20 largest companies at London Stock Exchange (LSE) and estimates the volatility of the DAX and FTSE 100 daily return processes from 1998 to 2007."}, {"input_sequence": "%clear variables and close windows\nclear all;\nclc;\n% Read data for FSE and LSE\nDS = load('FSE_LSE.dat');\nD = [DS(:, 1)]; % date\nr = [s(2 : end, :) - s(1 : (end - 1), :)]; % r(t)\nn = length(r); % sample size\n% Nonparametric volatility estimation, DAX\ny = [r(1 : (end - 1), r(2 : end, 1)];\nyy = [y(:, 1) y(:, 2).^2];\nhm = 0.04; % bandwidth\n[m1h yg] = lpregest(y(:, 1), y(:, 2), 1, hm); % estimate conditional mean function\nsh = [yg m2h(1, :)' - (m1h(1, :).^2)']; % conditional variance\nm1hx = interp1(yg, m1h(1, :)', y(:, 1)); % interpolate mean\nfigure\nsubplot(2, 1, 1)\nplot(shx_DAX .^ 0.5);\nhold on;\ntitle('DAX')\nxlim([1 n])\n% Nonparametric volatility estimation, FTSE 100\ny = [r(1 : (end - 1), 22) r(2 : end, 22)];\nshx_FTSE = interp1(yg, sh(:, 2), y(:, 1)); % interpolate variance\nsubplot(2, 1, 2)\nplot(shx_FTSE.^0.5);\ntitle('FTSE 100')\nxlabel('Time, t')\n", "output_sequence": "Reads the date, DAX index values, stock prices of 20 largest companies at Frankfurt Stock Exchange (FSE), FTSE 100 index values and stock prices of 20 largest companies at London Stock Exchange (LSE) and estimates the volatility of the DAX and FTSE 100 daily return processes from 1998 to 2007."}, {"input_sequence": "function y = quadk(x)\nI = (x.*x < 1);\nx = x.*I;\ny = (15/16) * I.* (1 - x.*x).^2;\n", "output_sequence": "Reads the date, DAX index values, stock prices of 20 largest companies at Frankfurt Stock Exchange (FSE), FTSE 100 index values and stock prices of 20 largest companies at London Stock Exchange (LSE) and estimates the volatility of the DAX and FTSE 100 daily return processes from 1998 to 2007."}, {"input_sequence": "# clear all variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# set working directory\n# setwd(\"C:/...\")\n# install and load packages\nlibraries = c(\"evd\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# data import\nData = read.csv(\"2004-2014_dax_ftse.csv\")\n# date variable as variable of class Date\nData$Date = as.Date(Data$Date, \"%Y-%m-%d\")\n# Create portfolio\nx = Data$BAYER + Data$BMW + Data$SIEMENS + Data$VOLKSWAGEN\nx = diff(x) # returns\nminus_x = -x # negative returns\nObs = length(x) # number of observations\nh = 250 # size of moving window\np = 0.95 # quantile for the Value at Risk\nn = 16 # observation window for estimating quantile in VaR\n# function ----\nblock_max = function(y,n,p){\nN = length(y)\nk = floor(N/n)\nz = rep(NaN, k)\nfor(j in 1:(k-1)){\nr = y[((j-1)*n+1):(j*n)]\nz[j] = max(r)\n}\nr = y[((k-1)*n+1):length(y)]\nz[k] = max(r)\n\nparmhat = fgev(z, std.err = FALSE)$param\nkappa = parmhat[\"shape\"]\ntau = -1/kappa\nalpha = parmhat[\"scale\"]\npext = p^n\nvar = beta+alpha/kappa*((-log(1-pext))^(-kappa)-1)\nout = c(var=var,tau=tau,alpha=alpha,beta=beta,kappa=kappa)\n}\n# Value at Risk ----\n# preallocation\nresults = data.frame(var=rep(NaN,Obs-h), tau=rep(NaN,Obs-h),\nbeta=rep(NaN,Obs-h), )\nfor(i in 1:(Obs-h)){\ny = minus_x[i:(i+h-1)]\nresults[i,] = block_max(y,n,p)\nv = -results$var\nL = x\noutlier = rep(NaN,Obs-h)\nexceedVaR = outlier\nfor(j in 1:(Obs-h)){\nexceedVaR[j] = (L[j+h]<v[j])\nif(exceedVaR[j]>0){\noutlier[j] = L[j+h]\np = sum(exceedVaR)/(Obs-h)\nK = which(is.finite(outlier))\noutlier = outlier[K]\nyplus = K * 0 + min(L[(h + 1):length(L)]) - 2\ndate_outlier = Data$Date[(h+2):length(Data$Date)]\ndate_outlier = date_outlier[K]\n# plot ----\nwindows()\nplot(Data$Date[(h+2):length(Data$Date)],x[(h+1):Obs], xlab = \"\",\nylab = \"\", col = \"blue\", pch = 18)\npoints(date_outlier, outlier, pch = 18, col = \"magenta\")\nlines(Data$Date[(h+2):length(Data$Date)], v, col= \"red\", lwd = 3)\npoints(date_outlier, yplus, pch = 3, col = \"dark green\")\nlegend(\"topleft\", c(\"Profit/Loss\", \"VaR\", \"Exceedances\"), pch = c(18, NA, 18),\nlwd = c(NA, 3, NA), col = c(\"blue\", \"red\", \"magenta\"))\ntitle(\"Block Maxima Model\")\n# Print the exceedances ratio\nprint(paste(\"Exceedances ratio:\", \"\", p))\n", "output_sequence": "Plots the Value-at-Risk estimation at 0.05 level for the portfolio composed by Bayer, BMW, siemens and Volkswagen. The Value at Risk is computed by means of the Block Maxima Model."}, {"input_sequence": "%% clear all variables and console\nclear\nclc\n%% close windows\nclose all\n%% data import\nformatSpec = '%{yyyy-MM-dd}D%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f';\nData = readtable('2004-2014_dax_ftse.csv','Delimiter',',', 'Format',formatSpec);\nData = Data(:,{'Date','BAYER','BMW', 'SIEMENS', 'VOLKSWAGEN'});\n%% create portfolio\nx = Data.BAYER + Data.BMW + Data.SIEMENS + Data.VOLKSWAGEN;\nx = diff(x); % returns\nminus_x = -x; % negative returns\nObs = length(x); % number of observations\np = 0.95; % quantile for the Value at Risk\nh = 250; % size of moving window\nn = 16; % size of moving window for estimating quantile in VaR\n%% Value at Risk\n% preallocation\nvar = NaN(1,Obs-h);\ntau = var;\nfor i = 1:Obs-h\ny = minus_x(i:i+h-1);\n[var(i),tau(i),alpha(i),beta(i),kappa(i)] = block_max(y,n,p);\nend;\n[v,K,outlier,yplus,p] = var_block_max_backtesting(x,var,h);\ndate_outlier = Data.Date(h+2:end);\ndate_outlier = date_outlier(K);\n%% plot\nplot(Data.Date(h+2:end),x(h+1:end), '.')\nhold on\nplot(Data.Date(h+2:end), v,'Color','red','LineWidth',2)\nplot(date_outlier,outlier,'.','Color','m')\nplot(date_outlier,yplus,'+','Color',[0,0.25098,0])\nlegend('Profit/Loss','VaR','Exceedances', 'Location', 'northwest')\ntitle('Block Maxima Model','FontSize',16,'FontWeight','Bold')\n", "output_sequence": "Plots the Value-at-Risk estimation at 0.05 level for the portfolio composed by Bayer, BMW, siemens and Volkswagen. The Value at Risk is computed by means of the Block Maxima Model."}, {"input_sequence": "%% clear all variables and console\nclear\nclc\n%% close windows\nclose all\n%% data import\nformatSpec = '%{yyyy-MM-dd}D%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f%f';\nData = readtable('2004-2014_dax_ftse.csv','Delimiter',',', 'Format',formatSpec);\nData = Data(:,{'Date','DAX30','FTSE100'});\n%% log-returns\nVrbls = table2array(Data(:,{'DAX30','FTSE100'}));\nret = diff(log(Vrbls(:,1)));\n%% estimated parameters of a multivariate GARCH model\n[Par, L, Ht, likelihoods, stdresid, stderrors, A, B, scores] = full_bekk_mvgarch(ret,1,1);\n%% print output\nformat long\nPar\nL\n%% the estimated variance and covariance processes\nn = numel(Ht); % number of array elements in Ht\nH = Ht(1:n)';\nH = reshape(H,4,n/4)';\n%% plot\nsubplot(3,1,1)\nplot(Data.Date(2:end),H(:,1))\ntitle('DAX')\nsubplot(3,1,2)\nplot(Data.Date(2:end),H(:,2))\ntitle('Covariance')\nsubplot(3,1,3)\nplot(Data.Date(2:end),H(:,4));\ntitle('FTSE 100')\n% Remark: the quantlet uses the following functions shown below (please save them as separate .m files):\n% fattailed_garch, fattailed_garchlikelihood, garchcore, hessian_2sided,\n% vech, scalar_bekk_mvgarch_likelihood, ivech, full_bekk_mvgarch_likelihood,\n% full_bekk_mvgarch,\n%% -------------------------------------------------------------------------\nfunction [parameters, likelihood, stderrors, robustSE, ht, scores] = fattailed_garch(data , p , q , errors, startingvals, options)\nt = size(data,1);\nif nargin<6\noptions=[];\nend\nif strcmp(errors,'NORMAL') || strcmp(errors,'STUDENTST') || strcmp(errors,'GED')\nif strcmp(errors,'NORMAL')\nerrortype = 1;\nelseif strcmp(errors,'STUDENTST')\nerrortype = 2;\nelse\nerrortype = 3;\nend\nelse\nerror('error must be one of the three strings NORMAL, STUDENTST, or GED');\nif size(data,2) > 1\nerror('Data series must be a column vector.')\nelseif isempty(data)\nerror('Data Series is Empty.')\nif (length(q) > 1) || any(q < 0)\nerror('Q must ba a single positive scalar or 0 for ARCH.')\nif (length(p) > 1) || any(p < 0)\nerror('P must be a single positive number.')\nelseif isempty(p)\nerror('P is empty.')\nif isempty(q) || q==0\nq = 0;\nm = max(p,q);\nif nargin<=4 || isempty(startingvals)\nalpha = .15*ones(p,1)/p;\nomega = (1-(sum(alpha)+sum(beta)))*cov(data); % set the uncond = to its expection\nif strcmp(errors,'STUDENTST')\nnu = 30;\nelseif strcmp(errors,'GED')\nnu = 1.7;\nomega = startingvals(1);\nnu=[];\nUB = [];\nsumA = [-eye(1+p+q); ...\n0 ones(1,p)\nsumB = [zeros(1+p+q,1); 1];\nif (nargin <= 5) || isempty(options)\noptions = optimset('fmincon');\noptions = optimset(options , 'TolFun' , 1e-006);\nsumB = sumB - [zeros(1+p+q,1); 1]*2*optimget(options, 'TolCon', 1e-6);\nif strcmp(errors,'STUDENTST')\nLB = zeros(1,p+q+2);\nLB(length(LB)) = 2.1;\nn = size(sumA,1);\nsumA = [sumA'; zeros(1,n)]';\nelseif strcmp(errors,'GED')\nLB(length(LB)) = 1.1;\nsumA = [sumA';zeros(1,n)]';\nLB = [];\nif errortype == 1\nstartingvals = [omega ; alpha ; beta];\n% Estimate the parameters.\nstdEstimate = std(data,1);\ndata = [stdEstimate(ones(m,1)) ; data];\nT = size(data,1);\n[parameters, ~, EXITFLAG, ~, ~] = fmincon('fattailed_garchlikelihood', startingvals ,sumA , sumB ,[] , [] , LB , UB,[],options, data, p , q, errortype, stdEstimate^2, T);\nif EXITFLAG<=0\nfprintf(1,'Not Sucessful! \\n')\nparameters(parameters < 0) = 0;\nhess = hessian_2sided('fattailed_garchlikelihood',parameters,data,p,q,errortype, stdEstimate^2, T);\nlikelihood=-likelihood;\nstderrors=hess^(-1);\nif nargout > 4\nh = max(abs(parameters/2),1e-2)*eps^(1/3);\nhplus = parameters+h;\nlikelihoodsplus = zeros(t,length(parameters));\nfor i=1:length(parameters)\nhparameters = parameters;\n[~, indivlike] = fattailed_garchlikelihood(hparameters,data,p,q,errortype, stdEstimate^2, T);\nlikelihoodsplus(:,i) = indivlike;\nfor i = 1:length(parameters)\nhparameters = parameters;\n[~, indivlike] = fattailed_garchlikelihood(hparameters,data,p,q,errortype, stdEstimate^2, T);\nlikelihoodsminus(:,i) = indivlike;\nscores = (likelihoodsplus-likelihoodsminus)./(2*repmat(h',t,1));\nscores = scores-repmat(mean(scores),t,1);\nB = scores'*scores;\nrobustSE = stderrors*B*stderrors;\nend\n%\n% Author's Comments\n% PURPOSE:\n% FATTAILED_GARCH(P,Q) parameter estimation with different error distributions, the NOrmal, The T,\n% and the Generalized Error Distribution\n%\n% USAGE:\n% [parameters, likelihood, stderrors, robustSE, ht, scores] = fattailed_garch(data , p , q , errors, startingvals, options)\n% INPUTS:\n% data: A single column of zero mean random data, normal or not for quasi likelihood\n% P: Non-negative, scalar integer representing a model order of the ARCH\n% process\n% Q: Positive, scalar integer representing a model order of the GARCH\n% process: Q is the number of lags of the lagged conditional variances included\n% Can be empty([]) for ARCH process\n% error: The type of error being assumed, valid types are:\n% 'NORMAL' - Gaussian Innovations\n% 'STUDENTST' - T-distributed errors\n% 'GED' - General Error Distribution\n%\n% startingvals: A (1+p+q) (plus 1 if STUDENTT OR GED is selected for the nu parameter) vector of starting vals.\n% If you do not provide, a naieve guess of 1/(2*max(p,q)+1) is used for the arch and garch parameters,\n% and omega is set to make the real unconditional variance equal\n% to the garch expectation of the expectation.\n% options: default options are below. You can provide an options vector. See HELP OPTIMSET\n% OUTPUTS:\n% parameters : a [1+p+q X 1] column of parameters with omega, alpha1, ..., alpha(p)\n% beta1, ... beta(q)\n% likelihood = the loglikelihood evaluated at he parameters\n% robustSE = QuasiLikelihood std errors which are robust to some forms of misspecification(see White 94)\n% stderrors = the inverse analytical hessian, not for quasi maximum liklihood\n% ht = the estimated time varying VARIANCES\n% scores = The numberical scores(# fo params by t) for M testing\n% COMMENTS:\n% GARCH(P,Q) the following(wrong) constratins are used(they are right for the (1,1) case or any Arch case\n% (1) Omega > 0\n% (2) Alpha(i) >= 0 for i = 1,2,...P\n% (4) sum(Alpha(i) + Beta(j)) < 1 for i = 1,2,...P and j = 1,2,...Q\n% (5) nu>2 of Students T and nu>1 for GED\n% The time-conditional variance, H(t), of a GARCH(P,Q) process is modeled\n% as follows:\n% H(t) = Omega + Alpha(1)*r_{t-1}^2 + Alpha(2)*r_{t-2}^2 +...+ Alpha(P)*r_{t-p}^2+...\n% Beta(1)*H(t-1)+ Beta(2)*H(t-2)+...+\n% Default Options\n%\n% options = optimset('fmincon');\n% options = optimset(options , 'TolFun' , 1e-003);\n% options = optimset(options , 'MaxFunEvals' , '400*numberOfVariables');\n% uses fFATTAILED_GARCHLIKELIHOOD and GARCHCORE. You should MEX, mex 'path\\garchcore.c', the MEX source\n% The included MEX is for R12 Windows and was compiled with VC++6. It\n% gives a 10-15 times speed improvement\n% -------------------------------------------------------------------------\nfunction [LLF, h, likelihoods] = fattailed_garchlikelihood(parameters , data , p , q, errortype, stdEstimate, T)\n[r,c]=size(parameters);\nif c>r\nparameters=parameters';\nparameters(parameters <= 0) = realmin;\nif errortype ~=1;\nnu = parameters(p+q+2);\nparameters = parameters(1:p+q+1);\nif isempty(q)\nm=p;\nm = max(p,q);\nh = garchcore(data,parameters,stdEstimate^2,p,q,m,T);\nTau = T-m;\nt = (m + 1):T;\nLLF = sum(log(h(t))) + sum((data(t).^2)./h(t));\nLLF = 0.5 * (LLF + (T - m)*log(2*pi));\nelseif errortype == 2\nLLF = Tau*gammaln(0.5*(nu+1)) - Tau*gammaln(nu/2) - Tau/2*log(pi*(nu-2));\nLLF = LLF - 0.5*sum(log(h(t))) - ((nu+1)/2)*sum(log(1 + (data(t).^2)./(h(t)*(nu-2)) ));\nLLF = -LLF;\nBeta = (2^(-2/nu) * gamma(1/nu)/gamma(3/nu))^(0.5);\nLLF = (Tau * log(nu)) - (Tau*log(Beta)) - (Tau*gammaln(1/nu)) - Tau*(1+1/nu)*log(2);\nLLF = LLF - 0.5 * sum(log(h(t))) - 0.5 * sum((abs(data(t)./(sqrt(h(t))*Beta))).^nu);\nLLF = -LLF;\nif nargout >1\nif errortype == 1\nlikelihoods = 0.5 * ((log(h(t))) + ((data(t).^2)./h(t)) + log(2*pi));\nlikelihoods = -likelihoods;\nelseif errortype == 2\nlikelihoods = gammaln(0.5*(nu+1)) - gammaln(nu/2) - 1/2*log(pi*(nu-2))...\n- 0.5*(log(h(t))) - ((nu+1)/2)*(log(1 + (data(t).^2)./(h(t)*(nu-2)) ));\nelse\nBeta = (2^(-2/nu) * gamma(1/nu)/gamma(3/nu))^(0.5);\nlikelihoods = (log(nu)/(Beta*(2^(1+1/nu))*gamma(1/nu))) - 0.5 * (log(h(t))) ...\n- 0.5 * ((abs(data(t)./(sqrt(h(t))*Beta))).^nu);\nend\nh = h(t);\n% Likelihood for fattailed garch estimation\n% [LLF, h, likelihoods] = fattailed_garchlikelihood(parameters , data , p , q, errortype, stdEstimate, T)\n% parameters: A vector of GARCH process aprams of the form [constant, arch, garch]\n% data: A set of zero mean residuals\n% p: The lag order length for ARCH\n% m: The max of p and q\n% error: The type of error being assumed, valid types are:\n% 1 if 'NORMAL'\n% stdEstimate: The std deviation of the data\n% T: Length of data\n% LLF: Minus 1 times the log likelihood\n% h: Time series of conditional volatilities\n% likelihoods Time series of likelihoods\n% This is a helper function for garchinmean\nfunction h = garchcore(data,parameters,covEst,p,q,m,T)\nh = zeros(size(data));\nh(1:m) = covEst;\nfor t = (m + 1):T\nh(t) = parameters' * [1 ; data(t-(1:p)).^2; h(t-(1:q)) ];\n% Forward recursion to construct h's\n% h=garchcore(data,parameters,covEst,p,q,m,T);\n% See garchlikelihood\n% Helper function part of UCSD_GARCH toolbox. Used if you do not use the MEX file.\n% You should use the MEX file.\nfunction H = hessian_2sided(f,x,varargin)\ntry\nfeval(f,x,varargin{:});\ncatch\nerror('There was an error evaluating the function. Please check the arguements.');\nn = size(x,1);\nfx = feval(f,x,varargin{:});\n% Compute the stepsize (h)\nh = eps.^(1/3)*max(abs(x),1e-2);\nxh = x+h;\nh = xh-x;\nee = sparse(1:n,1:n,h,n,n);\n% Compute forward and backward steps\ngp = zeros(n,1);\nfor i = 1:n\ngp(i) = feval(f,x+ee(:,i),varargin{:});\ngm = zeros(n,1);\ngm(i) = feval(f,x-ee(:,i),varargin{:});\nH = h*h';\n% Compute \"double\" forward and backward steps\nfor j = i:n\nHp(i,j) = feval(f,x+ee(:,i)+ee(:,j),varargin{:});\nHp(j,i) = Hp(i,j);\nHm(i,j) = feval(f,x-ee(:,i)-ee(:,j),varargin{:});\nHm(j,i) = Hm(i,j);\n% Compute the hessian\nfor j=i:n\nH(i,j) = (Hp(i,j)-gp(i)-gp(j)+fx+fx-gm(i)-gm(j)+Hm(i,j))/H(i,j)/2;\nH(j,i) = H(i,j);\n% PURPOSE:\n% Computes 2-sided finite difference Hessian\n% USAGE:\n% H = hessian_2sided(func,x,varargin)\n% func - function name, fval = func(x,varargin)\n% x - vector of parameters (n x 1)\n% varargin - optional arguments passed to the function\n% H - finite differnce, 2-sided hessian\n% Code originally from COMPECON toolbox [www4.ncsu.edu/~pfackler]\n% documentation modified to fit the format of the Ecoometrics Toolbox\n% by James P. LeSage, Dept of Economics\n% University of Toledo\n% 2801 W. Bancroft St,\n% Toledo, OH 43606\n% jlesage@spatial-econometrics.com\n% Further modified (to do 2-sided numerical derivs, rather than 1)\nfunction trandformeddata = vech(x)\ntrandformeddata = x(logical(tril(ones(size(x)))));\n% Transform a k by k matrix into a vector of size k*(k+1)/2 by 1, complements ivech\n% transformeddata=vech(data)\n% data: A k by k matrix\n% transformeddata - a k*(k+1)/2 by 1 vector for the form\n% [data(1,1) ... data(k,1)\nfunction [LLF,likelihoods,Ht]=scalar_bekk_mvgarch_likelihood(parameters,errors,p,q,k,k2,t)\n%The first k(k+1)/2 parameters are C, the next p are A, and the next q are B\nC = parameters(1:(k2));\nC = ivech(C);\nconst = C*C';\nuncond = cov(errors);\n\n% for starting up, both ee' and H have expectation uncond. We can leverage this to help the loops.\nm = max(p,q);\neeprime = zeros(k,k,t+m);\nfor i = 1:m\neeprime(:,:,i) = uncond;\nLLF = 0;\nerrors = [repmat(sqrt(diag(uncond))',m,1);errors];\nlikelihoods = zeros(t+m,1);\nfor i = m+1:t+m;\nHt(:,:,i) = const;\nfor j = 1:p\nHt(:,:,i) = Ht(:,:,i)+A(j)*(errors(i-j,:))'*(errors(i-j,:))*A(j);\nfor j = 1:q\nHt(:,:,i) = Ht(:,:,i)+B(j)*Ht(:,:,i-j)*B(j);\nlikelihoods(i) = k*log(2*pi)+(log(det(Ht(:,:,i)))+errors(i,:)*Ht(:,:,i)^(-1)*errors(i,:)');\nLLF = LLF + likelihoods(i);\nLLF = 0.5*(LLF);\nlikelihoods = 0.5*likelihoods(m+1:t+m);\nHt = Ht(:,:,m+1:t+m);\nif isnan(LLF)\nLLF = 1e6;\n% To Estimate a scalar BEKK multivariate GARCH likelihood.\n% [LLF,likelihoods,Ht]=scalar_bekk_mvgarch_likelihood(parameters,errors,p,q,k,k2,t);\n% parameters - a k*(k+1)/2 + p +q vector of model parameters of the form\n% [ivech(C);(A(1));...;(A(p));(B(1)); ...(B(q))]\n% errors - A zeromean t by k martix of residuals\n% p - The lag length of the innovation process\n% k - The number of data series\n% k2 - k*k\n% t - the length of the data series\n% LLF - The loglikelihood of the function at the optimum\n% Ht - A k x k x t 3 dimension matrix of conditional covariances\n% likelihoods - A t by 1 vector of individual likelihoods\nfunction transformeddata = ivech(data)\nt = size(data,1);\nsizeout = (-1+sqrt(1+8*t))/2;\ntransformeddata = zeros(sizeout);\nindex = 1;\nfor i = 1:sizeout\ntransformeddata(j,i) = data(index);\nindex = index + 1;\n% Transform a vector in to a lower triangular matrix for use by MVGARCH, complements vech\n% transformeddata=ivech(data)\n% data: A m by 1 vector to be transformed to a square k by k matrix.\n% M must ba solution to the equation k^1+k-2*m=0\n% transformeddata - a k by k lower matrix of the form\n% [ data(1) 0 ... 0\n% data(3) data(k+2) 0 ... 0\n% ... . ...\n% data(k) data(2k-1) ... data(m-1) ]\nfunction [LLF,likelihoods,Ht] = full_bekk_mvgarch_likelihood(parameters,errors,p,q,k,k2,t)\nA = parameters(k2+1:k2+k*k*p);\ntempA = zeros(k,k,p);\nfor i = 1:p\ntempA(:,:,i) = reshape(A((k*k*(i-1)+1):(k*k*i)),k,k);\nfor i = 1:q\ntempB(:,:,i) = reshape(B((k*k*(i-1)+1):(k*k*i)),k,k);\nA = tempA;\n% for starting up, both ee' and H have expectation uncond. We can leverage this to help the loops.\nfor i=1:m\nHt(:,:,i) = Ht(:,:,i) + A(:,:,j)*(errors(i-j,:))'*(errors(i-j,:))*A(:,:,j)';\n% To Estimate a full BEKK multivariate GARCH likelihood.\n% [LLF,likelihoods,Ht]=full_bekk_mvgarch_likelihood(parameters,errors,p,q,k,k2,t);\n% parameters - a k*(k+1)/2 + k^2 *p +k^2*q vector of model parameters of the form\n% [ivech(C);reshape(A(1),k*k,1);...;reshape(A(p),k*k,1);reshape(B,k*k,1); ...\n% reshape(B(q),k*k,1)]\nfunction [parameters, loglikelihood, Ht, likelihoods, stdresid, stderrors, A, B, scores] = full_bekk_mvgarch(data,p,q, BEKKoptions)\n% need to try and get some smart startgin values\nif size(data,2) > size(data,1)\ndata = data';\n[t, k] = size(data);\nk2 = k*(k+1)/2;\nscalaropt = optimset('fminunc');\nscalaropt = optimset(scalaropt,'TolFun',1e-1,'Display','iter','Diagnostics','on','DiffMaxChange',1e-2);\nstartingparameters = scalar_bekk_mvgarch(data,p,q,scalaropt);\nCChol = startingparameters(1:(k*(k+1))/2);\n% C=ivech(startingparameters(1:(k*(k+1))/2))*ivech(startingparameters(1:(k*(k+1))/2))';\nnewA = [];\nnewA = [newA diag(ones(k,1))*startingparameters(((k*(k+1))/2)+i)];\nnewA = reshape(newA,k*k*p,1);\nstartingparameters = [CChol;newA;newB];\nif nargin<=3 || isempty(BEKKoptions)\noptions=optimset('fminunc');\noptions.Display='iter';\noptions.MaxFunEvals=5000*length(startingparameters);\noptions=BEKKoptions;\nparameters = fminunc('full_bekk_mvgarch_likelihood',startingparameters,options,data,p,q,k,k2,t);\n[loglikelihood,likelihoods,Ht] = full_bekk_mvgarch_likelihood(parameters,data,p,q,k,k2,t);\nloglikelihood = -loglikelihood;\n% Standardized residuals\nstdresid=zeros(size(data));\nfor i = 1:t\nstdresid(i,:) = data(i,:)*Ht(:,:,i)^(-0.5);\n% Std Errors\nif nargout>=6\nA = hessian_2sided('full_bekk_mvgarch_likelihood',parameters,data,p,q,k,k2,t);\nh = max(abs(parameters/2),1e-2)*eps^(1/3);\nhplus = parameters+h;\nlikelihoodsplus = zeros(t,length(parameters));\nfor i = 1:length(parameters)\nhparameters = parameters;\n[~, indivlike] = full_bekk_mvgarch_likelihood(hparameters,data,p,q,k,k2,t);\nlikelihoodsplus(:,i) = indivlike;\nhparameters = parameters;\n[~, indivlike] = full_bekk_mvgarch_likelihood(hparameters,data,p,q,k,k2,t);\nlikelihoodsminus(:,i) = indivlike;\nscores = (likelihoodsplus-likelihoodsminus)./(2*repmat(h',t,1));\nB = cov(scores);\nstderrors = A^(-1)*B*A^(-1)*t^(-1);\n% To Estimate a full BEKK multivariate GARCH model. ****SEE WARNING AT END OF HELP FILE****\n% [parameters, loglikelihood, Ht, likelihoods, stdresid, stderrors, A, B, scores] = full_bekk_mvgarch(data,p,q,options);\n% data - A t by k matrix of zero mean residuals\n% p - The lag length of the innovation process\n% options - (optional) Options for the optimization(fminunc)\n% parameters - A (k*(k+1))/2+p*k^2+q*k^2 vector of estimated parameteters. F\n% or any k^2 set of Innovation or AR parameters X,\n% reshape(X,k,k) will give the correct matrix\n% To recover C, use ivech(parmaeters(1:(k*(k+1))/2)\n% loglikelihood - The loglikelihood of the function at the optimum\n% stdresid - A t by k matrix of multivariate standardized residuals\n% stderrors - A numParams^2 square matrix of robust Standad Errors(A^(-1)*B*A^(-1)*t^(-1))\n% A - The estimated inverse of the non-robust Standard errors\n% B - The estimated covariance of teh scores\n% scores - A t by numParams matrix of individual scores\nfunction [parameters, loglikelihood, Ht, likelihoods, stdresid, stderrors, A, B, scores] = scalar_bekk_mvgarch(data,p,q,BEKKoptions)\ndata=data';\n[t, k] = size(data);\ngarchmat = zeros(k,1+p+q);\noptions = optimset('fmincon');\noptions.TonCon = 1e-3;\noptions.Diagnostics = 'off';\noptions.LevenbergMarquardt = 'on';\nfor i = 1:k\ntemparam = fattailed_garch(data(:,i),p,q,'NORMAL',[],options);\ngarchmat(i,:) = temparam';\nA = mean(garchmat(:,2:p+1));\nC = cov(data);\nalpha0 = sqrt(A);\nStartC = C*(1-sum(alpha0.^2)-sum(beta0.^2));\nCChol = chol(StartC)';\nwarning off\nstartingparameters = [vech(CChol);alpha0;beta0];\nk2 = k*(k+1)/2;\noptions = optimset('fminunc');\noptions.Display = 'iter';\noptions = BEKKoptions;\nparameters = fminunc('scalar_bekk_mvgarch_likelihood',startingparameters,options,data,p,q,k,k2,t);\n[loglikelihood,likelihoods,Ht] = scalar_bekk_mvgarch_likelihood(parameters,data,p,q,k,k2,t);\nstdresid = zeros(size(data));\nA = hessian_2sided('scalar_bekk_mvgarch_likelihood',parameters,data,p,q,k,k2,t);\nhparameters=parameters;\n[~, indivlike] = scalar_bekk_mvgarch_likelihood(hparameters,data,p,q,k,k2,t);\n% To Estimate a scalar BEKK multivariate GARCH model. ****SEE WARNING AT END OF HELP FILE****\n% [parameters, loglikelihood, Ht, likelihoods, stdresid, stderrors, A, B, scores] = scalar_bekk_mvgarch(data,p,q,options);\n% parameters - A (k*(k+1))/2+p+q vector of estimated parameteters.\n% B - The estimated covariance of the scores\n", "output_sequence": "Estimates parameters of a multivariate GARCH model for the daily log-returns of DAX and FTSE100 from 10.05.2004 to 07.05.2014. It also plots the estimated variance and covariance processes."}, {"input_sequence": "# clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# install and load packages\nlibraries = c(\"TeachingDemos\", \"lattice\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# parameter settings\nn = 10000 # sample size\na = 2^16 + 3\nM = 2^31\nseed = 1298324 # makes sure that the same numbers are generated each time the quantlet is executed\ny = NULL\n# main computation\ny[1] = seed\ni = 2\nwhile (i <= n) {\ny[i] = (a * y[i - 1])%%M\ni = i + 1\n}\ny = y/M\n# output\nrotate.cloud(y[3:n] ~ y[1:(n - 2)] + y[2:(n - 1)], xlab = \"\", ylab = \"\", zlab = \"\",\ntype = \"p\", scales = list(arrows = FALSE, col = \"black\", distance = 1,\ntick.number = 8, cex = 0.7, x = list(labels = round(seq(0, 1, length = 10),\n", "output_sequence": "Generates uniform random numbers using RANDU generator and produces a 3d plot of generated numbers where the hyperplains can be visible."}, {"input_sequence": "function [var,ksi,beta,u] = var_pot(y,h,p,q)\nN = floor(h*q);\nys = sort(y,'descend');\nu = ys(N+1);\n\nparams = gpfit(z);\nwarning off\nksi = params(1);\nvar = u + beta/ksi*((h/N*(1-p))^(-ksi)-1);\nend\n", "output_sequence": "Provides Value at Risk estimates computed with Peaks Over Treshold model with generalized Pareto distribution."}, {"input_sequence": "# clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# load data\nx1 = read.table(\"BAYER_close_0012.dat\")\nr1 = diff(as.matrix(log(x1)))\n# Variance efficient portfolio\nportfolio = cbind(r1, r2, r4)\nopti = solve(cov(portfolio)) %*% c(1, 1, 1)\nopti = opti/sum(opti)\nlrplport = as.matrix(portfolio) %*% opti\n# plot\npar(mfrow = c(2, 1))\nplot(density(lrplport, kernel = \"biweight\", bw = 0.003), xlim = c(-0.1, 0.1), main = \"Density Estimate of the Positive Log-Returns of the Portfolio\",\nlwd = 2, col = \"blue3\", xlab = \"Loss(-)/Profit(+)\", ylab = \"Frequency\", ylim = c(0,\n40))\nplot(density(-lrplport, kernel = \"biweight\", bw = 0.003), xlim = c(-0.1, 0.1),\nmain = \"Density Estimate of the Negative Log-Returns of the Portfolio\", lwd = 2,\ncol = \"blue3\", xlab = \"Loss(+)/Profit(-)\", ylab = \"Frequency\", ylim = c(0,\n", "output_sequence": "Computes and plots the density of the loss-profit function for the variance efficient portfolio of Bayer, BMW, Siemens and Volkswagen log-returns for the period  1 January 2000 - 31 December 2012."}, {"input_sequence": "% create random sample\ndata = normrnd(0.05, 0.01, 2000, 1);\n% define input parameter Model\nModel.Data = data;\nModel.delta = 0.004;\nModel.n = length(data);\n% define input parameter Params\na = 0.26;\nsigma = 0.01;\nParams = [a, b, sigma];\n% execute function\n", "output_sequence": "Defines the log-likelihood function of the CIR model used in SFECIRmle."}, {"input_sequence": "function lnL = CIRml(Params, Model)\n% define input parameters\nlData = Model.Data;\nDataF = lData(1:end - 1);\na = Params(1);\n\n% compute relevant parameters for the log-likelihood function\nc = 2 * a / (sigma^2 * (1 - exp(-a * Model.delta)));\nu = c * exp(-a * Model.delta) * DataF;\nv = c * DataL;\nq = 2 * a * b / sigma^2 - 1;\nz = 2 * sqrt(u .* v);\nbf = besseli(q, z, 1);\n% compute log-likelihood\nlnL = -(Model.n - 1) * log(c) - sum(-u - v + 0.5 * q * log(v ./ u) + log(bf) + z);\n", "output_sequence": "Defines the log-likelihood function of the CIR model used in SFECIRmle."}, {"input_sequence": "import pandas as pd\nimport numpy as np\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.tsa.arima_process import ArmaProcess\nimport matplotlib.pyplot as plt\n# parameter settings\nlag = 30 # lag value\nn = 1000 #sampled values\nb = 0.5\nnp.random.seed(123)\n# Obtain MA(1) sample by sampling from a ARMA() model with no AR coefficient\nar1 = np.array([1])\nMA_object1 = ArmaProcess(ar1,ma1)\nsimulated_data_1 = MA_object1.generate_sample(nsample=1000)\nma1 = np.array([1,-b])\nsimulated_data_2 = MA_object1.generate_sample(nsample=1000)\nf, axarr = plt.subplots(2, 1,figsize=(11, 6))\nplot_acf(simulated_data_1,lags=lag,ax=axarr[0],title='Sample ACF of the Simulated MA(1) Process with '+r'$\\beta=0.5$',zero=False,alpha=None)\nplt.tight_layout()\nplt.show()\nplt.savefig('SFEacfma1_py.png')\n", "output_sequence": "Plots the autocorrelation function of an MA(1) (moving average) process."}, {"input_sequence": "# clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# parameter settings\nlag = \"30\" # lag value\nb = \"0.5\" # value of beta_1\n# Input beta_1\nmessage = \" input beta\"\ndefault = b\nb = winDialogString(message, default)\nb = type.convert(b, na.strings = \"NA\", as.is = FALSE, dec = \".\")\n# Input lag value\nmessage = \" input lag\"\ndefault = lag\nlag = winDialogString(message, default)\nlag = type.convert(lag, na.strings = \"NA\", as.is = FALSE, dec = \".\")\n# Plot\nplot(ARMAacf(ar = numeric(0), ma = b, lag.max = lag, pacf = FALSE), type = \"h\", xlab = \"lag\",\nylab = \"acf\")\ntitle(\"Sample autocorrelation function (acf)\")\n", "output_sequence": "Plots the autocorrelation function of an MA(1) (moving average) process."}, {"input_sequence": "# clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# Conditional Gaussian default probability\nCondDefaultProbGauss = function(rho, market.factor, default.time, hazard.rate) {\n# Determine default probability from hazard rate\ndp = drop(1 - exp(-hazard.rate * default.time))\n\n# Determine quantile for probability in standard normal distribution\nnormal.quantile = qnorm(dp, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE)\n# Determine value of Gaussian random variable according to factor model\nu = (normal.quantile - sqrt(rho) * market.factor)/sqrt(1 - rho)\n# Return conditional gaussian default probability\npnorm(u, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE)\n}\n# Conditional probability for pool loss rate of n/M\nCondPoolLossProb = function(rho, market.factor, default.time, hazard.rate, number.defaults,\nentities) {\nsapply(X = market.factor, FUN = function(market.factor, rho, default.time,\nhazard.rate, number.defaults, entities) {\n# Compute pool loss probability conditional on market factor\ncond.pool.loss.prob = dbinom(x = number.defaults, size = entities, prob = CondDefaultProbGauss(rho,\nmarket.factor, default.time, hazard.rate), log = FALSE)\n\n# Compute the market factor probability\nmarket.factor.prob = dnorm(market.factor, mean = 0, sd = 1, log = FALSE)\nreturn(cond.pool.loss.prob * market.factor.prob)\n}, rho = rho, default.time = default.time, hazard.rate = hazard.rate, number.default = number.defaults,\nentities = entities)\n# Unconditional probability for pool loss rate of n/M\nPoolLossProb = function(rho, default.time, hazard.rate, number.defaults, entities) {\nintegrate(f = CondPoolLossProb, lower = -Inf, upper = Inf, rho = rho, default.time = default.time,\nhazard.rate = hazard.rate, number.defaults = number.defaults, entities = entities,\nrel.tol = 1e-12, stop.on.error = FALSE)[[1]]\n# Model parameters\nrho.parameters = c(0.05, 0.1, 0.5)\nentities = 125\nnumber.defaults = 1:entities\nhazard.rate = 0.06\nmaturity = 5\ndefault.percentage = number.defaults/entities\ndensities = sapply(X = rho.parameters, FUN = function(rho) {\ndensity = sapply(X = number.defaults, FUN = PoolLossProb, rho = rho, default.time = maturity,\nhazard.rate = hazard.rate, entities = entities)\nreturn(density)\n})\n# plot\nplot(x = default.percentage, y = densities[, 1], main = \"Portfolio Loss Density\", type = \"l\", xlab = \"Portfolio Loss\",\nylab = \"Density\", col = \"red\")\nlines(x = default.percentage, y = densities[, 2], col = \"blue\")\n", "output_sequence": "Computes the density function of the loss in a portfolio of bonds or other credit instruments using a one-factor Gaussian copula. The marginal densities of default times are implemented as poisson distributions using constant default intensities. After computing the portfolio loss densities for various values of the dependence parameter rho, these densities are plotted."}, {"input_sequence": "# clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# load data\ndata = read.table(\"dax99.dat\")\ndax99 = data[, 2] # first line is date, second XetraDAX 1999\nret = diff(log(dax99))\nfh = density(ret, bw = 0.03) # estimate Dax return density\nmu = mean(ret) # empirical mean\nsi = sqrt(var(ret)) # empirical standard deviation (std)\nx = si * rnorm(400) + mu # generate artificial data from the same mean and std\nf = density(x, bw = 0.03, kernel = \"biweight\") # estimate its density\n# plot\nplot(fh, col = \"blue\", lwd = 2, main = \"DAX Density versus Normal Density\", xlab = \"Returns\", ylab = \"Density\")\nlines(f, lwd = 2, col = \"black\")\n", "output_sequence": "Compares a kernel estimation of the density of DAX returns distribution with a kernel estimation of a normal distribution with the same mean and variance. The data is taken from the Xetra-DAX 1999."}, {"input_sequence": "data = load('dax99.dat');\ndax99 = data(:,2); % first line is date, second XetraDAX 1999\nlndax99 = log(dax99);\nret = diff(lndax99);\nf = 'qua';% kernel function: quartic\nN = 400; % length of estimation vector\nh = 0.03; % bandwidth\nhold on\n[xi fh] = kerndens(ret, h, N, f);\n%[fh, xi] = ksdensity(ret, 'kernel','quartic','npoints', 400, 'width',0.03);\nset(gca, 'ylim', [0 13], 'FontWeight', 'bold', 'FontSize', 16);\nplot(xi, fh, 'b', 'LineWidth', 2)\nxlim([-0.07 0.07])\ntitle('DAX Density versus Normal Density', 'FontWeight', 'bold', 'FontSize', 16)\nmu = mean(ret); %; empirical mean\nsi = sqrt(var(ret)); %; empirical standard deviation (std)\nrandn('seed', 100);\nx = normrnd(mu, si, 400, 1); %; generate artifical data from the same mean and std\n[xj, f] = kerndens(x, h, N, f); %; estimate its density\nplot(xj, f, 'k', 'LineStyle', '-.', 'Linewidth', 2)\nhold off\nbox on\nset(gca, 'FontSize', 16, 'LineWidth', 2, 'FontWeight', 'bold');\nprint -painters -dpdf -r600 SFEdaxretDenNew.pdf\n", "output_sequence": "Compares a kernel estimation of the density of DAX returns distribution with a kernel estimation of a normal distribution with the same mean and variance. The data is taken from the Xetra-DAX 1999."}, {"input_sequence": "function [xg fh] = kerndens(x, h, N, f)\nn = size(x,1); %number of observations\nxr = diff([min(x), max(x)]);\nng = 400;\nxg = (xr + 7 * h) * (0:(ng - 1)) / (ng - 1) + min(x) - 3.5 * h;\nfk = zeros(ng,n);\nfor (j = 1:n)\nfk(:, j) = kern((xg - x(j)) / h, f) / (n * h); %Gaussian kernel\nend\nfh = sum(fk'); %Kernel density estimate\nfh = fh';\n", "output_sequence": "Compares a kernel estimation of the density of DAX returns distribution with a kernel estimation of a normal distribution with the same mean and variance. The data is taken from the Xetra-DAX 1999."}, {"input_sequence": "% kernel functions\nfunction y = kern(x, f)\n\nif strcmp(f, 'gau') % gaussian kernel\ny = normpdf(x);\nend\nif strcmp(f, 'qua') % quadric / biweight kernel\ny = 0.9375 .* (abs(x) < 1) .* (1 - x .^ 2) .^ 2;\nif strcmp(f, 'cosi') % cosine kernel\ny = (pi / 4) .* cos((pi / 2) .* x) .* (abs(x) < 1);\nif strcmp(f, 'tri') % triweight kernel\ny = 35 / 32 .* (abs(x) < 1) .* (1 - x .^ 2) .^ 3;\nif strcmp(f, 'tria') % triangular kernel\ny = (1 - abs(x)) .* (abs(x) < 1);\nif strcmp(f, 'uni') % uniform kernel\ny = 0.5 .* (abs(x) < 1);\nif strcmp(f, 'spline') % spline kernel\ny = 0.5 .* (exp(-abs(x) ./ sqrt(2))) .* (sin(abs(x) ./ sqrt(2) + pi / 4));\nif exist(f) == 0\ny = normpdf(x);\n", "output_sequence": "Compares a kernel estimation of the density of DAX returns distribution with a kernel estimation of a normal distribution with the same mean and variance. The data is taken from the Xetra-DAX 1999."}, {"input_sequence": "# clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# Parameters of the CIR model (a, b, sigma)\na = 0.221\nsigma = 0.055\ntau = 0.25 # time to maturity (in years)\nr = 0.02 # risk free rate at time t\n# Main computation\nphi = sqrt(a^2 + 2 * sigma^2)\ng = 2 * phi + (a + phi) * (exp(phi * tau) - 1)\nB = (2 * (exp(phi * tau) - 1))/g\nA = 2 * a * b/sigma^2 * log(2 * phi * exp((a + phi) * tau/2)/g)\nBondprice = exp(A - B * r)\nprint(paste(\"Bond price = \", Bondprice))\n", "output_sequence": "Calculates a price of a bond using CIR model."}, {"input_sequence": "clear\nclc\nclose all\n%% User inputs parameters\ndisp('Please input [a, b, sigma, tau, r] as: [0.221, 0.020, 0.020]');\ndisp(' ');\npara=input('[a, b, sigma, tau, r] =');\nwhile length(para) < 5\ndisp('Not enough input arguments. Please input in 1*5 vector form like [0.221, 0.020, 0.020]');\npara=input('[a, b, sigma, tau, r] as =');\nend\na=para(1);\nsigma=para(3);\ntau=para(4);\n%% Main computation\nphi = sqrt(a^2+2*sigma^2);\ng = 2*phi + (a+phi)*(exp(phi*tau)-1);\nB = (2*(exp(phi*tau)-1))./g;\nA = 2*a*b/sigma^2*log(2*phi*exp((a+phi)*tau/2)./g);\nBondprice = exp(A-B*r);\ndisp('Bond price =')\n", "output_sequence": "Calculates a price of a bond using CIR model."}, {"input_sequence": "# clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# install and load packages\nlibraries = c(\"wmtsa\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# Package dvfBm is not supported by CRAN anymore, this downloads the latest archived version\ninstall.packages(\"https://cran.r-project.org/src/contrib/Archive/dvfBm/dvfBm_1.0.tar.gz\", repos = NULL, type=\"source\")\nlibrary('dvfBm')\n# parameter settings\nn = 1000 + 1\nH1 = 0.2\ndz1 = perturbFBM(n, H1, type = \"no\", SNR = NULL, plot = FALSE)\nz1 = diff(dz1)\n# plot\npar(mfrow = c(2, 2))\nplot(z1, type = \"l\", col = \"blue\", xlab = \"\", ylab = \"\", main = \"H1 = 0.2\", cex.lab = 1.4,\ncex.main = 1.6)\nplot(z2, type = \"l\", col = \"blue\", xlab = \"\", ylab = \"\", main = \"H1 = 0.8\", cex.lab = 1.4,\nac1 = acf(z1, lag.max = 60, plot = F)\nplot(ac1, cex.axis = 1.6, cex.lab = 1.6, ylab = \"\", main = \"ACF\", cex.main = 1.6)\n", "output_sequence": "Produces plots and autocorrelation functions of fractional Gaussian noise with 2 different Hurst parameters."}, {"input_sequence": "# Install and load packages\nlibraries = c(\"PortfolioAnalytics\", \"PerformanceAnalytics\", \"zoo\",\n\"plotly\", \"plyr\", \"RiskPortfolios\", \"devtools\", \"PMwR\",\"Jmisc\",\n\"FRAPO\", \"R.utils\", \"xts\", \"fastcluster\", \"fPortfolio\",\n\"dplyr\", \"shiny\", \"anytime\", \"ivmte\", \"highfrequency\", \"fBasics\",\n\"tseries\", \"xtable\", \"grDevices\", \"mgcv\", \"tidyr\",\"Hmisc\", \"feather\",\n\"data.table\", \"mgcv\", \"car\", \"ggplot2\", \"grid\", \"animation\", \"tidyverse\",\n\"lubridate\", \"psych\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n", "output_sequence": "None"}, {"input_sequence": "####5Set working directory\n#setwd(\"~\")\n#### Load packages and libraries ####\nsource(\"CCIDPackages.R\")\nrm(list=ls(all=TRUE))\ngraphics.off()\nload('CCID.Rdata')\ncols = rainbow(length(coins), s = 1, v = 1, start = 0, end = max(1, n - 1)/n, alpha = 1)\nCC_returns_mat = as.data.frame(CC_returns)\nCC_returns_xts = xts::as.xts(CC_returns_mat, order.by = TIME_DATE[-1])\nCC_abs_returns_mat = as.data.frame(abs(CC_returns_mat))\nCC_abs_returns_xts = xts::as.xts(CC_abs_returns_mat, order.by = TIME_DATE[-1])\nCC_price_mat = as.data.frame(CC_price)\nCC_price_xts = xts::as.xts(CC_price_mat, order.by = TIME_DATE)\nCC_volume_mat = as.data.frame(CC_volume)\nCC_volume_mat = as.data.frame(CC_volume_mat, order.by = TIME_DATE)\nn = nrow(CC_returns_mat)\nresult = matrix(1, ncol = 7, nrow = ncol(CC_returns_mat), dimnames = list(coins, c(\"rho(ret):\",\n\"rho n(ret^2):\", \"rho(|ret|):\", \"S:\", \"JB:\", \"JB p-value:\")))\nrownames(result) = coins\n# 5 min returns histogramms\npdfname2 = paste0(\"Density of intraday CCs returns.pdf\")\npdf(file = pdfname2)\nhist(rnorm(length(CC_returns_xts), mean = mean(CC_returns_xts$BTC),\nsd = sd(CC_returns_xts$BTC)), freq = FALSE, breaks = 100,\nmain = \"Density of cryptos against normal distribution\",\nylim = c(0,1300), xlab = \"\")\nfor (i in 1:length(coins)) {\nlines(density(na.omit(CC_returns_xts[,i])), col = cols[i], lwd = 3)\n}\ndev.off()\n", "output_sequence": "None"}, {"input_sequence": "#### Load packages and libraries ####\nsource(\"CCIDPackages.R\")\nrm(list=ls(all=TRUE))\ngraphics.off()\nload('CCID.Rdata')\n# Create a list of daily means, temps_avg, and print this list\n#CC_returns_mat = as.data.frame(CC_returns)\n#CC_returns_xts = xts::as.xts(CC_returns_mat, order.by = TIME_DATE[-1])\n#CC_abs_returns_mat = as.data.frame(abs(CC_returns_mat))\n#CC_abs_returns_xts = xts::as.xts(CC_abs_returns_mat, order.by = TIME_DATE[-1])\nCC_price_mat = as.data.frame(CC_price)\nCC_price_xts = xts::as.xts(CC_price_mat, order.by = TIME_DATE)\nCC_volume_mat = as.data.frame(CC_volume)\nCC_volume_mat = as.data.frame(CC_volume_mat, order.by = TIME_DATE)\n#n = nrow(CC_returns_mat)\n#result = matrix(1, ncol = 7, nrow = ncol(CC_returns_mat), dimnames = list(coins, c(\"rho(ret):\",\n# \"rho n(ret^2):\", \"rho(|ret|):\", \"S:\", \"JB:\", \"JB p-value:\")))\n#rownames(result) = coins\nprice_daily = list()\n#abs_ret_daily_xts = list()\nep = endpoints(CC_volume_xts, on = \"days\")\nfor (i in c(1:(length(ep)-1))) {\nprice_daily[[i]] = CC_price_xts[index(CC_price_xts)[(ep[i]+1):(ep[i+1])], ]\nvolume_daily_xts[[i]] = volume_daily[[i]]\nvolume_daily[[i]] = as.data.frame(volume_daily[[i]])\ncolnames(volume_daily[[i]]) = coins\nrownames(volume_daily_xts[[i]]) = index(price_daily[[i]])[-1]\n\n}\n#### GAM for trading volume ####\nDT_vol = list()\nfor (coin in coins){\n\ni = which(coins==coin)\n#Gam Trading volume\nDT_vol[[coin]] = sapply(volume_daily, \"[\", i)\nfor (j in 1:length(volume_daily_xts)) {\nDT_vol[[coin]][[j]] = xts(DT_vol[[coin]][[j]], order.by = index(volume_daily_xts[[j]]))\n}\nDT_vol[[coin]] = do.call(rbind, lapply(DT_vol[[coin]], function(x) x))\ncolnames(DT_vol[[coin]]) = \"Volume\"\nDT_vol[[coin]] = as.data.frame(DT_vol[[coin]])\nDT_vol[[coin]]$date = as.character(as.Date(DT_vol[[coin]]$date_time))\nDT_vol[[coin]]$time = format(as.POSIXct(DT_vol[[coin]]$date_time), format = \"%H:%M:%S\")\nDT_vol[[coin]]$time_num = as.numeric(as.factor(DT_vol[[coin]]$time))\nDT_vol[[coin]]$week = weekdays(as.Date(DT_vol[[coin]]$date,'%Y-%m-%d'))\nn_date = unique(DT_vol[[coin]]$date)\ndata_r_vol = as.data.frame(DT_vol[[coin]][(DT_vol[[coin]]$date %in% n_date[1:62]),])\nN = nrow(data_r_vol) # number of observations in the train set\nwindow = 62 # number of days in the train set\nmatrix_gam_vol = data.table(Volume = data_r_vol[[1]],\nDaily = rep(1:288, window),\ngam_2 = gam(Volume ~ s(Weekly, bs = \"ps\", k = 7),\ndata = matrix_gam_vol,\nfamily = gaussian)\ngam_vol_cc[[coin]]$Weekly = gam_2\npdfname5 = paste0(\"Gam_weekly_vol\", coin, \".pdf\")\npdf(file = pdfname5)\nplot(gam_2, shade = TRUE, xlab = \"Time\", ylab = \"Absolute returns (vola)\")\ndev.off()\ngam_vol_cc[[coin]]$Daily = gam(Volume ~ s(Daily, bs = \"cr\", k = 288),\ndata = matrix_gam_vol,\npdfname6 = paste0(\"Gam_vol\", coin, \".pdf\")\npdf(file = pdfname6)\nplot(gam_vol_cc[[coin]]$Daily, shade = TRUE, xlab = \"Time\", ylab = \"Trading volume\", main = coin, xaxt='n')\naxis(1, seq(1,288, 24), format(as.POSIXlt(rownames(DT_vol[[coin]])[seq(1,288, 24)]), \"%H:%M\"), las = 2 )\ngam_4 = gam(Volume ~ s(Daily, bs = \"cr\", k = 288)+\ns(Weekly, bs = \"ps\", k = 7),\npdfname8 = paste0(\"Gam_vol_weekly_daily2D\", coin, \".pdf\")\npdf(file = pdfname8)\nplot(gam_4, shade = TRUE, xlab = \"Time\", ylab = \"Trading volume\", main = coin, xaxt='n')\npdfname9 = paste0(\"Gam_3D_volume\", coin, \".pdf\")\npdf(file = pdfname9)\nvis.gam(gam_4, n.grid = 50, theta = 35, phi = 32, zlab = \"\",\nticktype = \"detailed\", color = \"topo\", main = \"Trading Volume\")#axes=FALSE,\naxis(1, seq(1,288, 24), format(as.POSIXlt(rownames( DT_vol[[coin]])[seq(1,288, 24)]), \"%H:%M\"), las = 2 )\n", "output_sequence": "None"}, {"input_sequence": "# Install and load packages\nlibraries = c(\"PortfolioAnalytics\", \"PerformanceAnalytics\", \"zoo\",\n\"plotly\", \"plyr\", \"RiskPortfolios\", \"devtools\", \"PMwR\",\"Jmisc\",\n\"FRAPO\", \"R.utils\", \"xts\", \"fastcluster\", \"fPortfolio\",\n\"dplyr\", \"shiny\", \"anytime\", \"ivmte\", \"highfrequency\", \"fBasics\",\n\"tseries\", \"xtable\", \"grDevices\", \"mgcv\", \"tidyr\",\"Hmisc\", \"feather\",\n\"data.table\", \"mgcv\", \"car\", \"ggplot2\", \"grid\", \"animation\", \"tidyverse\",\n\"lubridate\", \"psych\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n", "output_sequence": "None"}, {"input_sequence": "#### Load packages and libraries ####\nsource(\"CCIDPackages.R\")\nrm(list=ls(all=TRUE))\ngraphics.off()\nload('CCID.Rdata')\n# Create a list of daily means, temps_avg, and print this list\nCC_returns_mat = as.data.frame(CC_returns)\nCC_returns_xts = xts::as.xts(CC_returns_mat, order.by = TIME_DATE[-1])\nCC_abs_returns_mat = as.data.frame(abs(CC_returns_mat))\nCC_abs_returns_xts = xts::as.xts(CC_abs_returns_mat, order.by = TIME_DATE[-1])\nCC_price_mat = as.data.frame(CC_price)\nCC_price_xts = xts::as.xts(CC_price_mat, order.by = TIME_DATE)\nn = nrow(CC_returns_mat)\nresult = matrix(1, ncol = 7, nrow = ncol(CC_returns_mat), dimnames = list(coins, c(\"rho(ret):\",\n\"rho n(ret^2):\", \"rho(|ret|):\", \"S:\", \"JB:\", \"JB p-value:\")))\nrownames(result) = coins\nprice_daily = list()\nabs_ret_daily_xts = list()\nep = endpoints(CC_price_xts, on = \"days\")\nfor (i in c(1:(length(ep)-1))) {\nprice_daily[[i]] = CC_price_xts[index(CC_price_xts)[(ep[i]+1):(ep[i+1])], ]\nret_daily[[i]] = as.data.frame(apply(price_daily[[i]], MARGIN = 2, function(x) diff(log(as.numeric(x)))))\nret_daily_xts[[i]] = ret_daily[[i]]\ncolnames(ret_daily[[i]]) = coins\nrownames(ret_daily_xts[[i]]) = index(price_daily[[i]])[-1]\nabs_ret_daily_xts[[i]] = xts(abs_ret_daily_xts[[i]],\norder.by = as.POSIXlt(rownames(abs_ret_daily_xts[[i]])),\nformat = \"%d.%m.%y %H:%M\")\nret_daily_xts[[i]] = xts(ret_daily_xts[[i]],\norder.by = as.POSIXlt(rownames(ret_daily_xts[[i]])),\nformat = \"%d.%m.%y %H:%M\")\n\n}\nret_daily_mean = aaply(laply(ret_daily_xts, as.matrix), c(2, 3), mean)\ncolnames(ret_daily_mean) = coins\nplot(zoo::zoo(abs_ret_daily_mean))#, ylim = c(0, 0.0046))\n#### GAM for volatility ####\nDT = list()\ngam_cc = list()\nfor (coin in coins){\n# coin = coins[1]\ni = which(coins==coin)\nDT[[coin]] = sapply(abs_ret_daily, \"[\", i)\nfor (j in 1:length(abs_ret_daily_xts)) {\nDT[[coin]][[j]] = xts(DT[[coin]][[j]], order.by = index(abs_ret_daily_xts[[j]]))\n}\nDT[[coin]] = do.call(rbind, lapply(DT[[coin]], function(x) x))\ncolnames(DT[[coin]]) = \"Abs_return\"\nDT[[coin]] = as.data.frame(DT[[coin]])\nDT[[coin]]$date_time = rownames(DT[[coin]] )\nDT[[coin]]$date = as.character(as.Date(DT[[coin]]$date_time))\nDT[[coin]]$time = format(as.POSIXct(DT[[coin]]$date_time), format = \"%H:%M:%S\")\nDT[[coin]]$time_num = as.numeric(as.factor(DT[[coin]]$time))\nDT[[coin]]$week = weekdays(as.Date(DT[[coin]]$date,'%Y-%m-%d'))\nn_date = unique(DT[[coin]]$date)\nperiod = 24*12-1 #12 per 5 min per hour - 1return = 287\ndata_r = as.data.frame(DT[[coin]][(DT[[coin]]$date %in% n_date[1:62]),])\nN = nrow(data_r) # number of observations in the train set\nmatrix_gam = data.table(Abs_return = data_r[[1]],\nDaily = rep(1:period, window),\nWeekly = data_r$week_num)\ngam_1 = gam(Abs_return ~ s(Weekly, bs = \"ps\", k = 7),\ndata = matrix_gam,\nfamily = gaussian)\ngam_cc[[coin]]$Weekly = gam_1\ngam_3 = gam(Abs_return ~ s(Daily, bs = \"cr\", k = period) +\ns(Weekly, bs = \"ps\", k = 7),\npdfname4 = paste0(\"Gam_weekly\", coin, \".pdf\")\npdf(file = pdfname4)\nplot(gam_1, shade = TRUE, xlab = \"Time\", ylab = \"Abs. returns (vola)\")#OK\ndev.off()\npdfname5 = paste0(\"Gam_weekly_daily2D\", coin, \".pdf\")\npdf(file = pdfname5)\nplot(gam_3, shade = TRUE, xlab = \"Time\", ylab = \"Abs. returns (vola)\")#OK\ngam_cc[[coin]]$Daily = gam(Abs_return ~ s(time_num, bs = \"cr\", k = 287),\ndata = DT[[coin]],\npdfname4 = paste0(\"Gam_\", coin, \".pdf\")\nplot(gam_cc[[coin]]$Daily, shade = TRUE, xlab = \"Time\", ylab = \"Abs. returns (vola)\", main = coin, xaxt='n')# OK\naxis(1, seq(1,287, 24), format(as.POSIXlt(rownames( DT[[coin]])[seq(1,287, 24)]), \"%H:%M\"), las = 2 )\n#3D plots\npdfname7 = paste0(\"Gam_3D_abs_ret\", coin, \".pdf\")\npdf(file = pdfname7)\nvis.gam(gam_3, n.grid = 50, theta = 35, phi = 32, zlab = \"\",\nticktype = \"detailed\", color = \"topo\", main = \"Volatility\", xaxt='n')\naxis(1, seq(1,288, 24), format(as.POSIXlt(rownames( DT[[coin]])[seq(1,288, 24)]), \"%H:%M\"), las = 2 )\n", "output_sequence": "None"}, {"input_sequence": "# Install and load packages\nlibraries = c(\"PortfolioAnalytics\", \"PerformanceAnalytics\", \"zoo\",\n\"plotly\", \"plyr\", \"RiskPortfolios\", \"devtools\", \"PMwR\",\"Jmisc\",\n\"FRAPO\", \"R.utils\", \"xts\", \"fastcluster\", \"fPortfolio\",\n\"dplyr\", \"shiny\", \"anytime\", \"ivmte\", \"highfrequency\", \"fBasics\",\n\"tseries\", \"xtable\", \"grDevices\", \"mgcv\", \"tidyr\",\"Hmisc\", \"feather\",\n\"data.table\", \"mgcv\", \"car\", \"ggplot2\", \"grid\", \"animation\", \"tidyverse\",\n\"lubridate\", \"psych\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n", "output_sequence": "None"}, {"input_sequence": "import math\nimport numpy as np\nimport pandas as pd\nimport scipy as sp\nimport numpy.random as npr\nimport scipy.stats as scs\nimport matplotlib.pyplot as plt\nimport numpy.random as npr\nimport datetime\nfrom scipy import optimize\n# SVCJ parameters\nmu = 0.042\nr = mu\nmu_y = -0.0492\nsigma_y = 2.061\nl = 0.0515\nalpha = 0.0102\nbeta = -0.188\nrho = 0.275\nsigma_v = 0.007\nrho_j = -0.210\nmu_v = 0.709\nv0 = 0.19**2\nkappa = 1-beta\nnpr.seed(12345)\ndt = 1/360.0 # dt\nm = int(360.0 * (1/dt)/360.0) # time horizon in days\nn = 1000000\n#trialrun\n#dt = 1/10\nT = m * dt\nw = npr.standard_normal([n,m])\nw2 = rho * w + sp.sqrt(1-rho**2) * npr.standard_normal([n,m])\nz_v = npr.exponential(mu_v, [n,m])\nz_y = npr.standard_normal([n,m]) * sigma_y + mu_y + rho_j * z_v\ndj = npr.binomial(1, l * dt, size=[n,m])\ns = np.zeros([n,m+1])\ns0 = 6500\ns[:,0] = s0 # initial CRIX level, p. 20\nfor i in range(1,m+1):\nv[:,i] = v[:,i-1] + kappa * (theta - np.maximum(0,v[:,i-1])) * dt + sigma_v * sp.sqrt(np.maximum(0,v[:,i-1])) * w2[:,i-1] + z_v[:,i-1] * dj[:,i-1]\ns[:,i] = s[:,i-1] * (1 + (r - l * (mu_y + rho_j * mu_v)) * dt + sp.sqrt(v[:,i-1] * dt) * w[:,i-1]) + z_v[:,i-1] * dj[:,i-1]\nplt.plot(np.transpose(s[:100]));\nplt.xlabel('time step')\nplt.ylabel('asset price process')\nplt.plot(np.transpose(sp.sqrt(v[:70])));\nplt.ylabel('volatility process')\n# Option pricing\ncp = np.exp(-mu * m * dt) * np.maximum(s[:,-1]-k,0).mean()\ndef callprice(S,K,T,sigma,r):\nd1=(sp.log(S/K) + (r + 0.5 * sigma**2)*T) / (sigma * sp.sqrt(T))\nreturn S*scs.norm.cdf(d1) - sp.exp(-r *T) * K * scs.norm.cdf(d2)\n#Implied volatility for given strike\ndef solve(sigma, cp, k, T):\nreturn (callprice(100, k, T, sigma, r) - cp)**2\nresult = sp.optimize.minimize(solve, 0.2, args=(cp, k, T))\n#result.x[0]\ns2 = s\ns = s2/s0 * 100\ndef call_and_vol(K, T): # T maturity in years\nm = int(np.round(T / dt))\ncp = np.exp(-r * m * dt) * np.maximum(s[:,-1]-K,0).mean()\niv = sp.optimize.minimize(solve, 0.4, args=(cp, K,T)).x[0]\nreturn [cp, iv]\n#[call_and_vol(120,0.5), callprice(100, 120, 0.5, 0.98, r)]\nstrike = np.arange(.7, 1.3, 0.1)*100\nprint(len(strike))\nstrike, ttm = np.meshgrid(strike, ttm)\npath = 'option_data.h5'\nh5 = pd.HDFStore(path, 'r')\ndata = h5['data'] # European call & put option data (3 maturities)\n#h5.close()\n#type(data)\n#data.info()\niv = np.zeros(strike.shape)\ndf = pd.DataFrame(columns=data.columns)\ni = 0\ndate = datetime.date(2019, 7, 4)\nfor k in range(strike.shape[1]):\n[x, y] = call_and_vol(strike[t,k], ttm[t,k])\niv[t,k] = y\ndf.loc[i] = [date, strike[t,k], x, date + datetime.timedelta(days=ttm[t,k]*365), 0]\n#compute Black-Scholes calibrated parameter\ndef solve_bs(sigma):\nreturn (callprice(100, df['Strike'], ttm.reshape(77), sigma, r) - df['Call'])**2\nfrom scipy.optimize import root\nimpliedvol = optimize.root(solve_bs,0.2, method='lm').x\nfrom mpl_toolkits.mplot3d import Axes3D\nfig = plt.figure(figsize=(12, 7))\nax = fig.gca(projection='3d') # set up canvas for 3D plotting\nsurf = ax.plot_surface(strike/100, ttm, iv, rstride=3,\ncmap=plt.cm.coolwarm, linewidth=0.5,\nantialiased=True) # creates 3D plot\nax.view_init(30, 30)\nax.set_xlabel('moneyness')\nax.set_ylabel('time-to-maturity')\nfig.colorbar(surf, shrink=0.5, aspect=5);\n", "output_sequence": "Simulation of the Euler-discretized Stochastic Volatility with Correlated Jumps model and Monte Carlo Option Pricing"}, {"input_sequence": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Thu Jan 21 13:12:03 2021\n@author: jane_hsieh\nGoal: Find out the potential anomalies of variable \u2013\"Tran Count\", and visualize the results\nHere we use a person's bank account transaction record, in the case of 'Japan', 'Australia',or 'USA'\nResource:\n1. For simple univariate anomaly detection, we can refer to the following for tutorial:\nhttps://www.ericsson.com/en/blog/2020/4/anomaly-detection-with-machine-learning\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\n# ==================================== -1. Download data ====================================================\n# Please download data from https://box.hu-berlin.de/d/0b7431e9ebd64a8487bd/;\n# The download link is password rotected. Please ask Prof. H\u00e4rdle for password;\n# Create a folder called 'Data' and put the downloaded xlsx to the folder.\ndata_dir = './Data'\n#change your current working directory\n# os.chdir('/Users/jane_hsieh/OneDrive - nctu.edu.tw/Data Science Analysis Templates/Anomaly Detection/Univariate Anomaly Detection')\n# os.getcwd()\n# parameters -----------------------------------#-\ncountry = 'Japan' 'Australia', 'USA' #-\n# ----------------------------------------------#-\n# ==================================== 0. Input data: FRM prices ====================================================\ndf = pd.read_excel(data_dir+'/Overseas spend(WH).xlsx', sheet_name=country, parse_dates=['CPD Date'], index_col = 'CPD Date')\n#0.1 Reorganize data\ndf.sort_index(inplace=True)\nprint('The earliest date of data is:\\t', df.index.min())\n### reset Datetime index from earliest to latest date (daily), in case of missing dates\ndf = df.reindex(pd.date_range(df.index.min(), df.index.max(), freq = 'D'))\n### Check if any missing date (missing data)\nprint('Check if any missing data in each variable:\\n',df.isnull().sum())\n'''\nFortunately, there's no missing data! Now we have a series of data from 2017-11-01 to 2019-06-30 '\n#df['Tran Count'].plot(figsize=(30,10))\n#plt.show()\n# ==================================== 1. Anomaly Detection: Find Anomalies Using Mean +/- 1.96*SD ====================================================\n# 1.1 Calculate the past statistics (mean, std), with a window size (i.e., past window days [including current day] of data would be summarized)\n# Note: to reduce lag effect, instead of using 'past window', you can use the window centored at current day (center=True)\nwindow = 31 #-\ncentered = False #True # or False #-\nthreshold = 1.96 #-\n\nrolling_stats = df['Tran Count'].rolling(window, center=centered).agg(['mean', 'std']) #<<<<<<<<<<<<<<<<<<<<<<<\ndf2 = df.join(rolling_stats) #combine the column-wise data into df\ndf2['Upper_SD'] = df2['mean'] + threshold * df2['std'] #upper bound of 95% confidence interval\n## Find possiblee anomalies by Mean +/- 1.96*SD\ndef Is_Anomaly(x):\nif pd.isna(x[['Tran Count', 'mean','std']]).sum() >0:\nreturn np.nan\nz = abs(x['Tran Count'] - x['mean'])/x['std']\nif z > threshold:\nreturn 1 # outlier\nelse:\nreturn 0 # normal\n\nanomalies = df2.apply(Is_Anomaly, axis=1)\nanomalies.name = 'anomalies'\nprint('The percentage of anomalies in data is {:.2f}%'.format(np.mean(anomalies)*100))\ndf2 = df2.join(anomalies)\n# 1.2. Visualization of the results -----------------------------------------------------------------\nanomalies = df2[df2['anomalies']==1]\nfig, ax = plt.subplots(figsize=(30,10))\nax.plot(df2.index, df2['Tran Count'], linestyle = '-', color='b', label='Tran Count')\nax.scatter( anomalies.index, anomalies['Tran Count'], color = 'r' )\n#legend = ax.legend(loc=\"upper right\", edgecolor=\"black\")\n#legend.get_frame().set_alpha(None)\n#legend.get_frame().set_facecolor((0, 0, 0))\n#ax.set_title(f'{country} - TS plot detecting anomalies with windowsize {window} (center={str(centered)})')\nplt.show()\nplt.savefig(f'{country} - TS plot detecting anomalies with windowsize {window} (center={str(centered)}).png', transparent=True) #<<<<<<<<<<<<<<<<<<<<<<<\nMean and SD can change drastically due to extreme values (possible anomalies)\nTo reduce the impact of extreme values, we may use Median and Median-absolute-deviation (MAD),\ninsted of mean and std, as standards for detecting anomalies\n# ==================================== 2. Anomaly Detection: Find Anomalies Using Median +/- 1.96*MAD ====================================================\n# 1.1 Calculate the past statistics (median, MAD), with a window size (i.e., past window days [including current day] of data would be summarized)\nfrom scipy import stats\n#x = df[:31]\n#stats.median_abs_deviation(x['Tran Count'])\nrolling_mdn = df['Tran Count'].rolling(window, center=centered).median()\nrolling_mdn.name = 'median'\nrolling_MAD = df['Tran Count'].rolling(window, center=centered).apply(stats.median_abs_deviation)\nrolling_MAD.name = 'MAD'\ndf3 = df.join([rolling_mdn, rolling_MAD]) #combine the column-wise data into df\ndf3['Upper_MAD'] = df3['median'] + threshold * df3['MAD'] #upper bound of robust 95% confidence interval\ndef Is_Anomaly_MAD(x):\nif pd.isna(x[['Tran Count', 'median','MAD']]).sum() >0:\nz = abs(x['Tran Count'] - x['median'])/x['MAD']\nanomalies = df3.apply(Is_Anomaly_MAD, axis=1)\ndf3 = df3.join(anomalies)\nanomalies = df3[df3['anomalies']==1]\nax.plot(df3.index, df3['Tran Count'], linestyle = '-', color='b', label='Tran Count')\nplt.savefig(f'{country} - TS plot detecting robust anomalies with windowsize {window} (center={str(centered)}).png', transparent=True) #<<<<<<<<<<<<<<<<<<<<<<<\n", "output_sequence": "Simulation of the Euler-discretized Stochastic Volatility with Correlated Jumps model and Monte Carlo Option Pricing"}, {"input_sequence": "import matplotlib.pyplot as plt\nimport numpy as np\nimport math\nfrom google.colab import files\nx = np.linspace(-4,4,2000)\n#g = 0 and h = 0.1\nh1 = 0.1\nT1=np.exp(h1*x**2/2)*x\nplt.plot(x, T1, c = \"magenta\")\nplt.plot(x,x, linestyle = \"dashed\", color = \"black\")\nplt.xlabel(\"Standard Normal Quantiles\")\nplt.ylim((-4,4))\nplt.xticks(range(-4, 5, 1))\nplt.gca().set_aspect('equal', adjustable='box')\nplt.savefig(\"g0h0_1.png\")\nfiles.download(\"g0h0_1.png\")\nplt.show()\n#g = -0.1 and h = 0.3\nh1 = 0.3\nT1=(1/g1)*np.exp(h1*x**2/2)*(np.exp(g1*x)-1)\nplt.plot(x, T1, c = \"blue\")\nplt.savefig(\"gn0_1h0_3.png\")\nfiles.download(\"gn0_1h0_3.png\")\n#g = 0.2 and h = 0.2\nh1 = 0.2\nplt.plot(x, T1, c = \"black\")\nplt.savefig(\"g0_2h0_2.png\")\nfiles.download(\"g0_2h0_2.png\")\n#g = 0.5 and h = 0.5\ng1 = -0.5\nplt.plot(x, T1, c = \"red\")\nplt.savefig(\"gn0_5h0_3.png\")\nfiles.download(\"gn0_5h0_3.png\")\n", "output_sequence": "TukeyQuantilePlot produces plots displaying the effect on quantiles after the tukey g- and h- transformation"}, {"input_sequence": "###########################################################\n\nK = function (u)\n{\nkk=length(u);\nret=vector(, length=kk);\nfor (i in c(1:kk))\n{\nret[i]=dnorm(u[i]); # Gaussian\n# if (abs(u[i])>1) {ret[i]=0;} else {ret[i]=(3*(1-u[i]^2)/4) }; # Epanechnikov\n}\nreturn(ret);\n}\nK.h = function (u, h)\nfor (i in c(1:kk)){ret[i]=K(u[i]/h)/h;}\n###########################################################\nrookley = function(money, sigma, r, tau)\nrm=length(money);\nsqrttau=sqrt(tau);\nexprt=exp(r*tau);\nrtau=r*tau;\nd1=(log(money)+tau*(r+0.5*sigma^2))/(sigma*sqrttau);\nd2=d1-sigma*sqrttau;\n\nd11 = 1/(money*sigma*sqrttau)+(-(log(money)+tau*r)/(sqrttau*sigma^2)+0.5*sqrttau)*sigma1;\n\nd21 = 1/(money*sigma*sqrttau)+(-(log(money)+tau*r)/(sqrttau*sigma^2)-0.5*sqrttau)*sigma1;\nd12 = -1/(sqrttau*money^2*sigma)-sigma1/(sqrttau*money*sigma^2)+ sigma2*(0.5*sqrttau-(log(money)+r*tau)/(sqrttau*sigma^2))+ sigma1*(2*sigma1*(log(money)+r*tau)/(sqrttau*sigma^3)-1/(sqrttau*money*sigma^2));\nf = pnorm(d1)-pnorm(d2)/(money*exprt);\nf1=dnorm(d1)*d11-dnorm(d2)*d21/(exprt*money)+pnorm(d2)/(exprt*money^2);\nf2=dnorm(d1)*d12-d1*dnorm(d1)*d11^2-dnorm(d2)*d22/(exprt*money)+dnorm(d2)*d21/(exprt*money^2) + d2*dnorm(d2)*d21^2/(exprt*money)-2*pnorm(d2)/(exprt*money^3)+dnorm(d2)*d21/(exprt*money^2);\n#cat(\"d1 \", d1[ngrid],\" d2 \",d2[ngrid],\" d11 \", d11[ngrid],\" d12 \", d12[ngrid],\" d21 \",d21[ngrid],\" d22 \",d22[ngrid])\nreturn(cbind(f, f1, f2));\n######################### kernel functions\nK.prime = function (u)\nret[i]=-u[i]*dnorm(u[i]); # Gaussian\n# if(abs(u[i])>1) {ret[i]=0} else {ret[i]=(-3*u[i]/2)}; # Epanechnikov\nKjp = function (u, j, N)\nkk=length(u);\nret=vector(, length=kk);\nfor (i in c(1:kk))\n{\nM = N;\nM[,j] = u[i]^{c(0:(dim(N)[1]-1))};\nif(abs(u[i])>1) {ret[i]=0;} else {ret[i]= K(u[i])*det(M)/det(N); };\n}\nreturn(ret);\n######################################## N, Q, C matrices\n#p=2;h=0.65; alpha=0.05;\nfunc.L = function(p, h, alpha)\nN = matrix(0, nrow=p+1, ncol=p+1);\nC = vector(, length=p+1);\nfor(i in c(0:p))\nif ((i>1) && (j>1)) {Q[i+1,j+1]= integrate(function (xx) xx^(i+j)*(K.prime(xx))^2, -Inf,Inf)$value- 0.5 *(i*(i-1)+j*(j-1))* integrate(function (x) x^(i+j-2)*(K(x))^2, -Inf,Inf)$value;}\nif ((i<=1) && (j<=1)) {Q[i+1,j+1]= integrate(function (xx) xx^(i+j)*(K.prime(xx))^2, -Inf,Inf)$value;}\nif ((i<=1) && (j>1)) {Q[i+1,j+1]= integrate(function (xx) xx^(i+j)*(K.prime(xx))^2, -Inf,Inf)$value- 0.5 *j*(j-1)* integrate(function (x) x^(i+j-2)*(K(x))^2, -Inf,Inf)$value;}\nN[i+1,j+1]= integrate(function (xx) xx^(i+j)*K(xx), -1,1)$value;\nN.i = solve(N);\nfor(j in c(0:p))\n{\nC[j+1]= (N.i%*%Q%*%N.i)[j+1,j+1] / integrate(function (xx) (Kjp(xx, j, N))^2, -Inf,Inf)$value;\nL[j+1] = factorial(j)* (n*h^(2*j+1))^(-0.5) * ( (-2*log(h))^(1/2) + (-2*log(h))^(-1/2)*(-log(-0.5*log(1-alpha)) + log(sqrt(C[j+1])/(2*pi))));\nprint(C);\nreturn(L);\n######################################### asymptotic bands\nbands = function(p, h, hat.theta, x, y, vx)\nL = func.L(p, h, 0.05);sigma2=1;\nup.band= matrix(0,nrow=length(vx), ncol=p+1);\nfor(i in c(1:length(vx)))\nxx=vx[i];\nB.V = matrix(0, nrow=p+1, ncol=p+1);\nfor(j in c(1:length(x)))\n#cat(\"ditheta\", dim(hat.theta),\"length(vx)\",length(vx),\"length(vx)\",length(x),\"length(y)\", length(y), length(x),\"i\",i,\"j\",j,\"\\n\");\nB.V = B.V + K.h(x[j]-xx,h) * (1/sigma2) * (solve(diag(h^c(0:p))) %*% t(t((x[j]-xx)^c(0:p))) %*% t((x[j]-xx)^c(0:p)) %*% solve(diag(h^c(0:p))));\nB.V=B.V/n;\nB.i = solve(B.V);\nV = B.i%*%K.V%*% B.i;\nV.all[i,]=diag(V);\n\nfor(ip in c(1:(p+1)))\nlo.band[i,ip] = - L[ip] * sqrt(V[ip, ip]);\n}\nmylist=list(L=L, V=V.all, lo.band=lo.band,\nreturn(mylist);\n", "output_sequence": "Pricing kernels implicit in option prices play a key role in assessing the risk aversion over equity returns. We deal with nonparametric estimation of the pricing kernel (PK) given by the ratio of the risk-neutral density estimator and the historical density (HD). The former density can be represented as the second derivative w.r.t. the European call option price function, which we estimate by nonparametric regression. HD is estimated nonparametrically too. In this framework, we develop the asymptotic distribution theory of the Empirical Pricing Kernel (EPK) in the L\u221e sense. Particularly, to evaluate the overall variation of the pricing kernel, we develop a uniform confidence band of the EPK. Furthermore, as an alternative to the asymptotic approach, we propose a bootstrap confidence band. The developed theory is helpful for testing parametric specifications of pricing kernels and has a direct extension to estimating risk aversion patterns. The established results are assessed and compared in a Monte-Carlo study. As a real application, we test risk aversion over time induced by the EPK."}, {"input_sequence": "setwd(\"d:/papers/epk/program/\")\n\nsource(\"EPK_library.r\")\nngrid = 200;\nbandwidth = 0.08;\ntau = 0.04722;\nread.dates = FALSE;\n#all = read.table(\"tick2006.txt\")\n#day1 = all[all[,1]==\"28-02-2006\",];\n#write.table(day1, file=\"28-02-2006.txt\", row.names=F, quote=F)\nXX = read.table(\"tick2006.txt\");\n#XX = XX[c(2,1,3,5,6,8,7,4)];\n#XX = cbind(rep(\"17-01-2001\", length=dim(XX)[1]), XX)\ncat(\"reading finished ....\\n\");\nif (read.dates==T)\n{\ndate = as.vector(XX[,1]);\ndate.dif = date[1];\nflag=1;\nwhile(flag==1)\n{\ndate.part = date.part[date.part!=date.dif[length(date.dif)]];\ncat(\"current date ... \", date.dif[length(date.dif)], \"\\n\");\nif (length(date.part>0)) {date.dif=c(date.dif, date.part[1]);} else {flag=0;}\n}\nwrite.table(date.dif, file=\"tick2006_dates.txt\", row.names=F, quote=F);\n} else { date.dif= read.table(\"tick2006_dates.txt\"); date.dif=date.dif[,1];}\n#############################################\n#for( iday in c(1:length(date.dif)))\niday=42;\nday1 = XX[XX[,1]==date.dif[iday],];\nday1 = day1[day1[,2]>0,];\ntau = day1[1,4];\n#name = \"28-02-2006\";\n#day1=read.table(paste(name,\".txt\", sep=\"\"));\nday1.mat = day1[day1[,4]== tau,];\nday1.call = day1.mat[day1.mat[,3]==1,];\n# compute the moneyness\nday1.call[,9]=day1.call[,7]/day1.call[,5];\n# only out and at the money options\nday1.call = day1.call[day1.call[,9]>=1,];\n# put to calls\nput2call = day1.put;\nput2call[,6] = put2call[,6]+ - put2call[,5]*exp(- mean(put2call[,8])* tau);\nput2call = put2call[order(put2call[,5]),];\ndata = rbind(put2call,day1.call);\ndata = data[(data[,2]>0.05),];\n# no - arbitrage condition\ndata = data[ ((data[,6]<data[,7]) && (data[,6]>data[,7]-data[,5]*exp(-data[,8]*data[,4])) ),]\n#write.table(data, file=paste(name,\"_cleaned.txt\", sep=\"\"), row.names=F, quote=F)\nn = dim(data)[1];\nprice.median = median(data[,7]);\n## regression for volatility\nvolas = data[,c(2,9)];\nnsample = dim(volas)[1];\nmon.min = min(volas[,2]);mon.max = max(volas[,2]);\nvolas[,2] = (volas[,2]-mon.min)/(mon.max-mon.min);\nmon.grid = seq(1/ngrid, 1, length=ngrid);\nmon.grid.scaled = (mon.min+mon.grid*(mon.max-mon.min));\nfor(i in c(1:ngrid))\nX = cbind(rep(1, length=nsample), volas[,2]-mon.grid[i],\nW = diag(K.h(volas[,2]-mon.grid[i], bandwidth));\nbeta = t(solve(t(X) %*% W %*% X) %*% t(X) %*% W %*% volas[,1]);\nif (i==1) {sigmas=beta[c(1:3)];} else {sigmas=rbind(sigmas, beta[c(1:3)]);}\n}\nsigmas[,3] = 2*sigmas[,3];\n#### bands for the derivatives\nbands.init = bands(2, bandwidth, sigmas, volas[,2], mon.grid);\n#### applying rookley\nfder = rookley(mon.grid.scaled, sigmas[,1], sigmas[,2]/(mon.max-mon.min), mean(data[,8]), tau);\n#### final computations\nstrike.grid = price.median / mon.grid.scaled;\nd2fdX2 = (mon.grid.scaled^2*fder[,3]+2*mon.grid.scaled*fder[,2])/strike.grid^2;\n#### bands for SPD\nd1=(log(mon.grid.scaled)+tau*(mean(data[,8])+0.5*(sigmas[,1])^2)) / (sqrt(sigmas[,1])*sqrt(tau));\nd2=d1-sqrt(sigmas[,1])*sqrt(tau);\ndgds= exp(-mean(data[,8])*tau) * ( mon.grid.scaled^2 * dnorm(d1) / strike.grid^2 - exp(-mean(data[,8])*tau)*dnorm(d2)/mon.grid.scaled);\nband.limit = bands.init$L[3] * sqrt(bands.init$V[,3]) * dgds ; #* exp(-mean(data[,8])*tau);\nSPD = new.env();\nSPD$SPD = price.median^2 * exp(mean(data[,8])*tau) * d2fdX2 / mon.grid.scaled;\nSPD=as.list(SPD);\ndax=read.table(\"dax_index.dat\"); dax=dax[,2];\ndax=dax[c((length(dax)-500):length(dax))];\ndax.scaled = 1+log(dax[c(ceiling(tau*31/0.0833):length(dax))]/dax[c(1:(length(dax)+1-ceiling(tau*31/0.0833)))]);\nkern=density(dax.scaled, bw=\"nrd0\", from=mon.grid.scaled[1], n=ngrid);\n########### Black-Scholes\nsigma.BS = mean((volas[volas[,1]<quantile(volas[,1], probs=0.75),])[,1]);\nq.BS = exp(-(log(mon.grid.scaled) - (mean(data[,8])-sigma.BS/2)*tau)^2/(2*tau*sigma.BS))/(mon.grid.scaled*sqrt(2*3.1415926*sigma.BS*tau));\np.BS = dnorm(log(mon.grid.scaled), mean=mean(log(dax.scaled)), sd = sd(log(dax.scaled)))/mon.grid.scaled;\n########### plotting\npdf(paste(\"q_\",date.dif[iday],\".pdf\", sep=\"\"), width=10, height=6, onefile=F);\nmatplot(mon.grid.scaled, cbind(SPD$SPD, SPD$lo, q.BS), type=\"l\",lty=1, lwd=c(2,1,1,2),col=c(2, \"blue3\", \"green3\"), xlab=\"moneyness\", ylab=\"q(K)\", cex=1.5, main=paste(date.dif[iday], \" tau = \", tau, \"\\n\"), xlim=c(.95, 1.15), ylim=c(0,20));\ndev.off()\npdf(paste(\"EPK_\",date.dif[iday],\".pdf\", sep=\"\"), width=10, height=6, onefile=F);\nmatplot(mon.grid.scaled, cbind(cbind(SPD$SPD, SPD$lo, SPD$up)/kern$y, q.BS/p.BS), type=\"l\",lty=1, lwd=c(2,1,1,2),col=c(2, \"blue3\", \"green3\"), xlab=\"moneyness\", ylab=\"EPK\", cex=1.5, main=paste(date.dif[iday], \" tau = \", tau, \"\\n\"), xlim=c(.95, 1.15), ylim=c(0,10));\npdf(paste(\"p_\",date.dif[iday],\".pdf\", sep=\"\"), width=10, height=6, onefile=F);\nmatplot(mon.grid.scaled, cbind(kern$y, p.BS), type=\"l\",lty=1, lwd=c(2,2),col=c(2, \"green3\"), xlab=\"moneyness\", ylab=\"q(K)\", cex=1.5, main=paste(date.dif[iday], \" tau = \", tau, \"\\n\"), xlim=c(.95, 1.15), ylim=c(0,10));\nmod = lm(log(SPD$SPD)~ log(mon.grid.scaled));\nutil.coef = mod$coef;\nif (exists(\"util.param\")==0) {util.param = c(exp(util.coef[1]), -util.coef[2], cor(log(SPD$SPD), log(mon.grid.scaled))^2);\n\n} # end of the days\n", "output_sequence": "Pricing kernels implicit in option prices play a key role in assessing the risk aversion over equity returns. We deal with nonparametric estimation of the pricing kernel (PK) given by the ratio of the risk-neutral density estimator and the historical density (HD). The former density can be represented as the second derivative w.r.t. the European call option price function, which we estimate by nonparametric regression. HD is estimated nonparametrically too. In this framework, we develop the asymptotic distribution theory of the Empirical Pricing Kernel (EPK) in the L\u221e sense. Particularly, to evaluate the overall variation of the pricing kernel, we develop a uniform confidence band of the EPK. Furthermore, as an alternative to the asymptotic approach, we propose a bootstrap confidence band. The developed theory is helpful for testing parametric specifications of pricing kernels and has a direct extension to estimating risk aversion patterns. The established results are assessed and compared in a Monte-Carlo study. As a real application, we test risk aversion over time induced by the EPK."}, {"input_sequence": "# clear cache and close windows\ngraphics.off()\nrm(list=ls(all=TRUE))\n# define eight points\neight = cbind(c(-3,-2,-2,-2,1,1,2,4),c(0,4,-1,-2,4,2,-4,-3))\neight = eight[c(8,7,3,1,4,2,6,5),]\ndev.new()\n# plot eight points using average linkage\npar(mfrow = c(1, 2))\nplot(eight,type=\"n\", xlab=\"price conciousness\",ylab=\"brand loyalty\",main=\"8 points\", xlim=c(-4,4))\n# agglomarate first two nearest points\nsegments(eight[5,1],eight[5,2 ],eight[3,1 ],eight[3,2],lwd=2,\"black\")\n# add third point via average\nsegments(eight[3,1],eight[3,2 ],eight[4,1 ],eight[4,2],lwd=2,lty=2,col=\"darkgrey\")\n# agglomerate second two nearest points\nsegments(eight[7,1],eight[7,2 ],eight[8,1 ],eight[8,2],lwd=2,\"black\")\n# agglomarate third two nearest points\nsegments(eight[1,1],eight[1,2 ],eight[2,1 ],eight[2,2],lwd=2,\"black\")\n# add point six to second subcluster via average\nsegments(eight[8,1],eight[8,2 ],eight[6,1 ],eight[6,2],lwd=2,lty=2,col=\"darkgrey\")\n# compute subcluster between 345 and 678 via average\nsegments(eight[4,1],eight[4,2],eight[6,1],eight[6,2],lwd=2,lty=3,col=\"skyblue\")\n# compute in case of merging of two last clusters:\nsegments(eight[2,1],eight[2,2 ],eight[6,1 ],eight[6,2],lwd=2,lty=3,col=\"lightgreen\")\npoints(eight, pch=21, cex=2.6, bg=\"white\")\ntext(eight,as.character(1:8),col=\"red3\", cex=1)\nplot(hclust(dist(eight,method=\"euclidean\")^2,method=\"average\"),ylab=\"squared Euclidean distance\",sub=\"\",xlab=\"\",main=\"average linkage dendrogram\")\n", "output_sequence": "Employs the average linkage using squared Euclidean distance matrices to perform a cluster analysis on an 8 points example"}, {"input_sequence": "rm(list = ls(all = T))\ngraphics.off()\ny = c(9, 11, 7, 11, 7, 15, 9, 14, 15)\na = rep(0, times = 10)\nx2 = c(a, b, a)\nlm1 = lm(y ~ x2 + x3) # linear model y = b0 + b1*x2 + b2*x3\nsummary(lm1)\nanova(lm1)\n", "output_sequence": "Estimates a linear model (ANOVA). The plot presents results of the estimation of a linear model"}, {"input_sequence": "# install.packages('decon')\nlibrary(decon)\nn = 3000\nx = rnorm(n, 3, 2)\nsig = 0.9\nu = sig * rnorm(n)\nw = x + u\ne = rnorm(n, 0, 0.1)\ny = 5 * x^2 + e\nbw1 = bw.dboot1(w, sig)\nm = DeconNpr(w, sig, y, error = \"normal\", from = min(x), to = max(x))\nplot(m, col = \"blue\", cex.axis = 1.2, font.axis = 1, cex.lab = 1.2, las = 1, lwd = 4, lty = 2, xlab = expression(paste(psi)),\nylab = expression(paste(m(psi))), main = \"\")\nks_uncon = ksmooth(x, y, kernel = \"normal\", 2, range.x = c(min(x), max(x)))\nlines(ks_uncon, col = \"black\", lwd = 4, lty = 1)\nks_con = ksmooth(w, y, kernel = \"normal\", 2, range.x = c(min(x), max(x)))\nlines(ks_con, col = \"red\", lwd = 4, lty = 3)\n", "output_sequence": "Plots the deconvoluted kernel regression curve, the kernel regression curve from the sample without measurement errors (i.e. kernel regression based on x) and the kernel regression curve from the sample with measurement errors (i.e. kernel regression based on z)."}, {"input_sequence": "# % ------------------------------------------------------------------------- % Book: MSE %\n# ------------------------------------------------------------------------- % Description: MSEnonpara1 generate n=300 t(3)\n# distributed random sample, and draws the kernel density curve of the estimated kernel density function using a % Gaussian\n# kernel. Since the kernel density function is biased in a finite sample, one can not compare it with the true density\n# directly. One rather compare it with the expectation of the kernel density function under the true density. Then it draws\n# the expectation of the kernel density function under the true density. %\n# ------------------------------------------------------------------------- % Usage: - %\n# ------------------------------------------------------------------------- % Output: 1. Draws n_1 observations from t(3)\n# distribution % and plots the kernel density curve of the estimated kernel density function using a % Gaussian kernel. %\n# 2. generate n observations from t(3) distribution and plots the expectation of the kernel density function under the true\n# density which is t(3) distribution. % 3. generate n observations from normal distribution with mean and standard deviation\n# from the generated sample, plots the expectation of the kernel density function under the true density which is normal\n# distribution with mean and standard deviation from the generated sample. %\n# ------------------------------------------------------------------------- % Example: n_1 = 300 % n_1 = 3000 %\nn_1 = 300\nobs = rt(n_1, 3)\nden_x = density(obs, kernel = \"gaussian\", bw = \"nrd0\")\nplot(den_x, lwd = 4, lty = 1, col = \"blue\", cex.axis = 1.2, font.axis = 1, cex.lab = 1.5, las = 0, xlab = \"x\", ylab = \"f(x)\",\nmain = \"\")\nn = 1e+06\nz = rt(n, 3)\nh = den_x$bw\ng = 1:512\nfor (j in 1:length(den_x$x)) {\nsum_k = 0\nfor (i in 1:length(z)) {\nu = (den_x$x[j] - z[i])/h\nk = exp(-u^2/2)/sqrt(2 * pi)\nsum_k = sum_k + k\n}\ng[j] = 1/(h * length(z)) * sum_k\n}\nlines(den_x$x, g, lwd = 3, lty = 2, col = \"red\")\nmu = mean(obs)\nsd = sd(obs)\nq = rnorm(n, mu, sd)\nf = 1:512\nfor (i in 1:length(q)) {\nu = (den_x$x[j] - q[i])/h\nf[j] = 1/(h * length(z)) * sum_k\nlines(den_x$x, f, lwd = 4, lty = 3, col = \"black\")\n", "output_sequence": "Generates an n=300 t(3) distributed random sample, and draws the kernel density curve of the estimated kernel density function using a Gaussian kernel, since the kernel density function is biased."}, {"input_sequence": "close all\nclear all\nclc\ndisp('Please input number of draws n as') ;\ndisp(' ') ;\nn = input('[n]=');\ny = normrnd(0,1,n,1); % Generate standard normal random numbers\ny = sort(y);\ncdfplot(y) % Plot the empirical distribution function\nhold on\nf = cdf('Normal',y,0,1); % Generate normal cumulative distribution function\nplot(y,f,'r','LineWidth',2.5)\nlegend('Empirical','Theoretical','Location','NW')\ntitle('EDF and CFD')\nxlabel('X')\nylabel('EDF(X), CDF(X)')\ngrid off\n[g,y] = ecdf(y);\ng = g(2:(n+1));\n[C,I] = max(abs(f-g))\n", "output_sequence": "Draws n observations from standard normal distribution and plots its empirical distribution function (edf) vs. the normal cumulative distribution function (cdf). Number of draws can be entered interactively. The point where the edf and cdf differs most is also reported."}, {"input_sequence": "# clear history\nrm(list = ls(all = TRUE))\ngraphics.off()\n# install/load packages\nlibraries = c(\"foreign\", \"stats\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\nDAXall = read.csv(\"DAX20000101-20111231.csv\")\ntime = seq(from = as.Date(\"2000-01-03\"), to = as.Date(\"2011-12-27\"), length.out = 626)\nDAX = DAXall[1:626, 8]\nplot(DAX ~ time, xlab = \"Time\", ylab = \"DAX30 Weekly Return\", type = \"l\", pch = 20, cex = 1, cex.axis = 1.2, cex.lab = 1.5,\nlab = c(3, 3, 0), main = \"\", col = \"blue4\")\nDAX1 = DAXall[2:209, 8]\nvar.test(DAX1, DAX2)\n", "output_sequence": "Plots the DAX returns from 2000 to 2011 and performs an F-test for variance comparison of specific time periods. From the plot a period of higher volatility is followed by a period of lower volatility and again followed by a period of higher volatility. To proof this finding, the time series is split into 3 intervals, each of which is tested against the other for variance comparison. The variances of the first and the third period differ significantly from the second period."}, {"input_sequence": "## clear history\nrm(list = ls(all = TRUE))\ngraphics.off()\n## Q-Q-Normal-Plots mit Sollgeraden\n## Mit festem seed zur Reproduktion\nset.seed(1234)\nw = rnorm(100)\nx = rnorm(100, mean = 5)\npar(mfrow = c(2, 2))\nqqnorm(w, main = \"Normal Q-Q-Plot der Stichprobe w\", xlab = \"theoretische Standardnormal-Quantile\",\nylab = \"empirische Quantile\")\nqqline(w)\nqqnorm(x, main = \"Normal Q-Q-Plot der Stichprobe x\", xlab = \"theoretische Standardnormal-Quantile\",\nqqline(x)\nqqnorm(y, main = \"Normal Q-Q-Plot der Stichprobe y\", xlab = \"theoretische Standardnormal-Quantile\",\nqqline(y)\nqqnorm(z, main = \"Normal Q-Q-Plot der Stichprobe z\", xlab = \"theoretische Standardnormal-Quantile\",\nqqline(z)\n", "output_sequence": "Produces the QQ plots for four simulated samples of N(0,1), N(5,1), N(0,9) and N(5,9). QQ-plots compare empirical quantiles of a distribution with theoretical quantiles of the standard normal distribution."}, {"input_sequence": "## clear history\nrm(list = ls(all = TRUE))\ngraphics.off()\n## install and load packages\nlibraries = c(\"QRM\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\npar(mfrow = c(1, 2))\nplot(rcopula.clayton(500, 3, 2), pch = 19, ann = F, cex = 0.6, cex.axis = 0.6)\n", "output_sequence": "Produces the plots of evaluating denisities of Archimedean copulae under (size=500, size of parameters=3, dimension=2) and (size=500, size of parameters=6, dimension=2)."}, {"input_sequence": "#### Two-Learner Approach with Logit as its base Learner ####\n# Includes the standard version of the logit two-learner and\n# the double robust version\nlibrary(rpart)\n#### Two-model Modified Outcome Logit ####\nMOM_Logit <- function(X, y, W, prop_score){\ndata <- data.frame(X,\".y\"=y)\n\n# Train model for W0\n# logit0 <- glm(.y~., family=binomial(link='logit'),\n# data = data[W==0,])\n#\n# Train model for W1\n# logit1 <- glm(.y~., family=binomial(link='logit'),\n# data = data[W==1,])\n# mu0 <- predict(logit0, X, type='response')\n# Train model for W0\nlogit0 <- lm(.y~., data = data[W==0,])\n# Train model for W1\nlogit1 <- lm(.y~., data = data[W==1,])\nmu0 <- predict(logit0, X)\ny_dr <- mu1 - mu0 + (W/prop_score)*(y-mu1) - ((1-W)/(1-prop_score))*(y-mu0)\ntau_reg <- lm(.y~., data=data.frame(\".y\"=y_dr,X))\nreturn(tau_reg)\n}\n#### Two-model Logit ####\nT_Logit <- function(X, y, W, prop_score=NULL){\nif(is.null(prop_score)){\nweights0 = NULL\n}else{\nweights0 = (1/(1-prop_score[W==0]))\n}\nlogit0 <- glm(.y~., family=binomial(link='logit'),\nweights = weights0,\ndata = data[W==0,])\nlogit1 <- glm(.y~., family=binomial(link='logit'),\nweights = weights1,\ndata = data[W==1,])\nt_logit <- list(model0=logit0, model1=logit1)\nclass(t_logit) <- c(\"tlearner\")\nreturn(t_logit)\n#### Two-model Reg ####\nT_Regression <- function(X, y, W, prop_score=NULL){\nreg0 <- lm(.y~.,\nweights = weights0,\ndata = data[W==0,])\nreg1 <- lm(.y~.,\nt_reg <- list(model0=reg0, model1=reg1)\nclass(t_reg) <- c(\"tlearner\")\nreturn(t_reg)\n# T-Tree\nT_rpart <- function(X, y, W, prop_score=NULL, method='class'){\nmodel0 <- rpart(.y~., data = data[W==0,],\nweights = weights0, method=method)\nmodel1 <- rpart(.y~., data = data[W==1,],\nmethod=method)\nt_learner <- list(model0=model0, model1=model1)\nclass(t_learner) <- c(\"tlearner\")\nreturn(t_learner)\n# Two-Model nnet\nT_NNet <- function(X, y, W, prop_score=NULL, trace=FALSE){\nmodel0 <- nnet(.y~., size=ncol(X),\ndata = data[W==0,],\nentropy=TRUE, maxit=500, MaxNWts=5000, decay=0.001, reltol = 1e-6, trace=trace)\nmodel1 <- nnet(.y~., size=ncol(X),\nweights = weights1,\ndata = data[W==1,],\nentropy=TRUE, maxit=500, MaxNWts=5000, decay=0.001, reltol= 1e-6, trace=trace)\n# Prediction function for two-model class\npredict.tlearner <- function(object, newdata, ...){\npred_diff <- predict(object$model1, newdata, ...) -\nif(!is.null(ncol(pred_diff))){\npred_diff <- pred_diff[,ncol(pred_diff)]\nreturn(pred_diff)\npredict(object$model0, newdata, ...)\n#### Two-Model Logit Double Robust ####\nT_Logit_DR <- function(X, y, W, prop_score=NULL){\ndata <- data.frame(X,\"y\"=y)\nprop_score = rep(0.5, nrow(data))\ny0 <- glm(y~., family=binomial(link='logit'),\ndata = data[W==0,])\ny0_hat <- predict(y0,X, type=\"response\")\ny1 <- glm(y~., family=binomial(link='logit'),\ndata = data[W==1,])\ny1_hat <- predict(y1,X,type=\"response\")\ny_mo <- (y1_hat - y0_hat) + (W*(y-y1_hat))/prop_score - ((1-W)*(y-y0_hat)/(1-prop_score))\nt_logit_dr <- glm(y_mo~.,data= cbind(X,\"y_mo\"=y_mo) )\nreturn(t_logit_dr)\n", "output_sequence": "Supervised randomization integrates the targeting model into randomized controlled trials and A/B tests as a stochastic policy. By replacing random targeting with supervised randomization, we reduce the costs of running randomized experiments or allow continuous collection of randomized trial data. We show how to fully correct downstream analysis for the bias effect by the supervised treatment allocation with the true probabilites to receive treatment, which are logged at the time of targeting."}, {"input_sequence": "# #### Static Quantile-based Mapping ####\n# map_propensity <- function(model_score, target_ratio, groups=9){\n# Platt scaling\n# platt_scaler <- glm(y~prob, family=binomial(link='logit'),\n# weights = ifelse(exp$none$y==1, 1/(mean(exp$none$y==1)), 1),\n# data = data.frame(cbind('y'=exp$none$y,'prob'=churn_pred))\n# )\n# treat_prob <- predict(platt_scaler, newdata=data.frame(prob=churn_pred), type='response')\n# Cut into groups based on the score quantiles\n# score_quantiles <- quantile(model_score,seq(0,1,1/groups))\n#\n# if(length(score_quantiles) != length(unique(score_quantiles))){\n# warning(paste(\"Reduced number of groups to\", length(score_quantiles)-1, \"because scores between groups were constant\"))\n# }\n# model_score <- cut(model_score,breaks=score_quantiles,labels=FALSE, include.lowest = TRUE)-1\n# Adjust to expected target ratio by shifting min or max\n# if(target_ratio<=0.5){\n# new_max <- 2*target_ratio - 0.05\n# model_score <- 0.05 + model_score* (new_max-0.05)/(groups-1)\n# if(target_ratio>0.5){\n# new_min <- 2*target_ratio - 0.95\n# model_score <- new_min + model_score* (0.95-new_min)/(groups-1)\n# return(model_score)\n# }\n#### Static Quantile-based Mapping ####\nget_score_quantiles_ <- function(model_score, groups=9){\n# Cut into groups based on the score quantiles\nscore_quantiles <- quantile(model_score,seq(0,1,1/groups))\n\nif(length(score_quantiles) != length(unique(score_quantiles))){\nwarning(paste(\"Reduced number of groups to\", length(score_quantiles)-1, \"because scores between groups were constant\"))\n}\nreturn(score_quantiles)\n}\nmap_propensity_quantiles <- function(model_score, target_ratio, score_breaks){\n# Platt scaling\n# platt_scaler <- glm(y~prob, family=binomial(link='logit'),\n# weights = ifelse(exp$none$y==1, 1/(mean(exp$none$y==1)), 1),\n# data = data.frame(cbind('y'=exp$none$y,'prob'=churn_pred))\n# )\n# treat_prob <- predict(platt_scaler, newdata=data.frame(prob=churn_pred), type='response')\nmodel_score <- cut(model_score, breaks=score_breaks,labels=FALSE, include.lowest = TRUE)-1\nn_groups <- length(unique(model_score))\n# Adjust to expected target ratio by shifting min or max\nif(target_ratio<=0.5){\nnew_max <- 2*target_ratio - 0.05\nmodel_score <- 0.05 + model_score* (new_max-0.05)/(n_groups-1)\nif(target_ratio>0.5){\nnew_min <- 2*target_ratio - 0.95\nmodel_score <- new_min + model_score* (0.95-new_min)/(n_groups-1)\nreturn(model_score)\n#### Generalized Logistic Mapping ####\ngeneralized_logistic_ <- function(x, A=0, K=1, nu=1, Q=1, C=1){\n# B: growth rate\n# nu: skew\nreturn(A + (K-A)/((C+Q*exp(-B*x))^(1/nu)))\nmap_propensity_logistic <- function(model_score, min_score, target_ratio=NULL){\nif(!is.null(target_ratio)){\nstop(\"target_ratio not implemented\")\n# Min-Max scaling based on e.g. training data model scores\nmodel_score_normalized <- -1 + (model_score - min_score)*(2)/(max_score-min_score)\nprop_score <- generalized_logistic_(model_score_normalized, A=0.05, B=5)\nreturn(prop_score)\n", "output_sequence": "Supervised randomization integrates the targeting model into randomized controlled trials and A/B tests as a stochastic policy. By replacing random targeting with supervised randomization, we reduce the costs of running randomized experiments or allow continuous collection of randomized trial data. We show how to fully correct downstream analysis for the bias effect by the supervised treatment allocation with the true probabilites to receive treatment, which are logged at the time of targeting."}, {"input_sequence": "#### Packages ####\n#install.packages(\"pacman\")\nlibrary(pacman)\npacman::p_load(\"ggplot2\",\"reshape2\",\"drtmle\",\"grf\",\"foreach\",\"uplift\",\"data.table\",\"ModelMetrics\", \"plyr\", \"parallel\", \"SuperLearner\")\nsource(\"t_logit.R\")\nsource(\"data_generating_process.R\")\nsource(\"supervised_randomization.R\")\nsource(\"costs.R\")\ncalc_ATE <- function(y, w, prop_score){\n#if(is.null(prop_score)){\n# prop_score = rep(0.5, length(y))\n#}\nreturn( (sum(y*w/prop_score) - sum(y*(1-w)/(1-prop_score)) ) /length(y) )\n}\n#### Simulation Classification ####\nY = NULL\nN_VAR=10\nN_CUSTOMER=20000\nset.seed(1234)\nN_ITER = 100\ntemp <- foreach(iter=1:N_ITER, .multicombine = FALSE, .combine='+', .final=function(x)x/N_ITER)%do%{\nexpCtrl <- expControl(n_var = N_VAR, mode = \"regression\",\nbeta_sd = 1 , beta_zero = 0, # >0 indicates more than 50% purchasers (for linear: for 5%, for nonlinear: -3.12)\nDGP=\"nonlinear\")\n\nX <- data.frame(sapply(1:N_VAR, function(x) rnorm(N_CUSTOMER, 0, 10))) #runif(N_CUSTOMER, min = -10, max=10)\nbalanced <- do_experiment(X, expControl = expCtrl, prop_score = 0.5)\nprop_score <- map_propensity_logistic(balanced$tau, min_score = quantile(balanced$tau, p=0.05), max_score = quantile(balanced$tau, p=0.95))\nsupervised <- do_experiment(X, expControl = expCtrl, prop_score = prop_score)\ntargeting_model_balanced <- T_Regression(X=balanced$X, y=balanced$y, prop_score = rep(0.5, times=nrow(balanced$X)))\ntargeting_model_supervised <- T_Regression(X=supervised$X, y=supervised$y, prop_score = supervised$prop_score)\n#targeting_model_balanced <- T_rpart(X=balanced$X, y=balanced$y, method='anova')#\n#targeting_model_supervised_corrected <- T_rpart(X=supervised$X, y=supervised$y, prop_score = supervised$prop_score, method='anova')\ntau_balanced <- predict(targeting_model_balanced, X)\ntau_supervised_corrected <- predict(targeting_model_supervised_corrected, X)\npred0_balanced <- predict(targeting_model_balanced$model0, X)\npred0_supervised_corrected <- predict(targeting_model_supervised_corrected$model0, X)\npred1_balanced <- predict(targeting_model_balanced$model1, X)\npred1_supervised_corrected <- predict(targeting_model_supervised_corrected$model1, X)\nmapping <- factor(cut(prop_score, breaks=seq(0.05,0.95,length.out = 10)))\n#mapping <- factor(cut(prop_score, include_lowest=TRUE, breaks=quantile(prop_score, probs = seq(0,1,0.1))))\nrbind(\n\"AB balanced (tau)\" = round(tapply((tau_balanced - balanced$tau)^2, mapping, mean), 3),\n\"supervised randomization (tau)\" = round(tapply((tau_supervised- supervised$tau)^2, mapping, mean), 3),\n\n# \"AB balanced\" = round(tapply( ((pred0_balanced - balanced$y)^2)[balanced$w==0], mapping[balanced$w==0], mean), 3),\n#\n# \"AB balanced\" = round(tapply( ((pred1_balanced - balanced$y )^2)[balanced$w==1], mean), 3),\n# Should be good for lower propensity scores and worse for higher prop. scores\n\"AB balanced (model 0)\" = round(tapply( ((pred0_balanced - balanced$y)^2)[balanced$w==0], mapping[balanced$w==0], mean), 3),\n# Should be worse for lower propensity scores and good/better for higher prop. scores\n\"AB balanced (model 1)\" = round(tapply( ((pred1_balanced - balanced$y )^2)[balanced$w==1], mean), 3),\n)\nprint(round(temp,2))\n# ------------- Classification -----------\nexpCtrl <- expControl(n_var = N_VAR, mode = \"classification\",\nbeta_sd = 1, beta_zero = 0, # >0 indicates more than 50% purchasers (for linear: for 5%, for nonlinear: -3.12)\ntargeting_model_balanced <- T_Logit(X=balanced$X, y=balanced$y, prop_score = rep(0.5, times=nrow(balanced$X)))\ntargeting_model_supervised <- T_Logit(X=supervised$X, y=supervised$y, prop_score = supervised$prop_score)\ntau_balanced <- predict(targeting_model_balanced, X, type=\"response\")\ntau_supervised_corrected <- predict(targeting_model_supervised_corrected, X, type=\"response\")\nprint(round(temp,3))\n", "output_sequence": "Supervised randomization integrates the targeting model into randomized controlled trials and A/B tests as a stochastic policy. By replacing random targeting with supervised randomization, we reduce the costs of running randomized experiments or allow continuous collection of randomized trial data. We show how to fully correct downstream analysis for the bias effect by the supervised treatment allocation with the true probabilites to receive treatment, which are logged at the time of targeting."}, {"input_sequence": "# Test ------------------------------\n#path <- \"../Data/DMC01\"\n#temp <- load_dmc01(path)\n# End Test --------------------------\nlibrary(data.table)\nload_dmc01 <- function(folder){\ndata_train <- fread(paste0(path,\"/dmc2001-train.txt\"))\n\ndata_class$AKTIV <- y_class$AKTIV[match(y_class$ID, data_class$ID)]\ndata <- rbind(data_train, data_class)\n# Change target variable coding to 1 (purchase) and 0 (no future purchase)\ndata$AKTIV <- ifelse(data$AKTIV==1,0,1)\n# Recode West/East variable\ndata[WO==\"F\", WO := NA]\n# factor variables\nfor (var in c(\"WO\",\"Regiotyp\",\"Bebautyp\", \"Strtyp\")){\nset(data, j=var, value=paste(\"level\",data[[var]],sep=\"_\"))\n#set(data, which(is.na(data[[var]])), var, \"UNKNOWN\")\nset(data, j=var, value=factor(data[[var]]))\n}\n# Merge rare levels for robustness over cross validation\ndata[Bebautyp==\"level_0\",\n# Impute missing with median and include missing indicator\n# Transform to factor\ndata[, \"missing_habitation\":=ifelse(data$Regiotyp == \"level_NA\", 1,0)]\nset(data, i=which(data[[var]] == \"level_NA\"), j=var, value= names(which.max(table(data[[var]]))) )\nset(data, j=var, value= factor(data[[var]]))\n# Length of customer relationship\ndata[, customer_age := 2001-jahrstart]\ndata[customer_age>10,customer_age := 0] # Correct for no history\ndata[, jahrstart := NULL]\n# Drop ID\ndata[, ID := NULL]\n# Median imputation for missing values\nfor (var in colnames(data)[which(sapply(data, is.numeric))]){\ntemp_median <- median(data[[var]], na.rm=TRUE)\nif(!is.integer(temp_median)){\nset(data, j=var, value=as.numeric(data[[var]]))\n}\nset(data, i=which(is.na(data[[var]])), j=var, value=median(data[[var]], na.rm=TRUE))\n# Drop due to inconsistent scaling between regions\ndata[, Kaufkraft := NULL]\n# Rename target variable\nsetnames(data, \"AKTIV\", \"y\")\n# Normalize non-ordinal\nfor(var in c(\"AnzHH\", \"AnzGew\", \"customer_age\")){\nset(data, j=var, value=scale(data[[var]]))\n# One-hot encode data\ndata <- data.frame(model.matrix(~.-1,data))\nreturn(data)\n}\n", "output_sequence": "Supervised randomization integrates the targeting model into randomized controlled trials and A/B tests as a stochastic policy. By replacing random targeting with supervised randomization, we reduce the costs of running randomized experiments or allow continuous collection of randomized trial data. We show how to fully correct downstream analysis for the bias effect by the supervised treatment allocation with the true probabilites to receive treatment, which are logged at the time of targeting."}, {"input_sequence": "#### Debugging\n#path <- \"/Users/hauptjoh/Data/UCI/bank_marketing/bank-additional/bank-additional-full.csv\"\n####\nlibrary(data.table)\n\nload_bankmarketing <- function(path){\ndata <- fread(path, stringsAsFactors = FALSE)\n\n# Duration is actual call duration during targeting\n# previous and pdays are only available for few customers\ndropList <- c(\"duration\", \"pdays\", \"previous\")\ndata[, (dropList) := NULL]\n# Drop very rare (<0.005) classes in 'marital', 'education' and 'default' to avoid issues during cross validation\ndata <- data[marital!='unknown' & education != 'illiterate' & default != 'yes', ]\n# Transform string outcome\ndata[, y:=ifelse(y==\"yes\",1,0)]\n# Standardize\nnumVars <- c(\"age\", \"campaign\", \"emp.var.rate\", \"cons.price.idx\",\n\"euribor3m\", \"nr.employed\")\ndata <- data.frame(data)\ndata[, numVars] <- sapply(data[, numVars], function(x) (x-mean(x))/sd(x) )\n# One-hot/dummy encode categorical variables\ndata <- data.frame(model.matrix(~.-1, data))\nreturn(data)\n}\n", "output_sequence": "Supervised randomization integrates the targeting model into randomized controlled trials and A/B tests as a stochastic policy. By replacing random targeting with supervised randomization, we reduce the costs of running randomized experiments or allow continuous collection of randomized trial data. We show how to fully correct downstream analysis for the bias effect by the supervised treatment allocation with the true probabilites to receive treatment, which are logged at the time of targeting."}, {"input_sequence": "# Test case (standard)\n# W <- rbinom(1000, 1, 0.5)\n# Y <- as.numeric(1/(1+exp(-y - W*tau))>0.5)\n# scores <- tau + rnorm(1000,0,0.2)\n# groups <- 10\n# p_treatment <- 0.5\n#\n# Test case (propensity scores)\n# Treatment with 0.8 if tau>0.5 and 0.2 if tau<0.5\n# W <- foreach(i=1:length(y), .combine=c)%do%{\n# if(tau[i]>0.5){\n# rbinom(1, 1, 0.8)\n# }else{\n# rbinom(1, 1, 0.2)\n# }\n# p_treatment <- ifelse(tau>0.5, 0.8,0.2)\n# Debugging test\n# uplift::qini(uplift::performance(scores, rep(0, length(scores)), Y, W))\n# qini_score(scores, Y, W, p_treatment = p_treatment, plotit=TRUE)\n\n# DEPRECATED\n#calc_ATE <- function(y, g, prop_score){\n# if(length(prop_score)==1){\n# prop_score = rep(prop_score, length(y))\n# }\n# return( sum(y*g/prop_score)/sum(g*1/prop_score) - sum(y*(1-g)/(1-prop_score))/sum((1-g)*1/prop_score) )\n# #return( (sum(y*g/prop_score) - sum(y*(1-g)/(1-prop_score)) ) /length(y) )\n#}\nlibrary(data.table)\n### QINI SCORE (propensity corrected)\nqini_score <- function(scores, Y, W, p_treatment=0.5, groups = 10, plotit=FALSE){\nif(length(unique(lengths(list(scores, Y, W)))) != 1){\nstop(\"input scores, Y, W must have same length\")\n}\n# mm matrix to contain scores, deciles, observed response and experiment group indicator\nmm <- cbind(tau_hat = scores, y = Y, ct = W, tau_hat_rank = rank(-scores), prop_score = p_treatment)\nbk <- unique(quantile(mm[, \"tau_hat_rank\"], probs = seq(0, 1, 1/groups)))\nif ((length(bk) - 1) != groups)\nwarning(\"uplift: due to ties in uplift predictions, the number of groups is less than \",\ngroups)\nmm <- cbind(mm, decile = cut(mm[, \"tau_hat_rank\"], breaks = bk, labels = NULL,\ninclude.lowest = TRUE))\nmm <- data.table(mm)\nsetorder(mm, ct,decile)\n# ATE per decile\n#uplift <- mm[, .(uplift=calc_ATE(y = y, g = ct, prop_score = prop_score)),by=decile]\n# No. of positive responses and group size\n#deciles <- mm[, .(y1=sum(y), n=.N),by=.(decile, ct)]\n# No. of positive responses and group size, corrected for propensity score\ndeciles <- mm[, .(y1=sum(y/prop_score), n=sum(1/prop_score)),by=.(decile, ct)]\n# Ratio of positve responses in each decile relative to total number of positive responses in exp. group\ndeciles[, inc.y1.ratio := y1/sum(n), by=ct]\n# Incremental gain in positive responses (as difference to control group)\ninc.gains <- cumsum(deciles[ct==1, inc.y1.ratio] - deciles[ct==0, inc.y1.ratio])\n# No. of positive responses per decile per experiment group\n# y1 <- tapply(mm[,\"y\"], INDEX = list(mm[,\"ct\"],mm[,\"decile\"]),sum)\n# No. of observations per decile per experiment group\n# n <- tapply(mm[,\"y\"], INDEX = list(mm[,\"ct\"],mm[,\"decile\"]),length)\n# Success ratio per decile per experiment group\n# r.y1 <- y1/n\n# Uplift as difference in success ratio per decile (ATE per decile)\n# uplift <- r.y1[\"1\",] - r.y1[\"0\",]\n#\n# Relative increase in gains incrementally including group after group\n# inc.gains <- cumsum( (y1[\"1\",]/sum(n[\"1\",])) - y1[\"0\",]/sum(n[\"0\",]) )\n####\noverall.inc.gains <- inc.gains[length(inc.gains)]\nrandom.inc.gains <- cumsum(rep(overall.inc.gains/groups,\ngroups))\nx_axis <- seq(1/groups, 1, 1/groups)\ny_axis <- inc.gains\nauuc <- 0\nfor (i in 2:length(x_axis)) {\nauuc <- auuc + 0.5 * (x_axis[i] - x_axis[i-1]) * (y_axis[i] + y_axis[i-1])\ny_axis.rand <- random.inc.gains\nauuc.rand <- 0\nauuc.rand <- auuc.rand + 0.5 * (x_axis[i] - x_axis[i-1]) * (y_axis.rand[i] + y_axis.rand[i - 1])\nqini <- auuc - auuc.rand\nif(plotit){\nminy <- 100 * min(c(random.inc.gains, inc.gains))\n\nplot(inc.gains * 100 ~ seq(100/groups, 100, 100/groups),\ntype = \"b\", col = \"blue\", lty = 2, xlab = \"Proportion of population targeted (%)\",\nylab = \"Cumulative incremental gains (pc pt)\", ylim = c(miny,\nmaxy) )#, ...)\nlines(random.inc.gains * 100 ~ seq(100/groups, 100, 100/groups),\ntype = \"l\", col = \"red\", lty = 1)\n#legend(\"topright\", c(\"Model\", \"Random\"), col = c(\"blue\",\n# \"red\"), lty = c(2, 1))\nreturn(unname(qini))\n}\n", "output_sequence": "Supervised randomization integrates the targeting model into randomized controlled trials and A/B tests as a stochastic policy. By replacing random targeting with supervised randomization, we reduce the costs of running randomized experiments or allow continuous collection of randomized trial data. We show how to fully correct downstream analysis for the bias effect by the supervised treatment allocation with the true probabilites to receive treatment, which are logged at the time of targeting."}, {"input_sequence": "#### Optimal threshold for targeting ####\nopt_tau <- function(offer_cost, customer_value){\nthreshold <- offer_cost/customer_value\nreturn(threshold)\n}\n#### Optimal binary targeting based on optimal threshold given cost setting ####\ntargeting_policy <- function(tau_hat, offer_cost, customer_value){\ntreatment <- as.numeric(tau_hat > threshold)\nreturn(treatment)\ntop_decile_targeting_policy <- function(tau_hat){\ntau_hat_order <- order(tau_hat, decreasing = TRUE)\ncutoff <- floor(0.1 * length(tau_hat))\ntreatment <- ifelse(tau_hat_order <= cutoff, 1, 0)\n#### Transformed outcome loss ####\ntransformed_outcome_loss <- function(treatment_effect, Y, W, p_treatment){\n###\n# Biased estimate of the MSE (tau - tau_hat)^2 via the transformed outcome\n# See Athey & Imbens (2016) Recursive Partitioning for Heterogeneous Causal Effects\n# Hitsch & Misra (2018): Heterogeneous Treatment Effects and Optimal Targeting Policy Evaluation\nY_transformed <- Y * (W - p_treatment) / (p_treatment * (1-p_treatment))\nreturn( mean( (Y_transformed - treatment_effect)^2) )\n#### Churn costs ####\nchurn_cost <- function(y, w, contact_cost, offer_cost, value){\ntotal <- sum((1-w)*(1-y)) * 0 +\nsum(w*y) * -(contact_cost + customer_value) +\nsum((1-w)*y) * -customer_value\n\nreturn(total)\n#(mean(w)* -cost_treatment_fix +\n#n_customer\n#### Catalogue Campaign Profit ####\ncatalogue_profit <- function(y, w, contact_cost, offer_cost=0, value, mode=\"total\"){\nif(!mode %in% c(\"total\",\"individual\")){\nstop(\"Argument mode must be one of ['total', 'individual]\")\n}\nprofit <- (\n# Not treated/no purchase\n(1-w)*(1-y) * 0 +\n# Treated/purchase\nw*y * (value - contact_cost) +\n# Treated/no purchase\nw*(1-y) * (-contact_cost)+\n# Not treated/purchase\n(1-w)*y * (value)\n)\nif(mode==\"total\"){\nprofit <- sum(profit)\nreturn(profit)\n#### Expected Profit on Usable Observations\nexpected_profit <- function(policy_decision, profit, treatment_group, p_treatment){\n###\n# Calculate expected profit of a candidate targeting policy on randomized trial data.\n# See Hitsch & Misra (2018): Heterogeneous Treatment Effects and Optimal Targeting Policy Evaluation for details\n# Intuitively: When treatment group matches policy decision then add the profit corrected by the prob. that the\n# random treatment group matches the policy decision (either e(x) or 1-e(x) )\nreturn( sum((1-treatment_group)*(1-policy_decision)/(1-p_treatment)*profit + (treatment_group)*(policy_decision)/(p_treatment)*profit) )\n", "output_sequence": "Supervised randomization integrates the targeting model into randomized controlled trials and A/B tests as a stochastic policy. By replacing random targeting with supervised randomization, we reduce the costs of running randomized experiments or allow continuous collection of randomized trial data. We show how to fully correct downstream analysis for the bias effect by the supervised treatment allocation with the true probabilites to receive treatment, which are logged at the time of targeting."}, {"input_sequence": "library(data.table)\nload_uplift19 <- function(path){\ndata <- fread(path, check.names = TRUE)\n# Keep only campaigns that assign a 15 absolute amount coupon for consistency\ndata <- data[campaignValue==15 & campaignUnit==\"CURRENCY\", ]\n\n# Keep only view counts<=60\ndata <- data[targetViewCount<=60, ]\n# Rename Treatment Indicator\ndata$treatmentGroup <- abs(data$controlGroup-1)\ndata$controlGroup <- NULL\n# Ignore unrelated discounts by adding them back to the basket\n# Simultanously add back the coupon cost of 15, where it applied\ndata[, checkoutAmount := checkoutAmount+checkoutDiscount]\ndata[, checkoutDiscount := NULL]\n# Drop variables\ndropVar <- c(\n# Index or and campaign descriptors\n\"epochSecond\",\"campaignId\", \"campaignUnit\",\n# Transformed outcome variable\n\"label\",\n# Cancellation during conversion process?\n\"dropOff\",\n# Confirm or Abort on banner that treatment was seen?\n\"confirmed\", \"aborted\",\n# Duplicate variables\n\"HoursSinceOldestSession\",\n# Baseline dummy for time of day\n\"IsEarlyMorning\"\n)\ndata[, (dropVar) := NULL]\n# Zip Code Simplification\ndata[,ZipCode := as.character(ZipCode)]\n# All German Zip Codes have 5 digits\ndata[nchar(ZipCode) != 5, ZipCode := \"Foreign\"]\n# The first digits indicates the most general zone\ndata[ZipCode != \"Foreign\", ZipCode := substring(ZipCode,1,1)]\n# Factor variables\ndata[, ZipCode := factor(ZipCode)]\ndata[, DeviceCategory := factor(DeviceCategory)]\n# Drop binary variables with <0.01 of positive values\ndropVar <- c(\"HasConfirmedBefore\", \"DidConfirmLastWeek\", \"InitPageWas.sale.\", \"ChannelIs.EMAIL.\",\n\"ScreenTypeIs.home.\",\n# Drop constant variables\ndata <- data[, sapply(data, function(x) length(unique(x))) != 1, with=FALSE]\n# Drop variables with a correlation >0.9\n# Multicollinearity\n# metavars_left = c('converted', 'checkoutAmount', 'treatmentGroup')\n# data = data[c(metavars_left, setdiff(names(data),metavars_left))]\n# num <- c(4:ncol(data)) # disregard meta-variables\n# cor_num_P <- cor(data[,num],use=\"pairwise.complete.obs\", method=\"pearson\")\n# #corrplot(corr = cor_num_P, method=\"pie\", tl.cex=0.3, diag=F,type=\"lower\")\n# corr_var <- findCorrelation(cor_num_P, cutoff = .90, verbose= TRUE, names= TRUE, exact = TRUE)\n# data <- data[ , !(names(data) %in% corr_var)]\ndropVarColl <- c(\"DidConvertLastYear\", \"ViewedBefore.overview.\",\n\"log.of.SecondsSinceFirst.home.\",\n\"log.of.NumberOfDifferent.sale.\",\n\"log.of.NumberOfDifferent.about.\", \"ClientKnown\", \"log.of.SecondsSinceOn.account.\",\n\"log.of.SecondsSpentOn.sale.\", \"ScreenTypeIs.product.\", \"ViewedBefore.account.\",\n\"ViewedBefore.about.\", \"log.of.ViewsOn.product.\",\n)\ndata[, (dropVarColl) := NULL]\ndata <- data.table(predict(caret::dummyVars(~., data, sep=\"_\", fullRank=TRUE), data), keep.rownames = FALSE)\n# Missing Values\n#apply(data, 2, function(x) any(is.na(x))) # no missing values\n# replace negative values with zero since variables should be non-zero\n#data <- as.data.frame(lapply(data, function(x){replace(x, x <0,0)})) # no negative values\n# Multivariate outliers\n# mod <- lm(converted ~ ., data=data)\n# cooksd <- cooks.distance(mod)\n# sample_size <- nrow(data)\n# plot(cooksd, pch=\"*\", cex=2, main=\"Outliers as determined by Cooks distance\")\n# abline(h = 4/sample_size, col=\"blue\")\n# text(x=1:length(cooksd)+1, y=cooksd, labels=ifelse(cooksd>4/sample_size, names(cooksd),\"\"), col=\"blue\")\n# outl <- as.numeric(names(cooksd)[(cooksd > (4/sample_size))])\n# data <- data[-outl, ] # removed 5000 observations that had outliers\nTARGET = \"converted\"\nPROFIT = \"checkoutAmount\"\nW = \"treatmentGroup\"\nreturn(list(\"X\"=data[,!c(TARGET, PROFIT, W), with=FALSE], \"Y\"=data[[TARGET]], \"value\"=data[[PROFIT]], \"W\"=data[[W]]))\n}\n#### TESTS ####\n#data <- load_uplift19(\"../data/explore.csv\")\n#str(data,1)\n#lm <- glm(w~., cbind(data$X, \"w\"=data$W), family = \"binomial\")\n#ModelMetrics::auc(actual=data$W, predict(lm, type='response'))\n", "output_sequence": "Supervised randomization integrates the targeting model into randomized controlled trials and A/B tests as a stochastic policy. By replacing random targeting with supervised randomization, we reduce the costs of running randomized experiments or allow continuous collection of randomized trial data. We show how to fully correct downstream analysis for the bias effect by the supervised treatment allocation with the true probabilites to receive treatment, which are logged at the time of targeting."}, {"input_sequence": "#### Packages ####\n#install.packages(\"pacman\")\nlibrary(pacman)\npacman::p_load(\"ggplot2\",\"reshape2\",\"caret\",\"drtmle\",\"grf\",\"foreach\",\"uplift\",\"data.table\",\"ModelMetrics\", \"plyr\", \"parallel\", \"SuperLearner\", \"tidyverse\", \"nnet\", \"xtable\")\nsource(\"supervised_randomization.R\")\nsource(\"t_logit.R\")\nRNGkind(\"L'Ecuyer-CMRG\")\ncalc_ATE <- function(y, w, prop_score){\n#if(is.null(prop_score)){\n# prop_score = rep(0.5, length(y))\n#}\nreturn( (sum(y*w/prop_score) - sum(y*(1-w)/(1-prop_score)) ) /length(y) )\n}\n#### Load Real data ####\n# Results path\nresults_path = \"../results/bankmarketing/\"\nsource(\"load_bankmarketing.R\")\npath = \"../data/bank-additional-full.csv\"\ndata <- load_bankmarketing(path)\n# source(\"load_dmc01.R\")\n# path <- \"../data/\"\n# data <- load_dmc01(path)\ntau_model <- function(X, hidden_layer, ATE){\nX <- as.matrix(X)\nno_var <- ncol(X)\nW1 <- matrix(rnorm(no_var*hidden_layer, 0, 1), nrow=no_var, ncol=hidden_layer)\nW2 <- matrix(rnorm(hidden_layer, 0, 1), nrow=hidden_layer, ncol=1)\n\nh <- X %*% W1\nh <- dlogis(X %*% W1)\no <- o / (25*sd(o))\no <- o - mean(o) + ATE\nreturn(o)\nX <- data[,!(colnames(data)==\"y\")]\n# Simulation bankmarketing\nX_tau <- X[,c(1:24, 43:48)]\nset.seed(123456789)\nTAU <- tau_model(X_tau, hidden_layer=ncol(X_tau), ATE=0.05)\nX <- X[,!(colnames(X) %in% c(\"age\", \"maritalmarried\", \"maritalsingle\"))]\n# Simulation DMC01 catalogue\n# X_tau <- X\n# TAU <- tau_model(X_tau, hidden_layer=ncol(X_tau), ATE=0.05)\n# X <- X[,!grepl(\"Typ*|PHARM*\", colnames(X))]\nquantile(TAU, probs = seq(0,1,0.025))\nR <- rbinom(nrow(data), size = 1, prob = abs(TAU))\nY <- cbind(Y0=data$y, Y1=data$y)\nfor(i in 1:nrow(Y)){\nif(R[i] == 1){\nif(TAU[i]>0) Y[i, ] <- c(0,1)\n}\n# Sanity check: ATE should match target ATE\nATE <- mean(Y[,2]) - mean(Y[,1])\nATE\n#### Existing Targeting Model ####\n# Vary error variance to make the existing model better or worse\n# for(e in seq(0,0.05, 0.005)){\n# tau_hat <- TAU + rnorm(length(tau), 0, e)\n# print(cor(tau_hat, tau, method = \"spearman\"))\n# }\n#### Create experiments ####\ndo_experiment <- function(Y1, Y0, prop_score){\n# Sample treatment group based on propensity\nW <- rbinom(length(Y1), 1, prop_score)\nY <- ifelse(W==1, Y1, Y0)\nreturn(list('treatment'=W, 'outcome'=Y))\n#### SET PARAMETERS ####\n# Cross-Validation split\nNO_FOLDS = 4\nNO_EXPERIMENT_ITER = 50\nIMBALANCED_EXP_RATIO = 0.666\nDEFAULT_TARGETING_ERROR = 0.025\n#set.seed(123456789)\ntest_idx_list <- caret::createFolds(Y[,2] - Y[,1], k = NO_FOLDS, list = TRUE, returnTrain = FALSE)\n# Model error\nUPLIFT_MODEL_ERROR <- c(0.025, 0.04, 0.08) #c(0, 0.005, 0.1, 0.15)\nuplift_model_error_list <- foreach(e=UPLIFT_MODEL_ERROR)%do% rnorm(length(TAU), 0, e)\n# Profit setting\nCONTACT_COST = 1 # Contact costs\nOFFER_COST = 0 # Price reduction\nVALUE = c(seq(5, 80, 5))\nexp_grid = data.frame(expand.grid(list(\n\"uplift_model_error_idx\" = 1:length(uplift_model_error_list),\n\"testset\"=1:length(test_idx_list),\n\"iteration\"=1:NO_EXPERIMENT_ITER,\n\"randomization\" = c(\"full\", \"imbalanced\", \"supervised\")\n)))\n# The uplift model error is only relevant for supervised randomization\nexp_grid <- exp_grid[ !((exp_grid$uplift_model_error_idx != 1) & (exp_grid$randomization != \"supervised\")), ]\n#### Register cluster for parallel processing ####\ncl <- makeCluster(25)\nregisterDoParallel(cl)\nRNGkind(\"L'Ecuyer-CMRG\")\nclusterSetRNGStream(cl,iseed = 123456789)\n#### Experiment Cost Summary ####\nexp_grid_ABtest <- rbind(exp_grid,\ndata.frame(expand.grid(list(\n\"uplift_model_error_idx\" = 1,\n\"testset\"=1:length(test_idx_list),\n\"iteration\"=1:NO_EXPERIMENT_ITER,\n\"randomization\" = c(\"none\", \"all\")\n)))\n)\n# For each quality level of existing model\nresult_exp <- foreach(exp_idx = 1:nrow(exp_grid_ABtest), .inorder=TRUE, .combine=rbind)%dopar%{\n# Select experiment from grid\nrandomization <- exp_grid_ABtest$randomization[exp_idx]\nuplift_model_error_idx <- exp_grid_ABtest$uplift_model_error_idx[exp_idx]\ntestset <- exp_grid_ABtest$testset[exp_idx]\n\n# Test set observations\ntest_idx <- test_idx_list[[testset ]]\n\n# Simulate existing uplift model\ntau_hat <- TAU + uplift_model_error_list[[uplift_model_error_idx ]]\n# Get score quantiles from training set for mapping\nscore_quantiles_ <- get_score_quantiles_(tau_hat[-test_idx], groups=18)\nsupervised_prop_score <- map_propensity_quantiles(model_score = tau_hat[-test_idx], score_breaks = score_quantiles_, target_ratio = 0.5)\n# supervised_prop_score <- map_propensity_logistic(model_score = tau_hat[-test_idx],\n# min_score = quantile(tau_hat[-test_idx], probs=0.05),\nif(randomization==\"none\") prop_score <- 0\nif(randomization==\"imbalanced\") prop_score <- IMBALANCED_EXP_RATIO\n# Assign treatment\nY1 <- Y[-test_idx, \"Y1\"]\nexperiment <- do_experiment(Y1=Y1, Y0=Y0, prop_score = prop_score)\ny <- experiment$outcome\n# Profit during experiment\ncampaign_margin <- numeric()\ni = 1\nfor(value in VALUE){\ncampaign_margin[i] <- value\ncampaign_profit[i] <- sum(w * (Y1 * value - CONTACT_COST) + (1-w)*(Y0 * value))\ni = i+1\n}\n# Collect results\ndata.frame(list(\n\"uplift_model_error\"=UPLIFT_MODEL_ERROR[uplift_model_error_idx], \"testset\"=testset, \"randomization_iteration\" = i, \"randomization\"=randomization,\n\"treatment_ratio\"=mean(w), \"campaign_margin\"=campaign_margin, \"campaign_profit\" = campaign_profit\n), stringsAsFactors = FALSE)\n### Experiment summary statistics\nlibrary(tidyverse)\nexp_summary <- result_exp[result_exp$campaign_margin==10,] %>% group_by(randomization, uplift_model_error) %>% summarize(\"outcome_ratio\"=mean(outcome_ratio), \"treatment_ratio\"=mean(treatment_ratio))\nrow.names(exp_summary) <- NULL\nprint(exp_summary)\nfwrite(data.table(exp_summary,keep.rownames = F),\npaste0(results_path, \"raw_experiment_summary_20190826.txt\"))\nrandomization_values <- c(\"none\", \"full\", \"supervised\",\"imbalanced\", \"all\")\nexp_summary$randomization <- factor(exp_summary$randomization, randomization_values,\nexp_summary_table <- melt(exp_summary, id.vars = c(\"randomization\", \"uplift_model_error\"))\nexp_summary_table <- dcast( exp_summary_table[exp_summary$randomization!=\"Supervised\" | exp_summary$uplift_model_error==DEFAULT_TARGETING_ERROR,], variable ~ randomization)\nsummary_values <- c(\"treatment_ratio\", \"outcome_ratio\")\nsummary_labels <- c(\"Targeted Fraction of Customers\", \"Conversion Rate\")\nexp_summary_table$variable <- factor(mapvalues(exp_summary_table$variable, summary_values,\nexp_summary_table <- exp_summary_table[order(exp_summary_table$variable),]\nsink(paste0(results_path, \"experiment_summary_table.txt\"))\nprint(exp_summary_table, digits=3)\ncat(\"\\n\\n\\n\")\nprint(xtable(exp_summary_table, digits=3), include.rownames=FALSE)\nsink()\n### Experiment profit\nexp_profit <- result_exp %>% group_by(randomization, uplift_model_error, campaign_margin) %>% summarize(\"campaign_profit\"=mean(campaign_profit))\nrow.names(exp_profit) <- NULL\nexp_profit$randomization <- factor(exp_profit$randomization, randomization_values,\nexp_profit_table <- melt(exp_profit, id.vars = c(\"randomization\", \"uplift_model_error\",\"campaign_margin\"))\n# Per person profit (divide by number of customer in training set)\nexp_profit_table$value <- exp_profit_table$value / mean(sapply(test_idx_list, function(test_idx) nrow(X)-length(test_idx)))\nexp_profit_table <- dcast( exp_profit_table[exp_profit$randomization!=\"Supervised\" | exp_profit$uplift_model_error==DEFAULT_TARGETING_ERROR,], campaign_margin ~ randomization)\nsink(paste0(results_path, \"experiment_profit_table.txt\"))\nprint(exp_profit_table, digits=2)\nprint(xtable(exp_profit_table), include.rownames=FALSE)\n#### ATE ####\nresult_ATE <- foreach(exp_idx = 1:nrow(exp_grid), .inorder=TRUE, .combine=rbind, .packages=c(\"drtmle\", \"SuperLearner\"))%dopar%{\n# Select experiment from grid\nrandomization <- exp_grid$randomization[exp_idx]\nuplift_model_error_idx <- exp_grid$uplift_model_error_idx[exp_idx]\ntestset <- exp_grid$testset[exp_idx]\n# Test set observations\ntest_idx <- test_idx_list[[testset ]]\n# Simulate existing uplift model\ntau_hat <- TAU + uplift_model_error_list[[uplift_model_error_idx ]]\n# Get score quantiles from training set for mapping\nscore_quantiles_ <- get_score_quantiles_(tau_hat[test_idx], groups=18)\nsupervised_prop_score <- map_propensity_quantiles(model_score = tau_hat[test_idx], score_breaks = score_quantiles_, target_ratio = 0.5)\n# supervised_prop_score <- map_propensity_logistic(model_score = tau_hat[-test_idx],\n# min_score = quantile(tau_hat[-test_idx], probs=0.05),\nif(randomization==\"none\") prop_score <- rep(0, nrow(X[test_idx,]))\nif(randomization==\"imbalanced\") prop_score <- rep(IMBALANCED_EXP_RATIO, nrow(X[test_idx,]))\nif(randomization==\"supervised\") prop_score <- supervised_prop_score\n# Assign treatment\nexperiment <- do_experiment(Y1=Y[test_idx, \"Y1\"], Y0=Y[test_idx, \"Y0\"], prop_score = prop_score)\ny <- experiment$outcome\nx <- X[test_idx, ]\n\n### ATE ###\nATE_temp <- list()\n# True ATE\nATE_temp[[\"true_ATE\"]] <- mean(tau_hat[test_idx])\n# IPW\nATE_temp[[\"IPW\"]] <- calc_ATE(y, w, prop_score = prop_score)\noutput <- data.frame(list(\n\"uplift_model_error\"=UPLIFT_MODEL_ERROR[uplift_model_error_idx], \"testset\"=testset, \"randomization_iteration\" = i, \"randomization\"=randomization,\n\"ATE_true\" = ATE_temp$true_ATE,\n\"estimator\"= \"IPW\", ATE_hat = ATE_temp$IPW\n), stringsAsFactors = FALSE)\n#if(randomization==\"supervised\"){\ngn <- list()\ngn[[1]] <- prop_score\nATE_temp[[\"doubly_robust\"]] <- drtmle::ci(drtmle::drtmle(Y=y,A=w,W=x,a_0 = c(1,0),\nfamily=binomial(),\nmaxIter = 1),contrast=c(1,-1))$drtmle[1]\noutput <- rbind(output,\n\"ATE_true\" = ATE_temp$true_ATE,\n\"estimator\"= \"DR\", ATE_hat = ATE_temp$doubly_robust\n), stringsAsFactors = FALSE)\n)\nsaveRDS(result_ATE, paste0(results_path, \"ATE_predictions_20190821.rds\"))\n# Keep only one target model error rate\nresult_ATE_table <- result_ATE[result_ATE$randomization!=\"supervised\" | result_ATE$uplift_model_error==DEFAULT_TARGETING_ERROR,]\n# Sort the boxplots by sorting the factor variable\nresult_ATE_table$estimator <- factor(result_ATE_table$estimator, levels = c(\"IPW\",\"DR\"), labels=c(\"IPW\", \"DR\"))\n# Create the boxplot\nATE_plot <- ggplot(data = result_ATE[result_ATE_table$estimator==\"DR\"|result_ATE_table$randomization==\"supervised\",], aes(x=interaction(randomization, estimator, lex.order=TRUE), y=ATE_hat)) +\ngeom_boxplot() +\nstat_summary(fun.y = \"mean\", geom = \"point\", colour = \"blue\", shape = 15) + #, size = 2\ngeom_hline(aes(yintercept=ATE),linetype=\"dashed\") +\nlabs(x=\"Randomization\", y = \"ATE\") +\nscale_x_discrete(labels=c(\"full.DR\" = \"full (balanced)\", \"imbalanced.DR\" = \"full (imbalanced)\",\n\"supervised.IPW\" = \"supervised (IPW)\",\"supervised.DR\" = \"supervised (DR)\")) +\ntheme_classic()+\ntheme(axis.text.x = element_text(size=12, color=\"black\"))+\n# axis.text.y = element_text(size=14),\nylim(c(0.02,0.08))\nggsave(paste0(\"ATE_boxplot_\",NO_FOLDS*NO_EXPERIMENT_ITER, \".pdf\"), plot = ATE_plot, device = \"pdf\", path=results_path,\nwidth = 14, height = 8, units = \"cm\",\ndpi = 400)\n##### CATE #####\n# Set model parameters\n# CF\nMTRY= ceil(sqrt(ncol(X)))\nNUM.TREES=500\nCI.GROUP.SIZE=1\nMIN.NODE.SIZE = 20\nSAMPLE.FRACTION = 0.5\npred_CATE <- foreach(exp_idx = 1:nrow(exp_grid), .inorder=TRUE, .packages=c(\"grf\", \"rpart\", \"nnet\"), .export=c(\"predict.tlearner\"))%dopar%{\nX_train <- X[-test_idx, ]\nscore_quantiles_ <- get_score_quantiles_(tau_hat[-test_idx], groups=18)\nsupervised_prop_score <- map_propensity_quantiles(model_score = tau_hat[-test_idx], score_breaks = score_quantiles_, target_ratio = 0.5)\n# supervised_prop_score <- map_propensity_logistic(model_score = tau_hat[-test_idx],\nif(randomization==\"none\") prop_score <- rep(0, nrow(X_train))\nif(randomization==\"imbalanced\") prop_score <- rep(IMBALANCED_EXP_RATIO, nrow(X_train))\nexperiment <- do_experiment(Y1=Y[-test_idx, \"Y1\"], Y0=Y[-test_idx, \"Y0\"], prop_score = prop_score)\n### CATE ###\npred <- list()\n## ATE baseline\nate_hat <- calc_ATE(y = y, w = w, prop_score = prop_score)\npred[[\"ATE\"]] <- rep(ate_hat, times = nrow(X_test) )\n## Oracle regression\npred[[\"true\"]] <- TAU[test_idx]\noracle_reg <- lm(.y~., data = data.frame(\".y\"=TAU[-test_idx], X_train), weights = ifelse(w==1, prop_score,\npred[['oracle_regression']] <- unname(predict(oracle_reg, X_test))\n## T Logit\nt_logit <- T_Logit(X = X_train, y = y, W = w, prop_score = prop_score)\npred[['t-logit']] <- unname(predict(t_logit, X_test, type='response'))\n## MOM Logit (see Knaus 2018)\n#mom_logit <- MOM_Logit(X = X_train, y = y, W = w, prop_score = prop_score)\n#pred[['mom-logit']] <- unname(predict(mom_logit, X_test))\n## T Tree\n# t_rpart <- T_rpart(X = X_train, y = y, W = w, prop_score = prop_score, method=\"class\")\n# pred[['t-tree']] <- unname(predict(t_rpart, X_test, type='prob'))\n## T Neural Network\n#t_nnet <- T_NNet(X = X_train, y = y, W = w, prop_score = prop_score, trace=TRUE)\n#pred[['t-nnet']] <- unname(predict(t_nnet, X_test, type='raw'))\n#Causal Forest\ncf <- grf::causal_forest(X=X_train,Y=y, W=w,\nW.hat = prop_score,\nci.group.size = CI.GROUP.SIZE,\nnum.trees=NUM.TREES, min.node.size=MIN.NODE.SIZE,\nmtry=MTRY, sample.fraction = SAMPLE.FRACTION)\npred[['CF']] <- predict(cf, X_test)[,1]\n# Collect results\nreturn(pred)\nsaveRDS(pred_CATE, \"../CATE_predictions_20190826.rds\")\n#\nCATE_perf <- foreach(exp_idx=1:nrow(exp_grid), .combine=rbind, .packages = \"data.table\", .inorder=TRUE, .export=)%dopar%{\n# Select experiment from grid\nrandomization <- exp_grid$randomization[exp_idx]\nuplift_model_error_idx <- exp_grid$uplift_model_error_idx[exp_idx]\ntestset <- exp_grid$testset[exp_idx]\ntest_idx <- test_idx_list[[testset]]\nY_test <- Y[test_idx, ]\ntau_scores <- pred_CATE[[exp_idx]]\n# Results\nraw <- list()\nraw[[\"mae\"]] <- sapply(tau_scores, function(x) mean(abs(x - TAU[test_idx])))\nraw[[\"qini\"]] <- sapply(tau_scores, function(x) qini_score(scores = c(x, x), Y = c(Y_test[,\"Y1\"], W = c(rep(1, length(Y_test[,\"Y1\"])), rep(0, length(Y_test[,\"Y0\"])))))\nfor(value in VALUE){\ntargeting_decisions <- sapply(tau_scores, function(x) targeting_policy(tau_hat=x, offer_cost = CONTACT_COST, customer_value = value), simplify = FALSE)\nraw[[paste0(\"targeting_ratio_\", value)]] <- sapply(targeting_decisions, mean)\nraw[[paste0(\"profit_cutoff_\", value)]] <- sapply(targeting_decisions, function(x) sum(x * (Y_test[,\"Y1\"] * value - CONTACT_COST) + (1-x)*(Y_test[,\"Y0\"] * value)))\ntargeting_decisions_top_decile <- sapply(tau_scores, function(x) top_decile_targeting_policy(x), simplify=FALSE)\nraw[[paste0(\"profit_decile_\", value)]] <- sapply(targeting_decisions_top_decile, function(x) sum(x * (Y_test[,\"Y1\"] * value - CONTACT_COST) + (1-x)*(Y_test[,\"Y0\"] * value)))\n}\ntemp <- data.frame(raw)\ntemp <- cbind(temp, \"model\"=row.names(temp))\ntemp <- melt(temp, id.vars = \"model\", variable.name=c(\"metric\"))\ntemp <- cbind(\ntemp,\ndata.frame(list(\"uplift_model_error\"=UPLIFT_MODEL_ERROR[uplift_model_error_idx], \"testset\"=testset, \"randomization_iteration\" = i, \"randomization\"=randomization),\nstringsAsFactors = FALSE, row.names = NULL)\n)\nreturn(temp)\nCATE_summary <- CATE_perf %>% group_by(randomization, uplift_model_error, metric, model) %>% summarize(\"value\"=mean(value))\nmodel_names <- c(\"true\",\"ATE\", \"oracle_regression\", \"t-logit\", \"t-tree\", \"mom-logit\", \"CF\", \"t-nnet\")\nCATE_summary$model <- factor(CATE_summary$model, levels = model_names, labels = model_names)\nCATE_summary <- data.frame(CATE_summary)\nrow.names(CATE_summary) <- NULL\nsaveRDS(CATE_summary, \"../CATE_summary_20190826.rds\")\n### Statistical Performance Table\nstat_table <- dcast(CATE_summary[ CATE_summary$metric %in% c(\"mae\",\"qini\") & CATE_summary$model %in% c(\"ATE\",\"t-logit\",\"CF\") & (CATE_summary$randomization!=\"supervised\" | CATE_summary$uplift_model_error==DEFAULT_TARGETING_ERROR),],\nrandomization ~ model+ metric, row.names=FALSE)\nrow.names(stat_table) <- NULL\nsink(paste0(results_path, \"CATE_stat_table.txt\"))\nprint(stat_table, digits=4)\nprint(\nxtable(stat_table, digits=4),\ninclude.rownames=FALSE)\n### Profit Performance Table\n## Targeting ratio as a sanity check\nstat_table <- dcast(CATE_summary[ grepl(pattern = \"targeting_ratio\", CATE_summary$metric) & CATE_summary$model %in% c(\"ATE\",\"t-logit\",\"CF\") & (CATE_summary$randomization!=\"supervised\" | CATE_summary$uplift_model_error==DEFAULT_TARGETING_ERROR),],\nmetric ~ model + randomization, row.names=FALSE)\nrow.names(stat_table) <- NULL\nsink(paste0(results_path, \"CATE_targeting-ratio_table.txt\"))\nprint(stat_table)\n## Profit based on profit decision\nstat_table <- dcast(CATE_summary[ grepl(pattern = \"profit_cutoff\", CATE_summary$metric) & CATE_summary$model %in% c(\"t-logit\",\"CF\") & (CATE_summary$randomization!=\"supervised\" | CATE_summary$uplift_model_error==DEFAULT_TARGETING_ERROR),],\nrow.names(stat_table) <- NULL\nstat_table$metric <- str_remove(stat_table$metric, \"profit_cutoff_\")\n# Per customer profit\nstat_table[, 2:ncol(stat_table)] <- stat_table[, 2:ncol(stat_table)]/mean(sapply(test_idx_list, length))\nsink(paste0(results_path, \"CATE_profit_table.txt\"))\nprint(stat_table, digits=2)\n## Profit at fixed targeting rate\nstat_table <- dcast(CATE_summary[ grepl(pattern = \"profit_decile\", CATE_summary$metric) & CATE_summary$model %in% c(\"t-logit\",\"CF\") & CATE_summary$uplift_model_error==DEFAULT_TARGETING_ERROR,],\nrow.names(stat_table) <- NULL\n### Targeting model performance robustness check ####\n## Experiment profit\nerror_exp_profit <- result_exp %>% group_by(randomization, uplift_model_error, campaign_margin) %>% summarize(\"campaign_profit\"=mean(campaign_profit))\nrow.names(error_exp_profit) <- NULL\nerror_exp_profit$randomization <- factor(error_exp_profit$randomization, randomization_values,\nerror_exp_profit_table <- melt(error_exp_profit, id.vars = c(\"randomization\", \"uplift_model_error\",\"campaign_margin\"))\nerror_exp_profit_table$value <- error_exp_profit_table$value / mean(sapply(test_idx_list, function(test_idx) nrow(X)-length(test_idx)))\nerror_exp_profit_table <- dcast( error_exp_profit_table[error_exp_profit$randomization==\"Supervised\",], campaign_margin ~ uplift_model_error)\nprint(xtable(error_exp_profit_table), include.rownames=FALSE)\n## Model profit\n# Statistical performance\nerror_model_profit <- dcast(CATE_summary[ CATE_summary$metric %in% c(\"mae\",\"qini\") & CATE_summary$model %in% c(\"ATE\",\"t-logit\",\"CF\") & CATE_summary$randomization==\"supervised\",],\nuplift_model_error ~ model+ metric, row.names=FALSE)\nprint(xtable(error_model_profit, digits=3), include.rownames=FALSE)\n# CATE Profit\nerror_model_profit <- dcast(CATE_summary[ grepl(pattern = \"profit_cutoff\", CATE_summary$metric) & CATE_summary$model %in% c(\"t-logit\",\"CF\") & CATE_summary$randomization==\"supervised\",], #\nmetric ~ model + uplift_model_error, row.names=FALSE)\nerror_model_profit[, 2:ncol(error_model_profit)] <- error_model_profit[, 2:ncol(error_model_profit)]/mean(sapply(test_idx_list, length))\nprint(xtable(error_model_profit, digits=2), include.rownames=FALSE)\n", "output_sequence": "Supervised randomization integrates the targeting model into randomized controlled trials and A/B tests as a stochastic policy. By replacing random targeting with supervised randomization, we reduce the costs of running randomized experiments or allow continuous collection of randomized trial data. We show how to fully correct downstream analysis for the bias effect by the supervised treatment allocation with the true probabilites to receive treatment, which are logged at the time of targeting."}, {"input_sequence": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as pl\ndate = pd.date_range(start = \"05/04/2017\", periods = 2776, freq = \"30min\")\nh = pd.DataFrame(0, index = np.arange(9), columns = [\"bin\", \"dd3\", \"dd5\"])\nfor j in range(3):\ndf = pd.read_csv(\"<path>/\" + str(j + 3) + \"0weights1.csv\")\nv = np.sort(df.values, 1)[:, 0:5]\nres = np.sum(v, axis = 1) * 50\nd = pd.DataFrame(data = [date, res]).T\nd.columns = ['date', 'value']\nfor i in range(9):\nh.iloc[i, 0] = i + 1\nh.iloc[i, j + 1] = pd.DataFrame.mean(d.iloc[336 * i : 336 * (i + 1) - 1,1])\n\nh.plot(x = \"bin\",y = [\"dd3\", \"dd5\"], kind = \"bar\",\ncolor = [\"blue\", \"red\", \"gray\"], legend = False)\npl.xlabel(\"Week number\")\npl.ylabel(\"Value (%)\")\npl.title(\"Av. investment (in %) for 5 least invested coins\")\n", "output_sequence": "Outputs a plot that displays the weight (in %) for the 5 least held coins for different target drawdowns in Experiment 1. A higher value means better diversification."}, {"input_sequence": "source(XFGPowerLawEst1)\n# sample period\nstartDate = ymd(20081231)\nDate = startDate %m+% months(c(1:(length(alpha))))\n# store the Power Law estimation results in 'PL'\nPL = cbind(as.Date(Date), alpha, Xmin, KS, P)\n# load fundamental variables\nTVs = as.matrix(read.csv(\"Bitcoin_Data1.csv\", header = T))\nTVs = TVs[match(\"3/1/2009\", TVs[, 1]):match(\"31/12/2013\", TVs[, 1]), ]\nstartDate = ymd(20090103)\ndmy = seq.Date(as.Date(startDate), as.Date(endDate), by = \"1 day\")\nYear = year(dmy)\nym = as.numeric(paste(Mon, Year, sep = \".\"))\nTVs[, 1] = ym\n# convert daily data to monthly data\nDuni = unique(TVs[, 1])\nfor (i in 1:length(Duni)) {\nm = apply(matrix(as.numeric(as.matrix(TVs[(TVs[, 1] == Duni[i]), ])[, -c(1:2)]), ncol = (ncol(TVs) - 2)), 2, sum)\nm = matrix(m, nrow = 1)\n}\n# store Power Law parameters with Transaction Variables in PL.C\nPL.C = cbind(PL, M.TVs)\n# remove the date column\nPL.s = PL.s[, -1]\ncolnames(PL.s)[5:length(colnames(PL.s))] = colnames(TVs)[-c(1:2)]\nPL.s = as.matrix(PL.s)\n# run linear regression days.destroyed, 8\nDays.destroyed.alpha.fit = lm(diff(as.numeric(PL.s[, 8])) ~ D.Alpha)\nDays.destroyed.alpha.table = summary(Days.destroyed.alpha.fit)\n# MB.1\nMB..alpha.fit = lm(diff(as.numeric(PL.s[, 9])) ~ D.Alpha)\nMB..alpha.table = summary(MB..alpha.fit)\n# difficulty, 11\nDifficulty.alpha.fit = lm(diff(as.numeric(PL.s[, 12])) ~ D.Alpha)\nDifficulty.alpha.table = summary(Difficulty.alpha.fit)\n# Hashrate 14\nHashrate.alpha.fit = lm(diff(as.numeric(PL.s[, 15])) ~ D.Alpha)\nHashrate.alpha.table = summary(Hashrate.alpha.fit)\n# Market.cap.usd 15\nMarket.cap.alpha.fit = lm(diff(as.numeric(PL.s[, 16])) ~ D.Alpha)\nMarket.cap.alpha.table = summary(Market.cap.alpha.fit)\n# price.usd 16\nMarket.price.alpha.fit = lm(diff(as.numeric(PL.s[, 17])) ~ D.Alpha)\nMarket.price.alpha.table = summary(Market.price.alpha.fit)\n# miners.Revenue 17\nMiners.Revenue.alpha.fit = lm(diff(as.numeric(PL.s[, 18])) ~ D.Alpha)\nMiners.Revenue.alpha.table = summary(Miners.Revenue.alpha.fit)\n# network.deficit.usd 18\nNetwork.Deficit.alpha.fit = lm(diff(as.numeric(PL.s[, 19])) ~ D.Alpha)\nNetwork.Deficit.alpha.table = summary(Network.Deficit.alpha.fit)\n# No. of Transactions 26\nNo.Transaction.alpha.fit = lm(diff(as.numeric(PL.s[, 26])) ~ D.Alpha)\nNo.Transaction.alpha.table = summary(No.Transaction.alpha.fit)\n# Ratio 27\nRatio.alpha.fit = lm(as.numeric(PL.s[-1, 29]) ~ D.Alpha)\nRatio.alpha.table = summary(Ratio.alpha.fit)\n# Draw Table 1\ntexreg(list(Days.destroyed.alpha.fit,\nMB..alpha.fit,\nMiners.Revenue.alpha.fit,\nstars = c(0.01, 0.05, 0.1),\ncaption.above = TRUE,\ncaption = \"Estimation Results for Bitcoin and Auroracoin\",\ndcolumn = FALSE,\ncustom.model.names = c(\"Days Destroyed\",\n\"MB.1\",\noverride.se = list(Days.destroyed.alpha.table$coef[, 2],\nsource(XFGPowerLawEst2)\nstartDate = ymd(20140228)\nDate = startDate %m+% months(c(1:(length(alpha.2))))\n# using difference data\nD.Alpha = diff(alpha.2)\n# read transaction data\nData = read.csv(\"Auroracoin.csv\",header=T)\n# Regression days.destroyed,\ndays.destroyed.alpha.fita = lm(diff(as.numeric(Data[, 8])) ~ D.Alpha)\ndays.destroyed.alpha.tablea = summary(days.destroyed.alpha.fita)\n# cost.transaction,\ncost.transaction.alpha.fita = lm(diff(as.numeric(Data[, 6])) ~ D.Alpha)\ncost.transaction.alpha.tablea = summary(cost.transaction.alpha.fita)\n# difficulty,\ndifficulty.alpha.fita = lm(diff(as.numeric(Data[, 3])) ~ D.Alpha)\ndifficulty.alpha.tablea = summary(difficulty.alpha.fita)\n# Transaction.volume,usd 12\nTransactionVolumeu.alpha.fita = lm(diff(as.numeric(Data[, 5])) ~ D.Alpha)\nTransactionVolumeu.alpha.tablea = summary(TransactionVolumeu.alpha.fita)\n# Price\nPrice.alpha.fita = lm(diff(as.numeric(Data[, 2])) ~ D.Alpha)\nPrice.alpha.tablea = summary(Price.alpha.fita)\n# No.of.Transactions\nNo.of.Transactions.alpha.fita = lm(diff(as.numeric(Data[, 4])) ~ D.Alpha)\nNo.of.Transactions.alpha.tablea = summary(No.of.Transactions.alpha.fita)\n# Transaction.fees\nTransaction.fees.alpha.fita = lm(diff(as.numeric(Data[, 7])) ~ D.Alpha)\nTransaction.fees.alpha.tablea = summary(Transaction.fees.alpha.fita)\n# Draw Table 2\ntexreg(list(days.destroyed.alpha.fita,\ncost.transaction.alpha.fita,\n\"Cost.transaction\",\noverride.se = list(days.destroyed.alpha.tablea$coef[, 2],\nsource(XFGPowerLawEst3)\n### store the results in 'PL'\n# combine the transaction data with the power law estimation results\n# remove the data column\n# store PL.s in PL.S\nPL.S = PL.s\n# Using periods when powerlaw is well fitted\nPL.s = PL.S[(P > 0.1), ]\n# run linear regression for table 3 days.destroyed, 8\n# MB.1, 9\n# Draw Table 3\nstars = c(0.01, 0.05, 0.1),\ncaption.above = TRUE,\ncaption = \"Estimation Results for Bitcoin and Auroracoin\",\ndcolumn = FALSE,\ncustom.model.names = c(\"Days Destroyed\",\n\"MB.1\",\noverride.se = list(Days.destroyed.alpha.table$coef[, 2],\n# Part 4: Table 4 Using periods when powerlaw is well fitted\nPL.s = PL.S[(P > 0.1), ] # choosing power law periods\nP.value = as.numeric(PL.s[-1, 4])\n# days.destroyed, 8\nDays.destroyed.P.value.fit = lm(diff(as.numeric(PL.s[, 8])) ~ P.value)\nDays.destroyed.P.value.table = summary(Days.destroyed.P.value.fit)\nMB..P.value.fit = lm(diff(as.numeric(PL.s[, 9])) ~ P.value)\nMB..P.value.table = summary(MB..P.value.fit)\nDifficulty.P.value.fit = lm(diff(as.numeric(PL.s[, 12])) ~ P.value)\nDifficulty.P.value.table = summary(Difficulty.P.value.fit)\nHashrate.P.value.fit = lm(diff(as.numeric(PL.s[, 15])) ~ P.value)\nHashrate.P.value.table = summary(Hashrate.P.value.fit)\nMarket.cap.P.value.fit = lm(diff(as.numeric(PL.s[, 16])) ~ P.value)\nMarket.cap.P.value.table = summary(Market.cap.P.value.fit)\nMarket.price.P.value.fit = lm(diff(as.numeric(PL.s[, 17])) ~ P.value)\nMarket.price.P.value.table = summary(Market.price.P.value.fit)\nMiners.Revenue.P.value.fit = lm(diff(as.numeric(PL.s[, 18])) ~ P.value)\nMiners.Revenue.P.value.table = summary(Miners.Revenue.P.value.fit)\nNetwork.Deficit.P.value.fit = lm(diff(as.numeric(PL.s[, 19])) ~ P.value)\nNetwork.Deficit.P.value.table = summary(Network.Deficit.P.value.fit)\nNo.Transaction.P.value.fit = lm(diff(as.numeric(PL.s[, 26])) ~ P.value)\nNo.Transaction.P.value.table = summary(No.Transaction.P.value.fit)\nRatio.P.value.fit = lm(as.numeric(PL.s[-1, 29]) ~ P.value)\nRatio.P.value.table = summary(Ratio.P.value.fit)\n# Draw Table 4\ntexreg(list(Days.destroyed.P.value.fit,\nMB..P.value.fit,\nMiners.Revenue.P.value.fit,\noverride.se = list(Days.destroyed.P.value.table$coef[, 2],\n# Part 5: Table 5 & 6 Using whole periods\nPL.s = PL.S\nPs = as.numeric((as.numeric(PL.s[-1, 4]) > 0.1))\n# run linear regression for table 5 & 6 days.destroyed, 8\nDays.destroyed.Ps.fit = lm(diff(as.numeric(PL.s[, 8])) ~ Ps)\nDays.destroyed.Ps.table = summary(Days.destroyed.Ps.fit)\nMB..Ps.fit = lm(diff(as.numeric(PL.s[, 9])) ~ Ps)\nMB..Ps.table = summary(MB..Ps.fit)\nDifficulty.Ps.fit = lm(diff(as.numeric(PL.s[, 12])) ~ Ps)\nDifficulty.Ps.table = summary(Difficulty.Ps.fit)\nHashrate.Ps.fit = lm(diff(as.numeric(PL.s[, 15])) ~ Ps)\nHashrate.Ps.table = summary(Hashrate.Ps.fit)\nMarket.cap.Ps.fit = lm(diff(as.numeric(PL.s[, 16])) ~ Ps)\nMarket.cap.Ps.table = summary(Market.cap.Ps.fit)\nMarket.price.Ps.fit = lm(diff(as.numeric(PL.s[, 17])) ~ Ps)\nMarket.price.Ps.table = summary(Market.price.Ps.fit)\nMiners.Revenue.Ps.fit = lm(diff(as.numeric(PL.s[, 18])) ~ Ps)\nMiners.Revenue.Ps.table = summary(Miners.Revenue.Ps.fit)\nNetwork.Deficit.Ps.fit = lm(diff(as.numeric(PL.s[, 19])) ~ Ps)\nNetwork.Deficit.Ps.table = summary(Network.Deficit.Ps.fit)\nNo.Transaction.Ps.fit = lm(diff(as.numeric(PL.s[, 26])) ~ Ps)\nNo.Transaction.Ps.table = summary(No.Transaction.Ps.fit)\nRatio.Ps.fit = lm(as.numeric(PL.s[-1, 29]) ~ Ps)\nRatio.Ps.table = summary(Ratio.Ps.fit)\n# Draw Table 5\ntexreg(list(Days.destroyed.Ps.fit,\nMB..Ps.fit,\nDifficulty.Ps.fit,\nMiners.Revenue.Ps.fit,\noverride.se = list(Days.destroyed.Ps.table$coef[, 2],\n# Draw Table 6\n", "output_sequence": "Generates the Latex code for tables of regression results in 'Risk Analysis of Cryptos as Alternative Asset Class'."}, {"input_sequence": "# clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\nlibrary(data.table)\nlibrary(lubridate)\nlibrary(xts)\nlibrary(forecast)\nlibrary(ggplot2)\nlibrary(rugarch)\n## settings ##\nSys.setenv(LANG = \"en\") # set environment language to English\nSys.setlocale(\"LC_TIME\", \"en_US.UTF-8\") # set timestamp language to English\n##\n## themes & functions ##\n#\nts_theme <- theme(panel.border = element_blank(), panel.background = element_blank(),\npanel.grid.minor = element_line(colour = \"grey90\"),\naxis.text = element_text(size = 14, face = \"bold\"),\n)\n# Changing strings to numerics\nstrip <- function(x){\nz <- gsub(\"[^0-9,.]\", \"\", x)\ngsub(\",\", \".\", z)\n}\n## read data ##\nma_energy <- fread(\"m_a_energy.csv\", dec = \";\")\n## reshape ##\nnum_cols <- c(\"Deal Value\", \"Acquiror Net Sales LTM\", \"Acquiror EBIT LTM\", \"Acquiror EBITDA LTM\", \"Acquiror Pre-tax Income LTM\",\n\"Acquiror Net Income LTM\", \"Acquiror Earnings Per Share LTM\", \"Acquiror Total Assets\", \"Net Sales LTM\",\n\"EBIT LTM\", \"EBITDA LTM\", \"Pre-tax Income LTM\", \"Cash and Short Term\", \"Total Assets\", \"Short Term Debt\",\n\"Net Debt\", \"Total Liabilities\", \"Total Debt\", \"Common Equity\", Value at Announcement\",\n\"Equity Value at Effective Date\", \"Acquiror Financial Advisor Credit\", \"Target Financial Advisor Imputed Fees Per Advisor\",\n\"Target Legal Advisor Credit\", \"Deal Value inc. Net Debt of Target\")\ndate_cols <- c(\"Announcement Date\", \"Date of Acquiror Financials\", \"Date of Target Financials\", \"Effective Date\")\nma_energy[, (num_cols) := lapply(.SD, function(x) as.numeric(strip(x))), .SDcols = num_cols]\nma_energy[, year := year(`Announcement Date`)]\nma_energy[, yearmonth := as.yearmon(`Announcement Date`, \"%Y %m\")]\nma_energy[, quarter := quarter(`Announcement Date`, with_year = TRUE)]\nma_energy[, month := month(`Announcement Date`)]\nsetkey(ma_energy, `Announcement Date`)\n## get number of m&a per month ##\nN_per_month <- ma_energy[`Deal Status` %in% \"Completed\", .N, by = c(\"yearmonth\")]\nN_per_month[, Date := as.Date(yearmonth)]\nDT_N <- data.table(\"Date\" = seq(from = as.Date(N_per_month$yearmonth[1]), to = as.Date(N_per_month$yearmonth[nrow(N_per_month)]), by = \"1 month\"))\nDT_N[N_per_month, N := i.N, on = \"Date\"]\nDT_N[is.na(N), N := 0]\nDT_N[, Diff_N := c(0, diff(N))] # first differences\nsetkey(DT_N, Date) # sort by date\nN_per_month <- DT_N[Date > as.Date(\"1996-12-31\") & Date < as.Date(\"2019-01-01\")] # subset to only include values after liberalization wave\nN_per_month <- N_per_month[complete.cases(N_per_month)] # keep only rows without any NA values\nsetkey(N_per_month, Date)\nN_per_month[, index := 1:nrow(N_per_month)]\n# time series dataset #\nMERGERS_ACQUISITIONS_ENERGY <- xts(N_per_month[Date < as.Date(\"2020-01-01\")]$Diff_N, order.by = as.yearmon(N_per_month[Date < as.Date(\"2020-01-01\")]$Date), frequency = 12)\n# create timeseries dataset\ndataset <- N_per_month$Diff_N # add the desired vector of M&A here\ndate <- N_per_month$Date # date index\n# plot distribution #\nbins <- min(500, grDevices::nclass.FD(na.exclude(MERGERS_ACQUISITIONS_ENERGY)))\nbinwidth <- (max(MERGERS_ACQUISITIONS_ENERGY, na.rm = TRUE) - min(MERGERS_ACQUISITIONS_ENERGY, na.rm = TRUE))/bins\n# Real observations blue chart\nggplot(N_per_month, aes(x=date,xend=date,y=0,yend=N)) +\ngeom_segment(aes(color=\"Net Revenue\")) +\nts_theme +\ntheme(legend.position = c(0, 1),legend.justification = c(0, 1))+\nscale_color_manual(values = c(\"#00BFC4\"), breaks = c(\"Net Revenue\")) +\nlabs(colour=\"Legend\", x = \"Calendar Year\", y = \"Number of M&A\", title = \"M&A / Month in Energy Sector (Germany)\") +\ntheme(panel.background = element_rect(fill = \"transparent\"), # bg of the panel\npanel.grid.major = element_blank(), # get rid of major grid\nlegend.background = element_rect(fill = \"transparent\"), # get rid of legend bg\nlegend.box.background = element_rect(fill = \"transparent\")) + theme(legend.position= \"none\") # get rid of legend panel bg\nggsave(p, filename = \"RealMAGermany.png\", height = 7, width = 10, bg = \"transparent\")\n##\nDT_N_subs <- DT_N[Date > as.Date(\"1996-12-31\")]\n## plot distribution vs estimated Poisson distribution #\n#Setting different lambdas\npoisson1 <- rpois(10^6, lambda=2.05)\n# Number of sample points for each lambda\nnsample1 <- floor(nrow(DT_N_subs)*0.25)\nnsample3 <- nrow(DT_N_subs)-nsample1-nsample2\n#Poisson = sample(m_a_pois, nrow(DT_N_subs))\nm_a_pois<-c(sample(poisson1,nsample1), sample(poisson3, nsample3))\n#m_a_pois <- rpois(10^6, mean(DT_N_subs$N))\nreal_vs_pois <- data.table(\"Date\" = DT_N_subs$Date,\"Real\" = DT_N_subs$N, \"Poisson\" = m_a_pois)\nDT_N_subs[, Poisson := sample(m_a_pois, nrow(DT_N_subs))]\nnames(DT_N_subs)[2] <- c(\"Real\")\nDT_N_subs[, Real := as.numeric(Real)]\nDT_N_plot <- melt(DT_N_subs, id.vars = \"Date\", measure.vars = c(\"Poisson\", \"Real\"))\ndataset_distr <- DT_N_plot\n# Poisson dist density\npar(bg=NA)\np <- ggplot(real_vs_pois, aes(x=Date,xend=Date,y=0,yend=Poisson)) +\ngeom_segment(col = \"#F8766D\") + # if yend = Poisson\ncoord_cartesian(ylim = c(max(real_vs_pois$Real),0)) +\nlabs(colour=\"Legend\", x = \"Calendar Year\", y = \"Number of M&A\", title = \"Distribution of M&A / Month in Energy Sector (Germany)\") +\nlegend.box.background = element_rect(fill = \"transparent\")) # get rid of legend panel bg\nggsave(p, filename = \"PoissonMAGermany.png\", height = 7, width = 10, bg = \"transparent\")\n", "output_sequence": "Analyzes mergers & acquisitions on the German energy market, including approximation with Poisson distributions with time varying lambdas"}, {"input_sequence": "# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Fri Oct 25 23:20:12 2019\n\n@author: David\nimport pandas as pd\nimport requests\nfrom bs4 import BeautifulSoup\n# =============================================================================\n# Scraper\ndf_btc_wallets = pd.DataFrame()\nres = requests.get(\"https://bitinfocharts.com/top-100-richest-bitcoin-addresses.html\")\nsoup = BeautifulSoup(res.content,'lxml')\n'''Page 1'''\ntable = soup.find_all('table')[2]\ndf = pd.read_html(str(table))\ndf_top = df[0]\nheader = ['ranking', 'address_full', 'balance', 'percentage_coins', 'first_in', 'number_ins', 'first_out', 'number_outs']\ndf_top.columns = header\ntable = soup.find_all('table')[3]\ndf_bottom = df[0]\ndf_bottom.columns = header\ndf_btc_wallets = df_btc_wallets.append(df_top)\n'''Page 2ff'''\nfor page in range(2, 101):\nprint(\"Appended page: \" + str(page))\n\nres = requests.get(\"https://bitinfocharts.com/top-100-richest-bitcoin-addresses-\" + str(page) + \".html\")\nsoup = BeautifulSoup(res.content,'lxml')\ntable = soup.find_all('table')[0]\ndf = pd.read_html(str(table))\ndf_top = df[0]\ndf_top.columns = header\ntable = soup.find_all('table')[1]\ndf_bottom = df[0]\ndf_bottom.columns = header\ndf_btc_wallets = df_btc_wallets.append(df_top)\n# Data Cleaning\ndf = df_btc_wallets\ndef get_owner(address_full):\n\"\"\"Removes unneccessary chars and numbers from the enity name\n\"\"\"\naddress_full.replace(' ', ' ')\nwallet_owner = address_full.split(' ')[-1]\nnumbers = sum(c.isdigit() for c in wallet_owner)\nlength = len(wallet_owner)\nif numbers <= 1 and length < 25:\nreturn wallet_owner.strip()\nelse:\nreturn \"unknown\"\ndef check_wallet_type(row):\n'''\nEXCHANGE = more than 200 ins\nWHALE = more than 10.000 Bitcoin and less than 200 ins\nif row['owner'] != \"unknown\":\nreturn \"Exchange\"\n#elif row['number_ins'] >= 200:\n# return \"EXCHANGE\"\n#elif row['balance'] >= 10000:\n# return \"WHALE\"\n#else:\n# return \"BIG_FISH\"\ndf[\"address_full\"] = df[\"address_full\"].apply(lambda x: x.replace('wallet:', ' '))\ndf[\"owner\"] = df[\"address_full\"].apply(get_owner)\ndf[\"address\"] = df[\"address_full\"].apply(lambda x: x.split(' ')[0])\ndf[\"balance\"] = df[\"balance\"].replace(regex=[','], value='')\ndf[\"balance\"] = pd.to_numeric(df[\"balance\"])\ndf['category'] = df.apply(check_wallet_type, axis=1)\n# Export\ndf.to_csv('wallets_cleaned.csv', index = False)\ndf = df[['address', 'owner', 'category']]\ndf.to_csv('wallets_bitinfocharts.csv', index = False)\n", "output_sequence": "Scraping and preprocessing of labeled btc addresses from differen websites. Collection of specified transactions from a full bitcoin blockchain node hosted on Google Big Query"}, {"input_sequence": "# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Fri Oct 25 23:20:12 2019\n\n@author: David\nimport pandas as pd\nfrom bs4 import BeautifulSoup\nimport threading\nfrom torrequest import TorRequest #for windows https://github.com/erdiaker/torrequest/issues/2\n# =============================================================================\n# Scraper\n# =============================================================================\naddress_df = pd.read_csv(\"data/unkown_wallets.csv\", index_col=False)\naddress_list = np.array_split(address_df, 50)\nwallet_list = []\ndef scrape_owner(df):\n\"\"\"Function that scrapes addresses and the corresponding entity from\nbitinfocharts.com.\nUsage of tor requests and threading for annonymous scraping in parallel\n\"\"\"\nwith TorRequest(proxy_port=9050, ctrl_port=9051, password=None) as tr:\nresp = tr.get('http://ipecho.net/plain')\nproxy = resp.text\nprint(proxy)\nproxy_list.append(proxy)\ntr.reset_identity()\n\nfor index, row in df.iterrows():\naddress = row['address']\ntry:\nurl = \"https://bitinfocharts.com/bitcoin/address/\" + address\nres = tr.get(url)\nsoup = BeautifulSoup(res.content,'lxml')\ntable = soup.find_all('table')[1]\ndf = pd.read_html(str(table))[0]\nowner = df.iloc[0,3]\nowner = owner.replace('wallet:', ' ').strip()\n\nwallet_list.append([address, owner])\nprint(\"Appended wallet \" + str(len(wallet_list)) + \" (\" + proxy + \")\")\ntime.sleep(random.uniform(1, 2))\nexcept:\nprint(\"Error:\", url, sep=\" \")\ntime.sleep(random.uniform(1,10))\nprint(\">>>finished<<<\")\n\nfor counter, df in enumerate(address_list):\nprint(\"--THREAD \" + str(counter) + \" STARTED\")\nthread_scrape_owner = threading.Thread(target=scrape_owner, args=(df,))\nthread_scrape_owner.start()\ntime.sleep(random.uniform(1,5))\n# Export to csv\nwallets = pd.DataFrame(wallet_list, columns = ['address', 'owner'])\nwallets['category'] = 'Exchange'\nwallets.to_csv('wallets_bitinfocharts_with_numbers.csv', index = False)\ndef remove_digits(address):\n\"\"\"Helper function to separate addresses with real entity names from unknown\nentities consisting only of numbers\nnumbers = sum(c.isdigit() for c in address)\nif numbers <= 2:\nreturn address.strip()\nelse:\nreturn \"unknown\"\nwallets[\"owner\"] = wallets[\"owner\"].apply(remove_digits)\nwallets = wallets[wallets['owner'] != 'unknown']\nwallets.to_csv('wallets_bitinfochart_no_numbers.csv', index = False)\n", "output_sequence": "Scraping and preprocessing of labeled btc addresses from differen websites. Collection of specified transactions from a full bitcoin blockchain node hosted on Google Big Query"}, {"input_sequence": "# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Fri Nov 15 20:33:51 2019\n\n@author: David\nimport pandas as pd\nimport requests\nfrom bs4 import BeautifulSoup\ndf_btc_wallets = pd.DataFrame()\nres = requests.get(\"https://www.cryptoground.com/mtgox-cold-wallet-monitor/\")\nsoup = BeautifulSoup(res.content,'lxml')\n'''Page 1'''\ntable = soup.find_all('table')[3]\ndf = pd.read_html(str(table))[0]\ndf = df[[0]]\ndf.columns = ['address']\ndf['owner'] = 'Mt.Gox'\ndf['category'] = 'Exchange'\ndf.to_csv('data/wallets_cryptoground.csv', index = False)\n", "output_sequence": "Scraping and preprocessing of labeled btc addresses from differen websites. Collection of specified transactions from a full bitcoin blockchain node hosted on Google Big Query"}, {"input_sequence": "# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Tue Dec 10 16:38:40 2019\n\n@author: David\nimport pandas as pd\n# =============================================================================\n# Scrape labeled addresses\n# !Please run scripts manually due to very long runtime!\n'''\n#WalletExplorer.com (20 mio addresses for categories Exchange, Service, Pool, Gambling, Historic)\n#--> sraper_walletexplorer.py\n#Bitinfocharts.com\n#--> scraper_bitinfocharts.py (top 10.000 BTC adresses)\n#--> scraper_bitinfocharts_tor.py (scrape specific addresses one by one. TOR Browser neccessary)\n#Cryptoground\n#--> Scrape specific addresses for Mt.Gox Exchange\n# Preprocess and merge all addresses from different data sources\n#-->wallet_preprocessing.py\n# Get all transactions over a specific value of btc\nfrom bigquery_btc_node import get_all_tx_over_value\nbtc=100\nlarge_tx = get_all_tx_over_value(btc)\nlarge_tx.to_csv(\"transactions_\" + str(btc) +\"BTC.csv\", index=False)\n# Get complete transaction history for samples of addresses per category\n# !GOOGLE DEVELOPER CREDENTIALS NECCESSARY!\nfrom bigquery_btc_node import get_all_tx_from_address\ndf = pd.read_csv(\"../data/btc_wallets.csv\")\nexchange = df[df['category'] == 'Exchange'].sample(n=10000, random_state = 1)['address'].to_list()\ncategory_list = [exchange, gambling, service, mixer, mining]\nfor address_list, category_name in zip(category_list, category_names):\nall_tx = get_all_tx_from_address(address_list)\nall_tx['category'] = category_name\nall_tx.to_csv(\"address_\" + category_name + \".csv\", index=False)\nprint(category_name, \"saved to csv\", sep=\" \")\n# Get complete transaction history for unknown addresses\n# Addresses will be processed in bulk for memory issues\naddress_df = pd.read_csv(\"../data/addresses_unknown.csv\")\ndf_split = np.array_split(df, 10)\ni=0\nfor address_list in df_split:\ni += 1\naddress_list = address_list['address'].to_list()\nall_tx.to_csv(\"address_unknown_chunk_\" + str(i) + \".csv\", index=False)\nprint(str(i) + \"chunks saved to csv\")\n", "output_sequence": "Scraping and preprocessing of labeled btc addresses from differen websites. Collection of specified transactions from a full bitcoin blockchain node hosted on Google Big Query"}, {"input_sequence": "# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Sun Nov 3 19:23:55 2019\n\n@author: David\nimport pandas as pd\nimport requests\nfrom bs4 import BeautifulSoup\nimport re\nimport threading\nimport traceback\nfrom lxml.html import fromstring\nfrom itertools import cycle\nfrom sqlalchemy import create_engine\nimport importlib.util\n#DB connection\nspec = importlib.util.spec_from_file_location(\"module.name\", \"C:/Users/David/Dropbox/Code/config.py\")\nconfig = importlib.util.module_from_spec(spec)\nspec.loader.exec_module(config)\nDB_CREDENTIALS = config.sqlalchemy_DATASTIG_CRYPTO\nengine = create_engine(DB_CREDENTIALS)\n#proxies = pd.read_csv('proxies.txt', names=['ips'], header=None, index_col=False) #https://proxyscrape.com/free-proxy-list\n#proxies = proxies.ips.values.tolist()\nproxies_used = []\ncategories_names = ['Exchange', 'Pools', 'Services', 'Gambling', 'Historic']\ndef scrape_owner(owner, category_name, proxy):\n\"\"\"Function that scrapes addresses and the corresponding entity from\nwalletexplorer.com\nUsage of proxies and threading for annonymous scraping in parallel.\nSaves the scraped addresses in an sql database.\n\"\"\"\n\nurl_pages = \"https://www.walletexplorer.com/wallet/\" + owner\nres, proxy = get_response(proxy, url_pages)\nres = requests.get(url_pages)\nsoup = BeautifulSoup(res.content,'lxml')\ntmp = []\nfor data in soup.find_all('div', class_='paging'):\nfor a in data.find_all('a'):\ntmp.append(a.get('href'))\ntmp = tmp[-1]\nmax_page = re.findall(r'\\d+', tmp)[0]\nmax_page = int(max_page)\nprint(\"Total Pages for\", owner, max_page, sep = \" \")\n\nfor page in range(1,max_page):\ntry:\nurl = \"https://www.walletexplorer.com/wallet/\" + owner + \"/addresses?page=\" + str(page)\nres, proxy = get_response(proxy, url)\nsoup = BeautifulSoup(res.content,'lxml')\ntable = soup.find_all('table')[0]\ndf = pd.read_html(str(table))[0]\ndf['owner'] = owner\ndf['category'] = category_name\ndf = df.drop(columns=['balance', 'incoming txs', 'last used in block'])\n#df_all_wallets = df_all_wallets.append(df)\ndf.to_sql(\"wallets_new\", engine, index=False, if_exists='append')\nprint(proxy, owner, \"appended page: \",str(page) , sep=\" \")\ntime.sleep(1)\nexcept Exception :\nprint(traceback.format_exc())\nbreak\n\nprint(\">>>\" + owner + \" + str(page) + \" finished<<<\")\nproxies_used.remove(proxy)\n\n# =============================================================================\n# Proxies\ndef get_proxies():\n\"\"\"Scrapes a list of proxies from free-proxy-list.net\n\"\"\"\nurl = 'https://free-proxy-list.net/'\nresponse = requests.get(url)\nparser = fromstring(response.text)\nproxies = set()\nfor i in parser.xpath('//tbody/tr')[:100]:\nif i.xpath('.//td[7][contains(text(),\"yes\")]'):\n#Grabbing IP and corresponding PORT\nproxy = \":\".join([i.xpath('.//td[1]/text()')[0],\nproxies.add(proxy)\nreturn proxies\ndef find_working_proxy(url):\n\"\"\"Helper function that returns a proxie that is working for\nproxies = get_proxies()\nproxy_pool = cycle(proxies)\nfound_proxy = False\ntest_proxy = next(proxy_pool)\nif test_proxy not in proxies_used:\nproxies_used.append(test_proxy)\nprint(len(proxies_used), \"/\" ,len(proxies), \" proxies used\", sep =\"\")\ntry:\nres = requests.get(url, proxies={\"http\": test_proxy, \"https\": test_proxy})\nprint(test_proxy + \" connected\")\nfound_proxy = True\nreturn res, test_proxy\nexcept:\nprint(test_proxy + \" not working. Skipping\")\ndef get_response(proxy, url):\n\"\"\"Helper function that checks if the proxies is working for specified url\n\"\"\"\nfound_proxy = False\nwhile found_proxy == False:\ntry:\nres = requests.get(url, proxies={\"http\": proxy, \"https\": proxy})\nfound_proxy = True\nreturn res, proxy\nexcept:\nprint(\"connection error \" + owner)\nres, proxy = find_working_proxy(url)\nreturn res, proxy\n# START\nprint('Searching for connection. Please wait')\nurl = \"https://www.walletexplorer.com/\"\nres, proxy = find_working_proxy(url)\nsoup = BeautifulSoup(res.content,'lxml')\ncategories_lists = soup.find_all('ul')\nfor counter, category in enumerate(categories_names):\n#if counter == 1 or counter == 2 or counter == 3:\ncategory_name = categories_names[counter]\n\nowners_list = []\nfor litag in categories_list.find_all('li'):\npattern = r'\\(.*?\\)'\nowner = litag.text\nowner = re.sub(pattern, '', owner).strip()\nowners_list.append(owner)\n\nfor counter, owner in enumerate(owners_list):\nproxy = find_working_proxy(\"https://www.walletexplorer.com/\")\nprint(\"--THREAD \" + str(counter) + \"/\" + str(len(owners_list)) + \" STARTED \" + owner + \"--\")\nthread_scrape_owner = threading.Thread(target=scrape_owner, args=(owner, category_name, proxy))\nthread_scrape_owner.start()\n", "output_sequence": "Scraping and preprocessing of labeled btc addresses from differen websites. Collection of specified transactions from a full bitcoin blockchain node hosted on Google Big Query"}, {"input_sequence": "# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Thu Dec 5 12:19:36 2019\n\n@author: David\nimport pandas as pd\n# =============================================================================\n# Load Data Sources\n# address, owner, category\nwallets = pd.DataFrame()\nadr_walletexplorer = pd.read_csv(\"../data/wallets_walletexplorer.csv\", index_col=False)\nwallets = wallets.append(adr_walletexplorer)\nadr_bitinfocharts = pd.read_csv(\"../data/wallets_bitinfochart.csv\", index_col=False)\nwallets = wallets.append(adr_bitinfocharts)\nadr_cryptoground = pd.read_csv(\"../data/wallets_cryptoground.csv\", index_col=False)\nwallets = wallets.append(adr_cryptoground)\nadr_bitinfocharts_missing = pd.read_csv(\"../data/wallets_bitinfochart_no_numbers.csv\", index_col=False)\nwallets = wallets.append(adr_bitinfocharts_missing)\nwallets = wallets.dropna()\nwallets = wallets[['address', 'owner', 'category']]\nwallets = wallets.drop_duplicates(subset='address')\n# Recategorize specific categories\nwallets.loc[wallets.owner == 'MiddleEarthMarketplace', 'category'] = 'Service'\n#change specific owner and category values\nwallets.loc[wallets.owner == 'Xapo.com-2', 'owner'] = 'Xapo.com'\nwallets.loc[wallets.owner == 'DPR Seized Coins 2', 'category'] = 'Service'\n#Change duplicate name\nwallets.loc[wallets['owner'].str.contains('HelixMixer'), 'owner'] = 'HelixMixer'\n#Remove Historic\nwallets = wallets[wallets['category'] != 'Historic']\n# Export data to CSV\nwallet_owners = pd.DataFrame()\nwallet_owners = wallets.groupby(['owner', 'category']).agg(['count'], as_index=False).reset_index()\nwallet_owners.columns = ['owner', 'category', 'count']\nwallet_owners.to_csv(\"btc_wallets_owner.csv\", index = False)\n#Export to csv\nwallets.to_csv(\"btc_wallets.csv\", index = False)\n", "output_sequence": "Scraping and preprocessing of labeled btc addresses from differen websites. Collection of specified transactions from a full bitcoin blockchain node hosted on Google Big Query"}, {"input_sequence": "# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Wed Nov 6 18:23:51 2019\n\n@author: David\nhttps://console.cloud.google.com/bigquery?project=crypto-257815&folder&organizationId&p=bigquery-public-data&d=crypto_bitcoin&t=transactions&page=table\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom google.cloud import bigquery #pip install google-cloud-bigquery\nos.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=r\"C:\\Users\\David\\Dropbox\\Code\\Crypto-4c44e65fd97d.json\" #https://cloud.google.com/docs/authentication/getting-started\nclient = bigquery.Client()\n# =============================================================================\n# Helpers\ndef get_atts(obj, filter=\"\"):\n\"\"\"Helper function wich prints the public attributes and methods of the given object.\nCan filter the results for simple term.\n\"\"\"\nreturn [a for a in dir(obj) if not a.startswith('_') and filter in a]\ndef estimate_gigabytes_scanned(query, bq_client):\n\"\"\"A function to estimate query size.\nmy_job_config = bigquery.job.QueryJobConfig()\nmy_job_config.dry_run = True\nmy_job = bq_client.query(query, job_config=my_job_config)\nBYTES_PER_GB = 2**30\nestimate = my_job.total_bytes_processed / BYTES_PER_GB\nprint(f\"This query will process {estimate} GBs.\")\n# get transactions with max transaction value\ndef get_all_tx_over_value(btc):\n\"\"\"Pulls all transactions where the receiver of the transaction received\nmore than a specified BTC value\nbtc_satoshi = 100000000 #btc in satoshi\nsatoshi_amount = btc * btc_satoshi\n\nquery = \"\"\"\nSELECT\n`hash`,\nblock_timestamp,\narray_to_string(inputs.addresses, \",\") as sender,\noutput_value / 100000000 as value\nFROM `bigquery-public-data.crypto_bitcoin.transactions`\nJOIN UNNEST (inputs) AS inputs\nWHERE outputs.value >= @satoshis\nAND inputs.addresses IS NOT NULL\nGROUP BY `hash`, block_timestamp, sender, receiver, value\n\nquery_params = [\nbigquery.ScalarQueryParameter(\"satoshis\", \"INT64\", satoshi_amount),\n]\nestimate_gigabytes_scanned(query, client)\njob_config = bigquery.QueryJobConfig()\njob_config.query_parameters = query_params\nquery_job = client.query(\nquery,\njob_config=job_config,\n)\nresult = query_job.result()\ntx = result.to_dataframe()\nprint(\"Successfully pulled transactions from bigquery\")\nreturn tx\n# get all transactions for a list of addresses\n# =============================================================================\ndef get_all_tx_from_address(list_addresses):\n\"\"\"Pulls the complete transaction history from a list of specified\nBTC addresses\nSELECT\narray_to_string(i.addresses, \",\") as address,\ni.transaction_hash,\ni.block_number,\ni.value / 100000000 as value_btc ,\nt.`hash`,\nt.block_timestamp,\nt.input_count,\nt.output_value / 100000000 as tx_value_btc ,\n'input' as type\n\nFROM `bigquery-public-data.crypto_bitcoin.inputs` as i\nINNER JOIN `bigquery-public-data.crypto_bitcoin.transactions` AS t ON t.hash = i.transaction_hash\nWHERE array_to_string(i.addresses, ',') in UNNEST(@address)\nUNION ALL\narray_to_string(o.addresses, \",\") as address,\no.transaction_hash,\no.block_number,\no.value / 100000000 as value_btc ,\n'output' as type\nFROM `bigquery-public-data.crypto_bitcoin.outputs` as o\nINNER JOIN `bigquery-public-data.crypto_bitcoin.transactions` AS t ON t.hash = o.transaction_hash\nWHERE array_to_string(o.addresses, ',') in UNNEST(@address)\nprint(estimate_gigabytes_scanned(query, client))\nbigquery.ArrayQueryParameter(\"address\", \"STRING\", list_addresses),\nquery,\njob_config=job_config,\ntx = result.to_dataframe()\ntx = tx.drop(columns=['transaction_hash'])\n\n", "output_sequence": "Scraping and preprocessing of labeled btc addresses from differen websites. Collection of specified transactions from a full bitcoin blockchain node hosted on Google Big Query"}, {"input_sequence": "import requests\nimport pandas as pd\nimport numpy as np\nimport datetime\n#Source: https://blog.cryptocompare.com/cryptocompare-api-quick-start-guide-f7abbd20d260\n\n#This function establishes a request to the online plattform BitFinex:\ndef get_data_spec(coin, date, time_period):\n\"\"\" Query the API for the historical price data starting from \"date\". \"\"\"\nurl = \"https://min-api.cryptocompare.com/data/{}?fsym={}&e=BitFinex&tsym=USD&toTs={}\".format(time_period, coin, date)\nr = requests.get(url)\nipdata = r.json()\nreturn ipdata\n#This function collects the cryptocurrency data from for the specified time period:\ndef get_df_spec(time_period, coin, from_date, to_date):\n\"\"\" Get historical price data between two dates. If further apart than query limit then query multiple times. \"\"\"\ndate = to_date\nholder = []\nwhile date > from_date:\n# Now we use the new function to query specific coins\ndata = get_data_spec(coin, date, time_period)\nholder.append(pd.DataFrame(data[\"Data\"]))\ndate = data['TimeFrom']\n\ndef compute_Returns(data):\ndata[\"Average_price\"]=(data[\"close\"]+data[\"open\"])/2\ndata[\"returns\"]=data[\"Average_price\"].divide(data[\"Average_price\"].shift())-1\ndata= data.iloc[1:]\n%%time\nhourly_df = {}\nstart_unix = time.mktime(datetime.datetime.strptime(\"01/05/2019\", \"%d/%m/%Y\").timetuple())\nerrorcoin = [] # for coins with no data avaliable\nfor coin in coins:\ntry:\nhourly_df[coin] = compute_Returns(get_df_spec('histohour',coin, start_unix, end_unix))\nexcept:\nprint(coin + ': error')\nimport bitfinex\n#Create api instance of the v2 API\napi_v2 = bitfinex.bitfinex_v2.api_v2()\nc = 'zec'\npair = c+'usd' # Currency pair of interest\nbin_size = '1h' # This will return minute data\nlimit = 5000 # We want the maximum of 5000 data points\n# Define the start date\nt_start = datetime.datetime(2017, 11, 0, 0)\nt_start = time.mktime(t_start.timetuple()) * 1000\n# Define the end date\nt_stop = datetime.datetime(2018, 5, 1, 0, 0)\nt_stop = time.mktime(t_stop.timetuple()) * 1000\npair_data = api_v2.candles(symbol=pair, interval=bin_size,\nlimit=limit, start=t_start, end=t_stop)\nimport pandas as pd\n# Create pandas data frame and clean/format data\nnames = ['time', 'open', 'close', 'high', 'low', 'volume']\ndf = pd.DataFrame(pair_data, columns=names)\ndf.drop_duplicates(inplace=True)\ndf['time'] = pd.to_datetime(df['time'], unit='ms')\ndf.set_index('time', inplace=True)\nzec = compute_Returns(df)\ndef merge_dfs_on_column(dataframes, labels, col):\n'''Merge a single column of each dataframe into a new combined dataframe'''\nseries_dict = {}\nfor index in range(len(dataframes)):\nseries_dict[labels[index]] = dataframes[index][col]\nhourlyPrice = merge_dfs_on_column(list(hourly_df.values()), list(hourly_df.keys()), 'Average_price')\nhourlyPrice.to_csv('1718hourlyPrice_withZEC.csv')\n## 5. Macro Index\n### S&P 500\nsp500 = pd.read_csv('./S&P 500 Historical Data.csv')\nfrom datetime import datetime\n# convert str to datetime\nsp500.Date = sp500.Date.apply(lambda x: pd.to_datetime(datetime.strptime(x, '%b %d, %Y')))\n# generate hourly time index\ntime_idx = pd.date_range(sp500.Date.iloc[-1], sp500.Date[0], freq=\"1h\")\n# set time as index\nsp500.set_index('Date',inplace=True)\n# reverse dataframe, now the date is ascending\n# because sp500 is daily data and only for work days\n# create new df with new hourly idx\ndf = pd.DataFrame(time_idx)\ndf.columns =['Datetime']\ndf.set_index('Datetime',inplace=True)\ndf['sp500']= sp500['Price']\n# fill nan with prescending value\ndf['sp500']=df['sp500'].str.replace(',', '')\n# calculate return\nsp500RT = df[\"sp500\"].divide(df[\"sp500\"].shift())-1\nimport matplotlib.pyplot as plt\nplt.figure(figsize=[20,10])\n### VIX\nvix = pd.read_csv('./vixcurrent.csv')\nvix.columns = vix.iloc[0].values\nvix.drop(0,inplace=True)\nvix.Date =vix.Date.apply(lambda x: pd.to_datetime(datetime.strptime(x, '%m/%d/%Y')))\nvix.set_index('Date',inplace= True)\nvix = vix.astype(float)\ndf['vix']=vix['Price']\nvixRT = df[\"vix\"].divide(df[\"vix\"].shift())-1\n", "output_sequence": "1) Retrieval of the hourly price data of cryptocurrencies via API call 2) Calculation of the hourly returns 3) Retrieval of the daily macro-economic variables via API call and transform it into an hourly data 4) For macro Index S&P 500 and VIX, NaN (value on weekends and holidays) were filled with the value of previous the non NaN value"}, {"input_sequence": "# clear all variables\nrm(list = ls(all = TRUE))\ngraphics.off()\n# install and load packages\nlibraries = c(\"MASS\", \"dr\", \"scatterplot3d\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# parameter settings\nset.seed(2010)\nn = 300 # number of observations\nx = cbind(rnorm(n), rnorm(n), # n x 3 matrix, the explanatory variable\ne = rnorm(n) # n vector, the noise variable\nb2 = c(1, -1, # projection vector\ny = x %*% b1 + ((x %*% b1)^3) + 4 * ((x %*% b2)^2) + e # n vector, the response variable\nedr = dr(y ~ x, method = \"sir\", nslices = h) # effective dimension reduction space\nf = edr$evectors # matrix of the estimated EDR-directions\ng = edr$evalues # vector of eigenvalues\n# matrices for the true indices and the true responses\nm1 = cbind(x %*% b1, y)\nm1 = m1[order(m1[, 1]), ]\nsg = sum(g)\ng = g/sg\npsi = c(g[1], g[1] + g[2], g[1] + g[2] + g[3]) # the ratio of the sum of the 3 eigenvalues and the sum of all eigenvalues\npar(mfcol = c(2, 2), mgp = c(2, 1, 0))\n# plot of the response versus the first estimated EDR-direction\np11 = cbind(x %*% f[, 1], y)\nplot(p11, col = \"blue\", xlab = \"first index\", ylab = \"response\", main = \"XBeta1 vs Response\",\ncex.lab = 1.2, cex.main = 1.2, cex.axis = 0.8)\n# plot of the response versus the second estimated EDR-direction\np21 = cbind(x %*% f[, 2], y)\nplot(p21, col = \"blue\", xlab = \"second index\", ylab = \"response\", main = \"XBeta2 vs Response\",\n# three-dimensional plot of the first two directions and the response\np12 = cbind(x %*% f[, 1], x %*% f[, 2], y)\nscatterplot3d(p12, xlab = \"first index\", ylab = \"second index\", zlab = \"response\",\nbox = TRUE, axis = TRUE, color = \"blue\", main = \"XBeta1 Response\", grid = FALSE,\ncex.axis = 0.6)\ni = c(1, 2, 3)\nig = cbind(i, g)\np22 = rbind(ig, psii)\n# plot of the eigenvalues and the cumulative sum\nplot(p22, xlab = \"K\", ylab = \"Psi(k) Eigenvalues\", main = \"Scree Plot\", pch = c(8,\n8, 1, 1), cex.lab = 1.2, cex.main = 1.2, cex.axis = 0.8)\ndev.new()\n# plots of the true response versus the true indices\nplot(m1, xlab = \"first index\", ylab = \"response\", main = \"First true index vs Response\",\ncex.lab = 1.2, cex.axis = 1.2, cex.main = 1.8)\nplot(m2, xlab = \"second index\", ylab = \"response\", main = \"Second true index vs Response\",\n", "output_sequence": "Generates a data set and applies the sliced inverse regression algorithm (SIR) for dimension reduction."}, {"input_sequence": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = pd.read_csv(\"journaux.dat\", sep = \"\\s+\", header=None)\na = x.sum(axis = 1)\ne = np.reshape(np.array(a), (len(a), -1)) @ (np.reshape(np.array(b), (-1, len(b)))/sum(a))\n# chi-matrix\ncc = (x - e)/np.sqrt(e)\ng, l, d = np.linalg.svd(cc, full_matrices = False)\nll = l**2 #eigenvalues\naux = np.cumsum(ll)/sum(ll)\nperc = np.vstack((ll, aux))\nr1 = np.multiply(np.tile(l, (g.shape[0], 1)), g)\nr = r1/np.tile(np.reshape(np.array(np.sqrt(a)), (len(a), -1)), (1, g.shape[1]))\ns1 = np.multiply(np.tile(l, (d.shape[0], 1)), d.T)\ns = s1/np.tile(np.reshape(np.array(np.sqrt(b)), (len(b), -1)), (1, d.shape[1]))\n# contribution in r\ncar = np.tile(np.reshape(np.array(a), (len(a), -1)), (1, r.shape[1])) \\\n# contribution in s\ncas = np.tile(np.reshape(np.array(b), (len(b), -1)), (1, s.shape[1])) \\\n\ntypes = [\"va\", \"ff\", \"bj\",\n\"bl\", \"vm\", \"fn\", \"fo\"]\nregions = [\"brw\", \"bxl\", \"anv\", \"brf\", \"foc\", \"hai\", \"lig\", \"lux\"]\nfig, ax = plt.subplots(figsize = (7, 7))\nax.scatter(r[:, 0], r[:, 1], c = \"w\")\nfor i in range(0, len(types)):\nax.text(r[i, 0], r[i, 1], types[i], c = \"b\", fontsize = 16)\nfor i in range(0, len(regions)):\nax.text(s[i, 0], s[i, 1], regions[i], c = \"r\", fontsize = 14)\nax.set_xlim(-1.2, )\nax.axvline(0, c = \"k\")\nax.set_xlabel(\"r_1, s_1\", fontsize = 14)\nplt.title(\"Journal Data\", fontsize = 18)\nplt.show()\n", "output_sequence": "Performs a correspondence analysis for the Belgian journal data, shows the eigenvalues of the singular value decomposition of the chi-matrix and displays graphically its factorial decomposition."}, {"input_sequence": "# clear all variables\nrm(list = ls(all = TRUE))\ngraphics.off()\n# load data\nx = read.table(\"journaux.dat\")\na = rowSums(x)\ne = matrix(a) %*% b/sum(a)\n# chi-matrix\ncc = (x - e)/sqrt(e)\n# singular value decomposition\nsv = svd(cc)\ng = sv$u\n# eigenvalues\nll = l * l\n# cumulated percentage of the variance\naux = cumsum(ll)/sum(ll)\nperc = cbind(ll, aux)\nr1 = matrix(l, nrow = nrow(g), ncol = ncol(g), byrow = T) * g\ncar = matrix(matrix(a), nrow = nrow(r), ncol = ncol(r), byrow = F) * r^2/matrix(l^2,\nnrow = nrow(r), ncol = ncol(r), byrow = T) # contribution in r\ncas = matrix(matrix(b), nrow = nrow(s), ncol = ncol(s), byrow = F) * s^2/matrix(l^2,\nnrow = nrow(s), ncol = ncol(s), byrow = T) # contribution in s\nrr = r[, 1:2]\n# labels for journals\ntypes = c(\"va\", \"vb\", \"ff\", \"bj\",\n\"vm\", \"fn\", \"fo\")\n# labels for regions\nregions = c(\"brw\", \"bxl\", \"anv\", \"brf\", \"foc\", \"hai\", \"lig\", \"lux\")\n# plot\nplot(rr, type = \"n\", xlim = c(-1.1, 1.5), ylim = c(-1.1, 0.6), xlab = \"r_1,s_1\",\nylab = \"r_2,s_2\", main = \"Journal Data\", cex.axis = 1.2, cex.lab = 1.2, cex.main = 1.6)\npoints(ss, type = \"n\")\ntext(rr, types, cex = 1.5, col = \"blue\")\ntext(ss, regions, col = \"red\")\nabline(h = 0, v = 0, lwd = 2)\n", "output_sequence": "Performs a correspondence analysis for the Belgian journal data, shows the eigenvalues of the singular value decomposition of the chi-matrix and displays graphically its factorial decomposition."}, {"input_sequence": "# clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# install and load packages\nlibraries = c(\"kernlab\", \"tseries\", \"quadprog\", \"zoo\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# load data and create an SVM classification model of type 'ksvm\nG = read.matrix(\"Bankruptcy100by100noNA.txt\", header = TRUE, sep = \"\")\nbankmodel_draw = ksvm(G[, c(7, 28)], G[, c(3)], type = \"C-svc\", kernel = \"rbfdot\",\nkpar = list(sigma = 1/2), C = 1)\n# Print output of SVM\nprint(bankmodel_draw)\n# Create plot of SVM classification model\nplot(bankmodel_draw, data = G[, c(7, 28)])\n", "output_sequence": "Plots an SVM classification plot of bankruptcy data set with sigma = 2 and C = 1."}, {"input_sequence": "# clear all variables\nrm(list = ls(all = TRUE))\ngraphics.off()\n# load data\ndata = read.table(\"bostonh.dat\")\nx = data\n# data transfomation\nK = as.numeric(data[, 14] > median(data[, 14])) + 1\nx = cbind(log(x[, 1]), x[, 2], x[, 3], x[, 4], log(x[, 5]), log(x[, 14]), K)\n# subset creation for subset means\nz = data.frame(x)\nz1 = subset(z, z$K == 1)\nm1 = apply(z1, 2, mean)\ni = 0\nop = par(mfrow = c(6, 6), cex = 0.15)\nwhile (i < 6) {\ni = i + 1\nj = 0\nwhile (j < 6) {\nj = j + 1\n\nif (i == j) {\nboxplot(x[, i] ~ K, at = 1:2, axes = FALSE)\nlines(c(0.6, 1.4), c(m1[i], lty = \"dotted\", lwd = 1.2, col = \"red3\")\n}\nif (i > j) {\nyy = cbind(x[, j], x[, i], K)\nplot(yy[, -3], col = as.numeric(K), xlab = \"X\", ylab = \"Y\", cex = 4,\naxes = FALSE)\nif (i < j) {\nplot(i, type = \"n\", axes = FALSE, xlab = \"\", ylab = \"\", main = \"\")\n}\n}\ntitle(main = list(\"Scatterplot matrix for transformed Boston Housing\", cex = 8),\nline = -16, outer = TRUE)\n", "output_sequence": "Plots the scatterplot matrix for the transformed Boston housing data variables X1, ... , X5 and X14."}, {"input_sequence": "# clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# install and load packages\nlibraries = c(\"lattice\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# Define x1 and x2 between [-1:1].\nx1 = seq(-1, 1, 0.05)\nn1 = length(x1)\nx2 = seq(-1, 1, 0.05)\nn2 = length(x2)\n# Set beta\nb = c(20, 1, 2, -8, 6)\nL = NULL\n# Calculate y\nfor (i in 1:n1) {\nxi = x1[i]\ntemp = NULL\nfor (j in 1:n2) {\nxj = x2[j]\nLij = b[1] + b[2] * xi + b[3] * xj + b[4] * xi^2 + b[5] * xj^2 + b[6] * xi *\nxj\ntemp = cbind(temp, Lij)\n}\nL = rbind(L, temp)\n}\nwireframe(L, drape = T, xlab = list(\"X2\", rot = 30, cex = 1.2), main = expression(paste(\"3-D response surface\")),\nylab = list(\"X1\", rot = -40, cex = 1.2), zlab = list(\"Y\", cex = 1.1), scales = list(arrows = FALSE,\ncol = \"black\", distance = 1, tick.number = 8, cex = 0.7, x = list(at = seq(1,\n41, 5), labels = round(seq(-1, 1, length = 9), 1)), y = list(at = seq(1,\ndev.new()\ncontour(L, col = rainbow(15), xlab = \"X1\", ylab = \"X2\", main = expression(paste(\"Contour plot\")),\n)\n", "output_sequence": "Plots 3D response surfaces and a contour plot for the variable y and the two factors that explain the variation of y via the quadratic response model."}, {"input_sequence": "# clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# install and load packages\nlibraries = c(\"glmnet\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# load data\ndata = read.table(\"carc.dat\")\n# recode the response, y = 1 for y > 6000, otherwise y = 0\ny = data[, 2]\nn = length(y)\ny = ifelse(y <= 6000, 0, 1)\nx1 = data[, 3]\nx = cbind(x2, x3, x10, x12)\n# lasso\nalpa = 1\n(lasso.regress = glmnet(x, y, family = \"binomial\", alpha = alpa, nlambda = 100))\nsummary(lasso.regress)\n# extract coefficients at a single value of lambda\ncoef(lasso.regress, s = 0.01)\n# make predictions\nyfit = predict(lasso.regress, newx = x[1:n, ], s = c(0.01))\n# plot estimates\nwin.graph()\nplot(lasso.regress, lwd = 3, main = \"Lasso estimates\")\n# plot predictions\nplot(yfit, y, main = \"Lasso predictions\")\n", "output_sequence": "Performs a logit model using the Lasso methodology. The estimates become nonzero at a point that means the variables enter the model equation sequentially as the shrinkage parameter increases. The Lasso technique results in variable selection. Finally, the resulting Lasso estimates and predictions are plotted."}, {"input_sequence": "%% clear all variables and console and close windows\nclear\nclc\nclose all\nx = [[3 2 1 10]; [2 7 3 4]];\nd = dist(x);\ndelta = [1 2 3 d(1,2)\n1 3 2 d(1,3)\nfig = [1 d(2,3)\n3 d(1,2)\n\n%% plot\nscatter(fig(:, 1), fig(:, 2), 'b', 'fill')\nxlim([0 7])\ntitle('Dissimilarities and Distances')\nxlabel('Dissimilarity')\nylabel('Distance')\nfor i=1:5\nline([fig(i, 1) fig(i + 1, 1)], [fig(i, 2) fig(i + 1, 2)],...\n'Color', 'k', 'LineWidth',1.5)\nend\nlabels = {'(2,3)', '(3,4)'};\ntext(fig(:, 1) + 0.2, fig(:, 2), labels, 'Color', 'r')\n", "output_sequence": "Illustrates the PAV algorithm for nonmetric MDS for car marks data."}, {"input_sequence": "import numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.array([[3, 2, 1, 10], [2, 7, 3, 4]]).T\nd = np.zeros([len(x), len(x)])\nfor i in range(0, len(x)):\nd[i, j] = np.linalg.norm(x[i, :] - x[j, :])\nf1 = [1, 2]]\ny = [d[1, 2], d[0, 2], d[0, 1], d[1, 3], d[0, 3], d[2, 3]]\nlabels = [\"(2,3)\", \"(3,4)\"]\nfig, ax = plt.subplots(figsize = (7, 7))\nax.plot(range(1, 7), y, c = \"k\")\nax.scatter(range(1, 7), y)\nfor i in range(0, 6):\nax.text(i+1 +0.05, y[i] +0.1, labels[i], fontsize = 14, c = \"r\")\nplt.xlim(0, 7)\nplt.xlabel(\"Dissimilarity\")\nplt.title(\"Dissimilarities and Distances\")\n", "output_sequence": "Illustrates the PAV algorithm for nonmetric MDS for car marks data."}, {"input_sequence": "# clear all variables\nrm(list = ls(all = TRUE))\ngraphics.off()\nx = cbind(c(3, 2, 1, 10), c(2, 7, 3, 4))\nd = as.matrix(dist(x))\nd1 = c(1, 2, 3, d[1, 2])\ndelta = cbind(d1, d2, d6)\nf1 = c(1, d[2, 3])\nfig = rbind(f1, f2, f6)\n# plot\nplot(fig, pch = 15, col = \"blue\", xlim = c(0, 7), ylim = c(0, 10), xlab = \"Dissimilarity\",\nylab = \"Distance\", main = \"Dissimilarities and Distances\", cex.axis = 1.2, cex.lab = 1.2,\ncex.main = 1.8)\nlines(fig, lwd = 3)\ntext(fig, labels = c(\"(2,3)\", \"(1,2)\", pos = 4,\ncol = \"red\")\n", "output_sequence": "Illustrates the PAV algorithm for nonmetric MDS for car marks data."}, {"input_sequence": "# clear all variables\nrm(list = ls(all = TRUE))\ngraphics.off()\nrAC = function(name, n, d, theta) {\nillegalpar = switch(name, clayton = (theta < 0), gumbel = (theta < 1), frank = (theta <\n0), BB9 = ((theta[1] < 1) | (theta[2] < 0)), GIG = ((theta[2] < 0) | (theta[3] <\n0))))\nif (illegalpar)\nstop(\"Illegal parameter value\")\nindependence = switch(name, clayton = (theta == 0), gumbel = (theta == 1), frank = (theta ==\n0), BB9 = (theta[1] == 1), GIG = FALSE)\nU = runif(n * d)\nU = matrix(U, nrow = n, ncol = d)\nif (independence)\nreturn(U)\nY = switch(name, clayton = rgamma(n, 1/theta), gumbel = rstable(n, 1/theta) *\n(cos(pi/(2 * theta)))^theta, frank = rFrankMix(n, theta), BB9 = rBB9Mix(n,\ntheta), GIG = rGIG(n, theta[1],\nY = matrix(Y, nrow = n, ncol = d)\nphi.inverse = switch(name, clayton = function(t, theta) {\n(1 + t)^(-1/theta)\n}, gumbel = function(t, theta) {\nexp(-t^(1/theta))\n}, frank = function(t, theta) {\n(-1/theta) * log(1 - (1 - exp(-theta)) * exp(-t))\n}, BB9 = function(t, theta) {\nexp(-(theta[2]^theta[1] + t)^(1/theta[1]) + theta[2])\n}, GIG = function(t, theta) {\nlambda = theta[1]\nif (chi == 0) out = (1 + 2 * t/psi)^(-lambda) else if (psi == 0) out = 2^(lambda +\n1) * exp(besselM3(lambda, sqrt(2 * chi * t), logvalue = TRUE) - lambda *\nlog(2 * chi * t)/2)/gamma(-lambda) else out = exp(besselM3(lambda, sqrt(chi *\n(psi + 2 * t)), logvalue = TRUE) + lambda * log(chi * psi)/2 - besselM3(lambda,\nout\n})\nphi.inverse(-log(U)/Y, theta)\n}\nrstable = function(n, alpha, beta = 1) {\nt0 = atan(beta * tan((pi * alpha)/2))/alpha\nTheta = pi * (runif(n) - 0.5)\nW = -log(runif(n))\nterm1 = sin(alpha * (t0 + Theta))/(cos(alpha * t0) * cos(Theta))^(1/alpha)\nterm2 = ((cos(alpha * t0 + (alpha - 1) * Theta))/W)^((1 - alpha)/alpha)\nterm1 * term2\n# special call to rAC for backwards compatibility\nrcopula.gumbel = function(n, theta, d) {\nrAC(\"gumbel\", n, d, theta)\nsample.gc = rcopula.gumbel(10000, theta = 3, d = 2)\n# using qnorm to apply sigma = 1\nsample.metagc = apply(sample.gc, 2, qnorm)\n# plot\nplot(sample.metagc, xlim = c(-4, 4), ylim = c(-4, 4), xlab = \"u\", ylab = \"v\", lwd = 3,\ncex.axis = 2, cex.lab = 2)\ntitle(\"Sample for fixed theta and sigma\")\n", "output_sequence": "Produces Gumbel-Hougaard copula sampling for fixed parameters sigma and theta."}, {"input_sequence": "# clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# install and load packages\nlibraries = c(\"MASS\", \"mnormt\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# parameter settings\nn = 200 # number of draws\nmu = c(3, 2) # mean vector\nsig = matrix(c(1, -1.5, 4), ncol = 2) # covariance matrix\n# bivariate normal sample\nset.seed(80)\ny = mvrnorm(n, mu, sig, 2)\n# bivariate normal density\nxgrid = seq(from = (mu[1] - 3 * sqrt(sig[1, 1])), to = (mu[1] + 3 * sqrt(sig[1, 1])),\nlength.out = 200)\nygrid = seq(from = (mu[2] - 3 * sqrt(sig[2, 2])), to = (mu[2] + 3 * sqrt(sig[2, 2])),\nz = outer(xgrid, ygrid, FUN = function(xgrid, ygrid) {\ndmnorm(cbind(xgrid, ygrid), mean = mu, varcov = sig)\n# Plot\npar(mfrow = c(1, 2))\nplot(y, col = \"black\", ylab = \"X2\", xlab = \"X1\", xlim = range(xgrid), ylim = range(ygrid))\ntitle(\"Normal sample\")\n# Contour ellipses\ncontour(xgrid, ygrid, z, xlim = range(xgrid), ylim = range(ygrid), nlevels = 10, col = c(\"blue\",\n\"black\", \"yellow\", \"cyan\", \"red\", \"magenta\", \"green\", \"blue\", \"black\"), lwd = 3,\ncex.axis = 1, xlab = \"X1\", ylab = \"X2\")\ntitle(\"Contour Ellipses\")\n", "output_sequence": "Computes a scatterplot of a normal sample and the contour ellipses for mu =(3,2) and sigma = (1,-1.5)~(-1.5,4)."}, {"input_sequence": "# clear all variables\nrm(list = ls(all = TRUE))\ngraphics.off()\n# load data\nx = read.table(\"bostonh.dat\")\nxt = x\n# Transformations\nxt[, 1] = log(x[, 1])\nxt[, 7] = ((x[, 7]^2.5))/10000\nxt[, 8] = log(x[, 8])\nxt[, 11] = exp(0.4 * x[, 11])/1000\nxt[, 13] = sqrt(x[, 13])\nZ = cbind(rep(1, length(xt[, 1])), xt[, 1], xt[, 2], xt[, 3], xt[, 4], xt[, 5], +xt[,\n6], xt[, 7], xt[, 8], xt[, 9], xt[, 10], xt[, 11], xt[, 12], xt[, 13])\ny = xt[, 14]\nmn = dim(Z)\ndf = mn[1] - mn[2]\nb = solve(t(Z) %*% Z) %*% t(Z) %*% y\nyhat = Z %*% b\nr = y - yhat\nmse = t(r) %*% r/df\ncovar = solve(t(Z) %*% Z) %*% diag(rep(mse, 14))\nse = sqrt(diag(covar))\nt = b/se\nt2 = abs(t)\nk = t2^2/(df + t2^2)\np = 0.5 * (1 + sign(t2) * pbeta(k, 0.5, * df))\nPvalues = 2 * (1 - p)\ntablex = cbind(round(b, 4), round(se, 4), round(t, 3), round(Pvalues, 4))\nprint(\"Table with coefficient estimates, Standard error, value of the +\\nt-statistic and p-value (for the intercept (first line) and the 13 +\\nvariables (lines 2 to 14))\")\ntablex\n", "output_sequence": "Builds a linear regression model for the complete transformed Boston housing data."}, {"input_sequence": "# clear all variables\nrm(list = ls(all = TRUE))\ngraphics.off()\n# install and load packages\nlibraries = c(\"simba\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\nx = read.table(\"carmean2.dat\") # load data\nx = as.matrix(x[, 2:9]) # retrieve Renault, Rover, Toyota\nx1719 = x[c(17:19), ]\nx.mu = apply(x1719, 2, mean) # column means\ny = matrix(0, nrow(x1719), # empty matrix\n# fill binary matrix: if x(i,k) > x_bar(k): 1, else 0\nfor (i in 1:nrow(x1719)) {\nif (x1719[i, k] > x.mu[k]) {\ny[i, k] = 1\n} else {\ny[i, k] = 0\n}\n}\n}\n# similarity coefficients for binary data\nsim(y, method = \"jaccard\") # jaccard\nsim(y, method = \"simplematching\") # simple matching\nsim(y, method = \"roger\") # tanimoto\n", "output_sequence": "Computes the Jaccard, simple matching and Tanimoto proximity coefficients for binary car data."}, {"input_sequence": "# Clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# Install and load packages\nlibraries = c(\"MASS\", \"lars\", \"scales\", \"mvtnorm\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)} )\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# Lasso from lars with BIC stopping rule\nlasso.bic = function(x, y, win) {\n\nm = n.param\nxbeta = numeric(0)\nfor (j in 1:n.sim) {\n\nxbeta1 = numeric(0)\nfor (i in 1:(n - win + 1)) {\n\n# Normalization of columns of x\nywin = y[[j]][i:(i + win - 1)]\nnwin = nrow(xwin)\nmeanx = drop(one %*% xwin)/nwin\nxc = scale(xwin, meanx, FALSE)\nnormx = sqrt(drop(one %*% (xc^2)))\nnames(normx) = NULL\nxs = scale(xc, FALSE, normx)\n# OLS fit to standardized x\nout.ls = lm(ywin ~ xs)\nbeta.ols = out.ls$coeff[2:(m + 1)]\nobject = lars(xs, ywin, type = \"lasso\", normalize = FALSE)\n# Get min BIC\nsig2f = summary(out.ls)$sigma^2\nbic = log(nwin) * object$df/nwin + as.vector(object$RSS)/(nwin * sig2f)\nstep.bic = which.min(bic)\nlambda = object$lambda[step.bic] # Lambda minimizing BIC\nfit = predict.lars(object, xs, s = step.bic, type = \"fit\",\nmode = \"step\")$fit\ncoefftmp = predict.lars(object, xs, s = step.bic, type = \"coef\",\nmode = \"step\")$coefficients\ncoeff = coefftmp/normx # Get unscaled coefficients\nst = sum(coeff != 0) # Number of nonzero coefficients\nmse = sum((ywin - fit)^2)/(nwin - st - 1)\nxbtmp = xwin %*% coeff\nlambda.tmp = (t(restmp) %*% xbtmp) / (sqrt(nwin) * sum(abs(coeff)))\nxbeta1 = c(xbeta1, xbtmp)\nres.norm1 = c(res.norm1, sqrt(sum(restmp^2)))\ncoeff.norm1 = c(coeff.norm1, sum(abs(coeff)))\nlambda.bic1 = c(lambda.bic1, lambda)\nact.set1 = c(act.set1, st)\ncond.num1 = c(cond.num1, kappa(xwin))\n}\nxbeta = cbind(xbeta, xbeta1)\nres.norm = cbind(res.norm, res.norm1)\nlambda.bic = cbind(lambda.bic, lambda.bic1)\nact.set = cbind(act.set, act.set1)\ncond.num = cbind(cond.num, cond.num1)\nprint(j)\n}\nmean.rn = apply(res.norm, 1, mean)\nvalues = list(lambda.bic, lambda.hat, act.set, res.norm, coeff.norm, cond.num,\nmean.rn,\nnames(values) = c(\"lambda.bic\", \"act.set\", \"res.norm\",\n\"cond.num\", \"mean.rn\",\n\"mean.k\")\nreturn(values)\n}\n# Simulation setup\nn.obs = 1000 # no of observations\nw = 110 # moving window size\nseed1 = 20150206 # seed simulation X\n# Check if n.obs is even (otherwise add one observation)\nif(n.obs %% 2 == 1) n.obs = n.obs + 1 ;\nn.cp = n.obs/2\n# True beta coefficients\ntmp1 = c(1, 1, 1)\ntmp2 = rep(0, 95)\nb = c(tmp1, tmp2)\n# Simulation of the design matrix with change of condition number after n.cp of observations\nmu1 = rep(0, n.param)\nr = 0.5\nSigma = matrix(0, nrow = n.param, ncol = n.param)\nfor (i in 1:n.param) {\nif (i == j){\nSigma[i, j] = 1\n}else {\nSigma[i, j] = r^abs(i - j)\n}\nX = list()\nset.seed(seed1)\nfor (i in 1:n.sim){\nX1 = mvrnorm(n = n.cp, mu1, Sigma)\nX2 = rmvt(n = (n.obs - n.cp), sigma = Sigma, df = 2)\nX[[i]] = rbind(X1, X2)\n}\n# Simulation of the error term\neps = list()\nset.seed(seed2)\neps[[i]] = rnorm(n.obs, mean = 0, sd = 0.1)\n# Computation of Y\nY = list()\nY.tmp = numeric(0)\nfor (j in 1:n.obs){\nY.tmp = c(Y.tmp, b %*% X[[i]][j, ] + eps[[i]][j])\nY[[i]] = Y.tmp\n# Lasso estimation with LARS for moving window of length w\nout_des = lasso.bic(X, Y, w)\n# Lambda\npar(mar = c(3, 5, 1, 1))\nplot(out_des$lambda.bic[, 1], type = \"l\", col = alpha(\"darkblue\", 0.05), axes = FALSE,\nxlab = \"\", frame = TRUE, cex.main = 1.5, ylab = expression(paste(lambda)),\nxlim = c(-(w + 10), (n.obs - w + 10)),\nylim = c(min(out_des$lambda.bic),\naxis(1, at = c(-w, n.cp - w, n.obs - w), labels = c(\"0\", paste(expression(\"t =\"), n.cp),\nn.obs), cex.axis = 1.2)\naxis(2, cex.axis = 1.2)\nabline(v = (n.cp - w), lty = 3)\nfor (i in 2:n.sim) {\ntmp = out_des$lambda.bic[, i]\nlines(tmp, col = alpha(\"darkblue\", 0.05))\nlines(out_des$mean.lb, col = \"red3\")\nplot(out_des$mean.lb, type = \"l\", col = \"red3\", axes = FALSE,\nframe = TRUE, cex.main = 1.5, ylab = expression(paste(\"Average \", lambda)),\nylim = c(min(out_des$mean.lb),\n# Cardinality of active set q\nplot(out_des$act.set[, 1], type = \"l\", col = alpha(\"darkblue\", 0.05), axes = FALSE,\nxlab = \"\", frame = TRUE, cex.main = 1.5, ylab = \"q\",\nylim = c(min(out_des$act.set),\ntmp = out_des$act.set[, i]\nlines(out_des$mean.as, col = \"red3\")\nplot(out_des$mean.as, type = \"l\", col = \"red3\", axes = FALSE,\nframe = TRUE, cex.main = 1.5, ylab = \"Average q\",\nylim = c(min(out_des$mean.as),\naxis(1, at = c(-w, n.cp - w, n.obs - w),\nlabels = c(\"0\", paste(expression(\"t =\"), n.cp), n.obs), cex.axis = 1.2)\n# L2-norm of residuals\nplot(out_des$res.norm[, 1], type = \"l\", col = alpha(\"darkblue\", 0.05), axes = FALSE,\nxlab = \"\", frame = TRUE, cex.main = 1.5, ylab = expression(paste(\"RSS\" ^ {1/2})),\nylim = c(min(out_des$res.norm),\ntmp = out_des$res.norm[, i]\nlines(out_des$mean.rn, col = \"red3\")\nplot(out_des$mean.rn, type = \"l\", col = \"red3\", axes = FALSE,\nframe = TRUE, cex.main = 1.5, ylab = expression(paste(\"Average RSS\" ^ {1/2})),\nylim = c(min(out_des$mean.rn),\n# L1-norm of Beta\nplot(out_des$coeff.norm[, 1], type = \"l\", col = alpha(\"darkblue\", 0.05), axes = FALSE,\nxlab = \"\", frame = TRUE, cex.main = 1.5,\nylab = expression(paste(\"||\", hat(beta), \"|| \" [1])),\nylim = c(min(out_des$coeff.norm),\ntmp = out_des$coeff.norm[, i]\nlines(out_des$mean.cn, col = \"red3\")\nplot(out_des$mean.cn, type = \"l\", col = \"red3\", axes = FALSE,\nframe = TRUE, cex.main = 1.5,\nylab = expression(paste(\"Average ||\", hat(beta), \"|| \" [1])),\nylim = c(min(out_des$mean.cn),\n# Condition number of X'X\nplot(out_des$cond.num[, 1], type = \"l\", col = alpha(\"darkblue\", 0.05), axes = FALSE,\nxlab = \"\", frame = TRUE, cex.main = 1.5, ylab = expression(paste(kappa, \" of X'X\")),\nylim = c(min(out_des$cond.num),\ntmp = out_des$cond.num[, i]\nlines(out_des$mean.k, col = \"red3\")\nplot(out_des$mean.k, type = \"l\", col = \"red3\", axes = FALSE,\nframe = TRUE, cex.main = 1.5, ylab = expression(paste(\"Average \", kappa, \" of X'X\")),\nylim = c(min(out_des$mean.k),\naxis(1, at = c(-w, n.cp - w, n.obs - w), labels = c(\"0\", paste(expression(\"t =\"), n.cp),\n", "output_sequence": "Performs LASSO regression in a moving window by using BIC criterion to choose penalty parameter (lambda). The simulated data contains a break point after which the condition number of a matrix [t(X)X] changes. Plots time series of lambda in LASSO regression. Furthermore, the cardinality of the active set q, the L2-norm of the residuals, the L1-norm of the parameter beta and the condition number of the squared design matrix [t(X)X] are plotted. All of the plots contain results from a number of simulations and the average over all of them."}, {"input_sequence": "# import matplotlib\n# matplotlib.use('TKAgg')\nfrom matplotlib import animation\nfrom numpy import append, cos, linspace, pi, sin, zeros\nimport matplotlib.pyplot as plt\nfrom IPython.display import HTML\n# PLEASE NOTE IN SPYDER YOU SHOULD DISABLE THE ACTIVE SUPPORT in PREFs\n# elephant parameters\nparameters = [70 - 30j, 67 + 8j, 10 - 10j, -14 - 60j, 20 + 50j]\n# patrick's happy spermwhale\n# parameters = [30 - 10j, 20 + 20j, 40 + 10j, 20 - 50j, -40 + 10j]\n# philipp's flying swan\n# parameters = [1 - 2j, 9 + 9j, 1 - 2j, 9 + 9j, 0 + 0j]\n# kathrin's hungry animal\n# parameters = [50 - 50j, 30 + 10j, 5 - 2j, -5 - 6j, 20 + 20j]\n# anna\u2019s happy hippo\n# parameters = [50 - 15j, 5 + 2j, -10 - 10j, -14 - 60j, 5 + 30j]\n# fabio\u2019s bird with right wing paralysis\n# parameters = [50 - 15j, 5 + 2j, -1 - 5j, -14 - 60j, 18 - 40j]\n# for pea shooter see code below\ndef fourier(t, C):\nf = zeros(t.shape)\nfor k in range(len(C)):\nf += C.real[k] * cos(k * t) + C.imag[k] * sin(k * t)\nreturn f\ndef elephant(t, p):\nnpar = 6\nCx = zeros((npar,), dtype='complex')\nCx[1] = p[0].real * 1j\nCy[1] = p[3].imag + p[0].imag * 1j\nCx[2] = p[1].real * 1j\nx = append(fourier(t, Cy), [p[4].real])\nreturn x, y\ndef init_plot():\n# draw the body of the elephant & create trunk\nx, y = elephant(linspace(2.9 * pi, 0.4 + 3.3 * pi, 1000), parameters)\nfor ii in range(len(y) - 1):\ny[ii] -= sin(((x[ii] - x[0]) * pi / len(y))) * sin(float(0)) * parameters[4].real\ntrunk.set_data(x, y)\nreturn trunk,\ndef move_trunk(i):\n# move trunk to new position (but don't move eye stored at end or array)\ny[ii] -= sin(((x[ii] - x[0]) * pi / len(y))) * sin(float(i)) * parameters[4].real\nfig, ax = plt.subplots()\n# initial the elephant body\nx, y = elephant(t=linspace(0.4 + 1.3 * pi, 2.9 * pi, 1000), p=parameters)\nplt.plot(x, y, 'b.')\nplt.xlim([-75, 90])\nplt.axis('off')\ntrunk, = ax.plot([], [], 'b.') # initialize trunk\nani = animation.FuncAnimation(fig=fig,\nfunc=move_trunk,\nani\nHTML(ani.to_html5_video())\n# Uncomment if you would like to save video externally\nWriter = animation.writers['ffmpeg']\nmetadata = dict(title='Elephant Trunk Wiggling', artist='Jenny Beth\u00e4user')\nwriter = Writer(fps=30, metadata=metadata, bitrate=1800)\nani.save(filename='bulldog_trunk_wiggle.mp4', writer=writer)\nplt.show()\n", "output_sequence": "Using complex number and basic python to draw a parrot, and creating its png and MP4 file."}, {"input_sequence": "library(\"crypto2\")\nc_list <- crypto_list()\nbtc_name <- c_list[1,1:4]\ndf <- crypto_history(coin_list=btc_name, start_date = \"20140101\", end_date = \"20221231\")\ndf <- df[,c(\"timestamp\", \"close\")]\nrm(list = ls(all = TRUE))\nwdir=getwd()\n# load package\nlibraries = c(\"FKF\",\"quantmod\",\"ggplot2\",\"rjson\",\"car\",\"magick\",\"gganimate\",\"hrbrthemes\",\"gifski\",\"dplyr\",\"viridis\",\n\"ggplot2\",\"rjson\",\"car\",\"magick\",\"gganimate\",\"plotly\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# people can change the wdir, it is the address where you put the results\ndir.create(\"Results\")\nsave = paste0(wdir, '/', \"Results/\")\ncount = nrow(df)\nprice = df$close\nreturn = c(0, diff(log(df$close))) #observations\n#Allocate space:\nxhat = rep(0,count) #a posteri estimate at each step\nxhatminus=rep(0,count) #a priori estimate\nK=rep(0,count) #gain\n#initialise guesses: assume true_value=0, error is 1.0\nxhat[1] <- return[1]\nP[1] <- 1\nQ_Select = c(0.03) # could change to a range\n#estimate of measurement variance\nR = 0.03\nfor (Q in Q_Select){\nfor (k in 2:count){\n#time update\nxhatminus[k] <- xhat[k-1]\nPminus[k] <- P[k-1] + Q\n\n#measurement update\nK[k] = Pminus[k] / (Pminus[k] + R)\nxhat[k] = xhatminus[k] + K[k] * (return[k] - xhatminus[k])\nP[k] = (1-K[k]) * Pminus[k]\n}\npng(paste0(save,'Return of BTC- observable & predicted returns.png'),width=600,height=600,units=\"px\",bg = \"transparent\")\nx= as.Date(df$timestamp)\nplot(x,return,col=\"blue\",type=\"l\",xlab = \"Date\",ylab =\"Return\",lwd = 1.5,cex.axis=1.5,cex.lab=1.5,cex.main=1.5)\nlines(x,xhatminus, col = \"red\",type=\"l\",lwd = 1.5)\nlegend(x = \"topright\", legend=c(\"Predict return\", \"Actual return\"),\ncol=c(\"blue\", \"red\"), lty=1:2, cex=0.8)\ndev.off()\n}\n# movie of parameter interactions\ngif_plot = data.frame(date = df$timestamp, xhatminus = xhatminus, return = return)\ngif1 <- gif_plot %>%\nggplot(aes(x=xhatminus, y=return)) +\ngeom_line(colour = 'grey') +\ngeom_point() +\ntheme_ipsum() +\ntheme(panel.background = element_rect(fill = \"transparent\", colour = NA),\npanel.grid.major = element_blank(),\naxis.line = element_line(colour = \"black\")) +\ntransition_reveal(as.Date(date)) +\nggtitle(\"Date: {frame_along}\")\n#save .gif\nanimate(gif1, renderer = gifski_renderer(paste0(save,'Observable return vs Predicted return.gif')), bg = \"transparent\",\nnframes = 100)\n# MSE of different combination of Q and R\nQ_Select = seq(from=0, to=0.03, by=0.0001)# could change to a range 0.001\nR_Select = seq(from=0, to=0.03, by=0.0001)\nResult = matrix(nrow = length(Q_Select),ncol\nfor (i in 1 : length(Q_Select)){ #row number\ntimestart<-Sys.time()\nfor (j in 1 : length(R_Select)){ #Column number\nQ = Q_Select[i]\nxhat = rep(0,count) #a posteri estimate at each step\nxhatminus=rep(0,count) #a priori estimate\nK=rep(0,count) #gain\n#initialise guesses: assume true_value=0, error is 1.0\nxhat[1] <- return[1]\nP[1] <- 1\nfor (k in 2:count){\n#time update\nxhatminus[k] <- xhat[k-1]\n\n#measurement update\nK[k] = Pminus[k] / (Pminus[k] + R)\nxhat[k] = xhatminus[k] + K[k] * (return[k] - xhatminus[k])\nP[k] = (1-K[k]) * Pminus[k]\n}\nResult[i,j] = mean((return - xhatminus)^2)\ntimeend<-Sys.time()\nrunningtime<-timeend-timestart\nprint(paste0(timeend, \" runing \", runningtime,\" finish \", as.character(Q)))\nplot_Result = list(Q = Q_Select, R = R_Select, MSE= Result)\np <- plot_ly(x = plot_Result$Q, y = plot_Result$R, z = plot_Result$MSE)\n# add_heatmap(p)\nadd_contour(p)\n", "output_sequence": "Use the Apple stock price data to predict the return by Kalman filter"}, {"input_sequence": "# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Mon Dec 5 15:52:19 2022\n\n@author: Tracy Zhou\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nimport umap\n# check directory\nos.getcwd()\ndirect = os.getcwd()\n#df.to_csv(direct + '/Pfizer_Press.csv')\ndf = pd.read_csv(\"Pfizer_data.csv\").dropna()\ndf = df[['Positive','Negative','Polarity','Subjectivity','tag_n','trends', 'vary','jump','return', 'vol','year','win']]\ndf['tag_n'] = df['tag_n'].astype(int)\n#df.head()\ndata = pd.read_csv(\"Pfizer_data.csv\").dropna()\ndata = data[['vol','vary','jump','return','Positive','Negative','Polarity','Subjectivity','tag_n','trends']]\n#data.head()\ndata = StandardScaler().fit_transform(data)\nreducer = umap.UMAP(random_state=30)\nembedding = reducer.fit_transform(data)\nplt.scatter(\nembedding[:, 1],\n#c = np.arange(embedding.shape[0]),\nc=[sns.color_palette()[x] for x in df.tag_n]\n)\nplt.axis('square')\nplt.savefig(\"Pfizer_umap.png\", transparent = True, dpi=180)\ndata = data[['vol','vary','jump','return','positive','negative','Polarity','Subjectivity','tag_n','trends']]\n#print(data)\ndigits = StandardScaler().fit_transform(data)\nmapper = umap.UMAP().fit(digits.data)\n#print(mapper)\nimport umap.plot\numap.plot.connectivity(mapper, show_points=True)\nplt.savefig(\"Pfizer_umap2.png\", transparent = True, dpi=180)\nfrom PIL import Image\nimage = Image.open('Pfizer_umap2.png')\nimage = image.convert('RGBA')\nprint(image.mode)\n# Transparency\nnewImage = []\nfor item in image.getdata():\nif item[:3] == (255, 255):\nnewImage.append((255, 255, 0))\nelse:\nnewImage.append(item)\nimage.putdata(newImage)\nimage.save('Pfizer_umap2.png')\n", "output_sequence": "['Study the relationship between press release and market performance with the case of Pfizer during the pandemic.', 'use UMAP to reduce dimension and conduct clustering.']"}, {"input_sequence": "#import nltk\n#nltk.download()\nfrom selenium import webdriver\nimport matplotlib.pyplot as plt\nimport re\nfrom nltk.corpus import stopwords\nimport os\nfrom selenium.webdriver.common.by import By\nimport pandas as pd\n#import pysentiment as ps\n#from wordcloud import WordCloud\n#path_direct = os.getcwd()\n#os.chdir(path_direct + '/pyning')\n#p = r\"\"\n#os.chdir(p)\n# Start Selenium\nbrowser = webdriver.Chrome(r\"D:\\DEDA_Class_2022_410707007_Sentiment Analysis\\chromedriver.exe\")\nurl = \"https://edition.cnn.com/2020/08/21/politics/joe-biden-democratic-presidential-nomination-speech/index.html\"\n# Extract text\nbrowser.get(url)\n#content = browser.find_element_by_class_name(\"t-single__content-Body-content\")\ntext = content.text\nbrowser.close()\nbrowser.stop_client()\n# Some expressions still left\n# Differ between quotes!\nexpression = \"\u2013|\u2014|[()]|(\\\u201c)|(\\\u201d)|(\\\u201c)|(\\\u201d)|(\\,|\\.|-|\\;|\\<|\\>)|(\\\\n)|(\\\\t)|(\\=)|(\\|)|(\\-)|(\\')|(\\\u2019)\"\ncleantextCAP = re.sub(expression, '', text)\ncleantext = cleantextCAP.lower()\n# Count and create dictionary\ndat = list(cleantext.split())\ndict1 = {}\nfor i in range(len(dat)):\nprint(i)\nword = dat[i]\ndict1[word] = dat.count(word)\ncontinue\n# Filter Stopwords\nkeys = list(dict1)\nfiltered_words = [word for word in keys if word not in stopwords.words('english')]\ndict2 = dict((k, dict1[k]) for k in filtered_words if k in filtered_words)\n#keys in stopwords.words(\"english\")\n# Resort in list\n# Reconvert to dictionary\ndef valueSelection(dictionary, length, startindex = 0): # length is length of highest consecutive value vector\n\n# Test input\nlengthDict = len(dictionary)\nif length > lengthDict:\nreturn print(\"length is longer than dictionary length\");\nelse:\nd = dictionary\nitems = [(v, k) for k, v in d.items()]\nitems.sort()\nitems.reverse()\nitemsOut = [(k, v) for v, k in items]\nhighest = itemsOut[startindex:startindex + length]\ndd = dict(highest)\nwanted_keys = dd.keys()\ndictshow = dict((k, d[k]) for k in wanted_keys if k in d)\nreturn dictshow;\ndictshow = valueSelection(dictionary = dict2, length = 7, startindex = 0)\n# Plot\nn = range(len(dictshow))\nplt.bar(n, dictshow.values(), align='center')\nplt.xticks(n, dictshow.keys())\nplt.title(\"Most frequent Words\")\nplt.savefig(\"DEDA_Class_2022_410707007_Sentiment Analysis_a.png\")\nplt.show()\n# Overview\noverview = valueSelection(dictionary = dict2, length =500, startindex = 0)\nnOverview = range(len(overview.keys()))\nplt.bar(nOverview, overview.values(), color = \"g\", tick_label = \"\")\nplt.title(\"Word Frequency Overview\")\nplt.xticks([])\nplt.savefig(\"DEDA_Class_2022_410707007_Sentiment Analysis_b.png\")\nimport pysentiment as ps\n# Sentiment Analysis\nhiv4 = ps.HIV4()\ntokens = hiv4.tokenize(cleantext)\nscore = hiv4.get_score(tokens)\nprint(score)\nprint('Positive:'+str(score['Positive']))\nprint('Subjectivity:'+str(score['Subjectivity']))\ntext_file = open(\"Joe Biden speech.txt\", \"w\",encoding='UTF-8')\ntext_file.write('Joe Biden takes on Trump-era traumas in career-defining speech'+'\\n')\ntext_file.write('Positive:'+str(score['Positive'])+'\\n')\ntext_file.close()\npd_tab = pd.DataFrame.from_dict(score, orient='index')\npd_tab.columns=['Joe Biden takes on Trump-era traumas in career-defining speech']\nax = plt.axes(frame_on=False)\nax.xaxis.set_visible(False)\n\npd.plotting.table(ax, pd_tab.round(2), loc='center')\nplt.tight_layout()\nplt.savefig('DEDA_Class_2022_410707007_Sentiment Analysis.png')\n#table_pd = pd.DataFrame(score)\n# Polarity\n# Formula: (Positive - Negative)/(Positive + Negative)\n# Subjectivity\n# Formula: (Positive + Negative)/N\n", "output_sequence": "Using Web Scraping to extract and study information from texts and perform sentiment analysis"}, {"input_sequence": "# Clear all variables\nrm(list = ls(all = TRUE))\ngraphics.off()\n# Install and load packages\nlibraries = c(\"RGoogleAnalytics\", \"httpuv\", \"xtable\", \"knitr\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# setwd(\"C:/R/GoA/QNet\")\noauth_token = Auth(client.id = \"XXX\", client.secret = \"YYY\")\nGetProfiles(oauth_token)\ns_date = \"2013-11-16\"\n###########################################################################\n# Quantlet downloads by country\nquery.list.Country = Init( start.date = s_date,\nend.date = e_date,\ndimensions = \"ga:country\",\nmetrics = \"ga:totalEvents\",\nfilters = \"ga:eventCategory==QNetShow\",\nsort = \"-ga:totalEvents\",\nmax.results = 1000,\ntable.id = \"ga:78690351\")\nga.query = QueryBuilder(query.list.Country)\nga.df = GetReportData(ga.query, oauth_token)\n# Parameters for the length and the column names of the output\nsamplesize = 10\ncolumnnames = c(\"Country\", \"Downloads\")\ncolnames(ga.df) = columnnames\nqn_top_frm = ga.df[1:samplesize,]\n# output as data frame, top countries as defined by samplesize\nqn_top_frm\n# convert to Latex output\nprint(xtable(qn_top_frm, align = \"|r|l|r|\", digits = 0), include.rownames = F)\n# convert to Markdown format\n( k_t = kable(qn_top_frm, row.names = F) )\nwriteLines(k_t, con = \"DownloadsByCountry.md\")\n", "output_sequence": "Extracts recent download statistics from Google Analytics starting November 2013 for each Quantlet. Top ten countries are taken and the final results are presented as an R data frame. The API query specifies the parameters dimensions, metrics and filters conditioning the desired Event Tracking criteria."}, {"input_sequence": "# clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# install and load packages\nlibraries = c(\"POT\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# load data\na = read.table(\"BAYER_close_0012.dat\")\n# Portfolio\nd = a + b + c + e\nn1 = dim(d)[1] # length of portfolio\nx = log(d[1:n1 - 1, ]/d[2:n1, ]) # negative log-returns\ngpd = fitgpd(x, quantile(x, 0.95), est = \"mle\") #\nn = gpd$nat\nthr = gpd$threshold\nscale = gpd$param[1]\ndata = gpd$data\nexc = gpd$exceed\nt = (1:n)/(n + 1)\ny1 = qgpd(t, scale = scale, shape = shape)\ngpdt = sort(exc) - thr\ny2 = pgpd(gpdt, scale = scale, shape = shape)\n# plot\nplot(y2, t, col = \"blue\", pch = 15, bg = \"blue\", xlab = \"\", ylab = \"\", main = \"PP plot, Generalized Pareto Distribution\")\n", "output_sequence": "Estimates the parameters of a Generalized Pareto distribution for the negative log-returns of portfolio (Bayer, BMW, Siemens, VW)for the time period from 2000-01-01 to 2012-12-31 and produces a P-P plot."}, {"input_sequence": "# Graph 24h CRIX\n\nrequire(jsonlite)\njson_file <- \"http://crix.hu-berlin.de/data/crix_hf.json\"\ncrix <- fromJSON(json_file)\ncrix$date <- as.POSIXct(crix$date, format=\"%Y-%m-%d %H:%M:%S\")\nplot(crix$date, crix$price/1000, xaxt='n', pch=20, las=1,\nxlab='24th to 25th of December 2017', ylab='CRIX Price/1000')\naxis.POSIXct(1, crix$date, format=\"%H:%M:%S\")\n", "output_sequence": "Produces full and 24h ts plots based on historical hf CRIX data (CRIX Plot)"}, {"input_sequence": "import pandas as pd\nimport numpy as np\n%matplotlib inline\nimport datetime\nimport matplotlib.pyplot as plt\n#the points are taken from the Chapter 6 Delaunay Triangulation (Delaunay_Triangulations_ETH_Zuerich.pdf)\ndata_points =np.array([[1.5, 0.2], [4.4, 0.7], [3.1, 1],[0.2, 1.7], [2.2, 1.7], [3.8, 2], [1.4, 2.4], [4.3, 2.9], [2.5, 3.1], [1.2, 3.2]])\nfrom scipy.spatial import Delaunay\ndef alpha_shape(points, alpha, only_outer=True):\n\"\"\"\nCompute the alpha shape (concave hull) of a set of points.\n:param points: np.array of shape (n,2) points.\n:param alpha: value.\n:param only_outer: boolean value to specify if we keep only the outer border\nor also inner edges.\n:return: set of (i,j) pairs representing edges of the alpha-shape. (i,j) are\nthe indices in the points array.\nassert points.shape[0] > 3, \"Need at least four points\"\ndef add_edge(edges, i, j):\n\"\"\"\nAdd a line between the i-th and j-th points,\nif not in the list already\nif (i, j) in edges or (j, i) in edges:\n# already added\nassert (j, i) in edges, \"Can't go twice over same directed edge right?\"\nif only_outer:\n# if both neighboring triangles are in shape, it's not a boundary edge\nedges.remove((j, i))\nreturn\nedges.add((i, j))\ntri = Delaunay(points)\nedges = set()\n# Loop over triangles:\n# ia, ic = indices of corner points of the triangle\nfor ia, ic in tri.vertices:\npa = points[ia]\n# Computing radius of triangle circumcircle\n# www.mathalino.com/reviewer/derivation-of-formulas/derivation-of-formula-for-radius-of-circumcircle\na = np.sqrt((pa[0] - pb[0]) ** 2 + (pa[1] - pb[1]) ** 2)\ns = (a + b + c) / 2.0\narea = np.sqrt(s * (s - a) * (s - b) * (s - c))\ncircum_r = a * b * c / (4.0 * area)\nif circum_r < alpha:\nadd_edge(edges, ia, ib)\nreturn edges\ntri=Delaunay(data_points)\nprint(tri.vertices)\nfor a, b, c in tri.vertices:\nfor i, j in [(a, b), (b, c), (c, a)]:\nplt.plot(data_points[[i, j], data_points[[i, j], color='k')\nplt.savefig(f'./DelaunayTriangulation.png', transparent=True)\nplt.show()\n# Computing the alpha shape\n#print(tri.vertices)\n#for a, b, c in tri.vertices:\n# for i, j in [(a, b), (b, c), (c, a)]:\n# plt.plot(data_points[[i, j], data_points[[i, j], color='gray')\n#plt.show()\nfor alpha in [0.6, 0.7, 0.75,0.8,0.9,0.92,0.93,1,1.1,2,3]:\n#plot as a graph basis the DT in gray color\n\nfor a, b, c in tri.vertices:\nfor i, j in [(a, b), (b, c), (c, a)]:\nplt.plot(data_points[[i, j], data_points[[i, j], color='tab:gray')\n\nedges = alpha_shape(data_points, alpha=alpha, only_outer=True)\n#fig=plt.figure(figsize=(4,4))\n#ax=fig.add_subplot(2, 2, 2)\n#circle1 = plt.Circle((0, 0), 0.2, color='r')\n#ax.add_patch(circle1)\nfor i, j in edges:\nplt.plot(data_points[[i, j], data_points[[i, j], color='r')#, label='alpha')\n# ax.legend(loc='best')\nplt.scatter(*zip(*data_points), color='r') #color=['b', 'g', 'y'])\n#ax.add_artist(circle1)\nplt.axis('off')\nplt.savefig(f'./a-shape_{alpha}.png', transparent=True)\nplt.show()\n", "output_sequence": "['Delaunay Triangulation over a set of 10 points', 'Several plots of Alpha shapes over the same 10 points for different values of Alpha']"}, {"input_sequence": "# clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n#Load libraries\nlibraries = c(\"foreach\",\"MASS\",\"quantreg\",\"KernSmooth\",\"doParallel\",\"plyr\",\"ggplot2\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\nsource(\"cbands_funcs.r\")\n## Parameters\ncl = 7 # Number of cores to use\nB = 1000\nalpha = 0.05\ngridn = 100\nbeta = -2 #change here the beta for the LETF in question\n# Load data and apply moneyness scaling\nmonivdataLETF = read.table('mivttmdata_05_SDS.csv',sep=',')\n#Clean the data\nxspy = as.matrix(monivdataSPY[,1])\nmoniv = data.frame(cbind(xspy,yspy))\ncolnames(moniv) = c('mon','iv')\nfreqs = count(moniv,vars = 'iv') # 'plyr' library needed\nfreqs = freqs[order(-freqs$freq),]\ngood_ivs = freqs$iv[freqs$freq < 12]\nyspy = as.matrix(moniv$iv[(moniv$iv %in% good_ivs) == TRUE])\netfvol = as.matrix(yspy)\nx = as.matrix(monivdataLETF[,1])\nmoniv = data.frame(cbind(x,y))\ngood_ivs = freqs$iv[freqs$freq < 3]\nx = as.matrix(moniv$mon[(moniv$iv %in% good_ivs) == TRUE])\nttm = as.matrix(monivdataLETF[,3])\nttm = as.matrix(ttm[(moniv$iv %in% good_ivs) == TRUE])\nScMonKf = (x/exp(-0.5*beta*(beta-1)*(mean(etfvol)^2)*ttm))^(1/beta)\nx = ScMonKf\n#Choose bandwidths\nn = nrow(x)\nhg = 0.15\ng = n^(-1/9)\n# Sort input data\ny = y[order(x)]\n# Scale x to [0, 1]\nxmin = min(x)\nx = (x - xmin) / (xmax - xmin)\nh = median(abs(x-median(x)))/0.6745*(4/3/n)^0.2\n# Initial fit\nyhat.h = lnrob(x, y, h = h, maxiter = 1000, x0 = x)\nehat = y - yhat.h$fv\nehh = median(abs(ehat-median(ehat)))/0.6745*(4/3/n)^0.2\nyhat.grid.h = lnrob(x, y, h = h, maxiter = 1000, x0 = seq(0, 1, length.out = gridn))\n# Empirical pdf of x at gridpoints\nfxd = bkde(x, gridsize = gridn, range.x = c(yhat.grid.h$xx[1], yhat.grid.h$xx[gridn]))\nfl = vector(length = gridn, mode = \"numeric\")\nfor (k in 1: gridn){\n# Conditional pdf f(e|x)at gridpoints\nnom = sum((kernelq((x - yhat.grid.h$xx[k]) / (hg)) *\nyhat.grid.h$psi1((y - yhat.grid.h$fv[k]), deriv = 1)))\ndenom = sum(kernelq((x - yhat.grid.h$xx[k])/(hg)))\nfl[k] = nom / denom\n\n# Conditional E(psi^2(e))\nnom = sum((kernelq((x - yhat.grid.h$xx[k])/(h)) *\nyhat.grid.h$psi1((y - yhat.grid.h$fv[k]), deriv = 0)^2))\ndenom = sum(kernelq((x - yhat.grid.h$xx[k])/(hg)))\nfll[k] = nom / denom\n}\nbandt = (fxd$y)^(1/2) * abs(fl / sqrt(fll))\n# Bootstrap\npack = c(\"MASS\", \"KernSmooth\", \"Rlab\", \"quantreg\")\ncl = makeCluster(cl)\nregisterDoParallel(cl)\nd = vector(length = B, mode = \"numeric\")\nd = foreach(i = 1:B, .packages = pack)%dopar%{\nestar = lprq3( yhat.h$xx, (y - yhat.h$fv), h = hg, x0 = yhat.grid.h$xx )\nystar = yhat.grid.g$fv + estar$fv\nfitstar = lnrob(yhat.grid.h$xx, ystar, h = h, maxiter = 50, x0 = yhat.grid.h$xx )\nd.m = max(abs(bandt*abs(fitstar$fv - yhat.grid.g$fv)))\nstopCluster(cl)\nd = unlist(d)\ndstar = quantile(d[d!=0], probs = 1 - alpha)\ndstar = dstar * {bandt}^(-1)\n# Construct asymptotic confidence bands\ncc = 1 / 4\nlambda = 1 / 2 / sqrt(pi)\ndelta = - log(h) / log(n)\ndd = sqrt(2 * delta * log(n)) + (2 * delta * log(n))^(-1/2) * log(cc / 2 / pi)\ncn = log(2) - log(abs(log(1 - alpha)))\nband = (n * h)^(- 1/2) * bandt^{-1} * (dd + cn * (2 * delta * log(n))^(-1/2)) * sqrt(lambda)\n# Scale back\nx = ( x * (xmax - xmin) ) + xmin\n# plot(x, y, xlab = \"Moneyness\", ylab = \"Implied volatility\", main = \"SDS\")\n# lines(x.grid, yhat.grid.h$fv, lwd = 4, col = \"blue\")\n# lines(x.grid, (yhat.grid.h$fv - dstar), col = \"red\", lty = 2, lwd = 4)\n#Create a \"beautiful\" plot with ggplot2\nxyframe = data.frame(cbind(x,y))\ncolnames(xyframe) = c('moneyness','impvol')\nssoframe = data.frame(cbind(x.grid,yhat.grid.h$fv,yhat.grid.h$fv - dstar,yhat.grid.h$fv + dstar))\ncolnames(ssoframe) = c('mgrid','ivest','lowbnd','upbnd')\nbeauplot = ggplot() + geom_point(data = xyframe, aes(x = moneyness, y = impvol),size=1.5,colour=\"#666666\") +\ngeom_line(data = ssoframe, aes(x = mgrid, y = ivest),colour=\"#000099\",size=1.5) +\nbeauplot + ggtitle('SDS')\n", "output_sequence": "Calculate and plot uniform bootstrap confidence bands for the ProShares UltraShort S&P500 LETF option implied volatility at the time-to-maturity 0.5 years"}, {"input_sequence": "#Functions\nlibrary(\"foreach\")\nlibrary(\"quantreg\")\nlibrary(\"KernSmooth\")\nlibrary(\"doParallel\")\nkernelq = function(u){\ndnorm(u, mean = 0, sd = 1)\n}\n# M-type smoother\nlnrob = function(x, y, h, maxiter, x0 = seq(0,1, length.out = 100)){\n\nxx = sort(x0)\nxx = (xx - min(xx))/(max(xx) - min(xx))\nfv = xx\nx = (x - min(x))/(max(x)-min(x))\nfor (i in 1:length(xx)) {\nz = x - xx[i]\nwx = dnorm(z/h)\nr = rlm(y ~ z, weights = wx, method = \"M\", maxit = maxiter)\nu = r$wresid\nfv[i] = r$coefficients[[1]]\n}\n\"psi1\" = r$psi\nreturn( list(xx = xx, fv = fv, dv = dv, \"psi1\" = psi1) )\n# Quantile regression with specific tau\nlprq2 = function(x, y, h, tau, x0) {\nxx = sort(x0)\nx = (x - min(x)) / (max(x) - min(x))\n\nfor(i in 1:length(xx)){\nr = rq(y ~ z, tau = tau, weights = wx, method = \"br\")\nfv[i] = r$coef[1.]\nlist(xx = xx, fv = fv, dv = dv)\n# Quantile regression with random tau\nlprq3 = function(x, y, h, x0){\nxx = sort(x0)\nxx = (xx - min(xx)) / (max(xx) - min(xx))\nfv = xx\nx = (x - min(x)) / (max(x) - min(x))\ntau = runif(1)\nfor(i in 1:length(xx)) {\nr = rq(y ~ z, weights = wx, tau = runif(1), ci = FALSE)\n", "output_sequence": "Calculate and plot uniform bootstrap confidence bands for the ProShares UltraShort S&P500 LETF option implied volatility at the time-to-maturity 0.5 years"}, {"input_sequence": "# clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n#Load libraries\nlibraries = c(\"foreach\",\"MASS\",\"quantreg\",\"KernSmooth\",\"doParallel\",\"plyr\",\"ggplot2\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\nsource(\"cbands_funcs.r\")\n## Parameters\ncl = 7 # Number of cores to use\nB = 1000\nalpha = 0.05\ngridn = 100\nbeta = 2 #change here the beta for the LETF in question\n# Load data and apply moneyness scaling\nmonivdataLETF = read.table('mivttmdata_05_SSO.csv',sep=',')\n#Clean the data\nxspy = as.matrix(monivdataSPY[,1])\nmoniv = data.frame(cbind(xspy,yspy))\ncolnames(moniv) = c('mon','iv')\nfreqs = count(moniv,vars = 'iv') # 'plyr' library needed\nfreqs = freqs[order(-freqs$freq),]\ngood_ivs = freqs$iv[freqs$freq < 12]\nyspy = as.matrix(moniv$iv[(moniv$iv %in% good_ivs) == TRUE])\netfvol = as.matrix(yspy)\nx = as.matrix(monivdataLETF[,1])\nmoniv = data.frame(cbind(x,y))\ngood_ivs = freqs$iv[freqs$freq < 3]\nx = as.matrix(moniv$mon[(moniv$iv %in% good_ivs) == TRUE])\nttm = as.matrix(monivdataLETF[,3])\nttm = as.matrix(ttm[(moniv$iv %in% good_ivs) == TRUE])\nScMonKf = (x/exp(-0.5*beta*(beta-1)*(mean(etfvol)^2)*ttm))^(1/beta)\nx = ScMonKf\n#Choose bandwidths\nn = nrow(x)\nhg = 0.15\ng = n^(-1/9)\n# Sort input data\ny = y[order(x)]\n# Scale x to [0, 1]\nxmin = min(x)\nx = (x - xmin) / (xmax - xmin)\nh = median(abs(x-median(x)))/0.6745*(4/3/n)^0.2\n# Initial fit\nyhat.h = lnrob(x, y, h = h, maxiter = 1000, x0 = x)\nehat = y - yhat.h$fv\nehh = median(abs(ehat-median(ehat)))/0.6745*(4/3/n)^0.2\nyhat.grid.h = lnrob(x, y, h = h, maxiter = 1000, x0 = seq(0, 1, length.out = gridn))\n# Empirical pdf of x at gridpoints\nfxd = bkde(x, gridsize = gridn, range.x = c(yhat.grid.h$xx[1], yhat.grid.h$xx[gridn]))\nfl = vector(length = gridn, mode = \"numeric\")\nfor (k in 1: gridn){\n# Conditional pdf f(e|x)at gridpoints\nnom = sum((kernelq((x - yhat.grid.h$xx[k]) / (hg)) *\nyhat.grid.h$psi1((y - yhat.grid.h$fv[k]), deriv = 1)))\ndenom = sum(kernelq((x - yhat.grid.h$xx[k])/(hg)))\nfl[k] = nom / denom\n\n# Conditional E(psi^2(e))\nnom = sum((kernelq((x - yhat.grid.h$xx[k])/(h)) *\nyhat.grid.h$psi1((y - yhat.grid.h$fv[k]), deriv = 0)^2))\ndenom = sum(kernelq((x - yhat.grid.h$xx[k])/(hg)))\nfll[k] = nom / denom\n}\nbandt = (fxd$y)^(1/2) * abs(fl / sqrt(fll))\n# Bootstrap\npack = c(\"MASS\", \"KernSmooth\", \"Rlab\", \"quantreg\")\ncl = makeCluster(cl)\nregisterDoParallel(cl)\nd = vector(length = B, mode = \"numeric\")\nd = foreach(i = 1:B, .packages = pack)%dopar%{\nestar = lprq3( yhat.h$xx, (y - yhat.h$fv), h = hg, x0 = yhat.grid.h$xx )\nystar = yhat.grid.g$fv + estar$fv\nfitstar = lnrob(yhat.grid.h$xx, ystar, h = h, maxiter = 50, x0 = yhat.grid.h$xx )\nd.m = max(abs(bandt*abs(fitstar$fv - yhat.grid.g$fv)))\nstopCluster(cl)\nd = unlist(d)\ndstar = quantile(d[d!=0], probs = 1 - alpha)\ndstar = dstar * {bandt}^(-1)\n# Construct asymptotic confidence bands\ncc = 1 / 4\nlambda = 1 / 2 / sqrt(pi)\ndelta = - log(h) / log(n)\ndd = sqrt(2 * delta * log(n)) + (2 * delta * log(n))^(-1/2) * log(cc / 2 / pi)\ncn = log(2) - log(abs(log(1 - alpha)))\nband = (n * h)^(- 1/2) * bandt^{-1} * (dd + cn * (2 * delta * log(n))^(-1/2)) * sqrt(lambda)\n# Scale back\nx = ( x * (xmax - xmin) ) + xmin\n# plot(x, y, xlab = \"Moneyness\", ylab = \"Implied volatility\", main = \"SSO\")\n# lines(x.grid, yhat.grid.h$fv, lwd = 4, col = \"blue\")\n# lines(x.grid, (yhat.grid.h$fv - dstar), col = \"red\", lty = 2, lwd = 4)\n#Create a \"beautiful\" plot with ggplot2\nxyframe = data.frame(cbind(x,y))\ncolnames(xyframe) = c('moneyness','impvol')\nssoframe = data.frame(cbind(x.grid,yhat.grid.h$fv,yhat.grid.h$fv - dstar,yhat.grid.h$fv + dstar))\ncolnames(ssoframe) = c('mgrid','ivest','lowbnd','upbnd')\nbeauplot = ggplot() + geom_point(data = xyframe, aes(x = moneyness, y = impvol),size=1.5,colour=\"#666666\") +\ngeom_line(data = ssoframe, aes(x = mgrid, y = ivest),colour=\"#000099\",size=1.5) +\nbeauplot + ggtitle('SSO')\n", "output_sequence": "Calculate and plot uniform bootstrap confidence bands for the ProShares Ultra S&P500 LETF option implied      \t\t\t                      volatility at the time-to-maturity 0.5 years"}, {"input_sequence": "#Functions\nlibrary(\"foreach\")\nlibrary(\"quantreg\")\nlibrary(\"KernSmooth\")\nlibrary(\"doParallel\")\nkernelq = function(u){\ndnorm(u, mean = 0, sd = 1)\n}\n# M-type smoother\nlnrob = function(x, y, h, maxiter, x0 = seq(0,1, length.out = 100)){\n\nxx = sort(x0)\nxx = (xx - min(xx))/(max(xx) - min(xx))\nfv = xx\nx = (x - min(x))/(max(x)-min(x))\nfor (i in 1:length(xx)) {\nz = x - xx[i]\nwx = dnorm(z/h)\nr = rlm(y ~ z, weights = wx, method = \"M\", maxit = maxiter)\nu = r$wresid\nfv[i] = r$coefficients[[1]]\n}\n\"psi1\" = r$psi\nreturn( list(xx = xx, fv = fv, dv = dv, \"psi1\" = psi1) )\n# Quantile regression with specific tau\nlprq2 = function(x, y, h, tau, x0) {\nxx = sort(x0)\nx = (x - min(x)) / (max(x) - min(x))\n\nfor(i in 1:length(xx)){\nr = rq(y ~ z, tau = tau, weights = wx, method = \"br\")\nfv[i] = r$coef[1.]\nlist(xx = xx, fv = fv, dv = dv)\n# Quantile regression with random tau\nlprq3 = function(x, y, h, x0){\nxx = sort(x0)\nxx = (xx - min(xx)) / (max(xx) - min(xx))\nfv = xx\nx = (x - min(x)) / (max(x) - min(x))\ntau = runif(1)\nfor(i in 1:length(xx)) {\nr = rq(y ~ z, weights = wx, tau = runif(1), ci = FALSE)\n", "output_sequence": "Calculate and plot uniform bootstrap confidence bands for the ProShares Ultra S&P500 LETF option implied      \t\t\t                      volatility at the time-to-maturity 0.5 years"}, {"input_sequence": "#Functions\nlibrary(\"foreach\")\nlibrary(\"quantreg\")\nlibrary(\"KernSmooth\")\nlibrary(\"doParallel\")\nkernelq = function(u){\ndnorm(u, mean = 0, sd = 1)\n}\n# M-type smoother\nlnrob = function(x, y, h, maxiter, x0 = seq(0,1, length.out = 100)){\n\nxx = sort(x0)\nxx = (xx - min(xx))/(max(xx) - min(xx))\nfv = xx\nx = (x - min(x))/(max(x)-min(x))\nfor (i in 1:length(xx)) {\nz = x - xx[i]\nwx = dnorm(z/h)\nr = rlm(y ~ z, weights = wx, method = \"M\", maxit = maxiter)\nu = r$wresid\nfv[i] = r$coefficients[[1]]\n}\n\"psi1\" = r$psi\nreturn( list(xx = xx, fv = fv, dv = dv, \"psi1\" = psi1) )\n# Quantile regression with specific tau\nlprq2 = function(x, y, h, tau, x0) {\nxx = sort(x0)\nx = (x - min(x)) / (max(x) - min(x))\n\nfor(i in 1:length(xx)){\nr = rq(y ~ z, tau = tau, weights = wx, method = \"br\")\nfv[i] = r$coef[1.]\nlist(xx = xx, fv = fv, dv = dv)\n# Quantile regression with random tau\nlprq3 = function(x, y, h, x0){\nxx = sort(x0)\nxx = (xx - min(xx)) / (max(xx) - min(xx))\nfv = xx\nx = (x - min(x)) / (max(x) - min(x))\ntau = runif(1)\nfor(i in 1:length(xx)) {\nr = rq(y ~ z, weights = wx, tau = runif(1), ci = FALSE)\n", "output_sequence": "Calculate and plot uniform bootstrap confidence bands for the ProShares Ultra S&P500 (SSO) LETF option implied volatility at the time-to-maturity 0.6 years"}, {"input_sequence": "# clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n#Load libraries\nlibraries = c(\"foreach\",\"MASS\",\"quantreg\",\"KernSmooth\",\"doParallel\",\"plyr\",\"ggplot2\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\nsource(\"cbands_funcs.r\")\n## Parameters\ncl = 7 # Number of cores to use\nB = 1000\nalpha = 0.05\ngridn = 100\nbeta = 2 #change here the beta for the LETF in question\n# Load data and apply moneyness scaling\nmonivdataLETF = read.table('mivttmdata_06_SSO.csv',sep=',')\n#Clean the data\nxspy = as.matrix(monivdataSPY[,1])\nmoniv = data.frame(cbind(xspy,yspy))\ncolnames(moniv) = c('mon','iv')\nfreqs = count(moniv,vars = 'iv') # 'plyr' library needed\nfreqs = freqs[order(-freqs$freq),]\ngood_ivs = freqs$iv[freqs$freq < 10 & freqs$freq != 8 & freqs$freq != 7 & freqs$freq != 5]\nyspy = as.matrix(moniv$iv[(moniv$iv %in% good_ivs) == TRUE])\netfvol = as.matrix(yspy)\nx = as.matrix(monivdataLETF[,1])\nmoniv = data.frame(cbind(x,y))\ngood_ivs = freqs$iv[freqs$freq < 3]\nx = as.matrix(moniv$mon[(moniv$iv %in% good_ivs) == TRUE])\nttm = as.matrix(monivdataLETF[,3])\nttm = as.matrix(ttm[(moniv$iv %in% good_ivs) == TRUE])\nScMonKf = (x/exp(-0.5*beta*(beta-1)*(mean(etfvol)^2)*ttm))^(1/beta)\nx = ScMonKf\n#Choose bandwidths\nn = nrow(x)\nhg = 0.15\ng = n^(-1/9)\n# Sort input data\ny = y[order(x)]\n# Scale x to [0, 1]\nxmin = min(x)\nx = (x - xmin) / (xmax - xmin)\nh = median(abs(x-median(x)))/0.6745*(4/3/n)^0.2\n# Initial fit\nyhat.h = lnrob(x, y, h = h, maxiter = 1000, x0 = x)\nehat = y - yhat.h$fv\nehh = median(abs(ehat-median(ehat)))/0.6745*(4/3/n)^0.2\nyhat.grid.h = lnrob(x, y, h = h, maxiter = 1000, x0 = seq(0, 1, length.out = gridn))\n# Empirical pdf of x at gridpoints\nfxd = bkde(x, gridsize = gridn, range.x = c(yhat.grid.h$xx[1], yhat.grid.h$xx[gridn]))\nfl = vector(length = gridn, mode = \"numeric\")\nfor (k in 1: gridn){\n# Conditional pdf f(e|x)at gridpoints\nnom = sum((kernelq((x - yhat.grid.h$xx[k]) / (hg)) *\nyhat.grid.h$psi1((y - yhat.grid.h$fv[k]), deriv = 1)))\ndenom = sum(kernelq((x - yhat.grid.h$xx[k])/(hg)))\nfl[k] = nom / denom\n\n# Conditional E(psi^2(e))\nnom = sum((kernelq((x - yhat.grid.h$xx[k])/(h)) *\nyhat.grid.h$psi1((y - yhat.grid.h$fv[k]), deriv = 0)^2))\ndenom = sum(kernelq((x - yhat.grid.h$xx[k])/(hg)))\nfll[k] = nom / denom\n}\nbandt = (fxd$y)^(1/2) * abs(fl / sqrt(fll))\n# Bootstrap\npack = c(\"MASS\", \"KernSmooth\", \"Rlab\", \"quantreg\")\ncl = makeCluster(cl)\nregisterDoParallel(cl)\nd = vector(length = B, mode = \"numeric\")\nd = foreach(i = 1:B, .packages = pack)%dopar%{\nestar = lprq3( yhat.h$xx, (y - yhat.h$fv), h = hg, x0 = yhat.grid.h$xx )\nystar = yhat.grid.g$fv + estar$fv\nfitstar = lnrob(yhat.grid.h$xx, ystar, h = h, maxiter = 50, x0 = yhat.grid.h$xx )\nd.m = max(abs(bandt*abs(fitstar$fv - yhat.grid.g$fv)))\nstopCluster(cl)\nd = unlist(d)\ndstar = quantile(d[d!=0], probs = 1 - alpha)\ndstar = dstar * {bandt}^(-1)\n# Construct asymptotic confidence bands\ncc = 1 / 4\nlambda = 1 / 2 / sqrt(pi)\ndelta = - log(h) / log(n)\ndd = sqrt(2 * delta * log(n)) + (2 * delta * log(n))^(-1/2) * log(cc / 2 / pi)\ncn = log(2) - log(abs(log(1 - alpha)))\nband = (n * h)^(- 1/2) * bandt^{-1} * (dd + cn * (2 * delta * log(n))^(-1/2)) * sqrt(lambda)\n# Scale back\nx = ( x * (xmax - xmin) ) + xmin\n# plot(x, y, xlab = \"Moneyness\", ylab = \"Implied volatility\", main = \"SSO\")\n# lines(x.grid, yhat.grid.h$fv, lwd = 4, col = \"blue\")\n# lines(x.grid, (yhat.grid.h$fv - dstar), col = \"red\", lty = 2, lwd = 4)\n#Create a \"beautiful\" plot with ggplot2\nxyframe = data.frame(cbind(x,y))\ncolnames(xyframe) = c('moneyness','impvol')\nletfframe = data.frame(cbind(x.grid,yhat.grid.h$fv,yhat.grid.h$fv - dstar,yhat.grid.h$fv + dstar))\ncolnames(letfframe) = c('mgrid','ivest','lowbnd','upbnd')\nbeauplot = ggplot() + geom_point(data = xyframe, aes(x = moneyness, y = impvol),size=1.5,colour=\"#666666\") +\ngeom_line(data = letfframe, aes(x = mgrid, y = ivest),colour=\"#000099\",size=1.5) +\nbeauplot + ggtitle('SSO')\n#write.table(letfframe,file = 'sso_cleaned06.csv',sep=',')\n", "output_sequence": "Calculate and plot uniform bootstrap confidence bands for the ProShares Ultra S&P500 (SSO) LETF option implied volatility at the time-to-maturity 0.6 years"}, {"input_sequence": "%% Load data\n%Clear workspace and close all windows\nclc\nclear\nclose all\nload ivplotdata\nrate = 0.001;\ntpoint = 182;\ndate = datestr(dates(tpoint));\n%calculate TTM\n%16-Jan-2016 is the expiration date for the options here\ndtm = daysact(date,'16-Jan-2016');\nttm = dtm/365;\n%% Calculate maturity-IV matrices for SPY and 4 LETF indices\nspyKappa = log( spy.strikes(tpoint,:)./spy.underl(tpoint) );\n%clean the NaN values\nspymat = [spyKappa;spy.iv(tpoint,:)];\nspymat = spymat(:,all(~isnan(spymat)));\nssomat = [ssoKappa;sso.iv(tpoint,:)];\nssomat = ssomat(:,all(~isnan(ssomat)));\nsdsmat = [sdsKappa;sds.iv(tpoint,:)];\nsdsmat = sdsmat(:,all(~isnan(sdsmat)));\nupromat = [uproKappa;upro.iv(tpoint,:)];\nupromat = upromat(:,all(~isnan(upromat)));\nspxumat = [spxuKappa;spxu.iv(tpoint,:)];\nspxumat = spxumat(:,all(~isnan(spxumat)));\n%% Sort the data\n%for SPY\n[~,spyI] = sort(spymat(1,:),'ascend');\nspyMAT = spymat(:,spyI);\n%for SSO\n[~,ssoI] = sort(ssomat(1,:),'ascend');\nssoMAT = ssomat(:,ssoI);\n%for SDS\n[~,sdsI] = sort(sdsmat(1,:),'ascend');\nsdsMAT = sdsmat(:,sdsI);\n%for UPRO\n[~,uproI] = sort(upromat(1,:),'ascend');\nuproMAT = upromat(:,uproI);\n%for SPXU\n[~,spxuI] = sort(spxumat(1,:),'ascend');\nspxuMAT = spxumat(:,spxuI);\n%% Plot the IVs before moneyness scaling\n%Define leverage ratios\nssobeta = 2;\nspxubeta = -3;\nnonleviv = spyMAT(2,:);\n%SSO\nssoiv = ssoMAT(2,:);\n%SDS\nsdsiv = sdsMAT(2,:);\n%UPRO\nuproiv = uproMAT(2,:);\n%SPXU\nspxuiv = spxuMAT(2,:);\nfigure\nsubplot(2,2,1)\nplot(nonlevmat(35:end),nonleviv(35:end),'*','Color', 'blue','LineWidth',3)\nhold on\nplot(ssoma(25:end),ssoiv(25:end)./abs(ssobeta),'o','Color', 'red','LineWidth',3)\ntitle(strcat('SSO, ', {' '} , num2str(dtm), {' '}, 'days to maturity: before scaling'))\nxlabel('LM')\nylabel('Implied Vol')\nhold off\nsubplot(2,2,2)\nplot(sdsma(25:end),sdsiv(25:end)./abs(sdsbeta),'o','Color', 'red','LineWidth',3)\ntitle(strcat('SDS, ', {' '} , num2str(dtm), {' '}, 'days to maturity: before scaling'))\nsubplot(2,2,3)\nplot(uproma(25:end),uproiv(25:end)./abs(uprobeta),'o','Color', 'red','LineWidth',3)\ntitle(strcat('UPRO, ', {' '} , num2str(dtm), {' '}, 'days to maturity: before scaling'))\nsubplot(2,2,4)\nplot(spxuma(25:end),spxuiv(25:end)./abs(spxubeta),'o','Color', 'red','LineWidth',3)\ntitle(strcat('SPXU, ', {' '} , num2str(dtm), {' '}, 'days to maturity: before scaling'))\n", "output_sequence": "Plot the implied volatilities for 4 leveraged ETFs versus the unleveraged SPY ETF implied volatility before moneyness scaling"}, {"input_sequence": "# ------------------------------------------------------------------------------\n# Project: MTS - Modeling of Term Structure for Inflation Estimation\n# ------------------------------------------------------------------------------\n# Quantlet: MTS_afns_france\n# Description: The estimation results for France derived from the AFNS model in\n# multi-maturity term structue. Graphic showing the filtered\n# and predicted state variables.\n# Keywords: Kalman filter, optimization, MLE, maximum likelihood, bond, plot,\n# filter, estimation, extrapolation, dynamics, term structure,\n# interest-rate\n# See also:\n# Author: Shi Chen\n## clear history\nrm(list = ls(all = TRUE))\ngraphics.off()\n## install and load packages\nlibraries = c(\"zoo\", \"FKF\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\nsetwd(\"\")\n## read data of France\nfrdata1 = read.csv(\"fr_nom.csv\", header = F, sep = \";\")\nfrdate = as.character(frdata1[, 2])\nst = which(frdate == \"30.06.2006\")\nfrdata11 = frdata1[(st:et), 3:14]\nfrdata2 = read.csv(\"fr_inf.csv\", header = F, sep = \";\")\nfrdate = as.character(frdata2[, 1])\nfrdata22 = frdata2[(st:et), 2:9]\nfrnom = cbind(frdata11[, 2], frdata11[, 4], frdata11[, 9])\nfrmat = c(3, 5, 10)\nfrjoi = cbind(frnom[, 1], frinf[, 1])\nfor (i in 2:length(frmat)) {\nfrjoi = cbind(frjoi, frnom[, i], frinf[, i])\n}\ny51 = t(frjoi)\nyieldadj_joint = function(sigma11, sigma12 = 0, sigma13 = 0, sigma21 = 0,\nsigma22, = 0, sigma31 = 0, sigma32 = 0, sigma33, lambda,\ntime = maturity) {\nAtilde = sigma11^2 + sigma12^2 + sigma13^2 + sigma44^2\nDtilde = sigma11 * sigma21 + sigma12 * sigma22 + sigma13 * sigma23\n\nadj1 = Atilde * time^2/6\nadj2 = Btilde * (1/(2 * lambda^2) - (1 - exp(-lambda * time))/(lambda^3 * time)\n+ (1 - exp(-2 * time * lambda))/(3 * lambda^3 * time))\nadj3 = Ctilde * (1/(2 * lambda^2) + exp(-lambda * time)/(lambda^2)\n- time * exp(-2 * lambda * time)/(4 * lambda)\nadj4 = Dtilde * (time/(2 * lambda)\n+ exp(-lambda * time)/(lambda^2)\nadj5 = Etilde * (3 * exp(-lambda * time)/(lambda^2) + time/(2 * lambda)\n+ time * exp(-lambda * time)/lambda\nadj6 = Ftilde * (1/(lambda^2) + exp(-lambda * time)/(lambda^2)\n- exp(-2 * lambda * time)/(2 * lambda^2)\n- 3 * (1 - exp(-lambda * time))/(lambda^3 * time)\nreturn(adj1 + adj2 + adj3 + adj4 + adj5 + adj6)\nMeloading_joint = function(lambda, alphaS, time = maturity) {\nrow1 = c(1, (1 - exp(-lambda * time))/(lambda * time), (1 - exp(-lambda * time))/(lambda * time)\n- exp(-lambda * time), 0)\nrow2 = c(0, alphaS * ((1 - exp(-lambda * time))/(lambda * time)), alphaC *\n((1 - exp(-lambda * time))/(lambda * time) - exp(-lambda * time)), 1)\nMatrixB = rbind(row1, row2)\nreturn(MatrixB)\nafnsss = function(t1, t2, t10,\nt14, s1, g2, l1, h2, h4) {\nTt = matrix(c(t1, t2, t10,\nt14, t16), nr = 4)\nZt = rbind(Meloading_joint(lambda = l1, alphaS = g1, alphaC = g2, time = frmat[1]),\nct = matrix(rep(c(yieldadj_joint(sigma11 = h1, sigma22 = h2, sigma33 = h3,\nsigma44 = h4, lambda = l1, time = frmat[1]),\nyieldadj_joint(sigma11 = h1, sigma22 = h2, sigma33 = h3,\nsigma44 = h4, lambda = l1, time = frmat[2]),\nyieldadj_joint(sigma11 = h1, sigma22 = h2, sigma33 = h3, sigma44 = h4,\nlambda = l1, time = frmat[1])), each = 2), nr = 6, nc = 1)\ndt = matrix(c(1 - t1, 1 - t6, t10, 1 -\nt11, 1 - t16), nr = 4) %*% matrix(c(s1, s2, s4), nr = 4)\nGGt = matrix(0.1 * diag(6), nr = 6, nc = 6)\nH = diag(c(h1^2, h2^2, h4^2), nr = 4)\nHHt = Tt %*% H %*% t(Tt)\na0 = c(s1, s2, s4)\nP0 = HHt * 10\nreturn(list(a0 = a0, P0 = P0, ct = ct, dt = dt, Zt = Zt, Tt = Tt, GGt = GGt,\nHHt = HHt))\nobjective = function(theta, yt) {\nsp = afnsss(theta[\"t1\"], theta[\"t2\"],\ntheta[\"h3\"],\nans = fkf(a0 = sp$a0, P0 = sp$P0, dt = sp$dt, ct = sp$ct, Tt = sp$Tt,\nZt = sp$Zt, HHt = sp$HHt, GGt = sp$GGt, yt = yt)\nreturn(-ans$logLik)\ntheta <- c(t = c(0.8, 0, 0.8, 0, 0.8, 0, 0.8), s = c(0.2, 0.2),\ng = c(0.6, 0.6), l1 = c(0.7), h = c(0.2, 0.2))\nfit = optim(theta, objective, yt = y51, hessian = TRUE)\nsp = afnsss(fit$par[\"t1\"],\nfit$par[\"t15\"],\nfit$par[\"h2\"],\nans = fkf(a0 = sp$a0, P0 = sp$P0, dt = sp$dt, ct = sp$ct, Tt = sp$Tt, Zt = sp$Zt,\nHHt = sp$HHt, GGt = sp$GGt, yt = y51)\nres = matrix(rowMeans(ans$vt[, 2:103]), nr = 6)\njoifr0915ans = ans\nsave(joifr0915ans, file = \"joifr0915ans.RData\")\n## The plots of filtered and predicted state variables Another approach:\n## plot.fkf(ans, CI=NA)\nplot(ans$at[1, -1], type = \"l\", col = \"red\", ylab = \"State variables\",\nxlab = \"\", ylim = c(-6, 6), lwd = 2)\nlines(ans$att[1, -1], lty = 2, col = \"red\", lwd = 2)\n", "output_sequence": "Shows the estimation results derived from the AFNS model in a multi-maturity term structure for France. Graphic showing the filtered and predicted state variables."}, {"input_sequence": "# ------------------------------------------------------------------------------\n# Project: MTS - Modeling of Term Structure for Inflation Estimation\n# ------------------------------------------------------------------------------\n# Quantlet: MTS_afns_sweden\n# Description: The estimation results for Sweden derived from the AFNS model in\n# multi-maturity term structue. Graphic showing the filtered\n# and predicted state variables.\n# Keywords: Kalman filter, optimization, MLE, maximum likelihood, bond, plot,\n# filter, estimation, extrapolation, dynamics, term structure,\n# interest-rate\n# See also:\n# Author: Shi Chen\nrm(list = ls(all = TRUE))\ngraphics.off()\n## install and load packages\nlibraries = c(\"zoo\", \"FKF\", \"expm\", \"Matrix\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n## read data of Sweden\nswdata1 = read.csv(\"sweden_nom.csv\", header = F, sep = \";\")\nswdate = as.character(swdata1[, 1])\nst = which(swdate == \"30.04.2007\")\nswdata11 = swdata1[(st:et), 2:8]\nswdata2 = read.csv(\"sweden_inf.csv\", header = F, sep = \";\")\nswdate = as.character(swdata2[, 1])\nswdata22 = swdata2[(st:et), 2:6]\nswnom = cbind(swdata11[, 1], swdata11[, 4], swdata11[, 5])\nswmat = c(2, 5, 7)\nswjoi = cbind(swnom[, 1], swinf[, 1])\nfor (i in 2:length(swmat)) {\nswjoi = cbind(swjoi, swnom[, i], swinf[, i])\n}\ny51 = t(swjoi)\nyieldadj_joint = function(sigma11, sigma12 = 0, sigma13 = 0, sigma21 = 0,\nsigma22, = 0, sigma31 = 0, sigma32 = 0, sigma33, lambda,\ntime = maturity) {\nAtilde = sigma11^2 + sigma12^2 + sigma13^2 + sigma44^2\nDtilde = sigma11 * sigma21 + sigma12 * sigma22 + sigma13 * sigma23\n\nadj1 = Atilde * time^2/6\nadj2 = Btilde * (1/(2 * lambda^2) - (1 - exp(-lambda * time))/(lambda^3 *\ntime) + (1 - exp(-2 * time * lambda))/(3 * lambda^3 * time))\nadj3 = Ctilde * (1/(2 * lambda^2) + exp(-lambda * time)/(lambda^2) -\ntime * exp(-2 * lambda * time)/(4 * lambda) - 3 * exp(-2 * lambda *\ntime)/(4 * lambda^2) - 2 * (1 - exp(-lambda * time))/(lambda^3 *\ntime) + 5 * (1 - exp(-2 * lambda * time))/(8 * lambda^3 * time))\nadj4 = Dtilde * (time/(2 * lambda) + exp(-lambda * time)/(lambda^2) -\n(1 - exp(-lambda * time))/(lambda^3 * time))\nadj5 = Etilde * (3 * exp(-lambda * time)/(lambda^2) + time/(2 * lambda) +\ntime * exp(-lambda * time)/lambda - 3 * (1 - exp(-lambda * time))/(lambda^3 *\ntime))\nadj6 = Ftilde * (1/(lambda^2) + exp(-lambda * time)/(lambda^2) - exp(-2 *\nlambda * time)/(2 * lambda^2) - 3 * (1 - exp(-lambda * time))/(lambda^3 *\ntime) + 3 * (1 - exp(-2 * lambda * time))/(4 * lambda^3 * time))\nreturn(adj1 + adj2 + adj3 + adj4 + adj5 + adj6)\nMeloading_joint = function(lambda, alphaS, time = maturity) {\nrow1 = c(1, (1 - exp(-lambda * time))/(lambda * time), (1 - exp(-lambda *\ntime))/(lambda * time) - exp(-lambda * time), 0)\nrow2 = c(0, alphaS * ((1 - exp(-lambda * time))/(lambda * time)), alphaC *\n((1 - exp(-lambda * time))/(lambda * time) - exp(-lambda * time)),\n1)\nMatrixB = rbind(row1, row2)\nreturn(MatrixB)\nafnsss = function(t1, t2, t10,\nt14, s1, g2, l1, h2, h4) {\nTt = matrix(c(t1, t2, t10,\nt14, t16), nr = 4)\nZt = rbind(Meloading_joint(lambda = l1, alphaS = g1, alphaC = g2, time = swmat[1]),\nct = matrix(rep(c(yieldadj_joint(sigma11 = h1, sigma22 = h2, sigma33 = h3,\nsigma44 = h4, lambda = l1, time = swmat[1]), yieldadj_joint(sigma11 = h1,\nsigma22 = h2, sigma33 = h3, sigma44 = h4, lambda = l1, time = swmat[2]),\nyieldadj_joint(sigma11 = h1, sigma22 = h2, sigma33 = h3, sigma44 = h4,\nlambda = l1, time = swmat[1])), each = 2), nr = 6, nc = 1)\ndt = matrix(c(1 - t1, 1 - t6, t10, 1 -\nt11, 1 - t16), nr = 4) %*% matrix(c(s1, s2,\ns3, s4), nr = 4)\nGGt = matrix(0.1 * diag(6), nr = 6, nc = 6)\nH = diag(c(h1^2, h2^2, h4^2), nr = 4)\nHHt = Tt %*% H %*% t(Tt)\na0 = c(s1, s2, s4)\nP0 = HHt * 10\nreturn(list(a0 = a0, P0 = P0, ct = ct, dt = dt, Zt = Zt, Tt = Tt, GGt = GGt,\nHHt = HHt))\nobjective = function(theta, yt) {\nsp = afnsss(theta[\"t1\"], theta[\"t2\"],\ntheta[\"h3\"],\nans = fkf(a0 = sp$a0, P0 = sp$P0, dt = sp$dt, ct = sp$ct, Tt = sp$Tt,\nZt = sp$Zt, HHt = sp$HHt, GGt = sp$GGt, yt = yt)\nreturn(-ans$logLik)\ntheta = c(t = c(0.8, 0, 0.8, 0, 0.8, 0, 0.8),\ns = c(0.2, 0.2), g = c(0.6, 0.6), l1 = c(0.7), h = c(0.2,\n0.2, 0.2))\nfit = optim(theta, objective, yt = y51, hessian = TRUE)\nsp = afnsss(fit$par[\"t1\"],\nans = fkf(a0 = sp$a0, P0 = sp$P0, dt = sp$dt, ct = sp$ct, Tt = sp$Tt, Zt = sp$Zt,\nHHt = sp$HHt, GGt = sp$GGt, yt = y51)\njoisw0714ans = ans\nsave(joisw0714ans, file = \"joisw0714ans.RData\")\n## The plots of filtered and predicted state variables Another approach:\n## plot.fkf(ans, CI=NA)\nplot(ans$at[1, -1], type = \"l\", col = \"red\", ylab = \"State variables\",\nxlab = \"\", ylim = c(-6, 6), lwd = 2)\nlines(ans$att[1, -1], lty = 2, col = \"red\", lwd = 2)\n", "output_sequence": "Shows the estimation results derived from the AFNS model in a multi-maturity term structure for Sweden. Graphic showing the filtered and predicted state variables."}, {"input_sequence": "import math\nimport numpy as np\nimport pandas as pd\nimport scipy as sp\nimport numpy.random as npr\nimport scipy.stats as scs\nimport matplotlib.pyplot as plt\nimport numpy.random as npr\nimport datetime\nfrom scipy import optimize\n# SVCJ parameters\nmu = 0.042\nr = mu\nmu_y = -0.0492\nsigma_y = 2.061\nl = 0.0515\nalpha = 0.0102\nbeta = -0.188\nrho = 0.275\nsigma_v = 0.007\nrho_j = -0.210\nmu_v = 0.709\nv0 = 0.19**2\nkappa = 1-beta\nnpr.seed(12345)\ndt = 1/360.0 # dt\nm = int(360.0 * (1/dt)/360.0) # time horizon in days\nn = 1000000\n#trialrun\n#dt = 1/10\nT = m * dt\nw = npr.standard_normal([n,m])\nw2 = rho * w + sp.sqrt(1-rho**2) * npr.standard_normal([n,m])\nz_v = npr.exponential(mu_v, [n,m])\nz_y = npr.standard_normal([n,m]) * sigma_y + mu_y + rho_j * z_v\ndj = npr.binomial(1, l * dt, size=[n,m])\ns = np.zeros([n,m+1])\ns0 = 6500\ns[:,0] = s0 # initial CRIX level, p. 20\nfor i in range(1,m+1):\nv[:,i] = v[:,i-1] + kappa * (theta - np.maximum(0,v[:,i-1])) * dt + sigma_v * sp.sqrt(np.maximum(0,v[:,i-1])) * w2[:,i-1] + z_v[:,i-1] * dj[:,i-1]\ns[:,i] = s[:,i-1] * (1 + (r - l * (mu_y + rho_j * mu_v)) * dt + sp.sqrt(v[:,i-1] * dt) * w[:,i-1]) + z_v[:,i-1] * dj[:,i-1]\nplt.plot(np.transpose(s[:100]));\nplt.xlabel('time step')\nplt.ylabel('asset price process')\nplt.plot(np.transpose(sp.sqrt(v[:70])));\nplt.ylabel('volatility process')\n# Option pricing\ncp = np.exp(-mu * m * dt) * np.maximum(s[:,-1]-k,0).mean()\ndef callprice(S,K,T,sigma,r):\nd1=(sp.log(S/K) + (r + 0.5 * sigma**2)*T) / (sigma * sp.sqrt(T))\nreturn S*scs.norm.cdf(d1) - sp.exp(-r *T) * K * scs.norm.cdf(d2)\n#Implied volatility for given strike\ndef solve(sigma, cp, k, T):\nreturn (callprice(100, k, T, sigma, r) - cp)**2\nresult = sp.optimize.minimize(solve, 0.2, args=(cp, k, T))\n#result.x[0]\ns2 = s\ns = s2/s0 * 100\ndef call_and_vol(K, T): # T maturity in years\nm = int(np.round(T / dt))\ncp = np.exp(-r * m * dt) * np.maximum(s[:,-1]-K,0).mean()\niv = sp.optimize.minimize(solve, 0.4, args=(cp, K,T)).x[0]\nreturn [cp, iv]\n#[call_and_vol(120,0.5), callprice(100, 120, 0.5, 0.98, r)]\nstrike = np.arange(.7, 1.3, 0.1)*100\nprint(len(strike))\nstrike, ttm = np.meshgrid(strike, ttm)\npath = 'option_data.h5'\nh5 = pd.HDFStore(path, 'r')\ndata = h5['data'] # European call & put option data (3 maturities)\n#h5.close()\n#type(data)\n#data.info()\niv = np.zeros(strike.shape)\ndf = pd.DataFrame(columns=data.columns)\ni = 0\ndate = datetime.date(2019, 7, 4)\nfor k in range(strike.shape[1]):\n[x, y] = call_and_vol(strike[t,k], ttm[t,k])\niv[t,k] = y\ndf.loc[i] = [date, strike[t,k], x, date + datetime.timedelta(days=ttm[t,k]*365), 0]\n#compute Black-Scholes calibrated parameter\ndef solve_bs(sigma):\nreturn (callprice(100, df['Strike'], ttm.reshape(77), sigma, r) - df['Call'])**2\nfrom scipy.optimize import root\nimpliedvol = optimize.root(solve_bs,0.2, method='lm').x\nfrom mpl_toolkits.mplot3d import Axes3D\nfig = plt.figure(figsize=(12, 7))\nax = fig.gca(projection='3d') # set up canvas for 3D plotting\nsurf = ax.plot_surface(strike/100, ttm, iv, rstride=3,\ncmap=plt.cm.coolwarm, linewidth=0.5,\nantialiased=True) # creates 3D plot\nax.view_init(30, 30)\nax.set_xlabel('moneyness')\nax.set_ylabel('time-to-maturity')\nfig.colorbar(surf, shrink=0.5, aspect=5);\n", "output_sequence": "Simulation of the Euler-discretized Stochastic Volatility with Correlated Jumps model and Monte Carlo Option Pricing"}, {"input_sequence": "#------------------------\n# JOIN ALL DATA SOURCES #\n#------------------------\n\n# the variation where only crime in populated census tracts is counted! Otherwise, use tractID as leading ID and\n# not census$ctlabel\n# Load Libraries --------------------------------------------------------------\nlibrary(RPostgreSQL) # to access PSQL data\nlibrary(data.table) # efficient memory management\nlibrary(lubridate) # to access isoweek\nlibrary(dplyr) # for data manipulation\nlibrary(dtplyr) # to preserve data.table class in left_joins\nlibrary(sp) # to load shapefile\nlibrary(tidyr) # to create dummy matrices\nlibrary(Matrix) # for sparse matrices\nlibrary(rgdal) # to read shapefiles\nlibrary(beepr) # used while loading the taxi data\n# helper functions ------------------------------------------------------------\nsource(\"feature_helperfunctions.R\")\n# OBJECTIVE -------------------------------------------------------------------\n# CREATE 2 FINAL VAR TABLES OF THE FOLLOWING FORM:\n# id | y | week | Census | Taxi | POI | Twitter | County |\n# ______________________________________________________________ --------|---| 22 | |------| |\n# --------|---| 23 | |------| | ______________________________________________________________ where\n# '---' denote weekly changing values and empty spaces denote constant values one table captures violent crime, the\n# other property crime\n# define analysis timeframe\nstart_day = as.Date(\"2015-06-01\")\ntime_window = unique(isoweek(seq(start_day, end_day, by = \"days\")))\n# CENSUS ----------------------------------------------------------------------\ncensus = fread(\"D:/crime-data/2010_NYC_Census_analysis.csv\", stringsAsFactors = F)\nfinal_frame = data.table(gid = rep(census$gid, length(time_window)), week = rep(time_window, each = nrow(census)))\n# create lag_frame for taxi feature\nlag_frame = data.table(gid = rep(census$gid, length(time_window)), week = rep(time_window - 1, each = nrow(census)))\n# join census data\nfinal_frame = left_join(final_frame, census)\n# CRIME DATA ------------------------------------------------------------------ load data\ncon1 = dbConnect(dbDriver(\"PostgreSQL\"), dbname = \"crime-data\", user = \"postgres\", host = \"localhost\")\ncrime_data = dbGetQuery(con1, \"SELECT id, cmplnt_fr_dt, ofns_desc, ctlabel, loc_gid FROM crimes_sub\")\nsetDT(crime_data)\n# subset for 2015\ncrime_data[, `:=`(cmplnt_fr_dt,\ncrime15 = subset(crime_data, format.Date(cmplnt_fr_dt, \"%Y\") == \"2015\")\n# separate into offenses\nproperty_crime = c(\"BURGLARY\", \"ARSON\", \"GRAND LARCENY\", \"PETIT LARCENY\", \"GRAND LARCENY OF MOTOR VEHICLE\")\nviolent_crime = c(\"ROBBERY\", \"MURDER & NON-NEGL. MANSLAUGHTER\", \"FELONY ASSAULT\")\n# per week for Jun - Nov + one week earlier for lagged taxi feature\nweekly_property_crime15 = weekly_crime_aggregation(data = crime15, weeks = c(time_window[1] - 1, time_window), crime = property_crime)\n# Join crime data\nfinal_frame_property = left_join(final_frame, weekly_property_crime15)\n# set lag_frame\nlag_frame_property = left_join(lag_frame, weekly_property_crime15)\n# add zero crime\nlag_frame_property[is.na(occ), `:=`(occ, 0)]\n# set keys for week, gid\nsetkey(final_frame_property, week, gid)\nrm(crime_data, crime15, property_crime, con1, final_frame)\n# TAXI DATA -------------------------------------------------------------------\n# there are three different settings, taxi(normalised by destination), taxi1 (normalised by source), taxi (not\n# normalised) the final selection uses taxi, therefore only the initial code using 'tidy_sparse' is shown, not how\n# to include it separately (which is the same as for 'taxi')\ncon2 = dbConnect(dbDriver(\"PostgreSQL\"), dbname = \"nyc-taxi-data\", user = \"root\", host = \"localhost\")\n# takes very long\ntaxi = dbReadTable(con2, \"analysis_data\")\nbeep(5)\nsetDT(taxi)\nsetkey(taxi, week, pickup_nyct2010_gid, dropoff_nyct2010_gid)\n# exclude all trips which did not start or end in NYC\ntaxi = na.omit(taxi, cols = c(\"pickup_nyct2010_gid\", \"dropoff_nyct2010_gid\"))\n# exclude trips to areas without census data\ntaxi = taxi[(pickup_nyct2010_gid %in% census$gid) & (dropoff_nyct2010_gid %in% census$gid)]\n# 70,266,249 valid trips\n# save(taxi, file = 'taxi15.Rdata')\n# create sparse contingency table for each week\ntaxi_list = lapply(time_window, function(x) xtabs(~taxi_factor(pickup_nyct2010_gid, census$gid) + taxi_factor(dropoff_nyct2010_gid,\ncensus$gid), data = taxi[week == x], sparse = T))\n# set ii = 0, normalise by ....\ntaxi_list = tidy_sparse(taxi_list, normal = \"destination\")\n# taxi_list1 = tidy_sparse(taxi_list, normal ='source') taxi_list2 = tidy_sparse(taxi_list, normal ='counts')\n# set names\nnames(taxi_list1) = time_window\n# save(taxi_list, file = 'taxi_list15.Rdata')\n# multiply flow matrix by lagged crime vector (use the other taxi lists for the other settings)\ntaxi_property_crime_list = sapply(time_window, function(x) taxi_list[[paste(x)]] %*% lag_frame_property[week == (x -\n1), occ], USE.NAMES = T)\ntaxi_violent_crime_list = sapply(time_window, function(x) taxi_list[[paste(x)]] %*% lag_frame_violent[week == (x - 1),\nocc], USE.NAMES = T)\nnames(taxi_property_crime_list) = time_window\n# Joining taxi data\nfor (k in time_window) {\nset(final_frame_property, i = which(final_frame_property[, week] == k), j = \"taxi\", value = taxi_property_crime_list[[paste(k)]][,\n1])\n}\nset(final_frame_violent, i = which(final_frame_violent[, week] == k), j = \"taxi\", value = taxi_violent_crime_list[[paste(k)]][,\nrm(con2, taxi, k, taxi_property_crime_list, taxi_list, census, lag_frame_property,\n# FOURSQUARE ------------------------------------------------------------------\n# 1) match the coordinates of the foursquare venues to the NYC census tract shapefile 2) clean up data by counting\n# how many venues per category in function 'foursquare_aggregation' 3) join to the final_frames 4) from those raw\n# counts, normalise by total number of venues in the tract 5) calculate a heterogeneity measure of how many\n# different measures are in the tract\n# read foursquare data\nf4s_frame = fread(\"D:/crime-data/Foursquare/f4s_data2.csv\")\n# 1) match foursquare coordinates to census tracts\n# set spatial dataframe and ensure correct mapping\ncoordinates(f4s_frame) = c(3, 2) # set coordinates on longitude, latitude\nproj4string(f4s_frame) = CRS(\"+proj=longlat +datum=WGS84\")\n# read shapefile\ntracts = sp:::spTransform(readOGR(\"D:/crime-data/nyc-taxi-data/nyct2010_15b/nyct2010.shp\", layer = \"nyct2010\"), CRS(\"+proj=longlat +datum=WGS84\"))\ntracts@data$gid = as.numeric(rownames(tracts@data)) + 1\nnames(tracts@data) = tolower(names(tracts@data))\n# returns vector the same length as f4s_frame with census ID\nf4s_frame$gid = over(f4s_frame, tracts[, \"gid\"])\n# transform from spatial dataframe into data.table\nfoursquare = as.data.table(f4s_frame)\n# save(foursquare, file = 'foursquare_complete.Rdata')\n# 2) clean up data\nfoursquare = foursquare_aggregation(foursquare)\n# 3) join to final_frames\nfinal_frame_violent = left_join(final_frame_violent, foursquare)\n# ensure that areas not present in foursquare data are set to zero in final_frame\nfor (i in names(final_frame_violent)) {\nfinal_frame_violent[is.na(get(i)), `:=`((i), 0)]\n# 4) copy the existing columns and append 'A' to their names\nfinal_frame_violent[, `:=`(paste0(names(foursquare)[-1], \"A\"), final_frame_violent[, names(foursquare)[-1], with = F])]\n# set indices\ncols = names(final_frame_property)[grepl(\"A\",\ncols0 = names(foursquare)[-1]\n# divide by total number of venues in the census tract\nfinal_frame_violent[, `:=`((cols), final_frame_violent[, cols, with = F]/rowSums(final_frame_violent[, cols, with = F]))]\n# set NaN (division by zero) to 0\n# 5) calculate a heterogeneity measure sum how many venues per category divide the number of venues in each tract by\n# the corresponding sum to obtain p_i calculate H = -sum(p_i * log(p_i))\nfinal_frame_violent[, `:=`(H, apply(final_frame_violent[, cols, with = F], 1, function(x) -sum(x * log(x), na.rm = T)))]\nrm(foursquare, tracts, f4s_frame, cols, cols0)\n# TWITTER DATA ----------------------------------------------------------------\n# 1) load twitter data and remove tweets not in the analysis timeframe create night variable by counting tweets sent\n# between 22pm-6am 2) join with final_frames 3) create log(counts) variables 4) normalise tweet activity by dividing\n# by NTA activity\n# 1) load and manipulate\ncon3 = dbConnect(dbDriver(\"PostgreSQL\"), dbname = \"foursquare\", user = \"root\", host = \"localhost\")\ntwitter = dbReadTable(con3, \"twitter\")\ntwitter = twitter_aggregation(twitter, time_window)\n# save(twitter, file = 'aggregated_tweets15.RData')\ntwitter_names = c(\"tweet_counts\", \"night_tweets\")\n# 2) join\nfinal_frame_violent = left_join(final_frame_violent, twitter)\n# replace NA by 0\nfor (i in twitter_names) {\n# 3) take log\nfinal_frame_property[, `:=`(c(\"log_tweets\", \"log_night\"), lapply(.SD, log)), .SDcols = twitter_names]\n# replace log(0) = -Inf by 0\ninvisible(lapply(c(\"log_tweets\", \"log_night\"), function(.name) set(final_frame_violent, which(is.infinite(final_frame_violent[[.name]])),\nj = .name, value = 0)))\ninvisible(lapply(c(\"log_tweets\", \"log_night\"), function(.name) set(final_frame_property, which(is.infinite(final_frame_property[[.name]])),\n# 4) normalise\nfinal_frame_violent[, `:=`(c(\"NTA_counts\", \"NTA_night\"), .SD/lapply(.SD, sum)), by = NTACode, .SDcols = twitter_names]\nrm(twitter, con3, twitter_names)\n# INCLUDE COUNTY DUMMY ---------------------------------------------------------\nfinal_frame_property[, `:=`(county, substr(ctlabel, 1, 5))]\nfinal_frame_property[, `:=`(county, recode(final_frame_property$county, `36005` = \"bronx\", `36047` = \"kings\", `36061` = \"ny\",\n`36081` = \"queens\"))]\ndummies = unique(final_frame_property[, county])\nfinal_frame_property[, `:=`((dummies), lapply(dummies, function(x) county == x))]\nfinal_frame_violent[, `:=`(county, substr(ctlabel, 1, 5))]\nfinal_frame_violent[, `:=`(county, recode(final_frame_violent$county, `36005` = \"bronx\", `36047` = \"kings\", `36061` = \"ny\",\nfinal_frame_violent[, `:=`((dummies), lapply(dummies, function(x) county == x))]\nfinal_frame_property[, `:=`(c(\"ctlabel\", \"county\"), NULL)]\nrm(dummies)\n# FINAL ADJUSTMENTS -----------------------------------------------------------\nsetcolorder(final_frame_property, c(\"gid\", \"week\", \"occ\", \"population\", \"age\", \"young_male\", \"white\", \"black\",\n\"asian\", \"hispanic\", \"vacancy\", \"female_head\", \"institution\", \"tweet_counts\", \"night_tweets\", \"log_night\",\n\"NTA_counts\", \"entertainment\", \"uni\", \"food\", \"professional\", \"nightlife\", \"outdoors\", \"shops\", \"travel\",\n\"residential\", \"ny\", \"kings\", \"bronx\", \"queens\", \"taxi\", \"entertainmentA\", \"uniA\", \"foodA\", \"professionalA\",\n\"nightlifeA\", \"outdoorsA\", \"shopsA\", \"travelA\", \"residentialA\", \"H\", \"NTACode\"))\nsetcolorder(final_frame_violent, c(\"gid\", \"week\", \"occ\", \"population\", \"age\", \"young_male\", \"white\", \"black\",\nsave(final_frame_property, file = \"final_frame_property15.Rdata\")\n# load('final_frame_property15.Rdata')\n", "output_sequence": "creates data frames of merged census data, crime data, taxi data separately for violent and property crime"}, {"input_sequence": "# Data Preparation Functions --------------------------------------------------\n\n# TAXI DATA FUNCTIONS ---------------------------------------------------------\n# Function to ensure correct labels for contingency matrix\ntaxi_factor = function(x, y) {\nfactor(as.character(x), levels = sort(unique(y)))\n}\n# diagonal for sparse matrix\nsparse_diagonal = function(x) {\ndiag(x) = 0\ndrop0(x)\n# tidy diagonal and normalise by destination/source\ntidy_sparse = function(lt, normal) {\n\n# tidy diagonal\nlt = lapply(lt, sparse_diagonal)\n# normalisation by destination\nif (normal == \"destination\") {\nlt = lapply(lt, function(y) sweep(y, 2, colSums(y, na.rm = T), FUN = \"/\"))\n} else if (normal == \"source\") {\n# normalisation by source\nlt = lapply(lt, function(y) sweep(y, 1, rowSums(y, na.rm = T), FUN = \"/\"))\n} else if (normal == \"counts\") {\nlt = lt\n} else {\nstop(\"invalid normalisation\")\n}\n# set NaN values (possible due to division by 0) to sparse\nlt = lapply(lt, function(y) {\ny@x[is.na(y@x)] = 0\ny = drop0(y)\ny\n})\nreturn(lt)\n# CRIME DATA FUNCTIONS --------------------------------------------------------\n# aggregates case crime data to weekly data per census tracts for aggr=T, all violent/property crime cases are\n# counted, for aggr=F, the crime types within are preserved\nweekly_crime_aggregation = function(data, weeks, crime, aggr = T) {\nwarning(\"This function aggregates based on week. Careful if data from multiple years are used\")\n# Sanity checks\ncols = c(\"ofns_desc\", \"loc_gid\", \"cmplnt_fr_dt\")\nif (any(!(cols %in% colnames(data))))\nstop(\"invalid data columns: ofns_desc, loc_gid, cmplt_fr_dt\")\nif (is.numeric(weeks) == F)\nstop(\"invalid weeks format: numeric vector\")\nif ((is.vector(crime) & is.character(crime)) == F)\nstop(\"invalid format for crime: character vector\")\n# if aggregation, no distintion between crime type offenses is made (ie more aggregation)\nif (aggr == T) {\nresult = data %>% # subset to cases in crime type\nfilter(ofns_desc %in% crime) %>% # filter crimes in analysis weeks\nmutate(week = lubridate::isoweek(cmplnt_fr_dt)) %>% filter(week %in% weeks) %>% group_by(week, loc_gid) %>%\nsummarise(occ = n()) %>% rename(gid = loc_gid) %>% setorder(week, gid)\nsetDT(result)\n\n} else if (aggr == F) {\n# for aggr == F, separate between offenses\nmutate(week = lubridate::isoweek(cmplnt_fr_dt)) %>% filter(week %in% weeks) %>% group_by(week, loc_gid, ofns_desc) %>%\nsummarise(occ = n()) %>% rename(gid = loc_gid) %>% setorder(Week, gid)\nresult = result[complete.cases(result), ]\nreturn(result)\n# same idea as in weekly_crime_aggregation, in this case, counts all crimes and does not count per week\ncrime_aggregation = function(data, weeks, crime, aggr = T) {\nif (!(is.vector(crime) & is.character(crime)))\ndplyr::filter(ofns_desc %in% crime) %>% # filter crimes in analysis weeks\nmutate(week = lubridate::isoweek(cmplnt_fr_dt)) %>% filter(week %in% weeks) %>% group_by(loc_gid) %>% summarise(occ = n()) %>%\nrename(gid = loc_gid) %>% setorder(gid)\nmutate(week = lubridate::isoweek(cmplnt_fr_dt)) %>% filter(week %in% weeks) %>% group_by(loc_gid, ofns_desc) %>%\nsummarise(occ = n()) %>% dplyr::rename(gid = loc_gid) %>% setorder(gid)\n# aggregates dataframe with individuals tweets and census tract id to frame with counts and night counts per week\ntwitter_aggregation = function(data, weeks) {\ncols = c(\"created_at\", \"gid\")\nif (any((cols %in% colnames(data)) == F))\nstop(\"invalid data columns: created_at, gid\")\n# define if a tweet is sent at night\nnighttime = c(22, 23, 0, 1, 2, 3, 4, 5)\nresult = twitter %>% # remove cases outside of census tracts (ie ferries)\nfilter(complete.cases(gid)) %>% # set week and nighttime variable\nmutate(week = lubridate::isoweek(created_at), night = ifelse(hour(created_at) %in% nighttime, 1, 0)) %>% filter(week %in%\nweeks) %>% group_by(week, gid) %>% summarise(tweet_counts = n(), night_tweets = sum(night))\nsetDT(result)\n# for foursquare table with venue data incl. categories and census tract id, aggregates to counts of each category\n# per census tract\nfoursquare_aggregation = function(data) {\nstopifnot(c(\"category\", \"gid\") %in% colnames(data))\nresult = data %>% filter(complete.cases(.)) %>% # translate categories\nmutate(category = recode(category, `4d4b7104d754a06370d81259` = \"entertainment\", `4d4b7105d754a06372d81259` = \"uni\",\n`4d4b7105d754a06374d81259` = \"food\", `4d4b7105d754a06376d81259` = \"nightlife\", `4d4b7105d754a06377d81259` = \"outdoors\",\n`4d4b7105d754a06379d81259` = \"travel\")) %>% # group by census tract\ngroup_by(gid, category) %>% dplyr::summarise(counts = n()) %>% # mutate to new columns\ntidyr::spread(category, counts) %>% # replace NA values\nmutate_each(funs(replace(., is.na(.), 0)), -gid)\n", "output_sequence": "creates data frames of merged census data, crime data, taxi data separately for violent and property crime"}, {"input_sequence": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport plotly\nimport plotly.express as px\nimport plotly.graph_objects as go\nart_df = pd.read_csv('quartrely_art_sales.csv', delimiter=';')\nart_df = art_df.rename(columns={'Unnamed: 0': 'Quarter'})\nart_df = art_df.set_index('Quarter')\nfig = make_subplots(specs=[[{\"secondary_y\": False}]])\nfig['layout'].update(height=800, width=1200,\ntitle='',\nshowlegend=False,\nfont=dict(family='Times New Roman', size=20))\nfig.add_trace(go.Bar(name='Post war',\ny=art_df['Post War'],\nx=art_df.index,\nmarker_color='rgba(30, 56, 136,1)' ,\nmarker_line_width=0,\n), secondary_y=False)\nfig.add_trace(go.Bar(name='Contemporary',\ny=art_df['Contemporary'],\nmarker_color='rgba(71, 168, 189, 1)',\nmarker_line_width=0\nfig.add_trace(go.Bar(name='Ultra contemporary',\ny=art_df['Ultra Contemporary'],\nmarker_color='rgba(245, 230, 99, 1)',\nfig.add_trace(go.Bar(name='NFT',\ny=art_df['NFT'],\nmarker_color='rgba(255, 173, 105,1)',\nfig.add_trace(go.Scatter(name='NFT cusum',\ny=art_df['NFT'].cumsum(),\nline=dict(color='black', dash='dot', width=2)\nfig['layout']['xaxis'].update(title='Date')\nfig.update_xaxes(showline=True, linewidth=1, linecolor='black',\nmirror=True,\nshowgrid=False)\nfig.update_yaxes(showline=True, linewidth=1, linecolor='black',\nmirror=True,\nfig.update_layout({'plot_bgcolor': 'rgba(0,0,0,0)',\n'paper_bgcolor': 'rgba(0,0,0,0)'},\nfont_color='black',\nbargap=0.5)\nfig.show()\n# fig.write_image('./art_econ.pdf')\n# plotly.offline.plot(fig, filename='all_indices.html')\n", "output_sequence": "Quarterly sales in USD millions of post war, contemporary, ultra contemporary, NFT art markets, plotted in order. The dotted line is the cumulative sales on NFT."}, {"input_sequence": "%% clear all variables and console and close windows\nclear\nclc\nclose all\n%% load data\nx = load('journaux.dat');\na = sum(x, 2);\ne = a * b/sum(a);\n% Chi-matrix\ncc = (x - e)./sqrt(e);\n[g1, l1, d1] = svd(cc);\n% change the sign of some columns in g and d. This is done to get the same\n% results as R.\ng = g1(:,1:10)\ng(:,[2:4 9]) = -g(:,[2:4 9]);\nl = diag(l1)';\nd(:,[2:4 9]) = -d(:,[2:4 9]);\nll = l.*l;\naux = cumsum(ll)./sum(ll);\nperc = [ll',aux'];\nr1 = repmat(l, 15 ,1).*g; % multiplies each column of g with each corresponding element of l\nr = r1./repmat(sqrt(a), 1, 10); % divides each row of r1 with each corresponding element of sqrt(a)\ns1 = repmat(l, 10, 1).*d; % multiplies each column of d with each corresponding element of l\ns = s1./repmat(sqrt(b)', 1, 10); % divides each row of s1 with each corresponding element of sqrt(b)\nrr = r(:,1:2);\n% names of the labels in graph\ntypes = ['va'; 'ff'; 'bj';\n'bk'; 'vm'; 'fn'; 'fo'];\nregions = ['brw'; 'bxl'; 'anv'; 'brf'; 'foc'; 'hai'; 'lig';\n'lim'; 'lux'];\nhold on\ntext(rr(:,1), rr(:,2), types, 'Color', 'b', 'FontSize',12)\nset(gca,'box','on')\nline([-1.5 1.5], [0 0], 'Color', 'k')\nxlim([-1.1 1.5])\ntitle('Journal Data')\nxlabel('r_1, s_1')\n", "output_sequence": "Performs a correspondence analysis for the Belgian journal data, shows the eigenvalues of the singular value decomposition of the chi-matrix and displays graphically its factorial decomposition."}, {"input_sequence": "# clear all variables\nrm(list = ls(all = TRUE))\ngraphics.off()\n# load data\nx = read.table(\"journaux.dat\")\na = rowSums(x)\ne = matrix(a) %*% b/sum(a)\n# chi-matrix\ncc = (x - e)/sqrt(e)\n# singular value decomposition\nsv = svd(cc)\ng = sv$u\n# eigenvalues\nll = l * l\n# cumulated percentage of the variance\naux = cumsum(ll)/sum(ll)\nperc = cbind(ll, aux)\nr1 = matrix(l, nrow = nrow(g), ncol = ncol(g), byrow = T) * g\ncar = matrix(matrix(a), nrow = nrow(r), ncol = ncol(r), byrow = F) * r^2/matrix(l^2,\nnrow = nrow(r), ncol = ncol(r), byrow = T) # contribution in r\ncas = matrix(matrix(b), nrow = nrow(s), ncol = ncol(s), byrow = F) * s^2/matrix(l^2,\nnrow = nrow(s), ncol = ncol(s), byrow = T) # contribution in s\nrr = r[, 1:2]\n# labels for journals\ntypes = c(\"va\", \"vb\", \"ff\", \"bj\",\n\"vm\", \"fn\", \"fo\")\n# labels for regions\nregions = c(\"brw\", \"bxl\", \"anv\", \"brf\", \"foc\", \"hai\", \"lig\", \"lux\")\n# plot\nplot(rr, type = \"n\", xlim = c(-1.1, 1.5), ylim = c(-1.1, 0.6), xlab = \"r_1,s_1\",\nylab = \"r_2,s_2\", main = \"Journal Data\", cex.axis = 1.2, cex.lab = 1.2, cex.main = 1.6)\npoints(ss, type = \"n\")\ntext(rr, types, cex = 1.5, col = \"blue\")\ntext(ss, regions, col = \"red\")\nabline(h = 0, v = 0, lwd = 2)\n", "output_sequence": "Performs a correspondence analysis for the Belgian journal data, shows the eigenvalues of the singular value decomposition of the chi-matrix and displays graphically its factorial decomposition."}, {"input_sequence": "# clear all variables\nrm(list = ls(all = TRUE))\ngraphics.off()\n# install and load packages\nlibraries = c(\"MASS\", \"dr\", \"scatterplot3d\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# parameter settings\nset.seed(2010)\nn = 300 # number of observations\nx = cbind(rnorm(n), rnorm(n), # n x 3 matrix, the explanatory variable\ne = rnorm(n) # n vector, the noise variable\nb2 = c(1, -1, # projection vector\ny = x %*% b1 + ((x %*% b1)^3) + 4 * ((x %*% b2)^2) + e # n vector, the response variable\nedr = dr(y ~ x, method = \"sir\", nslices = h) # effective dimension reduction space\nf = edr$evectors # matrix of the estimated EDR-directions\ng = edr$evalues # vector of eigenvalues\n# matrices for the true indices and the true responses\nm1 = cbind(x %*% b1, y)\nm1 = m1[order(m1[, 1]), ]\nsg = sum(g)\ng = g/sg\npsi = c(g[1], g[1] + g[2], g[1] + g[2] + g[3]) # the ratio of the sum of the 3 eigenvalues and the sum of all eigenvalues\npar(mfcol = c(2, 2), mgp = c(2, 1, 0))\n# plot of the response versus the first estimated EDR-direction\np11 = cbind(x %*% f[, 1], y)\nplot(p11, col = \"blue\", xlab = \"first index\", ylab = \"response\", main = \"XBeta1 vs Response\",\ncex.lab = 1.2, cex.main = 1.2, cex.axis = 0.8)\n# plot of the response versus the second estimated EDR-direction\np21 = cbind(x %*% f[, 2], y)\nplot(p21, col = \"blue\", xlab = \"second index\", ylab = \"response\", main = \"XBeta2 vs Response\",\n# three-dimensional plot of the first two directions and the response\np12 = cbind(x %*% f[, 1], x %*% f[, 2], y)\nscatterplot3d(p12, xlab = \"first index\", ylab = \"second index\", zlab = \"response\",\nbox = TRUE, axis = TRUE, color = \"blue\", main = \"XBeta1 Response\", grid = FALSE,\ncex.axis = 0.6)\ni = c(1, 2, 3)\nig = cbind(i, g)\np22 = rbind(ig, psii)\n# plot of the eigenvalues and the cumulative sum\nplot(p22, xlab = \"K\", ylab = \"Psi(k) Eigenvalues\", main = \"Scree Plot\", pch = c(8,\n8, 1, 1), cex.lab = 1.2, cex.main = 1.2, cex.axis = 0.8)\ndev.new()\n# plots of the true response versus the true indices\nplot(m1, xlab = \"first index\", ylab = \"response\", main = \"First true index vs Response\",\ncex.lab = 1.2, cex.axis = 1.2, cex.main = 1.8)\nplot(m2, xlab = \"second index\", ylab = \"response\", main = \"Second true index vs Response\",\n", "output_sequence": "Generates a data set and applies the sliced inverse regression algorithm (SIR) for dimension reduction."}, {"input_sequence": "% ------------------------------------------------------------------------------\n% Book: MVA\n% ------------------------------------------------------------------------------\n% Quantlet: MVAsirdata\n% Description: MVAsirdata generates a data set and applies the sliced inverse\n% regression algorithm (SIR) for dimension reduction.\n% Usage: -\n% Inputs: None\n% Output: Effective dimension reduction directions (EDR-directions) for the\n% simulated data and plots for the response versus the estimated\n% EDR-directions, a three-dimensional plot for the first two\n% directions and the response, plot for the eigenvalues and the\n% cumulative sum and plots for the true response versus the true\n% indices.\n% Example: -\n% Author: Zografia Anastasiadou 20100805;\n% Awdesch Melzer 20120417\n%clear variables and close windows\nclear all;\nclc\n%number of observations\nn = 300;\n%number of elements in each slice\nns = 20;\n%number of slices\nhOld = floor(n/ns);\n%n x 3 matrix, the explanatory variable\nx = normrnd(0,1,n,3);\n%n vector, the noise variable\ne = normrnd(0,1,n,1);\n%projection vectors\nb2 = [1,-1,-1]';\n%n vector, the response variable\ny = x*b1+((x*b1).^3)+4*((x*b2).^2)+e;\nh=-20;\n[n ndim] = size(x); % number of observations and number of dimension\n\n% calculate the covariance matrix and its inverse root to standardize X\n% step 1 in original article\nxb = mean(x); % mean of x\ns = (x'*x - n.*xb'*xb)./(n-1); % cov of x\n[evec,eval] = eigs(s); % eigendecomposititon of cov\nsi2 = evec*diag(sqrt(1./diag(eval)))*evec'; % compute cov(x)^(-1/2)\nxt = (x-repmat(xb,length(x),1))*si2; % stand. x to mean=0, cov=I\n% construct slices in Y space\n% step 2 in original article\ndata = sortrows([y,xt]);\n% build slices\n% ns = number of slices\n% condit = n values controlling which x-data fall\n% in a slice depending on choic\n% choice = vector of ns choices to build the slice subset of x\n% case 1: h<=-2 -> each slice with <= abs(h) elements\n% if n div h != 0 then first and last slice get the remainder\nh = abs(h);\nns = floor(n / h);\ncondit = [1:1:n]'; % enumber the x values\nchoice0 = [1:1:ns]';\nchoice = [choice0.*h]; % take h values in each slice\nif (h*ns ~= n) % if there are remaining values\nhk = floor((n-h*ns)/2);\nif (hk>=0)\nchoice = [hk;choice+hk]; % generate a new first ...\nend\nchoice = [choice;n]; % ... and last slice\nend\nns = length(choice); % number of slices\n% compute width of slices and vector of upper limit of slice intervals\nslwidth = (data(n,1) - data(1,1))/h;\nfirst = data(1,1)-slwidth;\nlast=data(n,1);\nslend = first:slwidth:last;\nif (slend(ns) > data(n-1,1)) % does only the last value\nelse\nslend = [slend,data(n,1)]; % if not, build another slice\ncondit = data(:,1); % choose on original y values\nchoice = slend'; % choice to end of slice\nns = length(choice); % compute the number of slices\n\n% run over all slices, compute estimate for V = Cov(E(x|y))\n% step 3\nhk = 0;\n% initialise V matrix\nind = ones(n,1); % index vector of length n% 0 means,\nj=0;\nsborder=0;\nv=0;\nwhile j<ns\nj=j+1;\nsborder = (condit(:) <= choice(j)); % sborder is index\np = sum(sborder); % if jth obs. belongs\n% current slice\nif p~=0\nind = ind - sborder; % don't take this values\n% in further slices\nxslice = data(find(sborder),2:end); % get sliced x-values\nxmean = mean(xslice);\nv = v + xmean'*xmean*length(xslice); % compute V\nhk = hk+1;\nend\nv1 = (v+v')/2/n; % for numerical errors\n[b,eigen] = eigs (v1); % step 5, eigenvectors of V\nb = si2 * b; % calculate e.d.r. direction\ndata = sqrt(sum(b.^2))'; % ... and standardize them\nedr = b./repmat(data,1,3);\nf=edr;\ng=diag(eigen);\n%matrices for the true indices and the true responses\nm1 = [x*b1,y];\nm1 = sortrows(m1,1);\nsg = sum(g);\ng = g/sg;\n%the ratio of the sum of the 3 eigenvalues and the sum of all eigenvalues\npsi = [g(1),g(1)+g(2),g(1)+g(2)+g(3)]';\nfigure(1)\nsubplot(2,2,1)\n%plot of the response versus the first estimated EDR-direction\np11 = [x*f(:,1),y];\nplot(p11(:,1),p11(:,2),'bo')\nxlabel('first index')\nylabel('response')\ntitle('XBeta1 vs Response')\nbox on\n%plot of the response versus the second estimated EDR-direction\nsubplot(2,2,3)\np21 = [x*f(:,2),y];\nplot(p21(:,1),p21(:,2),'bo')\nxlabel('second index')\ntitle('XBeta2 vs Response')\n%three-dimensional plot of the first two directions and the response\nsubplot(2,2,2)\np12 = [x*f(:,1),x*f(:,2),y];\nscatter3(p12(:,1),p12(:,2),p12(:,3),'ko'),view(-60,80)\nview(-44,13)\nylabel('second index')\nzlabel('response')\ntitle('XBeta1 XBeta2 Response')\ni = [1,2,3]';\nig = [i,g];\npsii = [i,psi];\n%plot of the eigenvalues and the cumulative sum\nsubplot(2,2,4)\nplot(p22(1:3,2),'k*')\nhold on\nplot(p22(4:6,2),'bo')\nxlabel('K')\nylabel('Psi(k) Eigenvalues')\ntitle('Scree Plot')\n%plots of the true response versus the true indices\nfigure(2)\nplot(m1(:,1),m1(:,2),'o')\ntitle('True index vs Response')\nfigure(3)\nplot(m2(:,1),m2(:,2),'o')\n", "output_sequence": "Generates a data set and applies the sliced inverse regression algorithm (SIR) for dimension reduction."}, {"input_sequence": "%% clear variables and close windows\nclear all\nclc\n\n%% set input\ntheta = 3;\nn = 10000;\nU = copularnd('Gumbel',theta,n);\nmu = 0; %mean\ns = 1; %std\nyx = erfinv(2*U-1).*sqrt(2).*s + mu;\n%% plot\nplot(yx(:,1),yx(:,2),'.')\ntitle(['Gumbel Copula, {ittheta} = ',sprintf('%0.2f',theta)])\nxlabel('u')\nxlim([-4,4])\n", "output_sequence": "Produces Gumbel-Hougaard copula sampling for fixed parameters sigma and theta."}, {"input_sequence": "# clear all variables\nrm(list = ls(all = TRUE))\ngraphics.off()\nrAC = function(name, n, d, theta) {\nillegalpar = switch(name, clayton = (theta < 0), gumbel = (theta < 1), frank = (theta <\n0), BB9 = ((theta[1] < 1) | (theta[2] < 0)), GIG = ((theta[2] < 0) | (theta[3] <\n0))))\nif (illegalpar)\nstop(\"Illegal parameter value\")\nindependence = switch(name, clayton = (theta == 0), gumbel = (theta == 1), frank = (theta ==\n0), BB9 = (theta[1] == 1), GIG = FALSE)\nU = runif(n * d)\nU = matrix(U, nrow = n, ncol = d)\nif (independence)\nreturn(U)\nY = switch(name, clayton = rgamma(n, 1/theta), gumbel = rstable(n, 1/theta) *\n(cos(pi/(2 * theta)))^theta, frank = rFrankMix(n, theta), BB9 = rBB9Mix(n,\ntheta), GIG = rGIG(n, theta[1],\nY = matrix(Y, nrow = n, ncol = d)\nphi.inverse = switch(name, clayton = function(t, theta) {\n(1 + t)^(-1/theta)\n}, gumbel = function(t, theta) {\nexp(-t^(1/theta))\n}, frank = function(t, theta) {\n(-1/theta) * log(1 - (1 - exp(-theta)) * exp(-t))\n}, BB9 = function(t, theta) {\nexp(-(theta[2]^theta[1] + t)^(1/theta[1]) + theta[2])\n}, GIG = function(t, theta) {\nlambda = theta[1]\nif (chi == 0) out = (1 + 2 * t/psi)^(-lambda) else if (psi == 0) out = 2^(lambda +\n1) * exp(besselM3(lambda, sqrt(2 * chi * t), logvalue = TRUE) - lambda *\nlog(2 * chi * t)/2)/gamma(-lambda) else out = exp(besselM3(lambda, sqrt(chi *\n(psi + 2 * t)), logvalue = TRUE) + lambda * log(chi * psi)/2 - besselM3(lambda,\nout\n})\nphi.inverse(-log(U)/Y, theta)\n}\nrstable = function(n, alpha, beta = 1) {\nt0 = atan(beta * tan((pi * alpha)/2))/alpha\nTheta = pi * (runif(n) - 0.5)\nW = -log(runif(n))\nterm1 = sin(alpha * (t0 + Theta))/(cos(alpha * t0) * cos(Theta))^(1/alpha)\nterm2 = ((cos(alpha * t0 + (alpha - 1) * Theta))/W)^((1 - alpha)/alpha)\nterm1 * term2\n# special call to rAC for backwards compatibility\nrcopula.gumbel = function(n, theta, d) {\nrAC(\"gumbel\", n, d, theta)\nsample.gc = rcopula.gumbel(10000, theta = 3, d = 2)\n# using qnorm to apply sigma = 1\nsample.metagc = apply(sample.gc, 2, qnorm)\n# plot\nplot(sample.metagc, xlim = c(-4, 4), ylim = c(-4, 4), xlab = \"u\", ylab = \"v\", lwd = 3,\ncex.axis = 2, cex.lab = 2)\ntitle(\"Sample for fixed theta and sigma\")\n", "output_sequence": "Produces Gumbel-Hougaard copula sampling for fixed parameters sigma and theta."}, {"input_sequence": "# clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# install and load packages\nlibraries = c(\"lattice\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# Define x1 and x2 between [-1:1].\nx1 = seq(-1, 1, 0.05)\nn1 = length(x1)\nx2 = seq(-1, 1, 0.05)\nn2 = length(x2)\n# Set beta\nb = c(20, 1, 2, -8, 6)\nL = NULL\n# Calculate y\nfor (i in 1:n1) {\nxi = x1[i]\ntemp = NULL\nfor (j in 1:n2) {\nxj = x2[j]\nLij = b[1] + b[2] * xi + b[3] * xj + b[4] * xi^2 + b[5] * xj^2 + b[6] * xi *\nxj\ntemp = cbind(temp, Lij)\n}\nL = rbind(L, temp)\n}\nwireframe(L, drape = T, xlab = list(\"X2\", rot = 30, cex = 1.2), main = expression(paste(\"3-D response surface\")),\nylab = list(\"X1\", rot = -40, cex = 1.2), zlab = list(\"Y\", cex = 1.1), scales = list(arrows = FALSE,\ncol = \"black\", distance = 1, tick.number = 8, cex = 0.7, x = list(at = seq(1,\n41, 5), labels = round(seq(-1, 1, length = 9), 1)), y = list(at = seq(1,\ndev.new()\ncontour(L, col = rainbow(15), xlab = \"X1\", ylab = \"X2\", main = expression(paste(\"Contour plot\")),\n)\n", "output_sequence": "Plots 3D response surfaces and a contour plot for the variable y and the two factors that explain the variation of y via the quadratic response model."}, {"input_sequence": "%% clear all variables and console and close windows\nclear\nclc\nclose all\n\n%% Define x1 and x2 between [-1:1].\nx1 = (-1:0.05:1)';\n[n1, p1] = size(x1);\nx2 = (-1:0.05:1)';\n[n2, p2] = size(x2);\nb = [20 1 2 -8 -6 6]'; %Set beta\nL = [];\n%% Compute y\nfor i = 1:n1\nxi = x1(i);\ntemp = [];\nfor j = 1:n2\nxj = x2(j);\nLij = b(1) + b(2) * xi + b(3) * xj + b(4) * xi^2 + b(5) * xj^2 + b(6) * xi * xj;\ntemp = [temp Lij];\nend\nL = [L; temp];\nend\n%% Plot mesh grid\nfigure\nmesh(x2, x1, L')\nxlabel ('X_2')\n%% Plot contour lines\ncontour(x1, x2, L', 20)\nxlabel ('X_1')\n%% Plot x1 and x2 as surface\nsurf(x2, x1, L')\n", "output_sequence": "Plots 3D response surfaces and a contour plot for the variable y and the two factors that explain the variation of y via the quadratic response model."}, {"input_sequence": "%% clear all variables\nclear\nclc\nclose all\n\n%% load data\nx = load('bostonh.dat');\ny = x(:,1:5);\nmed = median(y(:,6));\n%% Transformations\nyt(:,1) = log(y(:,1));\n%% plot\nk = 1;\nfor i = 1:length(y)\nif y(i,6)<med\ny1(k,:) = yt(i,:);\nk = k+1;\nelse\ny2(l,:) = yt(i,:);\nl = l+1;\nend\nend\nfor i = 2:6\nfor j = 1:i-1\nsubplot(6,6,(i-1)*6+j)\nplot(y1(:,j),y1(:,i),'ok','MarkerSize',5)\nhold on\nplot(y2(:,j),y2(:,i),'or','MarkerSize',5)\naxis off\nhold off\n% Creating two vectors with the values of the variable 'X1' higher and\n% repectively lower than the median\nlow1(k) = yt(i,1);\nk = k+1;\nhi1(l) = yt(i,1);\nl = l+1;\n% Plotting the boxplots for values<median, respectively\nlow1 = low1';\nhi1 = hi1';\nsubplot(6,6,1)\nhold on\nauxhi1 = ones(length(low1),1).*NaN;\nboxplot([low1 auxhi1],'Symbol','o','Color','k')\nline([0.85 1.15],[mean(low1) mean(low1)],'Color','k','LineStyle',':','LineWidth',1.2)\nauxlow1 = ones(length(hi1),1).*NaN;\nboxplot([auxlow1 hi1],'Symbol','o','Color','r')\nline([1.85 2.15],[mean(hi1) mean(hi1)],'Color','r','LineStyle',':','LineWidth',1.2)\nylim([1.5*min(low1) 2*max(hi1)])\naxis off\n% Creating two vectors with the values of the variable 'X2' higher and\n% repectively lower than the median.\nlow2(k) = yt(i,2);\nl = l+1;\nlow2 = low2';\nhi2 = hi2';\nsubplot(6,6,8)\nauxhi2 = ones(length(low2),1).*NaN;\nboxplot([low2 auxhi2],'Symbol','o','Color','k')\nline([0.85 1.15],[mean(low2) mean(low2)],'Color','k','LineStyle',':','LineWidth',1.2)\nauxlow2 = ones(length(hi2),1).*NaN;\nboxplot([auxlow2 hi2],'Symbol','o','Color','r')\nline([1.85 2.15],[mean(hi2) mean(hi2)],'Color','r','LineStyle',':','LineWidth',1.2)\nylim([min(low2) max(hi2)])\n% Creating two vectors with the values of the variable 'X3' higher and\nlow3(k) = yt(i,3);\nlow3 = low3';\nhi3 = hi3';\nsubplot(6,6,15)\nauxhi3 = ones(length(low3),1).*NaN;\nboxplot([low3 auxhi3],'Symbol','o','Color','k')\nline([0.85 1.15],[mean(low3) mean(low3)],'Color','k','LineStyle',':','LineWidth',1.2)\nauxlow3 = ones(length(hi3),1).*NaN;\nboxplot([auxlow3 hi3],'Symbol','o','Color','r')\nline([1.85 2.15],[mean(hi3) mean(hi3)],'Color','r','LineStyle',':','LineWidth',1.2)\nylim([0.1*min(low3) 1.5*max(hi3)])\n% Creating two vectors with the values of the\n% variable 'X4' higher and repectively lower than the median\nlow4(k) = yt(i,4);\nlow4 = low4';\nhi4 = hi4';\nsubplot(6,6,22)\nauxhi4 = ones(length(low4),1).*NaN;\nboxplot([low4 auxhi4],'Symbol','o','Color','k')\nline([0.85 1.15],[mean(low4) mean(low4)],'Color','k','LineStyle',':','LineWidth',1.2)\nauxlow4 = ones(length(hi4),1).*NaN;\nboxplot([auxlow4 hi4],'Symbol','o','Color','r')\nline([1.85 2.15],[mean(hi4) mean(hi4)],'Color','r','LineStyle',':','LineWidth',1.2)\nylim([min(low4) max(hi4)])\n%Creating two vectors with the values of the\n%variable 'X5' higher and repectively lower than the median\nlow5(k) = yt(i,5);\nk = k+1;\nhi5(l) = yt(i,5);\nlow5 = low5';\nhi5 = hi5';\nsubplot(6,6,29)\nauxhi5 = ones(length(low5),1).*NaN;\nboxplot([low5 auxhi5],'Symbol','o','Color','k')\nline([0.85 1.15],[mean(low5) mean(low5)],'Color','k','LineStyle',':','LineWidth',1.2)\nauxlow5 = ones(length(hi5),1).*NaN;\nboxplot([auxlow5 hi5],'Symbol','o','Color','r')\nline([1.85 2.15],[mean(hi5) mean(hi5)],'Color','r','LineStyle',':','LineWidth',1.2)\nylim([min(low5) 1.2*max(hi5)])\n%variable 'X14' higher and repectively lower than the median\nlow14(k) = yt(i,6);\nk = k+1;\nhi14(l)= yt(i,6);\nl = l+1;\nlow14 = low14';\nhi14 = hi14';\nsubplot(6,6,36)\nauxhi14 = ones(length(low14),1).*NaN;\nboxplot([low14 auxhi14],'Symbol','o','Color','k')\nline([0.85 1.15],[mean(low14) mean(low14)],'Color','k','LineStyle',':','LineWidth',1.2)\nauxlow14 = ones(length(hi14),1).*NaN;\nboxplot([auxlow14 hi14],'Symbol','o','Color','r')\nline([1.85 2.15],[mean(hi14) mean(hi14)],'Color','r','LineStyle',':','LineWidth',1.2)\nylim([min(low14) max(hi14)])\n", "output_sequence": "Plots the scatterplot matrix for the transformed Boston housing data variables X1, ... , X5 and X14."}, {"input_sequence": "# clear all variables\nrm(list = ls(all = TRUE))\ngraphics.off()\n# load data\ndata = read.table(\"bostonh.dat\")\nx = data\n# data transfomation\nK = as.numeric(data[, 14] > median(data[, 14])) + 1\nx = cbind(log(x[, 1]), x[, 2], x[, 3], x[, 4], log(x[, 5]), log(x[, 14]), K)\n# subset creation for subset means\nz = data.frame(x)\nz1 = subset(z, z$K == 1)\nm1 = apply(z1, 2, mean)\ni = 0\nop = par(mfrow = c(6, 6), cex = 0.15)\nwhile (i < 6) {\ni = i + 1\nj = 0\nwhile (j < 6) {\nj = j + 1\n\nif (i == j) {\nboxplot(x[, i] ~ K, at = 1:2, axes = FALSE)\nlines(c(0.6, 1.4), c(m1[i], lty = \"dotted\", lwd = 1.2, col = \"red3\")\n}\nif (i > j) {\nyy = cbind(x[, j], x[, i], K)\nplot(yy[, -3], col = as.numeric(K), xlab = \"X\", ylab = \"Y\", cex = 4,\naxes = FALSE)\nif (i < j) {\nplot(i, type = \"n\", axes = FALSE, xlab = \"\", ylab = \"\", main = \"\")\n}\n}\ntitle(main = list(\"Scatterplot matrix for transformed Boston Housing\", cex = 8),\nline = -16, outer = TRUE)\n", "output_sequence": "Plots the scatterplot matrix for the transformed Boston housing data variables X1, ... , X5 and X14."}, {"input_sequence": "# clear all variables\nrm(list = ls(all = TRUE))\ngraphics.off()\nx = cbind(c(3, 2, 1, 10), c(2, 7, 3, 4))\nd = as.matrix(dist(x))\nd1 = c(1, 2, 3, d[1, 2])\ndelta = cbind(d1, d2, d6)\nf1 = c(1, d[2, 3])\nfig = rbind(f1, f2, f6)\n# plot\nplot(fig, pch = 15, col = \"blue\", xlim = c(0, 7), ylim = c(0, 10), xlab = \"Dissimilarity\",\nylab = \"Distance\", main = \"Dissimilarities and Distances\", cex.axis = 1.2, cex.lab = 1.2,\ncex.main = 1.8)\nlines(fig, lwd = 3)\ntext(fig, labels = c(\"(2,3)\", \"(1,2)\", pos = 4,\ncol = \"red\")\n", "output_sequence": "Illustrates the PAV algorithm for nonmetric MDS for car brands data."}, {"input_sequence": "%% clear all variables and console and close windows\nclear\nclc\nclose all\nx = [[3 2 1 10]; [2 7 3 4]];\nd = dist(x);\ndelta = [1 2 3 d(1,2)\n1 3 2 d(1,3)\nfig = [1 d(2,3)\n3 d(1,2)\n\n%% plot\nscatter(fig(:, 1), fig(:, 2), 'b', 'fill')\nxlim([0 7])\ntitle('Dissimilarities and Distances')\nxlabel('Dissimilarity')\nylabel('Distance')\nfor i=1:5\nline([fig(i, 1) fig(i + 1, 1)], [fig(i, 2) fig(i + 1, 2)],...\n'Color', 'k', 'LineWidth',1.5)\nend\nlabels = {'(2,3)', '(3,4)'};\ntext(fig(:, 1) + 0.2, fig(:, 2), labels, 'Color', 'r')\n", "output_sequence": "Illustrates the PAV algorithm for nonmetric MDS for car brands data."}, {"input_sequence": "# clear all variables\nrm(list = ls(all = TRUE))\ngraphics.off()\n# load data\nx = read.table(\"bostonh.dat\")\nxt = x\n# Transformations\nxt[, 1] = log(x[, 1])\nxt[, 7] = ((x[, 7]^2.5))/10000\nxt[, 8] = log(x[, 8])\nxt[, 11] = exp(0.4 * x[, 11])/1000\nxt[, 13] = sqrt(x[, 13])\nZ = cbind(rep(1, length(xt[, 1])), xt[, 1], xt[, 2], xt[, 3], xt[, 4], xt[, 5], +xt[,\n6], xt[, 7], xt[, 8], xt[, 9], xt[, 10], xt[, 11], xt[, 12], xt[, 13])\ny = xt[, 14]\nmn = dim(Z)\ndf = mn[1] - mn[2]\nb = solve(t(Z) %*% Z) %*% t(Z) %*% y\nyhat = Z %*% b\nr = y - yhat\nmse = t(r) %*% r/df\ncovar = solve(t(Z) %*% Z) %*% diag(rep(mse, 14))\nse = sqrt(diag(covar))\nt = b/se\nt2 = abs(t)\nk = t2^2/(df + t2^2)\np = 0.5 * (1 + sign(t2) * pbeta(k, 0.5, * df))\nPvalues = 2 * (1 - p)\ntablex = cbind(round(b, 4), round(se, 4), round(t, 3), round(Pvalues, 4))\nprint(\"Table with coefficient estimates, Standard error, value of the +\\nt-statistic and p-value (for the intercept (first line) and the 13 +\\nvariables (lines 2 to 14))\")\ntablex\n", "output_sequence": "Builds a linear regression model for the complete transformed Boston housing data."}, {"input_sequence": "%% clear loaded variables and close windows\nclose all\nclc\nclear all\n\n%% load data\nx = load('bostonh.dat');\nxt = x;\n% transformations\nxt(:,1) = log(x(:,1));\nxt(:,7) = (x(:,7).^(2.5))/10000;\nxt(:,8) = log(x(:,8));\nxt(:,11) = exp(0.4*x(:,11))/1000;\nxt(:,13) = sqrt(x(:,13));\nZ = [ones(size(xt(:,1))) xt(:,1) xt(:,13)];\ny = xt(:,14);\n[m n] = size(Z);\ndf = m-n;\nb = inv(Z'*Z)*Z'*y;\nyhat = Z*b;\nr = y - yhat;\nmse = r'*r./df;\ncovar = inv(Z'*Z).*mse;\nse = sqrt(diag(covar));\nt = b./se;\nk = t2.^2./(df + t2.^2);\np = 0.5.*(1+sign(t2).*betainc( k, 0.5, 0.5*df)) ;\nPvalues = 2*(1-p);\nfor i=1:n\ntablex(i,1) = b(i);\nend\n%% output\ndisp('Table with coefficient estimates, Standard error, value of the t-statistic and ')\ndisp('p-value (for the intercept (first line) and the 13 variables (lines 2 to 14))')\ntablex\n", "output_sequence": "Builds a linear regression model for the complete transformed Boston housing data."}, {"input_sequence": "# clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# install and load packages\nlibraries = c(\"MASS\", \"mnormt\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# parameter settings\nn = 200 # number of draws\nmu = c(3, 2) # mean vector\nsig = matrix(c(1, -1.5, 4), ncol = 2) # covariance matrix\n# bivariate normal sample\nset.seed(80)\ny = mvrnorm(n, mu, sig, 2)\n# bivariate normal density\nxgrid = seq(from = (mu[1] - 3 * sqrt(sig[1, 1])), to = (mu[1] + 3 * sqrt(sig[1, 1])),\nlength.out = 200)\nygrid = seq(from = (mu[2] - 3 * sqrt(sig[2, 2])), to = (mu[2] + 3 * sqrt(sig[2, 2])),\nz = outer(xgrid, ygrid, FUN = function(xgrid, ygrid) {\ndmnorm(cbind(xgrid, ygrid), mean = mu, varcov = sig)\n# Plot\npar(mfrow = c(1, 2))\nplot(y, col = \"black\", ylab = \"X2\", xlab = \"X1\", xlim = range(xgrid), ylim = range(ygrid))\ntitle(\"Normal sample\")\n# Contour ellipses\ncontour(xgrid, ygrid, z, xlim = range(xgrid), ylim = range(ygrid), nlevels = 10, col = c(\"blue\",\n\"black\", \"yellow\", \"cyan\", \"red\", \"magenta\", \"green\", \"blue\", \"black\"), lwd = 3,\ncex.axis = 1, xlab = \"X1\", ylab = \"X2\")\ntitle(\"Contour Ellipses\")\n", "output_sequence": "Computes a scatterplot of a normal sample and the contour ellipses for mu =(3,2) and sigma = (1,-1.5)~(-1.5,4)."}, {"input_sequence": "%%clear all variables\nclear all\nclc\n\n%% input parameters\nn = 200;\nmu = [3 2];\nsig = [1 -1.5 ; -1.5 4];\n[vector,value] = eig(sig); %Calculate Eigenvalues and Eigenvectors\nll = sqrt(value);\nsh = vector*ll*vector';\nnr = normrnd(0,1,n,size(sig,1));\ny = nr*sh';\nfor i = 1:length(sig)\ny(:,i) = y(:,i)+mu(i);\nend\n%% plot\nhold on\nsubplot(1,2,1)\nscatter(y(:,1),y(:,2),25,'k')\ntitle('Normal sample')\nxlabel('X1')\n%Contour ellipses\nsubplot(1,2,2)\nymin = min(y);\nyrange = max(y)-min(y);\nng(:,1:2) = 30;\n% Constructing the grid\nd = yrange./(ng-1);\nmin1 = ymin(1,1);\nm1 = min1;\nmin2 = ymin(1,2);\nm2 = min2;\ngrid(1,1) = min1;\nfor i = 1:900\n\nif mod(i-1,30)==0\nmin1 = m1;\nelse\ngrid(i,1) = min1+d(1,1);\nend;\nmin2 = min2+d(1,2);\ngrid(i,2) = min2;\nend;\nscatter(grid(:,1),grid(:,2));\nfor i=1:2\ndiff(:,i) = (grid(:,i)-mu(i));\nff = diff*inv(sig);\nff = ff.*diff;\nfor i = 1:length(ff)\nf1(i) = -(ff(i,1)+ff(i,2));\nf2=exp(f1/2);\nf = f2/sqrt(det(2*pi*sig));\ng = [grid,f'];\n% Plotting the contours\nj = 1;\nxx(j,k) = g(i,2);\nj = j + 1;\nif mod(i,30)==0\nj = 1;\nend\nyy(j,k) = g(i,1);\nif mod(i,30) == 0\nk = k+1;\nzz(j,k) = g(i,3);\n[C,h] = contour(xx,yy,zz,9);\ntitle('Contour Ellipses')\nxlabel('X2')\nview(90,-90)\nhold off\n", "output_sequence": "Computes a scatterplot of a normal sample and the contour ellipses for mu =(3,2) and sigma = (1,-1.5)~(-1.5,4)."}, {"input_sequence": "# clear all variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# specify working directory and load the data\n# setwd(\"...\")\nBIC = read.table(\"BIC\")\ndates = dates[-c(1:5575), ]\ndates = as.matrix(dates)\nlabels = as.numeric(format(as.Date(dates, \"%Y-%m-%d\"), \"%Y\"))\nwhere.put = c(which(diff(labels) == 1) + 1)\n# The plot\npar(mai = (c(0, 0.8, 0.2) + 0.4), mgp = c(3, 0.5, 0))\nplot_colours = c(\"black\", \"red3\", \"blue3\", \"light grey\")\nplot(BIC[, 3], pch = 15, xlab = \"\", ylab = \"\", ylim = c(0, 0.06), axes = FALSE, type = \"l\", col = plot_colours[4])\naxis(4, ylim = c(0, 0.06), col = plot_colours[4], col.axis = \"light grey\", las = 1, cex = 0.8, tck = -0.02)\npar(new = T)\nplot(BIC[, 1], xaxt = \"n\", type = \"l\", xlab = \"\", ylab = \"BIC\", ylim = c(-250, 25), pch = 21, col = plot_colours[1],\nlas = 1, cex = 0.8, tck = -0.02)\n# points where changes in structure\nfor (i in 2: dim(BIC)[1]) {\nif (BIC[i, 2] != BIC[(i - 1), 2]) {\npoints(i, -250, pch = 19, col = \"red\")\n}\n}\naxis(1, at = where.put, labels = labels[where.put], cex = 0.8, tck = -0.02)\nlines(BIC[, 5], type = \"l\", pch = 22, lty = 2, lwd = 2, col = plot_colours[2])\n# add legend\nlegend(\"topleft\", c(\"HAC\", \"Gauss\", \"AC\", expression(\"||X||\"[2])), lty = 1:3, lwd = 2, col = plot_colours[1:4],\ncex = 0.8, ncol = 4)\n", "output_sequence": "Plots the BIC calculated in a rolling window with r = 250 for hierarchical Archimedean copula (HAC), Archimedean copula (AC) and Gauss copula. The HAC and AC are based on Gumbel generators. In addition, the plot includes the dynamics of the parameters of HAC. It includes dots where the structure of HAC changes and the L2 norm of the difference in the matrix of dependence parameters. The data underlying the BIC are residuals from fitting GARCH(1,1) to log returns of DAX, Dow Jones and Nikkei."}, {"input_sequence": "# -*- coding: utf-8 -*-\n\"\"\"\nCreated Jan 13, 2018\n@author: Alex Truesdale\nJupyter Notebook for charting output from transportation location data.\nimport os\nimport openpyxl\nimport pandas as pd\nimport time\nimport gmplot\n# Introduction.\nprint('==================================================================================')\nprint(\"\\n This script reads in a selection of spatial data collected for this project. \\n \\\nThe data selection here contains the lat, lng coordinates of all transit \\n \\\npoints in the city. These points are then plotted on a Google Maps instance \\n \\\nand written to an HTML file, which can be opened in your browser. \\n\")\nprint()\ntime.sleep(7)\nprint(\"------ reading data from 'transit_points.xlsx'\")\ndata_workbook = pd.ExcelFile('transit_points.xlsx')\ndf_sbahn = pd.read_excel(data_workbook, 'sbahn', header = None)\ntime.sleep(1)\nprint(\"------ see data format (first 20 rows of SBahn locations):\", '\\n')\ntime.sleep(3)\nprint(df_sbahn.head(20), '\\n')\ntime.sleep(2)\n# Define lat / lng collections (tuples).\nsbahn_lat_list = tuple(list(df_sbahn[0]))\n# Change dir. to visualisation dir.\nprint(\"------ changing directory to 'runtime_generated/html_renderings' \\n\")\nos.chdir('runtime_generated/html_renderings/')\n# Initialise raw map with api key.\n# key_02 = 'xxxxxxxxxxxxxx'\n# raw_map_all = gmplot.GoogleMapPlotter(52.5112264, 13.415641, 10.81, apikey = key_02)\nprint(\"------ initialising 5 blank maps for plotting\")\nprint(\"------ NOTE: this map will have a 'developer only' banner over it. For \\n \\\nthe maps seen in 'pre_generated/output_maps', one must have \\n \\\na valid Google Cloud project with an API key that is \\n \\\nprovisioned for using the JavaScript Maps API. \\n\")\ntime.sleep(6)\n# Initialise raw maps.\nraw_map_bus = gmplot.GoogleMapPlotter(52.5112264, 13.415641, 10.81)\n# Plot scatter points for bus points.\nprint(\"------ plotting points: bus stops\")\nraw_map_bus.scatter(bus_lat_list, bus_lng_list, '#2E8B57', size = 75, marker = False)\ntime.sleep(.5)\n# Trigger write / compile method to create final map.\nraw_map_bus.draw('transport_bus.html')\nprint(\"------ saving map as: 'transport_bus.html' in 'runtime_generated/html_renderings' \\n\")\n# Plot scatter points for tram points.\nprint(\"------ plotting points: tram stops\")\nraw_map_tram.scatter(tram_lat_list, tram_lng_list, '#8FBC8F', size = 140, marker = False)\nraw_map_tram.draw('transport_tram.html')\nprint(\"------ saving map as: 'transport_tram.html' in 'runtime_generated/html_renderings' \\n\")\n# Plot scatter points for ubahn points.\nprint(\"------ plotting points: ubahn stops\")\nraw_map_ubahn.scatter(ubahn_lat_list, ubahn_lng_list, '#4682B4', size = 190, marker = False)\nraw_map_ubahn.draw('transport_ubahn.html')\nprint(\"------ saving map as: 'transport_ubahn.html' in 'runtime_generated/html_renderings' \\n\")\n# Plot scatter points for sbahn points.\nprint(\"------ plotting points: sbahn stops\")\nraw_map_sbahn.scatter(sbahn_lat_list, sbahn_lng_list, '#FF7F50', size = 190, marker = False)\nraw_map_sbahn.draw('transport_sbahn.html')\nprint(\"------ saving map as: 'transport_sbahn.html' in 'runtime_generated/html_renderings' \\n\")\n# Plot scatter points for all transit types.\nprint(\"------ plotting points: all transit\")\nraw_map_all.scatter(tram_lat_list, tram_lng_list, '#8FBC8F', size = 140, marker = False)\nraw_map_all.draw('transport_all.html')\nprint(\"------ saving map as: 'transport_all.html' in 'runtime_generated/html_renderings' \\n\")\n", "output_sequence": "Example Quantlet for charting spatial data collected for this project. This file plots all transit data for the city of Berlin, DE."}, {"input_sequence": "function r = drchrnd(a,n)\np = length(a);\nr = gamrnd(repmat(a,n,1),1,n,p);\n", "output_sequence": "Plots of Dirichlet distribution."}, {"input_sequence": "a = [1 1 1];\nn = 1000;\nr = drchrnd(a,n);\n\nHD=scatter3(r(:,1),r(:,2),r(:,3),'MarkerFaceColor',[1 0 0]);\ndirection = [1 0 0];\n", "output_sequence": "Plots of Dirichlet distribution."}, {"input_sequence": "# clear cache and close windows\ngraphics.off()\nrm(list=ls(all=TRUE))\n\n# define eight points\neight = cbind(c(-3,-2,-2,-2,1,1,2,4),c(0,4,-1,-2,4,2,-4,-3))\neight = eight[c(8,7,3,1,4,2,6,5),]\ndev.new()\n# plot eight points using average linkage\npar(mfrow = c(1, 2))\nplot(eight,type=\"n\", xlab=\"price conciousness\",ylab=\"brand loyalty\",main=\"8 points\", xlim=c(-4,4))\n# agglomarate first two nearest points\nsegments(eight[5,1],eight[5,2 ],eight[3,1 ],eight[3,2],lwd=2,\"black\")\n# add third point via average\nsegments(eight[3,1],eight[3,2 ],eight[4,1 ],eight[4,2],lwd=2,lty=2,col=\"darkgrey\")\n# agglomerate second two nearest points\nsegments(eight[7,1],eight[7,2 ],eight[8,1 ],eight[8,2],lwd=2,\"black\")\n# agglomarate third two nearest points\nsegments(eight[1,1],eight[1,2 ],eight[2,1 ],eight[2,2],lwd=2,\"black\")\n# add point six to second subcluster via average\nsegments(eight[8,1],eight[8,2 ],eight[6,1 ],eight[6,2],lwd=2,lty=2,col=\"darkgrey\")\n# compute subcluster between 345 and 678 via average\nsegments(eight[4,1],eight[4,2],eight[6,1],eight[6,2],lwd=2,lty=3,col=\"skyblue\")\n# compute in case of merging of two last clusters:\nsegments(eight[2,1],eight[2,2 ],eight[6,1 ],eight[6,2],lwd=2,lty=3,col=\"lightgreen\")\npoints(eight, pch=21, cex=2.6, bg=\"white\")\ntext(eight,as.character(1:8),col=\"red3\", cex=1)\nplot(hclust(dist(eight,method=\"euclidean\")^2,method=\"average\"),ylab=\"squared Euclidean distance\",sub=\"\",xlab=\"\",main=\"average linkage dendrogram\")\n", "output_sequence": "Employs the average linkage using squared Euclidean distance matrices to perform a cluster analysis on an 8 points example"}, {"input_sequence": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.cluster import hierarchy\neight = np.array(([-3, -2, 1, 2, 4], [0, 4, -1, 4, 2, -4, -3])).T\neight = eight[[7,6,2,0,3,1,5,4], :]\nd = np.zeros([8,8])\nfor i in range(0, 8):\nd[i, j] = np.linalg.norm(eight[i, :] - eight[j, :])\ndd = (d**2)\nddd = dd[1:, :-1][:, 0]\nfor i in range(1, 7):\nddd = np.concatenate((ddd, dd[1:, :-1][i:, i]))\nZ = hierarchy.linkage(ddd, 'average')\nfig = plt.figure(figsize = (15, 10))\nfig.add_subplot(1, 2, 1)\nplt.xlim(-4.2, 4.2)\nplt.title(\"8 points\", fontsize = 16)\nplt.ylabel(\"brand loyalty\")\nplt.xlabel(\"price conciousness\")\n# agglomarate first two nearest points\nplt.plot([eight[4, 0], eight[2, 0]], [eight[4, 1], eight[2, 1]], c = \"black\", zorder = 0)\n# add third point via average\nplt.plot([eight[2, 0], eight[3, 0]], [eight[2, 1], eight[3, 1]], c = \"grey\", zorder = 0, linestyle = \"--\")\n# agglomerate second two nearest points\nplt.plot([eight[6, 0], eight[7, 0]], [eight[6, 1], eight[7, 1]], c = \"black\", zorder = 0)\n# agglomarate third two nearest points\nplt.plot([eight[0, 0], eight[1, 0]], [eight[0, 1], eight[1, 1]], c = \"black\", zorder = 0)\n# add point six to second subcluster via average\nplt.plot([eight[7, 0], eight[5, 0]], [eight[7, 1], eight[5, 1]], c = \"grey\", zorder = 0, linestyle = \"--\")\n# compute subcluster between 345 and 678 via average\nfor i in range(2, 5):\nplt.plot([eight[i, 0], eight[j, 0]], [eight[i, 1], eight[j, 1]],\nc = \"lightskyblue\", zorder = 0, linestyle = \"dotted\")\n# compute in case of merging of two last clusters:\nfor i in [0, 1]:\nfor j in range(2, 8):\nc = \"lightgreen\", zorder = 0, linestyle = \"dotted\")\nplt.scatter(eight[:, 0], eight[:, 1], c = \"w\", edgecolors = \"black\", s = 500)\nplt.text(eight[i, 0]-0.1, eight[i, 1]-0.1, str(i), fontsize = 20, color = \"r\")\nfig.add_subplot(1, 2, 2)\nh = hierarchy.dendrogram(Z)\nplt.title(\"Average linkage dendrogram\", fontsize = 16)\nplt.ylabel(\"Squared Euclidean Distance\")\nplt.show()\n", "output_sequence": "Employs the average linkage using squared Euclidean distance matrices to perform a cluster analysis on an 8 points example"}, {"input_sequence": "clear all\nclose all\nclc\n\n% Read data\nload('carc.txt')\nreg = carc(:,13); % region: 1=US, 3=EU, 2=Japan\nx = carc(:,2); % miles per gallon\nx = 2.352*100./x; % litres per 100 km instead miles per gallon\nn = length(x); % number of observations\n% percentage of regions; estimates apriori probabilities\npi1 = sum((reg==1))/n;\npi3 = 1-pi1-pi2;\n% separating data by region\nx1 = x(find(reg==1));\n% means of groups by region\nm1 = mean(x1);\n% common means\nm12 = (m1+m2)/2;\n% common variance\ns = ((length(x1)-1)*var(x1)+(length(x2)-1)*var(x2)+(length(x3)-1)*var(x3))/(length(x)-3);\n\n% allocating regions (1,2,3) to data with the Bayes rule\nalloc1= 1*((x-m12).*inv(s).*(m1-m2)>log(pi2/pi1)).*((x-m13).*inv(s).*(m1-m3)>log(pi3/pi1));\n% sorting rules\nrule12= m12+log(pi2/pi1)/(inv(s)*(m1-m2));\nalloc = alloc1+alloc2+alloc3; % vector with elements (1,2,3)\naper = 1-sum(reg==alloc)/n; % percent of incorrectly sorted data\nmist = (alloc~=reg); % position of sorting mistakes\ny = (reg+0.1*normrnd(0,1,n,1));\n% color, filling\npch = reg;\npch1 = num2str(ones(n,1));\ncolour = ones(3,n);\nfor i=1:n\nif(mist(i)==1)\nif(pch(i) ==1)\ncolour(:,i) = [0, 0.7, 0.5]';\nend\nif(pch(i) ==2)\ncolour(:,i) = [0, 1]';\nif(pch(i) ==3)\ncolour(:,i) = [1, 0, 0]';\nend\nif(mist(i)==0)\nend\nfor i = 1:n\nif(pch(i)==1)\npch1(i) = 's';\nend\nif(pch(i)==2)\npch1(i) = 'o';\nif(pch(i)==3)\npch1(i) = '^';\n\n% plot of sorted data; full symbols - data sorted into the wrong category\nfigure(1)\naxis off\nhold on\nplot(x(i),y(i),pch1(i),'Color',colour(:,i),'MarkerFaceColor',fill(:,i))\n% lines showing the sorting rules\nline([rule12;rule12],[0.8;2.2],'Color','k','LineWidth',2)\ndisp('aper')\n", "output_sequence": "Employs the average linkage using squared Euclidean distance matrices to perform a cluster analysis on an 8 points example"}, {"input_sequence": "graphics.off()\nrm(list=ls(all=TRUE))\n# setwd(\"C:/...\") # change working directory\nload(\"carc.rda\")\nreg = as.numeric(carc[,13])\nx = carc[,2]\nx = 2.352*100/x #litres per 100 km instead miles per gallon\nn = length(x)\n#percentage of regions; estimates apriori probabilities\npi1 = sum(reg==1)/n\npi3 = 1-pi1-pi2\n#separating data by region\nx1 = x[reg==1]\n#means of groups by region\nm1 = mean(x1)\nm3 = mean(x3)\n#common means\nm12 = (m1+m2)/2\n#common variance\ns = ((length(x1)-1)*var(x1)+(length(x2)-1)*var(x2)+(length(x3)-1)*var(x3))/(length(x)-3)\n\n#allocating regions (1,2,3) to data with the Bayes rule\nalloc1 = 1*((x-m12)*solve(s)*(m1-m2)>log(pi2/pi1))*((x-m13)*solve(s)*(m1-m3)>log(pi3/pi1))\n#sorting rules\nrule12 = m12+log(pi2/pi1)/(solve(s)*(m1-m2))\nalloc = alloc1+alloc2+alloc3 #vector with elements (1,2,3)\naper = 1-sum(reg==alloc)/n #percent of incorrectly sorted data\nmist = (alloc!=reg) #position of sorting mistakes\npar(mar=c(0,0,0,0))\n#plot of sorted data; full symbols - data sorted into the wrong category\nplot(x,(reg+0.1*rnorm(n,0,1)),col=(reg+2),pch=(reg-1+mist*15),xlim=c(min(rule12,rule13,rule23),max(x)),axes=FALSE,ann=FALSE)\n#lines showing the sorting rules\nlines(c(rule12,rule12),c(0.8,2.2),lwd=2)\nprint(\"aper\")\n", "output_sequence": "Employs the average linkage using squared Euclidean distance matrices to perform a cluster analysis on an 8 points example"}, {"input_sequence": "\n# clear variables and close windows\nrm(list=ls(all=TRUE))\ngraphics.off()\n\n# load data\nload(\"carc.rda\")\ncarc=carc[,c(\"M\",\"W\",\"D\",\"C\",\"P\")]\nnames(carc)=c(\"Mileage\",\"Weight\",\"Displacement\",\"Origin\",\"Price\")\nattach(carc)\nopar=par(mfrow=c(2,2))\nplot(log(Mileage)~log(Weight))\n# large displacement with small weight:\n#> identify(log(Displacement)~log(Weight))\n#[1] 10 17\n# Mileage Weight Displacement Origin Price\n#Buick_Opel 26 2230 304 US 4453\n# reasonable model...\nsummary(lm1<-lm(log(Mileage)~log(Weight)+log(Displacement)+Origin))\n# model without origin\nlm2<-lm(log(Mileage)~log(Weight)+log(Displacement))\n# test whether origin is significant\nanova(lm1,lm2)\n#summary(lm3o<-lm(Mileage~(Weight+I(Weight^2)+Origin)^2))\n#anova(lm3,lm3o)\ndev.new()\nplot(lm1)\n# the outlier:\ncarc[71,]\n# Mileage Weight Displacement Origin Price\n#VW_Rabbit_Diesel 41 2040 90 Europe 5397\n# other suspicious observations:\n# carc[c(16,46),]\n# Mileage Weight Displacement Origin Price\n#Cad._Seville 21 4290 350 US 15906\nsummary(lm3<-lm(log(Mileage)~log(Weight)+Origin))\n## interaction is not significant\n#summary(lm3.int<-lm(log(Mileage)~(log(Weight)+Origin)^2))\n#anova(lm3,lm3.int)\npar(mfrow=c(1,1))\nplot(log(Mileage)~log(Weight),pch=as.numeric(Origin)-(Origin==\"US\")-2*(Origin==\"Europe\"),col=as.numeric(Origin)+1)\noo=order(carc$Weight)\nc3=coef(lm3)\nabline(c(c3[1],c3[2]),col=2) # US\nabline(c(c3[1]+c3[3],c3[2]),col=3,lty=2) # Japan\npar(opar)\ndetach(carc)\n", "output_sequence": "Employs the average linkage using squared Euclidean distance matrices to perform a cluster analysis on an 8 points example"}, {"input_sequence": "% Clear workspace\nclear all\nclc\n\n% Add path where your files are located\n% addpath C:\\Users\\\n% Load data set\nload carc.txt;\ncardata = carc;\n% ----------------------------------------------------------------------\n% Define variables\nmileage = cardata(:,2);\ndisplacement = cardata(:,11);\n% Descriptive plots I\nfigure(1)\ns1 = subplot(2, 2, 1)\nscatter(log(weight), log(mileage), 'k')\nbox on\nxlabel('log(Weight)')\ns2 = subplot(2, 2, 2)\nscatter(log(displacement), log(mileage), 'k')\nxlabel('log(Displacement)')\ns3 = subplot(2, 2, 3)\nboxplot(log(mileage), origin, 'labels', {'US';'Japan';'Europe'})\nxlabel('Origin')\ns4 = subplot(2, 2, 4)\nscatter(log(weight), log(displacement), 'k')\nylabel('log(Displacement)')\n% print -painters -dpdf -r600 SMSlinregcar01.pdf\n% Plot of regression lines with group-specific means\n[h, a, c, s] = aoctool(log(weight), log(mileage), origin, 0.05, '', 'off', 'parallel lines')\nslope = cell2mat(c(6, 2));\nfigure(2)\nplot(log(weight(find(origin==1))),log(mileage(find(origin==1))),'sr')\nhold on\nplot(log(weight(find(origin==2))),log(mileage(find(origin==2))),'^g')\nus_line = refline(slope, s.intercepts(1));\nset(us_line,'LineStyle','-', 'Color','r')\nlegend('US','Japan','Europe')\nxlabel('log(Weight)','FontSize',16,'FontWeight','Bold')\nxlim([7.4 8.6])\nhold off\n% print -painters -dpdf -r600 SMSlinregcar02.pdf\n", "output_sequence": "Employs the average linkage using squared Euclidean distance matrices to perform a cluster analysis on an 8 points example"}, {"input_sequence": "#---------------------------------------------------------------------\n# Book: SMS\n# ---------------------------------------------------------------------\n# Quantlet: SMScovbank\n# Description: It calculates the covariance matrix (and eigevalues)\n# of the Swiss Bank (bank2.dat) dataset and the variance of\n# the counterfeit bank notes (observations 101-200) after they\n# were lineary transformed by a vector a = (1, 1)'.\n# Usage: -\n# See also: SMSanovapull, SMScovbank, SMSdete2pull,\n# SMSlinregpull, SMSscabank45\n# Keywords: covariance, eigenvalues, spectral decomposition,\n# multivariate, multi dimensional, variance, transformation,\n# decomposition, eigenvalue decomposition, Covariance\n# Inputs: None\n# Output: covariance matrix (and eigevalues) of the transformed\n# counterfeit bank notes from bank2 dataset (bank2.rda)\n# Example:\n#\n# Covariance matrix:\n# round(cov(bank2),4)\n# Length Height Left Height Inner Frame Lower Inner Frame Upper Diagonal\n# Length 0.1418 0.0314 -0.1032 -0.0185 0.0843\n# Inner Frame Lower -0.1032 0.2158 0.2841 2.0869 0.1645 -1.0370\n# Diagonal 0.0843 -0.2093 -0.2405 -1.0370 -0.5496 1.3277\n# Eigenvalues:\n# e\n# 3.00030487 0.93562052 0.24341371 0.19465874 0.08521185 0.03551468\n# Variance of the transformed (summed over rows) counterfeit bank notes\n# v\n# 1.742282\n# Author: Dana Chromikova\n# clear variables and close windows\nrm(list=ls(all=TRUE))\ngraphics.off()\n# setwd(\"C:/...\") # set working directory\nload(\"bank2.rda\") # load data\nV = var(bank2) # variance matrix\nV\ne = eigen(V)$val # eigenvalues of V\ne\na = rep(1,times=6)\nv = var(as.matrix(bank2[101:200,])%*%a) # the variance of the row sums\nv\n", "output_sequence": "calculates the covariance matrix (and eigevalues) of the Swiss Bank (bank2.dat) dataset and the variance of the counterfeit bank notes (observations 101-200) after they were lineary transformed by a vector a = (1, 1, 1, 1, 1, 1)"}, {"input_sequence": "% -------------------------------------------------------------------------\n% Book: SMS\n% -------------------------------------------------------------------------\n% Quantlet: SMScovbank\n% Description: It calculates the covariance matrix (and eigevalues)\n% of the Swiss Bank (bank2.dat) dataset and the variance of\n% the counterfeit bank notes (observations 101-200) after they\n% were lineary transformed by a vector a = (1, 1)'.\n% Input: None.\n% Output: Eigenvalues and covariance matrix for the bank2 dataset.\n% Results:\n%\n% Covariance matrix:\n% V =\n% 0.1418 0.0314 -0.1032 -0.0185 0.0843\n% 0.0231 0.1084 0.1633 0.2841 0.1300 -0.2405\n% 0.0843 -0.2093 -0.2405 -1.0370 -0.5496 1.3277\n% Eigenvalues:\n% e =\n% 3.0003 0\n% Variance of the transformed counterfeit bank notes:\n% v =\n% 1.7423\n% Keywords: covariance, eigenvalues, spectral decomposition,\n% decomposition, eigenvalue decomposition, multivariate,\n% multi dimensional, variance, transformation, Covariance\n% See also: SMSanovapull,\n% SMSscabank45\n% Author: Awdesch Melzer 20131023\nclear all\nclc\nload bank2.dat;\nV = cov(bank2); % variance matrix\nV\n[v, e] = eigs(V); % eigenvalues of V\ne\na = ones(6,1);\nv = var((bank2(101:200,:))*a); % the variance of the row sum\nv\n", "output_sequence": "calculates the covariance matrix (and eigevalues) of the Swiss Bank (bank2.dat) dataset and the variance of the counterfeit bank notes (observations 101-200) after they were lineary transformed by a vector a = (1, 1, 1, 1, 1, 1)"}, {"input_sequence": "rm(list=ls(all=TRUE))\ngraphics.off()\n## subroutine SIR\nsir = function(x, y, h){\n# -----------------------------------------------------------------\n# Quantlet: sir\n# Keywords: sliced inverse regression, dimension reduction\n# Description: Calculates the effective dimension-reduction (edr)\n# directions by Sliced Inverse Regression (Li, 1991)\n# Reference: Li (1991), \"Sliced inverse regression,\n# Journal of the American Statistical Association, 86, 316-342\n# Usage sir (x, y, h)\n# Input\n# Parameter x\n# Definition n x p matrix, the explonatory variable\n# Parameter y\n# Definition n vector, the dependent variable\n# Parameter h\n# Definition scalar, number of slices (h>=2)\n# width of slice (0 < h < 1)\n# Output\n# Parameter edr\n# Definition p x p matrix containing estimates of the edr\n# Parameter eigsv\n# Definition p vector of the eigsvvalues\n# Author: R: Awdesch Melzer 20130205\nn = nrow(x) # number of observations\nndim = ncol(x) # number of dimension\nif(-h > n){\nstop (\"Number of slice elements can't exceed number of data\")\n}\nx = as.matrix(x)\n# calculate the covariance matrix and its inverse root to standardize X\n# step 1 in original article\nxb = apply(x,2,mean) # mean of x\ns = (t(x)%*%x - n*xb%*%t(xb))/(n-1) # cov of x\nE = eigen(s) # eigsvdecomposititon of cov\neval = E$values\n#Adjust second Eigenvector\nif (any(evec[,1]<0)){\nevec=evec*(-1);\n\nsi2 = evec%*%diag(sqrt(1/eval))%*%t(evec) # compute cov(x)^(-1/2)\nxt = (x-xb)%*%si2 # stand. x to mean=0, cov=I\n# construct slices in Y space\n# step 2 in original article\ndata = cbind(y,xt)\ndata = data[order(data[,1]),] # sort data with respect to y\n# build slices\n# ns = number of slices\n# condit = n values controlling which x-data fall\n# in a slice depending on choic\n# choice = vector of ns choices to build the slice subset of x\n# case 1: h<=-2 -> each slice with <= abs(h) elements\n# if n div h != 0 then first and last slice get the remainder\nhold = h\nif (hold <= -2){\nh = abs(hold)\nns = floor(n / h)\ncondit = seq(1, n, 1) # enumber the x values\nchoice = seq(1, ns, 1)*h # take h values in each slice\nif (h*ns != n){ # if there are remaining values\nhk = floor((n-h*ns)/2)\nif (hk>=0){\nchoice = c(hk,choice+hk) # generate a new first ...\n}\nchoice = c(choice,n) # ... and last slice\nns = length(choice) # number of slices\n# case 2: h>=2 -> take h slices with equal width\n}else if(hold >= 2){\nns = hold\nslwidth = (data[n,1] - data[1,1]) / ns # width of slices\n# compute higher value of all ns slices\nslend = seq ((data[1,1]+slwidth), length=ns,by=slwidth)\nslend[ns] = data[n,1] # avoids numerical problems\ncondit = data[,1] # choose on original y values\nchoice = slend # choice to end of slice\n# case 3: 0<h<1 -> take slices with range in percent. of the y-range\n}else if((0<hold) &&(hold<=1)) {\nh = hold\nns = floor(1/h) # number of slices\n# compute width of slices and vector of upper limit of slice intervals\nslwidth = (data[n,1] - data[1,1]) * h\nslend = seq(data[1,1]+slwidth, length=ns, by=slwidth)\nif (slend[ns] > data[n-1,1]){ # does only the last value\n}else{\nslend = c(slend,data[n,1]) # if not, build another slice\ncondit = data[,1] # choose on original y values\nchoice = slend # choice to end of slice\nns = length(choice) # compute the number of slices\nif(h>0||h<=-2){\nstop(\"Error: h must not lay in (-2,0] !!\")\n}\nif(ns==1){\nmessage(\"Warning: Only one slice is generated !\")\n# run over all slices, compute estimate for V = Cov(E(x|y))\n# step 3\nhk = 0\nv = matrix(0,ndim, ndim) # initialise V matrix\nind = matrix(1,n,1) # index vector of length n# 0 means,\n# this values was already choosen\nd = ndim\nndim = ndim+1 # sorted x values are in data[,2:ndim]\nj = 1\nwhile (j <= ns){\nsborder = as.numeric(condit <= choice[j] & ind) # sborder is index\np = sum(sborder) # if jth obs. belongs\nif (p != 0){ # current slice\nind = ind - sborder # don't take this values\n# in further slices\nxslice = data[which(sborder==1),2:ndim] # get sliced x-values\nxmean = apply(xslice,2,mean)\nv = v + xmean%*%t(xmean)*nrow(xslice) # compute V\nhk = hk+1\nj = j+1\nif(sum(ind)!=0){\nmessage(\"Error: sum(ind) elements unused !!\")\nv = (v+t(v))/2/n # for numerical errors\nES = eigen (v) # step 5, eigsvvectors of V\neigsv = ES$values\nb = ES$vectors\n#Adjust second Eigenvector\n#if (any(b[,1]<0)){\n# b=b*(-1);\n#}\nb = si2 %*% b # calculate e.d.r. direction\ndata = sqrt(sum(b^2)) # ... and standardize them\nedr = b/data\nh = cbind(eigsv,t(edr))\nh = h[order(h[,1],decreasing=T),] # sort eigsvvalues\neigsv = h[,1] # and e.d.r. directions\nedr = t(h[,2:ncol(h)])\nreturn(list(edr=edr, eigsv=eigsv) )\n## main calculation\nload(\"uscomp.rda\")\nx = matrix(0,nrow(uscomp),ncol(uscomp))\nfor(i in 1:(ncol(uscomp)-1)){\nx[,i] = as.numeric(as.character(uscomp[,i]))\nx[65,2]=1601\nx[,7] = as.numeric(uscomp[,7])\nm = x\n# get rid of the outliers\nkeep = c(c(1:37),39,c(41:79))\n#keep=1:79\nx = m[keep,c(1,2,4,5,6)]\ny = m[keep,3]\nt = rownames(uscomp)\nn = nrow(x)\nx = log(x-matrix(apply(x,2,min),nrow(x),ncol(x),byrow=T)+(matrix(apply(x,2,max),nrow(x),ncol(x),byrow=T)-matrix(apply(x,2,min),nrow(x),ncol(x),byrow=T))/200)\nEDR = sir(x,y,h=-11) #{f,g}\nf = EDR$edr\nf[,1:3] = t(t(f[,1:3])*sign(f[1,1]))\ng = EDR$eigsv\nsg = sum(g)\ng = g/sg\npsi = cumsum(g) # % explained by eigenvalue\ni = 1:length(g)\nig = cbind(i,g)\np11 = cbind(as.matrix(x)%*%(f[,1]),y)\npar(mfrow=c(2,2))\nplot(p11, type=\"n\", xlab=\"1st projection\", ylab=\"market value\")\npoints(p11,pch=19,cex=0.7,col=\"blue3\")\nplot(p12, type=\"n\", xlab=\"2nd projection\", ylab=\"market value\")\npoints(p12,pch=19,cex=0.7,col=\"blue3\")\nplot(p21, type=\"n\", xlab=\"3rd projection\", ylab=\"market value\")\npoints(p21,pch=19,cex=0.7,col=\"blue3\")\nplot(ig, type=\"n\", xlab=\"k\",ylab=\"\",ylim=c(0,1))\ntitle(\"SIR scree plot\")\npoints(ig, pch=\"*\",col=\"black\",cex=1)\nprint(\"eigenvectors\")\nf\nprint(\"eigenvalues and proportions\")\ncbind(g,cumsum(g))\n", "output_sequence": "calculates the covariance matrix (and eigevalues) of the Swiss Bank (bank2.dat) dataset and the variance of the counterfeit bank notes (observations 101-200) after they were lineary transformed by a vector a = (1, 1, 1, 1, 1, 1)"}, {"input_sequence": "# clear variables and close windows\nrm(list=ls(all=TRUE))\ngraphics.off()\n# install and load packages\nlibraries = c(\"stats\",\"MASS\",\"KernSmooth\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# setwd(\"C:/...\") # set working directory\nload(\"bank2.rda\")\ntruth = factor(rep(c(\"Genuine\",\"Forged\"),each=100))\n# centering\nx = t(t(as.matrix(bank2))-sapply(bank2,mean))\n# sphering\nt = eigen(var(x))\nv12 = t$vectors%*%diag(1/sqrt(t$values))%*%t(t$vectors)\nx = x%*%v12\nfisher = as.matrix(x)%*%(lda(truth~x)$scaling)\nfisher = (fisher-mean(fisher))/sd(fisher)\nprincipal = prcomp(x,scale.=TRUE)$x[,1]\nest.p = bkde(principal, bandwidth=0.25)\ndum = rbind(c(min(c(est.p$x,est.f$x)),-0.2),c(max(c(est.p$x,est.f$x)),max(c(est.p$y,est.f$y))))\nplot(dum,type=\"n\",main=\"Fisher's LDA and PC projection\",xlab=\"\",ylab=\"\")\nlines(est.f,col=\"red\", lwd=2)\nlines(est.p,col=\"blue\", lty=2, lwd=2)\nt = (runif(200)*0.09)-0.095\npoints(t~fisher,col=\"red\",pch=c(rep(1,100),rep(2,100)),cex=1)\nt = (runif(200)*0.09)-0.195\npoints(t~principal,col=\"blue\",pch=c(rep(1,100),rep(2,100)),cex=1)\n", "output_sequence": "Reads the Swiss bank notes data and spheres it to run Fisher's LDA and PC projection on it."}, {"input_sequence": "# clear variables and close windows\nrm(list=ls(all=TRUE))\ngraphics.off()\n# install and load packages\nlibraries = c(\"mvpart\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# Subroutine for perfect sample generation\ntuto = function(seed,n){\nset.seed(seed)\nxdat = cbind(runif(n),runif(n))\nindex = 1+(xdat[,2]<=0.5)+(xdat[,2]<=0.5)*(xdat[,1]<=0.5)+(xdat[,2]>0.5)*(xdat[,1]<=0.5)*(xdat[,2]>0.75)\nlayout = 1*(index==1)+0*(index==2)+4*(index==3)\nydat = index\ny = list(xdat=xdat,ydat=ydat,layout=layout,color=color)\nreturn(y)\n}\n# main calculation\ndata = tuto(1,100)\nx = data$xdat\ncolor = character(length=nrow(x))\ncolor[which(data$color==0)]=\"black\"\npoint = numeric(length=nrow(x))\npoint[which(data$layout==0)]=19\nopar=par(mfrow=c(1,2))\nplot(x, col=color,pch=point,cex=1.1,ylab=expression(x[2]),xlab=expression(x[1]))\ntitle(\"CART example\")\ny = data$ydat\ncontr = rpart.control(minsplit=1, usesurrogate=1, minbucket=1, maxdepth=30)\n# create classification tree\nt2 = rpart(y~x[,1]+x[,2],parms=\"gini\",x=TRUE,y=TRUE,control=contr)\npar(mar=c(2,1,4,2)+0.1)\n# plot classification tree\n# dev.new()\nplot(t2)\ntext(t2,cex=0.75,xpd=NA)\n", "output_sequence": "Reads the Swiss bank notes data and spheres it to run Fisher's LDA and PC projection on it."}, {"input_sequence": "% ---------------------------------------------------------------------\n% Book: SMS(2)\n% ---------------------------------------------------------------------\n% Quantlet: SMSlinregbank2\n% --------------------------------------------------------------------\n% Description: SMSlinregbank2 computes a linear regression for length\n% of diagonal of counterfeit bank notes (bank2.dat).\n% Usage: -\n% Inputs: bank2.dat\n% Output: linear regression for length of diagonal of counterfeit\n% bank notes (bank2.dat).\n% Example: 7.19\n% Results:\n% md1 =\n%\n% Linear regression model:\n% y ~ 1 + x1 + x2 + x3 + x4 + x5\n% Estimated Coefficients:\n% Estimate SE tStat pValue\n% (Intercept) 47.345 34.935 1.3552 0.17859\n% x1 0.3193 0.14831 2.153 0.033879\n% Number of observations: 100, Error degrees of freedom: 94\n% Root Mean Squared Error: 0.471\n% R-squared: 0.322, Adjusted R-Squared 0.286\n% F-statistic vs. constant model: 8.93, p-value = 5.76e-07\n% Keywords: linear model, linear regression, least squares, R-squared\n% See also: SMSlinregbank2, SMSprofil, SMSprofplasma,\n% SMStestcov,\n% SMStestuscomp\n% Author: Philipp Jackmuth\n\n% clear variables and close windows\nclear all\nclc\n% load data\nbank2 = load('bank2.dat');\n% split up dependent vs. independent variables for regression\nX = bank2(101:200,1:5);\n% compute linear regression\nmd1=LinearModel.fit(X,Y)\n", "output_sequence": "Reads the Swiss bank notes data and spheres it to run Fisher's LDA and PC projection on it."}, {"input_sequence": "# ---------------------------------------------------------------------\n# Book: SMS(2)\n# ---------------------------------------------------------------------\n# Quantlet: SMSlinregbank2\n# --------------------------------------------------------------------\n# Description: SMSlinregbank2 computes a linear regression for length\n# of diagonal of counterfeit bank notes (bank2.dat).\n# Usage: -\n# Inputs: bank2.dat\n# Output: linear regression for length of diagonal of counterfeit\n# bank notes (bank2.dat).\n# Example: 7.19\n# Results:\n# model1 =\n#\n# Linear regression model:\n# Call:\n# lm(formula = Diagonal ~ ., data = bank2, subset = 101:200)\n# Residuals:\n# Min 1Q Median 3Q Max\n# -1.16606 -0.28914 0.31843 1.21257\n# Coefficients:\n# Estimate Std. Error t value Pr(>|t|)\n# (Intercept) 47.34541 34.93498 1.355 0.17859\n# Length 0.31930 0.14831 2.153 0.03388 *\n# `Height Left` -0.50683 0.24829 -2.041 0.04403 *\n# `Inner Frame Lower` 0.33250 0.05963 5.576 2.35e-07 ***\n# ---\n# Signif. codes: 0 \u2018***\u2019 0.001 \u2018**\u2019 0.01 \u2018*\u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1\n# Residual standard error: 0.4714 on 94 degrees of freedom\n# Multiple R-squared: 0.322, Adjusted R-squared: 0.2859\n# F-statistic: 8.927 on 5 and 94 DF, p-value: 5.757e-07\n# Keywords: linear model, linear regression, least squares, R-squared\n# See also: SMSlinregbank2, SMSprofil, SMSprofplasma,\n# SMStestcov,\n# SMStestuscomp\n# Author: Zdenek Hlavka\n# clear variables and close windows\nrm(list=ls(all=TRUE))\ngraphics.off()\n# setwd(\"C:/...\") # set working directory\nload(\"bank2.rda\")\nmodel1=lm(Diagonal~.,data=bank2,subset=101:200)\nsummary(model1)\n", "output_sequence": "Reads the Swiss bank notes data and spheres it to run Fisher's LDA and PC projection on it."}, {"input_sequence": "# clear variables and close windows\nrm(list=ls(all=TRUE))\ngraphics.off()\n# install and load packages\nlibraries = c(\"MASS\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# load data\nload(\"bank2.rda\")\n# factor variable\ncounter = rep(c(\"G\",\"F\"),each=100)\n# main calculation\nsol = cmdscale(dist(bank2))\ncorr = cor(cbind(sol,bank2))[3:8,1:2]\ncorr = t(t(corr[,1:2])*sign(corr[1,1]))\n# plot\nopar = par(mfrow=c(1,2))\n# plot of coordinates\nplot(sol,type=\"n\",xlab=expression(x[1]),ylab=expression(x[2]),main=\"metric MDS\")\ntext(sol,counter,col=rep(1:2,each=100))\n# plot of correlations of mds with original variables\nplot(c(-1.1,1.1),c(-1.1,1.1),type=\"n\",main=\"correlations\",xlab=expression(x[1]),ylab=expression(x[2]))\nucircle = cbind(cos((0:360)/180*3.14159),sin((0:360)/180*3.14159))\npoints(ucircle,type=\"l\",lty=\"dotted\")\nabline(h = 0)\ntext(corr,labels=colnames(bank2),col=\"black\",xpd=NA)\npar(opar)\n", "output_sequence": "Reads the Swiss bank notes data and spheres it to run Fisher's LDA and PC projection on it."}, {"input_sequence": "% clear variables and close windows\nclear all\nclc\n% load data\nload('bank2.dat')\ncounter = cellstr(strvcat([repmat('G',100,1);repmat('F',100,1)]));\n% main calculation\nsol = mdscale(dist(bank2'),2);\ncor = corr([sol,bank2]);\ncor = cor(:,1:2).*(-1);\n% plot of coordinates\nfigure(1)\nsubplot(1,2,1)\nplot(sol(:,1),sol(:,2),'wo')\nxlabel('x_1','FontSize',16,'FontWeight','Bold')\ntext(sol(1:100,1), sol(1:100,2),counter(1:100),'Color','k','FontSize',16,'FontWeight','Bold')\nbox on\nset(gca,'LineWidth',1.6,'FontSize',16,'FontWeight','Bold')\nnames = cellstr(strvcat('Length','Height Left','Height Right',...\n'Inner Frame Lower','Inner Frame Upper', 'Diagonal'));\nucircle = [cos((0:360)/180*3.14159)' , sin((0:360)/180*3.14159)'];\n% plot of correlations of mds with original variables\nsubplot(1,2,2)\nplot(ucircle(:,1),ucircle(:,2),'b:','LineWidth',1.6)\ntitle('correlations','FontSize',16,'FontWeight','Bold')\nylim([-1.01,1.01])\nline([0,0]',[-1.5,1.5]','LineWidth',1.6)\ntext(cor(:,1)-0.2,cor(:,2),names,'FontSize',14)\n", "output_sequence": "Reads the Swiss bank notes data and spheres it to run Fisher's LDA and PC projection on it."}, {"input_sequence": "% ----------------------------------------------------------------------\n% Book: SMS\n% ----------------------------------------------------------------------\n% Quantlet: SMSanovapull\n% Description: SMSanovapull performes a linear regression and analysis\n% of variance (ANOVA) for three marketing strategies.\n% A company decides to compare the effect of three marketing\n% strategies:\n% 1. advertisement in local newspaper\n% 2. presence of sales assistant\n% 3. special presentation in shop windows,\n% on the sales of their portfolio in 30 shops.\n% The 30 shops were divided into 3 groups of 10 shops.\n% The sales using the strategies 1, 2, and 3 were\n% y1 = (9, 11, 7, 11, 13)',\n% y3 = (18, 14, 9, 14, 15)', respectively.\n% A null hypothesis of equality of means of the three groups\n% is tested. The standard approach of using ANOVA leads to\n% an F-test. The alternative proposition is to use a\n% linear factor model using the strategies as regression\n% variables to build curve that corresponds to the alternative\n% hypothesis of the ANOVA with three horizontal lines,\n% each for one strategy. The F-test of testing equality of\n% the coefficients to zero corresponds to the null of testing\n% the marketing strategies having no effect.\n% The output shows rejection of the null hypothesis.\n% For more information see Exercise 3.20.\n% Inputs: None.\n% Outputs: Summary statistics for linear regression and ANOVA and boxplot.\n% Result:\n% Regression:\n% b =\n%\n% 8.2000\n% bint =\n% 5.8417 10.5583\n% 1.1583 3.3417\n% r =\n% -1.4500\n% rint =\n% -6.1756 3.2756\n% -5.2067 4.3067\n% -8.0116 1.1116\n% -7.4708 2.0708\n% -2.5033 7.1033\n% -10.0410 -1.3590\n% -1.5556 7.6556\n% -2.6409 6.7409\n% -10.0920 -1.8080\n% -3.6920 5.7920\n% stats =\n% 0.3890 17.8246 0.0002 5.6804\n% p =\n% 0.0012\n% table =\n% 'Source' 'SS' 'df' 'MS' 'F' 'Prob>F'\n% 'Groups' [102.6000] [ 2] [51.3000] [8.7831] [0.0012]\n% 'Error' [157.7000] [27] [ 5.8407] []\n% stats =\n% gnames: {3x1 cell}\n% n: [10 10]\n% source: 'anova1'\n% means: [10.6000 15.1000]\n% df: 27\n% s: 2.4168\n% Keywords: regression, linear model, analysis of variance, factor-model,\n% linear model, variance, F-test, F-statistic, F test, mean,\n% heteroskedasticity, test, Testing\n%-----------------------------------------------------------------------\n% See also: SMScovbank, SMSdete2pull,\n% SMSscabank45\n% Author: Awdesch Melzer 20131023\nclear all\nclc\ny = [9,11,10,12,7,11,12,10,11,13,10,15,11,15,15,13,7,15,13,10,18,14,17,9,14,17,16,14,17,15]';\nx = [1*ones(10,1);2*ones(10,1);3*ones(10,1)]; % factor variable for the strategies\nX = [ones(30,1),x]; % regression variable matrix\n[b,bint,r,rint,stats] = regress(y,X) % regression and summary\n[p,table,stats] = anova1(y,x) % anova table\nset(gca,'LineWidth',1.6,'FontSize',16,'FontWeight','Bold');\n", "output_sequence": "Reads the Swiss bank notes data and spheres it to run Fisher's LDA and PC projection on it."}, {"input_sequence": "#---------------------------------------------------------------------\n# Book: SMS\n# ---------------------------------------------------------------------\n# Quantlet: SMSanovapull\n# Description: SMSanovapull performes a linear regression and analysis\n# of variance (ANOVA) for three marketing strategies.\n# A company decides to compare the effect of three marketing\n# strategies:\n# 1. advertisement in local newspaper\n# 2. presence of sales assistant\n# 3. special presentation in shop windows,\n# on the sales of their portfolio in 30 shops.\n# The 30 shops were divided into 3 groups of 10 shops.\n# The sales using the strategies 1, 2, and 3 were\n# y1 = (9, 11, 7, 11, 13)',\n# y3 = (18, 14, 9, 14, 15)', respectively.\n# A null hypothesis of equality of means of the three groups\n# is tested. The standard approach of using ANOVA leads to\n# an F-test. The alternative proposition is to use a\n# linear factor model using the strategies as regression\n# variables to build curve that corresponds to the alternative\n# hypothesis of the ANOVA with three horizontal lines,\n# each for one strategy. The F-test of testing equality of\n# the coefficients to zero corresponds to the null of testing\n# the marketing strategies having no effect.\n# The output shows rejection of the null hypothesis.\n# For more information see Exercise 3.20.\n# Usage: -\n# See also: SMSanovapull, SMScovbank, SMSdete2pull,\n# SMSlinregpull, SMSscabank45\n# Keywords: anova, analysis of variance, factor-model, linear model,\n# variance, F-test, F-statistic, F test, mean,\n# heteroskedasticity, test, Testing\n# Inputs: None\n# Output: ANOVA table for Exercise 3.20.\n# Example:\n# Analysis of Variance Table\n#\n# anova(lm1)\n# Response: y\n# Df Sum Sq Mean Sq F value Pr(>F)\n# x 2 102.6 51.300 8.7831 0.001153 **\n# Residuals 27 157.7 5.841\n# ---\n# Signif. codes: 0 \u2018***\u2019 0.001 \u2018**\u2019 0.01 \u2018*\u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1\n# summary(lm1)\n# Call:\n# lm(formula = y ~ x)\n# Residuals:\n# Min 1Q Median 3Q Max\n# -6.1 0.4 1.9\n# Coefficients:\n# Estimate Std. Error t value Pr(>|t|)\n# (Intercept) 10.6000 0.7642 13.870 8.44e-14 ***\n# x2 1.8000 1.0808 1.665 0.107392\n# Residual standard error: 2.417 on 27 degrees of freedom\n# Multiple R-squared: 0.3942, Adjusted R-squared: 0.3493\n# F-statistic: 8.783 on 2 and 27 DF, p-value: 0.001153\n# Author: Dana Chromikova\n# clear variables and close windows\nrm(list=ls(all=TRUE))\ngraphics.off()\ny = c(9,11,10,12,7,11,12,10,11,13,10,15,11,15,15,13,7,15,13,10,18,14,17,9,14,17,16,14,17,15)\nx = factor(rep(1:3,each=10)) # factor variable for 3 strategies and 30 observations\nlm1 = lm(y~x) # linear model\nanova(lm1) # anova table\nsummary(lm1) # regression summary\n", "output_sequence": "Reads the Swiss bank notes data and spheres it to run Fisher's LDA and PC projection on it."}, {"input_sequence": "# clear history, close windows\nrm(list=ls(all=TRUE))\ngraphics.off()\n# setwd(\"C:/...\") # set working directory\nload(\"timebudget.rda\")\ntimebudget = as.matrix(timebudget)\nn = nrow(timebudget) # rows of data matrix\none = matrix(1,n,n)\nh = diag(n)-one/n # centering matrix\nxs = h%*%timebudget # centered data\nxs = xs/sqrt(n)\nxs2 = t(xs)%*%xs\nei = eigen(xs2) # spectral decomposition\nlambda = ei$values\ngamma = ei$vectors\nw = -t(t(gamma)*sqrt(lambda)) # coordinates of food\nw = w[,1:2]\nz = -xs%*%gamma # coordinates of families\nz = z[,1:2]\ntau = cumsum(lambda)/sum(lambda)\ntau\n##########\n# different code: output differs from original Xplore\n# transformation\n# x = t((t(as.matrix(timebudget))-mean(timebudget)))/sqrt(nrow(timebudget))\n# singular value decomposition\n# deco = svd(x)\n# w=deco$v[,1:2]%*%diag(sqrt(deco$d[1:2]))\n##########\n# plot\nopar=par(mfrow=c(1,2))\nplot(w,type=\"n\",xlab=expression(w[1]),ylab=expression(w[2]))\ntitle(\"activities\")\ntext(w,colnames(timebudget))\nlines(c(0,0),c(-500,500),lwd=2)\nplot(z,type=\"n\",xlab=expression(z[1]),ylab=expression(z[2]))\ntitle(\"individuals\")\ntext(z,row.names(timebudget))\n", "output_sequence": "Reads the Swiss bank notes data and spheres it to run Fisher's LDA and PC projection on it."}, {"input_sequence": "\n% delete history\nclear all\nclc\n\n% load data\ntime = load('timebudget.dat');\n% labels\ncols = cellstr(strvcat('prof','tran','hous','kids','shop','pers','eat','slee','tele','leis'));\nrows = cellstr(strvcat('maus','waus','wnus','mmus','wmus','msus','wsus','mawe','wawe','wnwe','mmwe','wmwe','mswe','wswe','mayo','wayo','wnyo','mmyo','wmyo','msyo','wsyo','maea','waea','wnea','mmea','wmea','msea','wsea'));\n% retrieve dimensions\n[n p] = size(time); % rows and columns of data matrix\none = ones(n,n);\nh = diag(ones(n,1))-one/n; % centering matrix\nxs = h*time; % centered data\nxs = xs/sqrt(n); % correct for sample size\nxs2 = xs'*xs;\n[xvec xval] = eig(xs2); % spectral decomposition\nxval = diag(xval);\n[xval, ind] = sort(xval,'descend');% index eigenvalues by size\nxvec = xvec(:,ind); % reorder eigenvectors according to eigenvalues\nlambda = xval; % eigenvalues\nw = -(gamma'.*repmat(sqrt(abs(lambda)),1,p))'; % coordinates of activities\nw = w(:,1:2);\nz = -xs*gamma; % coordinates of individuals\nz = z(:,1:2);\ntau = cumsum(lambda)/sum(lambda)% percentage of explained variation\n% visualization\nsubplot(2,1,1)\nplot(w(:,1),w(:,2),'wo')\nxlabel('w_1','FontSize',16,'FontWeight','Bold')\nfor i=1:p\ntext(w(i,1),w(i,2),cols(i),'FontSize',16, 'Color', 'blue')\nend\nline([0,0]',[-300,300]')\naxis([min(w(:,1))-10 max(w(:,1))+10\nset(gca,'FontSize',16,'LineWidth',2,'FontWeight','bold')\nsubplot(2,1,2)\nplot(z(:,1),z(:,2),'wo')\nxlabel('z_1','FontSize',16,'FontWeight','Bold')\nfor i=1:n\ntext(z(i,1),z(i,2),rows(i),'FontSize',16, 'Color', 'blue')\naxis([min(z(:,1))-10 max(z(:,1))+10\n", "output_sequence": "Reads the Swiss bank notes data and spheres it to run Fisher's LDA and PC projection on it."}, {"input_sequence": "% clear cache and close windows\nclear all\nclc\n\n%% example data set containing 8 points\neight = [-3,-2,-2,-2,1,1,2,4;\n0,4,-1,-2,4,2,-4,-3]';\neight = eight([8,7,3,1,4,2,6,5]',:);\n%% cluster analysis employing ward algorithm\nfigure(1)\nplot(eight,'w')\nhold on\nplot(eight(1:2,1),eight(1:2,2),'k','LineWidth',2)\nlab = strvcat(num2str((1:8)'));\nylim([-4.2,4.2])\ntext(eight(:,1),eight(:,2),lab,'Color','r','FontSize',14)\nxlabel('first coordinate','FontSize',16,'FontWeight','Bold')\n%% single linkage for squared Euclidean distance matrix\n% Draw line segments between pairs of points\n% euclidean distance matrix\nd = pdist(eight,'euclidean');\n% squared euclidean distance matrix\ndd = d.^2;\n% cluster analysis with ward algorithm\nss = linkage(dd,'single');\n% Dendrogram for the 8 data points after single linkage\nfigure(2)\nsubplot(1,2,1)\n[H,T] = dendrogram(ss,'colorthreshold','default');\nset(H,'LineWidth',2)\ntitle('Single Linkage Dendrogram - 8 points','FontSize',16,'FontWeight','Bold')\nylabel('Squared Euclidean Distance','FontSize',16,'FontWeight','Bold')\nbox on\n%% single linkage for Euclidean distance matrix\nss = linkage(d,'single');\nsubplot(1,2,2)\n", "output_sequence": "Reads the Swiss bank notes data and spheres it to run Fisher's LDA and PC projection on it."}, {"input_sequence": "# clear cache and close windows\ngraphics.off()\nrm(list=ls(all=TRUE))\n\n# define eight points\neight=cbind(c(-3,-2,-2,-2,1,1,2,4),c(0,4,-1,-2,4,2,-4,-3))\neight=eight[c(8,7,3,1,4,2,6,5),]\n# plot eight points according to single linkage algorithm\npar(mfrow = c(1, 2))\nplot(eight,type=\"n\",xlab=\"price conciousness\",ylab=\"brand loyalty\",xlim=c(-4,4), main=\"8 points\")\nsegments(eight[1,1],eight[1,2 ],eight[2,1 ],eight[2,2],lwd=2)\npoints(eight, pch=21, cex=3, bg=\"white\")\ntext(eight,as.character(1:8),col=\"red3\",xlab=\"first coordinate\", ylab=\"second coordinate\", main=\"8 points\",cex=1.5)\nplot(hclust(dist(eight,method=\"euclidean\")^2,method=\"ward.D\"),ylab=\"squared Euclidean distance\", xlab=\"\",sub=\"\",main=\"Ward dendrogram\")\ndev.new()\npar(mfrow = c(1, 2),mar=c(2, 4, 2) + 0.1)\nplot(hclust(dist(eight,method=\"euclidean\")^2,method=\"single\"),ylab=\"squared Euclidean distance\",main=\"single linkage dendrogram\",xlab=\"\",sub=\"\")\n", "output_sequence": "Reads the Swiss bank notes data and spheres it to run Fisher's LDA and PC projection on it."}, {"input_sequence": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.cluster import hierarchy\neight = np.array(([-3, -2, 1, 2, 4], [0, 4, -1, 4, 2, -4, -3])).T\neight = eight[[7,6,2,0,3,1,5,4], :]\nd = np.zeros([8,8])\nfor i in range(0, 8):\nd[i, j] = np.linalg.norm(eight[i, :] - eight[j, :])\ndd = (d**2)\nddd = dd[1:, :-1][:, 0]\nfor i in range(1, 7):\nddd = np.concatenate((ddd, dd[1:, :-1][i:, i]))\nZ = hierarchy.linkage(ddd, 'ward')\nfig = plt.figure(figsize = (15, 10))\nfig.add_subplot(1, 2, 1)\nplt.xlim(-4.2, 4.2)\nplt.title(\"8 points\", fontsize = 16)\nplt.ylabel(\"brand loyalty\")\nplt.xlabel(\"price conciousness\")\nplt.plot([eight[0, 0], eight[1, 0]], [eight[0, 1], eight[1, 1]], c = \"black\", zorder = 0)\nplt.scatter(eight[:, 0], eight[:, 1], c = \"w\", edgecolors = \"black\", s = 500)\nplt.text(eight[i, 0]-0.1, eight[i, 1]-0.1, str(i), fontsize = 20, color = \"r\")\n\nfig.add_subplot(1, 2, 2)\nh = hierarchy.dendrogram(Z)\nplt.title(\"Ward dendrogram\", fontsize = 16)\nplt.ylabel(\"Squared Euclidean Distance\")\nplt.show()\ndddd = d[1:, :-1][:, 0]\ndddd = np.concatenate((dddd, d[1:, :-1][i:, i]))\nZ1 = hierarchy.linkage(ddd, 'single')\nh = hierarchy.dendrogram(Z1)\nplt.title(\"Single Linkage Dendrogram\", fontsize = 16)\nh = hierarchy.dendrogram(Z2)\nplt.ylabel(\"Euclidean Distance\")\n", "output_sequence": "Reads the Swiss bank notes data and spheres it to run Fisher's LDA and PC projection on it."}, {"input_sequence": "% clear variables and close windows\nclear all\nclc\n% main calculation\ny1 = [1,3,2,5,4,6];\ny2 = [7,8,9,10,12,11];\ny3 = [13,15,14,16,17,18];\nY1 = [y1;y2;y3]; %loading preferences - 3 levels of motors\ny1 = [1,3,2,7,8,9,13,15,14];\ny2 = [5,4,6,10,12,11,16,17,18];\nY2 = [y1;y2]; %loading preferences - 2 levels of safety\ny1 = [1,7,13,5,10,16];\nY3 = [y1;y2;y3]; %loading preferences - 3 levels of doors\nalpha = 0.975; % 95% confidence interval level\nY1\nDoIt(Y1,alpha)\nY2\nDoIt(Y2,alpha)\nY3\n", "output_sequence": "Reads the Swiss bank notes data and spheres it to run Fisher's LDA and PC projection on it."}, {"input_sequence": "# clear variables and close windows\nrm(list=ls(all=TRUE))\ngraphics.off()\n\n# main calculation\ny1 = c(1,3,2,5,4,6)\ny2 = c(7,8,9,10,12,11)\ny3 = c(13,15,14,16,17,18)\nY1 = rbind(y1,y2,y3) #loading preferences - 3 levels of motors\ny1 = c(1,3,2,7,8,9,13,15,14)\ny2 = c(5,4,6,10,12,11,16,17,18)\nY2 = rbind(y1,y2) #loading preferences - 2 levels of safety\ny1 = c(1,7,13,5,10,16)\nY3 = rbind(y1,y2,y3) #loading preferences - 3 levels of doors\nCId = function(y){ #auxiliary functions for 95% confidence interval\nmean(y)-qt(.975,length(y)-1)*sd(y)/sqrt(length(y))\n}\nCIu = function(y){\nmean(y)+qt(.975,length(y)-1)*sd(y)/sqrt(length(y))\nDoIt = function(Y){ #function that solves the problem...\ny = NULL #alternative representation of Y\nfor (i in 1:nrow(Y)){\ny = c(y,Y[i,])\n}\n#factor of the data - argument for levene.test\nFactor = c(rep(1:nrow(Y),each=ncol(Y)))\n#auxiliary terms for lm\nX = kronecker(diag(1,nrow(Y)),c(rep(1,ncol(Y))))\nA = anova(lm(y~X)) #ANOVA\nprint(\"GROUPS DESCRIPTION\") #print table\nprmatrix(round(cbind(apply(Y,1,length),apply(Y,1,mean),apply(Y,1,sd),apply(Y,1,CId),apply(Y,1,CIu)),4),collab=c(\"Count\",\"Mean\",\"St. Dev.\",\"95% Conf. I.\",\"for Mean\"),quote=FALSE)\nprint(\"----------------------------------------------------------------------- \")\nprint(\"ANALYSIS OF VARIANCE\")\nprmatrix(cbind(c(\"Between Groups\",\"Within Groups\"),A$Df,round(A$\"Sum Sq\",4),round(A$\"F value\",4),round(A$\"Pr(>F)\",4)),collab=c(\"Source of Variance\",\"Df.\",\"Sum of Sq.\",\"F value\",\"Sign.\"),right=TRUE,quote=FALSE,na.print=\" \")\n#print(\"LEVENE TEST FOR HOMOGENITY OF VARIANCES\")\n#prmatrix(round(cbind(as.matrix(L$statistic)[1],L$p.value),4),collab=c(\"Statistic\",\"Sign.\"),quote=FALSE)\nY1\nDoIt(Y1)\nY2\nDoIt(Y2)\nY3\nDoIt(Y3)\n", "output_sequence": "Reads the Swiss bank notes data and spheres it to run Fisher's LDA and PC projection on it."}, {"input_sequence": "rm(list=ls(all=TRUE))\ngraphics.off()\n# install and load packages\nlibraries = c(\"dr\",\"MASS\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# set seed for pseudo random numbers\nset.seed(0)\nn = 400 #number of observations\n# vectors of normal random numbers\nx1 = rnorm(n)\nx = cbind(x1,x2,x3,x4)\n# error term\ne = rnorm(n)\nb1 = c(1,3,0,0)\ny = ((x%*%b1)^2)+((x%*%b2)^4)+e\n\nEDR = dr(y~x,method=\"sir\")\nf = EDR$evectors\n#f1 = ((x%*%b1)^2)+((x%*%b2)^4)\n#m1 = cbind((x%*%b1),y)\n#m1 = m1[order(m1[,2],decreasing=T),]\nsg = sum(g)\ng = g/sg\npsi = cumsum(g)\nig = cbind(i,g)\np11 = cbind(as.matrix(x)%*%f[,1],y)\npar(mfrow=c(2,2))\nplot(p11, type=\"n\", xlab=\"1st projection\", ylab=\"response\")\npoints(p11,pch=19,cex=0.7,col=\"blue3\")\nplot(p12, type=\"n\", xlab=\"2nd projection\", ylab=\"response\")\npoints(p12,pch=19,cex=0.7,col=\"blue3\")\nplot(p21, type=\"n\", xlab=\"3rd projection\", ylab=\"response\")\npoints(p21,pch=19,cex=0.7,col=\"blue3\")\nplot(ig, type=\"n\", xlab=\"k\",ylab=\"\",ylim=c(0,1))\ntitle(\"SIR scree plot\")\npoints(ig, pch=\"*\",col=\"black\",cex=1)\n", "output_sequence": "Reads the Swiss bank notes data and spheres it to run Fisher's LDA and PC projection on it."}, {"input_sequence": "% clear memory and close windows\nclear all\nclc\n\ndata = load('uscrime.dat'); % load data\nn = size(data,1); % number of observations\n% labels for crimes\ncrime = strvcat('murder', 'rape', 'robbery', 'assault', 'burglary', 'larceny', 'autotheft');\ncrime = cellstr(crime);\n% labels for regions\nstate = strvcat('ME','NH','VT','MA','RI','CT','NY','NJ','PA','OH','IN','IL','MI','WI','MN','IA','MO','ND','SD','NE','KS','DE','MD','VA','VW','NC','SC','GA','FL','KY','TN','AL','MS','AR','LA','OK','TX','MT','ID','WY','CO','NM','AZ','UT','NV','WA','OR','CA','AK','HI');\nstate = strcat(state,num2str(data(:,10)));\nstate = cellstr(state);\nx = data(:,3:9);\nx = x-repmat(mean(x),size(x,1),1); % centering\nx = x*diag(1./(std(x)));\n%% ward method for squared Euclidean distance matrix\n% Draw line segments between pairs of points\nd = pdist(x,'euclidean'); % L1 distance\nss = linkage(d,'ward'); % cluster analysis with ward algorithm\n% Dendrogram for the data points after ward linkage\nfigure(1)\n[H,T] = dendrogram(ss,n,'colorthreshold',15,'Orientation','left','Labels',state);\nset(H,'LineWidth',2)\ntitle('Ward dendrogram for US crime','FontSize',16,'FontWeight','Bold')\nbox on\nset(gca,'LineWidth',1.6,'FontSize',8,'FontWeight','Bold')\n%% pca\n[ve, va] = eig(cov(x)); % eigensystem analysis\nva = diag(va); % retrieve from diagonal\n[va, ind] = sort(va,'descend'); % sort in decresing order, index\nve = ve(:,ind); % sorting vectors according to index\nve(:,[3,4])=\ny = x*ve; % PCA transformation\ny = y(:,1:2);\nclus = 4;\ngpoints = cluster(ss,'maxclust',clus);\ngmean = zeros(clus,size(x,2));\nfor i=1:clus\ng = find(gpoints==i);\ngmean(i,:) = mean(x(g,:));\nend\n% Colors for Sectors\ncolour = ones(3,n);\nfor i=1:n\nif(gpoints(i)==1)\ncolour(:,i) = [1, 0.7, 0.3]';\nend\nif(gpoints(i)==2)\ncolour(:,i) = [0.4, 0, 1]';\nif(gpoints(i)==3)\ncolour(:,i) = [0, 0.7, 0.5]';\nif(gpoints(i)==4)\ncolour(:,i) = [1, 0, 0.4]';\nfigure(2)\nplot(y2(:,1),y2(:,2),'w')\ntitle('US crimes: four clusters','FontSize',16,'FontWeight','Bold')\nxlabel('PC1','FontSize',16,'FontWeight','Bold')\nhold on\nfor i=1:n\ntext(y2(i,1)+0.15,y2(i,2),state(i),'Color',colour(:,i),'FontSize',14)\nend\nbox on\nset(gca,'LineWidth',1.6,'FontSize',16,'FontWeight','Bold')\n%% table of means in four clusters\ndisp('Cluster means for each variable (columns) and each cluster (rows)')\ndisp(crime')\n", "output_sequence": "Reads the Swiss bank notes data and spheres it to run Fisher's LDA and PC projection on it."}, {"input_sequence": "# clear history and close windows\ngraphics.off()\nrm(list=ls(all=TRUE))\n\n# setwd(\"C:/...\") #set your working directory, create there a subdirectory 'data' for the datasets\n\nload(\"uscrime.rda\")\nop = par(mfrow=c(1,2))\nlab = paste(row.names(uscrime),as.numeric(uscrime$reg))\nrow.names(uscrime)=lab\nx = uscrime[,3:9]\nx = t(t(x)-apply(x,2,mean))\nx = as.matrix(x)%*%diag(1/(apply(x,2,sd)))\nhc = hclust(dist(x),\"ward.D\")\ncl = cutree(hc,4)\nnames(cl)\nopar = par(mar=c(2, 5, 4, 2) + 0.1)\nplot(hc,labels=lab,main=\"Ward dendrogram for US crime\",xlab=\"\",sub=\"\",cex=0.7,ylab=\"Euclidean distance\")\npr = prcomp(x)\nplot(pr$x[,1:2],type=\"n\",main=\"US crime\")\ntext(pr$x[,1:2],lab,col=as.numeric(cl)+1)\n#par(op)\n## Cluster 1\nsubstr(lab[cl==1],1,5)\n# Cluster 2\nsubstr(lab[cl==2],1,5)\n# cluster 3\nsubstr(lab[cl==3],1,5)\n# cluster 4\nsubstr(lab[cl==4],1,5)\n## x is dataframe\nx = data.frame(x)\ncolnames(x)=colnames(uscrime)[3:9]\n# clusters and regions\ntable(uscrime$reg,cl)\n## table of means in four clusters\nsapply(x,tapply,cl,mean)\n", "output_sequence": "Reads the Swiss bank notes data and spheres it to run Fisher's LDA and PC projection on it."}, {"input_sequence": "# clear variables and close windows\nrm(list=ls(all=TRUE))\ngraphics.off()\n# install and load packages\nlibraries = c(\"MASS\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# variables\ndesc = c(\"Nissan\",\"Kia\",\"BMW\",\"Audi\")\n#D = cbind(c(0,2,4,3),c(2,0,6,5),c(4,6,0,1),c(3,5,1,0))\nD\n# main calculation\nb = isoMDS(D, k = 2, maxit = 1000,tol=1e-10)\n# 2D map\nplot(b$points,xlab=expression(x[1]),ylab=expression(x[2]),xlim=c(-4,4),ylim=c(-4,4),main=\"nonmetric MDS\",type=\"n\")\ntext(b$points,desc,col=\"blue\")\n", "output_sequence": "Reads the Swiss bank notes data and spheres it to run Fisher's LDA and PC projection on it."}, {"input_sequence": "% clear variables and close windows\nclear all\nclc\n% variables\ndesc = cellstr(strvcat('Nissan','Kia','BMW','Audi'));\n%D = cbind(c(0,2,4,3),c(2,0,6,5),c(4,6,0,1),c(3,5,1,0))\nD = [[0,2,5,3]',[2,0,6,4]',[5,6,0,1]',[3,4,1,0]'];\nD\n% main calculation\n[Y,stress,disparities] = mdscale(D,2,'criterion','stress')\n% 2D map\nplot(Y(:,1),Y(:,2),'wo')\nxlabel('x_1','FontSize',16,'FontWeight','Bold')\nxlim([-4,4])\ntitle('nonmetric MDS','FontSize',16,'FontWeight','Bold')\ntext(Y(:,1),Y(:,2),desc,'color','b','FontSize',16,'FontWeight','Bold')\nbox on\nset(gca,'LineWidth',1.6,'FontSize',16,'FontWeight','Bold')\n", "output_sequence": "Reads the Swiss bank notes data and spheres it to run Fisher's LDA and PC projection on it."}, {"input_sequence": "rm(list=ls(all=TRUE))\ngraphics.off()\n\n# install and load packages\nlibraries = c(\"MASS\", \"hexbin\", \"KernSmooth\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\nset.seed(100);\nn = 300;\nsigma = diag(x = 1, nrow = 2, ncol = 2);\nxx = mvrnorm(n, mu, sigma);\nnd = 15 # number of grid points in each dimension\nnd = matrix(1,1,nrow = dim(xx)[2])*nd # matrix 2x1 with numbers of grid points in each dimension\nd = c(apply(t(xx), 1, max) - apply(t(xx), 1, min))/(nd-1)\nh = c(2.6073*sqrt(var(xx[,1]))*n^(-1/6),2.6073*sqrt(var(xx[,2]))*n^(-1/6)) # bandwidth a la Scott's rule of thumb\nminmaxx = c(min(xx[,1]),\nest = bkde2D(xx, bandwidth=h/2, gridsize=nd, truncate=TRUE, range.x=list(minmaxx, minmaxy)) # estimates 2dimensional density\ncontour(est$x1, est$x2, est$fhat, col=\"blue\", frame.plot=TRUE,main=\"2D kernel estimate\")\npoints(xx,pch=20)\ndev.new()\n", "output_sequence": "Reads the Swiss bank notes data and spheres it to run Fisher's LDA and PC projection on it."}, {"input_sequence": "%Series is ozone series from Box and Tiao (1973)\nclear\n\nbtoz = load(fullfile('data', 'btozone.dat'));\n[nb, junk] = size(btoz);\nnyb = nb - 12;\nbg_year = 1955;\nfreq = 12;\nyor = btoz(1:nyb, 1);\nct = deltafil(Ya(:, 2:3), 0, 1, 0, freq);\nY = [Ya(:, 1), ct];\nx = [];\nrnamesrg = [];\nfor i = 1:3\nrnamesrg = char(rnamesrg, ['int', num2str(i)]);\nend\nyor = log(yor);\n%1) first model\np = 1;\n%estimate model using HR method\nest = 1;\n[yd, beta] = diffest(yor, Y, freq, 0, d, ds, 0, est); %differenced series\nydc = yd(:, 1) - yd(:, 2:4) * beta; %differenced corrected series\n%estimate model using the differenced corrected series\n[strv, ferror] = estvarmaxpqrPQR(ydc, x, freq, [p, q, 0], [ps, qs, 0], 1, 1);\n% define model. is (1,0,0)(0,1,1)_12\nphi1(:, :, 1) = 1;\n%no mean in the model\n%setup model\nphi1(:, :, 2) = strv.phis3(:, :, 2);\nSigma1 = strv.sigmar3;\n%create structure and put model into innovations state space form\n[str, ferror] = suvarmapqPQ(phi1, th1, Phi1, Th1, Sigma1, freq);\nZ = str.Z;\nnalpha = size(T, 1);\n% ii=[0 0 nalpha]; ins=eye(nalpha);\nndelta = freq * ds + d; %number of unit roots\n[ins, ii, ferror] = incossm(T, H, ndelta);\nchb = 1;\n%computation of the profile likelihood\nX = Y;\n% [e,f,hb,Mb,A,P,qyy,R,olsres]=scakfle2prof(yor,X,Z,G,W,T,H,ins,ii,chb);\n[e2, f2, hb2, A2, qyy2, R2] = scakflepc(yor, X, Z, G, W, T, H, ins, ii, chb);\nFf = (e2' * e2) * f2^2;\n%number of parameters\nnr = 2;\ndn = double(nyb); %information criteria\ndnp = double(nr+nreg+nalpha);\naic1 = dn * (log(2*pi) + log(Ff)) + 2 * dnp;\n%2) second model\np = 0;\n% define model. is (0,1,1)(0,1,1)_12\nphi2(:, :, 1) = 1;\nth2(:, :, 2) = strv.thetas3(:, :, 2);\nSigma2 = strv.sigmar3;\n%create structure and put model into innovartions state space form\n[str, ferror] = suvarmapqPQ(phi2, th2, Phi2, Th2, Sigma2, freq);\naic2 = dn * (log(2*pi) + log(Ff)) + 2 * dnp;\ndisp('information criteria')\nbic1\n", "output_sequence": "An ARIMA model for the ozone series of Box and Tiao (1975) is identified using the BIC and AIC criteria, both computed by means of the profile likelihood."}, {"input_sequence": "%\n%script file to illustrate two covariance factorization methods. The first\n%one is Tunnicliffe-Wilson method and the second one uses the DARE.\n%\nclear\n\nphi(:, :, 1) = eye(2);\nth(:, :, 2) = [6.5, -2.; 15, -4.5];\nSigma = [4., 1.;\nnc = 2;\n[c, ierror] = macgf(phi, th, Sigma, nc);\ndisp('Autocovariance matrices of lags 0 to 1:')\nfor i = 1:2\ndisp(c(:, :, i))\nend\ndisp('covariance factorization using Tunnicliffe-Wilson method')\n[Omega, Theta, ierror, iter, normdif] = pmspectfac(c, 50)\ndisp('press any key to continue')\npause\ndisp('compute autocovariances of the invertible model')\nphin(:, :, 1) = eye(2);\nSigman = Omega;\n[cn, ierror] = macgf(phin, thn, Sigman, nc);\ndisp(cn(:, :, i))\ndisp('covariance factorization using the DARE')\n[Thetad, Omegad] = ssmspectfac(c)\n", "output_sequence": "The autocovariances of a vector moving averrage process of order one are first computed and then the spectral factorization of these covariances is performed using two methods."}, {"input_sequence": "%script file for paragraph 5.11 in Tsay (2014)\n%\nclear\ndata = load(fullfile('data', 'q-4macro.dat'));\nda = (data(:, 4:7));\nzt = [log(da(:, 1)), da(:, 2), log(da(:, 4)), da(:, 3)];\n[nx, mx] = size(zt)\ntdx = [1:nx] / 4 + 1959; %time index\nsubplot(2, 2, 1)\nplot(tdx, zt(:, 1))\nxlabel('time');\nylabel('ln(gnp)');\naxis('tight');\nsubplot(2, 2, 2)\nplot(tdx, zt(:, 3))\nylabel('lnm1');\nsubplot(2, 2, 3)\nplot(tdx, zt(:, 2))\nylabel('tb3m');\nsubplot(2, 2, 4)\nplot(tdx, zt(:, 4))\nylabel('gs10');\ndisp('press any key to continue')\npause\nclose all\ndisp(' ')\ndisp('identify a VAR model for the series')\nmaxlag = 13;\nprt = 1;\nlagsopt = varident(zt, maxlag, prt);\n% %Matlab-Econ function for Johansen cointegration test\n% lags=1:2;\n% [h,pValue,stat,cValue,mles] = jcitest(y,'lags',lags);\n[D, nr, yd, DA, ferror] = mcrcregr(zt);\ndisp('number of unit roots according to the crc criterion:')\ndisp(nr)\ndisp('number of unit roots according to Tsay (2014): 2')\ndisp('this will be used in the following')\nx = [];\nseas = 1;\n%number of unit roots in the model\nnr = 2;\n%estimate ''differencing polynomial'' D and obtain differenced series yd\n%note that betaor is parametrized in DA\n[D, DA, yd, ferror] = mdfestim1r(zt, x, prt, nr);\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n% Identification and estimation of a VAR model for the differenced series\n% parameterized in error correction form\ndisp('identify a VAR model for the differenced series')\nmaxlag = 8;\nlagsopt = varident(yd, maxlag, prt);\ndisp('estimation of a VAR(5) model for the differenced series')\ndisp('using the Hannan-Rissanen method')\ndisp('')\n%estimation of the model for the differenced series\n%model for the differenced series: VAR(5).\nhr3 = 0;\nfinv2 = 1;\nmstainv = 1;\nnsig = [1, 1];\npr = 5;\nqr = 0; %VAR(5)\n[str, ferror] = estvarmaxpqrPQR(yd, x, seas, [pr, qr, 0], [0, 0], hr3, finv2, mstainv, nsig, tsig);\nphi(:, :, 1) = str.phis3(:, :, 1);\nfor i = 1:pr\nphi(:, :, i+1) = str.phis3(:, :, i+1);\nend\nPhi(:, :, 1) = phi(:, :, 1);\nth(:, :, 1) = str.thetas3(:, :, 1);\nfor i = 1:qr\nth(:, :, i+1) = str.thetas3(:, :, i+1);\nTh(:, :, 1) = phi(:, :, 1);\nSigma = str.sigmar3;\nclear str\nphit = pmatmul(phi, Phi);\n[Pi, Lambda, alpha, betap, ferror] = mid2mecf(phit, D, DA);\n%set up varma model in error correction form. Parameters are defined in\n%terms of the error correction form.\n[str, ferror] = suvarmapqPQe(Lambda, alpha, betap, th, Sigma, seas);\ndisp('estimation of the VAR(6) model in error correction form')\nY = [];\nconstant = 1;\nresult = varmapqPQestime(zt, str, Y, constant);\ndisp(' ');\ndisp('******************** Results from estimation ********************');\nmprintr(result)\n%create estimated model\nxvrf = result.xvf;\n[ydf, xvv, xff, DAf, Dr, ferror] = pr2varmapqPQd(zt, xvrf, str);\n[Lambdaf, alphaf, betapf, thf, Lf, ferror] = pr2ecf(xvv, xff, DAf, str);\n[phif, De, ferror] = mecf2mid(Lambdaf, alphaf, betapf);\nphit = pmatmul(phif, Dr);\ndisp('***** Estimated overall AR part *****');\nclear in\nin.fid = 1;\nin.fmt = char('%12.4f');\ntit = 'AR';\nstrt = 1;\nmprintar(phit(:, :, 2:end-1), in, tit, strt);\nin.fmt = char('%12.7f');\ntit = 'Sigma:';\nmprintar(result.Sigmar, in, tit);\ntit = 'Constant:';\nmprintar((sum(phif, 3) * result.h)', in, tit);\n%error correction matrices\ndisp('***** AR part in error correction form *****');\ntit = 'Lambda';\nmprintar(Lambdaf(:, :, 2:end), in, tit, strt);\n% reparameterize betap and alpha\n[nb, mb] = size(betapf);\nalphaf = alphaf * betapf(:, 1:nb);\nbetapf = betapf(:, 1:nb) \\ betapf;\ndisp('estimated beta transposed')\ndisp(betapf)\ndisp('estimated alpha')\ndisp(alphaf)\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n% Estimation of the VAR(6) model parameterized in terms of the model for\n% the differenced series\n%set up varma model for the differenced series. Parameters are defined in\n%terms of the model for the differenced series, contained in phi, th, Phi,\n%Th and Sigma, and the parameters of the differencing matrix polynomial,\n%contained in DA.\n[str, ferror] = suvarmapqPQ(phi, th, Phi, Th, Sigma, seas);\n%eliminate insignificant parameters\nfor k = 2:pr + 1\nfor i = 1:mx\nif phi(i, j, k) == 0\nend\nend\n[str, ferror] = fixvarmapqPQ(str);\n%add unit root information to the model structure\n[str, ferror] = aurirvarmapqPQ(str, nr, DA);\ndisp('estimation of the VAR(5) model for the differenced series. ')\n%estimate model with cointegration relations imposed\nresult = varmapqPQestimd(zt, str, Y, constant);\n[phif, thf, Phif, Thf, Lf, ferror] = pr2varmapqPQ(xvv, xff, str);\n%estimated VAR model with cointegration rank imposed\n% in.cnames = char(' AR(1):',' ',' AR(2):',' ',' AR(3):',' ',' ',...\n% ' AR(4):',' ',' AR(5):',' ',' AR(6):',' ',' ');\n% mprint(phit(:,:,2:end),in);\nmprintar(phit(:, :, 2:7), in, tit, strt);\n% in.cnames = char(' Sigma:',' ',' Constant:');\n% mprint([result.Sigmar sum(phif,3)*result.h],in);\n[Pi, Lambda, alpha, betafp, ferror] = mid2mecf(phif, Dr, DAf);\n% in.cnames = char(' Lambda(1):',' ',' Lambda(2):',' ',' ',...\n% ' Lambda(3):',' ',' Lambda(4):',' ',' ',...\n% mprint(Lambda(:,:,2:6),in);\nmprintar(Lambdaf(:, :, 2:6), in, tit, strt);\n%reparameterize betap and alpha\n[nb, mb] = size(betafp);\nalpha = alpha * betafp(:, 1:nb);\nbetafp = betafp(:, 1:nb) \\ betafp;\ndisp(betafp)\ndisp(alpha)\ndisp('******** Computation of OLS Residuals ********');\n[str, ferror] = suvarmapqPQ(phif, thf, Phif, Thf, result.Sigmar, seas);\ns = mx;\nkro = pr * ones(1, s);\nm = 0;\n[strc, ferror] = matechelon(kro, s, m);\nthh(:, :, pr+1) = zeros(s);\nstrc.phis = phif;\nstrc.gammas = [];\nstrc = armaxe2sse(strc);\nstrc.sigmar2 = str.Sigma;\nY = eye(s);\ntol = 1.d-10;\nmaxupdt = [];\n[e, E, rSigmat] = sqrt_ckms(ydf, Y, strc, maxupdt, tol);\n[ne, me] = size(e);\nrecr = zeros(ne, me);\nnbeta = s;\nfor ii = 1:ne\nind = (ii - 1) * nbeta + 1:ii * nbeta; % V=rSigmat(ind,:);\nrecr(ii, :) = e(ii, :) - (E(ind, :)' * result.h)';\n% %compute recursive residuals\n% %set up regression matrices\n% X=eye(s); W=[];\n% Sigmax=strc.sigmar2;\n% [L,p] = chol(Sigmax,'lower'); Lm1=pinv(L);\n% %set up system matrices\n% T=strc.Fs; G=Lm1; H=strc.Ks*Lm1;\n% %set up initial conditions\n% ndelta=0; %number of unit roots\n% [ins,i,ferror]=incossm(T,H,ndelta);\n% [Xt,Pt,g,M,initf,recrs,recr]=scakffsqrt(ydf,X,Z,G,W,T,H,ins,i);\n%plot recursive residuals\nplot(recr(:, 1)), legend('recr(:,1)'), pause\n%compute autocovariance and autocorrelation matrices of ols residuals\nlag = 24;\nic = 1;\ndisp('******** OLS Residuals: ********');\nstre = mautcov(recr, lag, ic, nr);\ndisp('Correlation matrix at lag 0:')\ndisp(stre.r0)\ndisp('Q statistics:')\ndisp(stre.qstat)\ndisp('p-values of Q statistics:')\ndisp(stre.pval)\n[m, n] = size(stre.pval);\nt = 1:m;\nplot(t, stre.pval, t, 0.05*ones(1, m))\nlegend('p-values of Q statistics:')\n% Identification and estimation of a VARMA model in echelon form the\n% differenced series\ndisp('the absolute values of the eigenvalues of ')\ndisp('the transition matrix in the former VAR')\ndisp('model are:')\nabs(eig(str.T))\ndisp('there is one eigenvalue close to one. This')\ndisp('suggests that there is only one cointegration')\ndisp('relationaship. In fact, ')\nnr = 3;\n%add information about the Kronecker indices\n%estimate the Kronecker indices for the differenced series\nmaxorder = 8;\n[order, kro, scm] = varmaxscmidn(yd, x, seas, maxorder, hr3, prt);\ndisp('estimated Kronecker Indices for the differenced series')\ndisp('using function \"varmaxscmidn\":')\ndisp(kro)\n%identify a VARMA(p,q) model for the differenced series\nmaxlag = 3;\n[lagsopt, ferror] = lratiopqr(yd, x, seas, maxlag, prt);\ndisp('estimated orders in VARMA(p,q,r):')\nlagsopt\ndisp('estimate model in Echelon Form for the Differenced Series: ')\ndisp('Kronecker Indices are [2 2 2] eliminating some insignificant ')\ndisp('paramters')\n%estimate VARMA model in echelon form the differenced series and\n%aliminate some nonsignificant parameters\nkro = [2, 2];\nnsig = [1, 0];\nstrv = estvarmaxkro(yd, x, seas, kro, hr3, finv2, mstainv, nsig, tsig);\n[strv, ferror] = aurirvarmapqPQ(strv, nr, DA);\n%add freq to the model structure\nstrv.freq = seas;\n%estimate model\ns = size(zt, 2);\n[xvfk, strx, ferror] = mexactestimcd(zt, strv, Y, constant);\nxffk = [];\n[ydd, xvvk, xffk, DAfk, Drk, ferrork] = pr2varmapqPQd(zt, xvfk, strx);\nphifk = strx.phisexct;\n[nphi, prk] = size(phifk);\n[Pik, Lambdak, alphak, betafpk, ferror] = mid2mecf(phifk, Drk, DAfk);\ndisp('***** Estimated model in error correction form *****');\n% in.cnames = char(' Lambda(1):',' ',' Lambda(2):',' ',' ');\n% mprint(Lambdak(:,:,2:3),in);\nmprintar(Lambdak(:, :, 2:3), in, tit, strt);\n% in.cnames = char(' Theta(1):',' ',' Theta(2):',' ',' ');\n% mprint(thfk(:,:,2:3),in);\ntit = 'Theta';\nmprintar(thfk(:, :, 2:3), in, tit, strt);\ndisp('t-values of Phi and Theta matrices:')\n% in.cnames = char(' tv-Phi(1):',' ',' tv-Phi(2):',' ',' ');\n% mprint(strx.phitvexct(:,:,2:3),in);\ntit = 'tv-Phi';\nmprintar(strx.phitvexct(:, :, 2:3), in, tit, strt);\n% in.cnames = char(' tv-Theta(1):',' ',' tv-Theta(2):',' ',' ');\n% mprint(strx.thetatvexct(:,:,2:3),in);\ntit = 'tv-theta';\nmprintar(strx.thetatvexct(:, :, 2:3), in, tit, strt);\n[nb, mb] = size(betafpk);\nalphak = alphak * betafpk(:, 1:nb);\nbetafpk = betafpk(:, 1:nb) \\ betafpk;\ndisp(betafpk)\ndisp(alphak)\n%constant is phifk(1)*mu\n% mprint([strx.sigmarexct sum(phifk,3)*strx.musexct],in);\nmprintar(strx.sigmarexct, in, tit);\nmprintar((sum(phifk, 3) * strx.musexct)', in, tit);\nstrc.phis = phifk;\nstrc.sigmar2 = strx.sigmarexct;\n[e, E, rSigmat] = sqrt_ckms(ydd, Y, strc, maxupdt, tol);\nrecrs = zeros(ne, me);\nrecrs(ii, :) = e(ii, :) - (E(ind, :)' * strx.musexct)';\n%plot residuals\nplot(recrs(:, 1)), legend('recrs(:,1)'), pause\n%compute autocovariance and autocorrelation matrices of rec. residuals\nnr = 0; % nr=strx.nparm+3;\ndisp('******** Residuals: ********');\nstre = mautcov(recrs, lag, ic, nr);\n", "output_sequence": "The Example in Paragraph 5.11 in Tsay (2014) is considered. "}, {"input_sequence": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Sat Dec 15 20:39:16 2018\n@author: xinwenni\n# please install these modules before you run the code:\n#!pip install gensim\n#!pip install matplotlib\nimport os\nimport pandas as pd\n## `nltk.download('punkt')\nimport numpy as np\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.stem.porter import PorterStemmer\nimport nltk\nnltk.download('stopwords')\nfrom gensim import corpora\nfrom collections import defaultdict\nfrom gensim.test.utils import common_corpus\nfrom gensim.models import LdaSeqModel\nfrom gensim.corpora import Dictionary, bleicorpus\nfrom gensim.models import ldaseqmodel\nimport matplotlib.pyplot as plt\n# Please change the working directory to your path!\n# os.chdir(\"/Users/xinwenni/LDA-DTM/DTM\")\ndef BasicCleanText(raw_text):\ncleantextprep = str(raw_text)\n\nexpression = \"[^a-zA-Z0-9 ]\" # keep only letters, numbers and whitespace\ncleantextCAP = re.sub(expression, '', cleantextprep) # apply regex\ncleantext = cleantextCAP.lower() # lower case\n# Tokenization\ntokenizer = RegexpTokenizer(r'\\w+')\ntokens = tokenizer.tokenize(cleantext)\n# create English stop words list\n#en_stop = get_stop_words('en')\nstop = set(stopwords.words('english'))\n# remove stop words from tokens\n#stopped_tokens = [i for i in tokens if not i in en_stop]\n\n# Create p_stemmer of class PorterStemmer\np_stemmer = PorterStemmer()\n# stem token\ntexts_clean = [p_stemmer.stem(i) for i in stopped_tokens]\nreturn texts_clean;\n# load data\n#df = pd.read_csv('df02.csv',encoding=\"ISO-8859-1\")\ntime=df['date']\ndf['year']=time.str.slice(0,4)\ndf = pd.concat([df, pd.DataFrame(columns = ['year']),\npd.DataFrame(columns = ['clean_content'])])\ntime_stamps=np.arange(int(df.year.min(axis=0)),int(df.year.max(axis=0))+1,1)\ntime_stamps=list(time_stamps)\nfor i in range(len(df)):\ncontent=df.iat[i,3]\ncontent_clean=BasicCleanText(raw_text=content)\ncontent_clean=\" \".join(content_clean)\ndf.iat[i,4]=content_clean\n# find out the time slice\ngp=df.groupby(by=['year'])\ntotal_yearly_list=list(gp.size())\ndocuments=list(df['clean_content'])\nstoplist=stopwords\nstoplist = set('for a of the and to in'.split())\ntexts = [[word for word in document.lower().split() if word not in stoplist]\nfor document in documents]\n# drop the words only appers once\nfrequency = defaultdict(int)\nfor text in texts:\nfor token in text:\nfrequency[token] += 1\ntexts = [[token for token in text if frequency[token] > 1]\nfor text in texts]\ndictionary = corpora.Dictionary(texts) # generate the dictionary\ndictionary.compactify() #\ndictionary.save(os.path.join('dictionary.dict')) # store the dictionary, for future reference\nprint(dictionary)\n#Save vocabulary\nvocFile = open(os.path.join( 'vocabulary.dat'),'w')\nfor word in dictionary.values():\nvocFile.write(word+'\\n')\nvocFile.close()\nprint(vocFile)\n#Prevent storing the words of each document in the RAM\nclass MyCorpus(object):\ndef __iter__(self):\nfor document in documents:\n# assume there's one document per line, tokens separated by whitespace\nyield dictionary.doc2bow(document.lower().split())\ncorpus_memory_friendly = MyCorpus()\nmultFile = open(os.path.join( 'foo-mult.dat'),'w')\nfor vector in corpus_memory_friendly: # load one vector into memory at a time\nmultFile.write(str(len(vector)) + ' ')\nfor (wordID, weigth) in vector:\nmultFile.write(str(wordID) + ':' + str(weigth) + ' ')\nmultFile.write('\\n')\nmultFile.close()\nprint(multFile)\ntime_slice=total_yearly_list\n#LdaSeqModel(corpus=None, time_slice=None, id2word=None, alphas=0.01, num_topics=10, initialize='gensim', sstats=None, lda_model=None, obs_variance=0.5, chain_variance=0.005, passes=10, random_state=None, lda_inference_max_iter=25, em_min_iter=6, chunksize=100)\n#use LdaSeqModel to generate DTM results\n#ldaseq = LdaSeqModel(corpus=corpus_memory_friendly, id2word=dictionary, time_slice=time_slice, num_topics=5)\n# for given time, the distriibution of each topic\nldaseq.print_topics(time=1)\n# for given topic the word distribution over time\nDTM_topic_0=ldaseq.print_topic_times(topic=0, top_terms=10)\ndef topic_time(DTM_topic,time_stamps):\nfor i in range(len(time_slice)-1):\nif i==0:\ntemp_a1=pd.DataFrame(DTM_topic[i])\ntemp_a1.columns = ['words', time_stamps[i]]\ntemp_a1=pd.merge(temp_a1,temp_a2)\nelse:\ntopic_words_time=temp_a1\nreturn topic_words_time\n\ntopic1_words_time=topic_time(DTM_topic_0,time_stamps)\n#plot the dynamic movement of topic 1\ntopic1_words=list(topic1_words_time['words'])\nplt.figure()\nfor i in range(0,5):\nplt.plot(time_stamps, topic1_words_time.ix[i,1:],marker=\".\",label=topic1_words[i])\n#plt.xlim((-1, 2))\nplt.legend(loc='best')\nplt.title('Topic 1')\nplt.savefig('Topic4-1.png',transparent=True)\nplt.show()\n#plot the dynamic movement of topic2\ntopic=topic2_words_time\ntopic_words=list(topic['words'])\nfor i in range(1,6):\nplt.plot(time_stamps, topic.ix[i,1:],marker=\".\",label=topic_words[i])\nplt.title('Topic 2')\nplt.savefig('Topic4-2.png',transparent=True)\n#plot the dynamic movement of topic3\ntopic=topic3_words_time\nplt.title('Topic 3')\nplt.savefig('Topic4-3.png',transparent=True)\n#plot the dynamic movement of topic4\ntopic=topic4_words_time\nplt.title('Topic 4')\nplt.savefig('Topic4-4.png',transparent=True)\n#plot the dynamic movement of topic5\ntopic=topic5_words_time\nfor i in [0,1,2,4,5]:\nplt.title('Topic 5')\nplt.savefig('Topic5.png',transparent=True)\n", "output_sequence": "Analysis the word evolution of Cryptocurrency regulation news during 2013-2018 using Dynamic Topic Model "}, {"input_sequence": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Mon Mar 29 13:15:11 2021\n@author: xinwenni\n##########################################################\n# Content:\n# part I: Install and import packages and load data\n# part II: Pre-process and vectorize the documents\n# Part III: Training LDA model\n# Part IV: Find the optimal number of topics using coherence_values\n# Part V: Compute similarity of topics\n# Part VI: Visualize the topics\n# Part I: install and import packages and load data\n# please install these modules before you run the code:\n#!pip install gensim\n#!pip install matplotlib\nimport os\nimport pandas as pd\n## `nltk.download('punkt')\nimport numpy as np\n# NLTK\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.stem.porter import PorterStemmer\nfrom gensim.models import Phrases\nimport nltk\nnltk.download('stopwords')\n# Gensim\nimport gensim\nimport gensim.corpora as corpora\nfrom gensim.utils import simple_preprocess\nfrom gensim.matutils import kullback_leibler, jaccard, hellinger, sparse2full\n#from gensim.test.utils import common_corpus\nfrom gensim.models import LdaModel\nfrom gensim.test.utils import datapath\n#from gensim.models import LdaSeqModel\n#from gensim.corpora import Dictionary, bleicorpus\n#from gensim import models, similarities\n# spacy for lemmatization\nimport spacy\nfrom scipy.stats import wasserstein_distance\n# Plotting tools\nimport pyLDAvis\nimport pyLDAvis.gensim # don't skip this\nimport matplotlib.pyplot as plt\n#matplotlib inline\n# Enable logging for gensim - optional\nimport logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\nfrom collections import defaultdict\nfrom pprint import pprint\n############################\n# Please change the working directory to your path!\n# os.chdir(\"/Users/xinwenni/LDA-DTM/DTM\")\n# load data\n#df = pd.read_csv('df02.csv',encoding=\"ISO-8859-1\")\nprint(len(df))\nprint(df.iloc[0,:])\n# Part II: Pre-process and vectorize the documents\n# Convert to list\ndata= df.body.values.tolist()\ndef clean_data(data):\n# Remove Emails\ndata = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n\n# Remove new line characters\ndata = [re.sub('\\s+', ' ', sent) for sent in data]\n# Remove distracting single quotes\ndata = [re.sub(\"\\'\", \"\", sent) for sent in data]\npprint(data[:1])\nreturn data\ndef sent_to_words(sentences):\nfor sentence in sentences:\nyield(gensim.utils.simple_preprocess(str(sentence), deacc=True)) # deacc=True removes punctuations\n# Define functions for stopwords, bigrams, and lemmatization\ndef remove_stopwords(texts):\nreturn [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\ndef make_bigrams(texts):\nreturn [bigram_mod[doc] for doc in texts]\ndef make_trigrams(texts):\nreturn [trigram_mod[bigram_mod[doc]] for doc in texts]\ndef lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n\"\"\"https://spacy.io/api/annotation\"\"\"\ntexts_out = []\nfor sent in texts:\ndoc = nlp(\" \".join(sent))\ntexts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\nreturn texts_out\ndef get_lemm(data):\ndata_words = list(sent_to_words(data))\n# print(data_words[:1])\n# Build the bigram and trigram models\nbigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\ntrigram = gensim.models.Phrases(bigram[data_words], threshold=100)\n# Faster way to get a sentence clubbed as a trigram/bigram\nbigram_mod = gensim.models.phrases.Phraser(bigram)\n# See trigram example\n# print(trigram_mod[bigram_mod[data_words[0]]])\n# Remove Stop Words\ndata_words_nostops=remove_stopwords(data_words)\n# Form Bigrams\n# data_words_bigrams = make_bigrams(data_words_nostops)\n# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n# python3 -m spacy download en\nnlp = spacy.load('en', disable=['parser', 'ner'])\n# Do lemmatization keeping only noun, adj, vb, adv\ndata_lemmatized = lemmatization(data_words_nostops, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n# print(data_lemmatized[:1])\nreturn data_lemmatized\n# simple clean the data first\ndata=clean_data(data)\n# define stopwords\nstop_words = stopwords.words('english')\nstop_words.extend(['from', 'use','also'])\nnlp = spacy.load('en', disable=['parser', 'ner'])\n# Tokenize and lemmatize data\ndata_lemmatized=get_lemm(data)\n# Create Dictionary\nid2word= corpora.Dictionary(data_lemmatized)\n# Create Corpus\ntexts = data_lemmatized\n# Term Document Frequency\ncorpus = [id2word.doc2bow(text) for text in texts]\n#Let\u2019s see how many tokens and documents we have to train on.\nprint('Number of unique tokens: %d' % len(id2word))\n# Part III: Training LDA model\n# Set training parameters.\nnum_topics = 10\nchunksize = 2000\npasses = 20\niterations = 400\neval_every = None # Don't evaluate model perplexity, takes too much time.\n#\n## Make a index to word dictionary.\n#temp = dictionary[0] # This is only to \"load\" the dictionary.\n#id2word = dictionary.id2token\nmodel = LdaModel(\ncorpus=corpus,\nid2word=id2word,\nchunksize=chunksize,\nalpha='auto',\niterations=iterations,\nnum_topics=num_topics,\npasses=passes,\neval_every=eval_every\n)\ntop_topics = model.top_topics(corpus) #, num_words=20)\n# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\navg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\nprint('Average topic coherence: %.4f.' % avg_topic_coherence)\npprint(top_topics)\n# Part IV: find the optimal number of topics using coherence_values\ndef compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n\"\"\"\nCompute c_v coherence for various number of topics\nParameters:\n----------\ndictionary : Gensim dictionary\ncorpus : Gensim corpus\ntexts : List of input texts\nlimit : Max num of topics\nReturns:\n-------\nmodel_list : List of LDA topic models\ncoherence_values : Coherence values corresponding to the LDA model with respective number of topics\ncoherence_values = []\nmodel_list = []\nfor num_topics in range(start, limit, step):\n# model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)\nupdate_every=1,\nmodel_list.append(model)\ncoherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\ncoherence_values.append(coherencemodel.get_coherence())\nreturn model_list, coherence_values\n# Can take a long time to run.\nmodel_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_lemmatized, start=3, limit=17, step=2)\n# Show graph\nfilename='Num_Topic_CV'\nlimit=17; start=3; step=2;\nx = range(start, limit, step)\nplt.plot(x, coherence_values)\nplt.xlabel(\"Num Topics\")\nplt.ylabel(\"Coherence score\")\n#plt.ylim([0.25,0.45])\n#plt.legend((\"coherence_values\"), loc='best')\nplt.savefig(filename,dpi = 720,transparent=True)\nplt.show()\n# optimal number is 11, then adjust the topic number, and retrain the LDA model\nnum_topic=11\nlda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\nid2word=id2word,\nlda_model.show_topics()\n# Save model to disk.\ntemp_file = datapath(\"lda_model\")\nlda_model.save(temp_file)\n# Print the Keyword in the 10 topics\npprint(lda_model.print_topics(0,10))\ndoc_lda = lda_model[corpus]\n\n# Part V: Compute similarity of topics\ndef plot_difference_matplotlib(mdiff, title=\"\", annotation=None):\n\"\"\"Helper function to plot difference between models.\nUses matplotlib as the backend.\"\"\"\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(figsize=(18, 14))\ndata = ax.imshow(mdiff, cmap='RdBu_r', origin='lower')\nplt.title(title)\nplt.colorbar(data)\ntry:\nget_ipython()\nimport plotly.offline as py\nexcept Exception:\n#\n# Fall back to matplotlib if we're not in a notebook, or if plotly is\n# unavailable for whatever reason.\nplot_difference = plot_difference_matplotlib\nelse:\npy.init_notebook_mode()\nplot_difference = plot_difference_plotly\nmdiff, annotation = lda_model.diff(lda_model, distance='hellinger', num_words=50)\nplot_difference(mdiff, annotation=annotation)\nplt.tick_params(labelsize=23)\nplt.savefig(\"topic_distance_H.png\",dpi = 360,transparent=True)\n# Part VI: Visualize the topics\npyLDAvis.enable_notebook()\nvis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\npyLDAvis.show(vis)\n", "output_sequence": "Analysis the word evolution of Cryptocurrency regulation news during 2013-2018 using Dynamic Topic Model "}]}
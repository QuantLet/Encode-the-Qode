qname,Description,Keywords,Author,REPO_NAME,code_scripts,ParsedSource,num_parsed_code_tokens
SFEbitreeCDiv,Computes European/American option prices using a binomial tree for assets with/without continuous dividends.,"binomial, tree, asset, call, put, option, option-price, european-option, dividends, financial, black-scholes",ERROR,QID-1453-SFEbitreeCDiv,1,numpy matplotlib matplotlib.lines Line2D calc_parameters T  N  sigma  r  div  P up movement) calc_price S0  K  u  d  N  r  dt  q  type  option  calculate the values at maturity T Using the recursion formula for pricing in the CRR model:   from  T-dt  T-2*dt  ....  dt  0)  decide for maximum  create list of time values plot asset_values  option_values  snell_values  time_values  type  customize legend plt.savefig 'American_Put.png'  transparent=True) ###### MAIN ################  calculate all prices   plot tree of American Put,76
BitcoinOptionTrading,"Deviations in Risk Neutral Density (RND) and Historical Density (HD) can be used to develop trading strategies. The Risk Neutral Density is estimated via Rookley’s Method, which uses Local Polynomial Regression to smooth the implied volatility surface. In order to estimate the Historical Density, a GARCH(1,1) model is used to fit the bahavior of the underlying. In the scope of this thesis two python packages were developed and deployed on pypi.org - localpoly and spd_trading.","Bitcoin, Pricing Kernel, Bitcoin Option, Option, State Price Density, Rookley, GARCH, Risk Neutral Density, Historical Density, RND, HD, Local Polynomial Regression, Local Polynomial Estimation",Franziska Wehrmann,BitcoinOptionTrading,27,numpy pandas spd_trading risk_neutral_density spd_trading historical_density spd_trading kernel util.general create_dates util.data RndData util.configcore config util.decorators exception_handler logging  -----------------------------------------------------------------------------  algorithm plot  kernel plot - comparison of rnd and hd  save bandwidths in file  ------------------------------------------------------------------------ MAIN  ------------------------------------------------------------------------ MAIN  ------------------------------------------------------------------------ MAIN,38
BitcoinOptionTrading,"Deviations in Risk Neutral Density (RND) and Historical Density (HD) can be used to develop trading strategies. The Risk Neutral Density is estimated via Rookley’s Method, which uses Local Polynomial Regression to smooth the implied volatility surface. In order to estimate the Historical Density, a GARCH(1,1) model is used to fit the bahavior of the underlying. In the scope of this thesis two python packages were developed and deployed on pypi.org - localpoly and spd_trading.","Bitcoin, Pricing Kernel, Bitcoin Option, Option, State Price Density, Rookley, GARCH, Risk Neutral Density, Historical Density, RND, HD, Local Polynomial Regression, Local Polynomial Estimation",Franziska Wehrmann,BitcoinOptionTrading,27,"pandas os datetime datetime util.data_processing data_processing util.connect_db connect_db  cursor = collection.aggregate [      { ""$group"": {          '_id': None           ""max"": { ""$max"": ""$timestamp"" }           ""min"": { ""$min"": ""$timestamp"" }      }}  ])  print list cursor))  datetime 2020 3 5)  -------------------------------------------------------- MERGE TRADES  write_in_db coll=coll  df=trades)  ------------------------------------------------------------- MERGE S  write_in_db coll=coll  df=S)",48
BitcoinOptionTrading,"Deviations in Risk Neutral Density (RND) and Historical Density (HD) can be used to develop trading strategies. The Risk Neutral Density is estimated via Rookley’s Method, which uses Local Polynomial Regression to smooth the implied volatility surface. In order to estimate the Historical Density, a GARCH(1,1) model is used to fit the bahavior of the underlying. In the scope of this thesis two python packages were developed and deployed on pypi.org - localpoly and spd_trading.","Bitcoin, Pricing Kernel, Bitcoin Option, Option, State Price Density, Rookley, GARCH, Risk Neutral Density, Historical Density, RND, HD, Local Polynomial Regression, Local Polynomial Estimation",Franziska Wehrmann,BitcoinOptionTrading,27,pathlib Path sys matplotlib pyplot pandas util.configcore config  ------------------------------------------------------------------------ MAIN  correction to boxplot mean +/- std get median replace_by_median value  describe replace_by_bound value  describe replace_lower value  describe  -------------------- boxplot,29
BitcoinOptionTrading,"Deviations in Risk Neutral Density (RND) and Historical Density (HD) can be used to develop trading strategies. The Risk Neutral Density is estimated via Rookley’s Method, which uses Local Polynomial Regression to smooth the implied volatility surface. In order to estimate the Historical Density, a GARCH(1,1) model is used to fit the bahavior of the underlying. In the scope of this thesis two python packages were developed and deployed on pypi.org - localpoly and spd_trading.","Bitcoin, Pricing Kernel, Bitcoin Option, Option, State Price Density, Rookley, GARCH, Risk Neutral Density, Historical Density, RND, HD, Local Polynomial Regression, Local Polynomial Estimation",Franziska Wehrmann,BitcoinOptionTrading,27,pathlib Path sys matplotlib pyplot numpy util.general add_days util.data load_trades util.configcore config  --------------------------------------------------------------- STRATEGY PLOT  calculate other days and tau  ---------------------------------------------------------------------- Figure  ------------------------------------------------------------------- Densities  ------------------------------------------------------------------- Densities  calculate other days and tau,31
BitcoinOptionTrading,"Deviations in Risk Neutral Density (RND) and Historical Density (HD) can be used to develop trading strategies. The Risk Neutral Density is estimated via Rookley’s Method, which uses Local Polynomial Regression to smooth the implied volatility surface. In order to estimate the Historical Density, a GARCH(1,1) model is used to fit the bahavior of the underlying. In the scope of this thesis two python packages were developed and deployed on pypi.org - localpoly and spd_trading.","Bitcoin, Pricing Kernel, Bitcoin Option, Option, State Price Density, Rookley, GARCH, Risk Neutral Density, Historical Density, RND, HD, Local Polynomial Regression, Local Polynomial Estimation",Franziska Wehrmann,BitcoinOptionTrading,27,pathlib Path sys numpy matplotlib pyplot statsmodels.api qqplot spd_trading historical_density util.configcore config util.data HdData get_HD_model day  tau_day  -------------------------------------------------------------------- QQ-PLOTS create_garch_qq HD  ------------------------------------------------------------------ PARAMETERS substract_offset x  offset=205 add_offset x  offset=205,29
BitcoinOptionTrading,"Deviations in Risk Neutral Density (RND) and Historical Density (HD) can be used to develop trading strategies. The Risk Neutral Density is estimated via Rookley’s Method, which uses Local Polynomial Regression to smooth the implied volatility surface. In order to estimate the Historical Density, a GARCH(1,1) model is used to fit the bahavior of the underlying. In the scope of this thesis two python packages were developed and deployed on pypi.org - localpoly and spd_trading.","Bitcoin, Pricing Kernel, Bitcoin Option, Option, State Price Density, Rookley, GARCH, Risk Neutral Density, Historical Density, RND, HD, Local Polynomial Regression, Local Polynomial Estimation",Franziska Wehrmann,BitcoinOptionTrading,27,"dash dash.dependencies Input dash_table dash_core_components dash_html_components pandas util.plotly plotly_plot util.general add_days util.configcore config evaluate dff # Aggregation Table """""")  split_filter_part filter_part  word operators need spaces after them in the filter string   but we don't want these later update_table filter  these operators match pandas series operator method names  this is a simplification of the front-end filtering logic   only works with complete fields in standard format reset_to_page_0 filter_query update_graphs row_ids  selected_row_ids  active_cell  When the table is first rendered  `derived_virtual_data` and  `derived_virtual_selected_rows` will be `None`. This is due to an  idiosyncrasy in Dash  unsupplied properties are always None and Dash  calls the dependent callbacks when the component is first rendered).  So  if `rows` is `None`  then the component was just rendered  and its value will be the same as the component's dataframe.  Instead of setting `None` in here  you could also set  `derived_virtual_data=df.to_rows 'dict')` when you initialize  the component.  pandas Series works enough like a list for this to be OK FF69B4"" if id == active_row_id else ""#7FDBFF"" if id in selected_id_set else ""#0074D9"" for id in row_ids]  check if column exists - user may have deleted it  If `column.deletable=False`  then you don't  need to do this check. update_plot row_ids  selected_row_ids  active_cell",199
BitcoinOptionTrading,"Deviations in Risk Neutral Density (RND) and Historical Density (HD) can be used to develop trading strategies. The Risk Neutral Density is estimated via Rookley’s Method, which uses Local Polynomial Regression to smooth the implied volatility surface. In order to estimate the Historical Density, a GARCH(1,1) model is used to fit the bahavior of the underlying. In the scope of this thesis two python packages were developed and deployed on pypi.org - localpoly and spd_trading.","Bitcoin, Pricing Kernel, Bitcoin Option, Option, State Price Density, Rookley, GARCH, Risk Neutral Density, Historical Density, RND, HD, Local Polynomial Regression, Local Polynomial Estimation",Franziska Wehrmann,BitcoinOptionTrading,27,pathlib Path sys pandas datetime datetime matplotlib pyplot matplotlib util.connect_db connect_db util.configcore config  ------------------------------------------------------------------------ PLOT  ----------------------------------------------------------------------- INSET  ----------------------------------------------------------------------- BOXEN,19
BitcoinOptionTrading,"Deviations in Risk Neutral Density (RND) and Historical Density (HD) can be used to develop trading strategies. The Risk Neutral Density is estimated via Rookley’s Method, which uses Local Polynomial Regression to smooth the implied volatility surface. In order to estimate the Historical Density, a GARCH(1,1) model is used to fit the bahavior of the underlying. In the scope of this thesis two python packages were developed and deployed on pypi.org - localpoly and spd_trading.","Bitcoin, Pricing Kernel, Bitcoin Option, Option, State Price Density, Rookley, GARCH, Risk Neutral Density, Historical Density, RND, HD, Local Polynomial Regression, Local Polynomial Estimation",Franziska Wehrmann,BitcoinOptionTrading,27,pathlib Path sys numpy matplotlib pyplot localpoly.base LocalPolynomialRegressionCV spd_trading risk_neutral_density util.data RndData util.configcore config  -------------------------------------------------------------------- SETTINGS  --------------------------------------------------------- FIND MINIMAL MSE x2  ------------------------------------------------------------------------ PLOT,23
BitcoinOptionTrading,"Deviations in Risk Neutral Density (RND) and Historical Density (HD) can be used to develop trading strategies. The Risk Neutral Density is estimated via Rookley’s Method, which uses Local Polynomial Regression to smooth the implied volatility surface. In order to estimate the Historical Density, a GARCH(1,1) model is used to fit the bahavior of the underlying. In the scope of this thesis two python packages were developed and deployed on pypi.org - localpoly and spd_trading.","Bitcoin, Pricing Kernel, Bitcoin Option, Option, State Price Density, Rookley, GARCH, Risk Neutral Density, Historical Density, RND, HD, Local Polynomial Regression, Local Polynomial Estimation",Franziska Wehrmann,BitcoinOptionTrading,27,"pandas matplotlib pyplot matplotlib datetime datetime pathlib Path sys util.general create_dates util.general add_days util.configcore config util.data RndData group_dataframe_by_date df  which_date  mode=""all_dates""  ------------------------------------------------------------ CASHFLOW BARPLOT  ---- find missing days  inside of interval  outside of interval  ------------  ax.set_xlim a.date.min )  a.date.max ))  ------------------------------------------------------------ Payoff by Expiration Date",45
BitcoinOptionTrading,"Deviations in Risk Neutral Density (RND) and Historical Density (HD) can be used to develop trading strategies. The Risk Neutral Density is estimated via Rookley’s Method, which uses Local Polynomial Regression to smooth the implied volatility surface. In order to estimate the Historical Density, a GARCH(1,1) model is used to fit the bahavior of the underlying. In the scope of this thesis two python packages were developed and deployed on pypi.org - localpoly and spd_trading.","Bitcoin, Pricing Kernel, Bitcoin Option, Option, State Price Density, Rookley, GARCH, Risk Neutral Density, Historical Density, RND, HD, Local Polynomial Regression, Local Polynomial Estimation",Franziska Wehrmann,BitcoinOptionTrading,27,pandas numpy datetime datetime util.connect_db connect_db __init__ self  day  tau_day  data  data on trading_day _format_data self  data _options_in_interval self  option  moneyness  action  left  right  option interval _get_payoff self  K  ST  option execute_options self _calculate_fee self  P  S  max_fee_BTC=0.0004  max_fee_pct=0.2 _payoff_call self  action  K  price  ST _payoff_put self  action  K  price  ST _get_option_buy_sell_payoff self  option  action  K  price  ST _get_trading_payoffs self  data  only applies to TAKER ORDERS K1 self K2 self S1 self S2 self _get_expected_payoff payoff  density  ---------------------------------------------------------- TRADING STRATEGIES,80
BitcoinOptionTrading,"Deviations in Risk Neutral Density (RND) and Historical Density (HD) can be used to develop trading strategies. The Risk Neutral Density is estimated via Rookley’s Method, which uses Local Polynomial Regression to smooth the implied volatility surface. In order to estimate the Historical Density, a GARCH(1,1) model is used to fit the bahavior of the underlying. In the scope of this thesis two python packages were developed and deployed on pypi.org - localpoly and spd_trading.","Bitcoin, Pricing Kernel, Bitcoin Option, Option, State Price Density, Rookley, GARCH, Risk Neutral Density, Historical Density, RND, HD, Local Polynomial Regression, Local Polynomial Estimation",Franziska Wehrmann,BitcoinOptionTrading,27, required for QuantLet Graph,4
BitcoinOptionTrading,"Deviations in Risk Neutral Density (RND) and Historical Density (HD) can be used to develop trading strategies. The Risk Neutral Density is estimated via Rookley’s Method, which uses Local Polynomial Regression to smooth the implied volatility surface. In order to estimate the Historical Density, a GARCH(1,1) model is used to fit the bahavior of the underlying. In the scope of this thesis two python packages were developed and deployed on pypi.org - localpoly and spd_trading.","Bitcoin, Pricing Kernel, Bitcoin Option, Option, State Price Density, Rookley, GARCH, Risk Neutral Density, Historical Density, RND, HD, Local Polynomial Regression, Local Polynomial Estimation",Franziska Wehrmann,BitcoinOptionTrading,27,pathlib Path sys numpy matplotlib pyplot localpoly.base LocalPolynomialRegressionCV spd_trading risk_neutral_density util.data RndData util.configcore config  -------------------------------------------------------------------- SETTINGS  algorithm plot,18
BitcoinOptionTrading,"Deviations in Risk Neutral Density (RND) and Historical Density (HD) can be used to develop trading strategies. The Risk Neutral Density is estimated via Rookley’s Method, which uses Local Polynomial Regression to smooth the implied volatility surface. In order to estimate the Historical Density, a GARCH(1,1) model is used to fit the bahavior of the underlying. In the scope of this thesis two python packages were developed and deployed on pypi.org - localpoly and spd_trading.","Bitcoin, Pricing Kernel, Bitcoin Option, Option, State Price Density, Rookley, GARCH, Risk Neutral Density, Historical Density, RND, HD, Local Polynomial Regression, Local Polynomial Estimation",Franziska Wehrmann,BitcoinOptionTrading,27,pathlib Path pathlib typing pydantic BaseModel strictyaml load  Project Directories find_config_file  fetch_config_from_yaml cfg_path: Path = None create_and_validate_config parsed_config: YAML = None  specify the data attribute from the strictyaml YAML type.  add DATASET_DIR prefix,33
BitcoinOptionTrading,"Deviations in Risk Neutral Density (RND) and Historical Density (HD) can be used to develop trading strategies. The Risk Neutral Density is estimated via Rookley’s Method, which uses Local Polynomial Regression to smooth the implied volatility surface. In order to estimate the Historical Density, a GARCH(1,1) model is used to fit the bahavior of the underlying. In the scope of this thesis two python packages were developed and deployed on pypi.org - localpoly and spd_trading.","Bitcoin, Pricing Kernel, Bitcoin Option, Option, State Price Density, Rookley, GARCH, Risk Neutral Density, Historical Density, RND, HD, Local Polynomial Regression, Local Polynomial Estimation",Franziska Wehrmann,BitcoinOptionTrading,27,numpy spd_trading risk_neutral_density spd_trading historical_density spd_trading kernel util.general create_dates util.data RndData util.configcore config logging  -----------------------------------------------------------------------------  algorithm plot  kernel plot - comparison of rnd and hd  ------------------------------------------------------------------------ MAIN  ------------------------------------------------------------------------ MAIN  ------------------------------------------------------------------------ MAIN  ------------------------------------------------------------ CALCULATE RND HD,35
BitcoinOptionTrading,"Deviations in Risk Neutral Density (RND) and Historical Density (HD) can be used to develop trading strategies. The Risk Neutral Density is estimated via Rookley’s Method, which uses Local Polynomial Regression to smooth the implied volatility surface. In order to estimate the Historical Density, a GARCH(1,1) model is used to fit the bahavior of the underlying. In the scope of this thesis two python packages were developed and deployed on pypi.org - localpoly and spd_trading.","Bitcoin, Pricing Kernel, Bitcoin Option, Option, State Price Density, Rookley, GARCH, Risk Neutral Density, Historical Density, RND, HD, Local Polynomial Regression, Local Polynomial Estimation",Franziska Wehrmann,BitcoinOptionTrading,27,pathlib Path sys math matplotlib pyplot numpy spd_trading risk_neutral_density localpoly.base LocalPolynomialRegressionCV util.data RndData util.configcore config util.general sort_values_by_X chunks lst  n get_slice X  y  slice  no_slices=15  ----------------------------------------------------- 2020-03-12 T=15 IV SMILE  ---------------------------------------------------- 2020-04-19 T=12 - Density,35
BitcoinOptionTrading,"Deviations in Risk Neutral Density (RND) and Historical Density (HD) can be used to develop trading strategies. The Risk Neutral Density is estimated via Rookley’s Method, which uses Local Polynomial Regression to smooth the implied volatility surface. In order to estimate the Historical Density, a GARCH(1,1) model is used to fit the bahavior of the underlying. In the scope of this thesis two python packages were developed and deployed on pypi.org - localpoly and spd_trading.","Bitcoin, Pricing Kernel, Bitcoin Option, Option, State Price Density, Rookley, GARCH, Risk Neutral Density, Historical Density, RND, HD, Local Polynomial Regression, Local Polynomial Estimation",Franziska Wehrmann,BitcoinOptionTrading,27,"datetime datetime hashlib inst2date s create_id row  attributes=[""timestamp""  ""M""  ""iv""] data_processing trades  trades['p'] = trades.amount * trades.index_price  renaming and scaling for SPD",22
BitcoinOptionTrading,"Deviations in Risk Neutral Density (RND) and Historical Density (HD) can be used to develop trading strategies. The Risk Neutral Density is estimated via Rookley’s Method, which uses Local Polynomial Regression to smooth the implied volatility surface. In order to estimate the Historical Density, a GARCH(1,1) model is used to fit the bahavior of the underlying. In the scope of this thesis two python packages were developed and deployed on pypi.org - localpoly and spd_trading.","Bitcoin, Pricing Kernel, Bitcoin Option, Option, State Price Density, Rookley, GARCH, Risk Neutral Density, Historical Density, RND, HD, Local Polynomial Regression, Local Polynomial Estimation",Franziska Wehrmann,BitcoinOptionTrading,27,pymongo pymongo InsertOne pandas json connect_db port=27017  db=MONGO_DB get_as_df collection  query write_in_db coll  df bulk_write coll  df  ordered=False,18
BitcoinOptionTrading,"Deviations in Risk Neutral Density (RND) and Historical Density (HD) can be used to develop trading strategies. The Risk Neutral Density is estimated via Rookley’s Method, which uses Local Polynomial Regression to smooth the implied volatility surface. In order to estimate the Historical Density, a GARCH(1,1) model is used to fit the bahavior of the underlying. In the scope of this thesis two python packages were developed and deployed on pypi.org - localpoly and spd_trading.","Bitcoin, Pricing Kernel, Bitcoin Option, Option, State Price Density, Rookley, GARCH, Risk Neutral Density, Historical Density, RND, HD, Local Polynomial Regression, Local Polynomial Estimation",Franziska Wehrmann,BitcoinOptionTrading,27,pathlib Path sys numpy pandas os matplotlib pyplot matplotlib.pyplot cm util.data load_rnd_hd util.configcore config  ------------------------------------------------ RND HD DENSITIES FOR ONE DAY strp_file filename  ------------------------------------------------------------------------ PLOT,25
BitcoinOptionTrading,"Deviations in Risk Neutral Density (RND) and Historical Density (HD) can be used to develop trading strategies. The Risk Neutral Density is estimated via Rookley’s Method, which uses Local Polynomial Regression to smooth the implied volatility surface. In order to estimate the Historical Density, a GARCH(1,1) model is used to fit the bahavior of the underlying. In the scope of this thesis two python packages were developed and deployed on pypi.org - localpoly and spd_trading.","Bitcoin, Pricing Kernel, Bitcoin Option, Option, State Price Density, Rookley, GARCH, Risk Neutral Density, Historical Density, RND, HD, Local Polynomial Regression, Local Polynomial Estimation",Franziska Wehrmann,BitcoinOptionTrading,27,pathlib Path sys matplotlib pyplot numpy util.general add_days util.data load_trades util.configcore config  --------------------------------------------------------------- STRATEGY PLOT  calculate other days and tau  ---------------------------------------------------------------------- Figure  ------------------------------------------------------------------- Densities  --------------------------------------------------------------------- PAYOFFS,26
BitcoinOptionTrading,"Deviations in Risk Neutral Density (RND) and Historical Density (HD) can be used to develop trading strategies. The Risk Neutral Density is estimated via Rookley’s Method, which uses Local Polynomial Regression to smooth the implied volatility surface. In order to estimate the Historical Density, a GARCH(1,1) model is used to fit the bahavior of the underlying. In the scope of this thesis two python packages were developed and deployed on pypi.org - localpoly and spd_trading.","Bitcoin, Pricing Kernel, Bitcoin Option, Option, State Price Density, Rookley, GARCH, Risk Neutral Density, Historical Density, RND, HD, Local Polynomial Regression, Local Polynomial Estimation",Franziska Wehrmann,BitcoinOptionTrading,27,"os os.path join plotly plotly.subplots make_subplots numpy util.data load_trades util.general add_days translate_color c plotly_plot trading_day  trading_tau  trade_type  x=0.5  Build figure  --------------------------------------------------------------- Densities  ------------------------------------------------------------- Kernel Plot  K_bound 0061ff""  width=0)   buy_areas 0061ff""  width=0)   sell_areas 0061ff""  width=0)   kernel  available options  trades  ------------------------------------------------------------- Payoffs  ST = df_trades.ST.iloc[0]  single payoffs  total payoff  fig.update_yaxes rangemode=""nonnegative"")  fig.update_xaxes range=[1 - x  1 + x])  fig = plotly_plot trade_data_directory  day  tau_day  trade_type)  fig.show )",65
BitcoinOptionTrading,"Deviations in Risk Neutral Density (RND) and Historical Density (HD) can be used to develop trading strategies. The Risk Neutral Density is estimated via Rookley’s Method, which uses Local Polynomial Regression to smooth the implied volatility surface. In order to estimate the Historical Density, a GARCH(1,1) model is used to fit the bahavior of the underlying. In the scope of this thesis two python packages were developed and deployed on pypi.org - localpoly and spd_trading.","Bitcoin, Pricing Kernel, Bitcoin Option, Option, State Price Density, Rookley, GARCH, Risk Neutral Density, Historical Density, RND, HD, Local Polynomial Regression, Local Polynomial Estimation",Franziska Wehrmann,BitcoinOptionTrading,27,"sshtunnel SSHTunnelForwarder pymongo pprint  ssh -i IRTG firstname_lastname@35.205.115.90 get_as_df collection  query  query = {      ""$and"": [          {""timestamp"": {""$gte"": start_ts}}           {""timestamp"": {""$lte"": end_ts}}       ]  }  df_all = get_as_df collection  query) datetime datetime",31
BitcoinOptionTrading,"Deviations in Risk Neutral Density (RND) and Historical Density (HD) can be used to develop trading strategies. The Risk Neutral Density is estimated via Rookley’s Method, which uses Local Polynomial Regression to smooth the implied volatility surface. In order to estimate the Historical Density, a GARCH(1,1) model is used to fit the bahavior of the underlying. In the scope of this thesis two python packages were developed and deployed on pypi.org - localpoly and spd_trading.","Bitcoin, Pricing Kernel, Bitcoin Option, Option, State Price Density, Rookley, GARCH, Risk Neutral Density, Historical Density, RND, HD, Local Polynomial Regression, Local Polynomial Estimation",Franziska Wehrmann,BitcoinOptionTrading,27,os os.path join pandas datetime datetime util.connect_db connect_db  --------------------------------------------------------------------- BINANCE format_date date_str  --------------------------------------------------------------------- DERIBIT,14
BitcoinOptionTrading,"Deviations in Risk Neutral Density (RND) and Historical Density (HD) can be used to develop trading strategies. The Risk Neutral Density is estimated via Rookley’s Method, which uses Local Polynomial Regression to smooth the implied volatility surface. In order to estimate the Historical Density, a GARCH(1,1) model is used to fit the bahavior of the underlying. In the scope of this thesis two python packages were developed and deployed on pypi.org - localpoly and spd_trading.","Bitcoin, Pricing Kernel, Bitcoin Option, Option, State Price Density, Rookley, GARCH, Risk Neutral Density, Historical Density, RND, HD, Local Polynomial Regression, Local Polynomial Estimation",Franziska Wehrmann,BitcoinOptionTrading,27,"pickle pandas copy deepcopy logging util.connect_db connect_db util.configcore config __init__ self  cutoff load_data self  date_str analyse self  date=None  sortby=""date"" delete_duplicates self filter_data self  date  tau_day  mode=""unique"" add_option_color self  data  blue - put  red - call  ----------------------------------------------------------------------------------------------------------------- DATA datetime datetime nearest items  pivot date_str2int date_str __init__ self  target=""price"" _load_data self filter_data self  date get_S0 self  day __init__ self replace_by_median self  value  describe replace_by_bound self  value  describe replace_lower self  value  describe get_table self  ----------------------------------------------------------- SAVE AND LOAD RND HD save_rnd_hd day  tau_day  RND  HD  Kernel load_rnd_hd day  tau_day  ----------------------------------------------------------- TRADES - LOAD SAVE load_trades trading_day  trading_tau  trade",95
BitcoinOptionTrading,"Deviations in Risk Neutral Density (RND) and Historical Density (HD) can be used to develop trading strategies. The Risk Neutral Density is estimated via Rookley’s Method, which uses Local Polynomial Regression to smooth the implied volatility surface. In order to estimate the Historical Density, a GARCH(1,1) model is used to fit the bahavior of the underlying. In the scope of this thesis two python packages were developed and deployed on pypi.org - localpoly and spd_trading.","Bitcoin, Pricing Kernel, Bitcoin Option, Option, State Price Density, Rookley, GARCH, Risk Neutral Density, Historical Density, RND, HD, Local Polynomial Regression, Local Polynomial Estimation",Franziska Wehrmann,BitcoinOptionTrading,27,"pandas numpy typing Tuple datetime datetime create_dates start: str  end: str  mode: str = ""daily"" load_tau_section_parameters tau_section: str  tau_section == ""huge"": add_days daystr: str  days: int  type: datetime  type: datetime  type: str sort_values_by_X X: np.ndarray  y: np.ndarray",37
BitcoinOptionTrading,"Deviations in Risk Neutral Density (RND) and Historical Density (HD) can be used to develop trading strategies. The Risk Neutral Density is estimated via Rookley’s Method, which uses Local Polynomial Regression to smooth the implied volatility surface. In order to estimate the Historical Density, a GARCH(1,1) model is used to fit the bahavior of the underlying. In the scope of this thesis two python packages were developed and deployed on pypi.org - localpoly and spd_trading.","Bitcoin, Pricing Kernel, Bitcoin Option, Option, State Price Density, Rookley, GARCH, Risk Neutral Density, Historical Density, RND, HD, Local Polynomial Regression, Local Polynomial Estimation",Franziska Wehrmann,BitcoinOptionTrading,27,"pandas logging util.general create_dates util.data RndData util.configcore config util  default='warn'  BUG: count intervals - K1/K2 takes also 4 intervals?  TODO: check if ""total"" is farther at the front of table  TODO: plots x-axis ""Probability"" not ""Density""  ----------------------------------------------------------- LOAD DATA hd_curve  RND  load what is needed for density and kernel-plot. If does not exist  break  -------------------------- LOAD OPTIONS THAT ARE OFFERED NEXT DAY  ------------------------- TRY IF FIND RESULT FOR TRADING STRATEGY  ----------------------------------------- KERNEL DEVIATES FROM 1?  evaluation day?  evaluation tau?",79
BitcoinOptionTrading,"Deviations in Risk Neutral Density (RND) and Historical Density (HD) can be used to develop trading strategies. The Risk Neutral Density is estimated via Rookley’s Method, which uses Local Polynomial Regression to smooth the implied volatility surface. In order to estimate the Historical Density, a GARCH(1,1) model is used to fit the bahavior of the underlying. In the scope of this thesis two python packages were developed and deployed on pypi.org - localpoly and spd_trading.","Bitcoin, Pricing Kernel, Bitcoin Option, Option, State Price Density, Rookley, GARCH, Risk Neutral Density, Historical Density, RND, HD, Local Polynomial Regression, Local Polynomial Estimation",Franziska Wehrmann,BitcoinOptionTrading,27,"pathlib Path sys numpy matplotlib pyplot localpoly.base LocalPolynomialRegression util.data RndData util.configcore config util.general sort_values_by_X plot_locpoly_weights X  y  x_points  h_1  h_2  kernel=""gaussian""  widht  hight  density points  changes apply to the y-axis  both major and minor ticks are affected  ticks along the bottom edge are off  ticks along the top edge are off  labels along the bottom edge are off)  weights 1f77b4""  ""#ff7f0e""  ""#2ca02c""]):  -------------------------------------------------------------------- SETTINGS",64
BitcoinOptionTrading,"Deviations in Risk Neutral Density (RND) and Historical Density (HD) can be used to develop trading strategies. The Risk Neutral Density is estimated via Rookley’s Method, which uses Local Polynomial Regression to smooth the implied volatility surface. In order to estimate the Historical Density, a GARCH(1,1) model is used to fit the bahavior of the underlying. In the scope of this thesis two python packages were developed and deployed on pypi.org - localpoly and spd_trading.","Bitcoin, Pricing Kernel, Bitcoin Option, Option, State Price Density, Rookley, GARCH, Risk Neutral Density, Historical Density, RND, HD, Local Polynomial Regression, Local Polynomial Estimation",Franziska Wehrmann,BitcoinOptionTrading,27,logging exception_handler function_to_wrap wrapper *args  **kwargs,6
DCA_evaluation_Silhouette-Score_and_Calinski-Harabaz-Score,Computing and plotting the Silhouette Score as well as the Calinski Harabaz Score for a k-means and several hierarchical clusterings (different distance measures),"hierarchical clustering, clustering evaluation, Silhouette-Score, Calinski-Harabaz-Score, k-means",Luisa Krawczyk,Clustering_Evaluation,1, -*- coding: utf-8 -*- sklearn.cluster KMeans sklearn.metrics silhouette_score sklearn.metrics calinski_harabaz_score matplotlib pandas scipy.cluster.hierarchy linkage scipy.cluster.hierarchy fcluster scipy.spatial.distance pdist pylab rcParams seaborn sklearn sklearn.cluster AgglomerativeClustering sklearn  Input: matrix is supposed to be a term-document matrix  SILHOUETTE SCORE using k-means++ here by default  inluding classical k-means  CALINSKI HARABAZ SCORE   k-means++ classical k-means,50
DEDA_Class_2017_Condition&Iteration,Introduce the conditional execution and how to iterate by using,ERROR,Junjie Hu,DEDA_Class_2017_Condition&Iteration,1," if ) takes 1 argument to evaluate if the argument is True or False  False  0  empty sequence will be evaluated as False  True  False  and  or  True  False  True  True  False  False  operator ""is"" is not ""==""  ""is"" is used to evaluate if two 2 variables pointed to the same object  ""=="" is used to evaluate the equivalence of 2 variables  True  False  True  True  2 variables pointed to the same object will vary simultaneously  what if I want to create different object but don't want to copy the values manually?  The answer is deep copy  different data structure may provide different methods.  Further more see: https://docs.python.org/3/library/copy.html  False  using id )  you can check the memory address of a variable  the same object will have the same memory address  Important: Try not using index in the loop while do it in a pythonic way  This is a NOT recommended way to iterate  because it's ugly and less efficient.  Using any name you want in the for loop  what if I want to use the index?  using the keyword ""continue"" to skip a loop and key word ""break"" to stop whole loop.  a loop embed in another loop  infinite loop",200
DEDA2021/22_CointegrationCryptocurrenciesCovid19,"Focuses on the cointegration of price and return of cryptocurrencies and macro indicators, pandemic situation and media index","crpytocurrencies, Covid-19, cointegration, VECM ",Zihan Zhang and Peiqi ZHU,DEDA_WS21_22_Cointegration_Crypto,2,pandas numpy matplotlib.backends.backend_pdf PdfPages  read the crypto currency data  read and process the treasury data  save the treasury dataframe to csv  read  process and save the oil indicators csv  read  process and save the S&P index  read  process and save the gold price  read  process and save the global death data  read  process and save the global infected data  merge the dataframes  read the media data  select the finance related topics: stock loss and stock gain  select the crypto entities related data: Binance and Coinbase  merge the media index data  convert string to float in the data  different sets of column names for test  exclude missing value in the data  merge the processed data,114
DEDA2021/22_CointegrationCryptocurrenciesCovid19,"Focuses on the cointegration of price and return of cryptocurrencies and macro indicators, pandemic situation and media index","crpytocurrencies, Covid-19, cointegration, VECM ",Zihan Zhang and Peiqi ZHU,DEDA_WS21_22_Cointegration_Crypto,2,pandas statsmodels matplotlib numpy coinbase.wallet.client Client  from Historic_Crypto import HistoricalData statsmodels.tsa.stattools adfuller matplotlib.backends.backend_pdf PdfPages matplotlib.dates DateFormatter matplotlib  get the media data,21
LLE_p2p,Plotting the p2p lendings using LLE,ERROR,Elizaveta Zinovyeva,LLE_p2p,4,!/usr/bin/env python  coding: utf-8  In[5]: pandas  In[1]: numpy matplotlib mpl_toolkits.mplot3d Axes3D sklearn.manifold locally_linear_embedding sklearn.decomposition PCA seaborn os random sklearn.model_selection train_test_split sklearn.model_selection StratifiedKFold time lightgbm sklearn.metrics log_loss sklearn.neighbors KNeighborsClassifier gc sklearn.linear_model LogisticRegression  In[2]:  In[3]: ########### RANDOMNESS  seed function seed_everything seed = 42  set seed  In[6]:  In[7]:  In[8]: reduction data  reduction_type='LLE'  add_features=True  n_neighbors=15  n_components=2  random_state=0  n_jobs=20  In[9]:  Uncomment to use the required dimensionality reduction technique data = reduction data  reduction_type='LLE'  add_features=True) data = reduction data  reduction_type='TSNE'  add_features=True) data = reduction data  reduction_type='PCA'  add_features=True) data = reduction data  reduction_type='Isomap'  add_features=True) data = reduction data  reduction_type='LLE'  add_features=False  n_components=10)  In[10]:  In[11]:  In[12]:  cores  cross-validation  data partitinoing ########### PLACEHOLDERS  placeholders  predictions  In[13]: ########### CROSS-VALIDATION LOOP  data partitioning # add noise to train to reduce overfitting trn_x += np.random.normal 0  0.1  trn_x.shape)  print data dimensions print 'Data shape:'  trn_y.shape  val_y.shape)      train Ridge  save predictions  print performance  clear memory  print overall performance      In[14]:  In[15]:  In[16]:  In[ ]:,149
LLE_p2p,Plotting the p2p lendings using LLE,ERROR,Elizaveta Zinovyeva,LLE_p2p,4,!/usr/bin/env python  coding: utf-8  In[1]: numpy pandas matplotlib mpl_toolkits.mplot3d Axes3D sklearn manifold seaborn  In[2]:  In[3]:  In[4]:  In[5]: colors row  In[12]:  In[13]:  In[11]:  In[17]:,23
pyTSA_TSComposition,This Quantlet  produces the examples for various ways of composition of simple time series and plots results(Figures 2.10-2.11 from the Book).,"time series, additive composition, multiplicative composition, visualisation","Huang Changquan, Alla Petukhina",pyTSA_TSComposition,1,numpy pandas matplotlib,3
FRM_1_Marketcap_scraping,"1)scrape the top 200 coin prices and total market cap of the Crptocurrency market over 2 years
 2)souce: https://coinmarketcap.com/historical/
 3)startdate='20170101', enddate='20191120', weekly data","Cryptocurrency, market capitalization, web scraping, BeautifulSoap, coinmarketcap","Qi Wu, Seokhee Moon",FRM_1_Marketcap_scraping,2,!/usr/bin/env python  coding: utf-8  ## Market Cap for 2017-2019 for 200 coins as well as total market Cap  In[22]: pandas numpy requests re bs4 BeautifulSoup time sleep  Create date range for historical snapshots  Retrieve historical snapshot data from date  Extract html containers for each coin  Extract total Market Cap of the day  convert the string into float  Delete the first redundant column  Create data frame of data  Write data to file,71
alphashape_animation,Animated illustration of alpha shapes for different values of alpha.,ERROR,Jovanka Lili Matic,alphashape_animation,2,"!/usr/bin/env python  coding: utf-8  In[2]: scipy.spatial Delaunay numpy matplotlib matplotlib.animation FuncAnimation IPython.display HTML Code citation from https://stackoverflow.com/questions/23073170/calculate-bounding-polygon-of-alpha-shape-from-the-delaunay-triangulation alpha_shape points  alpha  only_outer=True add_edge edges  i  j  already added  if both neighboring triangles are in shape  it's not a boundary edge  Loop over triangles:  ia  ib  ic = indices of corner points of the triangle  In[3]:  In[4]: matplotlib.pyplot  Computing the alpha shape  Plotting the output plt.rcParams[""axes.grid""] = False  In[19]: numpy matplotlib matplotlib.animation FuncAnimation update item  anim.save 'alpha_transparent.mov'  codec=""png""  dpi=100  bitrate=-1  savefig_kwargs={'transparent': True  'facecolor': 'none'})  HTML anim.to_html5_video ))",85
CSC_Parsing,"Parsing of smart contracts, data frame creation",ERROR,"Elizaveta Zinovyeva, Raphael Constantin Georg Reule",CSC_Parsing,6,!/usr/bin/env python  coding: utf-8  In[3]: os numpy pandas re json from keras.preprocessing.text import Tokenizer from keras.preprocessing.sequence import pad_sequences  In[4]:  In[5]: check_json row  In[7]:  In[9]:  functions contract_name_extract data extract contract name function_name_extract data extract function names and join to one string comments_extract data extract contract comments and join to one text  find all occurance streamed comments  /*COMMENT */) from string  find all occurance singleline comments  //COMMENT\n ) from string code_no_punct data  In[11]:  In[12]:  In[56]:  In[ ]:  In[38]:  In[12]:  In[16]:  In[17]:  In[ ]:,81
CSC_Parsing,"Parsing of smart contracts, data frame creation",ERROR,"Elizaveta Zinovyeva, Raphael Constantin Georg Reule",CSC_Parsing,6,!/usr/bin/env python  coding: utf-8  # Search for derivatives  In[7]: os numpy pandas re  In[8]:  In[9]:  In[12]:  print f  ' is empty')  In[15]:  In[35]:  try:  except:         pass  print f  ' is empty')  In[37]:  In[38]:  In[ ]:,35
CSC_Parsing,"Parsing of smart contracts, data frame creation",ERROR,"Elizaveta Zinovyeva, Raphael Constantin Georg Reule",CSC_Parsing,6,!/usr/bin/env python  coding: utf-8  In[12]: requests pandas numpy sys time os matplotlib pandas re  In[3]:  In[ ]: check_json row  In[4]:  functions contract_name_extract data extract contract name function_name_extract data extract function names and join to one string comments_extract data extract contract comments and join to one text  find all occurance streamed comments  /*COMMENT */) from string  find all occurance singleline comments  //COMMENT\n ) from string code_no_punct data  In[15]:  In[19]:  In[16]: try: except:  In[14]:  In[11]:  In[ ]:,75
pyTSA_ARMA,"This Quantlet simulates and plots ARMA(2,2) - autoregressive moving average process time series, its ACF and PACF","time series,  stationarity, autocorrelation, PACF, ACF, simulation, stochastic process, ARMA, moving average, autoregression",ERROR,pyTSA_ARMA,1,numpy pandas matplotlib statsmodels.tsa.arima_process arma_generate_sample PythonTsa.plot_acf_pacf acf_pacf_fig,7
RVOLprep,"Imports the raw data from six cryptocurrencies, builds a price weighted portfolio from all of them and calculated log returns and realized volatility. Additionally, it gives more information about the data with some graphs and descriptive statistics.","cryptocurrency, returns, RV, realized, volatility, price, weighted, portfolio",Ivan Mitkov,RVOLprep,2,"!/usr/bin/env python3  -*- coding: utf-8 -*- warnings  Import of packages  set wordking directiory os  Please note that the code works without any errors only if you run the whole script at once  If you run partial codes then you have to replace manually the working directory with your path  os.chdir r'/hier/comes/your/path/link') dalib.packages  Selecting only the ""open"" prices Graph of the daily prices assuming the day starts with the first observation of our data frame  Descriptive statistics. We concentrate only in the used in our empirical work period  Calculating portfolio  Visualization portfolio  Log returns for 5 mins frequency logreturns series  Log return visualization of our portfolio  Slicing the data frame in order not to have missingness  Saving a new column with the data  Calculating the all kinds of log returns and realized volatilites  which are necessery for the master thesis log_returns series  dateseries numpy sqrt  Apply log returns on both data frames      Saving the prepared data for further analysis",158
RVOLprep,"Imports the raw data from six cryptocurrencies, builds a price weighted portfolio from all of them and calculated log returns and realized volatility. Additionally, it gives more information about the data with some graphs and descriptive statistics.","cryptocurrency, returns, RV, realized, volatility, price, weighted, portfolio",Ivan Mitkov,RVOLprep,2,!/usr/bin/env python3  -*- coding: utf-8 -*- warnings  Make sure that all of the modules are already installed in Anaconda. If not    the following comands should be ran in Terminal  Improting the packages pickle pandas pandas Series matplotlib numpy statsmodels.graphics.tsaplots plot_acf statsmodels statsmodels.tsa.stattools adfuller statsmodels.tsa.stattools adfuller statsmodels.stats.diagnostic itertools sys pylab scipy scipy.stats kurtosis scipy.stats skew scipy.stats norm scipy.stats jarque_bera numpy.random normal math sklearn.preprocessing MinMaxScaler sklearn.metrics mean_squared_error keras.layers keras optimizers keras.constraints NonNeg keras.layers.advanced_activations,71
SL_2019_alpha_centauri_pirates_wordcloud,Creates a word cloud picture of frequently used words in Robert L. Stevensons Treasure Island. Based on another piece of python code by verani. Then creates another word clud	picture of frequently used words in the blurbs of the 1999 video game Alpha Centauri.,"LDA, word cloud, topic modelling, pirates, word frequency, treasure island, Alpha Centauri, Stars","Miriam Koberl, Philipp Schneider",SL_2019_Alpha_Centauri_Pirates_Wordcloud,1, -*- coding: utf-8 -*-  treasure island wordcloud  based on the christmas song wordcloud by verani. !/usr/bin/env python3  -*- coding: utf-8 -*- Please install these modules before you run the code  !pip install wordcloud !pip install nltk !pip install Pillow !pip install numpy !pip install gensim !pip install pandas ############################################################   Part I: Pirates wordcloud  ############################################################  matplotlib re os os path PIL Image wordcloud WordCloud numpy  Please make sure that all the required files are in your working directory  or change the working directory to your path!  keep only letters  numbers and whitespace  apply regex  lower case   Read the whole text.  Mask  Optional additional stopwords  Construct Word Cloud  no backgroundcolor and mode = 'RGBA' create transparency  Pass Text  store to file ############################################################   Part II: Alpha Centauri wordcloud  ############################################################   Please change the working directory to your path!  Read the whole text.  Mask  Construct Word Cloud  no backgroundcolor and mode = 'RGBA' create transparency  Pass Text  store to file  to show the picture   ,159
DEDA_NLP,ERROR,ERROR,Junjie Hu,DEDA_NLP,1,os pandas sudachipy tokenizer  pip3 install SudachiPy if package not found  CorePackage:  pip install https://object-storage.tyo2.conoha.io/v1/nc_2520839e1f9641b08211a5c85243124a/sudachi/SudachiDict_core-20191030.tar.gz  More information: https://github.com/WorksApplications/SudachiPy wordcloud WordCloud matplotlib TokenCleanText tker  mode  text  Read and Pre-process Data  Define Stopping Words  Define stop words and punctuation  not perfect!  Tokenize Text  Create WordCloud,43
Quantlet_Extraction_Evaluation_Visualisation,"Extraction, grading and clustering of the Quantlets in the GitHub Organization Quantlet with the use of the classes modules/QUANTLET.py and modules/METAFILE.py. With this program you can extract, update and save the data, model topics with Latent Semantic Analysis, compute different clusterings and visualize the clustering with t-Stochastic Neighbour embedding.","Text analysis, LSA, t-SNE, clustering, kmeans clustering, spectral clustering, visualisation",Marius Sterling,Quantlet,3,modules.QUANTLET QUANTLET os  Creates if necessary the folders in the list  looks for already saved files  if there is none  loads all data and save them  Update all existing metafiles in q  Update all existing metafiles and searches for new Quantlets  Saving data newly  return bad graded quantlets  Extract corpus and dictionary  document term matrix dtm  do tf-idf and extract document topic  matrix X  cluster the Quantlets with K-Means into groups  ,71
Quantlet_Extraction_Evaluation_Visualisation,"Extraction, grading and clustering of the Quantlets in the GitHub Organization Quantlet with the use of the classes modules/QUANTLET.py and modules/METAFILE.py. With this program you can extract, update and save the data, model topics with Latent Semantic Analysis, compute different clusterings and visualize the clustering with t-Stochastic Neighbour embedding.","Text analysis, LSA, t-SNE, clustering, kmeans clustering, spectral clustering, visualisation",Marius Sterling,Quantlet,3, -*- coding: utf-8 -*- itertools numpy nltk.corpus stopwords __init__ self  file  repo  content  commits                 [i for i in file_type if i not in ['png' 'jpg' 'jpeg' 'pdf']] pre_clean c yaml_debugger x clean_keys d  import copy; d = copy.deepcopy tmp)  [{k:v} for k v in d.items ) if '[' in k]:  TODO check with string distance which field is meant and combine list_to_string self create_keyword_list self __grading self content  quality of metainfo file  number of keywords  number of words in discription  number of words without stopwords in discription  indication why grade worse than A was given  number of pictures in   submission year _refs = [c.name.split '.')[0] for c in _contents if c.name.split '.')[1] in ['png' 'jpg' 'jpeg']] _pdfs = [c for c in _pdfs if c.name.split '.')[0] in _refs],128
Quantlet_Extraction_Evaluation_Visualisation,"Extraction, grading and clustering of the Quantlets in the GitHub Organization Quantlet with the use of the classes modules/QUANTLET.py and modules/METAFILE.py. With this program you can extract, update and save the data, model topics with Latent Semantic Analysis, compute different clusterings and visualize the clustering with t-Stochastic Neighbour embedding.","Text analysis, LSA, t-SNE, clustering, kmeans clustering, spectral clustering, visualisation",Marius Sterling,Quantlet,3," -*- coding: utf-8 -*- numpy pandas datetime github Github sklearn.feature_extraction.text CountVectorizer collections Counter scipy.sparse csc_matrix nltk.corpus stopwords nltk.stem.wordnet WordNetLemmatizer gensim.models Phrases gensim.models.phrases Phraser gensim.corpora Dictionary gensim.matutils corpus2dense sklearn.decomposition TruncatedSVD sklearn.pipeline make_pipeline sklearn.preprocessing Normalizer sklearn.cluster KMeans sklearn.metrics pairwise sklearn.manifold MDS tqdm tqdm gensim.sklearn_api TfIdfTransformer matplotlib time sleep modules.METAFILE METAFILE __init__ self  github_token=None  user=None stop_until_rate_reset self at_least_remaining=None __download_metafiles_from_repository self  repo  server_path='.'  override=None  get repo content from directory server_path update_existing_metafiles self update_all_metafiles self since=None download_metafiles_from_user self  repo_name=None  override=True save self  filepath load filepath grading self  save_path=None grades_equals=None get_last_commit self get_recently_changed_repos self  since create_readme self repos=None __readme_template name_of_quantlet  metainfo_original  pics  quantlet # [<img src=""https://github.com/QuantLet/Styleguide-and-FAQ/blob/master/pictures/qloqo.png"" alt=""Visit QuantNet"">] http://quantlet.de/) **') ## %s Code\n```%s' %  lang.upper )  lang))  see https://developer.github.com/v3/git/trees/#create-a-tree                     element = InputGitTreeElement '/'.join b.path.split '/')[:-1]) +'/'+'README.md'  '100644'  'blob'  readme.decode ))                         element = InputGitTreeElement v.directory +'/'+'README.md'  '100644'  'blob'  readme) text_preprocessing text \$\%\&\' \)\*\+\.\/\:\<\=\>\?\@\[\\\]\^\`\{\|\}\~\-_\ \;]""  re.UNICODE)  creating cleaned  see text_preprocessing) texts  excluding empty texts  {k:v for k v in docs_clean.items ) for i in v if len i)>15}  Add bigrams and trigrams to docs_clean  only ones that appear 10 times or more). txt_to_list txt  Filter out words that occur in less than leave_tokens_out_with_less_than_occurence  documents   or more than leave_tokens_out_with_ratio_of_occurence of the documents.  get bag-of-words representation of each quantlet sys.stdout = open os.devnull  'w') sys.stdout = sys.__stdout__ get_document_term_matrix self  corpus  dictionary get_SVD_explained_variance_ratio self  tdm  with_normalize=False get_corpus_tfidf self corpus dictionary lsa_model self  corpus  dictionary  num_topics=10 get_lsa_matrix self  lsa  corpus  dictionary cl_kmeans self  X  n_clusters cl_spectral self X n_clusters dist_metric='euclidean' From scikit - learn: [‘cityblock’  ‘cosine’  ‘euclidean’  ‘l1’  ‘l2’  ‘manhattan’].These metrics support sparse matrix inputs. From scipy.spatial.distance: [‘braycurtis’  ‘canberra’  ‘chebyshev’  ‘correlation’  ‘dice’  ‘hamming’  ‘jaccard’  ‘kulsinski’   ‘mahalanobis’  ‘matching’  ‘minkowski’  ‘rogerstanimoto’  ‘russellrao’  ‘seuclidean’  ‘sokalmichener’  ‘sokalsneath’  ‘sqeuclidean’  ‘yule’]  See the documentation for scipy.spatial.distance for details on these metrics.These metrics do not support sparse matrix inputs. cl_agglomerative self X  n_clusters  dist_metric='euclidean'  linkage= 'ward' cl_dbscan_n_cluster self X n_cluster dist_metric='euclidean' maxIter = 100  verbose = False lower=0 upper=40 half_find lower upper count=0 maxIter=maxIter verbose=verbose cl_dbscan self X eps dist_metric='euclidean' cl_dbscan_grid self  X  eps_grid  dist_metric='euclidean' n_cluster=None topic_labels self cl document_topic_matrix lsa top_n=5 take_care_of_bigrams=True clustering self  n_clusters top_n_words=5 tfidf=True cluster_algo='kmeans' dist_metric='euclidean' linkage=None directory='data/' file_ending n cluster_algo dist_metric linkage  need to be executed such that d.id2token is computed create_datanames_file self directory='' save_qlet_repo_file self  cluster_label file_name_ending  directory=''  TODO: Safe as JSON and read from file!!! correct_book pub __create_template v cluster id  TODO combine all tsne self X cluster_labels n_iter=5000 dist_metric='euclidean' DPI=150  save_directory='' save_ending='' file_type='pdf'",392
CrypOpt_ImpliedVola,Uses parameters of option to approximates the implied volatility via the Black-Scholes-Formula (red). Compares to implied volatility from Deribit API (blue).,"implied volatility, bitcoin, options",Franziska Wehrmann,CrypOpt_ImpliedVola,1,os pandas numpy scipy.stats norm matplotlib pyplot BSValue S  r  sigma  K  T  option  if negative  need to lower sigma  BS was too high)  if we crossed 0  we change sigma in smaller steps  did we stop because of convergance  or because max iterations  ------------------------------------------------------------------------ MAIN  ------------------------------------------------------------------- LOAD DATA  ---------------------------------------------------------------- CALCULATE IV  ------------------------------------------------------------------------ PLOT  blue: Deribit IV  red: BS IV,60
AOBDL_BERT,"Antisocial Online behaivor detection. This part is dedicated to pre-trained deep transformers, such as BERT and DistilBERTI",ERROR,Elizaveta Zinovyeva,AOBDL_BERT,2,!/usr/bin/env python  coding: utf-8  In[ ]: os numpy pandas gc  In[ ]: 2. Get the file  GOOGLE COLAB SETUP  Load the Drive helper and mount google.colab drive  This will prompt for authorization.  In[ ]: 3. Read file as panda dataframe  CHANGE TRAIN AND TEST  MIX TO GET SIMILAR DISTRIBUTION sklearn.model_selection train_test_split  In[ ]:  In[ ]:  In[ ]: warnings numpy pandas tensorflow tensorflow.keras.layers Dense tensorflow.keras.optimizers RMSprop tensorflow.keras.models Model tensorflow.keras.callbacks ModelCheckpoint transformers traitlets matplotlib seaborn tqdm.notebook tqdm tokenizers BertWordPieceTokenizer sklearn.metrics roc_auc_score tensorflow.keras backend sklearn.metrics roc_auc_score sklearn.metrics precision_score  In[ ]:  Create strategy from tpu  In[ ]: Transformer implementation is based on the following code: https://www.kaggle.com/miklgr500/jigsaw-tpu-bert-with-huggingface-and-keras  In[ ]:  First load the real tokenizer  Save the loaded tokenizer locally  Reload it with the huggingface tokenizers library  In[ ]: fast_encode texts  tokenizer  chunk_size=256  maxlen=512  In[ ]: build_model transformer  loss='binary_crossentropy'  max_len=512 x = tf.keras.layers.Dropout 0.35) cls_token)  In[ ]: sklearn.model_selection StratifiedKFold  In[ ]:  In[ ]:  In[ ]:  Tokenization within the CV to ensure not having overfitting model.layers[1].trainable = False  ### CV results  In[ ]:  In[ ]:  In[ ]:  In[ ]:  In[ ]:  In[ ]:  In[ ]:  In[ ]:  In[ ]:  In[ ]: model.layers[1].trainable = False  In[ ]:  In[ ]: validation_data=test_dataset   In[ ]:  In[ ]:  In[ ]:  In[ ]:  In[ ]:  In[ ]:  In[ ]:,206
DEDA_Class_SS2018_offerpage_scraper,"Scripts written in python to scrape details of room offers on wg-gesucht.de. It is done in two stages, first from the list of offers available and second the detail description of an offer.","Berlin, housing, expats, classification, advertisement, deposit, scam, rent",Jessie Hsieh,outdated_scraper,3,"!/usr/bin/env python3  -*- coding: utf-8 -*- pandas time selenium webdriver selenium.webdriver.common.by By selenium.webdriver.support.ui WebDriverWait selenium.webdriver.support expected_conditions selenium.common.exceptions TimeoutException selenium.webdriver.support.ui Select random randint  example exception option.add_argument ""--incognite"")  create new instance of chrome in incognite mode  take a nap first  go to website of interest     browser.get ""http://www.dpd.co.uk/apps/tracking/?reference=4454155341#results"")  wait up to 20 seconds for the page to load  the core to click and send message  //*[@id=""main_column""]/div[1]/div/div[18]/div/div/div[5]  before the pop-up window closes  browser.remove parent_h)  seconds //*[@id=""sicherheit_bestaetigung""]",72
DEDA_Class_SS2018_offerpage_scraper,"Scripts written in python to scrape details of room offers on wg-gesucht.de. It is done in two stages, first from the list of offers available and second the detail description of an offer.","Berlin, housing, expats, classification, advertisement, deposit, scam, rent",Jessie Hsieh,outdated_scraper,3,requests requests.auth HTTPBasicAuth time bs4 BeautifulSoup datetime os csv pandas random randint clean_text text clean_spaces text get_miete_breakdown soup_obj get_address_details soup_obj get_wohnung_details soup_obj get_text soup_obj get_membership soup_obj extract_offer offer_link  page_html  column_names  a function to scrape all information of the offer details in the offer page save_offerpage_html html_doc  path  link  a function to save the list page in html for debug and analysis  a function to be applied to every link in the offers collection  and calls the scraper functions OfferScraper_fromLink offer_link  wait a minute until it searches the webpage  package the request  send the request and catch the response  get the html of the list page  a function save them for analysis in the future  path for stored html documents of list pages  find the details from the webpage  join the detail found to the collection,135
DEDA_Class_SS2018_offerpage_scraper,"Scripts written in python to scrape details of room offers on wg-gesucht.de. It is done in two stages, first from the list of offers available and second the detail description of an offer.","Berlin, housing, expats, classification, advertisement, deposit, scam, rent",Jessie Hsieh,outdated_scraper,3,requests time bs4 BeautifulSoup datetime os selenium webdriver selenium.webdriver.common.by By selenium.webdriver.support.ui WebDriverWait selenium.webdriver.support expected_conditions selenium.common.exceptions TimeoutException pandas random randint clean_text text get_entrag entrag_element get_miete miete_element get_groesse groesse_element get_stadtteil stadtteil_element get_freiab freiab_element get_freibis freibis_element get_bewohner bewohner_element get_offer_link element def write_offer_to_csv csv_file  offer_list):  a function to write the scraped offer into the csv file prepared 	with open csv_file  'w') as f: 		writer = csv.writer f) 		writer.writerow offer_list) 	print 'written offer into csv!') scrap_list_page page_html  df  a function to scrape all information of the flat offers in the list page ## CHECK 			write_offer_to_csv csv_file_name  offer_list) save_listpage_html html_doc  path  page_number  a function to save the list page in html for debug and analysis ##CHECK get_list_page page_number  df  It processes the list page  grab the basic info  like members in the flat  area  title  the rent and the link  define the URL of the list page  wait a minute until it searches the webpage  package the request  send the request and catch the response  get the html of the list page  a function save them for analysis in the future  a function to scrape all information of the flat offers in the list page  path for stored html documents of list pages with open csv_file_name  'w') as f:  a function to run everyday to collect data from 20 pages,214
LDA-DTM_Shakespeare_LDA,"Analyse the word frequency and the topic distribution of Shakespeare's works: Hamlet, Julius Caesar and Romeo and Juliet","LDA, Shakespeare, topic modelling, word frequency, word cloud",Xinwen Ni,LDA-DTM_Shakespeare,4,"!/usr/bin/env python3  -*- coding: utf-8 -*- please install these modules before you run the code  !pip install wordcloud !pip install Pillow os path os PIL Image numpy matplotlib wordcloud WordCloud  Please change the working directory to your path! os.chdir ""/Users/xinwenni/LDA-DTM/Shakespeare"")   Read the whole text.  Mask  Optional additional stopwords  Construct Word Cloud  no backgroundcolor and mode = 'RGBA' create transparency  Pass Text  store to file  to show the picture   ",68
LDA-DTM_Shakespeare_LDA,"Analyse the word frequency and the topic distribution of Shakespeare's works: Hamlet, Julius Caesar and Romeo and Juliet","LDA, Shakespeare, topic modelling, word frequency, word cloud",Xinwen Ni,LDA-DTM_Shakespeare,4,"!/usr/bin/env python3  -*- coding: utf-8 -*- please install the modules before you run this code: !pip install matplotlib !pip install nltk !pip install gensim os re nltk os path nltk.stem WordNetLemmatizer nltk.stem.porter PorterStemmer nltk.corpus stopwords nltk  Importing Gensim gensim gensim corpora  Please change the working directory to your path! os.chdir ""/Users/xinwenni/LDA-DTM/Shakespeare"")  doc_l.pop )[0]  Regex cleaning  keep only letters  numbers and whitespace  apply regex  lower case  string clean doc  Creating the term dictionary of our courpus  where every unique term is assigned an index.  Converting list of documents  corpus) into Document Term Matrix using dictionary prepared above.  Creating the object for LDA model using gensim library  Running and Trainign LDA model on the document term matrix.",115
LDA-DTM_Shakespeare_LDA,"Analyse the word frequency and the topic distribution of Shakespeare's works: Hamlet, Julius Caesar and Romeo and Juliet","LDA, Shakespeare, topic modelling, word frequency, word cloud",Xinwen Ni,LDA-DTM_Shakespeare,4,"!/usr/bin/env python3  -*- coding: utf-8 -*-  please install these modules before run this code :  !pip install matplotlib !pip install nltk matplotlib re nltk.corpus stopwords os os path nltk  Please change the working directory to your path! os.chdir ""/Users/xinwenni/LDA-DTM/Shakespeare"")  text_file_temp= text_file.replace ""\n"" "" "")  Convert to string  Regex cleaning  keep only letters  numbers and whitespace  apply regex  lower case   apply regex  lower case   apply regex  lower case   apply regex  lower case   Save dictionaries for wordcloud  Filter Stopwords lst = [ value  key) for  key  value) in dict2.items )] lst.sort reverse=True) top_word=lst[: 10 ]  Resort in list  Reconvert to dictionary SequenceSelection dictionary  length  startindex = 0  length is length of highest consecutive value vector  Test input these 34 words are given as an example in the book from L.Borke & W. Haerdle  Plot most frequent words n = range len dictshow)) plt.bar n  dictshow.values )  align='center') plt.xticks n  dictshow.keys )) plt.title ""Most frequent Words"") plt.savefig ""FrequentWords.png""  transparent=True)",156
LDA-DTM_Shakespeare_LDA,"Analyse the word frequency and the topic distribution of Shakespeare's works: Hamlet, Julius Caesar and Romeo and Juliet","LDA, Shakespeare, topic modelling, word frequency, word cloud",Xinwen Ni,LDA-DTM_Shakespeare,4,"!/usr/bin/env python3  -*- coding: utf-8 -*- please install these module before you run the code : !pip install matplotlib !pip install nltk !pip install pandas re nltk.corpus stopwords os os path pandas nltk.tokenize RegexpTokenizer nltk.stem.porter PorterStemmer  Please change the working directory to your path! os.chdir ""/Users/xinwenni/LDA-DTM/Shakespeare"")  BasicCleanText raw_text  keep only letters  numbers and whitespace  apply regex  lower case   Tokenization  create English stop words list en_stop = get_stop_words 'en')  remove stop words from tokens stopped_tokens = [i for i in tokens if not i in en_stop]  Create p_stemmer of class PorterStemmer  stem token inport the texts WordFrequency words these 34 words are given as an example in the book from L.Borke & W. Haerdle matplotlib seaborn cmap = sns.cubehelix_palette start = 1  rot = 3  gamma=0.8  as_cmap = True) sns.heatmap data_heatmap  cmap = cmap  linewidths = 0.05  ax = ax)",139
LDA-DTM_NASDAQ,"LDA application with NASDAQ news, the word cloud, and word frequency","LDA, NASDAQ news, topic modelling, word cloud, word frequency",Xinwen Ni,LDA-DTM_nasdaq,2,"!/usr/bin/env python3  -*- coding: utf-8 -*- please install the modules before you run this code: !pip install nltk !pip install numpy !pip install matplotlib !pip install Pillow  please download the data from   https://drive.google.com/open?id=1YU6WgUffJj73q23tMB1ipUUzTzXSI6RJ os json re nltk nltk.stem WordNetLemmatizer nltk.stem.porter PorterStemmer nltk.corpus stopwords wordcloud WordCloud numpy os path matplotlib nltk PIL Image  Please change the working directory to your path! os.chdir ""/Users/xinwenni/LDA-DTM/nasdaq"")   keep only letters  numbers and whitespace  apply regex  lower case   Mask  Optional additional stopwords  Construct Word Cloud  no backgroundcolor and mode = 'RGBA' create transparency  Pass Text  store to file  to show the picture   关掉图像的坐标  Filter Stopwords nltk SequenceSelection dictionary  length  startindex = 0  length is length of highest consecutive value vector  Test input  Plot most frequent words  find the crypto currency related words frequency",127
LDA-DTM_NASDAQ,"LDA application with NASDAQ news, the word cloud, and word frequency","LDA, NASDAQ news, topic modelling, word cloud, word frequency",Xinwen Ni,LDA-DTM_nasdaq,2,!/usr/bin/env python3  -*- coding: utf-8 -*-  Run in python console nltk re numpy pandas pprint pprint  Gensim gensim gensim gensim.utils simple_preprocess gensim.models CoherenceModel  spacy for lemmatization spacy  Plotting tools pyLDAvis pyLDAvis  don't skip this matplotlib matplotlib inline  Enable logging for gensim - optional logging warnings  Run in terminal or command prompt python3 -m spacy download en nltk.corpus stopwords  Import Dataset  Convert to list  Remove Emails  Remove new line characters  Remove distracting single quotes sent_to_words sentences  deacc=True removes punctuations  Build the bigram and trigram models  higher threshold fewer phrases.  Faster way to get a sentence clubbed as a trigram/bigram  See trigram example  Define functions for stopwords  bigrams  trigrams and lemmatization remove_stopwords texts make_bigrams texts make_trigrams texts lemmatization texts  allowed_postags=['NOUN'  'ADJ'  'VERB'  'ADV']  Remove Stop Words  Form Bigrams  Initialize spacy 'en' model  keeping only tagger component  for efficiency)  python3 -m spacy download en  Do lemmatization keeping only noun  adj  vb  adv  Create Dictionary  Create Corpus  Term Document Frequency  View Gensim creates a unique id for each word in the document. The produced corpus shown above is a mapping of  word_id  word_frequency). For example   0  1) above implies  word id 0 occurs once in the first document. Likewise  word id 1 occurs twice and so on.  Human readable format of corpus  term-frequency)  Print the Keyword in the 10 topics  Compute Perplexity  a measure of how good the model is. lower the better.  Compute Coherence Score  Visualize the topics,235
GUBA_crawl,Project_Group8 file,"NUS, FE5225, tieba, stock comment, crawl",Wang Yuzhou,GUBA_crawl,5," -*- coding: utf-8 -*-  Define here the models for your spider middleware  See documentation in:  https://docs.scrapy.org/en/latest/topics/spider-middleware.html scrapy signals logging base64  import logging scrapy.downloadermiddlewares.useragent UserAgentMiddleware  隧道服务器  隧道id和密码  代理中间件 process_request self  request  spider  设置代理  设置代理身份认证  Python3 写法  Python2 写法  auth = ""Basic "" + base64.b64encode '%s:%s' %  tid  password)) __init__ self  user_agent='' process_request self  request  spider",54
GUBA_crawl,Project_Group8 file,"NUS, FE5225, tieba, stock comment, crawl",Wang Yuzhou,GUBA_crawl,5, -*- coding: utf-8 -*-  Scrapy settings for teiba project  For simplicity  this file contains only settings considered important or  commonly used. You can find more settings consulting the documentation:      https://docs.scrapy.org/en/latest/topics/settings.html      https://docs.scrapy.org/en/latest/topics/downloader-middleware.html      https://docs.scrapy.org/en/latest/topics/spider-middleware.html  Crawl responsibly by identifying yourself  and your website) on the user-agent USER_AGENT = 'teiba  +http://www.yourdomain.com)'  USER_AGENT = 'Hoteldata  +http://www.yourdomain.com)'  Obey robots.txt rules  ROBOTSTXT_OBEY = True  将此变量设为False  提高成功率  DOWNLOADER_MIDDLEWARES = {      'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware': 110       'teiba.middlewares.ProxyMiddleware': 100       'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': 2       'teiba.middlewares.AgentMiddleware': 1   }  Configure maximum concurrent requests performed by Scrapy  default: 16) CONCURRENT_REQUESTS = 32  Configure a delay for requests for the same website  default: 0)  See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay  See also autothrottle settings and docs DOWNLOAD_DELAY = 3  The download delay setting will honor only one of: CONCURRENT_REQUESTS_PER_DOMAIN = 16 CONCURRENT_REQUESTS_PER_IP = 16  Disable cookies  enabled by default) COOKIES_ENABLED = False  Disable Telnet Console  enabled by default) TELNETCONSOLE_ENABLED = False  Override the default request headers: DEFAULT_REQUEST_HEADERS = {    'Accept': 'text/html application/xhtml+xml application/xml;q=0.9 */*;q=0.8'     'Accept-Language': 'en'  }  Enable or disable spider middlewares  See https://docs.scrapy.org/en/latest/topics/spider-middleware.html SPIDER_MIDDLEWARES = {     'teiba.middlewares.TeibaSpiderMiddleware': 543  }  Enable or disable downloader middlewares  See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html DOWNLOADER_MIDDLEWARES = {     'teiba.middlewares.TeibaDownloaderMiddleware': 543  }  Enable or disable extensions  See https://docs.scrapy.org/en/latest/topics/extensions.html EXTENSIONS = {     'scrapy.extensions.telnet.TelnetConsole': None  }  Configure item pipelines  See https://docs.scrapy.org/en/latest/topics/item-pipeline.html  Enable and configure the AutoThrottle extension  disabled by default)  See https://docs.scrapy.org/en/latest/topics/autothrottle.html AUTOTHROTTLE_ENABLED = True  The initial download delay AUTOTHROTTLE_START_DELAY = 5  The maximum download delay to be set in case of high latencies AUTOTHROTTLE_MAX_DELAY = 60  The average number of requests Scrapy should be sending in parallel to  each remote server AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0  Enable showing throttling stats for every response received: AUTOTHROTTLE_DEBUG = False  Enable and configure HTTP caching  disabled by default)  See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings HTTPCACHE_ENABLED = True HTTPCACHE_EXPIRATION_SECS = 0 HTTPCACHE_DIR = 'httpcache' HTTPCACHE_IGNORE_HTTP_CODES = [] HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'  DEBUG WARNING  DOWNLOAD_DELAY = 0.25,291
GUBA_crawl,Project_Group8 file,"NUS, FE5225, tieba, stock comment, crawl",Wang Yuzhou,GUBA_crawl,5,scrapy.crawler CrawlerProcess scrapy.utils.project get_project_settings os   你需要将此处的spider_name替换为你自己的爬虫名称,6
GUBA_crawl,Project_Group8 file,"NUS, FE5225, tieba, stock comment, crawl",Wang Yuzhou,GUBA_crawl,5, -*- coding: utf-8 -*-  Define here the models for your scraped items  See documentation in:  https://docs.scrapy.org/en/latest/topics/items.html scrapy  define the fields for your item here like:  name = scrapy.Field )  exception = scrapy.Field ),33
GUBA_crawl,Project_Group8 file,"NUS, FE5225, tieba, stock comment, crawl",Wang Yuzhou,GUBA_crawl,5, -*- coding: utf-8 -*-  Define your item pipelines here  Don't forget to add your pipeline to the ITEM_PIPELINES setting  See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html json jsonlines os pymongo logging process_item self  item  spider  class JsonlinePipeline object):      def open_spider self  spider):          db_dir = r'C:\Users\ASUS\Desktop\teiba\teiba\db'          # stock_set = set file.split '.')[0] for file in os.listdir db_dir))          # stock_set = set file for file in os.listdir db_dir))          self.db_dir = db_dir          # self.stock_set = stock_set          # self.file = open r'C:\Users\ASUS\Desktop\HotelData\Hoteldata\database.jsonl' 'a' encoding='utf-8')      def close_spider self  spider):          # self.file.close )          pass      @staticmethod      def parse_item_id item_id):          type_  stock_id  news_id = item_id.split ' ')          new_id  page = news_id.split '_')          return type_  stock_id  new_id  page      def process_item self  item  spider):          item_id = item.get 'item_id')          if item_id:              type_  stock_id  new_id  page = self.parse_item_id item_id)              stock_path = os.path.join self.db_dir stock_id)              new_path = os.path.join stock_path new_id)              if not os.path.exists stock_path):                  os.mkdir stock_path)              elif not os.path.exists new_path):                  os.mkdir new_path)              # if stock_id not in self.stock_set:              #     stock_file_path = os.path.join self.db_dir stock_id)              #     if not os.path.exists stock_file_path):              #         os.makedirs stock_file_path)              #     self.stock_set.add stock_id)              with open os.path.join self.db_dir stock_id new_id f'{page}.json') 'w' encoding='utf-8') as f:                  jsontext = json.dumps dict item) ensure_ascii=False indent=4)                  f.write jsontext)          return item __init__ self  mongo_uri  mongo_db from_crawler cls  crawler open_spider self  spider process_item self  item  spider close_spider self  spider,206
SMSclus8pa,Employs the average linkage using squared Euclidean distance matrices to perform a cluster analysis on an 8 points example,"cluster-analysis, dendrogram, linkage, euclidean, distance, plot",Awdesch Melzer,SMSclus8pa,1,numpy matplotlib scipy.cluster hierarchy  agglomarate first two nearest points  add third point via average  agglomerate second two nearest points  agglomarate third two nearest points  add point six to second subcluster via average  compute subcluster between 345 and 678 via average  compute in case of merging of two last clusters:,49
Step2. HMM Algorithms Breakdown,Give a break down of the related algorithms in HMM – in Casino case of fair and loaded dice data,ERROR,ERROR,Step2. HMM Algorithms Breakdown,1,!/usr/bin/env python3  -*- coding: utf-8 -*- numpy pandas ============================================== 0. Input ==============================================  Observation O_t Parameters name: index name: index # HMM-related parameters  initial probs of hidden states {0  1}  [0: fair dice; 1: Loaded dice]; p S_0=1) = 1-pi_0 Transition matrix T = [p_{i j}] = [p Sj|Si)] Emission matrix E = [q_i^y] = [p O=y|S=i)] ============================================== 1. Alpha ============================================== Build_Alpha Obs  Pi  P  E Initial: Iterate: ============================================== 2. Beta ============================================== Build_Beta Obs  Pi  P  E Initial: Iterate: Termination:  Decoding1 matrix is p S_t=k|{O_t}_{t=1}^T) ============================================== 3. Viterbi & Traceback ================================================== Build_Viterbi Obs  Pi  P  E Initial: Iterate: Viterbi_Traceback V Termination: Iterate:,101
SDA_2019_St_Gallen_SMART_Sentiment_Analysis_Web_scraping,"This Quantlet scrapes the news website Finanzen.net for pre-specified articles that are linked to a specific stock. Afterwards, this Quantlet assigns sentiment weights to the words based on the stock movement followed by the publication.","Web scraping, word mining, sentiment analysis, automatic sentiment analysis, stock-market news",Oliver Kostorz,Web scraping,1,Import packages bs4 BeautifulSoup requests yfinance stop_words get_stop_words re datetime fuzzywuzzy process datetime pandas numpy math pickle os json Set working device ##Functions Clean html tags in string remove_html_tags text Get stock symbol getCompany text Rounds time downwards to earlier five minute interval roundtime time ##Data mining List containing relevant URLs for news-sample  Unpickling Add additional links to news articles to use for training the algorithm Refer to Meta-Information for full guide and requirements Pickling Defines dataframe to collect relevant info Define stopwords  not necessary but implemented for schooling purposes) Loop through all articles to extract relevant words and allocate SMART sentiment weights ##Data processing Finanzen.net specific code to extract news' body General text preparation code Deleting stopwords from text is not necessary with SMART method but included for educational purpose Finanzen.net specific code to extract stock's name Finanzen.net specific code to extract time of news Gathers stock's return history keeps code running if news date is faulty Calculate return and volatility  variance until news  variance after news Save in dict addressed cell change in cell,176
RCVJ_RealizedVolatility,"This quantlet includes two main code files. The first one named CAL_RV_BPV.py is the estimation of realized volatility, bipower volatility, threshold bipower volatility and separated jumps, threshold jumps. The second one named PLT_RV_BPV.py generates all related figures in the paper. Also some other auxiliary files that generate different statistics results.","Realized volatility, Bipower volatility, Threshold Jump, Local variation estimation, Cryptocurrencies, Global Market Indices, ACF",Junjie Hu,RCVJ_RealizedVolatility,8,Code.GlobalParams matplotlib os statsmodels.tsa.stattools acf matplotlib Code.RCVJ_DataOperation.IO_Funcs read_indices_full_RV itertools pandas Code.RCVJ_RealizedVolatility.CAL_RV_BPV CAL_AllRVs Code.RCVJ_RealizedVolatility.PLT_Funcs plot_acf multiple_acfs var_ts  max_lags  alpha  Compute ACF coeffiecnts  acfs.plot ) construct_variables_over_cryptos freq  alpha  cv  annualized=True    all params  annualized = True  freq = freqs[0]  alpha = alphas[0]  cv = cvs[1]  Extract RV variable over all cryptos acf_plot_over_cryptos acf_coef  file_name  acf_coef = acfs  Plot ACF coefficient decay over all cryptos acf_plot_over_assets acf_coefs  lines  file_name  max_lags=None  lines = list zip colors  styles  labels)) lines_generator number_of_lines calculate_cryptos_acf freq='5min'  alpha=0.9999  cv=3 plot_cryptos_acf freq='5min'  alpha=0.9999  cv=3 plot_indices_acf_compare_btcg coin='gemini_BTC'  Read Indices RV data  indices_rv_acf = multiple_acfs var_ts=indices_rv  max_lags=365  alpha=0.05)  Read Cryptos RV data  acf_plot_over_assets acf_coefs=indices_rv_acf  lines=lines  file_name='Indices_RV_ACF'  max_lags=300),104
RCVJ_RealizedVolatility,"This quantlet includes two main code files. The first one named CAL_RV_BPV.py is the estimation of realized volatility, bipower volatility, threshold bipower volatility and separated jumps, threshold jumps. The second one named PLT_RV_BPV.py generates all related figures in the paper. Also some other auxiliary files that generate different statistics results.","Realized volatility, Bipower volatility, Threshold Jump, Local variation estimation, Cryptocurrencies, Global Market Indices, ACF",Junjie Hu,RCVJ_RealizedVolatility,8,matplotlib Code.RCVJ_RealizedVolatility.PLT_Funcs Code.RCVJ_DataOperation.IO_Funcs read_dyos_clean  For now  doesn't support different freq on this plot plot_trading_volume coin_name,15
RCVJ_RealizedVolatility,"This quantlet includes two main code files. The first one named CAL_RV_BPV.py is the estimation of realized volatility, bipower volatility, threshold bipower volatility and separated jumps, threshold jumps. The second one named PLT_RV_BPV.py generates all related figures in the paper. Also some other auxiliary files that generate different statistics results.","Realized volatility, Bipower volatility, Threshold Jump, Local variation estimation, Cryptocurrencies, Global Market Indices, ACF",Junjie Hu,RCVJ_RealizedVolatility,8,matplotlib pandas datetime os numpy sklearn.neighbors KernelDensity scipy.stats gaussian_kde statsmodels.nonparametric.kde KDEUnivariate sklearn.model_selection GridSearchCV statsmodels Code.RCVJ_DataOperation.IO_Funcs read_dyos_clean Code.GlobalParams outplot_dir plot_RV_separation return_df: pd.Series  rv: pd.Series  bpv: pd.Series  jump: pd.Series  **kwargs  transform into square root realized volatility and bipower volatility  rv = np.sqrt rv)  bpv = np.sqrt bpv)  plot  plt.plot return_df.values  color='black'  linestyle='-'  markersize=3)  plt.xlim [dt.date 2015  12  31)  dt.date 2018  8  22)])  plt.plot np.sqrt rv)  color='blue'  linestyle='-'  linesize=3)  plt.bar x=rv.index  height=np.sqrt rv['RV']).values  color='blue'  width=0.6)  plt.title '$RV^{1/2}$')  plt.hist rv_crix  100)  plt.plot np.sqrt bpv)  color='orange'  linestyle='-'  markersize=3)  plt.title '$BPV^{1/2}$')  plt.plot np.sqrt jump)  color='red'  linestyle='-'  markersize=3)  plt.title '$J^{1/2}$') plot_RV_separation_notimeframe return_df  rv  bpv  jump  freq='5min'  plt.bar x=range len return_df)) height=return_df.values  color='black')  plt.xlim [dt.date 2015  12  31)  dt.date 2018  7  2)])  plt.ylim [0  0.05])  plt.hist rv_crix  100) plot_daily_vol logreturn_daily  ==================Plot daily volatility of bitcoin_hf plot_5min_vol logreturn_ts  ==================Plot 5-min volatility of bitcoin_hf  plt.xticks fontsize=7)  plt.show ) __init__ self  data_points  Default parameters  self.x_plot = np.linspace 0  1  1000)  self.file_name = file_name bandwidth_search self  method  x_grid = np.linspace min self.data_points)  max self.data_points)  int len self.data_points)/10)) pdf_calcualtion self  **kwargs  self.log_densities[f'{bandwidth}'] = log_dens plot_curve_hist_kde self  bin_num=None  hist_density=True  bandwidth=None  method='silverman'  def save_plot self  dir  num_bin  hist_density=True  refresh=True):      self.bandwidth_search )      self.log_density_estimate )      plot_exist = os.path.exists dir + f'/{self.file_name}_{self.kernel}_{self.band_width}.png')      if not plot_exist or refresh:          self.plot_curve_hist_kde log_dens=self.  bin_num=num_bin                                    hist_density=hist_density)          self.fig.savefig dir + f'/{self.file_name}_{self.kernel}_{self.band_width}.png'  dpi=300)          print f'Plot: {self.file_name}_{self.kernel}_{self.band_width}')          plt.close )      else:          print f'{self.file_name}_{self.kernel}_{self.band_width}.png: Existed!') plot_logreturn_hists coin  freq  low_cut  high_cut  zero=True  plt.figure figsize= 16  5))  plt.plot coin_full['Close'])  plt.ylim [0  6000])  plt.xlim [-0.00020  0.00020])  plt.show ) plot_price_return_volume time_index  price:pd.Series  logreturn:pd.Series  volume  high_count_value  fig = plt.subplots 2 1  figsize= 15 6))  a  = ax1.plot t  price.values  'b-'  linewidth=2)  Make the y-axis label  ticks and tick labels match the line color.  Add stripe  p = [a b]  ax1.legend p  [p_.get_label ) for p_ in p]              loc='upper right'  fontsize=13)  plt.show ) plot_acf y  lags=None  title=None  plt.savefig name + '.pdf'  dpi=300) plot_highest_return_day coin='BTC'  freq='5min'  logreturn from original file  =============================,306
RCVJ_RealizedVolatility,"This quantlet includes two main code files. The first one named CAL_RV_BPV.py is the estimation of realized volatility, bipower volatility, threshold bipower volatility and separated jumps, threshold jumps. The second one named PLT_RV_BPV.py generates all related figures in the paper. Also some other auxiliary files that generate different statistics results.","Realized volatility, Bipower volatility, Threshold Jump, Local variation estimation, Cryptocurrencies, Global Market Indices, ACF",Junjie Hu,RCVJ_RealizedVolatility,8,matplotlib Code.DataOperation.IO_Funcs read_coin_return pandas datetime itertools scipy.stats norm scipy Code.GlobalParams jump_test_statistics rv  bpv_ct  tpv_ct  print rv)  print ctbpv)  print cttpv) upper_incomplete_gamma_func gamma  cv high_vol_smoother theta  cv  gamma  cv = 3  Gamma 1  x) = exp -x) mu_function p st_norm_percentile q  q=0.9999  def plot_significant_jump z_test  sig_level  sig_ind):      test_z = z_test.copy deep=True)      test_z = test_z.to_frame )      test_z['sig_level'] = [sig_level]*len z_test)      test_z['sig_ind'] = sig_ind      plt.subplots 2 1 figsize= 15 5))      plt.subplot 2 1 1)      plt.plot test_z['z'])      plt.plot test_z['sig_level']  linewidth = 2  linestyle='--'  color='red')      plt.title 'Z-test')      plt.subplot 2 1 2)      # plt.plot test_z['sig_ind'])      plt.plot z_test['sig_jump'])      plt.title 'Significant Jumps')      plt.xlabel 'Date'  fontsize=16)      plt.tight_layout )      plt.savefig outplot_dir + 'rv_significant_jump.png'  dpi=300) realized_volatility_separation return_df  delta=1 / 288  alpha=0.9999  truncate_zero=False  annualized=True  return_df = return_df[ return_df.index.date < dt.date 2018  7  1)) &  return_df.index.date > dt.date 2015  12  30))]  ========Calculate realized volatility  ========Calculate Bipower variation  mu_1 = np.sqrt 2 / np.pi)  bpv = hf_quality_log_return_wthinday.groupby by=hf_quality_log_return_wthinday.index.date  axis=0).apply       lambda x: np.nansum x['return'].shift 1) * x['return']  axis=0)).to_frame )  ========Calculate Tripower variation  concat rvs  ========Calculate jump statistics  called z  ========Calculate jump component  ===Check with plot  test_z = rvs['z'].copy deep=True)  test_z = test_z.to_frame )  test_z['sig_level'] = [sig_level]*len rvs)  test_z['sig_ind'] = sig_ind  plt.subplots 2 1 figsize= 15 5))  plt.subplot 2 1 1)  plt.plot test_z['z'])  plt.plot test_z['sig_level']  linewidth = 2  linestyle='--'  color='red')  plt.title 'Z-test')  plt.subplot 2 1 2)  # plt.plot test_z['sig_ind'])  plt.plot rvs['sig_jump'])  plt.title 'Significant Jumps')  plt.xlabel 'Date'  fontsize=16)  plt.tight_layout )  plt.savefig outplot_dir + 'rv_significant_jump.png'  dpi=300)  ===End Check local_variation_estimate df  ii  cv  print f'{ii[0]}-{ii[-1]}')  Get corresponding logreturn from logreturn df  Get previous variance estimation  init with np.inf  x = np.array range len R))) / len R)  # Half gaussian kernel  for test  x for gaussian kernel K x)  Gaussian kernel on i/L and indicator function  for every observation  index function  1 when r^2 <= cv*V_pre  0: else  for every observation  Drop the i=-1 0 1 elements  i = [-L L]  print f'Estimated Variance: {V_estimate[0]}') variation_estimation_single_period daily_sample  iterate_num  cv  daily_sample.loc[:  'V_est'] = np.inf  daily_sample.loc[:  'ii'] = range len daily_sample))  print daily_sample)  print V_pre)  print V_est)  print daily_sample['V_est'].dropna )) ##iteration end  but may not be converged###: {V_diff}')  print daily_sample['V_est'].dropna )) variation_estimation_full_period full_sample  iterate_num  cv  Check estimation existence:  ts_v = ts_v.tz_convert tz=timezone)  ts_v = ts_v.tz_localize tz='UTC').tz_convert tz=timezone)  for test  daily_sample = ts_df[ts_df.index.date == dt.date 2017 7 18)]  First line estimate within whole sample that might cause using future information problem  Second line estimate within each day that doesn't affect RV for next day  ts_vest = variation_estimation_full_period return_df=ts_df  iterate_num=num_iteration_lve  cv=cv)  ====test local variation estimation  x = ts_df[ts_df.index.date == dt.date 2017  7  20)]  x_vest = variation_estimation_single_period x  num_iteration_lve  cv)  x_vest.plot )  ts_df = ts_df[ ts_df.index.date > dt.date 2017  1  1)) &  ts_df.index.date < dt.date 2017  8  1))]  ====end test  # break for test  if num == dt.date 2017 7 25):      break  ts_vest = ts_df.groupby by=ts_df.index.date  axis=0).apply       lambda x: variation_estimation_single_period x  num_iteration_lve  cv))  plt.plot ts_v['V_est'])  ts_v['indicate'].value_counts ) #  1: 212556  0: 4723  save after estimation immediately  plt.figure figsize= 15  6))  plt.plot ts_v['z'])  plt.plot ts_v['log_return'])  ctrv = ts_v.groupby by=ts_v.index.date  axis=0).apply lambda x:  x['z_1'] ** 2).sum axis=0))  =====Calculate C-Threshold Bipower variation  =====Calculate C-Threshold Tripower quartacity variation  concat C-Threshold variations  annualizing variance by multiplying 365  =====Calculate Realized variation  print ts_df.columns)  concat all variation estimators  ======Calculate jump test statistics  called ctz  all_RVs['ctz'].plot )  ====Calculate CT-Jump realized_volatility self CAL_AllRVs coin  freq  sample_num  cv  refresh  refresh_est  truncate_zero  alpha  annualized  tz_lag  timezone = dt.timezone dt.timedelta hours=tz_lag))  delta = 1 /  1440 / int freq.split 'min')[0]))  tz_lag is not functioning  TODO: adaptive to different timezone  all_RVs = all_RVs.tz_localize 'UTC').tz_convert timezone)  estimation = estimation.tz_localize 'UTC').tz_convert timezone)  truncate the data for now main_test_func   ======Test Code for class object  coins = ['gemini_BTC'  'gemini_ETH']  freqs = ['5min']  cvs = [3]  alpha = 0.9999  sample_num = 288  tz_lag = 0  For test special case  logreturn_hf  default = read_coin_diff_freq coin)  coin = coins[0]  freq = freqs[0]  cv = cvs[0]  alpha = alphas[0]  ! REFRESH ALL ESTIMATION RESULTS,629
RCVJ_RealizedVolatility,"This quantlet includes two main code files. The first one named CAL_RV_BPV.py is the estimation of realized volatility, bipower volatility, threshold bipower volatility and separated jumps, threshold jumps. The second one named PLT_RV_BPV.py generates all related figures in the paper. Also some other auxiliary files that generate different statistics results.","Realized volatility, Bipower volatility, Threshold Jump, Local variation estimation, Cryptocurrencies, Global Market Indices, ACF",Junjie Hu,RCVJ_RealizedVolatility,8,"matplotlib Code.DataOperation.IO_Funcs read_coin_return pandas datetime itertools scipy.stats norm scipy Code.GlobalParams jump_test_statistics rv  bpv_ct  tpv_ct  print rv)  print ctbpv)  print cttpv) upper_incomplete_gamma_func gamma  cv high_vol_smoother theta  cv  gamma  cv = 3  Gamma 1  x) = exp -x) mu_function p st_norm_percentile q  q=0.9999  def plot_significant_jump z_test  sig_level  sig_ind):      test_z = z_test.copy deep=True)      test_z = test_z.to_frame )      test_z['sig_level'] = [sig_level]*len z_test)      test_z['sig_ind'] = sig_ind      plt.subplots 2 1 figsize= 15 5))      plt.subplot 2 1 1)      plt.plot test_z['z'])      plt.plot test_z['sig_level']  linewidth = 2  linestyle='--'  color='red')      plt.title 'Z-test')      plt.subplot 2 1 2)      # plt.plot test_z['sig_ind'])      plt.plot z_test['sig_jump'])      plt.title 'Significant Jumps')      plt.xlabel 'Date'  fontsize=16)      plt.tight_layout )      plt.savefig outplot_dir + 'rv_significant_jump.png'  dpi=300) realized_volatility_separation return_df  delta=1 / 288  alpha=0.9999  truncate_zero=False  annualized=True  return_df = return_df[ return_df.index.date < dt.date 2018  7  1)) &  return_df.index.date > dt.date 2015  12  30))]  ========Calculate realized volatility  ========Calculate Bipower variation  mu_1 = np.sqrt 2 / np.pi)  bpv = hf_quality_log_return_wthinday.groupby by=hf_quality_log_return_wthinday.index.date  axis=0).apply       lambda x: np.nansum x['return'].shift 1) * x['return']  axis=0)).to_frame )  ========Calculate Tripower variation  concat rvs  ========Calculate jump statistics  called z  ========Calculate jump component  ===Check with plot  test_z = rvs['z'].copy deep=True)  test_z = test_z.to_frame )  test_z['sig_level'] = [sig_level]*len rvs)  test_z['sig_ind'] = sig_ind  plt.subplots 2 1 figsize= 15 5))  plt.subplot 2 1 1)  plt.plot test_z['z'])  plt.plot test_z['sig_level']  linewidth = 2  linestyle='--'  color='red')  plt.title 'Z-test')  plt.subplot 2 1 2)  # plt.plot test_z['sig_ind'])  plt.plot rvs['sig_jump'])  plt.title 'Significant Jumps')  plt.xlabel 'Date'  fontsize=16)  plt.tight_layout )  plt.savefig outplot_dir + 'rv_significant_jump.png'  dpi=300)  ===End Check local_variation_estimate df  ii  cv  print f'{ii[0]}-{ii[-1]}')  Get corresponding logreturn from logreturn df  Get previous variance estimation  init with np.inf  x = np.array range len R))) / len R)  # Half gaussian kernel  for test  x for gaussian kernel K x)  Gaussian kernel on i/L and indicator function  for every observation  index function  1 when r^2 <= cv*V_pre  0: else  for every observation  Drop the i=-1 0 1 elements  i = [-L L]  print f'Estimated Variance: {V_estimate[0]}') variation_estimation_single_period daily_sample  iterate_num  cv  daily_sample.loc[:  'V_est'] = np.inf  daily_sample.loc[:  'ii'] = range len daily_sample))  print daily_sample)  print V_pre)  print V_est)  print daily_sample['V_est'].dropna )) ##iteration end  but may not be converged###: {V_diff}')  print daily_sample['V_est'].dropna )) variation_estimation_full_period full_sample  iterate_num  cv  Check estimation existence:  ts_v = ts_v.tz_convert tz=timezone)  ts_v = ts_v.tz_localize tz='UTC').tz_convert tz=timezone)  for test  daily_sample = ts_df[ts_df.index.date == dt.date 2017 7 18)]  First line estimate within whole sample that might cause using future information problem  Second line estimate within each day that doesn't affect RV for next day  ts_vest = variation_estimation_full_period return_df=ts_df  iterate_num=num_iteration_lve  cv=cv)  ====test local variation estimation  x = ts_df[ts_df.index.date == dt.date 2017  7  20)]  x_vest = variation_estimation_single_period x  num_iteration_lve  cv)  x_vest.plot )  ts_df = ts_df[ ts_df.index.date > dt.date 2017  1  1)) &  ts_df.index.date < dt.date 2017  8  1))]  ====end test  # break for test  if num == dt.date 2017 7 25):      break  ts_vest = ts_df.groupby by=ts_df.index.date  axis=0).apply       lambda x: variation_estimation_single_period x  num_iteration_lve  cv))  plt.plot ts_v['V_est'])  ts_v['indicate'].value_counts ) #  1: 212556  0: 4723  save after estimation immediately  plt.figure figsize= 15  6))  plt.plot ts_v['z'])  plt.plot ts_v['log_return'])  ctrv = ts_v.groupby by=ts_v.index.date  axis=0).apply lambda x:  x['z_1'] ** 2).sum axis=0))  =====Calculate C-Threshold Bipower variation  =====Calculate C-Threshold Tripower quartacity variation  concat C-Threshold variations  annualizing variance by multiplying 365  =====Calculate Realized variation  print ts_df.columns)  concat all variation estimators  ======Calculate jump test statistics  called ctz  all_RVs['ctz'].plot )  ====Calculate CT-Jump realized_volatility self CAL_AllRVs coin  freq  sample_num  cv  refresh  refresh_est  truncate_zero  alpha  annualized  tz_lag  timezone = dt.timezone dt.timedelta hours=tz_lag))  delta = 1 /  1440 / int freq.split 'min')[0]))  tz_lag is not functioning  TODO: adaptive to different timezone  all_RVs = all_RVs.tz_localize 'UTC').tz_convert timezone)  estimation = estimation.tz_localize 'UTC').tz_convert timezone)  truncate the data for now main_test_func   ======Test Code for class object  coins = ['gemini_BTC'  'gemini_ETH']  freqs = ['5min']  cvs = [3]  alpha = 0.9999  sample_num = 288  tz_lag = 0  For test special case  logreturn_hf  default = read_coin_diff_freq coin)  coin = coins[0]  freq = freqs[0]  cv = cvs[0]  alpha = alphas[0]  ! REFRESH ALL ESTIMATION RESULTS pandas numpy os matplotlib pickle itertools matplotlib datetime Code.RCVJ_RealizedVolatility.CAL_RV_BPV All_RVs_Separation Code.RCVJ_RealizedVolatility.CAL_RV_BPV CAL_AllRVs Code.RCVJ_RealizedVolatility.PLT_Funcs Code.RCVJ_DataOperation.IO_Funcs read_coin_return statsmodels.nonparametric.kde KDEUnivariate Code.RCVJ_DataOperation.IO_Funcs read_dyos_clean Code.GlobalParams plt_local_variation_estimate threshold_rvs  cv  logreturn  ====== plot local variation estimate  try:      threshold_rvs  except NameError:      threshold_rvs = All_RVs_Separation ts_df=logreturn  cv=cv)  def plt_trvs_with_diffcvs cvs):      # =========plot threshold realized jumps with different cvs      for cv in cvs:          plt_threshold_realized_jumps cv) plt_highlow_rvs_intraday logreturn  =======  rv  bpv  jump = rvs  rv_crypto_sort = rv.sort_values by='RV'  axis=0  ascending=False)  bpv_crypto_sort = bpv.sort_values by='BPV'  axis=0  ascending=False)  ====================== Plot kernel estimation  plot kernel estimation plt_logretrun_distribution logreturn  asset_name  freq  drop_zero=False  rand_sample_size=None  replace=False  ====================== Plot distribution of log returns  coin_name = 'BTC'  freq = '5min'  logreturn = read_dyos_clean coin_name)  =========Test for  counts_day = high_counts_day['day'].value_counts )  close_price = pd.read_csv data_dir + f'/{asset_name}_USDT.csv'  index_col=0  parse_dates=True)['close']  volume = pd.read_csv data_dir + f'/{asset_name}_USDT.csv'  index_col=0  parse_dates=True)['volume']  high_days = counts_day.head 20).index  for high_day in high_days:      high_day_price = close_price[close_price.index.date == high_day]      plt.figure figsize= 15  5))      plt.plot high_day_price)      plt.title f'Intraday Close Price  {high_day}')      plt.savefig outplot_dir + f""HighFreqSameReturn/OneDaySample_{high_day}.png""  dpi=300)  value_count_after = logreturn_dropped['BTC_5min'].value_counts )  kernel_plotter = PlotKernelDensityEstimator data_points=data_df.values                                               file_name=f'KDE/KDE_{asset_name}_{var_name}')  kernel_plotter.band_widths = [0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1.0]  kernel_plotter.x_plot = np.linspace data_df.min )  data_df.max )  100000)  kernel_plotter.log_density_estimate )  kernel_plotter.plot_curve_hist_kde log_dens=kernel_plotter.log_densities['0.1']  bin_num=20000  hist_density=True)  kernel_plotter.save_plot dir=outplot_dir  num_bin=40000  hist_density=True)  histogram.savefig outplot_dir + f'{asset_name}_{freq}_logreturn_distribution.png'  dpi=300) plot_program_trading asset_name='BTC' plt_threshold_realized_jumps logreturn  rv  bpv  jump  **kwargs  self.threshold_rvs = threshold_realized_volatility_separation ts_df=self.logreturn  cv=cv)  ctrvs = self.threshold_rvs[0]  rv = self.rvs[0]  ctrvs['ct_jumps'] = rv.values - ctrvs['C_TBPV'].to_frame ).values  trvs_fig.savefig outplot_dir + f'RV_Separation/{coin_name}_{self.freq}_{cv}_CThresholdJumps.png'  dpi=300) plot_crypto_jumpseparation coin  freq  all_RVs  logreturn  cv=3  alpha=0.9999  plotter = RealizedVolatilityPlot_OneAsset return_ts=logreturn  rv=rv  bpv=bpv  asset_name=coin  ts_freq=freq)  Plot threshold RV separation  Raw Jump from BPV  jump = rv - bpv  plt.close )  BPV significant Jumps  plt.close )  CTBPV raw Jumps  plt.close )  CTBPV significant Jumps  plt.close )  plt.plot all_RVs[f'Jump_raw']) plot_index_jumpsepa index plot_extrem_volatile_day coin  freq  all_RVs  logreturn  alpha  plot daily price  rv  big significant jumps  close['date'] = close.index.date  close_hightj = close[close['date'].isin high_tj_dates)]  close_hightj.drop 'date' axis=1 inplace=True)  close_hightj.plot )  ==== Test  close_daily.plot )  rv_sigjump = all_RVs[['RV'  f'CTJump_{alpha}']]  plt.figure figsize= 15  8))  plt.subplot 3  1  1)  plt.plot close_daily  color='black')  plt.subplot 3  1  2)  plt.plot rv_sigjump['RV']  color='blue')  ax3 = plt.subplot 3  1  3)  plt.plot rv_sigjump[f'CTJump_{alpha}']  color='red')  # Add stripe  for timestamp in high_tj_dates:      ax3.axvline x=timestamp  linewidth=2  color='green')  diff = all_RVs['CTJump_raw'] - all_RVs['Jump_raw']  sort_diff = diff.sort_values axis=0  ascending=False)  coin_data = read_clean_gemini coin  freq  True)  plt.scatter x=logreturn_sp.index  y=logreturn_sp.values) plot_all_indices_rvjump   index = '.FCHI' plot_kde ts  bin_num  bandwidth  hist_density  var_name  cv  asset_name  freq  bd_method  coin = 'gemini_BTC'  freq = '5min'  cv = 3  alpha = 0.99  refresh = True  Plot histogram of ts  bpv  ctbpv  c-jump  jump  KDE on RV  plotter = RealizedVolatilityPlot_OneAsset return_ts=logreturn  rv=ts  asset_name=coin  ts_freq=freq) plot_crypto_rvs_kde all_RVs  coin  freq  cv  **kwargs  KDE on BPV  jump_sig = all_RVs['Jump_0.99']  tjump_sig = all_RVs['CTJump_0.99']  plot_kde ts=jump_sig  rv_name='JumpSig'  coin=coin  freq=freq  cv=cv)  plot_kde ts=tjump_sig  rv_name='TJump_sig'  coin=coin  freq=freq  cv=cv) plot_indices_kde  plot_funcs **kwargs  logreturn_hf  default = read_coin_diff_freq coin)  coin = coins[0]  freq = freqs[0]  cv = cvs[0]  Plot RV  BPV and jump separation on specified Cryptos  Plot KDE  Plot KDE on logreturn",1105
RCVJ_RealizedVolatility,"This quantlet includes two main code files. The first one named CAL_RV_BPV.py is the estimation of realized volatility, bipower volatility, threshold bipower volatility and separated jumps, threshold jumps. The second one named PLT_RV_BPV.py generates all related figures in the paper. Also some other auxiliary files that generate different statistics results.","Realized volatility, Bipower volatility, Threshold Jump, Local variation estimation, Cryptocurrencies, Global Market Indices, ACF",Junjie Hu,RCVJ_RealizedVolatility,8,pandas Code.GlobalParams Code.RCVJ_RealizedVolatility.CAL_RV_BPV CAL_AllRVs jump_summary_statistics coin  freq  cv=3  refresh=True  backup_data=True  alpha=0.99  annualized=True  TODO: Jumps dynamics with different alphas and cvs for different coins  coin = 'gemini_BTC'  freq = '5min'  cv = 3  refresh = True  backup_data = True  alpha = 0.99,41
RCVJ_RealizedVolatility,"This quantlet includes two main code files. The first one named CAL_RV_BPV.py is the estimation of realized volatility, bipower volatility, threshold bipower volatility and separated jumps, threshold jumps. The second one named PLT_RV_BPV.py generates all related figures in the paper. Also some other auxiliary files that generate different statistics results.","Realized volatility, Bipower volatility, Threshold Jump, Local variation estimation, Cryptocurrencies, Global Market Indices, ACF",Junjie Hu,RCVJ_RealizedVolatility,8,"pandas numpy os matplotlib pickle itertools matplotlib datetime Code.RCVJ_RealizedVolatility.CAL_RV_BPV All_RVs_Separation Code.RCVJ_RealizedVolatility.CAL_RV_BPV CAL_AllRVs Code.RCVJ_RealizedVolatility.PLT_Funcs Code.RCVJ_DataOperation.IO_Funcs read_coin_return statsmodels.nonparametric.kde KDEUnivariate Code.RCVJ_DataOperation.IO_Funcs read_dyos_clean Code.GlobalParams plt_local_variation_estimate threshold_rvs  cv  logreturn  ====== plot local variation estimate  try:      threshold_rvs  except NameError:      threshold_rvs = All_RVs_Separation ts_df=logreturn  cv=cv)  def plt_trvs_with_diffcvs cvs):      # =========plot threshold realized jumps with different cvs      for cv in cvs:          plt_threshold_realized_jumps cv) plt_highlow_rvs_intraday logreturn  =======  rv  bpv  jump = rvs  rv_crypto_sort = rv.sort_values by='RV'  axis=0  ascending=False)  bpv_crypto_sort = bpv.sort_values by='BPV'  axis=0  ascending=False)  ====================== Plot kernel estimation  plot kernel estimation plt_logretrun_distribution logreturn  asset_name  freq  drop_zero=False  rand_sample_size=None  replace=False  ====================== Plot distribution of log returns  coin_name = 'BTC'  freq = '5min'  logreturn = read_dyos_clean coin_name)  =========Test for  counts_day = high_counts_day['day'].value_counts )  close_price = pd.read_csv data_dir + f'/{asset_name}_USDT.csv'  index_col=0  parse_dates=True)['close']  volume = pd.read_csv data_dir + f'/{asset_name}_USDT.csv'  index_col=0  parse_dates=True)['volume']  high_days = counts_day.head 20).index  for high_day in high_days:      high_day_price = close_price[close_price.index.date == high_day]      plt.figure figsize= 15  5))      plt.plot high_day_price)      plt.title f'Intraday Close Price  {high_day}')      plt.savefig outplot_dir + f""HighFreqSameReturn/OneDaySample_{high_day}.png""  dpi=300)  value_count_after = logreturn_dropped['BTC_5min'].value_counts )  kernel_plotter = PlotKernelDensityEstimator data_points=data_df.values                                               file_name=f'KDE/KDE_{asset_name}_{var_name}')  kernel_plotter.band_widths = [0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1.0]  kernel_plotter.x_plot = np.linspace data_df.min )  data_df.max )  100000)  kernel_plotter.log_density_estimate )  kernel_plotter.plot_curve_hist_kde log_dens=kernel_plotter.log_densities['0.1']  bin_num=20000  hist_density=True)  kernel_plotter.save_plot dir=outplot_dir  num_bin=40000  hist_density=True)  histogram.savefig outplot_dir + f'{asset_name}_{freq}_logreturn_distribution.png'  dpi=300) plot_program_trading asset_name='BTC' plt_threshold_realized_jumps logreturn  rv  bpv  jump  **kwargs  self.threshold_rvs = threshold_realized_volatility_separation ts_df=self.logreturn  cv=cv)  ctrvs = self.threshold_rvs[0]  rv = self.rvs[0]  ctrvs['ct_jumps'] = rv.values - ctrvs['C_TBPV'].to_frame ).values  trvs_fig.savefig outplot_dir + f'RV_Separation/{coin_name}_{self.freq}_{cv}_CThresholdJumps.png'  dpi=300) plot_crypto_jumpseparation coin  freq  all_RVs  logreturn  cv=3  alpha=0.9999  plotter = RealizedVolatilityPlot_OneAsset return_ts=logreturn  rv=rv  bpv=bpv  asset_name=coin  ts_freq=freq)  Plot threshold RV separation  Raw Jump from BPV  jump = rv - bpv  plt.close )  BPV significant Jumps  plt.close )  CTBPV raw Jumps  plt.close )  CTBPV significant Jumps  plt.close )  plt.plot all_RVs[f'Jump_raw']) plot_index_jumpsepa index plot_extrem_volatile_day coin  freq  all_RVs  logreturn  alpha  plot daily price  rv  big significant jumps  close['date'] = close.index.date  close_hightj = close[close['date'].isin high_tj_dates)]  close_hightj.drop 'date' axis=1 inplace=True)  close_hightj.plot )  ==== Test  close_daily.plot )  rv_sigjump = all_RVs[['RV'  f'CTJump_{alpha}']]  plt.figure figsize= 15  8))  plt.subplot 3  1  1)  plt.plot close_daily  color='black')  plt.subplot 3  1  2)  plt.plot rv_sigjump['RV']  color='blue')  ax3 = plt.subplot 3  1  3)  plt.plot rv_sigjump[f'CTJump_{alpha}']  color='red')  # Add stripe  for timestamp in high_tj_dates:      ax3.axvline x=timestamp  linewidth=2  color='green')  diff = all_RVs['CTJump_raw'] - all_RVs['Jump_raw']  sort_diff = diff.sort_values axis=0  ascending=False)  coin_data = read_clean_gemini coin  freq  True)  plt.scatter x=logreturn_sp.index  y=logreturn_sp.values) plot_all_indices_rvjump   index = '.FCHI' plot_kde ts  bin_num  bandwidth  hist_density  var_name  cv  asset_name  freq  bd_method  coin = 'gemini_BTC'  freq = '5min'  cv = 3  alpha = 0.99  refresh = True  Plot histogram of ts  bpv  ctbpv  c-jump  jump  KDE on RV  plotter = RealizedVolatilityPlot_OneAsset return_ts=logreturn  rv=ts  asset_name=coin  ts_freq=freq) plot_crypto_rvs_kde all_RVs  coin  freq  cv  **kwargs  KDE on BPV  jump_sig = all_RVs['Jump_0.99']  tjump_sig = all_RVs['CTJump_0.99']  plot_kde ts=jump_sig  rv_name='JumpSig'  coin=coin  freq=freq  cv=cv)  plot_kde ts=tjump_sig  rv_name='TJump_sig'  coin=coin  freq=freq  cv=cv) plot_indices_kde  plot_funcs **kwargs  logreturn_hf  default = read_coin_diff_freq coin)  coin = coins[0]  freq = freqs[0]  cv = cvs[0]  Plot RV  BPV and jump separation on specified Cryptos  Plot KDE  Plot KDE on logreturn",476
RCVJ_RealizedVolatility,"This quantlet includes two main code files. The first one named CAL_RV_BPV.py is the estimation of realized volatility, bipower volatility, threshold bipower volatility and separated jumps, threshold jumps. The second one named PLT_RV_BPV.py generates all related figures in the paper. Also some other auxiliary files that generate different statistics results.","Realized volatility, Bipower volatility, Threshold Jump, Local variation estimation, Cryptocurrencies, Global Market Indices, ACF",Junjie Hu,RCVJ_RealizedVolatility,8,pandas statsmodels numpy Code.RCVJ_RealizedVolatility.CAL_RV_BPV CAL_AllRVs Code.RCVJ_RealizedVolatility.PLT_Funcs scipy.stats skew Code.RCVJ_DataOperation.IO_Funcs read_indices_full_RV statsmodels.tsa.stattools acf itertools Code.GlobalParams reformat_stats_number x  decimal_round  format_threshold  decimal_round=5  return format_string.format x*100) if x < format_threshold else x SummaryStatistics estimator_matrix  **kwargs  coin = 'gemini_BTC'  freq = '5min'  cv = 3  refresh = True  alpha = 0.9999  annualized=True  truncate_zero=True  decimal = 2  Save dir  Calculate RVs estimators  Preprocessing  Add More Variables  all_RVs['J_sqrt'] = np.sqrt all_RVs[f'Jump_{alpha}'])  all_RVs.loc[all_RVs[f'Jump_{alpha}'] == 0  f'Jump_{alpha}'] = np.nan  all_RVs['TJ_sqrt'] = np.sqrt all_RVs[f'CTJump_{alpha}'])  all_RVs.loc[all_RVs[f'CTJump_{alpha}'] == 0  f'CTJump_{alpha}'] = np.nan  Output statistics  main_vars = ['RV'  'RV_sqrt'  'log RV)'  'C'  'C_sqrt'  'log C)'  'TC'  'TC_sqrt'  'log TC)'  f'Jump_{alpha}'                'J_sqrt'  'log J+1)'  f'CTJump_{alpha}'  'TJ_sqrt'  'log TJ+1)']  main_var_statistics.columns = ['RV1/2'  'log RV)'  'C'  'TC'  f'Jump alpha)'  f'TJump alpha)']  print main_var_statistics.to_latex f'test.csv'))  print auxilinary_statistics.to_latex )) indices_summarystat decimal=2  format_threshold=0.0001  Read Data compute_summarystat   coins = ['gemini_BTC'  'gemini_ETH'  'BTC'  'ETH'  'XRP'  'LTC']  indices_summarystat decimal=2  format_threshold=0.0001),138
SDA_2020_Financial_Performance_Clustering,Scrape S&P 500 component stocks from the start of pandemic and Cluster their performance using different clusering methods,"Webscraping,Financial Performance,Hierarchical Clustering,Agglomerative Clustering,K-means Clustering","Haochen Li, Chung Shun Man",SDA_2020_St_Gallen_Financial_Performance_Clustering,1,"!/usr/bin/env python3  -*- coding: utf-8 -*- %% bs4 datetime os pandas pandas_datareader pickle requests numpy matplotlib pandas_datareader pandas_datareader data datetime datetime cvxopt cvxopt blas scipy ndimage scipy.stats kurtosis scipy.cluster hierarchy scipy.spatial distance_matrix sklearn manifold sklearn.cluster AgglomerativeClustering sklearn.datasets.samples_generator make_blobs pip install yellowbrick yellowbrick %%  set working path  please set your own working path %%  set working path  define object that can collect component stocks of S&P500 save_sp500_tickers  %%  define object that can collect data from yahoo finance get_data_from_yahoo reload_sp500=False  just in case your connection breaks  we'd like to save our progress! df = df.drop ""Symbol""  axis=1) %%  Compile data into one sheet compile_data  %%  Compute returns and drop the columns with zero values %% look if there are values missing drop the stock if more than 5 prices are missing  otherwise replace the missing values with the values in the previous row %%  Calculate the log returns  Compute the annualised returns  Calculate the covariance matrix  Calculate the volatility %%  Calculate Skewness and Kurtosis %%  Compile the dataset we need  save the dataset %%  Prepare Dataset %% ######## Part 1: Hierarchical Clustering ####### Plot1 scipy pylab scipy scipy.cluster.hierarchy fcluster plot the denrogram llf id %% ####### Part 2.1: agglomerative clustering matplotlib %% ###### Part 2.2: agglomerative clustering by industry %% #### Part 3: K-mean set-up matplotlib sklearn.cluster KMeans mpl_toolkits.mplot3d Axes3D %% Normalisation sklearn.preprocessing StandardScaler %% #### Part 3.1: K-mean clustering 3D %% #### Part 3.2: K-mean clustering 3D %% choosing the optimal k  This part needs to be done separately  because it eliminates the cluster colors) sklearn.cluster KMeans yellowbrick.cluster KElbowVisualizer  Fit the data to the visualizer",264
LPA_Empiricalstudy,"Applies the local parametric approach to detect change points, regime shifts and structural breaks in real life datasets and then compares the estimates from fixed rolling window estimates and LPA approach using RMSE. It also provides all the plots. The data is however downloaded with special permissions from Thomson Reuters and is not publicly available. If you have permission to access the data, please get in touch with us with permission and we will share the data with you","mergers & acquisitions, mergers, acquisitions, deals, poisson, time series, decomposition, forecasting, prediction, economics, estimation, adaptive parameters",Kainat Khowaja,LPA_Empiricalstudy,2,!/usr/bin/env python  coding: utf-8  In[3]: pandas io math sqrt matplotlib numpy datetime datetime scipy stats math  scikit-learn bootstrap sklearn.utils resample sklearn.metrics mean_squared_error sklearn.metrics mean_absolute_error seaborn decimal Decimal timeit # Setting defaults for plots  In[4]: Important functions generate_random_weights num  R is the term in poisson likelihood function. check our paper for more details R x w  To store the required sum   Add factorial of all the elements  lgamma replaces the factorial term in poisson pdf Likelihood of poisson distribution Likelihood theta x w = None n if weights equal 1  sum otherwise  adding a very small number to avoid the problem of theta=0 - R x w)  #Here R can be commented out because it doesn't play a role in MLE calculation MLE_estimator x  w = None  For stepwise function plot mystep x y z  ax=None  **kwargs  Vectorized poisson pdf calculator poisson yi  theta /math.factorial yi) #Factorial term here is commented for the same reasons as R above  These are the weights used in bootstrap procedure weights_matrix num  length weights for whole interval  This is likelihood of poisson for each y likelihoods y  theta Simplifying numpy notation plus a b c=0 d=0 e=0 h=0 i=0 j=0 k=0  l=0 neg a product a b c=1 d=1 e=1 h=1 i=1 j=1 poisson yi  theta l =  theta** yi)*np.exp -theta)) #/math.factorial yi) /math.factorial Decimal yi))) l = stats.poisson.pmf k=yi  mu=theta)  In[5]: Loading data. Change the name according to the required industry dataset_name= 'telecommunication_de' dataset_name= 'financials_de' dataset_name= 'energy_de'  In[8]:  Plotting the dataset DT_N.N.plot label='Number of M&A' color = 'k' )  ### Use this  code if you need to deal with missing data. running it once should be sufficient. Save the dataset and use that the next time    DT_N['Date'] =  pd.to_datetime DT_N['Date']  format='%d.%m.%y')  ###### Filling missing dates  DT_N = DT_N.set_index 'Date').resample 'MS').asfreq ).fillna 0)  ###### Saving this data as csv  DT_N.to_csv r'MA_'+dataset_name+'.csv')  In[88]: # Interval lengths and bounds # Use this code to find out the interval lengths and boundries for your desired parameters.  # There are two variants: Arithmetic and geomtric. In order to use one  the code lines linked to other one must be commented out starting from final time from theory: it controls size of the intervals Assuming homogeneity in first three month Arithmetic increase K = int io/n_0) #from theory  Geometric increase Calculated this analytically  number of intervals started from 1 because 0th interval already homo and K+1 because last point is not inclusive  Geometric increase Arithmetic increase n_k =  m+1)*n_0 n_k_plus1 =  m+2)*n_0  In[95]: # Main code changing data to numpy array because they are faster  Initializing starting from final time from theory Assuming homogeneity in first three month  number of intervals: calculated this analytically confidence level number of bootstraps that you want to do starting at a new point for each iteration Arithmetic increase K = int io/n_0)   Geometric increase Calculated this analytically started from 1 because 0th interval already homogeneous and K+1 because last point is not inclusive Arithmetic n_k =  k+1)*n_0 n_k_plus1 =  k+2)*n_0 Geometric increase  For each breakpoint that is in the set J  we calculate the test value and take max New intervals and their estimators  test statistics for this breakpoint and adding it to the list # Bootstrap procedure   Getting lengths of intervals length of A_k length of B_k generating weights and calculating estimators for the bootstrap intervals Test statistic for bootstrap for this breakpoint  since there are no changepoints  we provide artifical values to make the algo jump to the next interval  In[96]:  Since we got values from last time to first time  we reverse it  In[97]:  ## Once you have run the above code and stored the results  it is easier to use the results file and do the analysis on it  In[6]:  In[8]: Plotting the window size that is adaptively selected for each point in time  In[9]: Plotting the LPA estimate that is adaptively selected for each point in time  In[13]:  Using the last LPA estimate as a prediction for the next time period and comparing it with rolling average estimates number of periods ahead to forecast  DT_N.N.plot label='Number of M&A') DT_N.MLEs.plot label='MLEs') DT_N.N.plot label='Number of M&A' color = 'k' ) plt.xlabel 'Year')  Calculating moving averages using rolling window with time period 12 -we use mean because the max likelihood  estimator of poisson is sample mean  the number of points after which we will calculate error because these many points have to be left behind for calculation  Calculating moving averages plt.legend )  In[76]: Idea: Take the last point   n+1)th from the last) as estimate and use it as prediction of next n months  In[10]: Calculating histogram frequencies  bins  mids) relative frequency the most occuring window size for dataset  In[11]:  In[ ]:,779
pyTSA_SimVAR,"This Quantlet plots monthly time series of returns of Procter and Gamble from 1961 to 2016 and  their ACF and PACF (Example, 2.4 Figures 2.8-2.9 in the book)","time series, autocorrelation, returns, ACF, PACF, plot, visualisation","Huang Changquan, Alla Petukhina",pyTSA_SimVAR,1,statsmodels.tsa.vector_ar.var_model VARProcess numpy matplotlib  the constant vector is zero  Draw a sample of size 300 from the VAR 2) process  Plot the sample  Plot theoretical correlation functions,27
Time_Series_Forecasting_on_Crude_Oil_Price,Project_Group11 file,"NUS, FE5225,crude oil, price, LSTM","Peng Wenhui, Gui Yilin",Forecasting_Oil_Price_Using_Historical_Data,2,!/usr/bin/env python  coding: utf-8  In[2]: types pandas numpy keras.preprocessing sequence keras.models load_model  In[3]:  In[4]:  In[5]: matplotlib  Visualising the Data  In[6]:  In[ ]:  In[7]:  defining the batch size and number of epochs  In[8]: test set will be 10% of entire data set   length *= 1 - 0.1 print length)  In[9]: get the length of training data set  get_train_length dataset  batch_size  test_percent  substract test_percent to be excluded from training  reserved for testset  In[10]: print length)  In[11]: Adding timesteps * 2  In[12]:  Feature Scaling i.e we scale each and every value between 0 and 1  sklearn.preprocessing MinMaxScaler  In[13]:  Creating a data structure with n timesteps  In[14]:  Reshaping  In[15]:  Building the LSTM  Importing the Keras libraries and packages keras.layers Dense keras.layers Input keras.layers Dropout keras.models Model h5py  In[16]:  Initialising the LSTM Model with MAE Loss-Function  Using Functional API dropout_1=Dropout 0.5) lstm_1_mae)  In[17]:  1st LSTM Layer   In[18]:  2nd LSTM Layer   In[19]:  In[20]:  In[21]: get_test_length dataset  batch_size  In[22]:  In[23]:  In[24]:  construct test set subsetting scaling creating input data reshaping  In[25]: prediction reshaping inverse transform creating y_test data  reshaping  In[26]:  Visualising the results  In[27]: math sklearn.metrics mean_squared_error  In[28]: sklearn.metrics mean_absolute_error  In[29]: h5py  In[ ]:  In[ ]:  In[ ]:,191
RL_DescriptiveStatistics,"Outputs descriptive statistics for all coins used in the experiments (prices relative to Bitcoin, from 07/2015 to 10/2018, if available).","reinforcement learning, neural network, machine learning, portfolio management, cryptocurrency",Ilyas Agakishiev,RL_DescriptiveStatistics,1,"pandas numpy  Import from ""Database""    ",5
SMSdisthealth05_python,"Calculates distance matrices for Maine, New Hampshire and New York from the US health 2005 data set. The distance measures are Euclidean, Manhattan and maximum distance.","cluster-analysis, distance, euclidean, euclidean-distance-matrix, manhattan metric",Muhaimin,SMSdisthealth05_python,1, -*- coding: utf-8 -*- pandas scipy.spatial distance scipy.spatial distance_matrix numpy read data with url read data with url euclidean distance manhattan distance maximum distance,24
DEDA_class2019_SYSU_Abstract_LDA_wordcloud,Create a word cloud picture to make a short description of 130 abstracts,"word cloud, topic modelling, abstract analysis","FB, FS",DEDA_class2019_SYSU_Abstract_LDA_wordcloud,1,"matplotlib re nltk.corpus stopwords os os path PIL Image wordcloud WordCloud numpy  Please change the working directory to your path!  os.chdir ""/Users/xinwenni/LDA-DTM/xmas_song"")  the path you save your files  keep only letters  numbers and whitespace  apply regex  lower case  Read the whole text.  Mask  xmas_tree_pic = np.array Image.open path.join cwd_dir  ""xmas_tree2.png"")))  Optional additional stopwords  Construct Word Cloud  no backgroundcolor and mode = 'RGBA' create transparency  Pass Text  store to file",69
localpoly,This package uses Local Polynomial Regression to create a fit to the data. The model conveniently also estimates the first and second derivative of the fit. A Cross Validation finds the optimal bandwdith for the fit in case it is unknown.,"Local Polynomial Regression, Local Polynomial Estimation, locpol, Fit, Fitting, Taylor Expansion",Franziska Wehrmann,localpoly,3,numpy scipy.stats norm  ----------------- with tau ------- Rookley + Haerdle  Applied Quant. Finance) gaussian_kernel x  Xi  h epanechnikov_kernel x  Xi  h,21
localpoly,This package uses Local Polynomial Regression to create a fit to the data. The model conveniently also estimates the first and second derivative of the fit. A Cross Validation finds the optimal bandwdith for the fit in case it is unknown.,"Local Polynomial Regression, Local Polynomial Estimation, locpol, Fit, Fitting, Taylor Expansion",Franziska Wehrmann,localpoly,3,numpy pandas math random scipy  B-Spline chunks lst  n sort_values_by_X X  y create_partitions X  y  n_sections  sampling_type  not np.random.seed !!! bspline x  y  sections  degree=3,25
localpoly,This package uses Local Polynomial Regression to create a fit to the data. The model conveniently also estimates the first and second derivative of the fit. A Cross Validation finds the optimal bandwdith for the fit in case it is unknown.,"Local Polynomial Regression, Local Polynomial Estimation, locpol, Fit, Fitting, Taylor Expansion",Franziska Wehrmann,localpoly,3,"random numpy .utils.helpers sort_values_by_X .utils.kernels kernel_dict __init__ self  X  y  h  kernel=""gaussian""  gridsize=100 localpoly self  x  doesnt really happen  but in order to avoid possible errors   n n)   3 n)   3 3)   3 1)   3 1) fit self  prediction_interval  prediction interval of fit  fit of the function at point x  first derivative at point x  second derivative at point x  invoking the __init__ of the parent class  mean squared errors for bandwidths   optimal bandwidth within fine_list_of_bandwidths   ... same as above but with coarse_list_of_bandwidths  1) coarse parameter search  2) fine parameter search  around minimum of first search _bandwidth_cv_sampling self  list_of_bandwidths  for each bandwidth have mse - loss function  for bandwidth h in list_of_bandwidths  take out chunks of our data and do leave-out-prediction  for random y in y_test  extimate y_hat and calculate mse",131
FRM_6_Benchmark_BBW,1) Calculate Bollinger Bands Width for 12 coins and the weighted BBW according to market cap share (window size = 48h) 2) Compare the BBW of BTC to its price movement,"Cryptocurrency, Bollinger Bands Width, Weighted Bollinger Bands Width, Volatility, Benchmark","Qi Wu, Seokhee Moon",FRM_6_Benchmark_BBW,2,"!/usr/bin/env python  coding: utf-8  In[1]: numpy pandas plotly plotly  # Calculate the BBW for individual coins as well as the weighted BBW  In[22]:  window size  'BTC' 'ETH'  'XRP'  'BCH'  'LTC'  'EOS'  'XMR'  'NEO'  'MIOTA'  'DASH'  'ETC'  'ZEC'  In[25]:  fig = go.Figure )  for bbw in BBW.columns[2:-2]:      fig.add_trace go.Scatter x=BBW['time']  y=BBW[bbw]  opacity = 0.1  name = bbw))  fig.add_trace go.Scatter x=BBW['time']  y=BBW['BTC']  name = 'BTC'))  fig.add_trace go.Scatter x=BBW['time']  y=BBW['weighted BBW']  name = 'weighted BBW'))  fig.update_layout title=""Bollinger Bands Width"")  plotly.offline.plot fig filename=""Bollinger Bands Width.html"")  In[40]: matplotlib  In[24]:  ## Compare BTC BBW and BTC N_price  In[28]:  In[29]:  In[30]:  import plotly  fig = go.Figure )  fig.add_trace go.Scatter x=BBW.time  y=BBW['BTC'] name = 'BBW_BTC'))  fig.add_trace go.Scatter x=BBW.time  y = BTC_norm  name =  'Nprice_BTC'))  fig.update_layout title=""Bollinger Bands Width and Normalized BTC Price"")  plotly.offline.plot fig filename=""BBW_BTC vs N_Price_BTC.html"")",129
RCVJ_TimeSeriesAnalysis,"This quantlet includes two time series analysis code files. The first one is the HAR forecasting model calibration, in-sample and out-of-sample forecasting. The second one is calculating the realized utility function documented on Bollerslev et al (2018).","Realized volatility, Heterogenous Autoregression (HAR), Realized utility function, Cryptocurrencies, Global Market Indices, ACF",Junjie Hu,RCVJ_TimeSeriesAnalysis,4," Author   : John Tsang  Date     : December 7th  2017  Purpose  : Implement the Diebold-Mariano Test  DM test) to compare              forecast accuracy  Input    : 1) actual_lst: the list of actual values             2) pred1_lst : the first list of predicted values             3) pred2_lst : the second list of predicted values             4) h         : the number of stpes ahead             5) crit      : a string specifying the criterion                               i)  MSE : the mean squared error                             ii)  MAD : the mean absolute deviation                            iii) MAPE : the mean absolute percentage error                             iv) poly : use power function to weigh the errors             6) poly      : the power for crit power                              it is only meaningful when crit is ""poly"")  Condition: 1) length of actual_lst  pred1_lst and pred2_lst is equal             2) h must be an integer and it must be greater than 0 and less than                 the length of actual_lst.             3) crit must take the 4 values specified in Input             4) Each value of actual_lst  pred1_lst and pred2_lst must                be numerical values. Missing values will not be accepted.             5) power must be a numerical value.  Return   : a named-tuple of 2 elements             1) p_value : the p-value of the DM test             2) DM      : the test statistics of the DM test #########################################################  References:  Harvey  D.  Leybourne  S.  & Newbold  P.  1997). Testing the equality of     prediction mean squared errors. International Journal of forecasting      13 2)  281-291.  Diebold  F. X. and Mariano  R. S.  1995)  Comparing predictive accuracy      Journal of business & economic statistics 13 3)  253-264. ######################################################### dm_test actual_lst  pred1_lst  pred2_lst  h = 1  crit=""MSE""  power = 2  Routine for checking errors error_check   Check if h is an integer  Check the range of h  Check if lengths of actual values and predicted values are equal  Check range of h  Check if criterion supported  Check if every value of the input lists are numerical values  from re import compile as re_compile  comp = re_compile ""^\d+?\.\d+?$"")  def compiled_regex s):      """""" Returns True is string is a number. """"""      if comp.match s) is None:          return s.isdigit )      return True  for actual  pred1  pred2 in zip actual_lst  pred1_lst  pred2_lst):      is_actual_ok = compiled_regex str abs actual)))      is_pred1_ok = compiled_regex str abs pred1)))      is_pred2_ok = compiled_regex str abs pred2)))      if  not  is_actual_ok and is_pred1_ok and is_pred2_ok)):          msg = ""An element in the actual_lst  pred1_lst or pred2_lst is not numeric.""          rt = -1          return  rt msg)  # Error check  error_code = error_check )  # Raise error if cannot pass error check  if  error_code[0] == -1):      raise SyntaxError error_code[1])      return  Import libraries scipy.stats t collections pandas numpy  Initialise lists  convert every value of the lists into real values  Length of lists  as real numbers)  construct d according to crit  Mean of d          Find autocovariance and construct DM test statistics autocovariance Xi  N  k  Xs  0  1  2  Find p-value  Construct named tuple for return",468
RCVJ_TimeSeriesAnalysis,"This quantlet includes two time series analysis code files. The first one is the HAR forecasting model calibration, in-sample and out-of-sample forecasting. The second one is calculating the realized utility function documented on Bollerslev et al (2018).","Realized volatility, Heterogenous Autoregression (HAR), Realized utility function, Cryptocurrencies, Global Market Indices, ACF",Junjie Hu,RCVJ_TimeSeriesAnalysis,4,pandas numpy datetime Code.RCVJ_RealizedVolatility.CAL_RV_BPV CAL_AllRVs matplotlib statsmodels Code.GlobalParams itertools product itertools pickle plot_forecast_errors model  reg_df  rv_name  horizon  save_dir  model = reg_har_rvjs  return fig HAR_forecast_evaluation model  data_df  rv_name  horizon  Transform back Realized Variation  Mincer-Zarnowitz regression # ========Plot the forecast and true  plt.figure figsize= 15  5))  plt.plot compare_df['RV_true'])  plt.plot compare_df['RV_pred'])  diff = compare_df['RV_true'] - compare_df['RV_pred']  plt.figure figsize= 15  5))  plt.plot diff) # ========finish plot  MSE  HRMSE  QLIKE construct_regressors rv  c  jump  tc  t_jump  horizon  lags  rv_name  filter_jump  **kwargs  shift backward 1 day  so to form the forecast  RV_f is the dependant variable on the left hand side of regression equation  All below are explanatory variables  reg_df = pd.concat [RV_f  C_day  C_week  C_month  Jump_day  Jump_week  Jump_month]  axis=1)  Take different forms of RV and derived variables   Take logarithm and square root of variables  filter_jumps = ['AllJump'  'ConJump'  'NoJump']  return reg_df_out_all  reg_df_out_conjump  reg_df_out_nojump write_results_table result_df  model  model_name  pred_result HAR_fit reg_df  horizon  rv_name  b_jw * j_ t-lags[1]):t-1 +  b_jm * j_ t-lags[2]):t-1 +  === Choose mac lag for HAC  [7  14  60]  print 'Forecast horizon needs to be fixed')  =======test for data quality  for col in reg_df.columns:  nan_values = reg_df[reg_df[col].isna )]  plt.figure figsize= 15 5))  plt.plot reg_df[col])  plt.title col)  print col nan_values)  OLS estimation using HAC  Newey-West correction on residual term)  max lags = month lags + dependent variable lags  a = smf.ols HAR_TCJ  data=reg_df).fit cov_type='HAC'  cov_kwds={'maxlags': hac_max_lag})  a.summary )  ===========Finish test  print reg_har.summary ))  print reg_har_rvj.summary ))  print reg_har_rvjs.summary ))  print reg_har_rvtj.summary ))  print reg_har_rvtjs.summary ))  print reg_har_cj.summary ))  print reg_har_cjs.summary ))  print reg_har_tcj.summary ))  print reg_har_tcjs.summary ))  print reg_har_exp_rv_j.summary ))  print reg_har_exp_rv_js.summary ))  print reg_har_exp_rv_tj.summary ))  print reg_har_exp_rv_tjs.summary ))  print reg_har_exp_cj.summary ))  print reg_har_exp_cjs.summary ))  print reg_har_exp_tcj.summary ))  print reg_har_exp_tcjs.summary )) __init__ self  coin  freq  alpha  cv  tz_lag  rv_name  filter_jump  horizon  split_date  self.pred_split = pred_split  fixed arguments  Lags for daily  weekly and monthly read_all_variables self forecast_evaluate_record self  pred_split  plot_error  models = self.models  rv_pred is the forecast realized volatility  print model) model_fit self  save  construct all variables  Check if RV variables are read  self.rv  self.jump  self.t_jump  self.c  self.tc  self.logreturn  pred_split = 'train'  In sample fit models  evaluate forecasting power and write results to csv file insample_forecast self  save  plot_error  Evaluate the in sample forecast results out_sample_forecast self  save  plot_error  pred_split = 'test'  Evaluate the our of sample forecast results main_HAR_estimate_func   === ADJUST GLOBAL PARAMETERS IN FILE: GlobalParams.py  self.record_results self.models  self.reg_df  rv_name  filter_jump  horizon) quick_test_func   Quick Test  coins = ['gemini_BTC'  'gemini_ETH'  'BTC'  'ETH']  #   'XRP'  'LTC'  freqs = ['5min']  alphas = [0.9999]  cvs = [1  2  3  4  5]  alpha = alphas[0]  pred_split = 'train'  HAR_models.refresh = False  test regressors construction  out_sample_reg = construct_regressors rv=rv  c=c  tc=tc  jump=jump  t_jump=t_jump  horizon=horizon  lags=lags                                         rv_name=rv_name  filter_jump=filter_jump  pred_split='test'                                         split_date=split_date)  har_models = HAR_models.model_fit save=True)  HAR_models.model_fit save=True)  Execute Code.GlobalParams Code.RCVJ_TimeSeriesAnalysis.HAR_TCJ_Models HARTCJ_OLS_Estimation datetime matplotlib pandas numpy pickle itertools   Control of different of versions  self.rolling_win_size = rolling_win_size  Output dir  Parameters  Expect excess return = sr^2 / gamma  sr=1  gamma=2  self.gamma = 10  self.r_f = 0.06  self.models_ind = {name:num for name  num in zip model_names range 0 17))}  Functions to estimate models preprocess_data self  read_results=True  HAR instance  self.har_estimation.annualized = self.annualized  Fit the model calculate_realized_utility self  self.uow = pd.DataFrame columns=model_names) save_results self  self.uow = self.uow.to_frame ) realized_utility exp_exreturn  RV_exp  rv_t1  RV_exp = RV_exp.shift 1)  uow =uow_t.mean ) optimal_weight r_exp  r_f  gamma  RV_exp  col_name  col = 'HAR'  shift to next day to calculate utility annualized_dailyreturn logreturn_hf  window_size  annual_daily_return =  daily_return +1) ** 365 - 1  rolling_annual_return =  rolling_daily_return + 1) ** 365 - 1 average_utility weight_f_t1  rv_t  r_t  gamma  r_f  average_utility = prem_r -  gamma/2) * utility_df.rv  average_utility = prem_r -  gamma/ 2* 1+gamma))) * utility_df.rv  average_utility = prem_r -  gamma/2) *  prem_r**2) main_func   adjust as testing mode  Calculate realized utility  Save realized utility results test_func   Quick Test  utility_estimate.reg_df  utility_estimate.forecast_rv ),614
RCVJ_TimeSeriesAnalysis,"This quantlet includes two time series analysis code files. The first one is the HAR forecasting model calibration, in-sample and out-of-sample forecasting. The second one is calculating the realized utility function documented on Bollerslev et al (2018).","Realized volatility, Heterogenous Autoregression (HAR), Realized utility function, Cryptocurrencies, Global Market Indices, ACF",Junjie Hu,RCVJ_TimeSeriesAnalysis,4,pandas numpy datetime Code.RealizedVolatility.CAL_RV_BPV CAL_AllRVs matplotlib statsmodels Code.GlobalParams itertools product itertools pickle plot_forecast_errors model  reg_df  rv_name  horizon  save_dir  model = reg_har_rvjs  return fig HAR_forecast_evaluation model  data_df  rv_name  horizon  Transform back Realized Variation  Mincer-Zarnowitz regression # ========Plot the forecast and true  plt.figure figsize= 15  5))  plt.plot compare_df['RV_true'])  plt.plot compare_df['RV_pred'])  diff = compare_df['RV_true'] - compare_df['RV_pred']  plt.figure figsize= 15  5))  plt.plot diff) # ========finish plot  MSE  HRMSE  QLIKE construct_regressors rv  c  jump  tc  t_jump  horizon  lags  rv_name  filter_jump  **kwargs  shift backward 1 day  so to form the forecast  RV_f is the dependant variable on the left hand side of regression equation  All below are explanatory variables  reg_df = pd.concat [RV_f  C_day  C_week  C_month  Jump_day  Jump_week  Jump_month]  axis=1)  Take different forms of RV and derived variables   Take logarithm and square root of variables  filter_jumps = ['AllJump'  'ConJump'  'NoJump']  return reg_df_out_all  reg_df_out_conjump  reg_df_out_nojump write_results_table result_df  model  model_name  pred_result HAR_fit reg_df  horizon  rv_name  b_jw * j_ t-lags[1]):t-1 +  b_jm * j_ t-lags[2]):t-1 +  === Choose mac lag for HAC  [7  14  60]  print 'Forecast horizon needs to be fixed')  =======test for data quality  for col in reg_df.columns:  nan_values = reg_df[reg_df[col].isna )]  plt.figure figsize= 15 5))  plt.plot reg_df[col])  plt.title col)  print col nan_values)  OLS estimation using HAC  Newey-West correction on residual term)  max lags = month lags + dependent variable lags  a = smf.ols HAR_TCJ  data=reg_df).fit cov_type='HAC'  cov_kwds={'maxlags': hac_max_lag})  a.summary )  ===========Finish test  print reg_har.summary ))  print reg_har_rvj.summary ))  print reg_har_rvjs.summary ))  print reg_har_rvtj.summary ))  print reg_har_rvtjs.summary ))  print reg_har_cj.summary ))  print reg_har_cjs.summary ))  print reg_har_tcj.summary ))  print reg_har_tcjs.summary ))  print reg_har_exp_rv_j.summary ))  print reg_har_exp_rv_js.summary ))  print reg_har_exp_rv_tj.summary ))  print reg_har_exp_rv_tjs.summary ))  print reg_har_exp_cj.summary ))  print reg_har_exp_cjs.summary ))  print reg_har_exp_tcj.summary ))  print reg_har_exp_tcjs.summary )) __init__ self  coin  freq  alpha  cv  tz_lag  rv_name  filter_jump  horizon  split_date  self.pred_split = pred_split  fixed arguments  Lags for daily  weekly and monthly read_all_variables self forecast_evaluate_record self  pred_split  plot_error  models = self.models  rv_pred is the forecast realized volatility  print model) model_fit self  save  construct all variables  Check if RV variables are read  self.rv  self.jump  self.t_jump  self.c  self.tc  self.logreturn  pred_split = 'train'  In sample fit models  evaluate forecasting power and write results to csv file insample_forecast self  save  plot_error  Evaluate the in sample forecast results out_sample_forecast self  save  plot_error  pred_split = 'test'  Evaluate the our of sample forecast results main_HAR_estimate_func   === ADJUST GLOBAL PARAMETERS IN FILE: GlobalParams.py  self.record_results self.models  self.reg_df  rv_name  filter_jump  horizon) quick_test_func   Quick Test  coins = ['gemini_BTC'  'gemini_ETH'  'BTC'  'ETH']  #   'XRP'  'LTC'  freqs = ['5min']  alphas = [0.9999]  cvs = [1  2  3  4  5]  alpha = alphas[0]  pred_split = 'train'  HAR_models.refresh = False  test regressors construction  out_sample_reg = construct_regressors rv=rv  c=c  tc=tc  jump=jump  t_jump=t_jump  horizon=horizon  lags=lags                                         rv_name=rv_name  filter_jump=filter_jump  pred_split='test'                                         split_date=split_date)  har_models = HAR_models.model_fit save=True)  HAR_models.model_fit save=True)  Execute,446
RCVJ_TimeSeriesAnalysis,"This quantlet includes two time series analysis code files. The first one is the HAR forecasting model calibration, in-sample and out-of-sample forecasting. The second one is calculating the realized utility function documented on Bollerslev et al (2018).","Realized volatility, Heterogenous Autoregression (HAR), Realized utility function, Cryptocurrencies, Global Market Indices, ACF",Junjie Hu,RCVJ_TimeSeriesAnalysis,4,Code.GlobalParams Code.TimeSeriesAnalysis.HAR_TCJ_Models HARTCJ_OLS_Estimation datetime matplotlib pandas numpy pickle itertools Code.TimeSeriesAnalysis.dm_test dm_test   Control of different of versions  self.rolling_win_size = rolling_win_size  Output dir  Parameters  Expect excess return = sr^2 / gamma  sr=1  gamma=2  self.gamma = 10  self.r_f = 0.06  self.models_ind = {name:num for name  num in zip model_names range 0 17))}  Functions to estimate models preprocess_data self  read_results=True  HAR instance  self.har_estimation.annualized = self.annualized  Fit the model calculate_realized_utility self  self.uow = pd.DataFrame columns=model_names) dm_test_realized_utility self  base_model='HAR'  plot_error=False  pred1_lst is the based prediction series  DM > 0 pred2_lst has smaller errors  vice versa save_results self  self.uow = self.uow.to_frame ) realized_utility exp_exreturn  RV_exp  rv_t1 optimal_weight r_exp  r_f  gamma  RV_exp  col_name  col = 'HAR'  shift to next day to calculate utility annualized_dailyreturn logreturn_hf  window_size  annual_daily_return =  daily_return +1) ** 365 - 1  rolling_annual_return =  rolling_daily_return + 1) ** 365 - 1 average_utility weight_f_t1  rv_t  r_t  gamma  r_f  average_utility = prem_r -  gamma/2) * utility_df.rv  average_utility = prem_r -  gamma/ 2* 1+gamma))) * utility_df.rv  average_utility = prem_r -  gamma/2) *  prem_r**2) main_func   adjust as testing mode  Calculate realized utility  Save realized utility results test_func   Quick Test  utility_estimate.calculate_realized_utility )  ru_t = utility_estimate.ru_t.copy deep=True)  uow_t = utility_estimate.uow_t  utility_estimate.reg_df  utility_estimate.forecast_rv ),191
pyTSA_MacroUS2,"This Quantlet plots monthly time series of returns of Procter and Gamble from 1961 to 2016 and  their ACF and PACF (Example, 2.4 Figures 2.8-2.9 in the book)","time series, autocorrelation, returns, ACF, PACF, plot, visualisation","Huang Changquan, Alla Petukhina",pyTSA_MacroUSCointegration,1,numpy pandas matplotlib statsmodels statsmodels.tsa.stattools coint import statsmodels.api as sm,10
DEDA_Class_SS2018_ACO_RouteOptimisation,Use Ant Colony Optimisation(ACO) for Logistic Route Planning: A Case for Logistic Distribution of 7-Eleven Stores in the Xinyi District of Taipei.,"ant colony optimization, route optimization, retail logistics, google map geocoding API, google map distance matrix API, folium map, web scraping",Min-Bin Lin,ACO_RouteOptimisation,4,!/usr/bin/env python2  -*- coding: utf-8 -*-  import packages         dill operator itemgetter collections OrderedDict polyline folium googlemaps ACO_DataCollection Data ACO_Algorithm MMAS  google map api  2 500 free requests per day): geocoding and distance matrix  api key: https://developers.google.com/maps/documentation/geocoding/get-api-key  Pickle file names ###########################################################  load required data ###########################################################  data processing  sort the distance dictionary by values ###########################################################         apply ACO ### ########################################################### ## polyline ###  generate polyline for each path from google direction api  only compute till the last place ###########################################################         ## map the route ###  layer control   icon in the map  colour warehouse specially  point out the last shop 253494' '#2c7fb8' '#41b6c4' '#a1dab4' '#ffffcc']  decode all the polyline  overview_polyline)  map the path,108
DEDA_Class_SS2018_ACO_RouteOptimisation,Use Ant Colony Optimisation(ACO) for Logistic Route Planning: A Case for Logistic Distribution of 7-Eleven Stores in the Xinyi District of Taipei.,"ant colony optimization, route optimization, retail logistics, google map geocoding API, google map distance matrix API, folium map, web scraping",Min-Bin Lin,ACO_RouteOptimisation,4, -*- coding: utf-8 -*-  import packages dash dash_html_components numpy pandas ###########################################################  load data ###########################################################  highlight method  for dataframe) ffff33'  e4f6f8'  ff66ff'          62c2cc'  '#fdb813'  '#00ff7f'  '#f68b1f'] is_numeric val ColorRange val   max_val Generate_Table distance_matrix  max_rows=100  we need an empty column in the first square because the first row is also labels,49
DEDA_Class_SS2018_ACO_RouteOptimisation,Use Ant Colony Optimisation(ACO) for Logistic Route Planning: A Case for Logistic Distribution of 7-Eleven Stores in the Xinyi District of Taipei.,"ant colony optimization, route optimization, retail logistics, google map geocoding API, google map distance matrix API, folium map, web scraping",Min-Bin Lin,ACO_RouteOptimisation,4,"!/usr/bin/env python2  -*- coding: utf-8 -*-  import packages os requests itertools logging copy bs4 BeautifulSoup pandas numpy dill folium folium.plugins MarkerCluster scipy.spatial.distance squareform  google map api  2 500 free requests per day): geocoding and distance matrix  api key: https://developers.google.com/maps/documentation/geocoding/get-api-key googlemaps  Pickle file names ###########################################################  define required methods sort_distance distances  get first value in pair e.g.  `shop 135425` in    2623  'shop 135425 / shop 135942')  get items  we want to sort the list of lists  grab the sublists from the tuple  sort the list from biggest to smallest  sort the sublists __init__ self  name  address  entry_id  entry_type __repr__ self from_dict cls  de_dict to_dict self __init__ self __len__ self __repr__ self _next_id self  id_val  initial case  return 0 when not set  1...n case next_warehouse_id self add_shop self  shop_data add_warehouse self  wh_data find self  type_id  type_id str in format ""data.TYPES <int_id>"" get_pair self  pair_str split_pair cls  pair_str  like ""shop 119478 / warehouse 01"" make_type_id cls  place make_pair cls  place_one  place_two  take two places  return a string identifying the pair ###########################################################  ony run the codes when there  is no ""GEOCODED_DATA_PKL"" ########################################################### ## add warehouse ### ########################################################### ## 711s data ###        target area: Xinyi District  Taipei city zipcode: 110)  first is header ###########################################################  convert all the address into geocode  Latitude Longitude) ########################################################### ## create interaction map ###  create cluster  colour warehouse specially ###########################################################  create distance list  sorted with 0) and distance matrix  pairwise distance): in meter  get all combinations of every place with every other place  'OK' in  .... because the response strings are unicode  only have one row because we have one origin and one destination ## distance list ###  add self distance  0) into distance list  we want to group all 0 distances as the first in the list  and all elements with same name together  delete non-required data and save sorted distance ## distance matrix ###  extract distance values  form an squareform arrary  this is only for showing value on excel/number  data collection  pickle files",323
DEDA_Class_SS2018_ACO_RouteOptimisation,Use Ant Colony Optimisation(ACO) for Logistic Route Planning: A Case for Logistic Distribution of 7-Eleven Stores in the Xinyi District of Taipei.,"ant colony optimization, route optimization, retail logistics, google map geocoding API, google map distance matrix API, folium map, web scraping",Min-Bin Lin,ACO_RouteOptimisation,4,"!/usr/bin/env python2  -*- coding: utf-8 -*-  import package random math pow time logging itertools matplotlib ########################################################### ## max-min ant System  MMAS) ###  parameter setting  only type+number  inital shortest distance  inf)  problem construction addPlace self   create an empty dictionary for storing pheromones   create an empty dictionary for storing pheromone_deltas  create an empty list to store best tour  route)  PutAnts self  same as clear )  the inital point is ""warehouse 0"" for each ant  define the searching method   Search self  for checking feasibility  same as two_opt_search                 add the distance from end point to the inital place  only keep the best one        TabuList records every solution that have been visited  current distance < inital predefined shortest distance  replace it with current one  travel back to the initial place  update information UpdatePheromoneTrail self  only the best ant generate pheromone deltas  1/Lk  exclude  i i) ########################################################### ## the behaviour of ants ###    __init__ self  currPlace  place_names  dist_dict  pheromones  avaliable places  SelectNextPlace self  alpha  beta  no avaliblae destination  get 16 closest locations  selection probability for each place  roulette wheel MoveToNextPlace self  alpha  beta  exclude current place for monving to next step UpdatePathLen self  swape the sequence local_search self",192
SFEBarrier_Pricing_Tree,"Computes Barrier option prices using a binomial tree for assets with/without continuous dividends. barrier option types: up-and-out, up-and-in, down-and-out, down-and-in","binomial, tree, asset, call, put, option, option-price, barrier-option, up-and-out, up-and-in, down-and-out, down-and-in",Franziska Wehrmann,SFM_BarrierPricing_Tree,1,numpy matplotlib calc_parameters T  N  sigma  r  div  P up movement) knockout_01 assets  B  barrier_type calc_price S0  K  u  d  N  r  dt  q  option  barrier_type  calculate the values at maturity T  Next we use the recursion formula for pricing in the CRR model: calc_price_european S0  K  u  d  N  r  dt  q  option  calculate the values at maturity T  Using the recursion formula for pricing in the CRR model:  from  T-dt  T-2*dt  ....  dt  0) ###### MAIN ################  current stock price  strike price  time to maturity  volatility  interest rate  dividend  steps in tree  barrier  calculate all prices,98
FRM-EM-paper,Uplifted hierarchical risk parity based FRM portfolio construction.,"FRM (Financial Risk Meter), Emerging Markets, Uplifted Hierarchical Risk Parity, Inverse lambda, Min Variance",Souhir Ben Amor,FRM-EM-paper,2,"This file implements the upHRP algorithm  the MinVAr portfolio and the InvLamda portf. Code for classical HRP is based on Lopez de Prado  M.  2018). Advances in Financial  Machine Learning. Wiley. The code has been modified to create an uplifted portfolio strategies based on FRM adjacency mtrices and its adapted in order to be used with python 3 and the data set. Souhir Ben Amor @date: 20201010 """""" [0] Import library numpy matplotlib pandas [0]Upload input data  Financial Institutions of the 6 Emerging Markets and adjacency matrix  In[1]:   Load modules os  Set Working directory here  Import modules for Datastructuring and calc. pandas numpy scipy stats warnings tqdm tqdm  Modules for RHP algorithm matplotlib scipy  Modules for the network plot networkx networkx.convert_matrix from_numpy_matrix  Modules for Markowitz optimization cvxopt cvxopt  suppress warnings in clustering  In[2]:   define functions for HRP and IVP getIVP cov **kargs  Compute the inverse-variance portfolio getClusterVar cov  cItems  Compute variance per cluster  matrix slice getQuasiDiag link  Sort clustered items by distance  number of original items  make space  find clusters  item 1  item 2  re-sort  re-index getRecBipart cov sortIx  Compute HRP alloc  initialize all items in one cluster  bi-section  parse in pairs  cluster 1  cluster 2  weight 1  weight 2 correlDist corr  A distance matrix based on correlation  where 0<=d[i j]<=1   This is a proper distance metric  distance matrix plotCorrMatrix path  corr  labels=None  Heatmap of the correlation matrix  reset pylab  In[3]:   define function for MinVar portfolio  The MIT License  MIT)  Copyright  c) 2015 Christian Zielinski  Permission is hereby granted  free of charge  to any person obtaining a copy  of this software and associated documentation files  the ""Software"")  to deal  in the Software without restriction  including without limitation the rights  to use  copy  modify  merge  publish  distribute  sublicense  and/or sell  copies of the Software  and to permit persons to whom the Software is  furnished to do so  subject to the following conditions:  The above copyright notice and this permission notice shall be included in all  copies or substantial portions of the Software.  THE SOFTWARE IS PROVIDED ""AS IS""  WITHOUT WARRANTY OF ANY KIND  EXPRESS OR  IMPLIED  INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM  DAMAGES OR OTHER  LIABILITY  WHETHER IN AN ACTION OF CONTRACT  TORT OR OTHERWISE  ARISING FROM   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE  SOFTWARE.    min_var_portfolio cov_mat  allow_short=False  Constraints Gx <= h  x >= 0  Constraints Ax = b  sum x) = 1  Solve  Put weights into a labeled series  In[4]:   Define functions for network graphs Function to plot Network plots plotNetwork path corr  Transform it in a links data frame links=corr.stack ).reset_index ) Build graph  remove self-loops  replace values that are below threshold  create undirected graph from adj_matrix  set names to crypots  raise text positions # In[5]:  Loading and structuring crypto data sets  Deleting empty rows  define Date  as index  Calculating returns  calculate daily returns  Covariance matrix of the return matrix  Correlation matrix of the return matrix  define Date  as index  In[6]:  Heatmap and network analysis of corr. matrix  Plotting Correlation matrix heatmap  network plot of correlation matrix  Sort correlation matrix  recover labels   reorder  Plot sorted correlation matrix  Plot dendogram of the constituents 2) Cluster Data  reset pylab  In[7]: Function to calculate the HRP portfolio weights HRPportf cov corr 1) Cluster covariance matrix  recover labels 2) Allocate capital according to HRP  In[8]:  Compute the weights for the Markowitz MinVar and the HRP portfolio and the   IVP portfolio  Latex table output",593
FRM-EM-paper,Uplifted hierarchical risk parity based FRM portfolio construction.,"FRM (Financial Risk Meter), Emerging Markets, Uplifted Hierarchical Risk Parity, Inverse lambda, Min Variance",Souhir Ben Amor,FRM-EM-paper,2,"!/usr/bin/env python3  -*- coding: utf-8 -*- [0Import library numpy matplotlib pandas [0]Upload input data  Financial Institutions of the 6 Emerging Markets  In[1]:   Load modules os  Set Working directory here  Import modules for Datastructuring and calc. pandas numpy scipy stats warnings tqdm tqdm  Modules for RHP algorithm matplotlib scipy  Modules for the network plot networkx networkx.convert_matrix from_numpy_matrix  Modules for Markowitz optimization cvxopt cvxopt  suppress warnings in clustering  In[2]:   define functions for HRP and IVP getIVP cov **kargs  Compute the inverse-variance portfolio getClusterVar cov  cItems  Compute variance per cluster  matrix slice getQuasiDiag link  Sort clustered items by distance  number of original items  make space  find clusters  item 1  item 2  re-sort  re-index getRecBipart cov sortIx  Compute HRP alloc  initialize all items in one cluster  bi-section  parse in pairs  cluster 1  cluster 2  weight 1  weight 2 correlDist corr  A distance matrix based on correlation  where 0<=d[i j]<=1   This is a proper distance metric  distance matrix plotCorrMatrix path  corr  labels=None  Heatmap of the correlation matrix  reset pylab  In[3]:   define function for MinVar portfolio  The MIT License  MIT)  Copyright  c) 2015 Christian Zielinski  Permission is hereby granted  free of charge  to any person obtaining a copy  of this software and associated documentation files  the ""Software"")  to deal  in the Software without restriction  including without limitation the rights  to use  copy  modify  merge  publish  distribute  sublicense  and/or sell  copies of the Software  and to permit persons to whom the Software is  furnished to do so  subject to the following conditions:  The above copyright notice and this permission notice shall be included in all  copies or substantial portions of the Software.  THE SOFTWARE IS PROVIDED ""AS IS""  WITHOUT WARRANTY OF ANY KIND  EXPRESS OR  IMPLIED  INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM  DAMAGES OR OTHER  LIABILITY  WHETHER IN AN ACTION OF CONTRACT  TORT OR OTHERWISE  ARISING FROM   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE  SOFTWARE.    min_var_portfolio cov_mat  allow_short=False  Constraints Gx <= h  x >= 0  Constraints Ax = b  sum x) = 1  Solve  Put weights into a labeled series  In[4]:   Define functions for network graphs Function to plot Network plots plotNetwork path corr  Transform it in a links data frame links=corr.stack ).reset_index ) Build graph  remove self-loops  replace values that are below threshold  create undirected graph from adj_matrix  set names to crypots  raise text positions # In[5]:  Loading and structuring FIs_prices data sets  Deleting empty rows #FIs_prices[""date""] =FIs_prices['date'].map lambda x: str x)[:-9]) # Removing timestamp  define Date  as index  Calculating returns  calculate daily returns  Calculating covariance matrix  Covariance matrix of the return matrix  Correlation matrix of the return matrix  In[6]:  Heatmap and network analysis of corr. matrix  Plotting Correlation matrix heatmap  network plot of correlation matrix  Sort correlation matrix  recover labels   reorder  Plot sorted correlation matrix  Plot dendogram of the constituents 2) Cluster Data  reset pylab  In[7]: Function to calculate the HRP portfolio weights HRPportf cov corr 1) Cluster covariance matrix  recover labels 2) Allocate capital according to HRP  In[8]:  Compute the weights for the Markowitz MinVar and the HRP portfolio and the   IVP portfolio  Latex table output  In[9]:  Backtesting the three optimisation methods for FIs_prices dataset  Function to calculate the weigths in sample and then test out of sample Backtest_FIs_prices returns  rebal = 30  rebal = 30 default rebalancing after 1 month  Number of iterations without first set to train  Check for rebalancing date  In[10]:      Calculate the backtested portfolio returns  Calculate index  reset pylab  Calculate portfolio return and portfolio variance  Daily Standard deviation  Sharpe ratios  Latex table output",606
INET-eigcentr,Calculate the in- and out- eigenvector centrality scores of directed and weighted graphs,"Eigenvector, eigenvalue, centrality, network, graph, directed, weighted",Ya Qian,INET-eigcentr,2,!/usr/bin/env python  coding: utf-8  In[ ]: numpy matplotlib numpy genfromtxt operator networkx pandas csv  the series of 'beta_L_median' should be replaced with 'beta_L_lowertail' and 'beta_L_uppertail' when dealing with different cases print  mapping) in and out eigenvector centrality scores of each node,41
SDA_20201118_Final Project,The First Progress of Final Project of SDA I Class,ERROR,"Samingun Handoyo, Andreas Rony Wijaya",SDA_20201118_Final Project,1,pandas numpy statsmodels.tsa.api VAR statsmodels.tsa.stattools adfuller statsmodels.tools.eval_measures rmse statsmodels.tsa.stattools grangercausalitytests statsmodels.tsa.vector_ar.vecm coint_johansen matplotlib cointegration_test df  alpha=0.05 adjust val  length= 6): return str val).ljust length  Summary covid_ratio data  read data  Decorations plot for the Exchange rates  Decorations,36
Granger_Causality,Test for Granger causality among series,ERROR,"Samingun Handoyo, Andreas Rony Wijaya",Granger_Causality,1,pandas numpy statsmodels.tsa.api VAR statsmodels.tsa.stattools adfuller statsmodels.tools.eval_measures rmse statsmodels.tsa.stattools grangercausalitytests statsmodels.tsa.vector_ar.vecm coint_johansen matplotlib cointegration_test df  alpha=0.05 adjust val  length= 6): return str val).ljust length  Summary covid_ratio data  read data  Decorations plot for the Exchange rates  Decorations,36
Blockchain_mechanism_clustering,"Use distributional characteristics such as fourier power spectrum, moments, quantiles, global we optimums, as well as the measures for long term dependencies, risk and noise to summarise the information from crypto time series and conduct clustering via spectral clustering","Cryptocurrency, Blockchain mechanism , Distributional characteristics, Clustering, Box plot","Kainat Khowaja , Min-Bin Lin,",Blockchain_mechanism_clustering,2,"!/usr/bin/env python  coding: utf-8  In[8]: pandas io math sqrt matplotlib numpy datetime datetime scipy stats scipy fft math  scikit-learn bootstrap sklearn.utils resample sklearn.metrics mean_squared_error sklearn.metrics mean_absolute_error seaborn decimal Decimal timeit # Setting defaults for plots plt.rcParams['xtick.labelrotation'] = 45  # Data cleaning  In[9]: Loading the data dataset = 'Block_size' dataset = 'Block_time'  In[10]:  dealing with nulls  dropping nulls is not an option because we have time series.   Cutting time series is not an option because we want to have different starting points  Since we do not have many missing values after each coin is introduced  we use forward fill  If at any point we notice that the data is missing for a long time period  we will retrieve data from another channel  In[11]:  changing the column names To remove white space at the both ends of string: To replace white space everywhere  saving the clean data  In[12]:  enable if you want to take only the last year of observations  df = df.iloc[-366: :]  # Functions  In[13]: firstNonNan timeseries normalize timeseries standardize timeseries  # Analysis  In[14]:  we see that the scale of prices of different coins vary a lot. Maybe we can use returns instead    or should we normalize / standardize the prices???)  ### Formula for return rate    $ \text{Rate of return} = \frac{\text{current value - original value}}{\text{original value}}*100$  In[15]:  we see that ethereum  bitcoin and litecoin dominate the market return wise  ### Formula for normalizing    $ \text{Normalized series} = \frac{\text{current value - min values}}{\text{max value - min value}}*100$  In[16]:  we try normalizing here  In[17]:  detrending time series  df_detrend = df_detrend.apply normalize)  ### Formula for Standardizing     $ \text{Standardized series} = \frac{\text{current value -  mean}}{\text{standard deviation}}$	    In[18]:  In[19]:  choose the dataframe you want to use for fft and rename it here  # FFT    The FFT $y[k]$ of length $N$ of the a sequence $x[n]$ length $N$ is defined as  $$  y[k]=\sum_{n=0}^{N-1} e^{-2 \pi j \frac{k n}{N}} x[n]  $$      ### comments    You may have too little data for FFT/DWT to make sense. DTW may be better  but I also don't think it makes sense for sales data - why would there be a x-week temporal offset from one location to another? It's not as if the data were captured at unknown starting weeks.    FFT and DWT are good when your data will have interesting repetitive patterns  and you have A) a good temporal resolution  for audio data  e.g. 16000 Hz - I am talking about thousands of data points!) and B) you have no idea of what frequencies to expect. If you know e.g. you will have weekly patterns  e.g. no sales on sundays) then you should filter them with other algorithms instead.    DTW  dynamic time-warping) is good when you don't know when the event starts and how they align. Say you are capturing heart measurements. You cannot expect to have the hearts of two subjects to beat in synchronization. DTW will try to align this data  and may  or may not) succeed in matching e.g. an anomaly in the heart beat of two subjects. In theory...  #### Step 1    Perform a fast Fourier transform on the time series data. This decomposes your time series data into mean and frequency components and allows you to use variables for clustering that do not show heavy autocorrelation like many raw time series.    #### Step 2    If time series is real-valued  discard the second half of the fast Fourier transform elements because they are redundant.    #### Step 3    Separate the real and imaginary parts of each fast Fourier transform element.    #### Step 4    Perform model-based clustering on the real and imaginary parts of each frequency element.    #### Step 5    Plot the percentiles of the time series by cluster to examine their shape.    Alternately  you could omit the DC components of the fast Fourier transform to avoid your clusters being based on the mean and instead on the series defined by the Fourier transform  which represents the shape of the time series.    You will also want to calculate the amplitudes and phase angles from the fast Fourier transform so that you can explore the distribution of time series spectra within clusters. See this StackOverflow answer on how to do that for real-valued data.    You could also plot the percentiles of time series shape by cluster by computing the Fourier series from the amplitudes and phase angles  the resulting time series estimate will not perfectly match the original time series). You could also plot the percentiles of the raw time series data by cluster. Here is an example of such a plot  which came about from a harmonic analysis of NDVI data I just did today:    1st  25th  50th  75th  and 99th percentiles of period-level NDVI measures by clusters derived from model-based clustering using Mclust package in R    Finally  if your time series is not stationary  i.e.  mean and variance shift over time)  it may be more appropriate to use a wavelet transform rather than a Fourier transform. You would do so at the cost of information about frequencies while gaining information about location.  In[20]:  ## FFT   In[21]:  FFT with normalizing  defines how many frequencies should be taken  In[22]:  Calculation of x-axis  Create x-axis of frequencies in Hz L = np.arange 1  n_sample//2)); # Only plot the first half of freqs  In[23]:  Power spectrum  power per freq) # same as amplitude  Find all freqs with large power  Zero out all others  Zero out small Fourier coeffs. in Y  Inverse FFT for filtered time signal  In[24]:  Plotting each timeseries with its absolute frequency sorted)  plotting amplitudes:  amplitudes = 2 / n_samples * np.abs y_fft). I do 1/n because n is already divided by 2  ref: https://docs.scipy.org/doc/scipy/reference/tutorial/fft.html ax[i 2].bar freq[:n_sample//2] abs PSDclean[cols[i]].iloc[:n_sample//2])) ax[i 2].set_title 'Filtered signal')  In[26]:  calculating the characteristics tsfresh tsfresh !pip install hurst hurst compute_Hc !pip install nolds nolds  https://nolds.readthedocs.io/en/latest/  definitions given in   https://tsfresh.readthedocs.io/en/latest/api/tsfresh.feature_extraction.html#module-tsfresh.feature_extraction.feature_calculators https://tsfresh.readthedocs.io/en/latest/text/_generated/tsfresh.feature_extraction.feature_calculators.html  uncomment this if you want clustering without spectrum  PSD.loc['intercept'] = [i[0][1] for i in df.apply lambda a: tsfresh_calc.linear_trend a.dropna ) [{""attr"": ""intercept""}]))] PSD.loc['abs_energy'] = df.apply lambda a: tsfresh_calc.abs_energy a.dropna ))) PSD.loc['partial_autocorrelation'] = [i[0][1] for i in df.apply lambda a: tsfresh_calc.partial_autocorrelation a.dropna ) [{""lag"": 1}]))] PSD.loc['sample_entropy'] = df.apply lambda a: tsfresh_calc.sample_entropy a.dropna ))) PSD.loc['self_similarity'] = [i[0] for i in df.apply lambda a: compute_Hc a.dropna )))] PSD.to_csv 'data/price_characteristics.csv'  index=False) PSD.to_csv 'data/blocktime_characteristics.csv'  index=False) PSD.to_csv 'data/blocksize_characteristics.csv'  index=False)  In[27]: scipy  using the power for clustering df_fft_fixed_T = abs PSDclean.T) # using the cleaned power for clustering  In[28]: # Heirarchical Clustering sklearn.cluster AgglomerativeClustering printing clusters  In[29]: # Spectral Clustering sklearn.cluster SpectralClustering printing clusters  In[30]: # K-mean Clustering sklearn.cluster KMeans printing clusters  ## PCA  In[31]: sklearn.preprocessing StandardScaler  Separating out the features  Separating out the features  Standardizing the features  Separating out the target  In[32]: sklearn.decomposition PCA  In[33]:  In[34]: sklearn.decomposition PCA  In[35]: mpl_toolkits.mplot3d Axes3D  ax.text x  y  z  '%s' %  label)  size=20  zorder=1  color='k')",1121
SVCJ_MC,Simulation of the Euler-discretized Stochastic Volatility with Correlated Jumps model and Monte Carlo Option Pricing,"Euler discretization, Monte Carlo Option Pricing, SVCJ, Darrel Duffie",Jovanka Lili Matic,SVCJ_MC-1,2,!/usr/bin/env python  coding: utf-8  In[79]: math numpy pandas scipy numpy scipy matplotlib numpy datetime scipy optimize plt.style.use 'seaborn')  # sets the plotting style  In[80]:  SVCJ parameters  In[81]:  dt  time horizon in days trialrun dt     = 1/10 n      = 1000 m      = int 10* 1/dt)/10)  In[82]:  In[83]:  In[84]:  initial CRIX level  p. 20  In[85]:  In[86]:  In[87]:  In[88]:  Option pricing   For the price process to be a martingale  =risk-neutral)  this should be approximately $S_0$  6500).  In[89]:  Implied volatility  In[90]: callprice S K T sigma r callprice 100 100 1 0.2 0.05)  In[91]: Implied volatility for given strike solve sigma  cp  k  T result.x[0] callprice s0  k  T  result.x[0]  mu)  Normalise everything to $S_0=100$  In[92]:  In[93]: call_and_vol K  T  T maturity in years [call_and_vol 120 0.5)  callprice 100  120  0.5  0.98  r)] call_and_vol 120 0.5)  From here on we do everything on $S_0=100$  In[94]: strike.shape  In[95]:  European call & put option data  3 maturities) h5.close ) type data) data.info ) data.head )  In[102]:  In[ ]: # restrict to ATM options  In[ ]:  In[ ]: compute Black-Scholes calibrated parameter solve_bs sigma scipy.optimize root  In[ ]: mpl_toolkits.mplot3d Axes3D  set up canvas for 3D plotting  creates 3D plot,192
SFEacfma2,Plots the autocorrelation function of a MA(2) (moving average) process for different parameters.,ERROR,ERROR,QID-3164-SFEacfma2,1,pandas numpy statsmodels.graphics.tsaplots plot_acf statsmodels.tsa.arima_process arma_generate_sample matplotlib  parameter settings  lag value  add zero-lag and negate alphas,16
SFM_MND,Simulates a bivariate Normal distribution.,"Normal, bivariate, density",Daniel Traian Pele,SFM_MND,2,!/usr/bin/env python  coding: utf-8  In[10]: numpy matplotlib scipy.stats multivariate_normal mpl_toolkits.mplot3d Axes3D Parameters to set Create grid and multivariate normal Make a 3D plot  Change the size of plots  In[ ]:  In[ ]:,32
pyTSA_ReturnsPG2,"This Quantlet plots monthly time series of returns of Procter and Gamble from 1961 to 2016 and  their ACF and PACF (Example, 2.4 Figures 2.8-2.9 in the book)","time series, autocorrelation, returns, ACF, PACF, plot, visualisation","Huang Changquan, Alla Petukhina",pyTSA_ReturnsPG2,1,"pandas numpy matplotlib PythonTsa.plot_acf_pacf acf_pacf_fig statsmodels PythonTsa.LjungBoxtest plot_LB_pvalue scipy.stats norm arch arch_model statsmodels.graphics.api qqplot scipy stats  many pgret values are too small  which may affect convergence  of the optimizer when estimating the parameters  so rescale it.  nu denotes degree of freedom for the T distribution  arguments ""stats.t"" and ""distargs"" means T distribution and its  degree of freedom.",57
SDA_20201125_ProximityMeasure,Proximity measure for binary variables using Jaccard/Simple matching/Tanimoto methods,ERROR,Andreas Rony Wijaya,SDA_20201125_hw07_ProximityMeasure,2,!/usr/bin/env python  coding: utf-8  In[19]: pandas numpy sklearn.neighbors DistanceMetric scipy.spatial distance  In[20]: ## load dat file  ## retrieve Renault  Rover  Toyota  In[21]:  In[22]: ## transfer DataFrame to Numpy array  In[23]: ## calculate binary data ## x_mu ## fill binary matrix; if x i k)>x_mu k): 1  else 0  In[24]: ## Jaccard measure  In[25]: ## Simple matching  In[26]: ## Tanimoto measure,60
pyTSA_ReturnsPG,"This Quantlet plots monthly time series of returns of Procter and Gamble from 1961 to 2016 and  their ACF and PACF (Example, 2.4 Figures 2.8-2.9 in the book)","time series, autocorrelation, returns, ACF, PACF, plot, visualisation","Huang Changquan, Alla Petukhina",pyTSA_SeasonalityUSGDP,1,numpy pandas matplotlib statsmodels.tsa.seasonal seasonal_decompose statsmodels.tsa.statespace.tools diff PythonTsa.plot_acf_pacf acf_pacf_fig statsmodels.tsa.stattools kpss,11
DEDA_HClustering_tSNE_vs_PCA,Explanatory example for dimensionality reduction t-SNE vs PCA accompanying the hierarchical clustering project,"Dimensionality reduction, t-SNE, PCA, cluster analysis",Elizaveta Zinovyeva,DEDA_HClustering_tSNE_vs_PCA,3, Import modules mpl_toolkits.mplot3d Axes3D sklearn.datasets.samples_generator make_blobs sklearn.decomposition PCA sklearn.manifold TSNE matplotlib numpy  Fixing random state for reproducibility  Functions colors i  Simulate data  Plot original 3d data  PCA on 2-dimensions  Plot data with dimensionality reduction PCA  t-SNE on 2-dimensions  Plot data with dimensionality reduction PCA,44
DEDA_HClustering_tSNE_vs_PCA,Explanatory example for dimensionality reduction t-SNE vs PCA accompanying the hierarchical clustering project,"Dimensionality reduction, t-SNE, PCA, cluster analysis",Elizaveta Zinovyeva,DEDA_HClustering_tSNE_vs_PCA,3,!/usr/bin/env python  coding: utf-8  In[ ]:  Import modules mpl_toolkits.mplot3d Axes3D sklearn.datasets.samples_generator make_blobs sklearn.decomposition PCA sklearn.manifold TSNE matplotlib numpy  Fixing random state for reproducibility  Functions colors i  Simulate data  Plot original 3d data  PCA on 2-dimensions  Plot data with dimensionality reduction PCA  t-SNE on 2-dimensions  Plot data with dimensionality reduction PCA,50
AOBDL_LIME,"Antisocial Online behaivor detection. This part is dedicated to interpretability using LIME framework [Ribeiro, M. T. et. al  (2016, August). ""Why should i trust you?"" Explaining the predictions of any classifier.]",ERROR,Elizaveta Zinovyeva,AOBDL_LIME,4,!/usr/bin/env python  coding: utf-8  In[ ]: os numpy pandas gc  In[ ]: 2. Get the file  GOOGLE COLAB SETUP  Load the Drive helper and mount google.colab drive  This will prompt for authorization.  In[ ]: 3. Read file as panda dataframe  CHANGE TRAIN AND TEST  MIX TO GET SIMILAR DISTRIBUTION sklearn.model_selection train_test_split  In[ ]:  In[ ]:  In[ ]: warnings numpy pandas tensorflow tensorflow.keras.layers Dense tensorflow.keras.optimizers RMSprop tensorflow.keras.models Model tensorflow.keras.callbacks ModelCheckpoint transformers traitlets matplotlib seaborn tqdm.notebook tqdm tokenizers BertWordPieceTokenizer sklearn.metrics roc_auc_score tensorflow.keras backend sklearn.metrics roc_auc_score sklearn.metrics precision_score  In[ ]:  Create strategy from tpu  In[ ]:  First load the real tokenizer  Save the loaded tokenizer locally  Reload it with the huggingface tokenizers library  In[ ]: Transformer model is based on the following code: https://www.kaggle.com/miklgr500/jigsaw-tpu-bert-with-huggingface-and-keras  In[ ]: fast_encode texts  tokenizer  chunk_size=256  maxlen=512  In[ ]: build_model transformer  loss='binary_crossentropy'  max_len=512 x = tf.keras.layers.Dropout 0.35) cls_token)  In[ ]:  In[ ]:  In[ ]:  In[ ]:  In[ ]:  In[ ]:  In[ ]:  In[ ]:  In[ ]:  In[ ]:  In[ ]:  In[ ]:  In[ ]: lime lime_text  In[ ]: lime.lime_text LimeTextExplainer  In[ ]: fast_encode2 texts  tokenizer  chunk_size=256  maxlen=512 lime_classifier txt  In[ ]:  In[ ]:  In[ ]:  In[ ]:  In[ ]:  In[ ]:  In[ ]:  In[ ]:  Find 100 random toxic examples  In[ ]:  In[ ]:  In[ ]:  In[ ]:  In[ ]: toxicity text  df_toxic  In[ ]: tqdm tqdm  In[ ]:  In[ ]:  In[ ]: sklearn.preprocessing MinMaxScaler  In[ ]:  In[ ]:  In[ ]:  In[ ]:  In[ ]:  In[ ]:,238
AOBDL_LIME,"Antisocial Online behaivor detection. This part is dedicated to interpretability using LIME framework [Ribeiro, M. T. et. al  (2016, August). ""Why should i trust you?"" Explaining the predictions of any classifier.]",ERROR,Elizaveta Zinovyeva,AOBDL_LIME,4,!/usr/bin/env python  coding: utf-8  In[ ]: os numpy pandas gc  In[ ]: 2. Get the file  GOOGLE COLAB SETUP  Load the Drive helper and mount google.colab drive  This will prompt for authorization.  In[ ]:  GOOGLE COLAB SETUP  Load the Drive helper and mount google.colab drive  This will prompt for authorization. 2. Get the file 3. Read file as panda dataframe 2. Get the file 3. Read file as panda dataframe sklearn.model_selection train_test_split  In[ ]: sklearn.linear_model LogisticRegression tqdm.notebook tqdm sklearn.metrics roc_auc_score sklearn.metrics roc_auc_score sklearn.metrics precision_score  In[ ]:  In[ ]: lime lime_text sklearn.feature_extraction.text TfidfVectorizer  In[ ]: stop_words    = 'english'   In[ ]:  In[ ]: lime.lime_text LimeTextExplainer  In[ ]: sklearn.pipeline make_pipeline  In[ ]:  In[ ]:  In[ ]:  In[ ]:  In[ ]:  In[ ]:,119
SDA_FittingAnimal,Use parameters to fit a swimming fish,ERROR,"Piotr A. Zolnierczuk, Junjie Hu, Susan",SDA_FittingAnimal,1, -*- coding: utf-8 -*- matplotlib matplotlib animation numpy append matplotlib  elephant parameters parameters = [50 - 30j  18 + 8j  12 - 10j  -14 - 60j  20 + 20j  fish parameters parameters = [30 - 10j  20 + 20j  10 + 10j  20 + 50j  5 + 20j] fourier t  C elephant t  p init_plot   draw the body of the elephant  create trunk move_trunk i     x  y = elephant linspace 2.9 * pi  0.4 + 3.3 * pi  1000)  parameters)  move trunk to new position  but don't move eye stored at end or array)  initial the elephant body  initialize trunk,100
SC_open_source_scraping,"This Quantlet is dedicated to gathering data on the open source source codes of Solidity Smart Contracts using the API of https://etherscan.io/. Finally, the codes are parsed and preprocessed to extract comments and stored as a csv file.","API, parsing, scraping, regex, Ethereum, Solidity, smart contracts, Etherscan","Elizaveta Zinovyeva, Raphael Constantin Georg Reule",SC-open-source-scraping,4,"!/usr/bin/env python  coding: utf-8  In[ ]:  Used Etherscan.io APIs""  In[ ]: requests pandas numpy sys time os matplotlib tqdm tqdm  In[ ]: ## Load List of Contracts  In[ ]:  In[ ]:  In[ ]:  contants  In[ ]:  ### Create Folders  In[ ]:  In[ ]:  In[ ]:  In[ ]: scrape_ether_contract_and_write address_array  API_Token  time.sleep 0.01) # we can do 5 GET/POST requests per sec  save solidity source code  In[ ]:",67
SC_open_source_scraping,"This Quantlet is dedicated to gathering data on the open source source codes of Solidity Smart Contracts using the API of https://etherscan.io/. Finally, the codes are parsed and preprocessed to extract comments and stored as a csv file.","API, parsing, scraping, regex, Ethereum, Solidity, smart contracts, Etherscan","Elizaveta Zinovyeva, Raphael Constantin Georg Reule",SC-open-source-scraping,4,"!/usr/bin/env python  coding: utf-8  In[ ]: re pandas numpy pickle os tqdm tqdm  In[ ]: https://stackoverflow.com/questions/2319019/using-regex-to-remove-comments-from-source-files https://github.com/HektorLin/HU-IRTG/blob/master/Partition%20Codes%20and%20Comments.ipynb remove_comments string  first group captures quoted strings  double or single)  second group captures comments  //single-line or /* multi-line */) _replacer match  if the 2nd group  capturing comments) is not None   it means we have captured a non-quoted  real) comment string.  so we will return empty to remove the comment  otherwise  we will return the 1st group  captured quoted-string following the example above  distinguish group 1): """" or'' and group 2 true comments): \*.*\ or // return when a true comment is located leave_only_comments string  In[ ]:  In[ ]:  In[ ]:",107
StockCorr_Asset_Allocation,Use HRP method to get allocation weights for portfolio and create back test based on allocation,"stock, correlation, HRP, hierarchical risk parity, asset allocation, back test","Li Yilin, Mei Yuxin, Sun Qingwei, Xie Chuda, Zhang Yingxin",StockCorr_Asset_Allocation,3, -*- coding: utf-8 -*- pandas process numpy scipy os getIVP cov  Compute the inverse-variance portfolio  set inf to 0 getClusterVar cov cItems  Compute variance per cluster getHRP data sortIx cov.replace {np.nan:0} inplace=True)  Compute HRP alloc  initialize all items in one cluster  bi-section  parse in pairs  cluster 1  cluster 2  weight 1  weight 2 cluster data,55
StockCorr_Asset_Allocation,Use HRP method to get allocation weights for portfolio and create back test based on allocation,"stock, correlation, HRP, hierarchical risk parity, asset allocation, back test","Li Yilin, Mei Yuxin, Sun Qingwei, Xie Chuda, Zhang Yingxin",StockCorr_Asset_Allocation,3,os pandas matplotlib matplotlib seaborn scipy numpy logging imageio moviepy datetime datetime datetime timedelta scipy.cluster.hierarchy ClusterWarning warnings simplefilter warnings symbol_to_path symbol  please download from below link and put under downloaded_data/data folder   https://drive.google.com/file/d/1Uy0VmrkbKUAskGKAAQo45F8unrphAF14/view?usp=sharing get_symbols_from_file file_path get_data symbols  if first = True  construct df  df.set_index 'Date')  df.sort_index inplace=True) construct_index sym_file  index_file_name  use simple average of price here for index construction sector_index  industry_index  correlDist corr getQuasiDiag link cluster_plot data  start  end  figname  cluster=True run sym_file  category  data.to_csv 'data_{}.csv'.format category)) animate sym_file  category  start  end  interval=90  window=360  cluster=True main   run 'sp500_symbol.csv'  'SP500'),88
StockCorr_Asset_Allocation,Use HRP method to get allocation weights for portfolio and create back test based on allocation,"stock, correlation, HRP, hierarchical risk parity, asset allocation, back test","Li Yilin, Mei Yuxin, Sun Qingwei, Xie Chuda, Zhang Yingxin",StockCorr_Asset_Allocation,3, -*- coding: utf-8 -*- pandas numpy matplotlib matplotlib scipy stats process os get_cum_ret data get_max_drawdown vec plot_ret data benchmark plot_IC data  get factor data  get ret data  get SP500 data  ic  cumret  performance,33
CBR_BK,"This Quantlet is an example of training case-based reasoning and compared with other machine learning methods. The dataset is the German credit data publicly available in UCI. The credit reform data used in paper is confidential and can be collected based on requests from the Blockchain Research Center (BRC, https://blockchain-research-center.de/)","Decision support system, Bankruptcy prediction, Case-based reasoning, Explainable machine learning, Particle swarm optimization","Wei Li, Wolfgang Karl H ̈ardl",CBR_BK,3,!/usr/bin/env python3  -*- coding: utf-8 -*- sklearn.ensemble ExtraTreesClassifier sklearn.feature_selection chi2 skrebate ReliefF get_importance X_train  y_train  name,16
CBR_BK,"This Quantlet is an example of training case-based reasoning and compared with other machine learning methods. The dataset is the German credit data publicly available in UCI. The credit reform data used in paper is confidential and can be collected based on requests from the Blockchain Research Center (BRC, https://blockchain-research-center.de/)","Decision support system, Bankruptcy prediction, Case-based reasoning, Explainable machine learning, Particle swarm optimization","Wei Li, Wolfgang Karl H ̈ardl",CBR_BK,3,!/usr/bin/env python3  -*- coding: utf-8 -*- numpy pandas sklearn.preprocessing RobustScaler math time random sklearn.linear_model LogisticRegression sklearn.svm SVC sklearn.neighbors KNeighborsClassifier sklearn.tree DecisionTreeClassifier sklearn.ensemble RandomForestClassifier sklearn.model_selection train_test_split sklearn.metrics precision_score sklearn.model_selection KFold warnings timeit default_timer collections Counter numba jit numba cuda numba pyswarms logging  only show error sklearn preprocessing sklearn.preprocessing MinMaxScaler measure_others measure_others get_importance get_importance dist_computation w  p  p1  m  r q get_result k  dist  y_train  y_val  from sklearn.linear_model import Lasso compute_sim w  p  p1  m  ref  query  k  dist test w p p1 m q r get_result1 k  dist  y_train  y_test ############################################################################################## Importing the data ##############################################################################################  amount of fraud classes      Shuffle dataframe rows ############################################################################################ ############################################################################################ ############################################################################################## ############################################################################################## ############################################################################################# ############################################################################################## scale_imp importances ################################################################################################## #################################################################################################### get_f params pso  Dimension of X  Dimension of X opt_func X  number of particles ############################################################################################### ###############################################################################################    ############################################################################################### ############################################################################################### #################################################################################################   ##################################################################################################,131
CBR_BK,"This Quantlet is an example of training case-based reasoning and compared with other machine learning methods. The dataset is the German credit data publicly available in UCI. The credit reform data used in paper is confidential and can be collected based on requests from the Blockchain Research Center (BRC, https://blockchain-research-center.de/)","Decision support system, Bankruptcy prediction, Case-based reasoning, Explainable machine learning, Particle swarm optimization","Wei Li, Wolfgang Karl H ̈ardl",CBR_BK,3,!/usr/bin/env python3  -*- coding: utf-8 -*-  Classifier Libraries sklearn.linear_model LogisticRegression sklearn.svm SVC sklearn.neighbors KNeighborsClassifier sklearn.tree DecisionTreeClassifier sklearn.ensemble RandomForestClassifier sklearn.naive_bayes GaussianNB sklearn.metrics confusion_matrix scipy.stats uniform sklearn.model_selection cross_val_score sklearn.metrics precision_score  Use GridSearchCV to find the best parameters. sklearn.model_selection GridSearchCV measure_others X_train  y_train X_test y_test  Logistic Regression   We automatically get the logistic regression with the best parameters.  KNears best estimator  k nearest   Support Vector Classifier  SVC best estimator  DecisionTree Classifier  tree best estimator ####################################################################################### ##############################################################################################     ####################################################################################### ,73
Financial Report Analysis BERT W2V GLOVE,To extract and via word embedding methods to visualise financial reports.,"BERT, w2v, GLOVE, text analysis, LLE, dimension reduction",Francis Liu,Financial-Report-Analysis-BERT-W2V-GLOVE,10,"typing Any torch transformers BertTokenizer nltk nltk.tokenize sent_tokenize numpy pandas  OPTIONAL: if you want to have more information on what's happening  activate the logger as follows logging  logging.basicConfig level=logging.INFO) matplotlib  % matplotlib inline  Load pre-trained model tokenizer  vocabulary)  max_sent_length = 512 # BERT Base load_model   Load pretrain model of BERT text_tokening corporate_profile_text  tokenizer=tokenizer  verbose=0  Sentence tokenization  Remove unwanted characters  this function should be factored out later)  for checking  Turn index back to tokens  Print out token and id pair for checking  loop over each sentence  loop over each tokens and ids company2vector sent_token_id  model  average_over_sent=True  Turn corporate Profile to vector  Take the last hidden layer outputs and average over sentence and words  to numpy vector_of_each_whole_token corporate_profile_tokens  result  Indexing subwords  Sentence level  Token level  [second output][sent][the tokens] #""):  Taking mean of sub words  For each sentence in last layer  For each token in sentences  Taking mean over sub words  Create Pandas DataFrame",151
Financial Report Analysis BERT W2V GLOVE,To extract and via word embedding methods to visualise financial reports.,"BERT, w2v, GLOVE, text analysis, LLE, dimension reduction",Francis Liu,Financial-Report-Analysis-BERT-W2V-GLOVE,10,!/usr/bin/env python  coding: utf-8  In[1]:  In[129]:  In[ ]: PyPDF2 re  In[ ]: f_correction text  Correct the error made by pdf extractor  In[ ]: get_corporate_profile file_name  Take the text in the cooperate profile page  Seach for the first page with the word 'company profile'  print p)  In[133]:  In[ ]:,48
Financial Report Analysis BERT W2V GLOVE,To extract and via word embedding methods to visualise financial reports.,"BERT, w2v, GLOVE, text analysis, LLE, dimension reduction",Francis Liu,Financial-Report-Analysis-BERT-W2V-GLOVE,10,"PyPDF2 re f_correction text  Correct the error about ""fl"" and ""fi"" made by pdf extractor get_corporate_profile file_name  Take the text in the cooperate profile page  TCregex = r""table[\s{ 3}|\n]of[\s{ 3}|\n]content""  Seach for the first page with the word 'company profile which is not table of content'  need to check whether there is enough content  print p)",56
Financial Report Analysis BERT W2V GLOVE,To extract and via word embedding methods to visualise financial reports.,"BERT, w2v, GLOVE, text analysis, LLE, dimension reduction",Francis Liu,Financial-Report-Analysis-BERT-W2V-GLOVE,10,!/usr/bin/env python  coding: utf-8  In[ ]:  In[ ]: torch transformers BertTokenizer nltk nltk.tokenize sent_tokenize numpy  OPTIONAL: if you want to have more information on what's happening  activate the logger as follows logging logging.basicConfig level=logging.INFO) matplotlib  % matplotlib inline  Load pre-trained model tokenizer  vocabulary)  max_sent_length = 512 # BERT Base  In[7]: google.colab drive  In[8]:  In[ ]:  In[ ]:  Load Testing Data  In[ ]:  s.insert 0  '[CLS]')  s.insert -1  '[SEP]')  In[ ]:  In[13]:  For more information  In[ ]:  In[16]:  In[ ]:  In[20]:  In[ ]:  In[ ]:  In[57]:  In[ ]:  In[ ]: seaborn matplotlib numpy  In[ ]:  In[70]:  In[ ]:,97
Financial Report Analysis BERT W2V GLOVE,To extract and via word embedding methods to visualise financial reports.,"BERT, w2v, GLOVE, text analysis, LLE, dimension reduction",Francis Liu,Financial-Report-Analysis-BERT-W2V-GLOVE,10,!/usr/bin/env python  coding: utf-8  In[1]: numpy pandas sklearn.manifold TSNE matplotlib pyplot  In[5]:  In[2]:  In[3]:  In[4]:  In[5]:  In[6]:  In[7]:  In[8]:  prepare for plotting  In[13]:  In[14]: gensim.models.word2vec Word2Vec gensim  In[15]:  In[16]:  In[17]:  In[18]:  In[19]:  In[20]:  In[21]: gensim.models.keyedvectors KeyedVectors  model = Word2Vec.load 'GoogleNews-vectors-negative300.bin.gz')  In[22]:  In[23]:  In[32]:  In[33]:  In[34]:  In[ ]:,47
Financial Report Analysis BERT W2V GLOVE,To extract and via word embedding methods to visualise financial reports.,"BERT, w2v, GLOVE, text analysis, LLE, dimension reduction",Francis Liu,Financial-Report-Analysis-BERT-W2V-GLOVE,10,!/usr/bin/env python  coding: utf-8  In[1]:  In[ ]:  In[3]:  In[4]:  In[5]: companytoVector_BERT corporate_profile_extrator google.colab files os glob pandas  In[ ]:  In[7]:  In[ ]:  for debugging  np.save './outputs/'+name  result)  In[ ]:  In[12]:  In[14]:  In[ ]:  In[ ]:  In[ ]:  In[ ]:  Check time needed to run  In[ ]:,46
sentiment_analysis_2,Project_Group8 file,"NUS, FE5225, sentiment analysis, deep learning, second step",Wang Yuzhou,sentiment_analysis_2,3, -*- coding: utf-8 -*- paddle paddlehub 加载预训练模型 准备数据 for e in ds.get_train_examples ):  print e.text_a  e.label) 数据预处理：切词 配置优化策略 设置运行配置,19
sentiment_analysis_2,Project_Group8 file,"NUS, FE5225, sentiment analysis, deep learning, second step",Wang Yuzhou,sentiment_analysis_2,3, -*- coding: utf-8 -*- __future__ absolute_import __future__ division __future__ print_function collections namedtuple codecs os csv paddlehub.dataset InputExample __init__ self _load_train_examples self _load_dev_examples self _load_test_examples self get_train_examples self get_dev_examples self get_test_examples self get_labels self num_labels self _read_tsv self  input_file  quotechar=None  skip header,41
sentiment_analysis_2,Project_Group8 file,"NUS, FE5225, sentiment analysis, deep learning, second step",Wang Yuzhou,sentiment_analysis_2,3," -*- coding: utf-8 -*- pandas numpy gensim os jieba codecs import pydot gensim.models word2vec gensim.corpora.dictionary Dictionary sklearn.model_selection train_test_split keras.preprocessing.sequence pad_sequences keras.models Sequential keras.layers Embedding keras.utils to_categorical keras.utils.vis_utils plot_model itertools chain 读取并预处理数据 seg_word sentences 去除停用词 print seg_list)  seg_result = list chain *seg_list)) print seg_result) 读取停用词文件  去除停用词      word_list=list filter lambda x: x not in stopwords  seg_list)) tst=pd.Series [""中石化给大家发福利了"" ""这股完了，别再唱多了""]) seg_word tst) __init__ self  dirname __iter__ self word2vec_model sentences sentences = MySentences 'datasets') # a memory-friendly iterator  嵌入的维度 model['生活'] model.most_similar ['生活'])  取得所有单词  print vocab_list)  每个词语对应的索引  word_index = Dictionary ) word_index.doc2bow model.wv.vocab.keys ) allow_update=True)  print word_index)  w2indx = {v: k+1 for k  v in word_index.items )}#所有频数超过10的词语的索引  print w2indx)  w2vec = {word: model[word] for word in w2indx.keys )}  序列化 get_index sentence get_split_set data sentences  截长补短  取得标签  划分数据集 构建分类模型  让 Keras 的 Embedding 层使用训练好的Word2Vec权重 LSTM_model model X_train  X_test  Y_train  Y_test  weights=[embedding_matrix]   trainable=True)) 加载模型并形成初始权重矩阵 加载训练好的模型 模型中有的单词向量就用该向量表示，没有的则随机初始化一个向量  del cal_len'''",141
CRIX_stacking,Time series data preparation for simulation and analysis with GAN. Visualisation with stacking image of the CRIX time series both for the original series and its log returns.,"GAN, CRIX, time series, non stationarity, simulation",Ramona Merkl,CRIX_stacking,2,!/usr/bin/env python  coding: utf-8  In[1]: pandas numpy seaborn matplotlib statsmodels.graphics.tsaplots plot_acf matplotlib.colors ListedColormap datetime datetime PIL Image  In[82]:  In[83]:  In[84]:  In[85]:  In[59]: __init__ self path start scale_data self a b print 'min value in df:'+str np.min self.data.price_sc))) print 'max value in df:'+str np.max self.data.price_sc))) print self.data['price_sc']) ts_image self n_rows n_cols  image  1 sequentialize the data print '############') print str idx)+'/'+str self.data.price_sc.shape[0]-self.seq_len-1)) last 3 rows flip to right dimension ts_matrix = np.flip ts_matrix axis=0) Plot1 print self.data['date']) print ts) ax3.set_ylim -1.0  1.0)  Hide the right and top spines  Only show ticks on the left and bottom spines plt.savefig 'singletsimage.png' transparent=True)  In[12]: ## construct training set ###  1) split into training and test set; cut at 2019  In[18]:  In[64]:  original time series  In[65]:  log-returns  In[72]: plt.savefig 'fig4.png'  transparent=True)  In[ ]:,128
pyTSA_GDPChinaSARIMA,This Quantlet builds plots time series and ACF of Chinese quarterly GDP from 1992 to 2017,"time series, autocorrelation, Chinese GDP, plot, visualisation","Huang Changquan, Alla Petukhina",pyTSA_GDPChinaSARIMA,1,pandas numpy statsmodels matplotlib PythonTsa.plot_acf_pacf acf_pacf_fig PythonTsa.ModResidDiag plot_ResidDiag  drop 1+4 = 5 residuals at head.,15
Word-cloud-Search-engine-optimisation,This Quantlet build takes in as an input a PDF filedecomposes it by words and counts them by occurance. The output is saved as a JSON file.,"NLP, word cloud, search engine optimisation, SEO",ERROR,Word-cloud-Search-engine-optimisation-,4,flask Flask logging Script generate_word_cloud ,5
Word-cloud-Search-engine-optimisation,This Quantlet build takes in as an input a PDF filedecomposes it by words and counts them by occurance. The output is saved as a JSON file.,"NLP, word cloud, search engine optimisation, SEO",ERROR,Word-cloud-Search-engine-optimisation-,4,"!/usr/bin/env python  coding: utf-8  # PDF converter  # Importing the PDF.   Important to set the initial addresses of the import and export file    In[1]: PyPDF2  ## Convertation of the PDF to a string  Transforming the PDF to string by page and combining it into one file  In[5]:  setting initial variables  initiating cycle  ## REGEX-pattern cleaning  Data must be cleaned of empty spaces  web links and other junk  In[6]: re  *~unfortunately not all cases can be accounted for due to the unorganised nature of the imported presentations*  ## Dictionary creation  Splitting the string into words  In[13]:  ## Counting all of the occurances  In[8]:  ## Creating the dataframe to sort the words  Also deleting ""-""'s that are thought as of words  In[9]: pandas numpy  ## Filtering the final dictionary  Counting the number of letters in the words and setting a barrier for 0.75 percentile to be the cutoff for word counter  In[16]:  ## Exporting the JSON  In[12]:",155
Word-cloud-Search-engine-optimisation,This Quantlet build takes in as an input a PDF filedecomposes it by words and counts them by occurance. The output is saved as a JSON file.,"NLP, word cloud, search engine optimisation, SEO",ERROR,Word-cloud-Search-engine-optimisation-,4,"PyPDF2 re pandas numpy converter_pdf_json_count pdf_address  file_name=""""  import the file  transforming the PDF   initial first page text extraction  initiating cycle for text extraction  data cleaning  creating a dictionary  splitting the string into words  counting the occurrences  transforming the dictionary-counter into a dataframe  deleting ""-""'s seen as words [this part can be extracted based on further data]  sorting data  counting the word length to delete   fixed dictionary  exporting the JSON file converter_pdf_json_count ""/Users/wkhaerdle/Documents/Quantlets/NLP/testpdf3.pdf""  ""/Users/wkhaerdle/Documents/Quantlets/NLP/testjson3.json""  output=""file"")",74
pyTSA_ReturnsSP500,"This Quantlet plots monthly time series of returns of Procter and Gamble from 1961 to 2016 and  their ACF and PACF (Example, 2.4 Figures 2.8-2.9 in the book)","time series, autocorrelation, returns, ACF, PACF, plot, visualisation","Huang Changquan, Alla Petukhina",pyTSA_ReturnsSP500,1,pandas matplotlib statsmodels PythonTsa.plot_acf_pacf acf_pacf_fig PythonTsa.LjungBoxtest plot_LB_pvalue arch arch_model statsmodels.graphics.api qqplot,11
pyTSA_ReturnsDAX,"This Quantlet plots monthly time series of returns of Procter and Gamble from 1961 to 2016 and  their ACF and PACF (Example, 2.4 Figures 2.8-2.9 in the book)","time series, autocorrelation, returns, ACF, PACF, plot, visualisation","Huang Changquan, Alla Petukhina",pyTSA_ReturnsDAX,1,pandas numpy matplotlib PythonTsa.LjungBoxtest plot_LB_pvalue,5
SDA_20201029_hw4_LDA_Analysis,LDA analysis of the abstracts of a set of scientifique papers to understand main topics that are handled in those papers.,ERROR,"Bousbiat Hafsa, Andreas Rony Wijaya",SDA_20201029_hw4_LDA_Analysis,2,"!/usr/bin/env python  coding: utf-8  In[ ]:  In[ ]: feedparser  In[3]: requests  take the website source code back to you urllib  some useful functions to deal with website URLs bs4 BeautifulSoup  a package to parse website source code numpy  all the numerical calculation related methods re  regular expression package itertools  a package to do iteration works pickle  a package to save your file temporarily pandas  process structured data os  In[ ]:  the path you save your files  This link can represent the domain of a series of websites  abs_folder = cwd_dir + 'Abstracts/'  os.makedirs abs_folder  exist_ok=True)  get source code  parse source code  if paper[3][-3:] == 'txt':  if paper[3][-3:] == 'pdf':      abstract_parsed = soup paper_abstract_page.content)      main_part = abstract_parsed.find_all 'body')[0].text.strip )  with open abs_folder + f""{re.sub '[^a-zA-Z0-9 ]'  ''  paper[0])}.txt""  'w'  encoding='utf-8') as abs_f:      abs_f.write main_part)  In[ ]:  In[ ]: random os re nltk os path nltk.stem WordNetLemmatizer nltk.stem.porter PorterStemmer nltk.corpus stopwords nltk.corpus stopwords nltk.stem.wordnet WordNetLemmatizer string gensim gensim corpora matplotlib pandas numpy  In[ ]: doc_l.pop )[0]  Regex cleaning  keep only letters  numbers and whitespace  apply regex  apply regex  lower case  nltk clean doc  Importing Gensim  Creating the term dictionary of our courpus  where every unique term is assigned an index.  Converting list of documents  corpus) into Document Term Matrix using dictionary prepared above.  Creating the object for LDA model using gensim library  Running and Trainign LDA model on the document term matrix. print ldamodel.print_topics num_topics=6  num_words=5))  20 need to modify to match the length of vocabulary   In[ ]:  In[8]: plt.imshow zz  cmap='hot'  interpolation='nearest') plt.show )  plt.title ""heatmap xmas song"")",257
Data_Analysis,Analyse the hourly returns of the collected data of the four coins.,"data, analysis","Georg Velev, Iliyana Pekova",Data_Analysis,1,matplotlib,1
quantitative_trading_model,Project_Group8 file,"NUS, FE5225, quantitative trading, sentiment factor, backtest",Wang Yuzhou,quantitative_trading_model,4,pandas numpy copy os  data: rowname: date colname : stocks __init__ self  factor_path  direction  fwdrtn  mkt_index  save_path_factor  cycle  start  end  quantile  statuslimit  For a certain quantile  choose_quantile)  return weight  dataframe) getQuantileWeight self  choose_quantile  Return weight  dataframe) getWeight self  Type Long Short stocks  Hedge the future.   quantile return cal_net_value self  Type cal_coverage self cal_turnover self run self Type,57
quantitative_trading_model,Project_Group8 file,"NUS, FE5225, quantitative trading, sentiment factor, backtest",Wang Yuzhou,quantitative_trading_model,4,"os pandas Directory to import stock return  market return & trade status. Directory to import factor. Directory to output the result.  MAKE SURE the number here is right and we can find the right file.  Please delete result folders with the same name. del factor_csv  factors_exist Define the direction of factors. ##############################################################  Parameters of backtest.  Time horizon  can be vacation  progrmme will extract the maximum subset. Adjustment Cycle  trading frequency)  delay for holidays. DON'T change this parameter. up down limit: 1 is close at up limit price  -1 is close at down limit price 0 for normal close price and nan means not in the market.  updownlimit = pd.read_csv os.path.join path  ""UpDownLimitStatus.csv"")  index_col = 0  parse_dates = True).loc[start:end][::cycle] trade status: 0 means suspension 1 means on transaction  nan means not in the market. Stock chosen pool  can't be in suspension. status =  updownlimit == 0) &  TradeStatus == 1) Choose long short:""LS"" or future hedge:""L""   ""ALL"" for L & LS.",160
quantitative_trading_model,Project_Group8 file,"NUS, FE5225, quantitative trading, sentiment factor, backtest",Wang Yuzhou,quantitative_trading_model,4,"config helper backtestlite os singlefactor factor save_path_factor factor_path_all Type direction fwdrtn mkt_index cycle  start  end  quantile win year  statuslimit  Run the backtest  Drawing  save path for each factor  factor direction  default is Descending  Directory to import stock return  market return & trade status.  Directory to import factor.  Directory to output the result.  MAKE SURE the number here is right and we can find the right file.  factors = list set factors) - set ['.DS_S'  ''  '.DS_Store']) - set factors_exist))  # Please delete result folders with the same name.  del factor_csv  factors_exist  Define the direction of factors. ##############################################################  Parameters of backtest.  Time horizon  can be vacation  progrmme will extract the maximum subset.  Adjustment Cycle  trading frequency)  delay for holidays. DON'T change this parameter.  up down limit: 1 is close at up limit price  -1 is close at down limit price 0 for normal close price and nan means not in the market.  updownlimit = pd.read_csv os.path.join path  ""UpDownLimitStatus.csv"")  index_col = 0  parse_dates = True).loc[start:end][::cycle]  trade status: 0 means suspension 1 means on transaction  nan means not in the market.  Stock chosen pool  can't be in suspension.  status =  updownlimit == 0) &  TradeStatus == 1)  Choose long short:""LS"" or future hedge:""L""   ""ALL"" for L & LS.  save path for each factor  factor direction  default is Descending",215
quantitative_trading_model,Project_Group8 file,"NUS, FE5225, quantitative trading, sentiment factor, backtest",Wang Yuzhou,quantitative_trading_model,4,scipy stats scipy.stats norm numpy os matplotlib math copy pandas datetime __init__ self  factor  direction  factor_data  fwrtn  date  mkt_index  Portfolio_return  coverage  Turnover  save_path_factor Type drawDistrubution self drawCoverage self  win win = 48  of stock') drawTurnover self  win ##win = 48 drawQuantileRtn self drawQuantileRtn Calculates and saves the annualized volatility of the portfolio. Calculates the annualized return of the portfolio  and saves it to the field. data.index = pd.to_datetime self.Portfolio_return.index  format = '%Y%m%d') drawWealthCurve self win year  date1 = pd.to_datetime self.date format = '%Y%m%d')         print  lsdata)         print  date1) IR  MEAN  STDY drawLogWealthCurve self IR  MEAN  STDY drawmaxDrawdown self IR  MEAN  STDY calAnualReturn self  labels  win Calculates the annualized return of the portfolio  and saves it to the field. print rtn_all[-1]  plt.xticks range len labels)) labels rotation=90) calAnualVol self labels win Calculates and saves the annualized volatility of the portfolio. Calculates the annualized return of the portfolio  and saves it to the field.  plt.xticks range len labels)) labels rotation=90) drawSampleRtn self  win  year Calculates and saves the annualized volatility of the portfolio. serial correlation calSerial self  win year ##  print  serial) calIR self mkt_index win Calculates the annualized return of the portfolio  and saves it to the field. calSortinoRatio self mkt_index  win calSpearman self win year         date1 = pd.to_datetime self.date format = '%Y%m%d') len date1)): print  stocks) print  cor) plt.text 3  8 'a'  bbox={'avg':avg_IC 'std':std_IC  'np.min':np.min_IC 'avg/std':res})     drawICDecay self drawICDist self Spearman_cor main self win year labels,235
CascadeLDA,Multi-label Topic Modelling With Prior Knowledge using LDA-based models,"Bayesian, Gibbs sampling, Latent Dirichlet Allocation, machine learning, natural language processing, topic modelling",Ken Schröder,LDA_thesis,5,"gensim numpy scipy.stats truncnorm scipy scipy partition_label lab  d phi x vect_multinom prob_matrix get_stirling_numbers n load_corpus filename  d=3 csv  Increase max line length for csv.reader: get_zbar self get_ph self set_label self  label term_to_id self  term sample_z self  opt=1  Identify the labelset of document doc:  Select relevant data subsets in outer loop  Calculate the implicit update of a's mean.  Find and deduct the word-topic assignment:  Calculate probability of first part of Eq.  1)  Calculate probability of second part of Eq.  1)  Combine two parts and draw new word-topic assignment z_{d n}  Add back z_new to all relevant containers: sample_eta self sample_a self sample_beta self sample_m self run_training self  it=25  thinning=5  opt=1 ""  i) z_for_newdoc self  newdoc run_test self  newdoc  it=250  s=25  Find and deduct the word-topic assignment:  Calculate probability of first part of Eq.  1) display_topics self  n=10 label_predictions self  probs run_tests self  newdocs  it=250  s=25 split_data f=""thesis_data.csv""  d=3 train_it traindata  it=150  s=25  opt=1 test_it model  testdata  it=500  s=25",158
CascadeLDA,Multi-label Topic Modelling With Prior Knowledge using LDA-based models,"Bayesian, Gibbs sampling, Latent Dirichlet Allocation, machine learning, natural language processing, topic modelling",Ken Schröder,LDA_thesis,5,"gensim gensim.corpora dictionary numpy re load_corpus filename  d=3 csv  Increase max line length for csv.reader:  lab = [lab] partition_label lab  d __init__ self  docs  labs  labelset  dicti  alpha=0.001  beta=0.001 set_label self  label term_to_id self  term sub_corpus self  parent  Only keep the target labels  remove all other labels: they will be  gathered as the 'generic' topic get_sub_ph self  subdocs  sublabs  sublabset  it=150  thinning=12 go_down_tree self  it  s  Starting at 'root' as parent node:  Only for this root-level we retain the topic-word distr ph for 'root'  Take subset of the entire corpus. With label ""l*""  Run local LDA on subset - get those label-word distr.  This function also adds 'root' to sublabset  Get the local label ids and insert into global label-word:  Disregard ""root"" of every local label-word distr.  Take subset of the entire corpus. With label ""l*""  Run local LDA on subset - get those label-word distr.  This function also adds 'root' to sublabset  Get the local label ids and insert into global label-word:  Disregard ""root"" of every local label-word distr. prep4test self  doc  ph  Initiate with the 'garbage'/'root' label uniformly: cascade_test self  doc  it  thinning  labels  In CascadeLDA it can occur that prob.sum ) = 0. This  is forced to throw an error  else would have been warning: test_down_tree self  doc  it  thinning  threshold run_test self  docs  it  thinning  depth=""all""  v = int v)  Save the current state in MC chain and calc. average state: ""  i+1) __init__ self  docs  labs  labelset  dicti  alpha=0.001  beta=0.001 set_label self  label get_ph self training_iteration self run_training self  it=120  thinning=15 ""  i+1) split_data f=""thesis_data.csv""  d=3 prune_dict docs  lower=0.1  upper=0.9 train_it train_data  it=150  s=12  l=0.02  u=0.98  al=0.001  be=0.001",273
CascadeLDA,Multi-label Topic Modelling With Prior Knowledge using LDA-based models,"Bayesian, Gibbs sampling, Latent Dirichlet Allocation, machine learning, natural language processing, topic modelling",Ken Schröder,LDA_thesis,5,"LabeledLDA sklearn.metrics auc optparse OptionParser pickle numpy one_roc prob  real_binary fpr_tpr tp  fp  tn  fn precision_recall tp  fp  tn  fn rates y_prob  y_real_binary macro_auc_roc fprs  tprs n_error th_hat  y_real_binary  n get_f1 tps  fps  tns  fns binary_yreal label_strings  label_dict main   of iterations"")  of Gibbs samples: ""  int opt.it))  Remove root label from predictions  also not included in label sets)  Remove docs that were assigned to 'root' completely:",66
CascadeLDA,Multi-label Topic Modelling With Prior Knowledge using LDA-based models,"Bayesian, Gibbs sampling, Latent Dirichlet Allocation, machine learning, natural language processing, topic modelling",Ken Schröder,LDA_thesis,5,gensim gensim.corpora dictionary numpy numpy.random multinomial load_corpus filename  d csv  Increase max line length for csv.reader: __init__ self  docs  labs  labelset  dicti  alpha  beta set_label self  label training_iteration self run_training self  iters  thinning  %d ' %  n+1)) prep4test self  doc run_test self  newdocs  it  thinning  Save the current state in MC chain and calc. average state:  Only the document-topic distribution estimate theta is saved get_pred self  single_th  n=5 get_preds self  all_th  n=5 get_phi self get_theta self topwords_per_topic self  topwords=10 perplexity self split_data f  d=2 prune_dict docs  lower=0.1  upper=0.9 train_it traindata  it=30  s=3  al=0.001  be=0.001  l=0.05  u=0.95 test_it model  testdata  it=500  thinning=25  n=5,102
CascadeLDA,Multi-label Topic Modelling With Prior Knowledge using LDA-based models,"Bayesian, Gibbs sampling, Latent Dirichlet Allocation, machine learning, natural language processing, topic modelling",Ken Schröder,LDA_thesis,5,"CascadeLDA sklearn.metrics auc optparse OptionParser pickle one_roc prob  real_binary fpr_tpr tp  fp  tn  fn precision_recall tp  fp  tn  fn rates y_prob  y_real_binary macro_auc_roc fprs  tprs n_error th_hat  y_real_binary  n get_f1 tps  fps  tns  fns setup_theta l1p  l2p  l3p  model  Start adding the lowest labs and just add the 'rest'  too. It will be  overwritten later on with the correct value from the upper level  Multiple probs of local scope with the prob of upper level: binary_yreal label_strings  label_dict main   of iterations - train and test"")  Evaluate quality for all label depths:  of Gibbs samples: ""  int opt.it))  Selecting the relevant labels  Remove no-prediction and no-label documents",106
DEDA_Class_SS2018_IRTG_Guest_Matching,Analyzing textual of IRTG researchers then matching IRTG Researchers based on their interests,"matching, similarity, clustering",Natalie Habib,TextAnalytics,1,!/usr/bin/env python3  -*- coding: utf-8 -*- pandas  standard library for machine learning numpy  standard library for math  libraries for NLP nltk nltk.corpus stopwords nltk.tokenize word_tokenize nltk.stem WordNetLemmatizer  libraries for SVD  LSA and Clustering sklearn.decomposition TruncatedSVD sklearn.feature_extraction.text CountVectorizer sklearn.preprocessing Normalizer sklearn.cluster KMeans  library for requesting a location and visualizing the response on a world map  geopy.geocoders Nominatim  further libraries import re   # using regular expressions os  accessing environment of the operating system os.path expanduser  retrieving the home path  datetime date  requesting date _datetime timedelta  calculating time differences   data elements of the class AnalyseText __init__ self  name  header of the dataframe researchers  id period1 period2 position origin interests isActive  read a CSV-file with the function getFilePath the file path of the scraped content is requested  readCSV self  filepath  request for the filepath where the csv-files from the web scraping are stored   note: the csv-files are stored under HOME/IRTG/data    getFilePath self  name home = home.replace '\\' '/') getCleanedInterests self __str__ self ############################################################################################  Clean Data  ############################################################################################           splitName self  add new columns surname  forename  startTime  endTime  fill the new columns  the surename is in the id column from the beginning of the string until the ' '  the forename is in the id column after the ' ' until the ' '  the beginning of the time period is in the id column starting with a ' ' and ending with a '-'  the end of the time period is in the id column starting with a '-' and ending with a ')' self.researchers.loc[researcher_index  'forename'] =  researcher.split ' ' 1)[1]).split ' ' 1)[0] self.researchers.loc[researcher_index  'startTime'] =  researcher.split ' ')[1]).split '-')[0] self.researchers.loc[researcher_index  'endTime'] =  researcher.split '-')[1]).split ')')[0] ############################################################################################  Natural Language Processing  ############################################################################################    pre_process self text_clean = self.filtering_stop_words text_stop)  Filter stop words  filtering_stop_words self  one_line filtered_line = [w for w in word_tokens if not w in stop_words]  Filter punctuation  filtering_punctuation self  one_line  Transfrom words to lower case  transform_to_lower self  one_line  Lemmatize words assuming each word is a nound  pos = 'n') lemmatize_words self  one_line ############################################################################################  Extract Features - Latent Semantic Analysis using Singular Value Decomposition  ############################################################################################     computeDTM self  Compute document-term matrix          each row represents a document  each column represents a word -> each document is a n-dim vector computeLSA self  dtm  Compute SVD and LSA printLSAComponent self  Each LSA component is a linear combination of words  printLSADocuments self  dtm_lsa  Each document is a linear combination of the LSA components  printDocumentSimilarity self  dtm_lsa  Document similarity using LSA components clusterComponents self  dtm_lsa  Use LSA components as features in clustering ############################################################################################  ManipulateGeo ############################################################################################            newResearcherTable self 'address'  'longtitude'  'latitude'  request geo data from OSM by location  column 'origin' with the name of the university) requestGeoData self         for researcher in self.researchers.loc[: 'id']:  calculate the distance between two researchers calculateDistance self  researcher_A  researcher_B  present the distribution of the researchers on a world map  input df with geo data  output html     visualizeGeoDistribution self  AnalyseText.py    ,470
abstract_LDA_K_topics,Summarize K topics from 130 collected abstracts with LDA method,"LDA, topic modelling, abstract analysis, heatmap, web crawling","FB, FS, WKH, Muhaimin 20201101",abstract_LDA_K_topics,2,"!/usr/bin/env python  coding: utf-8  In[1]: load library for crawling requests  take the website source code back to you urllib  some useful functions to deal with website URLs bs4 BeautifulSoup  a package to parse website source code numpy  all the numerical calculation related methods re  regular expression package itertools  a package to do iteration works pickle  a package to save your file temporarily pandas  process structured data os  In[2]: define the link for crawling  This link can represent the domain of a series of websites crawling the abstract by link above.  get source code  parse source code  if paper[3][-3:] == 'txt':  if paper[3][-3:] == 'pdf':      abstract_parsed = soup paper_abstract_page.content)      main_part = abstract_parsed.find_all 'body')[0].text.strip )  with open abs_folder + f""{re.sub '[^a-zA-Z0-9 ]'  ''  paper[0])}.txt""  'w'  encoding='utf-8') as abs_f:      abs_f.write main_part)  In[3]: load library for LDA random os re nltk os path nltk.stem WordNetLemmatizer nltk.stem.porter PorterStemmer nltk.corpus stopwords nltk.corpus stopwords nltk.stem.wordnet WordNetLemmatizer string gensim gensim corpora matplotlib pandas numpy  In[5]: declare the number of topics want to be generate and notated by K number of topics  Regex cleaning  keep only letters  numbers and whitespace  apply regex  apply regex  lower case  nltk clean doc  Importing Gensim  Creating the term dictionary of our courpus  where every unique term is assigned an index.  Converting list of documents  corpus) into Document Term Matrix using dictionary prepared above.  Creating the object for LDA model using gensim library  Running and Trainign LDA model on the document term matrix. print ldamodel.print_topics num_topics=3  num_words=5))  20 need to modify to match the length of vocabulary  plt.imshow zz  cmap='hot'  interpolation='nearest') plt.show )  plt.title ""heatmap xmas song"")  In[ ]:",265
DEDA_Class_SS2018_Ticker_CRIX,Python Based Raspberry Pi Zero W OLED Setup live BTC ticker,"hash, algorithm, crix, cryptocurrency, ticker, bitcoin, scam, shitcoin",Raphael Reule,DEDA_Class_SS2018_CCT,5," Copyright  c) 2014 Adafruit Industries  Author: Tony DiCola  Permission is hereby granted  free of charge  to any person obtaining a copy  of this software and associated documentation files  the ""Software"")  to deal  in the Software without restriction  including without limitation the rights  to use  copy  modify  merge  publish  distribute  sublicense  and/or sell  copies of the Software  and to permit persons to whom the Software is  furnished to do so  subject to the following conditions:  The above copyright notice and this permission notice shall be included in  all copies or substantial portions of the Software.  THE SOFTWARE IS PROVIDED ""AS IS""  WITHOUT WARRANTY OF ANY KIND  EXPRESS OR  IMPLIED  INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM  DAMAGES OR OTHER  LIABILITY  WHETHER IN AN ACTION OF CONTRACT  TORT OR OTHERWISE  ARISING FROM   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN  THE SOFTWARE. time Adafruit_GPIO Adafruit_SSD1306 PIL Image PIL ImageDraw PIL ImageFont  Raspberry Pi pin configuration:  Note the following are only used with SPI:  Beaglebone Black pin configuration:  RST = 'P9_12'  Note the following are only used with SPI:  DC = 'P9_15'  SPI_PORT = 1  SPI_DEVICE = 0  128x32 display with hardware I2C:  128x64 display with hardware I2C:  disp = Adafruit_SSD1306.SSD1306_128_64 rst=RST)  Note you can change the I2C address by passing an i2c_address parameter like:  disp = Adafruit_SSD1306.SSD1306_128_64 rst=RST  i2c_address=0x3C)  Alternatively you can specify an explicit I2C bus number  for example  with the 128x32 display you would use:  disp = Adafruit_SSD1306.SSD1306_128_32 rst=RST  i2c_bus=2)  128x32 display with hardware SPI:  disp = Adafruit_SSD1306.SSD1306_128_32 rst=RST  dc=DC  spi=SPI.SpiDev SPI_PORT  SPI_DEVICE  max_speed_hz=8000000))  128x64 display with hardware SPI:  disp = Adafruit_SSD1306.SSD1306_128_64 rst=RST  dc=DC  spi=SPI.SpiDev SPI_PORT  SPI_DEVICE  max_speed_hz=8000000))  Alternatively you can specify a software SPI implementation by providing  digital GPIO pin numbers for all the required display pins.  For example  on a Raspberry Pi with the 128x32 display you might use:  disp = Adafruit_SSD1306.SSD1306_128_32 rst=RST  dc=DC  sclk=18  din=25  cs=22)  Initialize library.  Clear display.  Create blank image for drawing.  Make sure to create image with mode '1' for 1-bit color.  Get drawing object to draw on image.  Draw a black filled box to clear the image.  Draw some shapes.  First define some constants to allow easy resizing of shapes.  Move left to right keeping track of the current x position for drawing shapes.  Draw an ellipse.  Draw a rectangle.  Draw a triangle.  Draw an X.  Load default font.  Alternatively load a TTF font.  Make sure the .ttf font file is in the same directory as the python script!  Some other nice fonts to try: http://www.dafont.com/bitmap.php font = ImageFont.truetype 'Minecraftia.ttf'  8)  Write two lines of text.  Display image.",456
DEDA_Class_SS2018_Ticker_CRIX,Python Based Raspberry Pi Zero W OLED Setup live BTC ticker,"hash, algorithm, crix, cryptocurrency, ticker, bitcoin, scam, shitcoin",Raphael Reule,DEDA_Class_SS2018_CCT,5,"subprocess time Adafruit_GPIO Adafruit_SSD1306 Image ImageDraw ImageFont json urllib2  Raspberry Pi pin configuration:  128x64 display with hardware I2C:  Initialize library.  Clear display.  Create blank image for drawing.  Make sure to create image with mode '1' for 1-bit color.  Get drawing object to draw on image.  Load default font.  font = ImageFont.load_default )  font = ImageFont.truetype 'Ubuntu-M.ttf'  14)  Display Image  definitions time sleep  import requests  json  def getBitcoinPrice ): 	URL = 'https://www.bitstamp.net/api/ticker/' 	try: 		r = requests.get URL) 		priceFloat = float json.loads r.text)['last']) 		return priceFloat 	except requests.ConnectionError: 		print ""Error querying Bitstamp API""  now = time.time )  print getBitcoinPrice ) / p * q  sleep 1)  print 'USD Bitcoin Price'  print bbl  print 'USD Price Calculated'  print bbc  print 'GBP Bitcoin Price'  print pbl  print 'GBP Bitcoin Calculated'  print pbc",127
DEDA_Class_SS2018_Ticker_CRIX,Python Based Raspberry Pi Zero W OLED Setup live BTC ticker,"hash, algorithm, crix, cryptocurrency, ticker, bitcoin, scam, shitcoin",Raphael Reule,DEDA_Class_SS2018_CCT,5," Copyright  c) 2017 Adafruit Industries  Author: Tony DiCola & James DeVito  Permission is hereby granted  free of charge  to any person obtaining a copy  of this software and associated documentation files  the ""Software"")  to deal  in the Software without restriction  including without limitation the rights  to use  copy  modify  merge  publish  distribute  sublicense  and/or sell  copies of the Software  and to permit persons to whom the Software is  furnished to do so  subject to the following conditions:  The above copyright notice and this permission notice shall be included in  all copies or substantial portions of the Software.  THE SOFTWARE IS PROVIDED ""AS IS""  WITHOUT WARRANTY OF ANY KIND  EXPRESS OR  IMPLIED  INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM  DAMAGES OR OTHER  LIABILITY  WHETHER IN AN ACTION OF CONTRACT  TORT OR OTHERWISE  ARISING FROM   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN  THE SOFTWARE. time Adafruit_GPIO Adafruit_SSD1306 PIL Image PIL ImageDraw PIL ImageFont subprocess  Raspberry Pi pin configuration:  on the PiOLED this pin isnt used  Note the following are only used with SPI:  Beaglebone Black pin configuration:  RST = 'P9_12'  Note the following are only used with SPI:  DC = 'P9_15'  SPI_PORT = 1  SPI_DEVICE = 0  128x32 display with hardware I2C:  128x64 display with hardware I2C:  disp = Adafruit_SSD1306.SSD1306_128_64 rst=RST)  Note you can change the I2C address by passing an i2c_address parameter like:  disp = Adafruit_SSD1306.SSD1306_128_64 rst=RST  i2c_address=0x3C)  Alternatively you can specify an explicit I2C bus number  for example  with the 128x32 display you would use:  disp = Adafruit_SSD1306.SSD1306_128_32 rst=RST  i2c_bus=2)  128x32 display with hardware SPI:  disp = Adafruit_SSD1306.SSD1306_128_32 rst=RST  dc=DC  spi=SPI.SpiDev SPI_PORT  SPI_DEVICE  max_speed_hz=8000000))  128x64 display with hardware SPI:  disp = Adafruit_SSD1306.SSD1306_128_64 rst=RST  dc=DC  spi=SPI.SpiDev SPI_PORT  SPI_DEVICE  max_speed_hz=8000000))  Alternatively you can specify a software SPI implementation by providing  digital GPIO pin numbers for all the required display pins.  For example  on a Raspberry Pi with the 128x32 display you might use:  disp = Adafruit_SSD1306.SSD1306_128_32 rst=RST  dc=DC  sclk=18  din=25  cs=22)  Initialize library.  Clear display.  Create blank image for drawing.  Make sure to create image with mode '1' for 1-bit color.  Get drawing object to draw on image.  Draw a black filled box to clear the image.  Draw some shapes.  First define some constants to allow easy resizing of shapes.  Move left to right keeping track of the current x position for drawing shapes.  Load default font.  Alternatively load a TTF font.  Make sure the .ttf font file is in the same directory as the python script!  Some other nice fonts to try: http://www.dafont.com/bitmap.php  font = ImageFont.truetype 'Minecraftia.ttf'  8)  Draw a black filled box to clear the image.  Shell scripts for system monitoring from here : https://unix.stackexchange.com/questions/119126/command-to-display-memory-usage-disk-usage-and-cpu-load  Write two lines of text.  Display image.",473
DEDA_Class_SS2018_Ticker_CRIX,Python Based Raspberry Pi Zero W OLED Setup live BTC ticker,"hash, algorithm, crix, cryptocurrency, ticker, bitcoin, scam, shitcoin",Raphael Reule,DEDA_Class_SS2018_CCT,5,"subprocess time Adafruit_GPIO Adafruit_SSD1306 Image ImageDraw ImageFont json urllib2  Raspberry Pi pin configuration:  128x64 display with hardware I2C:  Initialize library.  Clear display.  Create blank image for drawing.  Make sure to create image with mode '1' for 1-bit color.  Get drawing object to draw on image.  Load default font.  font = ImageFont.load_default )  font = ImageFont.truetype 'Ubuntu-M.ttf'  14)  Display Image  definitions time sleep  import requests  json  def getBitcoinPrice ): 	URL = 'https://www.bitstamp.net/api/ticker/' 	try: 		r = requests.get URL) 		priceFloat = float json.loads r.text)['last']) 		return priceFloat 	except requests.ConnectionError: 		print ""Error querying Bitstamp API""  now = time.time ) Load requests via pip! $ sudo pip install requests import urllib.request  json  with urllib.request.urlopen ""http://thecrix.de/data/crix_hf.json"") as url:     data = json.loads url.read ).decode ))[-10:] def last_n_crix_values n):  with urllib.request.urlopen ""http://thecrix.de/data/crix_hf.json"") as url:      data = json.loads url.read ).decode ))[-n:] return data import urllib.request  json import pandas as pd def last_n_crix_values n):     with urllib.request.urlopen ""http://thecrix.de/data/crix_hf.json"") as url:         data = json.loads url.read ).decode ))[-n:]     return data pd.DataFrame last_n_crix_values 2)).values import urllib.request  json import pandas as pd def last_n_crix_values n):     with urllib.request.urlopen ""http://thecrix.de/data/crix_hf.json"") as url:         data = json.loads url.read ).decode ))[-n:]     return data d = last_n_crix_values 2) print pd.DataFrame d).values) print [i['date'] for i in d]) print [i['price'] for i in d]) print [[i['date'] i['price']] for i in d]) urllib pandas last_n_crix_values n [[i['date']  i['price']] for i in d]",218
DEDA_Class_SS2018_Ticker_CRIX,Python Based Raspberry Pi Zero W OLED Setup live BTC ticker,"hash, algorithm, crix, cryptocurrency, ticker, bitcoin, scam, shitcoin",Raphael Reule,DEDA_Class_SS2018_CCT,5, blink.py RPi time os PIL ImageFont PIL Image PIL ImageDraw urllib urllib,12
SL_2020_COVID19_in_Austria,Plots of COVID19 data in Austria in the time interval 27.02. - 02.04. in R. Additionaly a wordcloud - code from Python. Corona.txt is needed to create the wordcloud.,"corona, COVID-19, wordcloud, R-plots, ATX, Oil price",Chiara Wang and Julia Guggenberger,SL_2020_COVID19_in_Austria,4,!/usr/bin/env python  coding: utf-8  In[7]:  In[8]:  In[9]:  In[10]:  In[ ]:,10
SL_2020_COVID19_in_Austria,Plots of COVID19 data in Austria in the time interval 27.02. - 02.04. in R. Additionaly a wordcloud - code from Python. Corona.txt is needed to create the wordcloud.,"corona, COVID-19, wordcloud, R-plots, ATX, Oil price",Chiara Wang and Julia Guggenberger,SL_2020_COVID19_in_Austria,4,!/usr/bin/env python  coding: utf-8  In[ ]: wordcloud WordCloud  In[2]: matplotlib  In[4]: os  In[102]:  In[104]:  In[105]:  In[106]:  In[107]:  In[116]:  In[117]:  In[111]:  In[39]:  In[ ]:,23
SoNIC_simulation_study,"simulation study for SoNIC model with parameters N=T=100, s=1, K=2..30",ERROR,Yegor Klochkov,SoNIC_simulation_study,6,numpy scipy.stats norm get_missing_probabilities mat  conf=0.95  tol=0.1 missing_var X  delta  make_positive=True missing_covar X  Y  delta_x  delta_y,16
SoNIC_simulation_study,"simulation study for SoNIC model with parameters N=T=100, s=1, K=2..30",ERROR,Yegor Klochkov,SoNIC_simulation_study,6,"numpy matplotlib cvxopt matrix cvxopt.solvers qp seaborn scipy.linalg sqrtm math sklearn linear_model import read_data common common get_index_matrix cluster_num  ind  normalize=True  print ""WARNING!!!"") index_dist cluster_num  int1  int2 gauss_var1_process theta  sigma  T  variance of innovations is sigma * I lasso_from_covariance D  c  alpha  minimizes 1/2 x^{T}Dx - c^{T} x + \alpha \| v \|_1 random_basis n  k  orthogonal normalization of columns u_random n  k  cluster_num  index=None v_step D0  D1  alpha_v  z  make sure u^{\T}u = I! z_step cluster_num  D1  v  ind_old=None check this stupid function!!! <- apparently works... __init__ self  theta  u  v  index  loss  to implement: index choice; initial index + initial u  matrix competition  index competition?) alternating cluster_num  D0  D1  alpha_v  epochs=10  index_init=None print ""loss : {}"".format loss)) v_step_from_data x_train  y_train  alpha_v  z  make sure u^{\T}u = I! direct cluster_num  D0  D1  alpha_v  index_init=None loss print ""loss : {}"".format loss)) direct_from_data cluster_num  x_train  y_train  alpha_v  index_init=None  loss  print ""loss : {}"".format loss)) alternating_from_data cluster_num  x_train  y_train  alpha_v  epochs=100  index_init=None _func1 v  ind _func0 ind print ""K={}: epochs {}/{}"".format cluster_num  e  epochs)) matrix_competition type  repeat_num   *args  **kwargs simu n  c_num  s  T  pmin=1.0  define true index define true theta generate the time series include missing observations ax.set_xticklabels ax.get_xticklabels )  rotation=-90  fontsize=8) ax.set_yticklabels ax.get_yticklabels )  rotation=0  fontsize=8)  ax.set_xticklabels ax.get_xticklabels )  rotation=-90  fontsize=8)  ax.set_yticklabels ax.get_yticklabels )  rotation=0  fontsize=8) simu )",217
SoNIC_simulation_study,"simulation study for SoNIC model with parameters N=T=100, s=1, K=2..30",ERROR,Yegor Klochkov,SoNIC_simulation_study,6,common numpy sys pandas DataFrame,5
SoNIC_simulation_study,"simulation study for SoNIC model with parameters N=T=100, s=1, K=2..30",ERROR,Yegor Klochkov,SoNIC_simulation_study,6,numpy  func takes ind! find_new_home func  k  ind  i greedy_update func  k  ind __init__ self  index  value  message  nochange kmeans_greedy func  k  n  iter_limit=10000  init_index=None,25
SoNIC_simulation_study,"simulation study for SoNIC model with parameters N=T=100, s=1, K=2..30",ERROR,Yegor Klochkov,SoNIC_simulation_study,6,numpy pandas read_csv csv math sqrt find_homo_Be x  scale=4  res=20 check_interval l  r read_SIFI name_to_sifi read_stock_twits_user_sentiment name_to_twits  min_delta=0.1  min_days=200 avoid_empty = lambda x  err_t: x[0] if np.size x) > 0 else err_t read_crix_returns name_to_crix_price read_crypto_sentiment name_to_crypto_sentiment  top_num=50,37
SoNIC_simulation_study,"simulation study for SoNIC model with parameters N=T=100, s=1, K=2..30",ERROR,Yegor Klochkov,SoNIC_simulation_study,6,subprocess numpy pandas matplotlib math sqrt cl_nums = [2  4  6  8  10  12  14  16  18  20  22  24  26  28  30] ###########################  lambda graphs ###########################  loss with best lambda plt.show ) ###########################  cluster distances plt.show ),38
CMBbubblesbuilder,Generates JSON files from a MySQL table to be used by CMBbubbles for visualization using D3.js and CoffeeScript.,"data, grouped, JSON, database, transformation, text, tree, table","Torsten van den Berg, Sophie Burgard",CMBbubblesbuilder,1, -*- coding: utf-8 -*- os json pymysql  Set working directory  MySQL config and credentials  Database columns used to cluster  This construction has to be rewritten to a somewhat elegant solution  Format list according to expected JSON  Generate actual JSON  Write JSON to file,43
pyTSA_GARCH,"This Quantlet simulates ARMA(2,2) - autoregressive moving average process and draws the true ACF and PACF","time series,  stationarity, autocorrelation, PACF, ACF, simulation, stochastic process, ARMA, moving average, autoregression",ERROR,pyTSA_GARCH,1,numpy statsmodels matplotlib arch.univariate arch_model PythonTsa.plot_acf_pacf acf_pacf_fig PythonTsa.LjungBoxtest plot_LB_pvalue scipy.stats norm  in general  params = [r  omega  alpha  beta],19
Additive_Example,generate additive model and plot shapley values against both functions of the DGP.,"Shapley, additive model, SHAP, Interpretability, Explainable AI",Ratmir Miftachov,Additive_Example,2,!/usr/bin/env python  coding: utf-8  In[38]: numpy numpy matplotlib matplotlib import keras tensorflow tensorflow keras tensorflow.keras layers tensorflow.keras.models Sequential tensorflow.keras.layers Dense SHAP shap xgboost mpl_toolkits mplot3d pandas sklearn.datasets make_regression random  In[39]: Generate data from an additive model Step.1 Sinus and Cosinus on [0  2*pi] x_pandas = pd.DataFrame x) Sinus  f_1 x_1)  In[40]:  In[41]: Cosinus  f_2 x_2) y_cos_pandas = pd.DataFrame y_cos) -> for shap  In[42]:  DGP  + constant  In[43]: 3d plot  plotting  In[44]:  In[ ]: building the architecture of the NN  model expects 2 input variables and 12 layers in first hidden layer  second hidden layer has 8 nodes  second hidden layer has 8 nodes  output layer has 1 node  In[46]:  In[47]:  In[48]: shapley values for keras example  In[49]: works  In[50]: works  In[53]: This plot plt.legend )  In[54]: This plot plt.legend )  In[ ]:,132
SDA_20201126_hw08_Clustbank,Performs a PCA and a cluster analysis for 20 randomly chosen bank notes from the swiss bank notes dataset.,"cluster-analysis, plot, graphical representation, distance, euclidean, data visualization, dendrogram, principal-components, pca",Andreas Rony Wijaya,SDA_20201126_hw08_Clustbank,2,!/usr/bin/env python  coding: utf-8  In[6]: pandas numpy matplotlib matplotlib.patches Ellipse scipy cluster  In[7]: ## Load bank dataset  In[8]: ## PCA for 20 randomly chosen bank notes  In[9]: ## The dendrogram for the 20 bank notes after applying the Ward algorithm  In[10]: ## PCA with clusters,45
SFEcir,Displays the yield curve for given parameters under the model of Cox-Ingersoll-Ross (CIR).,"cir, graphical representation, interest-rate, plot, process, short-rate, simulation, time-series, yield, term structure","Joanna Tomanek, Christian M. Hafner",QID-1752-SFEcir,1,numpy pandas matplotlib  parameter settings  reversion rate  steady state of the short rate  volatility coefficient  time horizon in years  today's short rate  vector of maturities in years  the bond prices  the yields,32
CP2P_coinliqu,Illustration of Aave´s coin liquidation mechanism based on a theoretical CP2P contract for June 2021 and historical ETH prices. It is assumed that the fictional borrower does not take action to avoid liquidations.,"CP2P, coin liquidation, P2P Lending, Aave v1, collateralized loan",Ramona Merkl,CP2P_coinliqu,2,!/usr/bin/env python  coding: utf-8  In[1]: pandas numpy matplotlib pylab plot  In[2]:  In[3]:  In[4]:  calculate ETH prices in USDT  In[5]:  prices over one year  In[6]:  june  In[7]:  liquidation threshold LTV  contract interest payment  initial debt  initial number of coins  collateral  borrowing capacity  initial liquidation threshold in unit of price  initial LTV  In[8]: Ni N0  LTV  In[9]:  time horizon compared to one year  for annual interest)  time steps contract.head ) contract  number of liquidations  liquidation penalty  calculate LTV in every block  check if LTV is too high and calculate liquidation  if necessary  data before liquidation  update data  delete contract  if all coins gone  data after liquidation  In[10]:  In[11]:  In[12]:  translate into .tex table  In[13]:  In[14]:  Added block time  In[15]:  In[16]:  In[17]: plt.savefig 'liquidations_june.png'  transparent=True)  In[18]: plt.savefig 'coins_june.png'  transparent=True)  In[19]: ax.plot contract.Threshold  label='Price Threshold'  color='r'  linestyle='dashed') plt.savefig 'pricethreshold_june.png'  transparent=True)  In[20]: plt.savefig 'collateral_june.png'  transparent=True),140
BaumProjekt,"Applies random forests to classify real trees, based on Marius Sterlings DEDA_RandForest.","Random forest, feature importance, classification, decision tree comparision, trees, Linz",ERROR,Trees,1, -*- coding: utf-8 -*- Decision Trees for Real Trees pandas sklearn.model_selection train_test_split sklearn.ensemble RandomForestClassifier sklearn.tree export_graphviz sklearn.metrics confusion_matrix matplotlib matplotlib numpy pydot The file comes from https://www.data.gv.at/katalog/dataset/f660cf3f-afa9-4816-aafb-0098a36ca57d If downloaded anew  be careful with the seperators in the CSV.  Eine gattung auswählen  die anderen auskommentieren genus = baum[baum.Gattung == 'Chamaecyparis'] genus = baum[baum.Gattung == 'Fagus'] genus = baum[baum.Gattung == 'Gleditsia'] genus = baum[baum.Gattung == 'Larix'] genus = baum[baum.Gattung == 'Liquidambar'] genus = baum[baum.Gattung == 'Tilia'] Recoding species to numbers-   ratio of data that is used for testing  setting random seed   Keeps the ratio of the labels in train and test the same as in the initial data Hoehe Schirmdurchmesser Stammumfang  Number of Trees grown  Number of randomly picked features for split  Max Number of nested splits  Seed for random number generator Seeing how good the accuracy is on the test and training data sets. Extracting 10 trees from the random forest Print the first the first extracted tree and say how accurate it alone is in classifying the train and test data. Plots the importance of the features in the random forest.  Setting mesh grid for the map classification  Setting colors for the plots matplotlib  PiYG  Prediction probabilies for the different classes  ## Comparison  ### Accuracy vs. number of trees  Number of Trees grown  Number of randomly picked features for split   Max Number of nested splits  ### Accuracy vs. Number of Samples per leaf Careful! Some of the samples have some categories with very few items.  This means that the test or training dataset might have none of them   causing problems.,260
awc,Test Adaptive Weights Clustering on several data sets.,"cluster-analysis, kullback-leibler, nonparametric, data visualization, data mining","Kirill Efimov, Larisa Adamyan",q_awc,2,scipy matplotlib matplotlib numpy copy scipy pylab rcParams sklearn decomposition onclick event  X  weights  f  ax2  h_1  h_2 i = 234 ax2.axis 'equal') k_loc x k_stat x distance_matrix X get_neighbour_numbers h  dist_matrix  weights get_lambda_hash h  d  L_T print 'a'  np.max np.min  d / h / h_ratio * 10000 ).astype int)  10000-1)) get_lambda_table n  m  - 1. initialisation x  H x = x / H[0] a[i :] = np.minimum np.exp - x[i :]/max v[i]  h_0)-1.) / 0.002)  1.) a = a.astype bool) return np.ones  n n)) draw_step weights  X1  true_clusters  clustering  h_1  h_2  true_weights=None cluster_colors = ['r' 'lightgreen' 'b'  'hotpink'  'yellow'  'g'  'c'  'm'  'gold'  'firebrick'  'olive'  'springgreen'  'palevioletred'] plt.ylim [-0.3 3.3]) if np.size X1  1) > 2:     X = X1[:  [1  3]] el X = X1 X = X1 print 'h'  len true_clusters) print 'o' + cluster_colors[cluster_i] candidates = np.argsort X[points  0]) xrange len U)): print 'o' + cluster_colors[not_used_colors[true_color]] ax.scatter U[all_cliques[0]  0]  U[all_cliques[0]  1]  U[all_cliques[0]  2] zdir='z'  c=cluster_colors[not_used_colors[true_color]]) ax.scatter U[all_cliques[0]  2]  U[all_cliques[0]  0]  U[all_cliques[0]  1] zdir='z') print all_cliques[0] print points plt.show ) ball_volume r  dim correct_weights weights connect_alone weights find_unchanged_weights old_weights  weights print dif_weights print 'zeros'  zeros print np.sum unchanged == n) cluster_step X  l  weights  v  n  k  L_T  T  KL  dist_matrix  H  log_show  method  show_step  true_clusters  T_stat_show  propagation=False global old_weights if k != 1:     K = find_unchanged_weights old_weights  weights) old_weights = copy.deepcopy weights) max_dist = np.max dist_matrix  axis=1) print 't_1='  t_1[x  y] gg2 =  t_1.T == t_12x.T) *  t_12 < 0.5 * t_12x.T) E =  max_dist[i] < H[k-1]) *   max_dist[i+1:] < H[k-1]) q[E] = 1. / get_lambda_hash np.maximum max_dist[i]  max_dist[i+1:][E])  dist_matrix[i  i+1:][E]  L_T) print 't_12x='  t_12x[x  y]  't_21x='  t_12x[y  x]  't_1='  t_1[x  y] print 't_2='  t_1[y  x]  't_12='  t_12[x  y]  'e='  e[x  y]  'q='  q[x  y] T[gg2] = np.nan print 'T'  T[x  y]  T[y x] ##print 'T[' + str x) + ' ' + str y) + ']'  T[x  y] start_time = time.time ) weights = connect_alone weights) weights = correct_weights weights) print weights[276  :][I[276  :]] print 'T[203  302]'  T[203  302]  weights[203  302]#  np.isnan T[203  302]) weights[352  240] = 0 if k != 1:     weights[K  :][:  K] = old_weights[K  :][:  K] KL_init  print e[0  :] print q[0  :] KL =   e-q) * np.log  e / q)) +   q - e) ) * np.log    1 - e) /  1 - q)  )  ) KL =  -1) **  e > q) *  e * np.log e / q) +  1 - e) * np.log    1 - e) /  1 - q)  )  ) KL =  -1) **  e > q) *   -q) * np.log  e / q)) +   -1 + q) ) * np.log    1 - e) /  1 - q)  )  ) KL[np.isnan KL)] = 0  print KL[0 1] init X  n_neigh   weights_init n_0 = 12 weights = np.zeros  n  n)) weights_init = copy.deepcopy weights)   flag = np.zeros  n n)  dtype=np.int8) cluster X  l  true_clusters = []  true_weights=None   show_step = False  show_finish = False  T_stat_show = False  clustering = False  log_show = True  n_neigh = -1  method = 1  step=None ##KL = KL_init 1000) correct_weights weights) get_h_intervals dist_matrix  log_show=False print 'n_0='  n_0 plt.plot range len h_intervals))  h_intervals) plt.show ) return h_intervals  dist_matrix ## New idea   ## Another Idea print 'NNN'  neighbor_number_seq h_intervals = h_intervals[permutation_sort] return h_final  dist_matrix a.append a[-1]) a = a + a get_error weights  true_weights  separate_errors = False show_distances X A = np.zeros  n-1  1)) plt.show ),560
awc,Test Adaptive Weights Clustering on several data sets.,"cluster-analysis, kullback-leibler, nonparametric, data visualization, data mining","Kirill Efimov, Larisa Adamyan",q_awc,2,AWC_KL os load_dataset filename get_true_weights X  Y run_awc filename  lamda sys main argv,13
DEDA_RandForest,"Example code for Random Forest, extraction of single trees, feature importance, plotting classification maps and comparing random forest.","Random forest, feature importance, classification, decision tree, comparision",Marius Sterling,DEDA_RandForest,4,!/usr/bin/env python  coding: utf-8  # Random Forest  Loading packages  In[1]: pandas sklearn.model_selection train_test_split sklearn.ensemble RandomForestClassifier sklearn.tree export_graphviz sklearn.datasets load_breast_cancer sklearn.metrics confusion_matrix matplotlib matplotlib numpy pydot  Loading data and showing description  In[2]:  In[3]:  Extracting the feature matrix  X) and the target values  y) in a training and test dataset  In[4]:  ratio of data that is used for testing  setting random seed   Keeps the ratio of the labels in train and test the same as in the initial data  Declaring and fitting a Random Forest classifier on the feature matrix and target values  In[5]:  Number of Trees grown  Number of randomly picked features for split   Max Number of nested splits  Seed for random number generator oob_score=True  # Out of Bag accuracy score bootstrap=True #   Extracting the first 10 trees from the random forest.  In[6]:  ### Tree 1:  ![RF_one_tree_no_0] RF_one_tree_no_0.png)  In[7]:  ### Tree 2:  ![RF_one_tree_no_0] RF_one_tree_no_1.png)  In[8]:  ### Tree 3:  ![RF_one_tree_no_0] RF_one_tree_no_2.png)  In[9]:  ### Plotting feature importance  In[10]:  ### Fitting Random Forrest for 2 most important features and plotting decision  In[11]:  Setting mesh grid for the map classification  In[12]:  Setting colors for the plots  In[13]: matplotlib  PiYG  In[14]:  Prediction probabilies for the different classes  In[15]:  ## Comparison  ### Accuracy vs. number of trees  In[16]:  Number of Trees grown  Number of randomly picked features for split   Max Number of nested splits  ### Accuracy vs. Number of Samples per leaf  In[17]:  ### Accuracy vs. Maximum features  In[18]:  In[ ]:,234
DEDA_RandForest,"Example code for Random Forest, extraction of single trees, feature importance, plotting classification maps and comparing random forest.","Random forest, feature importance, classification, decision tree, comparision",Marius Sterling,DEDA_RandForest,4,!/usr/bin/env python  coding: utf-8  # Random Forest  Based on:  - https://www.datacamp.com/community/tutorials/random-forests-classifier-python  - https://towardsdatascience.com/random-forest-in-python-24d0893d51c0  - https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74  - https://mathematica.stackexchange.com/questions/98794/how-to-visualize-a-random-forest-classifier  In[6]: pandas sklearn.model_selection train_test_split sklearn.ensemble RandomForestClassifier sklearn.tree export_graphviz sklearn.datasets load_breast_cancer sklearn.metrics confusion_matrix matplotlib matplotlib numpy pydot  In[7]:  In[8]:  ratio of data that is used for testing  Keeps the ratio of the labels in train and test the same as in the initial data  ## Grid Search with CV  Defining paramter grid for GridSearchCV and setting up RandomForestClassifier.  In[11]: bootstrap=True oob_score=True)  RandomForestClassifier to be optimized  parameter grid  cross validation split  setting for parallization  -1: use all processors  see documentation  Refit estimator using best found parameters  Best parameter setting in the grid:  In[14]:  In[16]:,109
SFEBarrier_Pricing_Tree,"Computes Barrier option prices using a binomial tree for assets with/without continuous dividends. barrier option types: up-and-out, up-and-in, down-and-out, down-and-in","binomial, tree, asset, call, put, option, option-price, barrier-option, up-and-out, up-and-in, down-and-out, down-and-in",Franziska Wehrmann,SFEBarrier_Pricing_Tree,1,numpy matplotlib matplotlib.lines Line2D calc_parameters T  N  sigma  r  div  P up movement) knockout_01 assets  B  barrier_type calc_price S0  K  u  d  N  r  dt  q  option  barrier_type  calculate the values at maturity T Next we use the recursion formula for pricing in the CRR model: calc_price_european S0  K  u  d  N  r  dt  q  option  calculate the values at maturity T Using the recursion formula for pricing in the CRR model:   from  T-dt  T-2*dt  ....  dt  0) ###### MAIN ################  current stock price  strike price  time to maturity  volatility  interest rate  dividend  steps in tree  barrier  calculate all prices,100
GMMvb,Use Variational Bayes (VB) to approximate the density of Gaussian Mixture Models (GMM),"Variational Bayes, Gaussian Mixture Models, Approximation, Conditional density, Posterior",Rui Ren,GMMvb,1,numpy scipy matplotlib matplotlib matplotlib pandas seaborn import pymc3 as pm math scipy.stats multivariate_normal  hyperparameters  True parameter  Plot variables  Simulated data collections defaultdict  pick category  get point from distribution figure   alpha=0.2 colors transparency  Variational Inference - CAVI VI K  prior_std  n  data  Initialization  1000*3 matrix  Initiate CAVI iterations  mixture model parameter update step  categorical vector update step  compute ELBO or check for convergence of variational factors - here we do the latter  sort the values since the k mixture components dont maintain any ordering across multiple initializations  Run the CAVI algorithm,91
StockCorr_Clustering_Analysis,Uses hierarchical clustering method to analyse the correlation of stock returns along time,"stock, correlation, hierarchical clustering, temporal, sectors, industries","Li Yilin, Mei Yuxin, Sun Qingwei, Xie Chuda, Zhang Yingxin",StockCorr_Clustering_Analysis,2,!/usr/bin/env python  coding: utf-8  In[2]:  In[2]: os pandas matplotlib matplotlib seaborn scipy numpy logging imageio moviepy datetime datetime datetime timedelta scipy.cluster.hierarchy ClusterWarning warnings simplefilter warnings symbol_to_path symbol  download data from https://drive.google.com/file/d/1Uy0VmrkbKUAskGKAAQo45F8unrphAF14/view?usp=sharing  and save to downloaded_data/data/ folder get_symbols_from_file file_path  In[12]: get_data symbols  if first = True  construct df  df.set_index 'Date')  df.sort_index inplace=True)  In[3]: construct_index sym_file  index_file_name  use simple average of price here for index construction sector_index  industry_index   In[14]: correlDist corr getQuasiDiag link  In[15]: cluster_plot data  start  end  figname  cluster=True  In[8]: run sym_file  category  data.to_csv 'data_{}.csv'.format category))  In[5]: animate sym_file  category  start  end  interval=30  window=360  cluster=True  In[9]: main   run 'sp500_symbol.csv'  'SP500')  produce animations  In[ ]:,103
CMBhddscrap,"Extracts year of introduction, capacity and pricing data for hard drives from an online HTML table using basic pattern matching.","time-series, empirical, discrete, tree, distance, preprocessing, data adjustment, transformation, data, text, web scraping, web data, parser, pattern matching, regular expressions, selection, data set, conditional, frequencies, html, dom, xpath","Torsten van den Berg, Sophie Burgard",CMBhddscrap,1," -*- coding: utf-8 -*- os re csv requests  Set current path  with trailing slash)  USD / GB data set by Matthew Komorowski  Read HTML to string and drop all commata "" ""  Set column names  order is important  Regular expression:  - Match everything between the table data or header start and end tags    with optional attributes  - USD sign ""$"" is optional and not matched  Find matches using pattern  Length  number of columns) is used to  5  1375  Halt on length mismatch  [5  10  15  ...  number of matches]  0 is left out in order drop the original column headings  Group matches by every fifth item  Write to CSV",109
pyTSA_ MilkAv,This Quantlet builds time series of monthly recorded milk average per cow at a dairy cattle ranch in China from Jan.1962 to Dec.1975. It produces yearly and monthly box plots of this dataset,"box plot , time series, plot, seasonality","Huang Changquan, Alla Petukhina",pyTSA_MilkAv,1,numpy pandas matplotlib,3
CSC_Dapp_SC_scraping,"obtaining Dapp smart contracts using API, parsing source code",ERROR,"Elizaveta Zinovyeva, Raphael Constantin Georg Reule",CSC_Dapp_SC_scraping,4,"!/usr/bin/env python  coding: utf-8  In[38]: re pandas numpy pickle os tqdm tqdm  In[39]: https://stackoverflow.com/questions/2319019/using-regex-to-remove-comments-from-source-files https://github.com/HektorLin/HU-IRTG/blob/master/Partition%20Codes%20and%20Comments.ipynb remove_comments string  first group captures quoted strings  double or single)  second group captures comments  //single-line or /* multi-line */) _replacer match  if the 2nd group  capturing comments) is not None   it means we have captured a non-quoted  real) comment string.  so we will return empty to remove the comment  otherwise  we will return the 1st group  captured quoted-string following the example above  distinguish group 1): """" or'' and group 2 true comments): \*.*\ or // return when a true comment is located leave_only_comments string  In[40]:  In[41]:  In[42]:  In[43]:",103
CSC_Dapp_SC_scraping,"obtaining Dapp smart contracts using API, parsing source code",ERROR,"Elizaveta Zinovyeva, Raphael Constantin Georg Reule",CSC_Dapp_SC_scraping,4,"!/usr/bin/env python  coding: utf-8  In[1]:  Used Etherscan.io APIs""  In[6]: requests pandas numpy sys time os matplotlib  In[7]: ## Load List of Contracts  In[22]:  In[26]:  contants  In[28]:  ### Create Folders  In[35]:  In[36]:  In[37]:  In[42]: tqdm tqdm  In[48]: scrape_ether_contract_and_write address_array  API_Token  time.sleep 0.01) # we can do 5 GET/POST requests per sec  save solidity source code  In[49]:  In[ ]:",57
FRM_3_Get_InputData,"1) Retrieval of the hourly price data of cryptocurrencies via API call 2) Calculation of the hourly returns 3) Retrieval of the daily macro-economic variables via API call and transform it into an hourly data 4) For macro Index S&P 500 and VIX, NaN (value on weekends and holidays) were filled with the value of previous the non NaN value","cryptocurrency, macro-economic index, data, hourly price, hourly return, weekly transformation, retrieval, API call","Qi Wu, Seokhee Moon",FRM_3_Get_InputData,2,"!/usr/bin/env python  coding: utf-8  ## 1. Import packages  In[3]: requests pandas numpy datetime time  ## 2. Define function to get coin data from API  In[2]: Source: https://blog.cryptocompare.com/cryptocompare-api-quick-start-guide-f7abbd20d260 This function establishes a request to the online plattform BitFinex: get_data_spec coin  date  time_period This function collects the cryptocurrency data from for the specified time period:  get_df_spec time_period  coin  from_date  to_date  Now we use the new function to query specific coins compute_Returns data  ## 3. Get hourly data for 12 coins according to steady coin study  time period 17/18 six month  In[11]:  In[ ]:  for coins with no data avaliable  \n\nfor coin in coins:\n    try:\n        hourly_df[coin] = compute_Returns get_df_spec \'histohour\' coin   start_unix  end_unix))\n    except:\n        print coin + \': error\')\n        errorcoin.append coin)')  ### 3.1. New API for ZEC  In[5]: bitfinex Create api instance of the v2 API  In[6]:  In[7]:  Currency pair of interest\nbin_size = '1h' # This will return minute data\nlimit = 5000    # We want the maximum of 5000 data points \n# Define the start date\nt_start = datetime.datetime 2017  11  1  0  0)\nt_start = time.mktime t_start.timetuple )) * 1000\n# Define the end date\nt_stop    = datetime.datetime 2018  5  1  0  0)\nt_stop    = time.mktime t_stop.timetuple )) * 1000\n\npair_data = api_v2.candles symbol=pair  interval=bin_size   \n                           limit=limit  start=t_start  end=t_stop)\n\nimport pandas as pd\n\n# Create pandas data frame and clean/format data\nnames = ['time'  'open'  'close'  'high'  'low'  'volume']\ndf = pd.DataFrame pair_data  columns=names)\ndf.drop_duplicates inplace=True)\ndf['time'] = pd.to_datetime df['time']  unit='ms')\ndf.set_index 'time'  inplace=True)\ndf.sort_index inplace=True)"")  In[16]:  In[23]:  ## 4. Merge columns for different coins  In[28]: merge_dfs_on_column dataframes  labels  col  In[29]:  In[ ]:  ## 5. Macro Index     ### S&P 500    S&P 500 index get in https://www.investing.com  In[4]: datetime datetime  convert str to datetime  generate hourly time index   set time as index  reverse dataframe  now the date is ascending  In[5]:  because sp500 is daily data and only for work days  create new df with new hourly idx  fill nan with prescending value   In[6]:  In[7]:  calculate return  In[9]: matplotlib  ### VIX    http://www.cboe.com/products/vix-index-volatility/vix-options-and-futures/vix-index/vix-historical-data  In[10]:  In[11]:  fill nan with prescending value   In[12]:  In[13]:  In[14]:  In[15]:  In[21]:  In[22]:",327
CCIDcandles,"Builds 60 mins candles using on 5 minutes price data (Open, High, Low,Close price) of 11 cryptocurrencies and produces plots in pdf format","candles, high frequency data, cryptocurrency",Alla Petukhina,CCIDCandles,1,pandas  import pandas_datareader as datareader numpy matplotlib datetime datetime datetime from matplotlib.finance import candlestick2_ohlc mpl_finance candlestick2_ohlc matplotlib symbol = 'bch'  Converting date to pandas datetime format,26
Hsinchu_ubike_station_location,Plot the ubike station that located in hsinchu city area,ERROR,Muhaimin 0852627,Hsinchu_ubike_station_location,1, -*- coding: utf-8 -*- import the library folium folium.plugins MarkerCluster pandas requests json pprint pandas numpy matplotlib descartes googletrans Translator copy download the data ubike pprint.pprint json_tree) simple pre processing convert string to float convert string to float Define coordinates of where we want to center our map Create the map put the marker,54
TXTMcDlm,"Counts positive and negative words using the lexicon by Loughran and McDonald. Please install required Python packages before usage: os, io, collections, nltk.","text mining, data mining, counts, sentiment",Elisabeth Bommes,TXTMcDlm,1,"os io nltk.tokenize word_tokenize  Set working directory os.chdir '') ## Functions rempct s $%&\ )*+ -./:;<=>?@[\\]^_`{|}~€$' + '0123456789' + ""'"" cleaner txt wordcount words  dct collections Counter negwordcount words  dct  negdct  lngram collections Counter ngrams  Read text file  Additional input ## Read in LM lexicon  Negative  Positive ## Count positive and negative words  Tokenize -> Words  Number of words in article  Count words in lexicon  Count negated words in lexicon  Total number of positive/negative words  Print results",77
TXTfpbsupervised,"Estimates regularized linear model for sentiment classification with stochastic gradient descent (SGD) learning. The results are evaluated by using the training set of Malo et al. (2014) and computing a confusion matrix.
This training set is available at https://www.researchgate.net/publication/251231364_FinancialPhraseBank-v10 but a pickled and preprocessed version is available in this quantlet folder.
Please install required Python packages before usage: os, io, nltk, pickle, sklearn, pandas, numpy.","text mining, data mining, counts, sentiment, classification, support vector machine, L1-norm penalty",Elisabeth Bommes,TXTfpbsupervised,1,os  change folder pickle pandas numpy sklearn.pipeline Pipeline sklearn.feature_extraction.text CountVectorizer sklearn.feature_extraction.text TfidfTransformer sklearn.cross_validation StratifiedKFold sklearn.cross_validation cross_val_score  svm sklearn.linear_model SGDClassifier  grid search sklearn.grid_search GridSearchCV pprint pprint time time  set directory  functions tokenize txt scores row accuracy pred  actual  Note: Modified to allow for upsampling  keep original data give indices.  stackoverflow.com/questions/23455728/scikit-learn-balanced-subsampling balanced_subsample x  y  subsample_size = 1.0  set_seed = None indexer index1  index2 cver y  x  folds  seed  Upsample  Indices cv_pred x  y  custom_cv  piper  unique_y = True  data  change directory  load phrasebank  setup  CountVectorizer  TfidfTransformer  SGDClassifier  obtain cross validation set  set pipeline to best model from grid search  save best parameters  crossvalidation  Re-estimate winning model  Re-estimate winning model on same cv sample and predict,113
SFM_GDP_excess_losses,This program fits Generalized Pareto Distributions to excess losses of stock/index.,"Extreme Value Theory, Pareto ,excess, losses",Daniel Traian Pele,SFM_excess_losses,2,!/usr/bin/env python  coding: utf-8  In[4]: This program fits GPD to excess losses of stock/index pandas datetime numpy matplotlib scipy.stats genpareto  We will look at stock prices overtime end = datetime.date.today ) pandas_datareader data y=x.abs ).dropna )  create a sorted series of unique data  x-data for the ECDF: evenly spaced sequence of the uniques  size of the x_values  y-data for the ECDF:  all the values in raw data less than the ith value in x_values  fraction of that value with respect to the size of the x_values  pushing the value in the y_values  return both x and y values     plot the fit x.plot t  pdf)  In[ ]:  In[ ]:  In[ ]:  In[ ]:  In[ ]:,114
SoNIC_AAPL_BTC_stability,stability analysis and choice of the number of clustering of SoNIC model on AAPL and BTC sentiment weights,ERROR,Yegor Klochkov,SoNIC_AAPL_BTC_stability,5,numpy scipy.stats norm get_missing_probabilities mat  conf=0.95  tol=0.1 missing_var X  delta  make_positive=True missing_covar X  Y  delta_x  delta_y,16
SoNIC_AAPL_BTC_stability,stability analysis and choice of the number of clustering of SoNIC model on AAPL and BTC sentiment weights,ERROR,Yegor Klochkov,SoNIC_AAPL_BTC_stability,5,"numpy matplotlib cvxopt matrix cvxopt.solvers qp seaborn scipy.linalg sqrtm math sklearn linear_model import read_data common common get_index_matrix cluster_num  ind  normalize=True  print ""WARNING!!!"") index_dist cluster_num  int1  int2 gauss_var1_process theta  sigma  T  variance of innovations is sigma * I lasso_from_covariance D  c  alpha  minimizes 1/2 x^{T}Dx - c^{T} x + \alpha \| v \|_1 random_basis n  k  orthogonal normalization of columns u_random n  k  cluster_num  index=None v_step D0  D1  alpha_v  z  make sure u^{\T}u = I! z_step cluster_num  D1  v  ind_old=None check this stupid function!!! <- apparently works... __init__ self  theta  u  v  index  loss  to implement: index choice; initial index + initial u  matrix competition  index competition?) alternating cluster_num  D0  D1  alpha_v  epochs=10  index_init=None print ""loss : {}"".format loss)) v_step_from_data x_train  y_train  alpha_v  z  make sure u^{\T}u = I! direct cluster_num  D0  D1  alpha_v  index_init=None loss print ""loss : {}"".format loss)) direct_from_data cluster_num  x_train  y_train  alpha_v  index_init=None  loss  print ""loss : {}"".format loss)) alternating_from_data cluster_num  x_train  y_train  alpha_v  epochs=100  index_init=None _func1 v  ind _func0 ind print ""K={}: epochs {}/{}"".format cluster_num  e  epochs)) matrix_competition type  repeat_num   *args  **kwargs simu n  c_num  s  T  pmin=1.0  define true index define true theta generate the time series include missing observations ax.set_xticklabels ax.get_xticklabels )  rotation=-90  fontsize=8) ax.set_yticklabels ax.get_yticklabels )  rotation=0  fontsize=8)  ax.set_xticklabels ax.get_xticklabels )  rotation=-90  fontsize=8)  ax.set_yticklabels ax.get_yticklabels )  rotation=0  fontsize=8) simu )",217
SoNIC_AAPL_BTC_stability,stability analysis and choice of the number of clustering of SoNIC model on AAPL and BTC sentiment weights,ERROR,Yegor Klochkov,SoNIC_AAPL_BTC_stability,5,common common pandas read_csv numpy matplotlib seaborn sklearn cluster common.alternating matrix_competition generate_random_weights num  SHOULD ONLY BE APPLIED IN MAIN PROCESS!  let p > q and p + q = 1 then P X = 1 - \sqrt{q/p}) = p and P X = 1 + \sqrt{p/q}) = q  ensures that E X) = 1 and Var X) = 1 and X > 0 a.s.  moreover  taking p = 4/5 and q = 1/5 we have X \in [1 - 1/2  1 + 2] = [0.5  3] trans x,88
SoNIC_AAPL_BTC_stability,stability analysis and choice of the number of clustering of SoNIC model on AAPL and BTC sentiment weights,ERROR,Yegor Klochkov,SoNIC_AAPL_BTC_stability,5,numpy  func takes ind! find_new_home func  k  ind  i greedy_update func  k  ind __init__ self  index  value  message  nochange kmeans_greedy func  k  n  iter_limit=10000  init_index=None,25
SoNIC_AAPL_BTC_stability,stability analysis and choice of the number of clustering of SoNIC model on AAPL and BTC sentiment weights,ERROR,Yegor Klochkov,SoNIC_AAPL_BTC_stability,5,numpy pandas read_csv csv math sqrt find_homo_Be x  scale=4  res=20 check_interval l  r read_SIFI name_to_sifi read_stock_twits_user_sentiment name_to_twits  min_delta=0.1  min_days=200 avoid_empty = lambda x  err_t: x[0] if np.size x) > 0 else err_t read_crix_returns name_to_crix_price read_crypto_sentiment name_to_crypto_sentiment  top_num=50,37
pyTSA_ReturnsPG,"This Quantlet plots monthly time series of returns of Procter and Gamble from 1961 to 2016 and  their ACF and PACF (Example, 2.4 Figures 2.8-2.9 in the book)","time series, autocorrelation, returns, ACF, PACF, plot, visualisation","Huang Changquan, Alla Petukhina",pyTSA_UnitRootTest,1,numpy pandas statsmodels PythonTsa.plot_acf_pacf acf_pacf_fig matplotlib statsmodels.tsa.stattools adfuller,8
SDA_2019_St_Gallen_PD_with_ML_and_Sentiment_Analysis_Data_Preprocessing,"Preprocess raw data from WRDS, create separate .csv files for balance sheet, income statement and cash flow reports, Identify and match the companies that went bankrupt or disappeared, create labels for each type. Calculate financial ratios and save into a separate .csv file. Collect data of macroeconomic indicators from World Bank database, map into companies panel.","data collection, data cleaning, preprocessing, financial ratios, macroeconomic indicators",ERROR,Data_Preprocessing_,3, -*- coding: utf-8 -*- os pandas matplotlib  Set working directory import wids data import bankruptcy label data plot of bankrupt companies by year create disappered label data plot of disappeared companies by year create wids supdataset for label transform disapperared label data into sudataset for disap label transform bankruptcy label data into sudataset for bnkrpt label save the data,59
SDA_2019_St_Gallen_PD_with_ML_and_Sentiment_Analysis_Data_Preprocessing,"Preprocess raw data from WRDS, create separate .csv files for balance sheet, income statement and cash flow reports, Identify and match the companies that went bankrupt or disappeared, create labels for each type. Calculate financial ratios and save into a separate .csv file. Collect data of macroeconomic indicators from World Bank database, map into companies panel.","data collection, data cleaning, preprocessing, financial ratios, macroeconomic indicators",ERROR,Data_Preprocessing_,3," -*- coding: utf-8 -*- pandas numpy wbdata datetime  download macroenomic data from World Bank database   indicator ids can be found here: https://datacatalog.worldbank.org/ calculate finacial ratios for companies  NB! there are companies with 0 sales value  thus some indicators cannot be calculated  Profitability ratios  leverage ratios liquidity ratios Activity ratios ratiosdf.loc[  ratiosdf[ratio].isnull )) |  ratiosdf[ratio] == np.inf) |  ratiosdf[ratio] == -np.inf)) &  ratiosdf[""fyear""] == group[0])  ratio] = group[1][ratio].loc[ group[1][ratio].notnull )) &   group[1][ratio] != np.inf) &  group[1][ratio] != -np.inf))].mean ) ratiosdf[""net_prof_margin""] == -np.inf sum np.isinf np.array ratiosdf))) investigate = [] investigate.append list ratiosdf.columns)) investigate.append list sum np.isinf np.array ratiosdf))))) np.absolute ratiosdf.loc[:  ""net_prof_margin""])  == np.inf  fit macro data to companies panel  the panel data is unbalanced more efficient loop fdi is not scaled yet ratiosdf.to_csv 'combined_ratios_macro.csv') ",123
SDA_2019_St_Gallen_PD_with_ML_and_Sentiment_Analysis_Data_Preprocessing,"Preprocess raw data from WRDS, create separate .csv files for balance sheet, income statement and cash flow reports, Identify and match the companies that went bankrupt or disappeared, create labels for each type. Calculate financial ratios and save into a separate .csv file. Collect data of macroeconomic indicators from World Bank database, map into companies panel.","data collection, data cleaning, preprocessing, financial ratios, macroeconomic indicators",ERROR,Data_Preprocessing_,3," -*- coding: utf-8 -*- os pandas  Set working directory import dataset keep only data from dates after 2000 and before 2019 drop all non-US companies drop all financial service providers  according to global industry classification standard) drop all columns which only contain na drop columns which have the same values for all companies drop companies which has ""at"" = na / 0 replace zero/na revenue with small number to prevent division by zero Uncomment the following lines if you do not want ratios Creating ratios by dividing everything in the balancesheet by total assets   Creating ratios by dividing everything in the cashflow statment by total revenue Creating ratios by dividing everything in the income statement by total revenue drop all columns for which less than 1% of the data has entries save the preprocessed data",135
pyTSA_MacroUS2,"This Quantlet plots monthly time series of returns of Procter and Gamble from 1961 to 2016 and  their ACF and PACF (Example, 2.4 Figures 2.8-2.9 in the book)","time series, autocorrelation, returns, ACF, PACF, plot, visualisation","Huang Changquan, Alla Petukhina",pyTSA_MacroUS2,1,pandas statsmodels matplotlib PythonTsa.plot_acf_pacf acf_pacf_fig PythonTsa.LjungBoxtest plot_LB_pvalue statsmodels.tsa.regime_switching.tests.test_markov_regression fedfunds,9
SDA_20201125_hw07_DistanceMeasure,Distance measure for continuous variables using Euclidian/Manhattan/Maximum methods,ERROR,Andreas Rony Wijaya,SDA_20201125_hw07_DistanceMeasure,2,!/usr/bin/env python  coding: utf-8  In[32]: pandas numpy sklearn.neighbors DistanceMetric  In[33]: ## load csv file   ## regions: ME  NH  NY  In[34]:  In[35]: ## Euclidean distance  In[36]: ## Manhattan distance  In[37]: ## Maximum distance,32
SFERWSimu,Investigates time series of a random walk,"time series, random walk, RW, TS, first difference, non stationarity, python","Bharathi Srinivasan, David Berscheid",SFERWSimu,1,random_walk  os sys pandas pandas_datareader numpy statsmodels statsmodels statsmodels scipy arch arch_model matplotlib matplotlib  raw adjusted close prices  log returns tsplot y  lags=None  figsize= 10  8)  style='bmh' mpl.rcParams['font.family'] = 'Ubuntu Mono'  Random Walk without a drift  First difference of simulated Random Walk series  First difference of SPY prices,48
A-Shape_Crystal,ERROR,"Delaunay Triangulation, Alpha shape (a-shape)",Elena Ivanova,A-Shape_Crystal,2,!/usr/bin/env python  coding: utf-8  In[ ]: pandas numpy datetime matplotlib the points are taken from the Chapter 6 Delaunay Triangulation  Delaunay_Triangulations_ETH_Zuerich.pdf) scipy.spatial Delaunay numpy alpha_shape points  alpha  only_outer=True add_edge edges  i  j  already added  if both neighboring triangles are in shape  it's not a boundary edge  Loop over triangles:  ia  ib  ic = indices of corner points of the triangle  Computing radius of triangle circumcircle  www.mathalino.com/reviewer/derivation-of-formulas/derivation-of-formula-for-radius-of-circumcircle  Computing the alpha shape print tri.vertices) for a  b  c in tri.vertices:    for i  j in [ a  b)   b  c)   c  a)]:       plt.plot data_points[[i  j]  0]  data_points[[i  j]  1]  color='gray') plt.show ) plot as a graph basis the DT in gray color fig=plt.figure figsize= 4 4)) ax=fig.add_subplot 2  2  2) circle1 = plt.Circle  0  0)  0.2  color='r') ax.add_patch circle1)   label='alpha')  ax.legend loc='best') color=['b'  'g'  'r'  'c'  'm'  'y']) ax.add_artist circle1),137
DEDA_Class_2017_Introduction,"Introduce basic syntax, like numeric and string, and basic data structure, like list, tuple, set and dict in Python",ERROR,Junjie Hu,DEDA_Class_2017_Python_Introduction,2,!/usr/bin/env python  coding: utf-8  # Python Basic Syntax and Data Structure Introduction  # numeric operation  In[20]:  In[7]:  In[8]:  using back slash to escape the single quote.  In[9]:  using for loop to iterate all elements in the list.  In[12]:  a string is also an object  using the join method to connect all the words in the list.  In[13]:  slicing string by indices.  # Quiz:  Try changing the indices to negative.  In[14]:  see other methods of string object  like:  In[15]:  by using dir ) function  or help ) function  likewise   In[16]:  or a str instance  In[17]:  formatting string  using format method  In[18]:  using f string  you can write variable name into brackets  directly.  # tuple and set  In[22]:  tuple is like list  but tuple is immutable  In[23]:  ! Error will happen  tuple is not mutable  In[24]:  set  sets will drop duplicated elements automatically  In[25]:  In[26]:  In[27]:  In[29]:  # list  In[ ]: ']  how many elements in a list?  how many elements in a string?  adding elements into the list  append ) allows you to add 1 element at the end of the list  ['English'  'German'  'Chinese'  'Spanish']  insert ) allows you to add 1 element at arbitrary place  ['Python'  'C++'  'Java'  'C#']  extend ) allows you to add multiple elements at the end of the list  ['Python'  'C++'  'Java'  'C#'  ['python 2.7'  'python 3.6']]  ['English'  'German'  'Chinese'  'Spanish'  'Japanese'  'Korean']  remove elements  ['Python'  'C++'  'Java'  'C#']  'Korean' pops out. ['English'  'German'  'Chinese'  'Spanish'  'Japanese']  ['Python'  'C++'  'Java']  reorder in list  [22  95  32  11  43]  [11  22  32  43  95]  [95  43  32  22  11]  sorted function can return a new list instead of altering the original list  basic search in list  11  95  203  2  iterate in list  iterate in list with index  list to string  'Spanish  Japanese  German  English  Chinese'  ['Spanish'  'Japanese'  'German'  'English'  'Chinese']  looping in the list. The basic syntax is:  [func ele) for func ele) in a_list if func ele)]  For example:  format number in list to 2 digit  In[ ]:  In[ ]:  In[ ]:  In[ ]:  In[ ]:  In[ ]:  In[ ]:  # Dictionary  In[ ]:  initial a dict  find the value by using the key  'Anna'  get all the keys and put into a list  ['name'  'birth'  'gender'  'height']  get all values and put into a list  ['Anna'  '10-05-2000'  'female'  1.7]  get all key-value pairs and put into a list  add a key with value  change value of a key  alter multiple keys and values at a time  loop with key and vlaue,418
web_scraping_sciencedirect,"scraping on sciencedirect with scrapy, the data will be used to create topic modeling about covid19 based on the abstract",ERROR,"Muhaimin, Rasyid, Aziz, 07/12/2020",web_scraping_sciencedirect,3,scrapy Crawling.items AbstractItem time start_requests self parse self  response  header= headers open_article self  response  header= headers start_requests self parse self  response  header= headers,23
web_scraping_sciencedirect,"scraping on sciencedirect with scrapy, the data will be used to create topic modeling about covid19 based on the abstract",ERROR,"Muhaimin, Rasyid, Aziz, 07/12/2020",web_scraping_sciencedirect,3, -*- coding: utf-8 -*-  Web scraping on sciencedirect using scrapy.  The purpose of this code is scrape an abstract from the article on sciencedirect.  There are some keywords that used  especially about covid-19.  The data will analyze with topic modeling methods.  Author: aziz  muhaimin  Install scrapy  This step is required if you don't have scrapy library  Import library scrapy scrapy.linkextractors LinkExtractor pandas os json  Create a scrapy project called Crawling.  A folder named Crawling will be created in your device.  After the created  just change the directory into it  Type this on command prompt.  Type this on command prompt to change the directory  Type this on python.  Before start to crawl/scrape from the web. Make sure the spider script already in the folder spiders on '/content/Crawling/Crawling/spiders/'.  The spider script named sciencedirect.py  the file is exist in the github.  Moreover  overwrite the items.py script on '/content/Crawling/Crawling/' with the items.py script that exist in the github.  Then strat scraping  Type this on command prompt to start scraping  Convert json to dataframe  This result  data) will be processing first before analyze using LDA/DTM,180
web_scraping_sciencedirect,"scraping on sciencedirect with scrapy, the data will be used to create topic modeling about covid19 based on the abstract",ERROR,"Muhaimin, Rasyid, Aziz, 07/12/2020",web_scraping_sciencedirect,3, -*- coding: utf-8 -*-  Define here the models for your scraped items  See documentation in:  https://docs.scrapy.org/en/latest/topics/items.html scrapy  define the fields for your item here like:  pass  define the fields for your item here like:  tags = scrapy.Field )  datetime = scrapy.Field )  pass,43
DEDA_Class_SS2018_CrypPool,Quantify the degree of Mining Pool concentration for different cryptocurrencies,"Blockchain, Cryptocurrency, Gini Index, Herfindahl Hirschman Index",Ivan Perez - Mahdi Bayat,DEDA_Class_SS2018_Per_Bay_Crypto_Mining_Pool,2," -*- coding: utf-8 -*- pandas numpy matplotlib pyplot PIL Image  The following code includes ""unknown miner"" as a single group  On the second    part of the code the ""unkown miner"" is excluded and the analysis is repeated)  BITCOIN data loading  Do no forget to change the current directory  BITCOIN CASH data loading  ETH beta data loading eth_pools = eth.resample ""M"")[""miner""].value_counts ) eth_pools.to_csv ""eth_pools"")  LITECOIN data loading  DASH data loading dash_pools = dash.to_period 'M').groupby 'unixtime')['miner'].value_counts )  GINI CALCULATION  # Define Gini Formula    according to: https://en.wikipedia.org/wiki/Gini_coefficient  section: Alternate expressions) gini arr # first sort  Generate data frame with Gini Index  Plot GINI  Save plot with transparent background  Plot Lorenz Curve  Save plot with transparent background  Data for table of mining pool participation  HERFINDAHL HIRSCHMAN INDEX  HHI) CALCULATION hhi arr # first sort  Generate data frame with HHI Index  Plots HHI  Save plot with transparent background  Participation of the big 3  cumulative participation of the 3 most important pools) ##############################################################################  Excluding Unknown Miner  Generate data frame with Gini Index  without unknown miner)  PLOTS GINI  without unknown miner)  Save plot with transparent background  Data for table of mining pool participation  exluding unknown miners)  Generate data frame with HHI Index  without unknown miner)  Plot HHI  Save plot with transparent background  Participation of the big 3  cumulative participation of the 3 most important pools) ##############################################################################  Plots GINI vs Price  BTC price data loading   Save plot with transparent background  ETH price data loading  Save plot with transparent background  LTC price data loading  Save plot with transparent background  DASH price data loading  Save plot with transparent background ##############################################################################  Plots HHI vs Price  BTC  Save plot with transparent background  ETH  Save plot with transparent background  LTC  Save plot with transparent background  DASH  Save plot with transparent background ##############################################################################  Plots HHI vs difficulty  BTC  ETH  LTC  DASH ##############################################################################  VAR model statsmodels statsmodels.tsa.api VAR statsmodels.tsa.base.datetools dates_from_str statsmodels.tsa.stattools grangercausalitytests  Prepare data for the VAR model  VAR model for BTC  save results as image plt.text 0.01  0.05  str model.summary ))  {'fontsize': 12}) old approach  approach improved by OP -> monospace!  Save image with transparent background  Granger causality test  ""price causing hhi)  Save image with transparent background  Granger causality test  ""hhi causing price)  Save image with transparent background  VAR model for BTC in 2017 - 2018  save results as image plt.text 0.01  0.05  str model.summary ))  {'fontsize': 12}) old approach  approach improved by OP -> monospace!  Save image with transparent background",397
DEDA_Class_SS2018_CrypPool,Quantify the degree of Mining Pool concentration for different cryptocurrencies,"Blockchain, Cryptocurrency, Gini Index, Herfindahl Hirschman Index",Ivan Perez - Mahdi Bayat,DEDA_Class_SS2018_Per_Bay_Crypto_Mining_Pool,2, -*- coding: utf-8 -*- requests bs4 BeautifulSoup time # ------  EXAMPLE  -------  Erase the ''' to acttivate the lines for the example:  --------------------  CODE TO GET THE DATA  --------------------  2017 blocks are those in pages 215:780  so range should be range 215 780) r.write header),45
SL_2019_SentimentAnalysis_CryptoPosts,Creates a sentiment analysis of posts related to crypto currencies. The data for the analysis is taken from a crypto currency forum and is from the years 2013 to 2019. For the analysis the vader package is used.,"Sentiment Analysis, vader, textblob, Python, text data, data visualization, crypto currencies, wordcloud","Anna Pacher, Kathrin Spendier",SL_2019_SentimentAnalysis_CryptoPosts,3, -*- coding: utf-8 -*- !pip install vaderSentiment vaderSentiment.vaderSentiment SentimentIntensityAnalyzer sentiment_analyzer_scores sentence 2013 2014 2015 2016 2017 2018 2019  plot the scores over time matplotlib matplotlib  label for x-axis  Label for y-axis  set ranges for axes manually  Syntax: plt.axis [xmin  xmax  ymin  ymax])  plot the different lines: show legend  show grid  add title  save diagram  Diagramm anzeigen:,56
SL_2019_SentimentAnalysis_CryptoPosts,Creates a sentiment analysis of posts related to crypto currencies. The data for the analysis is taken from a crypto currency forum and is from the years 2013 to 2019. For the analysis the vader package is used.,"Sentiment Analysis, vader, textblob, Python, text data, data visualization, crypto currencies, wordcloud","Anna Pacher, Kathrin Spendier",SL_2019_SentimentAnalysis_CryptoPosts,3, -*- coding: utf-8 -*- textblob TextBlob  you may need to install textblob with  !pip install textblob  Out[1]: Sentiment polarity=0.3416666666666667  subjectivity=0.6583333333333333) Out[2]: Sentiment polarity=0.5084635416666666  subjectivity=0.6583333333333333)  ! improves the polarity rate Out[3]: Sentiment polarity=0.5532986111111111  subjectivity=0.7722222222222221)  :) improves the polarity rate Out[4]: Sentiment polarity=0.5084635416666666  subjectivity=0.6583333333333333)  upper case letters make no difference For German texts this package does not work: Out[4]: Sentiment polarity=0.3333333333333333  subjectivity=0.6666666666666666) Does not interpret slang words: Out[5]: Sentiment polarity=0.0  subjectivity=0.0) Out[6]: Sentiment polarity=0.0  subjectivity=0.0) Emojis are not implemented: Out[7]: Sentiment polarity=0.0  subjectivity=0.0) Out[8]: Sentiment polarity=0.0  subjectivity=0.0) Some acronyms are recognized: Out[9]: Sentiment polarity=0.8  subjectivity=0.7) Out[10]: Sentiment polarity=0.0  subjectivity=0.0) #################################################################### vaderSentiment.vaderSentiment SentimentIntensityAnalyzer  you may need to install vaderSentiment with  !pip install vaderSentiment  sentiment_analyzer_scores sentence Bitcoins are super cool----------------- {'neg': 0.0  'neu': 0.244  'pos': 0.756  'compound': 0.7351} Bitcoins are super cool!!!-------------- {'neg': 0.0  'neu': 0.22  'pos': 0.78  'compound': 0.795}  ! improves the sentiment score Bitcoins are super cool :) !!!---------- {'neg': 0.0  'neu': 0.229  'pos': 0.771  'compound': 0.8772}  :) & ! leads to a very high compound score Bitcoins are SUPER COOL!!!-------------- {'neg': 0.0  'neu': 0.19  'pos': 0.81  'compound': 0.8605}  capital letters improve the sentiment score For German texts this packages does not work: Bitcoins sind nicht super--------------- {'neg': 0.0  'neu': 0.435  'pos': 0.565  'compound': 0.5994} Successfully interprets slang words: Bitcoin sux----------------------------- {'neg': 0.714  'neu': 0.286  'pos': 0.0  'compound': -0.3612} Bitcoin kinda sux----------------------- {'neg': 0.525  'neu': 0.475  'pos': 0.0  'compound': -0.2975} Emojis are implemented  recognizes UTF-8 encoded emojis) Bitcoins make me 😊---------------------- {'neg': 0.0  'neu': 0.5  'pos': 0.5  'compound': 0.7184} Bitcoins make me 😭---------------------- {'neg': 0.383  'neu': 0.617  'pos': 0.0  'compound': -0.4767} Acronyms are recognized: LOL------------------------------------- {'neg': 0.0  'neu': 0.0  'pos': 1.0  'compound': 0.4215} XOXO------------------------------------ {'neg': 0.0  'neu': 0.0  'pos': 1.0  'compound': 0.6124,280
SL_2019_SentimentAnalysis_CryptoPosts,Creates a sentiment analysis of posts related to crypto currencies. The data for the analysis is taken from a crypto currency forum and is from the years 2013 to 2019. For the analysis the vader package is used.,"Sentiment Analysis, vader, textblob, Python, text data, data visualization, crypto currencies, wordcloud","Anna Pacher, Kathrin Spendier",SL_2019_SentimentAnalysis_CryptoPosts,3, -*- coding: utf-8 -*-  importing the necessery modules  wordcloud WordCloud matplotlib csv os os path PIL Image numpy nltk nltk.corpus stopwords  Please change the working directory to your path! import english stopwords  these are words that will be excluded from the wordcloud  for instance articles and prepositions)  file object is created   reader object is created   contents of reader object is stored .   data is stored in list of list format.   empty string is declare   iterating through list of rows   iterating through words in the row   concatenate the words   Mask ########################################  remove stopwords from WordCloud   show  200 words in the wordcloud .   plot the WordCloud image    store to file,108
DEDA_Class_2018WS_Berlin_Property_Analysis_OLS,"Ordinary Least Squares regression (OLS) for analysis of correlation between real estate listing prices and social / location-based features for real estate within Berlin, DE.","Spatial Analysis, Social Data, Real Estate, Linear Regression, OLS",Alex Truesdale,DEDA_Class_2018WS_Berlin_Property_Analysis_OLS,1, -*- coding: utf-8 -*- os time numpy pandas statsmodels  Introduction.  Read in data.  Fit regression model.  Summary of model.  Output summary at HTML and read back in as DataFrame.,29
DEDA_SVM_Swiss_PC,Determines and plots the decision boundary of a SVM classifier with linear kernel using the first 2 principal components of the Swiss banknote dataset.,"Support vector machines, SVM, classification, principal components, PC",Georg Keilbar,DEDA_SVM_Swiss_PC,1,numpy numpy genfromtxt matplotlib sklearn svm sklearn.decomposition PCA Load data Simple scatterplot plot_svm_nonlinear x  y  model_class  **model_params Fit model Define grid Prediction on grid Contour + scatter plot plt.gca ).set_aspect 'equal'  adjustable='box'),32
PythonTsa_call_functions,"This chapter first introduces the backshift operator which is one way to make a nonstationary time series stationary. It presents the AR, MA and ARMA models and discusses their properties. Also distinguishes the ARMA model from the ARMA process.","ARMA process, LjungBox test, ACF, PACF","Changquan Huang, Maria Culjak",STSM,1,!/usr/bin/env python3  -*- coding: utf-8 -*- numpy PythonTsa.LjungBoxtest PythonTsa.plot_acf_pacf statsmodels,10
SDA_2019_St_Gallen_POI_NLP_NETWORK_ENRON,The repository contains 3 parts : 1. Person of interests 2. Natural language process (LDA) 3. Network,"random forest, decision tree, naive bayes, lda, nlp, network, ,enron, email, financial ",ERROR,SDA_2019_St_Gallen_POI_NLP_NETWORK_ENRON,8,"!/usr/bin/env python  coding: utf-8  # Person of interest  ## Load packages  In[1]: # Loading the packages pickle matplotlib pyplot numpy pandas sklearn.metrics accuracy_score sklearn.metrics precision_score sklearn.metrics recall_score feature_format featureFormat  In[2]: We clean the dataset and select the features that interest us at first. For a start  we hypothetize that  salary shoud have a relation to POI as well as bonus should  In[3]:  We create the dataset reading the file:  In[5]:  Converting the given pickled Enron data to a pandas dataframe to be able to read it in a stylized way  set the index of df to be the employees series:  In[6]:  In[7]:  In[8]: We detect here that there's an anomaly  And detect that the problem is ""TOTAL""  In[9]:  Thus we take out Total and plot it  Drawing scatterplot with the modified dataframe  In[10]:  Finding out the integer index locations of POIs and non-POIs  In[11]: Let's further dig now into deferred income by making a scatterplot of payments to deferral to check for anomalies  Mmmm interesting to find there's some anomaly around it. So let's find out what it is about and drop it  Finding the non-POI employee having maximum 'deferral_payments'  In[12]:  Finding the non-POI employee having maximum 'deferral_payments' Now we drop it  ## We find out the long term incentives they may have.   In[45]:  Finding out the integer index locations of POIs and non-POIs So we find that one of them has a very high incentive  thus we may want to consider taking her out of the picture.  In[74]:  Scatterplot of fraction of mails from and to between poi and this_person w.r.t POI  In[14]:  In[15]: Finally  we can also suspect there may be a relationship the bonus and salary  thus we find the ratio.  We have learned there's something called ""the travel agency in the park"" which sounds too misterious. Thus we dig into it  Features of the index 'THE TRAVEL AGENCY IN THE PARK' Since it doesn't have valuable information  we will drop it.  In[16]:  In[17]: ## Now we can use more features to expand the results computeFraction  poi_messages  all_messages   occurred when created additive features  all emails)  In[81]:  # Algorithm application  ## We will specifically use four:  ### 1. Random forest classifier   ### 2. Decision tree  ### 3. Naive bayes  ### 4. Support vector  ## 1. Random forest classifier  In[84]: We start trying the predictors: sklearn.model_selection train_test_split sklearn.ensemble RandomForestClassifier Getting precision Getting recall  ## 2. Decision tree  In[86]: #### sklearn.model_selection KFold make training and testing sets sklearn.tree DecisionTreeClassifier Getting precision Getting recall sklearn.externals.six StringIO IPython.display Image sklearn.tree export_graphviz pydotplus  ## 3. Naive Bayes  In[90]: #### sklearn.naive_bayes GaussianNB sklearn.metrics classification_report  ## 4. Support vector machine  In[91]: ###  sklearn.model_selection GridSearchCV sklearn.svm SVC  In[ ]:",443
SDA_2019_St_Gallen_POI_NLP_NETWORK_ENRON,The repository contains 3 parts : 1. Person of interests 2. Natural language process (LDA) 3. Network,"random forest, decision tree, naive bayes, lda, nlp, network, ,enron, email, financial ",ERROR,SDA_2019_St_Gallen_POI_NLP_NETWORK_ENRON,8, Source = class material PIL Image numpy matplotlib wordcloud WordCloud os os path logo_word_enron   Pass Text  to show the picture    logo_word_uni  PIL Image numpy matplotlib wordcloud WordCloud os os path matplotlib.pyplot figure  Pass Text  to show the picture ,38
SDA_2019_St_Gallen_POI_NLP_NETWORK_ENRON,The repository contains 3 parts : 1. Person of interests 2. Natural language process (LDA) 3. Network,"random forest, decision tree, naive bayes, lda, nlp, network, ,enron, email, financial ",ERROR,SDA_2019_St_Gallen_POI_NLP_NETWORK_ENRON,8,"!/usr/bin/env python  coding: utf-8  In[1]: IPython.display Image  -_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_-  # Class : 7 325 1.00 Smart Data Analytics  # Authors : A. Morales  M. Virgolin  V. Porret  -_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_-  # <u> Part : Natural Language Processing</u>    The goal of this part is to analyze the keywords in the mails and try to classify them by topics.  The table of this part is : </p>  1. Import data and library</p>  2. Create a Dataframe</p>  3. Prepare text  mails) and libraries for NLP  5 steps)</p>  4. Model the data with the NLP  Genesim) tool</p>  5. Analyze the results</p>  6. 4Fun : WordCloud of the keywords</p>  -_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_-  ### 1- Import Data and Library  -_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_-  In[2]: pandas random numpy Source data matplotlib Select randomly only a few mails as the data.set is huge  Shape of the data :   In[34]:  Quick look at the data :  In[4]:  Info :  In[5]:  The dataset has two columns. The first one give the origin of the mail and the second one shows the message. As the mails are not cleaned  we first have to do it.   -_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_-  ## 2. Create a Dataframe  -_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_-  Some functions from the Source.py file are used for this step  In[6]:  The ""body"" are the mails analayzed in this Notebook.   -_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_-  ## 3. Prepare texts  mails) and libraries for NLP  -_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_-  We are going to follow those steps  as suggested steps given by https://medium.com/datadriveninvestor/nlp-with-lda-analyzing-topics-in-the-enron-email-dataset-20326b7ae36f).   It is essential to pass through each one to minimze the noise of our analysis.   0. Import more libraries   1. Collect the body of emails into a list   2. Tokenize the text into separate words<  3. Remove stop words to minimize the noise  4. Group bigrams  5. Consolidate similar terms using lemmatization  ### 3.0 Import more libraries  In[7]: gensim Gensim is an open-source library for unsupervised topic modeling. gensim gensim.utils simple_preprocess gensim.models CoherenceModel spacy To prepare text pyLDAvis Interactive topic model visualization pyLDAvis nltk Natural Language Toolkit re Ex : ""A"" and ""a"" nltk.corpus stopwords To delete stop words  ### 3.1 Collect the body of emails into a list  Example of one mail. Among this document  the same mail will be display as example. The number of this mail can be changed here.   In[8]: mail used in the examples below  In[9]: all_emails #not printed as it is quite huge  ### 3.2 Tokenize the text into separate words  The tokenize part is used to separate each words and remove the ponctuation.   In[10]:  Tokenize // List of words to_words mail  deacc= punctuations  ### 3.3 Remove stop words to minimize the noise  Words as ""the""  ""as"" or ""you"" are deleted.   In[12]: remove_Allstopwords texts  ### 3.4 Group bigrams    Example : Traffic and light = Traffic light  In[13]: Preparation:  Utilisation :  make_bigrams texts  ""Threshold"" represents a ""score threshold for forming the phrases  higher means fewer phrases). A phrase of words a followed by b is accepted if the score of the phrase is greater than threshold. see the scoring setting.""</p>  ""Min_count ignore all words and bigrams with total collected count lower than this.""  In[14]: Example :   This is a random mail but along our research  we saw for example than ""north"" and ""america"" were grouped together as weel as ""Philipp"" and ""Allen"". However  this tool does not correct the typo that we can see in some of the mails  as ""dealsactually"" or ""withcompetition"").   ### 3.5 Consolidate similar terms using lemmatization  Example : Driving - Drives - Drove  In[15]: lemmatization texts allowed_postags=['NOUN' 'ADJ'  'VERB'  'ADV']  Do lemmatization keeping only noun  adj  vb  adv  In[16]:  Words have been transformed in the way we wanted. </p>Ex : Bidding = bid  selling = sell  and so on  -_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_-  ## 4. Model the data with the NLP  Genesim) tool  -_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_-  Based on our research  we choose to use Latent Dirichlet Allocation  LDA )to expore the content of our data. An advantage of this method is that  unlike K-Mean  a word can belong to multiple categories. It is therefore more flexible and more adequat. One of the best library is Genesim  that we already imported at the step 3.0)  The steps for this part is :   1. Create a dictionary  2. Create a corpus  3. Generate the model  ### 4.1 Create a dictionary  Allow to get a register of the words in mails  In[17]:  ### 4.2 Create a corpus  Allow to get the register of the words with the number of times that those words have been seen.   In[18]:  ### 4.3 Generate the model  In[19]: IPython.display Image  source : https://medium.com/datadriveninvestor/nlp-with-lda-analyzing-topics-in-the-enron-email-dataset-20326b7ae36f  In[20]:  Build LDA model  In[21]:  The number each token is multiplied by is its weight. These values reflect how important a token is within that topic.  -_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_-  ## 5. Analyze the results  -_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_--_-_-_-  In[22]:  Visualize the topics  In[23]:  Size of the bubbles = how dominant a topic is through all the mails analyzed  mail sample)</p>  Words on the right = component of a topic </p>  If the bubbles are close to each other  it means that the topics are quite the same.     ************************************************  THIS ANALYSIS HAS BEEN MADE ON A RANDOM SAMPLE BASES. IT IS POSSIBLE THAT THE RESULTS ARE CHANGING. </p>   ***  Some topics are directly linked to the business or to the usual emails keywords  as the three first one) and are not very usefull for our analysis  : </p>  1.  power  market energy  project  price  employee  customer  plan  and so on). </p>  2.  attach  forward  regard  send  receive  message). </p>  3.  go  get  think  want)  </p>  However  in the topics number 4 5 7 8 9 some keywords could be seen as strange and unwelcome.   For example  having words as :   4.  transaction  debt  governor  legislation  negociate) </p>  5.  deal  problem  crisis  provision) </p>  7.  federal  tax  bankruptcy  purpose  replace  owner  responsible)  </p>  8.  firm  image  internet)  or </p>  9.  volume  security  manipulate  assumption  juridiction) </p>  in the same topic on a random sample of only 2 5% of the main document is clearly doubtful. Nevertheless  this keywords are not sufficient to dress a final conclusion even if  after what finally happened  this kind of analysis could have permitted to perceive a strucural problem.       ************************************************  Our model is not perfect and some of the reasons are presented here : </p>  The first reason is the sample size. The main document contains 500 000 emails. During our analysis  it took 20 minutes to analyze a sample representing only 2% of the main document.To improve in quality  a more powerful computer or sufficient time should be in place to obtain more consistent results. </p>    Secondly  some keywords such are parasites in the analysis. A more detailed analysis of each email and a more rigorous removal of parasitic keywords would noticeably improve the results. Same things for typo in email  where fo example two words are linked without white space</p>    Thirdly  we noted that the lemmatization function  while extremely useful for analysis  still has a disadvantage. Indeed  some proper names are deleted because they are not present in the database as a known word. Thus  people who are very often named in emails and who could be part of the people of interest are not involved in the results. However  if we are not using this function  words are nout grouped as we wanted and the results are worse.  </p>    Fourthly  the number of topics to select is quite random. Based on our research  some argue that a number of topics between 12 and 14 was the optimum. According to our results  too many topics  >12) increased errors and bubbles overlapping in the model  which is considered not to be very efficient. Since the analysis takes a long time to run  it would be sufficient to perform the test several times and compare the results. We did not do it for reasons of time optimization but it would be very easy to do so. </p>    Fifth  we conducted our analysis with Gensim. A second approach  carried out for example with mallet  another analysis model based on the natural language process  could make it possible to compare the results and draw precise conclusions. </p>    In other words  we obtained a mitigated result with strong room for improvement. We will probably upgrade this model in order to personally have a powerful text analysis tool.   *******************************      As observed on similar topics analysis  we calculated the perpexity and the coherance. </p>  ""In general  perplexity is a measurement of how well a probability model predicts a sample. In the context of Natural Language Processing  perplexity is one way to evaluate language models."" https://towardsdatascience.com/perplexity-intuition-and-derivation-105dd481c8f3. The more little  the better.     In[24]:  This result is not far from other experimentation  ~8). We can improve this number by increasing the number of mails analyzed.   In[25]:  Compute Coherence Score coherence_model_lda = CoherenceModel model=lda_model  texts=texts  dictionary=id2word  coherence='c_v') coherence_lda = coherence_model_lda.get_coherence ) print 'Coherence Score: ') #coherence_lda)  ""This score is trying to quantify the semantic similarities of the high scoring words within each topic. A high score means the result is more human-interpretable.""https://medium.com/datadriveninvestor/nlp-with-lda-analyzing-topics-in-the-enron-email-dataset-20326b7ae36f""  </p>  We have an error in this code. Even after many hours of research we did not recover it. However  before the modification  we had a result of around 0.455  which is not perfect but near the result of the other similar analysis. As we said earlier  it could be improved by different ways.   ## 4Fun : WordCloud of the keywords  <u> Logo Enron and Logo HSG</u></p>      Source : class material  In[26]: word__cloud  In[27]:  In[28]:  In[29]:  In[ ]:",1556
SDA_2019_St_Gallen_POI_NLP_NETWORK_ENRON,The repository contains 3 parts : 1. Person of interests 2. Natural language process (LDA) 3. Network,"random forest, decision tree, naive bayes, lda, nlp, network, ,enron, email, financial ",ERROR,SDA_2019_St_Gallen_POI_NLP_NETWORK_ENRON,8,"000000""> 1e2327""> start-of-content"" tabindex=""1"" class=""p-3 bg-blue text-white show-on-focus js-skip-to-content"">Skip to content</a> x2713;"" /> csrf-token=rvE8cVWEwmQZ9XosqFzlwbvYsdUU6i3dDukkSNltzNxrk6ahTgU5MicQejb7+egTDO46ys+c8asY/HSUn5hFdA=="" x2713;"" /><input type=""hidden"" name=""authenticity_token"" value=""0yO+LM0HBY0JsS+rJwRdFtEMsQT0AnX4leLQbUNtVCvlVxmEMaARK8Pl0i0jxSukww1fgULMmWt/6vrrbEDvEQ=="" /> a x2713;"" /><input type=""hidden"" name=""_method"" value=""put"" /><input type=""hidden"" name=""authenticity_token"" value=""IJc/eia8m1alB+MFiGe67WJs6jPRgXZi6j2twxd+A0jvkruJJathHWgEbWjh3HmITA3+lQ0BFT8glkxxPhwC1A=="" /> x2713;"" /><input type=""hidden"" name=""authenticity_token"" value=""+ahsvXGwheq0jg/Sxk8G3wuHjwf/tVFlMRhNlTQsHqLP3MsVjReRTH7a8lTCjnBtGYZhgkl7vfbbEGcTGwGlmA=="" /> x2713;"" /><input type=""hidden"" name=""authenticity_token"" value=""y7XESFG6xD/4tufE7JjyrE4nwykuVsFaH0ZxKQga8v22pq4BeKVjF79X0+DGNTfBBGHCgPWfVX8FbJkW2A+iNA=="" />      <input type=""hidden"" name=""repository_id"" value=""87695012""> show"">          <span data-menu-button> x2713;"" /><input type=""hidden"" name=""authenticity_token"" value=""p5hMohCz/qQmZnDumWiYAPSbbyWEQpumKIpnSVT8zo8hy2mt9Krz9myuZnzDz7Ma9Wb+Etn936GDRRpkJTnsKA=="" /> show; text:Unstar"">        <svg class=""octicon octicon-star v-align-text-bottom"" viewBox=""0 0 14 16"" version=""1.1"" width=""14"" height=""16"" aria-hidden=""true""><path fill-rule=""evenodd"" d=""M14 6l-4.9-.64L7 1 4.9 5.36 0 6l3.6 3.26L2.67 14 7 11.67 11.33 14l-.93-4.74L14 6z""/></svg> x2713;"" /><input type=""hidden"" name=""authenticity_token"" value=""PL2HzSCB/yC3bZdP3oWjvUn8SLuIThbVsyDTBticFHgwjQjSIdnFg9NsEzAizW+/vNaN47lsi/x3lG0ScSBsbg=="" /> show; text:Star"">        <svg class=""octicon octicon-star v-align-text-bottom"" viewBox=""0 0 14 16"" version=""1.1"" width=""14"" height=""16"" aria-hidden=""true""><path fill-rule=""evenodd"" d=""M14 6l-4.9-.64L7 1 4.9 5.36 0 6l3.6 3.26L2.67 14 7 11.67 11.33 14l-.93-4.74L14 6z""/></svg> x2713;"" /><input type=""hidden"" name=""authenticity_token"" value=""jVcmRn9NWRg2Hc9jKQHAqSjRT2rCyozvQTXZvyqhT6XEyTpYhdsDQvfNbDJWsfpCy0w+QS6dAc4LsQXipBxXNQ=="" /> show; text:Fork"" type=""submit"" title=""Fork your own copy of mohdkashif93/Enrol-POI-Classification to your account"" aria-label=""Fork your own copy of mohdkashif93/Enrol-POI-Classification to your account"">              <svg class=""octicon octicon-repo-forked v-align-text-bottom"" viewBox=""0 0 10 16"" version=""1.1"" width=""10"" height=""16"" aria-hidden=""true""><path fill-rule=""evenodd"" d=""M8 1a1.993 1.993 0 00-1 3.72V6L5 8 3 6V4.72A1.993 1.993 0 002 1a1.993 1.993 0 00-1 3.72V6.5l3 3v1.78A1.993 1.993 0 005 15a1.993 1.993 0 001-3.72V9.5l3-3V4.72A1.993 1.993 0 008 1zM2 4.2C1.34 4.2.8 3.65.8 3c0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2zm3 10c-.66 0-1.2-.55-1.2-1.2 0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2zm3-10c-.66 0-1.2-.55-1.2-1.2 0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2z""/></svg> js-repo-pjax-container"" href=""/mohdkashif93/Enrol-POI-Classification"">Enrol-POI-Classification</a></strong> js-repo-pjax-container""> x2713;"" /><input type=""hidden"" name=""authenticity_token"" value=""IPXL5jy+ocB5A/j5u39PeEOTRALhXO1fMAkIjSfqmcErM5ZWf95NP07Ianw2DVQzY9Uze4suiWzWG3Ndh3P7Fg=="" /> x2713;"" /><input type=""hidden"" name=""authenticity_token"" value=""kBMTJIDaeRrcsdD8IZkoZs58goU2vOD6RGg4RzsOLTpU9KtlI4uSlgPnv+SXaeXrDOP5ahFX6fFD0/plwbcNlA=="" /> x2713;"" /><input type=""hidden"" name=""authenticity_token"" value=""BVjplahNfs70wmT+939zsGjy5x8wVq95e2JjmRMDXPhYZ9//IvmEOs213cxKQwz7ArWe6dWM4UOqUreWtOjELA=="" /> d3ac3b;"" class=""octicon octicon-primitive-dot"" viewBox=""0 0 8 16"" version=""1.1"" width=""8"" height=""16"" aria-hidden=""true""><path fill-rule=""evenodd"" d=""M0 8c0-2.2 1.8-4 4-4s4 1.8 4 4-1.8 4-4 4-4-1.8-4-4z""/></svg> </span>!/usr/bin/python</span></td> 39;s </span></td> 39;s</span></td> 39;s what you would do:</span></td> 39;poi&#39; and is not checked for</span></td> </span> Key order - first branch is for Python 3 compatibility on mini-projects </span></td> </span> second branch is for compatibility on final project.</span></td> </span> Logic for deciding whether or not to add the data point.</span></td> </span> exclude &#39;poi&#39; class as criteria.</span></td> 39;</span>poi<span class=""pl-pds"">&#39;</span></span>:</td> </span>## if all features are zero and you want to remove</span></td> </span>## data points that are all zero  do that here</span></td> </span>## if any features for a given data point are zero</span></td> </span>## and you want to remove data points with any zeroes </span></td> </span>## handle that here</span></td> </span>## Append the data point if flagged for addition.</span></td> x2713;"" />",384
SDA_2019_St_Gallen_POI_NLP_NETWORK_ENRON,The repository contains 3 parts : 1. Person of interests 2. Natural language process (LDA) 3. Network,"random forest, decision tree, naive bayes, lda, nlp, network, ,enron, email, financial ",ERROR,SDA_2019_St_Gallen_POI_NLP_NETWORK_ENRON,8," ---  jupyter:    jupytext:      text_representation:        extension: .py        format_name: light        format_version: '1.4'        jupytext_version: 1.2.4    kernelspec:      display_name: Python 3      language: python      name: python3  ---  https://github.com/rhosse/Team-Lyrical/blob/9b059145dc26dc4e2624c6b6147da01d1f51fcdd/data_lemmatization.py  https://towardsdatascience.com/how-i-used-machine-learning-to-classify-emails-and-turn-them-into-insights-efed37c1e66 spacy nltk gensim Gensim is an open-source library for unsupervised topic modeling. gensim gensim.utils simple_preprocess gensim.models CoherenceModel spacy To prepare text pyLDAvis interactive topic model visualization pyLDAvis nltk Natural Language Toolkit re ex : ""A"" and ""a"" nltk.corpus stopwords to delete stop %matplotlib inline pandas random numpy data matplotlib parse_raw_message raw_message map_to_list emails  key parse_into_emails messages",79
SDA_2019_St_Gallen_POI_NLP_NETWORK_ENRON,The repository contains 3 parts : 1. Person of interests 2. Natural language process (LDA) 3. Network,"random forest, decision tree, naive bayes, lda, nlp, network, ,enron, email, financial ",ERROR,SDA_2019_St_Gallen_POI_NLP_NETWORK_ENRON,8,pandas,1
Proximity_Measure,Proximity measure for binary variables using Jaccard/Simple matching/Tanimoto methods,"proximity measure, binary variables, Jaccard, Simple matching, Tanimoto",Susan,Proximity_Measure,2,"!/usr/bin/env python  coding: utf-8  # Proximity Measure for Binary Variables    Briefing of Algorithm:  <br>  ▶ Read .dat file and convert to Numpy array  <br>  ▶ Calculate Mean and Binary matrix  <br>  ▶ Calculate Similarity Coefficient  <br>  ▶ Calculate Distance  distance = 1 – similarity coefficient)    In[1]: pandas numpy sklearn.neighbors DistanceMetric scipy.spatial distance  In[2]: ## load dat file  select specific car vendor: Renault  Rover  Toyota print ""Total records: ""  len df)) df  In[3]:  In[4]: ## transfer DataFrame to Numpy array print ""Data dimension: "" np.shape car_list))  In[5]: ## calculate binary data ## x_mu ## fill binary matrix; if x i k)>x_mu k): 1  else 0  In[6]: ## Calculate Jaccard measure ## Jaccard distance = I - Jaccard similarity coefficient  In[7]: ## Calculate Simple matching # SMD simple matching distance) = I- SMC simple matching coefficient)  In[8]: ## Calculate Tanimoto measure ## Tanimoto distance = I - Tanimoto similarity coefficient  In[ ]:",150
LDA-DTM_Christmas_Songs,Compare two Christmas Songs with their frequency words and their topics (LDA),"LDA, word cloud, topic modelling, Christmas Song, word frequency",Xinwen Ni,LDA-DTM_Christmas_Song,1,"!/usr/bin/env python3  -*- coding: utf-8 -*- Please install these modules before you run the code  !pip install wordcloud !pip install nltk !pip install Pillow !pip install numpy !pip install gensim !pip install pandas ############################################################   Part I: wordcloud  ############################################################  matplotlib re nltk.corpus stopwords os os path PIL Image wordcloud WordCloud numpy  Please change the working directory to your path! os.chdir ""/Users/xinwenni/LDA-DTM/xmas_song"")   keep only letters  numbers and whitespace  apply regex  lower case   Read the whole text.  Mask  Optional additional stopwords  Construct Word Cloud  no backgroundcolor and mode = 'RGBA' create transparency  Pass Text  store to file ############################################################  LDA ############################################################  os re nltk os path nltk.stem WordNetLemmatizer nltk.stem.porter PorterStemmer nltk.corpus stopwords nltk.corpus stopwords nltk.stem.wordnet WordNetLemmatizer string gensim gensim corpora matplotlib pandas numpy  Please change the working directory to your path! os.chdir ""/Users/xinwenni/LDA-DTM/xmas_song"")  doc_l.pop )[0]  Regex cleaning  keep only letters  numbers and whitespace  apply regex  lower case  nltk clean doc  Importing Gensim  Creating the term dictionary of our courpus  where every unique term is assigned an index.  Converting list of documents  corpus) into Document Term Matrix using dictionary prepared above.  Creating the object for LDA model using gensim library  Running and Trainign LDA model on the document term matrix. print ldamodel.print_topics num_topics=3  num_words=5))  20 need to modify to match the length of vocabulary                       columns[2]:""""                       columns[3]:""""                       columns[4]:""""                                                                                                      plt.imshow zz  cmap='hot'  interpolation='nearest') plt.show )  plt.title ""heatmap xmas song"")",222
DEDA_Class_2017_WebScrapingIntro,ERROR,ERROR,"Junjie Hu, Cathy YH Chen",DEDA_Class_2017_WebScrapingIntro,12,"## Please note: This file is not for execution ### requests json feedparser  retrieve RSS feedback  list all titles  list all description requests bs4 BeautifulSoup datetime pandas os  Using the requests module to get source code from the url  Using BeautifulSoup to parse webpage source code  Finding all the <p> tag content  initial empty list to load the data  Using .find_all ) method to search tag name and attribute  Loop in the <a> tag  strip ) method can remove specific chars at the head and tail  Formatting date see further more: https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior  Using try...except to catch unexpected condition so that program can keep running  If the tag doesn't exist  mark it as an empty list bs4 BeautifulSoup requests datetime os pandas pickle nasdaq_news_scraping page=1  refresh=False  Argument page equals 1 by default  Visit the home page if page equals 1  Change url by different argument  connect to the website if the webpage source code file is not exist of we need to refresh it  save the request object after scraping  else  we open the source code file from local disk to save time  Using the function defined previously with certain arguments as input  Import all the packages you need  always remember that you can find 99% packages you need in python requests  take the website source code back to you urllib  some useful functions to deal with website URLs bs4 BeautifulSoup  a package to parse website source code numpy  all the numerical calculation related methods re  regular expression package itertools  a package to do iteration works pickle  a package to save your file temporarily pandas  process structured data  the path you save your files  This link can represent the domain of a series of websites city_collection   get source code  parse source code  find the items with tag named 'dt'  iterate within all the items  get name of the province  get link to all the cities in the province  save dict in the dict  iterate with the province link to find all the cities  use the urllib package to join relative links in the proper way weather_collection link  print tr_item)  daily_weather = dict )  month_weather.append daily_weather)  Nice way to get a date string with certain format  '02d' means 2 digits   ==== Since I have already download the links to all the cities  you just need to execute from here:=====   ==== Otherwise  use function below to retrieve provinces information ======  initialize a dictionary to hold provinces information  This dictionary includes 'province_link' which means links to find the cities for each province  and the 'cities' which means city names and links  provinces_info = city_collection )  # Use this function to retrieve links to all the cities  This is called context management  with open can close the document automatically when the  write  change 'rb' -> 'wb'  pickle.dump provinces_info  cities_file)  # write  The structure is dict in dict  first layer keyword is province name  In each province you can find the cities  In each city  you can find the date  in the date  you can find weather record  Iterate over different provinces  Iterate cities within each provinces  Iterate over different months  Quiz: Try to convert the ""json""-like format to pandas DataFrame",525
DEDA_Class_2017_WebScrapingIntro,ERROR,ERROR,"Junjie Hu, Cathy YH Chen",DEDA_Class_2017_WebScrapingIntro,12, Simple class  The class Person is inherited from class object __init__ self  first  last  gender  age  self is the default argument that points to the instance  Using __init__ to initialize a class to take arguments  The class Student inherited from class Person __init__ self  first  last  gender  age  school  super ) method allows us to handle the arguments from parent class without copying  Child class can also be added new arguments describe self  describe is a method of class Student  stu_1 is an instance of class Student  Using the attributes in the object stu_1  Using the methods in the object stu_1,101
DEDA_Class_2017_WebScrapingIntro,ERROR,ERROR,"Junjie Hu, Cathy YH Chen",DEDA_Class_2017_WebScrapingIntro,12,"feedparser  There are some methods with name surrounded by double underscore called ""magic method""  Further more see: https://www.python-course.eu/python3_magic_methods.php __init__ self  url __eq__ self  other  __eq__ ) is a magic method that enables comparison two object with == __repr__ self  __repr__ ) is a magic method that enables customization default printed format get_titles self get_description self",55
DEDA_Class_2017_WebScrapingIntro,ERROR,ERROR,"Junjie Hu, Cathy YH Chen",DEDA_Class_2017_WebScrapingIntro,12,requests json pandas,3
DEDA_Class_2017_WebScrapingIntro,ERROR,ERROR,"Junjie Hu, Cathy YH Chen",DEDA_Class_2017_WebScrapingIntro,12,bs4 BeautifulSoup requests datetime os pandas pickle nasdaq_news_scraping page=1  refresh=False  Argument page equals 1 by default  Visit the home page if page equals 1  Change url by different argument  connect to the website if the webpage source code file is not exist of we need to refresh it  save the request object after scraping  else  we open the source code file from local disk to save time  direct = os.getcwd ) + '/DEDA_Class_2017_WebScrapingIntro'  Using the function defined previously with certain arguments as input,83
DEDA_Class_2017_WebScrapingIntro,ERROR,ERROR,"Junjie Hu, Cathy YH Chen",DEDA_Class_2017_WebScrapingIntro,12,requests bs4 BeautifulSoup datetime pandas os pickle  Using the requests module to get source code from the url  direct = os.getcwd ) + '/DEDA_Class_2017_WebScrapingIntro'  connect to the website if the webpage source code file is not exist of we need to refresh it  save the request object after scraping  else  we open the source code file from local disk to save time  Using BeautifulSoup to parse webpage source code  Finding all the <p> tag content  initial empty list to load the data  Using .find_all ) method to search tag name and attribute  Loop in the <a> tag  strip ) method can remove specific chars at the head and tail  Formatting date see further more: https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior  Using try...except to catch unexpected condition so that program can keep running  If the tag doesn't exist  mark it as an empty list,138
DEDA_Class_2017_WebScrapingIntro,ERROR,ERROR,"Junjie Hu, Cathy YH Chen",DEDA_Class_2017_WebScrapingIntro,12,requests xml,2
DEDA_Class_2017_WebScrapingIntro,ERROR,ERROR,"Junjie Hu, Cathy YH Chen",DEDA_Class_2017_WebScrapingIntro,12,os sys  add the module path to Python searching path ReadRSSClass  ReadRSSClass is the file name of the module code  Here we can print out the object's url in certain format  Here we use == to validate if two responses of two url are equal  Print out the titles  Print out the descriptions,53
DEDA_Class_2017_WebScrapingIntro,ERROR,ERROR,"Junjie Hu, Cathy YH Chen",DEDA_Class_2017_WebScrapingIntro,12,requests json pprint,3
DEDA_Class_2017_WebScrapingIntro,ERROR,ERROR,"Junjie Hu, Cathy YH Chen",DEDA_Class_2017_WebScrapingIntro,12," Import all the packages you need  always remember that you can find 99% packages you need in python requests  take the website source code back to you urllib  some useful functions to deal with website URLs bs4 BeautifulSoup  a package to parse website source code numpy  all the numerical calculation related methods re  regular expression package itertools  a package to do iteration works pickle  a package to save your file temporarily pandas  process structured data  the path you save your files  This link can represent the domain of a series of websites city_collection   get source code  parse source code  find the items with tag named 'dt'  iterate within all the items  get name of the province  get link to all the cities in the province  save dict in the dict  iterate with the province link to find all the cities  use the urllib package to join relative links in the proper way weather_collection link  print tr_item)  daily_weather = dict )  month_weather.append daily_weather)  Nice way to get a date string with certain format  '02d' means 2 digits   ==== Since I have already download the links to all the cities  you just need to execute from here:=====   ==== Otherwise  use function below to retrieve provinces information ======  initialize a dictionary to hold provinces information  This dictionary includes 'province_link' which means links to find the cities for each province  and the 'cities' which means city names and links  provinces_info = city_collection )  # Use this function to retrieve links to all the cities  This is called context management  with open can close the document automatically when the  write  change 'rb' -> 'wb'  pickle.dump provinces_info  cities_file)  # write  The structure is dict in dict  first layer keyword is province name  In each province you can find the cities  In each city  you can find the date  in the date  you can find weather record  Iterate over different provinces  Iterate cities within each provinces  Iterate over different months  Quiz: Try to convert the ""json""-like format to pandas DataFrame",333
DEDA_Class_2017_WebScrapingIntro,ERROR,ERROR,"Junjie Hu, Cathy YH Chen",DEDA_Class_2017_WebScrapingIntro,12,requests __init__ self login self  token search self  mode=None  **kwargs stream_symbol self  id  **kwargs,14
DEDA_Class_2017_WebScrapingIntro,ERROR,ERROR,"Junjie Hu, Cathy YH Chen",DEDA_Class_2017_WebScrapingIntro,12,feedparser  retrieve RSS feedback  list all titles  list all description,10
sinus_animation,Animated illustration of sine  curve.,ERROR,Jovanka Lili Matic,sinus_animation,1,numpy matplotlib plot ax  i,5
Distance_Measure,Distance measure for continuous variables using Euclidian/Mannheim/Maximum methods,"distance measure, Euclidian, Mannheim, Maximum, continuous variables",Susan,Distance_Measure,2,"!/usr/bin/env python  coding: utf-8  In[1]: pandas numpy sklearn.neighbors DistanceMetric  In[2]: ## load csv file  select specific regions: ME  NH  NY ## note that column 18-22 are information data only print ""Total records: ""  len df)) df.head 20) df_state = df.loc[df['Area'] == 'Maine'] print ""\n\nDataset for ME  NH  NY: "") df_state.head 3)  In[3]: ## transfer DataFrame to Numpy array  In[4]: ## Calculate diatance metrics for Euclidean distance  In[5]: ## Calculate diatance metrics for Manhattan distance  In[6]: ## Calculate diatance metrics for Maximum distance",82
Data_Collection,Retrieval of cryptocurrncy data via API call.,"cryptocurrency, data, retrieval, analysis, API call","Georg Velev, Iliyana Pekova",Data_Collection,1,Custom Imports requests array Source: https://blog.cryptocompare.com/cryptocompare-api-quick-start-guide-f7abbd20d260 This function establishes a request to the online plattform BitFinex: get_data_spec coin  date  time_period This function collects the cryptocurrency data from for the specified time period:  get_df_spec time_period  coin  from_date  to_date  Now we use the new function to query specific coins This function generates the average hourly prices from the opening and the closing price and uses the new feture to  compute the hourly returns: compute_Hourly_Returns data Set the start and the end date: Call the functions to retrieve the cryptocurrency data: Print overall information about the collected cryptocurrency data:,96
Group_6_Project_HMM_Bitcoin_model_prediction,Bitcoin price prediction using the constructed model,"HMM, Bitcoin, prediction, maximum likelihood, optimization","Changjie Jin, Ng Zhi Wei Aaron, You Pengxin, Chio Qi Jun, Zhang Yuxi",Group_6_Project_HMM_Bitcoin_model_prediction,1, -*- coding: utf-8 -*- os warnings logging itertools pandas numpy matplotlib hmmlearn.hmm GaussianHMM sklearn.model_selection train_test_split tqdm tqdm  Change plot style to ggplot  for better and more aesthetic visualisation) Read Bitcoin data  Compute the fraction change in close  high and low prices  which would be used a feature Set constants for model Construct logging  Set up the model Split the data for training and testing Report the log Prepare model input Report the log Fit the model Output the states  Compute all possible outcomes predict close prices for days get most probable outcome Predict close price Plot the predicted prices,99
opp_comp_abalone_analysis,The performances of various Ordered Pairwise Partitioning algorithm variants are compared to the performances of nominal classification algorithms and to the performance of the Ordinal Logistic Regression Model (Proportional Odds Model). For the analysis the models are applied to predict the number of abalone rings as a proxy of the age of the shellfish.,"Ordinal Logistic Regression, Proportional Odds Model, Machine Learning, Linear Discriminant Analysis, Neural Network, Support Vector Machine, Random Forest, Gradient Boosting",Steffen Thesdorf,opp_comp_abalone,3,!/usr/bin/env python3  -*- coding: utf-8 -*- numpy sklearn metrics ###############################################################################################   AMAE y_predict  y_test_select ############################################################################################### MAE y_predict  y_test_select ###############################################################################################   MSE y_predict  y_test_select ############################################################################################### performance_summary y_predict  y_test_select  conf=False  conf_label='',27
opp_comp_abalone_analysis,The performances of various Ordered Pairwise Partitioning algorithm variants are compared to the performances of nominal classification algorithms and to the performance of the Ordinal Logistic Regression Model (Proportional Odds Model). For the analysis the models are applied to predict the number of abalone rings as a proxy of the age of the shellfish.,"Ordinal Logistic Regression, Proportional Odds Model, Machine Learning, Linear Discriminant Analysis, Neural Network, Support Vector Machine, Random Forest, Gradient Boosting",Steffen Thesdorf,opp_comp_abalone,3,!/usr/bin/env python3  -*- coding: utf-8 -*- os numpy pandas sklearn.model_selection train_test_split sklearn.discriminant_analysis LinearDiscriminantAnalysis sklearn.svm SVC sklearn.neural_network MLPClassifier sklearn.ensemble GradientBoostingClassifier mord LogisticAT opp opp_onevsnext_predict perf performance_summary %% ## Data Preparation ###  Convert from Categorial to Numeric Input    Separate Labels from Input %%  Split Data  Bin Tail Output Values and Shift %%     ## Model Estimation ### %% %% %% ## Performance Evaluation ###,61
opp_comp_abalone_analysis,The performances of various Ordered Pairwise Partitioning algorithm variants are compared to the performances of nominal classification algorithms and to the performance of the Ordinal Logistic Regression Model (Proportional Odds Model). For the analysis the models are applied to predict the number of abalone rings as a proxy of the age of the shellfish.,"Ordinal Logistic Regression, Proportional Odds Model, Machine Learning, Linear Discriminant Analysis, Neural Network, Support Vector Machine, Random Forest, Gradient Boosting",Steffen Thesdorf,opp_comp_abalone,3, -*- coding: utf-8 -*- numpy opp_onevsfollowers_predict X_train  y_train  bin_class  X_test  forward=True #######################################################################################################     opp_onevsnext_predict X_train  y_train  bin_class  X_test  forward=True ####################################################################################################### orderedpartitions_predict X_train_select  y_train_select  bin_class  X_test_select  forward=True,25
pyTSA_AntiDiabetSales,"This Quantlet build and plots time series, log  and their rolling std of the of the anti-diabetic drug sales in Australia, as well as seasonally differenced series for the period from July 1991 to June 2008.","time series,  stationarity,  seasonality, stochastic process,  moving average, plot ",ERROR,pyTSA_AntiDiabetSales,1,pandas numpy pandas.plotting register_matplotlib_converters matplotlib statsmodels.tsa.stattools kpss  explicitly register matplotlib converters. plt.savefig 'pyTSA_AntiDiabetSales_fig3-7.png'  dpi = 1200   bbox_inches='tight'  transparent = True); plt.savefig 'pyTSA_AntiDiabetSales_fig3-8.png'  dpi = 1200   bbox_inches='tight'  transparent = True);,29
GMMcluster,Clustering CRIX Currency Index and its volatility VCRIX with Gaussian Mixture Model (GMM),"Clustering, Gaussian Mixture Model, Currency Index, CRIX, VCRIX",Rui Ren,GMMcluster,2,pandas matplotlib  read data  change it to df.date  it doesn't work,11
GMMcluster,Clustering CRIX Currency Index and its volatility VCRIX with Gaussian Mixture Model (GMM),"Clustering, Gaussian Mixture Model, Currency Index, CRIX, VCRIX",Rui Ren,GMMcluster,2,matplotlib seaborn matplotlib seaborn numpy sklearn.mixture GaussianMixture pandas plt.scatter cp  vol  c=labels  s=40  cmap='viridis') plt.savefig 'Clustering3.png'  transparent=True),17
LOBDeepPP_MSE_computation_T,"Joint Limit order book ask and bid price predition for multiple predicton horizons with a neural networks. This script computes the MSE loss for the trained model for the fixed order book level application for training, validation and test dataset.","Limit order book, finance, forecasting, prediction, MSE, evaluation",Marius Sterling,LOBDeepPP_MSE_computation_T,1,!/usr/bin/env python3  -*- coding: utf-8 -*- os json_tricks numpy sys LOB.LOB_keras_train_class LOB_keras_train_class  %%  %%  %%,15
UMAP,"Clustering results displayed in the reduced space by applying different types of dimension reduction methods, including PCA and tSNE.","K expectiles clustering, PCA, tSNE",Bingling Wang,UMAP,3,"!/usr/bin/env python  coding: utf-8  In[2]: matplotlib pandas sklearn.cluster KMeans numpy copy from sklearn.preprocessing import StandardScale mpl_toolkits.mplot3d Axes3D plotly  In[7]:  In[16]:  In[12]: plotly fig.write_html ""Coin-Gecko_PCA.html""  include_plotlyjs=""cdn"") plotly  In[13]: fig.write_html ""Coin-Gecko_t-SNE.html""  include_plotlyjs=""cdn"")",30
Regulatory_Complexity_Scraper,Scrapes the webpage http://www.buzer.de that contains all changes to the German Banking Act since 2006.,"Regulatory_Complexity, web, scraper, crawler, proxy, BeautifulSoup, urllib2",Sabine Bertram,Regulatory_Complexity_Scraper,1,!/usr/bin/python  -*- coding: utf-8 -*- ###############################################################################  HTM Scraper ############################################################################### os urllib2 bs4 BeautifulSoup pickle random  open proxy list  create directory for pages  scrape pages  read main page  convert it to a BeautifulSoup object  retrieve all of the anchor tags and save relevant pages  check for expired proxies and delete them,50
SFM_Rough_Rice_Futures_Price,(S)ARIMA Model for Rough Rice Futures Price from 2011 to 2021,"Time series, ARIMA, stationarity, first difference, Augmented Dickey-Fuller test, PACF, ACF, Price Forecasting",Xingjia Wang,SFM_Rough_Rice_Futures_Price,2,!/usr/bin/env python  coding: utf-8  # **Time Series analysis - Rice Price**  ![Time Series.PNG] data:image/png;base64 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)  In[5]: pandas numpy matplotlib seaborn datetime statsmodels.tsa.seasonal seasonal_decompose statsmodels.tsa.stattools adfuller statsmodels statsmodels.tsa.arima_model ARIMA statsmodels.graphics.tsaplots plot_acf import warnings warnings.filterwarnings 'ignore')  **Load dataset**  In[6]: load the data Dataset downloaded from https://www.investing.com/commodities/rough-rice-historical-data  In[7]:  In[8]:  **First insight into data**  In[9]: have a first insight into the development of Rice Price  In[10]:  In[11]:  In[12]: decomposes the time series into trend  seasonal and irregular components  **Checks for Stationarity - Augmented Dickey-Fuller test**  In[13]: check whether the original time series is stationary  In[14]:  In[15]: The time series is stationary at 10% sig. level  not at 1% and 5%.  In[16]:  Original Series  In[17]: rice_copy.diff ).plot )  In[18]:  to get stationary time series  try to take a first difference of the time series  In[19]:  In[20]: check whether the first difference of original time series is stationary  In[21]: The first difference of original time series is stationary  In[22]:  1st Differencing  In[23]: The first difference of original time series seems to be also autocorrelated  not white noise.   **Build Models**  In[24]: build ARIMA model from the plot above  a good guess for the parameter of  p d q) would be  1 1 1).  In[25]: But still try some other parameters of  p d q) for the ARIMA model.  In[26]:  In[28]:  In[29]:  Plot ACF and PACF charts for the seasonal first difference values  find the optimal parameters for SARIMA model.  In[39]: the autocorrelation of the differenced series is negative at lag 12  one year later).  In[44]: try different parameters for SARIMA model  In[ ]: based on AIC  SARIMA  1 1 1)x 1 0 1 12) will be choosed.    **Model diagnostics**  In[43]: to ensure that the residuals of the model is uncorrelated and normally distributed with zero-mean  In[33]: The qq-plot on the bottom left shows that the ordered distribution of residuals  blue dots) follows quite well the linear trend of the samples taken from a standard normal distribution with N 0  1). The residuals over time  top left plot) don’t display any obvious seasonality and appear to be white noise.  **Make Prediction**  In[34]: Forecast  In[35]:  In[36]:  accuracy of forecasts  compute the mean square error mse =   y_forecasted - y_truth) ** 2).mean ),363
Cryptocurrencies_and_Stablecoins,"We analyze cryptoasset markets (cryptocurrencies and stablecoins) at high frequency. We investigate intraday patterns. We show that Tether plays a crucial role as a safe haven and/or store of value facilitating trading in cryptocurrencies without going through traditional currencies. Markets centered on cryptocurrencies and stablecoins play a primary role aggregating preference/technology shocks and heterogeneous opinions, instead markets centered on the US Dollar play a marginal role on price formation.","stablecoin, cryptocurrency, intraday, time series, high frequency","Emilio Barucci, Giancarlo Giuffra Moncayo, Daniele Marazzina",Cryptocurrencies-and-Stablecoins-a-high-frequency-analysis,10,"argparse pyspark.sql SparkSession main input_dir  output_file  flag_monthly  flag_compressed read_df input_dir  pattern  spark  suffix=""csv"" write_to_csv df  output_file",16
Cryptocurrencies_and_Stablecoins,"We analyze cryptoasset markets (cryptocurrencies and stablecoins) at high frequency. We investigate intraday patterns. We show that Tether plays a crucial role as a safe haven and/or store of value facilitating trading in cryptocurrencies without going through traditional currencies. Markets centered on cryptocurrencies and stablecoins play a primary role aggregating preference/technology shocks and heterogeneous opinions, instead markets centered on the US Dollar play a marginal role on price formation.","stablecoin, cryptocurrency, intraday, time series, high frequency","Emilio Barucci, Giancarlo Giuffra Moncayo, Daniele Marazzina",Cryptocurrencies-and-Stablecoins-a-high-frequency-analysis,10,os sys matplotlib pandas numpy pretty measure replace_outliers_with_mean values generate_single_db_plot pair  hours  values  measure  x_ticks_frequency  sampling_frequency generate_daily_breakdown_plots df  pair  x_ticks_frequency  sampling_frequency=1 main file5m  file60m  file1m pairs = sorted df_1m['symbol'].unique )) stablecoin_usd_pairs  main_5_pairs ,32
Cryptocurrencies_and_Stablecoins,"We analyze cryptoasset markets (cryptocurrencies and stablecoins) at high frequency. We investigate intraday patterns. We show that Tether plays a crucial role as a safe haven and/or store of value facilitating trading in cryptocurrencies without going through traditional currencies. Markets centered on cryptocurrencies and stablecoins play a primary role aggregating preference/technology shocks and heterogeneous opinions, instead markets centered on the US Dollar play a marginal role on price formation.","stablecoin, cryptocurrency, intraday, time series, high frequency","Emilio Barucci, Giancarlo Giuffra Moncayo, Daniele Marazzina",Cryptocurrencies-and-Stablecoins-a-high-frequency-analysis,10,string sys pandas numpy argparse plot_daily_breakdown pretty build_monthly_row df  pair  months  column  multiplier=1. is_pair_and_month df  month  pair build_table df  pairs  months  column  tex_file  number_format  multiplier=1. make_df column  df  months  multiplier  pairs sum_monthly_values df  pair  months average_monthly_values df  df_weights  pair  months combine_arbitrage_tables df_profits  df_arbitrage_opportunities  df_arbitrage_spread  tex_file single_pair_tables df  months  pairs triangulation_tables df  months  pairs arbitrage_tables file  type_arbitrage build_stats_table df  quantity stats_table file parity_table file correlation_table file5  file60  threshold  significance_threshold pairs = sorted main_pairs )) build_correlation_matrix df  measure  pairs  threshold get_correlation_row df  main_entity get_significance_row df  main_entity inner_get_correlation_row df  main_entity single_pair_correlation_table file5  file60  significance_threshold pairs = sorted df_5m['symbol'].unique )) lower_triangle_to_value df_correlation  value desired_pair_order df main_pairs ,102
Cryptocurrencies_and_Stablecoins,"We analyze cryptoasset markets (cryptocurrencies and stablecoins) at high frequency. We investigate intraday patterns. We show that Tether plays a crucial role as a safe haven and/or store of value facilitating trading in cryptocurrencies without going through traditional currencies. Markets centered on cryptocurrencies and stablecoins play a primary role aggregating preference/technology shocks and heterogeneous opinions, instead markets centered on the US Dollar play a marginal role on price formation.","stablecoin, cryptocurrency, intraday, time series, high frequency","Emilio Barucci, Giancarlo Giuffra Moncayo, Daniele Marazzina",Cryptocurrencies-and-Stablecoins-a-high-frequency-analysis,10,"argparse datetime datetime numpy pandas statsmodels scipy.stats pearsonr filter_time_series df  rule_freq  leader  other  sampling to populate missing time points  1Min) or aggregate if larger than 1Min normalize df  columns linear_regression_analysis df_y  df_x calculate_correlations df_y  df_x calculate_coefficients df_original  lags  leader  other coefficients = linear_regression_analysis df[f'{other}_t']  df[ordered_regressor_columns]) lag_suffix shift_from_t prepare_regression_df df_original  lags  leader  other  removing points having missing values  rename columns combine_series dfl  dfo  modify leader from ethbtc to btceth main file  files df_table.to_latex f'results/tables/lead-lag-{leader}.tex'  index=False  na_rep=""-""  float_format='%.4f') transform_lagged_columns df_table  lag_columns  transformation",80
Cryptocurrencies_and_Stablecoins,"We analyze cryptoasset markets (cryptocurrencies and stablecoins) at high frequency. We investigate intraday patterns. We show that Tether plays a crucial role as a safe haven and/or store of value facilitating trading in cryptocurrencies without going through traditional currencies. Markets centered on cryptocurrencies and stablecoins play a primary role aggregating preference/technology shocks and heterogeneous opinions, instead markets centered on the US Dollar play a marginal role on price formation.","stablecoin, cryptocurrency, intraday, time series, high frequency","Emilio Barucci, Giancarlo Giuffra Moncayo, Daniele Marazzina",Cryptocurrencies-and-Stablecoins-a-high-frequency-analysis,10,datetime json sys boto3 matplotlib matplotlib pyspark pyspark.sql SparkSession pyspark.sql.functions udf pyspark.sql.types DateType generate_plot file  xs  ys  title  pairs  double_size=True  double default of  6.4  4.8) sample x  y  number_samples day_of date function_for aggregation main data  s3_bucket  xaxis  yaxis  filename  title  samples  aggregation  collect as pandas df in master node  aggregate  optional) and sample  create png file  upload to s3  x_axis is assumed to be a date,66
Cryptocurrencies_and_Stablecoins,"We analyze cryptoasset markets (cryptocurrencies and stablecoins) at high frequency. We investigate intraday patterns. We show that Tether plays a crucial role as a safe haven and/or store of value facilitating trading in cryptocurrencies without going through traditional currencies. Markets centered on cryptocurrencies and stablecoins play a primary role aggregating preference/technology shocks and heterogeneous opinions, instead markets centered on the US Dollar play a marginal role on price formation.","stablecoin, cryptocurrency, intraday, time series, high frequency","Emilio Barucci, Giancarlo Giuffra Moncayo, Daniele Marazzina",Cryptocurrencies-and-Stablecoins-a-high-frequency-analysis,10,"!/usr/bin/env python  coding: utf-8  # intraday-project  Repository for ""Cryptocurrencies and Stablecoins: a high frequency analysis""  E. Barucci  G. Giuffra  D. Marazzina    ## Requirements  - https://git-lfs.github.com/: since some of the files in the data directory are larger than the 100 MB natively handled by github.  - https://www.scala-sbt.org/: to compile the Scala project.  - https://spark.apache.org/: given the amount of data it was necessary to use this framework for big data analytics.  - In the `python` directory  the file `requirements.txt` contains the dependencies to run the different python scripts.    ## Structure  - `data`: it contains all data and is divided in the following subdirectories:    -  `provider-data`: it contains a small sample of the proprietary data leased from the data provider. We cannot include the whole sample since it will be a breach of contract. In addition  the total amount of data is around 5 TB.    - `processed-data`: Scala and some python scripts write the output of the spark jobs in this directory. Scripts also read from this directory as well since the processing of the data is composed of various steps.    - `clean-time-series`: Scala scripts write to this directory the cleaned time series that is then used by some python scripts for time series analysis.  - `emr`: it contains the script to launch the Spark scripts in Amazon EMR  https://aws.amazon.com/emr/)  as well as configuration files for the cluster and the jobs to be performed by the cluster.  - `python`: it contains all python scripts. Some are Spark scripts and others are scripts to generate tables  figures  and perform time series analysis.  - `scripts`: it contains the bash scripts to run the Spark jobs.  - `src`: it contains all Scala classes. It follows the structrue required by the `sbt` tool.    ## Output  - Tables 2-7 are obtained with Scala  `src` folder)  - Tables 8-13 are obtained with Python  `python` folder)  Due to the fact that the folder `provider-data` contains a small sample of the proprietary data leased from the data provider  it is not possible to obtain the same values of the tables in the article.  In[ ]:",344
Cryptocurrencies_and_Stablecoins,"We analyze cryptoasset markets (cryptocurrencies and stablecoins) at high frequency. We investigate intraday patterns. We show that Tether plays a crucial role as a safe haven and/or store of value facilitating trading in cryptocurrencies without going through traditional currencies. Markets centered on cryptocurrencies and stablecoins play a primary role aggregating preference/technology shocks and heterogeneous opinions, instead markets centered on the US Dollar play a marginal role on price formation.","stablecoin, cryptocurrency, intraday, time series, high frequency","Emilio Barucci, Giancarlo Giuffra Moncayo, Daniele Marazzina",Cryptocurrencies-and-Stablecoins-a-high-frequency-analysis,10,json sys datetime datetime itertools combinations pyspark.sql SparkSession pyspark.sql.functions col pyspark.sql.types StructType scipy.stats pearsonr time_interval time  interval  # calculates for day  ignores interval  initialSecond = datetime date.year  date.month  date.day  0  0  0  0  tzinfo=timezone.utc) get_df df  interval_minutes main data  interval_minutes  s3_bucket,41
Cryptocurrencies_and_Stablecoins,"We analyze cryptoasset markets (cryptocurrencies and stablecoins) at high frequency. We investigate intraday patterns. We show that Tether plays a crucial role as a safe haven and/or store of value facilitating trading in cryptocurrencies without going through traditional currencies. Markets centered on cryptocurrencies and stablecoins play a primary role aggregating preference/technology shocks and heterogeneous opinions, instead markets centered on the US Dollar play a marginal role on price formation.","stablecoin, cryptocurrency, intraday, time series, high frequency","Emilio Barucci, Giancarlo Giuffra Moncayo, Daniele Marazzina",Cryptocurrencies-and-Stablecoins-a-high-frequency-analysis,10,json sys datetime datetime itertools combinations pyspark.sql SparkSession pyspark.sql.functions col pyspark.sql.types StructType scipy.stats pearsonr time_interval time  interval get_df df  symbol  interval_minutes main data  interval_minutes  s3_bucket,25
Cryptocurrencies_and_Stablecoins,"We analyze cryptoasset markets (cryptocurrencies and stablecoins) at high frequency. We investigate intraday patterns. We show that Tether plays a crucial role as a safe haven and/or store of value facilitating trading in cryptocurrencies without going through traditional currencies. Markets centered on cryptocurrencies and stablecoins play a primary role aggregating preference/technology shocks and heterogeneous opinions, instead markets centered on the US Dollar play a marginal role on price formation.","stablecoin, cryptocurrency, intraday, time series, high frequency","Emilio Barucci, Giancarlo Giuffra Moncayo, Daniele Marazzina",Cryptocurrencies-and-Stablecoins-a-high-frequency-analysis,10,json shutil sys os tarfile datetime pandas boto3 build_filter_months from_month  to_month read_order_books_tars file_name  s3 main pair  from_month  to_month  read file with all tar file names  filter according to pair  from_month and to_month  loop through the tars  read tar from s3 bucket  extract locally  upload extracted files to s3 bucket  delete file after uploading  close and remove after uploading all files in tar,62
Regulatory_Complexity_Sentence_Vectors,Computes sentence embeddings according to Le et al. (2014) doc2vec using gensim,"Regulatory_Complexity, doc2vec, gensim, sentence-vectors, embeddings",Sabine Bertram,Regulatory_Complexity_Sentence_Vectors,1,!/usr/bin/python  -*- coding: utf-8 -*- ###############################################################################  Learning of sentence embeddings  input: modelData: a list of sentences  split into words  output: sentenceModel: a gensim model containing the embeddings and additional info ############################################################################### os pickle gensim ###############################################################################  import model training data  convert into labeled sentences doc2vec training ##################  screen output  parameters  use distributed memory model  sent vector dimensionality  Minimum word count  Number of threads to run in parallel  Context window size  Downsample setting for frequent words  no hierarchical sampling  negative sample  number of epochs in NN  training,85
Regulatory_Complexity_Word_Vectors,Computes word embeddings according to Mikolov et al. (2013) word2vec using gensim,"Regulatory_Complexity, word2vec, gensim, word-vectors, embeddings",Sabine Bertram,Regulatory_Complexity_Word_Vectors,1,!/usr/bin/python  -*- coding: utf-8 -*- ###############################################################################  Learning of word embeddings  input: modelData: a list of sentences  split into words  output: wordModel: a gensim model containing the embeddings and additional info ############################################################################### os pickle gensim ###############################################################################  import model training data  screen output  parameters  use skip gram model  Word vector dimensionality  Minimum word count  Number of threads to run in parallel  Context window size  Downsample setting for frequent words  negative sample  number of epochs in NN  training,75
RL_DrawdownFigures,Outputs the drawdown for each time period as a plot.,"reinforcement learning, neural network, machine learning, portfolio management, cryptocurrency",Ilyas Agakishiev,RL_DrawdownFigures,1,pandas datetime datetime matplotlib matplotlib.dates DateFormatter,6
USUnempGDP,"This Quantlet plots monthly time series of returns of Procter and Gamble from 1961 to 2016 and  their ACF and PACF (Example, 2.4 Figures 2.8-2.9 in the book)","time series, autocorrelation, returns, ACF, PACF, plot, visualisation","Huang Changquan, Alla Petukhina",pyTSA_MultiDimGN,1,numpy matplotlib PythonTsa.plot_multi_ACF multi_ACFfig PythonTsa.MultiCorrPvalue MultiTrCorrPvalue PythonTsa.plot_multi_Q_pvalue MultiQpvalue_plot,8
TXTMcDbl,"Counts positive and negative words using the lexicon by Liu et al. Please install required Python packages before usage: os, io, collections, nltk.","text mining, data mining, counts, sentiment",Elisabeth Bommes,TXTMcDbl,1,"os io nltk.tokenize word_tokenize  Set working directory ## Functions rempct s $%&\ )*+ ./:;<=>?@[\\]^_`{|}~€$' + '0123456789' + ""'"" cleaner txt wordcount words  dct collections Counter negwordcount words  dct  negdct  lngram collections Counter ngrams  Read text file  Additional input ## Read in LM lexicon  Negative  Positive ## Count positive and negative words  Tokenize -> Words  Number of words in article  Count words in lexicon  Count negated words in lexicon  Total number of positive/negative words  Print results",75
CRIX_Rebalancing_Analysis,An Analysis of CRX daily log returns - are there higher returns at lower risk on index rebalancing days.,"CRIX, non-parametric tests, daily logreturns, normality, t-test",Patrick Plum,SL_20190606_CRIXRebalancing,6, -*- coding: utf-8 -*- ############################## Import Modules ############################################# matplotlib matplotlib __main__ dates_of_ind_rebalance __main__ dates_of_first __main__ crix_constituents #################### Amount of currencies at a certain time preiod ######################## amount of currencies in each period plotting number of constituents over time 666666'  linestyle='-' alpha=0.7) ###########################################################################################,42
CRIX_Rebalancing_Analysis,An Analysis of CRX daily log returns - are there higher returns at lower risk on index rebalancing days.,"CRIX, non-parametric tests, daily logreturns, normality, t-test",Patrick Plum,SL_20190606_CRIXRebalancing,6, -*- coding: utf-8 -*- ############################## Import Modules ############################################# numpy matplotlib wordcloud WordCloud csv PIL Image ###### Wordcloud for relative frequency of appearance of a cryptocurrency in CRIX #######  make bag of words of CRIX members  reader object is created   contents of reader object is stored .   data is stored in list of list format.   initialize text   iterating through list of rows  iterating through words in the row   concatenate the words  lowercase all words to exclude redundancies Image as mask for the wordcloud generating the wordcloud  plot WordCloud image   ########################################################################################,89
CRIX_Rebalancing_Analysis,An Analysis of CRX daily log returns - are there higher returns at lower risk on index rebalancing days.,"CRIX, non-parametric tests, daily logreturns, normality, t-test",Patrick Plum,SL_20190606_CRIXRebalancing,6, -*- coding: utf-8 -*- ############################ Import Modules and data ####################################### numpy matplotlib matplotlib __main__ dates_of_first __main__ dates_of_others __main__ dates_of_ind_rebalance __main__ logreturn_at_first __main__ logreturn_others __main__ logreturn_at_rebalance ######## statistical inference on influence of index rebalancing ##########################  plotting 666666'  linestyle='-' alpha=0.7)  ...dates of others  ...dates of index amount rebalancing = 1st of each 3rd month  ...dates of usual index change = 1st of a month ############# Summary of mean and variances of daily log returns ######################### mean variance,75
CRIX_Rebalancing_Analysis,An Analysis of CRX daily log returns - are there higher returns at lower risk on index rebalancing days.,"CRIX, non-parametric tests, daily logreturns, normality, t-test",Patrick Plum,SL_20190606_CRIXRebalancing,6, -*- coding: utf-8 -*- ############################## Import Modules ########################################### pandas numpy json ############################## Read in data #############################################  read crix_constituents file  all constituents  read all CRIX data ##############################  Pre-process for further use ###############################  dates of index shift  =first of each month) converting to a more convenient data type  convert dates to dateformat  calculation of logreturns and adding to the dataframe  rebalancing of amount of constituents every 3 months  last month out of rule  see Nr_of_Cryptos.png)  get indices of monthly turns ...and of the rebalancing months  go for the logreturns of these  get other indices ... and get the logreturns of all other dates ###########################################################################################,102
CRIX_Rebalancing_Analysis,An Analysis of CRX daily log returns - are there higher returns at lower risk on index rebalancing days.,"CRIX, non-parametric tests, daily logreturns, normality, t-test",Patrick Plum,SL_20190606_CRIXRebalancing,6, -*- coding: utf-8 -*- ############################ Import Modules and data ####################################### numpy matplotlib scipy statsmodels statsmodels __main__ logreturn_at_first __main__ logreturn_others __main__ logreturn_at_rebalance ########################for normal data  paramteric tests ###################################  F-Test teststatistic  F-distributed  =comp_var) degrees of freedom1 degrees of freedom2  reject  if small -> clearly reject  t-test for checking  if means are significantly different - assumption: normality  variance equal or not according to previus result ################################### Testing for normality #################################  QQPLots 666666'  linestyle='-')  Shapiro Wilk's test  If p-value  = 2nd entry is small  reject normality) #################################for non-normal: non-paramteric test ###################### only account for values within 2 standard deviations ->95% center ->cut 2.5% on each side  Mann-Whitney-U Test for hypothesis test if distributions are similar ###########################################################################################,112
CRIX_Rebalancing_Analysis,An Analysis of CRX daily log returns - are there higher returns at lower risk on index rebalancing days.,"CRIX, non-parametric tests, daily logreturns, normality, t-test",Patrick Plum,SL_20190606_CRIXRebalancing,6, -*- coding: utf-8 -*- ############################## Import other scripts and variables ########################################## pandas ReadData ReadData crix ReadData dates_of_first ReadData dates_of_others ReadData dates_of_ind_rebalance ReadData logreturn_at_first ReadData logreturn_others ReadData logreturn_at_rebalance ReadData crix_constituents WordCloudCRIX PlotNumberofConst DailyLogReturns DailyLogReturns mean_logreturn_at_first DailyLogReturns mean_logreturn_at_rebalance DailyLogReturns mean_logreturn_others DailyLogReturns var_logreturn_at_first DailyLogReturns var_logreturn_at_rebalance DailyLogReturns var_logreturn_others Inference Inference ttest Inference Ftest Inference SWtest_first Inference SWtest_others Inference BFtest Inference MWUtest ####################################### Summary ############################################################ daily returns of first of the month have about 2.5*expected value and roughly 40% of the variance #############################################################################################################,78
SFETimegarch,"Plots the time series of a GARCH(1,1) process.","simulation, stochastic-process, process, time-series, plot, graphical representation, estimation, garch, autoregressive, process, stochastic, stochastic-process, volatility",Joanna Tomanek,QID-1799-SFEtimegarch,1,numpy matplotlib omega = 0.1  alpha = 0.15  beta = 0.8  number of observations  drop first observations   GARCH  1 1) coefficients alpha0 and alpha1 iterate over the oberservations  drop n1 observations ,31
elephant_wiggle_trunk,Use complex numbers as parameters to fit an elephant that wiggle its trunk. Creates an MP4 file of the wiggling elephant,ERROR,"Junjie Hu, WKH 20200924",FittingElephant,1,matplotlib matplotlib animation numpy append matplotlib  elephant parameters  last one is the eye fourier t  C elephant t  p init_plot   draw the body of the elephant  create trunk move_trunk i  move trunk to new position  but don't move eye stored at end or array)  initial the elephant body  initialize trunk,50
Cryptopunks_collecting_and_clustering,Collecting all the cryptopunks by webscraping and cluster them,"cryptopunks, cluster",Chen Zebo,Crtptopunks_collecting_and_clustering,6,!/usr/bin/env python  coding: utf-8  In[1]:  install cv2 package if you don't have this package cv2 os random numpy  In[2]:  make sure you have all the cryptopunks in your Crypto_punks folder. Otherwise you shoulod run Collecting_cryptopunks first  In[3]:  In[10]:  read the first images  set the fps for the speed of changing images  get the length and height of the image info  In[11]:  here random choose 1000 image index for creating the film  In[12]:  根据图片的大小，创建写入对象 （文件名，支持的编码器，5帧，视频大小（图片大小））  videoWrite = cv2.VideoWriter '0.mp4' fourcc fps  1920 1080))  In[13]:  循环读取图片 假设以数字顺序命名 print fileName)  In[ ]:,89
Cryptopunks_collecting_and_clustering,Collecting all the cryptopunks by webscraping and cluster them,"cryptopunks, cluster",Chen Zebo,Crtptopunks_collecting_and_clustering,6,"!/usr/bin/env python  coding: utf-8  In[1]:  install PCV if your didn't use it before the download address is: https://github.com/jesolem/PCV.   Download it and unzip it.Then in cmd console  you cd to the directory where you unzip and use the command: python setup.py install. os PIL PCV.clustering hcluster matplotlib.pyplot numpy requests bs4 BeautifulSoup pandas  In[2]:  create a list of images  You should have all 10 000 images in crpto_punks folder under your work directory  otherwise you should run Collecting_cryptopunks first   In[3]:  Here you can change the number of images you want to cluster  if you use all the 10000  it may be very time-consuming  In[4]:  extract feature vector  8 bins per color channel)  multi-dimensional histogram  In[5]:  clusters with some  arbitrary) threshold  see how many clusters by this condition  In[6]:  visualize clusters with some  arbitrary) threshold print elements[p]) print ""-------------------"") hcluster.draw_dendrogram tree imlist filename='cryptopunks_cluster.pdf')  In[7]: re time  In[11]:  note that it will scrape all the pictures and save them in your sys_path under ""cryptopunks"" folder  it will take a long long time to download all of the 10000 pictures  too small time gap will make you IP banned by Larvalabs  however long time gap make you a lot of time to scrape  In[40]: if p != 'NaN': print ""The average price of this cluster is {:0.2f}"".format mean price)))",214
Cryptopunks_collecting_and_clustering,Collecting all the cryptopunks by webscraping and cluster them,"cryptopunks, cluster",Chen Zebo,Crtptopunks_collecting_and_clustering,6,"!/usr/bin/env python  coding: utf-8  In[1]: requests bs4 BeautifulSoup pandas PIL Image io BytesIO os  In[2]: creat_dir_not_exist path  In[3]:  set headers  In[4]:  note that it will scrape all the pictures and save them in your sys_path under ""cryptopunks"" folder  it will take a long long time to download all of the 10000 pictures print url)  In[5]:  define a function to get any picture you want if you know the index getimage i print url)  In[6]:  In[ ]:",76
RVOLrnn,Forecasts the realized volatility of a portfolio from cryptocurrencies through of RNNs.,"cryptocurrency, RNN, LSTM, GRU, neural, network",Ivan Mitkov,RVOLrnn,2,!/usr/bin/env python3  -*- coding: utf-8 -*-  Import the required moduldes  keras.models Sequential keras.layers Dense keras.wrappers.scikit_learn KerasClassifier sklearn.model_selection GridSearchCV numpy numpy.random seed os train_test_data dataframe  fraction  RNN pandas numpy numpy sqrt  Split the targets into training/testing sets numpy math sklearn metrics keras optimizers pandas numpy keras.layers SimpleRNN keras.layers Dense keras.models Sequential keras optimizers math sklearn.metrics mean_squared_error numpy.random seed __init__ self  NN_type  nn_inputs  train_X  train_Y  test_X  test_Y create_nn self  n_layers  n_units  n_epochs  batch_size  activ_funct  learning_rate keras optimizers K.clear_session ) keras.layers SimpleRNN keras.layers Dense keras.layers LSTM keras.layers GRU keras.layers.advanced_activations LeakyReLU math prediction self pandas evaluation self math sqrt sklearn.metrics mean_squared_error error_df self pandas math sqrt sklearn.metrics mean_squared_error visualization self  test_visualization = False matplotlib pandas  Actual analysis                         warnings  Please note that the code works without any errors only if you run the whole script at once os  If you run partial codes then you have to replace manually the working directory with your path  os.chdir r'/hier/comes/your/path/RVOLrnn')  Import of packages dalib.packages  In order to keep constant outputs  avoiding different weights initialization numpy.random seed  Slicing the data frame  long version  short version          long version  short version                  Choose which scenario  Scenario high volatiolity time  longer training  Scenario high volatiolity time  short training  Scenario low volatiolity time  longer training  Scenario low volatiolity time  shor training  Import data  Train-Test-Data  Simple Recurrent Neural Network  Train  Predictions  Evaluation  Save loss errors  Visualization  LSTM Recurrent Neural Network   Train  Predictions  Evaluation  Save loss errors  Visualization  3. Train the GRU Recurrent Neural Network with the optimal parameters  Train  Predictions  Evaluation  Save loss errors  Visualization  Save the predictions      Save the errors,256
RVOLrnn,Forecasts the realized volatility of a portfolio from cryptocurrencies through of RNNs.,"cryptocurrency, RNN, LSTM, GRU, neural, network",Ivan Mitkov,RVOLrnn,2,!/usr/bin/env python3  -*- coding: utf-8 -*- warnings  Make sure that all of the modules are already installed in Anaconda. If not    the following comands should be ran in Terminal  Improting the packages pickle pandas pandas Series matplotlib numpy statsmodels.graphics.tsaplots plot_acf statsmodels statsmodels.tsa.stattools adfuller statsmodels.tsa.stattools adfuller statsmodels.stats.diagnostic itertools sys pylab scipy scipy.stats kurtosis scipy.stats skew scipy.stats norm scipy.stats jarque_bera numpy.random normal math sklearn.preprocessing MinMaxScaler sklearn.metrics mean_squared_error keras.layers keras optimizers keras.constraints NonNeg keras.layers.advanced_activations,71
DCA_comparison_cluster_sizes,Plotting cluster sizes for k-means,"k-means, visualization, clustering, plot, quantlet",Luisa Krawczyk,Cluster_sizes,1, -*- coding: utf-8 -*- sklearn.cluster KMeans matplotlib  choose the number of clusters you want   using k-means++ by default  Getting cluster sizes as a bar plot collections Counter,27
Quantlets as NFT,Upload your Code as an NFT on the Algorand Platform,"Quantlet, NFT, Algorand, Code, Testnet",Julian Winkel,quantinar_nft,4,algosdk.future.transaction PaymentTxn closeout_account my_address  secret_key  algod_client  build transaction  comment out the next two  2) lines to use suggested fees  Fifth argument is a close_remainder_to parameter that creates a payment txn that sends all of the remaining funds to the specified address. If you want to learn more  go to: https://developer.algorand.org/docs/reference/transactions/#payment-transaction  sign transaction  wait for confirmation	  utility for waiting on a transaction confirmation wait_for_confirmation client  transaction_id  timeout,66
Quantlets as NFT,Upload your Code as an NFT on the Algorand Platform,"Quantlet, NFT, Algorand, Code, Testnet",Julian Winkel,quantinar_nft,4,qrcode PIL Image gen_qr text  link  fname,7
Quantlets as NFT,Upload your Code as an NFT on the Algorand Platform,"Quantlet, NFT, Algorand, Code, Testnet",Julian Winkel,quantinar_nft,4,json base64 algosdk account algosdk.v2client algod time  This is a complete code example that:    1. Create a new test account    2. Ask to fund your created account create_account   Using Rand Labs Developer API  see https://github.com/algorand/py-algorand-sdk/issues/169  Change algod_token and algod_address to connect to a different client Generate new account for this transaction  Check your balance. It should be 0 microAlgos Fund the created account  Wait for the faucet to transfer funds  Check your balance. It should be 10000000 microAlgos  utility for waiting on a transaction confirmation wait_for_confirmation client  transaction_id  timeout,89
Quantlets as NFT,Upload your Code as an NFT on the Algorand Platform,"Quantlet, NFT, Algorand, Code, Testnet",Julian Winkel,quantinar_nft,4,"json hashlib csv algosdk mnemonic algosdk.v2client algod algosdk.future.transaction AssetConfigTxn createAccount create_account closeoutAccount closeout_account qr gen_qr pdb create_non_fungible_token _asset_name  _url  _destroy_asset = False  _append_file = True  _generate_qr = True  Generate QR   For ease of reference  add account public and private keys to  an accounts dict.  Using Rand Labs Developer API  see https://github.com/algorand/py-algorand-sdk/issues/169  Change algod_token and algod_address to connect to a different client  CREATE ASSET  Get network params for transactions before every transaction.  comment these two lines if you want to use suggested params  params.fee = 1000  params.flat_fee = True  JSON file f = open  'quantletNFTmetadata.json'  ""r"") ffffff""   Reading from file metadataJSON = json.loads f.read ))  Account 1 creates an asset called  and  sets Account 1 as the manager  reserve  freeze  and clawback address.  Asset Creation transaction  Sign with secret key of creator  Send the transaction to the network and retrieve the txid.  Wait for the transaction to be confirmed  Pull account info for the creator  account_info = algod_client.account_info accounts[1]['pk'])  get asset_id from tx  Get the new asset's information from the creator account  Inspect  Asset destroy transaction  Sign with secret key of creator  Send the transaction to the network and retrieve the txid.  Wait for the transaction to be confirmed  Asset was deleted.  utility for waiting on a transaction confirmation wait_for_confirmation client  transaction_id  timeout    Utility function used to print created asset for account and assetid print_created_asset algodclient  account  assetid  note: if you have an indexer instance available it is easier to just use this  response = myindexer.accounts asset_id = assetid)  then use 'account_info['created-assets'][0] to get info on the created asset    Utility function used to print asset holding for account and assetid print_asset_holding algodclient  account  assetid  note: if you have an indexer instance available it is easier to just use this  response = myindexer.accounts asset_id = assetid)  then loop thru the accounts returned and match the account you are looking for",308
LOBDeepPP_MSE_visualisation_T,"Joint Limit order book ask and bid price predition for multiple predicton horizons with a neural networks. This script creates plots meant for analysis of the fixed order book level application for training, validation and test dataset.","Limit order book, finance, forecasting, prediction, MSE, evaluation, plotting",Marius Sterling,LOBDeepPP_MSE_visualisation_T,1,"!/usr/bin/env python3  -*- coding: utf-8 -*- os json_tricks numpy matplotlib  %%  %%  %% Extracting MSE from errors  %% MSE for ask and bid for train  test and validation data     ax.legend loc='center left'  bbox_to_anchor= 1  0.5))  %% MSE vs prediction horizon with order levels for ask and bid seperated     plt.ylim [0.005  0.01])     plt.ylim [0.0035  0.02])     plt.ylim [0.0035  0.01])     plt.savefig f'{path_save}/ph_mse_ol_{ds}_zoom.pdf'                  bbox_inches='tight'  dpi=300)  %% MSE vs prediction horizon with order levels for ask and bid seperated         plt.ylim [0.0035  0.01])         plt.savefig f'{path_save}/ph_mse_ol_{ds}_{""bid"" if bid else ""ask""}_zoom.pdf'                      bbox_inches='tight'  dpi=300)",85
CRIX_stacking,Appliaction of a simple GAN framework with Wasserstein loss on CRIX stacking images,"WGAN, GAN, CRIX, time series, non stationarity, simulation, Wasserstein loss",Ramona Merkl,GAN_TS,2,"!/usr/bin/env python  coding: utf-8  In[1]: __future__ print_function tensorflow tensorflow.keras.datasets mnist tensorflow.keras.layers Input tensorflow.keras.layers BatchNormalization tensorflow.keras.models Sequential tensorflow.keras.optimizers RMSprop tensorflow matplotlib sys numpy pandas tensorflow.keras.datasets mnist  In[2]: __init__ self  Following parameter and optimizer set as recommended in paper  Build and compile the critic  Build the generator  The generator takes noise as input and generated imgs  For the combined model we will only train the generator  The critic takes generated images as input and determines validity  The combined model   stacked generator and critic) wasserstein_loss self  y_true  y_pred build_generator self build_critic self train self  epochs  batch_size=32  sample_interval=100  Load the dataset: either original TS or log returns X_train = np.load 'ts_images_train.npy'  allow_pickle=True)  Adversarial ground truths  ---------------------   Train Discriminator  ---------------------  Select a random batch of images  Sample noise as generator input  Generate a batch of new images  Train the critic  Clip critic weights  ---------------------   Train Generator  ---------------------  Plot the progress  If at save interval => save generated image samples self.save_models ) sample_images self  epoch axs[i  j].imshow np.fliplr gen_imgs[cnt  :  :  0])  cmap='Blues') fig.savefig ""rp_%d.png"" % epoch) files.download ""rp_%d.png"" % epoch) write_losses self ax.legend ) save_models self  In[3]:  In[181]:  create 25 sample images and their corresponding time series of length 54  In[223]:  take subsample from original dataset and scale it into [-1 1] in order to compare to generated series  In[238]: plt.title 'Example of Generated TS vs. Real CRIX log returns')  In[232]:  In[234]: plt.savefig 'example_logscaled'  transparent=True)  In[236]: fig.savefig f'stack_3000.png'  transparent=True)  In[274]:  In[277]:  compare 150 generated series to subsample of original one  In[276]:",246
DEDA_SVM_Swiss,Determines and plots the decision boundary of a SVM classifier with polynomial kernel of order 2 using the Swiss banknote dataset.,"Support vector machines, SVM, classification",Georg Keilbar,DEDA_SVM_Swiss,1,numpy numpy genfromtxt matplotlib sklearn svm Load data Simple scatterplot plot_svm_nonlinear x  y  model_class  **model_params Fit model Define grid Prediction on grid Contour + scatter plot,26
SDA_20201023_hw3_Ubike_Station,Mapping the u-bike station in Taipei with the number of u-bike left at that station.,ERROR,"Andreas Rony Wijaya, NCTU",SDA_20201023_hw3_Ubike_Station,2,!/usr/bin/env python  coding: utf-8  In[ ]: requests json pprint pandas numpy matplotlib  In[ ]:  In[ ]:  In[ ]:  In[ ]: folium folium.plugins MarkerCluster numpy matplotlib descartes  In[ ]:,28
DEDA_Class_SS2018_Folium_SiteSelection,Use Folium Package for Site Location Selection: A Case for New-entry Retailers to the Xinyi District of Taipei.,"facility location selection, geographic information, google map geocoding API, folium map, web scraping",Min-Bin Lin,Folium_SiteSelection,1,"!/usr/bin/env python2  -*- coding: utf-8 -*-  import packages os requests json geojson ast logging dill pandas bs4 BeautifulSoup selenium webdriver selenium.webdriver.support.ui Select folium folium plugins folium FeatureGroup pdfplumber shapely.geometry shape  google map api  2 500 free requests per day): geocoding and distance matrix  api key: https://developers.google.com/maps/documentation/geocoding/get-api-key googlemaps  Pickle file names ########################################################### __init__ self  name  address  entry_id  entry_type  lat  lon __repr__ self from_dict cls  de_dict to_dict self __init__ self __len__ self __repr__ self add_FamilyMart self  shop_data add_Eleven self  shop_data add_HiLife self  shop_data add_OKmart self  shop_data find self  type_id  type_id str in format ""data.TYPES <int_id>"" get_pair self  pair_str split_pair cls  pair_str  like ""shop 119478 / warehouse 01"" make_type_id cls  place make_pair cls  place_one  place_two  take two places  return a string identifying the pair ###########################################################  ony run the total codes when there is no ""GEOCODED_DATA_PKL"" ########################################################### ## FamilyMart data ###  add request condition ""Referer"" to solve blocking  the data we want is inside of a method  as a string  like somethingFunc [data])  looks like a dict  quacks like a dict  delete redundant string  input data into class ########################################################### ## 7Eleven data ###  first is header ########################################################### ## HiLife data ###  Use Firefox as the default browser  need to put geckodriver.exe in the bin  remove the empty string data ########################################################### ## OKmart data ###  use ast here because the unicode response is a string of strings   string with quotes in the string)  ast will evaluate the string into its python data types  [:6]: only till '信義崇安店 ' other shops don't exist anymore ########################################################### ## geocoding ###  FamilyMart has lat and lon already  7-Eleven  HiLife  OKmart ########################################################### ## MRT exits data ###  load Taipei MRT all exits data  all exits information  name:geocode)  import the geocode of the xinyi area  scale)  point in polygon with geojson by shapely  get the name for each coordinate pair in xinyi mrt exits ########################################################### ## population data  ""里""li) ###  extract data from pdf  https://github.com/jsvine/pdfplumber)  extract each page  extract tables  convert to dataframe  input missing data  due to the table frame)  remove english  ) remove_ascii text  drop the sum-up row  rename the column names ########################################################### ## load shape data  of whole taipei) ###  extrat li data only in xinyi area ########################################################### ## mapping ###  create future group for layer controll  icon in the map  choropleth  heatmap  add layer control",378
LDA-DTM_Policy_Risk_DTM,Analysis the word evolution of Cryptocurrency regulation news during 2013-2018 using Dynamic Topic Model ,"LDA, DTM, Topic Models, Cryptocurrency, regulation risk",Xinwen Ni,LDA-DTM_Regulation_Risk,2,"!/usr/bin/env python3  -*- coding: utf-8 -*-  please install these modules before you run the code: !pip install gensim !pip install pandas !pip install nltk !pip install matplotlib os re pandas # `nltk.download 'punkt') numpy nltk.corpus stopwords nltk.tokenize RegexpTokenizer nltk.stem.porter PorterStemmer nltk gensim corpora collections gensim.test.utils common_corpus gensim.models LdaSeqModel gensim.matutils hellinger gensim.corpora Dictionary gensim.models ldaseqmodel matplotlib  Please change the working directory to your path!  os.chdir ""/Users/xinwenni/LDA-DTM/DTM"")  BasicCleanText raw_text  keep only letters  numbers and whitespace  apply regex  lower case   Tokenization  create English stop words list en_stop = get_stop_words 'en')  remove stop words from tokens stopped_tokens = [i for i in tokens if not i in en_stop]  Create p_stemmer of class PorterStemmer  stem token  load data df = pd.read_csv 'df02.csv' encoding=""ISO-8859-1"")  find out the time slice  drop the words only appers once  generate the dictionary  store the dictionary  for future reference Save vocabulary Prevent storing the words of each document in the RAM __iter__ self  assume there's one document per line  tokens separated by whitespace  load one vector into memory at a time LdaSeqModel corpus=None  time_slice=None  id2word=None  alphas=0.01  num_topics=10  initialize='gensim'  sstats=None  lda_model=None  obs_variance=0.5  chain_variance=0.005  passes=10  random_state=None  lda_inference_max_iter=25  em_min_iter=6  em_max_iter=20  chunksize=100) use LdaSeqModel to generate DTM results ldaseq = LdaSeqModel corpus=corpus_memory_friendly  id2word=dictionary  time_slice=time_slice  num_topics=5)  for given time  the distriibution of each topic   for given topic the word distribution over time topic_time DTM_topic time_stamps plot the dynamic movement of topic 1 plt.xlim  -1  2)) plt.ylim  0  0.02)) plot the dynamic movement of topic2 plt.xlim  -1  2)) plt.ylim  0  0.02)) plot the dynamic movement of topic3 plt.xlim  -1  2)) plt.ylim  0  0.02)) plot the dynamic movement of topic4 plt.xlim  -1  2)) plt.ylim  0  0.02)) plot the dynamic movement of topic5 plt.xlim  -1  2)) plt.ylim  0  0.02))",280
LDA-DTM_Policy_Risk_DTM,Analysis the word evolution of Cryptocurrency regulation news during 2013-2018 using Dynamic Topic Model ,"LDA, DTM, Topic Models, Cryptocurrency, regulation risk",Xinwen Ni,LDA-DTM_Regulation_Risk,2,"!/usr/bin/env python3  -*- coding: utf-8 -*- #########################################################  Content:  part I:   Install and import packages and load data  part II:  Pre-process and vectorize the documents  Part III: Training LDA model  Part IV:  Find the optimal number of topics using coherence_values  Part V:   Compute similarity of topics   Part VI:  Visualize the topics ######################################################### #########################################################  Part I: install and import packages and load data   #########################################################  please install these modules before you run the code: !pip install gensim !pip install pandas !pip install nltk !pip install matplotlib os re pandas # `nltk.download 'punkt') numpy  NLTK nltk.corpus stopwords nltk.tokenize RegexpTokenizer nltk.stem.porter PorterStemmer nltk.stem.wordnet WordNetLemmatizer gensim.models Phrases nltk  Gensim gensim gensim gensim.utils simple_preprocess gensim.models CoherenceModel gensim.matutils kullback_leibler from gensim.test.utils import common_corpus gensim.models LdaModel gensim.test.utils datapath from gensim.models import LdaSeqModel from gensim.corpora import Dictionary  bleicorpus from gensim import models  similarities  spacy for lemmatization spacy scipy.stats wasserstein_distance  Plotting tools pyLDAvis pyLDAvis  don't skip this matplotlib matplotlib inline  Enable logging for gensim - optional logging matplotlib collections pprint pprint ###########################  Please change the working directory to your path!  os.chdir ""/Users/xinwenni/LDA-DTM/DTM"")  ###########################  load data df = pd.read_csv 'df02.csv' encoding=""ISO-8859-1"") #########################################################  Part II: Pre-process and vectorize the documents #########################################################  Convert to list clean_data data  Remove Emails  Remove new line characters  Remove distracting single quotes sent_to_words sentences  deacc=True removes punctuations  Define functions for stopwords  bigrams  trigrams and lemmatization remove_stopwords texts make_bigrams texts make_trigrams texts lemmatization texts  allowed_postags=['NOUN'  'ADJ'  'VERB'  'ADV'] get_lemm data     print data_words[:1])  Build the bigram and trigram models  higher threshold fewer phrases.  Faster way to get a sentence clubbed as a trigram/bigram  See trigram example     print trigram_mod[bigram_mod[data_words[0]]])  Remove Stop Words  Form Bigrams     data_words_bigrams = make_bigrams data_words_nostops)  Initialize spacy 'en' model  keeping only tagger component  for efficiency)  python3 -m spacy download en  Do lemmatization keeping only noun  adj  vb  adv data_lemmatized = lemmatization data_words_bigrams  allowed_postags=['NOUN'  'ADJ'  'VERB'  'ADV'])     print data_lemmatized[:1])  simple clean the data first   define stopwords  Tokenize and lemmatize data   Create Dictionary  Create Corpus  Term Document Frequency Let’s see how many tokens and documents we have to train on. #########################################################  Part III: Training LDA model  #########################################################  Set training parameters.  Don't evaluate model perplexity  takes too much time. # Make a index to word dictionary. temp = dictionary[0]  # This is only to ""load"" the dictionary. id2word = dictionary.id2token   num_words=20)  Average topic coherence is the sum of topic coherences of all topics  divided by the number of topics. #########################################################  Part IV: find the optimal number of topics using coherence_values ######################################################### compute_coherence_values dictionary  corpus  texts  limit  start=2  step=3         model = gensim.models.wrappers.LdaMallet mallet_path  corpus=corpus  num_topics=num_topics  id2word=id2word)  Can take a long time to run.  Show graph plt.ylim [0.25 0.45]) plt.legend  ""coherence_values"")  loc='best')  optimal number is 11  then adjust the topic number  and retrain the LDA model   Save model to disk.  Print the Keyword in the 10 topics #########################################################  Part V: Compute similarity of topics  ######################################################### plot_difference_matplotlib mdiff  title=""""  annotation=None matplotlib plotly  Fall back to matplotlib if we're not in a notebook  or if plotly is  unavailable for whatever reason. #########################################################  Part VI: Visualize the topics ######################################################### #########################################################",493
SL_2020_Genetic_Algorithm_UAV,Scheduling algorithm for UAV teams based on a genetic algorithm.,"Scheduling, UAV, Genetic, Tasks, Chromosomes",Eren Allak,SL_2020_Genetic_Algorithm_UAV,3, Import packages. numpy  Attributes.  Methods. fitness self  schedule  empty_team_id  n_time_slots_cycle  Remove emtpy schedule entries.  Ideal distribution of execution times.  Time distance between schedule elements.  The time distance of duration_next is ideal and gives f_1 = 1  100%).  f_1 = f_1.clip 0) # Restrict to positive values.  Value can be negative to penalize bad distribution.  Time distance to the start and end of planning cycle.  The schedule starting at 0 is ideal and gives f_cycle = 1  100%).  The schedule ending at  n_time_slots_cycle - duration_next) is ideal  and gives f_cycle = 1  100%).  Ideal number of executions in cycle. __init__ self  name  d1  d2  n1,104
SL_2020_Genetic_Algorithm_UAV,Scheduling algorithm for UAV teams based on a genetic algorithm.,"Scheduling, UAV, Genetic, Tasks, Chromosomes",Eren Allak,SL_2020_Genetic_Algorithm_UAV,3,"!/usr/bin/env python2  Packages.  Import packages. genetic_algorithm GeneticAlgorithm task Task  Parameters and constants. __init__ self  General Parameters.  Planning parameters.  The scheduling cycle is split into equal time slots.  Maximal number of task time slots.  Number of tasks.  Number of teams to carry out tasks.  Parameters for genetic algorithm.  Genetic representation of scheduling by a double array.  Arrays correspond to time slots for the start of the task and corresponding team id.  Constants.  Genetic algorithm parameters.  Randomized Simulation.  Main function. main   Define Tasks. Input:  - execution duration for a team in time slots  - ideal number of time slots until next execution  - ideal number of executions in whole planning cycle  Show fitness values and best schedule.  print ""Best schedules:"")  for i in range 1  6):      print ""Fitness: {0}"".format ga.fitness_values[-i]))      ga.print_schedule [ga.chromosomes[0][-i]  ga.chromosomes[1][-i]])      print  """")  print ""Worst schedules:"")  for i in range 0  5):      print ""Fitness: {0}"".format ga.fitness_values[i]))      ga.print_schedule [ga.chromosomes[0][i]  ga.chromosomes[1][i]])      print  """")",151
SL_2020_Genetic_Algorithm_UAV,Scheduling algorithm for UAV teams based on a genetic algorithm.,"Scheduling, UAV, Genetic, Tasks, Chromosomes",Eren Allak,SL_2020_Genetic_Algorithm_UAV,3," Import packages. numpy matplotlib  Static functions. check_occupied occupancy_mat  team_id  idx  duration set_occupied occupancy_mat  team_id  idx  duration  Attributes.  Methods. __init__ self  param  tasks_in  Set parameters.  Save tasks.  Initialize empty chromosomes as a double int array.  Initialize fitness values.  Initialize parents.  Initialize result variables. initialize_chromosomes self  Generate random chromosomes. select_chromosomes self  Generate cumulative probability for selection process.  Pick parent indices with proportional probability to their fitness. crossover self  Initialize empty chromosomes as a double int array.  Parent indices.  Crossover time slot.  Crossover team id.  Replace least fit chromosomes with new chromosomes. mutate self  Range of chromosomes allowed to mutate.  Mutate chromosomes. mutate2 self  Range of chromosomes allowed to mutate.  Mutate chromosomes. repair_chromosomes self  Correct for feasible schedules and compute fitness.  Occupancy table to identify infeasible schedules. start self  Seed random value.  Initialize Chromosomes.  Select and save parents.  Genetic operator: crossover.  Genetic operator: mutate.  Repair schedules to be feasible.  Save results.  Compute results.  print ""--------"")  print i)  print self.fitness_values)  print self.fitness_values.mean ))  print plot_results self  Plot results.  Create a figure containing a single axes.  for i in range 0 self.n_randomized_sim):      ax.plot np.arange 1  self.n_iterations+1)  self.sim_results_best_fitness[i :])      ax.plot np.arange 1  self.n_iterations+1)  self.sim_results_mean_fitness[i :])  Plot confidence interval.  Plot last simulation.  Configure plot appearance.  Try to fit the legend box located outside.  In inches. generate_random_chromosome self  Initialize time slots.  Random time slots  sorted in ascending order.  Random team IDs. ensure_feasibility self  schedule  team_occupancy  Iterate through the schedule and remove double missions on the same time slot.  Iterate through time slots in tasks and remove resource conflicts.  Check if any team is assigned for the time slot.  If not occupied  set occupied. Else  cancel the task. compute_fitness_value self  idx_chromosome  team_occupancy  Compute fitness value from task constraints.  Compute fitness value from resource usage. compute_fitness_vector self  schedule  team_occupancy  Compute fitness value from task constraints.  Compute fitness value from resource usage. sort_chromosomes_by_fitness self print_schedule self  schedule  Generate schedule in array.  header_time_table = np.zeros  self.n_task + 1  self.n_time_slots_cycle)  dtype=np.int)  header_time_table[0  :] = np.arange self.n_time_slots_cycle)  header_time_table[1:self.n_task + 1  :] = time_table  # Remove empty elements.  str_time_table = str header_time_table)  str_time_table = str_time_table.replace str self.empty_team_id)  ""--"")  print str_time_table print_team_time_slots self  idx",348
DEDA_Class_2018WS_Berlin_Property_Analysis_Clustering_Medical,"Use HDBSCAN clustering algorithm to cluster spatially heterogeneous medical points in lat, lng space within Berlin, DE.","HDBSCAN, Clustering, Spatial Analysis, Real Estate",Alex Truesdale,DEDA_Class_2018WS_Berlin_Property_Analysis_Clustering_Medical,2, -*- coding: utf-8 -*- hdbscan pandas numpy shapely.geometry MultiPoint clusterer objects_in  collection_in  min_cluster_size  min_samples  first_run  Run algorithm with haversine distance and ball-tree algorithm.  Count clusters and organise as pd.Series.  Store cluster locations in external list for proofing against original data. get_centermost_point cluster,42
DEDA_Class_2018WS_Berlin_Property_Analysis_Clustering_Medical,"Use HDBSCAN clustering algorithm to cluster spatially heterogeneous medical points in lat, lng space within Berlin, DE.","HDBSCAN, Clustering, Spatial Analysis, Real Estate",Alex Truesdale,DEDA_Class_2018WS_Berlin_Property_Analysis_Clustering_Medical,2, -*- coding: utf-8 -*- os time gmplot pandas clustering_module  Introduction.  Search for all university data; return as list of objects; create coordinate matrix of points.  First pass of clustering  greedy).  Find centremost points of clusters; reorder into new matrix.  Identify non-clustered clusters.  Define lat long coordinate lists.  Change dir. to visualisation dir.  Initialise map.  Plot scatter points. 07B3BC'  size = 40  marker = False) F76A7D'  size = 130  marker = False)  Trigger write / compile method to create final map.,80
DistributionalForecasts,We present a simple approach to forecasting conditional probability distributions of asset returns. We work with a parsimonious specification of ordered binary choice regression that imposes a connection on sign predictability across different quantiles. The model forecasts the future conditional probability distributions of returns quite precisely when using a past indicator and past volatility proxy as predictors. Direct benefits of the model are revealed in an empirical application to 29 most liquid U.S. stocks. The forecast probability distribution is translated to significant economic gains in a simple trading strategy. Our approach can also be useful in many other applications where conditional distribution forecasts are desired.,"asset returns, predictive distribution, conditional probability, probability forecasting, ordered binary choice",Jozef Barunik,DistributionalForecasts.jl,2,"!/usr/bin/env python  coding: utf-8  # DistributionalForecasts.jl    [![GitHub] http://pkg.julialang.org/badges/GitHub_0.6.svg)] http://pkg.julialang.org/detail/GitHub)    The code has been developed in Julia 0.6.4. version  as a code accompanying the Anatolyev and Barunik  2018) paper  and provides an estimation and inference for a model forecasting conditional probability distributions of asset returns  henceforth AB model). For further details  see    Anatolyev  S. and Barunik  J.  2018): *Forecasting dynamic return distributions based on ordered binary choice and  cross-quantile predictability connection*  manuscript [available here for download] https://ideas.repec.org/p/arx/papers/1711.05681.html)  Oct 2018)      ## Software requirements    [Julia] http://julialang.org/) together with few packages needs to be installed    ````julia  Pkg.add ""DataFrames"")  Pkg.add ""CSV"")  Pkg.add ""GLM"")  Pkg.add ""Optim"")  ````    ## Example: Forecasting dynamic return distributions  Note the full example is available as an interactive [IJulia] https://github.com/JuliaLang/IJulia.jl) notebook [here] https://github.com/barunik/DistributionalForecasts.jl/blob/master/Example.ipynb)      Load required packages  In[1]:  load main functions  Load example data  returns of XOM)  In[2]:  Choose number of cutoff levels and order of polynomials  In[3]:  no. of quantiles  choice of polynomial order  ## Parameter Estimation  Obtain fast parameter estimates of AB without inference. A vector of $js+p1+p2+2$ parameters is returned:    $$\delta_{0 1} \delta_{0 2} ... \delta_{0 js} \kappa_{0 1} ...\kappa_{p1+1 1} \kappa_{0 2} ...\kappa_{p2+1 2}$$      In[4]:  ## Inference  Estimate the AB model and obtain full inference and evaluation of fit  In[5]:  Estimates of intercepts $\delta_{0 1} \delta_{0 2} ... \delta_{0 js}$  In[6]:  Estimates of $\kappa_{0 1} ...\kappa_{p1+1 1} \kappa_{0 2} ...\kappa_{p2+1 2}$  In[7]:  standard errors for all coefficients  In[8]:  T-stats  In[9]:  Log Likelihood  In[11]:  Information criteria  AIC/BIC)  In[12]:  ## Recover Probabilities Predicted by the AB model  Obtain forecast of return distribution for time $t+1$ based on the in-sample window  In[16]:  ## Statistical Evaluation  A number of statistical tests from Gneiting and Raftery  2007)  and Gonzalez-Rivera and Sun  2015) are implemented in the *main.jl* file.     TBD    Gonzalez-Rivera  G. and Y. Sun  2015). Generalized autocontours: Evaluation of multivari- ate density models. International Journal of Forecasting 31 3)  799–814.    Gneiting  T. and A. Raftery  2007). Strictly proper scoring rules  prediction  and estimation. Journal of American Statistical Association 102  477)  359–378.    In[ ]:",329
exgb,Produces the Ensemble XGB model that creates artificial features during training. Model is tested using real data for Polish companies,"xgb, ensemble classifier, bankruptcy prediction, artificial features, risk modeling",Maciej Zieba,exgb,1,"numpy xgboost sklearn featuresNew       = np.zeros  NNewf 3) dtype = float) here draw  np.savetxt ""featuresNew.csv""  featuresNew  delimiter = "" "")",20
UCC,"Understanding Cryptocurrencies (UCC). Cryptocurrencies refer to a type of digital cash that use distributed ledger - or blockchain technology - to provide secure transactions. These currencies are generally misunderstood. While initially dismissed as fads or bubbles, most large central banks are considering launching their own version of national cryptocurrencies. In contrast to most data in financial economics, there is a plethora of detailed (free) data on the history of every transaction for the cryptocurrency complex. Further, there is little empirically-oriented research on this new asset class. This is an extraordinary research opportunity for academia. We provide a starting point by giving an insight into Cryptocurrency mechanisms and detailing summary statistics and focusing on potential future research avenues in financial economics.","hash, algorithm, crix, cryptocurrency, bitcoin, neural network, trading, fintech","Raphael Reule, Marius Sterling",UCC,3,!/usr/bin/env python  coding: utf-8  In[9]: pandas matplotlib matplotlib  In[10]:  Import data  drop LTC  set index as pd_datetime and include only data starting from May 2017  Monthly aggregated data  In[11]:  In[29]:  Closing prices  In[30]:  Closing prices excluding BTC  In[133]:  Closing prices of only Gold  BTC and SP500  In[33]:  Cumulative return over time  In[56]:  Daily QQ Plots statsmodels pandas sklearn preprocessing  In[65]:  Monthly QQ Plots  In[34]:  Rolling window corr 250 days window  In[35]:  Rolling window corr 100 days window  In[36]:  In[37]:  ## Tables  In[51]:  Parwise complete by default      print corr_df.to_latex ))  In[52]:  Log return statistics  NAs excluded by default  In[79]: scipy.stats jarque_bera  Jarque-Bera Test  In[101]: johansen_test coint_johansen  Implementation taken from: https://searchcode.com/codesearch/view/88477497/  Johansen test  In[98]:,112
UCC,"Understanding Cryptocurrencies (UCC). Cryptocurrencies refer to a type of digital cash that use distributed ledger - or blockchain technology - to provide secure transactions. These currencies are generally misunderstood. While initially dismissed as fads or bubbles, most large central banks are considering launching their own version of national cryptocurrencies. In contrast to most data in financial economics, there is a plethora of detailed (free) data on the history of every transaction for the cryptocurrency complex. Further, there is little empirically-oriented research on this new asset class. This is an extraordinary research opportunity for academia. We provide a starting point by giving an insight into Cryptocurrency mechanisms and detailing summary statistics and focusing on potential future research avenues in financial economics.","hash, algorithm, crix, cryptocurrency, bitcoin, neural network, trading, fintech","Raphael Reule, Marius Sterling",UCC,3,"numpy numpy zeros numpy.linalg inv statsmodels.regression.linear_model OLS rows x trimr x  front  end statsmodels mlag_ x  maxlag lag x  lag detrend y  order resid y  x coint_johansen x  p  k  print_on_console=True     % error checking on inputs     if  nargin ~= 3)      error 'Wrong # of inputs to johansen')     end  why this?  f is detrend transformed series  p is detrend data  dx    = trimr dx 1 0)  [k-1:]     print z.shape     print dx.shape  r0t   = dx - z* z\dx)  diff on lagged diffs  lx = trimr lag x k) k 0)     print 'rkt'  dx.shape  z.shape  rkt   = dx - z* z\dx)  level on lagged diffs  du  au = eig np.dot tmp  sig))  au is eval  du is evec  orig = np.dot tmp  sig)  % Normalize the eigen vectors such that  du'skk*du) = I  JP: the next part can be done much  easier  %      NOTE: At this point  the eigenvectors are aligned by column. To  %            physically move the column elements using the MATLAB sort   %            take the transpose to put the eigenvectors across the row  dt = transpose dt)  % sort eigenvalues and vectors  au  auind = np.sort diag au))  a = flipud au)  d = dt[aind :]  %NOTE: The eigenvectors have been sorted by row based on auind and moved to array ""d"".  %      Put the eigenvectors back in column format after the sort by taking the  %      transpose of ""d"". Since the eigenvectors have been physically moved  there is  %      no need for aind at all. To preserve existing programming  aind is reset back to  %      1  2  3  ....  d  =  transpose d)  test = np.dot transpose d)  np.dot skk  d))  %EXPLANATION:  The MATLAB sort function sorts from low to high. The flip realigns  %auind to go from the largest to the smallest eigenvalue  now aind). The original procedure  %physically moved the rows of dt  to d) based on the alignment in aind and then used  %aind as a column index to address the eigenvectors from high to low. This is a double  %sort. If you wanted to extract the eigenvector corresponding to the largest eigenvalue by   %using aind as a reference  you would get the correct eigenvector  but with sorted  %coefficients and  therefore  any follow-on calculation would seem to be in error.  %If alternative programming methods are used to evaluate the eigenvalues  e.g. Frame method  %followed by a root extraction on the characteristic equation  then the roots can be  %quickly sorted. One by one  the corresponding eigenvectors can be generated. The resultant  %array can be operated on using the Cholesky transformation  which enables a unit  %diagonalization of skk. But nowhere along the way are the coefficients within the  %eigenvector array ever changed. The final value of the ""beta"" array using either method  %should be the same.  % Compute the trace and max eigenvalue statistics */  columnsum ?  tmp = np.log 1-a)  lr1[i] = -t * np.sum tmp[i:])  end  % set up results structure  estimation results  residuals  transposed compared to matlab ? c_sjt n  p  PURPOSE: find critical values for Johansen trace statistic  ------------------------------------------------------------  USAGE:  jc = c_sjt n p)  where:    n = dimension of the VAR system                NOTE: routine doesn't work for n > 12            p = order of time polynomial in the null-hypothesis                  p = -1  no deterministic part                  p =  0  for constant term                  p =  1  for constant plus time-trend                  p >  1  returns no critical values  ------------------------------------------------------------  RETURNS: a  3x1) vector of percentiles for the trace           statistic for [90# 95# 99#]  ------------------------------------------------------------  NOTES: for n > 12  the function returns a  3x1) vector of zeros.         The values returned by the function were generated using         a method described in MacKinnon  1996)  using his FORTRAN         program johdist.f  ------------------------------------------------------------  SEE ALSO: johansen )  ------------------------------------------------------------  # References: MacKinnon  Haug  Michelis  1996) 'Numerical distribution  functions of likelihood ratio tests for cointegration'   Queen's University Institute for Economic Research Discussion paper.  -------------------------------------------------------  written by:  James P. LeSage  Dept of Economics  University of Toledo  2801 W. Bancroft St   Toledo  OH 43606  jlesage@spatial-econometrics.com  Ported to Python by Javier Garcia  javier.macro.trader@gmail.com  these are the values from Johansen's 1995 book  for comparison to the MacKinnon values  jcp0 = [ 2.98   4.14   7.02         10.35  12.21  16.16         21.58  24.08  29.19         36.58  39.71  46.00         55.54  59.24  66.71         78.30  86.36  91.12        104.93 109.93 119.58        135.16 140.74 151.70        169.30 175.47 187.82        207.21 214.07 226.95        248.77 256.23 270.47        293.83 301.95 318.14]; c_sja n  p  PURPOSE: find critical values for Johansen maximum eigenvalue statistic  ------------------------------------------------------------  USAGE:  jc = c_sja n p)  where:    n = dimension of the VAR system            p = order of time polynomial in the null-hypothesis                  p = -1  no deterministic part                  p =  0  for constant term                  p =  1  for constant plus time-trend                  p >  1  returns no critical values  ------------------------------------------------------------  RETURNS: a  3x1) vector of percentiles for the maximum eigenvalue           statistic for: [90# 95# 99#]  ------------------------------------------------------------  NOTES: for n > 12  the function returns a  3x1) vector of zeros.         The values returned by the function were generated using         a method described in MacKinnon  1996)  using his FORTRAN         program johdist.f  ------------------------------------------------------------  SEE ALSO: johansen )  ------------------------------------------------------------  References: MacKinnon  Haug  Michelis  1996) 'Numerical distribution  functions of likelihood ratio tests for cointegration'   Queen's University Institute for Economic Research Discussion paper.  -------------------------------------------------------  written by:  James P. LeSage  Dept of Economics  University of Toledo  2801 W. Bancroft St   Toledo  OH 43606  jlesage@spatial-econometrics.com  Ported to Python by Javier Garcia  javier.macro.trader@gmail.com",886
Finding_all_Combinations_of_initial_Weights,Finding all possible combinations of initial weights for a four-cryptocurrencies-portfolio,"initial weights, combinations","Georg Velev, Iliyana Pekova",Finding_all_Combinations_of_initial_Weights,1, Python3 program to find out all   combinations of positive   numbers that add upto given number   arr - array to store the combination   index - next location in array   num - given number   reducedNum - reduced number  findCombinationsUtil arr  index  num  reducedNum  Base condition   If combination is   found  print it   Find the previous number stored in arr[].   It helps in maintaining increasing order   note loop starts from previous   number i.e. at array location   index - 1   next element of array is k   call recursively with   reduced number   Function to find out all   combinations of positive numbers   that add upto given number.   It uses findCombinationsUtil )  findCombinations n  array to store the combinations   It can contain max n elements   find all combinations   Driver code ,122
pyTSA_GaussWN,This Quantlet generates Gaussian white noise,"simulation, white noise, Gaussian, normal","Huang Changquan, Alla Petukhina",pyTSA_GaussWN,1,numpy random pandas  for repeat matplotlib PythonTsa.plot_acf_pacf acf_pacf_fig,8
pyTSA_ExRateWN,"This Quantlet builds log-returns and double difference of log series for quarterly exchange rate of GBP to NZD and plots time series and their ACF   for the time span from March 1991 to September 2000 (example 2.3, Figures 2.5-2.7 from the Book)","time series, autocorrelation, exchange rate, white noise, visualisation, log-returns, Ljung-Box test, ACF","Huang Changquan, Alla Petukhina",pyTSA_ExRateWN,1,"pandas numpy matplotlib statsmodels.tsa.stattools acf PythonTsa.plot_acf_pacf acf_pacf_fig statsmodels.stats.diagnostic acorr_ljungbox delete ""NaN""  r for ACF; q for Ljung-Box statistics; p for p-values",21
pyTSA_SARMA,"This Quantlet simulates ARMA(2,2) - autoregressive moving average process and draws the true ACF and PACF","time series,  stationarity, autocorrelation, PACF, ACF, simulation, stochastic process, ARMA, moving average, autoregression",ERROR,pyTSA_SARMA,1,numpy PythonTsa.True_acf Tacf_pacf_fig matplotlib,4
DEDA_WebScrapingAndWordFrequency,"This repository contains code for Python Webscraping, Sentiment Analysis, Topic Models (Latent Dirichlet Allocation) and the generation of Word Clouds. Analysis is performed on several speeches, namely ""What to the Slave is the Fourth of July"" by Frederick Douglass and multiple speeches by Donald J. Trump.",ERROR,Julian Winkel,DEDA_WebScrapingAndWordFrequency,7," -*- coding: utf-8 -*-  -*- coding: utf-8 -*-  For Text Extraction pandas bs4 urllib  Further Analysis matplotlib re nltk.corpus stopwords os pysentiment  Read the whole text.  Regex cleaning  keep only letters  numbers and whitespace  apply regex  lower case   Save dictionaries for wordcloud  Count and create dictionary  Filter Stopwords  Resort in list  Reconvert to dictionary SequenceSelection dictionary  length  startindex = 0  length is length of highest consecutive value vector  Test input  Plot most frequent words  Overview plt.savefig ""overview.png"") plt.savefig 'overview.png'  transparent=True)  Sentiment Analysis  Polarity  Formula:  Positive - Negative)/ Positive + Negative)  Subjectivity  Formula:  Positive + Negative)/N",95
DEDA_WebScrapingAndWordFrequency,"This repository contains code for Python Webscraping, Sentiment Analysis, Topic Models (Latent Dirichlet Allocation) and the generation of Word Clouds. Analysis is performed on several speeches, namely ""What to the Slave is the Fourth of July"" by Frederick Douglass and multiple speeches by Donald J. Trump.",ERROR,Julian Winkel,DEDA_WebScrapingAndWordFrequency,7," -*- coding: utf-8 -*- os path os PIL Image numpy matplotlib wordcloud WordCloud  Read the whole text.  Mask  Optional additional stopwords  Construct Word Cloud  no backgroundcolor and mode = 'RGBA' create transparency  Pass Text  store to file  show  plt.figure )  plt.imshow wc  interpolation='bilinear')  plt.axis ""off"")  plt.imshow stormtrooper_mask  cmap=plt.cm.gray  interpolation='bilinear')  plt.axis ""off"")  plt.savefig ""stormtrooper.png"")  plt.show )",55
DEDA_WebScrapingAndWordFrequency,"This repository contains code for Python Webscraping, Sentiment Analysis, Topic Models (Latent Dirichlet Allocation) and the generation of Word Clouds. Analysis is performed on several speeches, namely ""What to the Slave is the Fourth of July"" by Frederick Douglass and multiple speeches by Donald J. Trump.",ERROR,Julian Winkel,DEDA_WebScrapingAndWordFrequency,7," -*- coding: utf-8 -*-  For Text Extraction pandas bs4 urllib  Further Analysis matplotlib re nltk.corpus stopwords os pysentiment os path  Define URL url = ""http://www.cnn.com/2017/01/20/politics/trump-inaugural-address/"" raw_html = urllib.request.urlopen url) parsed_html = bs4.BeautifulSoup raw_html  ""lxml"") # Choose lxml parser  type bs4.element.ResultSet text = parsed_html.find_all ""div""  class_ = ""zn-body__paragraph"")  Speech stored in list textl = [] # init for i in text:     textl.append i.get_text ))  Convert to string  Regex cleaning  keep only letters  numbers and whitespace  apply regex  lower case   Save dictionaries for wordcloud text_file = open ""Output.txt""  ""w"") text_file.write str cleantext)) text_file.close )  Count and create dictionary  Filter Stopwords  Resort in list  Reconvert to dictionary SequenceSelection dictionary  length  startindex = 0  length is length of highest consecutive value vector  Test input  Plot most frequent words  Overview plt.savefig ""overview.png"") plt.savefig 'overview.png'  transparent=True)  Sentiment Analysis  Polarity  Formula:  Positive - Negative)/ Positive + Negative)  Subjectivity  Formula:  Positive + Negative)/N",145
DEDA_WebScrapingAndWordFrequency,"This repository contains code for Python Webscraping, Sentiment Analysis, Topic Models (Latent Dirichlet Allocation) and the generation of Word Clouds. Analysis is performed on several speeches, namely ""What to the Slave is the Fourth of July"" by Frederick Douglass and multiple speeches by Donald J. Trump.",ERROR,Julian Winkel,DEDA_WebScrapingAndWordFrequency,7," -*- coding: utf-8 -*-  For Text Extraction pandas bs4 urllib  Further Analysis matplotlib re nltk.corpus stopwords os pysentiment  Define URL  Choose lxml parser  type bs4.element.ResultSet  Speech stored in list  init  Convert to string  Regex cleaning  keep only letters  numbers and whitespace  apply regex  lower case   Save dictionaries for wordcloud  Count and create dictionary  Filter Stopwords  Resort in list  Reconvert to dictionary SequenceSelection dictionary  length  startindex = 0  length is length of highest consecutive value vector  Test input  Plot most frequent words  Overview plt.savefig ""overview.png"") plt.savefig 'overview.png'  transparent=True)  Sentiment Analysis  Polarity  Formula:  Positive - Negative)/ Positive + Negative)  Subjectivity  Formula:  Positive + Negative)/N",102
DEDA_WebScrapingAndWordFrequency,"This repository contains code for Python Webscraping, Sentiment Analysis, Topic Models (Latent Dirichlet Allocation) and the generation of Word Clouds. Analysis is performed on several speeches, namely ""What to the Slave is the Fourth of July"" by Frederick Douglass and multiple speeches by Donald J. Trump.",ERROR,Julian Winkel,DEDA_WebScrapingAndWordFrequency,7, -*- coding: utf-8 -*- os re os path  Read the whole text.  Regex cleaning  keep only letters  numbers and whitespace  apply regex  lower case  nltk.corpus stopwords nltk.stem.wordnet WordNetLemmatizer string clean doc  Importing Gensim gensim gensim corpora  Creating the term dictionary of our courpus  where every unique term is assigned an index.  Converting list of documents  corpus) into Document Term Matrix using dictionary prepared above.  Creating the object for LDA model using gensim library  Running and Trainign LDA model on the document term matrix.,83
DEDA_WebScrapingAndWordFrequency,"This repository contains code for Python Webscraping, Sentiment Analysis, Topic Models (Latent Dirichlet Allocation) and the generation of Word Clouds. Analysis is performed on several speeches, namely ""What to the Slave is the Fourth of July"" by Frederick Douglass and multiple speeches by Donald J. Trump.",ERROR,Julian Winkel,DEDA_WebScrapingAndWordFrequency,7, -*- coding: utf-8 -*- os path wordcloud WordCloud  Read the whole text.  Generate a word cloud image  Display the generated image:  the matplotlib way: matplotlib  lower max_font_size  The pil way  if you don't have matplotlib)  image = wordcloud.to_image )  image.show ),41
DEDA_WebScrapingAndWordFrequency,"This repository contains code for Python Webscraping, Sentiment Analysis, Topic Models (Latent Dirichlet Allocation) and the generation of Word Clouds. Analysis is performed on several speeches, namely ""What to the Slave is the Fourth of July"" by Frederick Douglass and multiple speeches by Donald J. Trump.",ERROR,Julian Winkel,DEDA_WebScrapingAndWordFrequency,7," -*- coding: utf-8 -*- selenium webdriver matplotlib re nltk.corpus stopwords os pysentiment from wordcloud import WordCloud path_direct = os.getcwd ) os.chdir path_direct + '/pyning')  Start Selenium  Extract text  find and delete <...> combinations  find and delete /...> combinations  Takes only lists as input  Returns list as output SearchAndReplaceSeq html  opensign  closesign  delete continue # switch to next mark  first one is always open i += 1       Some expressions still left  Differ between quotes!  Count and create dictionary  Filter Stopwords keys in stopwords.words ""english"")  Resort in list  Reconvert to dictionary valueSelection dictionary  length  startindex = 0  length is length of highest consecutive value vector  Test input  Save dictionaries for wordcloud  Plot  Overview  Sentiment Analysis  Polarity  Formula:  Positive - Negative)/ Positive + Negative)  Subjectivity  Formula:  Positive + Negative)/N  Wordcloud  Generate a word cloud image",132
CV06_Exploration,"Exploratory Data Analysis for the final data frame that consists of tweet sentiments, google trend and market data","EDA, Sentiments, Exploration, Trends, Cryptocurrencies, Time Series",Fabian Schmidt,CV06_Exploration,4,"!/usr/bin/env python  coding: utf-8  In[1]: numpy pandas seaborn matplotlib matplotlib rcParams rcParams.update {'figure.autolayout': True}) sns.set color_codes=True) sns.set rc={'figure.figsize': 12 6.5)}) scipy stats  In[2]:  read the large csv file with specified chunksize   In[3]:  append each chunk df here   Each chunk is in df format   optional) --> perform data filtering  chunk_filter = chunk_preprocessing chunk)  Once the data filtering is done  append the chunk to list  concat the list into dataframe   ## EDA  ### Trends  In[4]:  In[5]:  In[6]:  In[7]:  In[8]:  Put the legend out of the figure  In[8]:  In[9]:  In[10]:  In[14]:  Put the legend out of the figure  ### Consolidated data frame  In[11]:  In[12]:  In[13]:  In[14]:  In[15]:  ### Publishing bevavior  In[16]:  Tweets Published"")  In[17]:  Length of tweets   of Tweets')  In[18]:  Ratings dist   of Tweets')  #### VCRIX  In[19]:  In[20]:  In[21]:  Put the legend out of the figure plt.legend bbox_to_anchor= 1.05  1)  loc=2  borderaxespad=0.)  In[22]:  In[23]:  decomposition statsmodels matplotlib matplotlib  Set the locator  every month  Specify the format - %b gives us Jan  Feb...  format the ticks  round to nearest years.  format the coords message box  format the price.  rotates and right aligns the x labels  and moves the bottom of the  axes up to make room for them  Specify formatter  In[24]:  Prepare data df['month'] = [d.strftime '%b') for d in df.date]  Draw Plot  Ratings dist   In[25]: statsmodels.tsa.stattools adfuller  In[27]:  In[28]:  Put the legend out of the figure plt.legend bbox_to_anchor= 1.05  1)  loc=2  borderaxespad=0.)  In[29]:  In[30]: statsmodels.tsa.stattools adfuller  In[31]:  In[32]:  In[33]:  Normality test  In[34]:  autocorrelation plots  In[35]:  In[36]:  Scale by 100 to get percentages  In[37]:  In[38]:  #### Sentiments  In[41]:  In[42]:  In[43]:  In[44]:  In[45]:  generate text  In[46]: wordcloud WordCloud  Generate a word cloud image plt.savefig 'tweets_wordcloud.png'  facecolor='k'  bbox_inches='tight')",272
CV06_Exploration,"Exploratory Data Analysis for the final data frame that consists of tweet sentiments, google trend and market data","EDA, Sentiments, Exploration, Trends, Cryptocurrencies, Time Series",Fabian Schmidt,CV06_Exploration,4,"!/usr/bin/env python  coding: utf-8  In[1]: numpy pandas seaborn matplotlib matplotlib rcParams rcParams.update {'figure.autolayout': True}) sns.set color_codes=True) sns.set rc={'figure.figsize': 12 6.5)}) scipy stats  In[2]:  read the large csv file with specified chunksize   In[3]:  append each chunk df here   Each chunk is in df format   optional) --> perform data filtering  chunk_filter = chunk_preprocessing chunk)  Once the data filtering is done  append the chunk to list  concat the list into dataframe   ## EDA  ### Trends  In[4]:  In[5]:  In[6]:  In[7]:  In[8]:  Put the legend out of the figure  In[9]:  In[10]:  In[13]:  In[14]:  Put the legend out of the figure  ### Consolidated data frame  In[15]:  In[16]:  In[17]:  In[18]:  In[19]:  ### Publishing bevavior  In[20]:  Tweets Published"")  In[21]:  Length of tweets   of Tweets')  In[22]:  Ratings dist   of Tweets')  #### VCRIX  In[23]:  In[24]:  In[25]:  Put the legend out of the figure plt.legend bbox_to_anchor= 1.05  1)  loc=2  borderaxespad=0.)  In[26]:  In[27]:  decomposition statsmodels matplotlib matplotlib  Set the locator  every month  Specify the format - %b gives us Jan  Feb...  format the ticks  round to nearest years.  format the coords message box  format the price.  rotates and right aligns the x labels  and moves the bottom of the  axes up to make room for them  Specify formatter  In[28]:  Prepare data df['month'] = [d.strftime '%b') for d in df.date]  Draw Plot  Ratings dist   In[29]: statsmodels.tsa.stattools adfuller  In[30]:  In[31]:  Put the legend out of the figure plt.legend bbox_to_anchor= 1.05  1)  loc=2  borderaxespad=0.)  In[32]:  In[33]: statsmodels.tsa.stattools adfuller  In[34]:  In[35]:  In[36]:  Normality test  In[37]:  autocorrelation plots  In[40]:  In[44]:  Scale by 100 to get percentages  In[47]:  In[48]:  #### Sentiments  In[2]:  In[3]:  In[4]:  In[5]:  In[6]:  generate text  In[10]: wordcloud WordCloud  Generate a word cloud image plt.savefig 'tweets_wordcloud.png'  facecolor='k'  bbox_inches='tight')",272
DEDA_StatisticsFinance,ERROR,ERROR,"Junjie Hu, Yves Hilpisch, Justin Hellermann, Isabell Fetzer",DEDA_StatisticsFinance,2,"!/usr/bin/env python  coding: utf-8  # **DEDA Unit 5** : Basic Statistics and Visualisation in Python  ### **Interactive Graphs**     In[6]:  pip install yfinance datetime datetime pandas plotly yfinance getData crypto  Define time window  Call function to retrieve data on Bitcoin   eth_data = getData 'ETH')  Plot graph  ### **Matrix Operations**   In[ ]:  Note: A here is the random matrix from above  Inverse matrix  Matrix multiplication operation  Generate a 20*20 identity matrix  Using .allclose ) function to evaluate two matrices are equal within tolerance  True  ### **Eigenvalues**   In[ ]:  [[3  1] [1  1]] would create a 2x2 matrix  Now the Jordan decomposition A = Gamma*Lambda*Gamma^T  Check the result  you might get something within numerical eps  True  Calculation of the square root of A  ### **Fourier Transformation**   In[ ]: PIL Image numpy.fft fft numpy  Open the image by using Python Imaging Library PIL)  Decoding and encoding image to float number  Processing Fourier transform  Filter the lower frequency  i.e. employ a high pass  Inverse Fourier transform  Keep the real part  Output the image  ### **Normal Distribution**   In[ ]: numpy matplotlib scipy plotly ipywidgets interact  Set up figure size of plot   length and width   general box size  In[ ]:  Plot the cdf and pdf in one figure f mu sigma  colour eeefef')  upper plot: pdf   rows  columns  which one)  lower plot: cdf  ### **Kerner Density Estimation**    In[ ]: numpy matplotlib scipy.stats norm sklearn.neighbors KernelDensity  Code reference: http://scikit-learn.org/stable/auto_examples/neighbors/plot_kde_1d.html  Set up figure size of plot   length and width   general box size  Create 2 normal distributed data set  convex combination parameter  Create x axis range  Create linear combination of 2 normal distributed random variable  In[ ]:  Plot the real distribution  Use 3 different kernels to estimate  Initial an object to use kernel function to fit data  bandwidth will    affect the result  Evaluate the density model on the data  Add text on the plot  position argument can be arbitrary  ax.text 6  0.38  ""N={0} points"".format N))  Add a legend to the left outside of the plot   ax.legend loc='upper left'  bbox_to_anchor =  1 0.5))  Plot the random points  squeeze them into narrow space  Set x-axis y-axis limit to adjust the figure  ### **Box-Muller Method on 2-dim normal distribution**    In[ ]: numpy matplotlib  For mu =  0 0)  covariance matrix Sigma = identity matrix  Number of random numbers  determines the size of the plotted points  a good size might be msize=5 for n=500 pts and msize=0.1 for n>50K    Change to cartesian coordinates  In[ ]:  Calculate sqrt A) with Jordan decomposition  Solve with matrix multiplication",411
DEDA_Class_2017_LinkedIn_Analysis,This repository contains python scripts to scrape information from LinkedIn and analyze talent distributions by gender and locations.,ERROR,Paul Jakob,DEDA_LinkedIn_Analysis,1, importing packages for request handling  data retrieval  processing and visualization requests os bs4 BeautifulSoup pandas collections Counter matplotlib numpy  packages for maps visualization googlemaps gmaps gmplot  packages for the gender predictions nltk nltk.corpus names random  import config file with credentials - path needs to be adapted CONFIG  put into routine with searchterm as input parameter  implement more secure  proofs)  set url for initial request and call website to create and retrieve session values  get all input values / loop over the inputs / identify the csrftoken and get the value  set values for the login form  open up the session by logging in successfully  Setup search parameters region and search term  Set range for the loop - should be dynamic  Get results page for every page and store them in an array  paging string  empty array where all result strings are stored  loop over every result page and retrieve the location  the 14th code tag of each page contains the user information  split the data string for every location   loop over locations and print out   empty array where all names are stored  loop over every result page and retrieve the first name  the 14th code tag of each page contains the user information  split the data string for every location   loop over locations and print out   remove unnecessary information  get the count of each City and show a diagram for the location distribution  sort descending  plot the graph  Maps integration   Geocoding a city  geocode_result = gmaps.geocode all_cities[0])  loop over all cities and get the geocoding --> also for duplicates for density on heatmap  initialize the map  try different plots / heatmap  scatter  marker gmap.plot latitudes  longitudes  'cornflowerblue'  edge_width=10) gmap.scatter latitudes  longitudes  '#3B0B39'  size=40  marker=False)  helper function gender_features name  feature set  remove unnecessary information of the first names  double name  titles  etc)  remove entries that aren't names  check for and remove all possible disruptions in strings  title  end character  two first names  double names  only one to pop therefore no loop  Now the model prediction starts - Using NLKTs Naive Bayes Model  retrieve test set  Mix up the list Process the names through feature extractor  Divide the feature sets into training and test sets  Train the naiveBayes classifier  Test the accuracy of the classifier on the test data  returns 81%  Predict the gender for all names in the list  sort descending  plot the graph,393
VisualTFIDF,Plots index terms of large text corpora with respect to their document frequency in the corpus,"TFIDF, document frequency, Vector Space Model, text mining, Visualisation, bar chart",Jerome Bau,VisualTFIDF,1,gensim models pandas numpy matplotlib __init__ self load_tfidf self  path load_dict self  path make_pandas_vec self pandas_vec_add_labels self barplot_term_frequencies self  top  plt.xticks od  ordered_pd.index[:top]),23
DEDA_Class_2019SS_Crypto_News_Sentiment_Scraping_Tool,"Interactive, terminal based scraping tool built to collect news story text from the New York Times, BBC, CNN, and Reuters. The user chooses the desired news source and enters 1-3 search terms. Resulting publications are saved to a .csv file for further processing needs of the user.","Scraping, Text Analysis, Selenium, Cryptocurrency",Alex Truesdale,DEDA_Class_2019SS_Crypto_News_Sentiment_Scraping_Tool,7,bs4 BeautifulSoup scraper_base ScraperBase pandas time __init__ self query_bbc self story_extractor self  raw_html content_fetcher self  elements_extracted nav_scrape row  iteration {row.name}'  end='\r'),21
DEDA_Class_2019SS_Crypto_News_Sentiment_Scraping_Tool,"Interactive, terminal based scraping tool built to collect news story text from the New York Times, BBC, CNN, and Reuters. The user chooses the desired news source and enters 1-3 search terms. Resulting publications are saved to a .csv file for further processing needs of the user.","Scraping, Text Analysis, Selenium, Cryptocurrency",Alex Truesdale,DEDA_Class_2019SS_Crypto_News_Sentiment_Scraping_Tool,7,"selenium webdriver selenium.webdriver.common.keys Keys selenium.webdriver.chrome.options Options selenium.webdriver ActionChains bs4 BeautifulSoup pandas time wait self  time_s: float __init__ self  fresh driver_bootup self  chrome_options.add_argument ""--headless"") user_defined_terms self",25
DEDA_Class_2019SS_Crypto_News_Sentiment_Scraping_Tool,"Interactive, terminal based scraping tool built to collect news story text from the New York Times, BBC, CNN, and Reuters. The user chooses the desired news source and enters 1-3 search terms. Resulting publications are saved to a .csv file for further processing needs of the user.","Scraping, Text Analysis, Selenium, Cryptocurrency",Alex Truesdale,DEDA_Class_2019SS_Crypto_News_Sentiment_Scraping_Tool,7,bs4 BeautifulSoup scraper_base ScraperBase pandas time __init__ self query_reuters self story_extractor self  raw_html content_fetcher self  elements_extracted nav_scrape row  iteration {row.name}'  end='\r'),21
DEDA_Class_2019SS_Crypto_News_Sentiment_Scraping_Tool,"Interactive, terminal based scraping tool built to collect news story text from the New York Times, BBC, CNN, and Reuters. The user chooses the desired news source and enters 1-3 search terms. Resulting publications are saved to a .csv file for further processing needs of the user.","Scraping, Text Analysis, Selenium, Cryptocurrency",Alex Truesdale,DEDA_Class_2019SS_Crypto_News_Sentiment_Scraping_Tool,7,bs4 BeautifulSoup scraper_base ScraperBase selenium.webdriver.support.ui WebDriverWait selenium.webdriver.support expected_conditions selenium.webdriver.common.by By pprint pprint pandas time re __init__ self query_cnn self story_extractor self  raw_html  base content_fetcher self  elements_extracted nav_scrape row  iteration {row.name}'  end='\r'),31
DEDA_Class_2019SS_Crypto_News_Sentiment_Scraping_Tool,"Interactive, terminal based scraping tool built to collect news story text from the New York Times, BBC, CNN, and Reuters. The user chooses the desired news source and enters 1-3 search terms. Resulting publications are saved to a .csv file for further processing needs of the user.","Scraping, Text Analysis, Selenium, Cryptocurrency",Alex Truesdale,DEDA_Class_2019SS_Crypto_News_Sentiment_Scraping_Tool,7,bs4 BeautifulSoup scraper_base ScraperBase pandas time __init__ self query_nyt self story_extractor self  raw_html  topic content_fetcher self  elements_extracted nav_scrape row {row.name}'  end='\r'),21
DEDA_Class_2019SS_Crypto_News_Sentiment_Scraping_Tool,"Interactive, terminal based scraping tool built to collect news story text from the New York Times, BBC, CNN, and Reuters. The user chooses the desired news source and enters 1-3 search terms. Resulting publications are saved to a .csv file for further processing needs of the user.","Scraping, Text Analysis, Selenium, Cryptocurrency",Alex Truesdale,DEDA_Class_2019SS_Crypto_News_Sentiment_Scraping_Tool,7,sys requirements pick pick source_selector source_selector main ,7
DEDA_Class_2019SS_Crypto_News_Sentiment_Scraping_Tool,"Interactive, terminal based scraping tool built to collect news story text from the New York Times, BBC, CNN, and Reuters. The user chooses the desired news source and enters 1-3 search terms. Resulting publications are saved to a .csv file for further processing needs of the user.","Scraping, Text Analysis, Selenium, Cryptocurrency",Alex Truesdale,DEDA_Class_2019SS_Crypto_News_Sentiment_Scraping_Tool,7,scraper_bbc BBCScraper scraper_nyt NYTScraper scraper_reuters ReutersScraper scraper_cnn CNNScraper source_selector selected_source,10
pyTSA_FemaleLabor,"This Quantlet simulates ARMA(2,2) - autoregressive moving average process and draws the true ACF and PACF","time series,  stationarity, autocorrelation, PACF, ACF, simulation, stochastic process, ARMA, moving average, autoregression",ERROR,pyTSA_FemaleLabor,1,pandas statsmodels matplotlib statsmodels.graphics.tsaplots month_plot PythonTsa.plot_acf_pacf acf_pacf_fig  k_diff is the order number of the ordinary difference.  k_seasonal_diff is the order number of the seasonal difference.  seasonal_periods is the seasonal period.,30
RL_MainComputation,"Trains the neural network. The input values are configured in net_config.json. Additionaly, the target drawdown 'D_target' can be set in nnagent.py in the function loss_function8. To run the code, open the command line and enter first 'python main.py --mode=generate --repeat=1' and then 'python main.py --mode=train --processes=1'. The output is then located in the 'train_package'. The basis for the code was provided by Z. Jiang and can be found here: 'https://github.com/ZhengyaoJiang/PGPortfolio/blob/master' Note that the command 'python main.py --mode=download_data' used to download data from Poloniex, but may not work anymore. A database is provided in the 'database' folder, to allow to replicate the experiments in the Thesis and do other experiments in the time range '01.07.2015 - 31.10.2018'. The database is in the rar format and needs to be extracted before running the code.","reinforcement learning, neural network, machine learning, portfolio management, cryptocurrency",Ilyas Agakishiev,RL_MainComputation,1,__future__ absolute_import json logging os time argparse ArgumentParser datetime datetime pgportfolio.tools.configprocess preprocess_config pgportfolio.tools.configprocess load_config pgportfolio.tools.trade save_test_data pgportfolio.tools.shortcut execute_backtest pgportfolio.resultprocess plot build_parser  main  pgportfolio pgportfolio pgportfolio.marketdata.datamatrices DataMatrices  This is used to export the test data _set_logging_by_algo console_level  file_level  algo  name _config_by_algo algo,41
DEDA_Class_2017_InputOutput,"Introduce importing packages, reading and writing files, using pandas to read and write structured data.",ERROR,Junjie Hu,DEDA_Class_2017_InputOutput,1," -*- coding: utf-8 -*-  Using ""import"" can input packages  modules)  .py files from defined paths  2 ways to import os  os module allows you to connect with your operation system  You can check  create  delete  rename your files and directories  Hint: path.exists )  listdir )  mkdir )  makedirs )  remove )  removedirs )  rename )  walk )  Give the package an alias numpy  1024  Instead of importing whole package  import only 1 method in the package pandas DataFrame  Using build-in function  open )  to open the file and using close ) to close the file  In python  we usually use syntax ""with open ) as container_name"" to load the content  There are 3 basic containers here:  read n) method will put n characters into a string  readline ) method will read one line once.  readlines ) method will put content into a list  every line is a string in the list  Using pickle to serialize data  save as binary format  Create a standard normal distribution data set matplotlib pickle  If you are writing plain text  you can use 'w'  If you want to save as binary format  you should use 'wb' pandas datetime  Pandas supports most of the common structured data formats  The read_csv method can take more arguments to satisfy your need  For example  you can specify the delimiter and decimal style  further more see: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html  Pandas will read files as DataFrame type  This is a very powerful data structure that you can do almost everything to the data.  For example  easily slicing rows and selecting columns  shape of DataFrame  sorting by value  check monotonicity increasing through time  sorting by index  reset index as numeric  add a row  Check null values  remove null values  duplicate a row  set a column as index  duplicates timestamp operations  operation  way1  potential risk  way2  drop by index  row operation  ~  take inverse  a simple build-in plot function of pandas  Save the new data as json format",323
SDA_2019_St_Gallen_PD_with_ML_and_Sentiment_Analysis_Machine_Learning,Implement machine learning algorithms for default prediction ,"machine learning, logistic regression, random forest, naive bayes classifier, adaptive boosting, decision tree, k-nearest neighbor classifier",ERROR,Machine_Learning,4," -*- coding: utf-8 -*- pandas sklearn.linear_model LogisticRegression sklearn.ensemble RandomForestClassifier sklearn.ensemble AdaBoostClassifier sklearn.svm SVC sklearn.naive_bayes GaussianNB sklearn.neighbors KNeighborsClassifier sklearn.naive_bayes MultinomialNB sklearn.tree DecisionTreeClassifier sklearn.model_selection GridSearchCV sklearn.metrics accuracy_score imblearn.pipeline Pipeline imblearn.over_sampling RandomOverSampler matplotlib matplotlib collections OrderedDict clasifier_names = [""Logistic Regression""  ""Random Forest""  ""Adaptive Boosting""  ""Support Vector Machines""  ""Naive Bayes""  ""K Nearest Neighbours""] classifiers = [LogReg  RandF  AdaBoost  SVM  NaivBay  Knn] parameters = [LogReg_para  RandF_para  AdaBoost_para  SVM_para  NaivBay_para  Knn_para] plot mean test scores plot mean train scores",73
SDA_2019_St_Gallen_PD_with_ML_and_Sentiment_Analysis_Machine_Learning,Implement machine learning algorithms for default prediction ,"machine learning, logistic regression, random forest, naive bayes classifier, adaptive boosting, decision tree, k-nearest neighbor classifier",ERROR,Machine_Learning,4," -*- coding: utf-8 -*- pandas sklearn.linear_model LogisticRegression sklearn.ensemble RandomForestClassifier sklearn.ensemble AdaBoostClassifier sklearn.svm SVC sklearn.naive_bayes GaussianNB sklearn.neighbors KNeighborsClassifier sklearn.naive_bayes MultinomialNB sklearn.tree DecisionTreeClassifier sklearn.model_selection GridSearchCV sklearn.metrics accuracy_score imblearn.pipeline Pipeline imblearn.over_sampling RandomOverSampler matplotlib matplotlib collections OrderedDict clasifier_names = [""Logistic Regression""  ""Random Forest""  ""Adaptive Boosting""  ""Support Vector Machines""  ""Naive Bayes""  ""K Nearest Neighbours""] classifiers = [LogReg  RandF  AdaBoost  SVM  NaivBay  Knn] parameters = [LogReg_para  RandF_para  AdaBoost_para  SVM_para  NaivBay_para  Knn_para] plot mean test scores plot mean train scores",73
SDA_2019_St_Gallen_PD_with_ML_and_Sentiment_Analysis_Machine_Learning,Implement machine learning algorithms for default prediction ,"machine learning, logistic regression, random forest, naive bayes classifier, adaptive boosting, decision tree, k-nearest neighbor classifier",ERROR,Machine_Learning,4," -*- coding: utf-8 -*- pandas sklearn.linear_model LogisticRegression sklearn.ensemble RandomForestClassifier sklearn.ensemble AdaBoostClassifier sklearn.svm SVC sklearn.naive_bayes GaussianNB sklearn.neighbors KNeighborsClassifier sklearn.tree DecisionTreeClassifier sklearn.model_selection GridSearchCV imblearn.pipeline Pipeline imblearn.over_sampling RandomOverSampler matplotlib matplotlib collections OrderedDict clasifier_names = [""Logistic Regression""  ""Random Forest""  ""Adaptive Boosting""  ""Support Vector Machines""  ""Naive Bayes""  ""K Nearest Neighbours""] classifiers = [LogReg  RandF  AdaBoost  SVM  NaivBay  Knn] parameters = [LogReg_para  RandF_para  AdaBoost_para  SVM_para  NaivBay_para  Knn_para] plot mean test scores plot mean train scores",69
SDA_2019_St_Gallen_PD_with_ML_and_Sentiment_Analysis_Machine_Learning,Implement machine learning algorithms for default prediction ,"machine learning, logistic regression, random forest, naive bayes classifier, adaptive boosting, decision tree, k-nearest neighbor classifier",ERROR,Machine_Learning,4, -*- coding: utf-8 -*- pickle pandas sklearn.linear_model LogisticRegression sklearn.ensemble RandomForestClassifier sklearn.ensemble AdaBoostClassifier sklearn.svm SVC sklearn.naive_bayes GaussianNB sklearn.neighbors KNeighborsClassifier sklearn.model_selection GridSearchCV sklearn.metrics accuracy_score load the data set a random state train model test model calculate scores print scores save classifier,39
kernel_density_estimation,Kernel Density Estimation Using Scikit-Learn,ERROR,Huong Vu 0856156,kernel_density_estimation,2,!/usr/bin/env python  coding: utf-8  In[75]: numpy scipy stats sklearn.neighbors KernelDensity sklearn.model_selection GridSearchCV matplotlib statsmodels.distributions.mixture_rvs mixture_rvs  In[76]:  Location  scale and weight for the two distributions  Sample from a mixture of distributions  In[91]:  create x_axis range the true distribution  KDE with Gaussian Kernel and different bandwidth  In[95]:  Plot the KDE for various bandwidths  Estimate the densities  Plot the true distribution  The scikit-learn library allows the tuning of the bandwidth parameter via cross-validation and returns the parameter value that maximizes the log-likelihood of data. The function we can use to achieve this is GridSearchCV )  which requires different values of the bandwidth parameter. The best model can be retrieved by using the best_estimator_ field of the GridSearchCV object.  In[106]:  In[107]:  Plot the KDE for optimal bandwidth  In the example above  a Gaussian kernel was used. Several other kernels are also available.  In[119]:  Plot the KDE for various kernels using optimal bandwidth  Estimate the densities  Plot the true distribution    ,155
SDA_20201023_hw3_WordCloud_JapaneseNews,ERROR,ERROR,"Junjie Hu, Andreas Rony Wijaya",SDA_20201023_hw3_WordCloud_JapaneseNews,2,!/usr/bin/env python  coding: utf-8  Create a word cloud out of Japanese news text from Yahoo  Author: Junjie Hu  Submitted by Andreas Rony Wijaya  Created time: 28.10.2020  In[1]:  In[2]: os pandas sudachipy tokenizer  pip3 install SudachiPy if package not found  CorePackage:  More information: https://github.com/WorksApplications/SudachiPy wordcloud WordCloud matplotlib  In[4]: TokenCleanText tker  mode  text  Read and Pre-process Data  Define Stopping Words  Define stop words and punctuation  not perfect!  Tokenize Text  In[6]:  Create WordCloud,70
TXTfpblexical,"Counts positive and negative words using the lexicon by Loughran and McDonald and the lexicon by Bing Liu. Then, the results are evaluated by using the training set of Malo et al. (2014) and computing a confusion matrix.
This training set is available at https://www.researchgate.net/publication/251231364_FinancialPhraseBank-v10 but a pickled and preprocessed version is available in this quantlet folder.
Please install required Python packages before usage: os, io, collections, nltk, pickle.","text mining, data mining, counts, sentiment",Elisabeth Bommes,TXTfpblexical,1," Please download the Financial Phrase Bank by  Malo  Pekka and Sinha  Ankur and Korhonen  Pekka and Wallenius  Jyrki and  Takala  Pyry  ""Good debt or bad debt""  Journal of the Association for Information Science and Technology  2014  https://www.researchgate.net/publication/251231364_FinancialPhraseBank-v10 os io pickle pandas  set path  functions tokenize txt scores row accuracy pred  actual subsample df  n  seed  lexical based wordcount words  dct collections Counter negwordcount words  dct  negdct  lngram nltk.util ngrams findneg word  wcneg lexcnt txt  pos_dct  neg_dct  negat_dct  lngram nltk word_tokenize  Count words in lexicon  Count negated words in lexicon  read data  change directory  load phrasebank  LM  negative dictionary  LM)  positive dictionary  LM)  BL  negative dictionary  BL)  positive dictionary  BL)",109
GAN_TS_recurrence,Time series simulation and analysis with GAN. Visualisation with recurrence plots.,"GAN, recurrence plot, time series, non stationarity, simulation",,GAN_TS_recurrence,5,pandas numpy matplotlib matplotlib rc seaborn datetime datetime __init__ self path scale_data self a b ts_image self n_rows n_cols  1 sequentialize the data ###########') last 3 rows flip to right dimension ts_matrix = np.flip ts_matrix axis=0) ts_matrix = np.flip ts_matrix axis=0) Plot1  Hide the right and top spines  Only show ticks on the left and bottom spines,57
GAN_TS_recurrence,Time series simulation and analysis with GAN. Visualisation with recurrence plots.,"GAN, recurrence plot, time series, non stationarity, simulation",,GAN_TS_recurrence,5,pandas numpy seaborn matplotlib PIL Image matplotlib rc matplotlib cm matplotlib.colors ListedColormap recurrence_plot ts window_size epsilon AR1_process alpha x0 mu sigma_e n_steps alt+5 for brackets sin start end num_steps wn n_steps make_transparent file Create custom color map ############### AR1  #################### Plot1  Hide the right and top spines  Only show ticks on the left and bottom spines ############## Sinus #################### Plot1  Hide the right and top spines  Only show ticks on the left and bottom spines ########## White Noise ################### Plot1  Hide the right and top spines  Only show ticks on the left and bottom spines,95
GAN_TS_recurrence,Time series simulation and analysis with GAN. Visualisation with recurrence plots.,"GAN, recurrence plot, time series, non stationarity, simulation",,GAN_TS_recurrence,5," -*- coding: utf-8 -*- __future__ print_function tensorflow.keras.datasets mnist tensorflow.keras.layers Input tensorflow.keras.layers BatchNormalization tensorflow.keras.models Sequential tensorflow.keras.optimizers RMSprop tensorflow google.colab files matplotlib sys numpy __init__ self  Following parameter and optimizer set as recommended in paper  Build and compile the critic  Build the generator  The generator takes noise as input and generated imgs  For the combined model we will only train the generator  The critic takes generated images as input and determines validity  The combined model   stacked generator and critic) wasserstein_loss self  y_true  y_pred build_generator self build_critic self train self  epochs  batch_size=32  sample_interval=100  Load the dataset  Rescale -1 to 1 X_train = 2*  X_train - X_train.min ))/ X_train.max )-X_train.min )))-1 X_train = np.expand_dims X_train  axis=3)  Adversarial ground truths  ---------------------   Train Discriminator  ---------------------  Select a random batch of images  Sample noise as generator input  Generate a batch of new images  Train the critic  Clip critic weights  ---------------------   Train Generator  ---------------------  Plot the progress  If at save interval => save generated image samples self.save_models ) sample_images self  epoch  Rescale images 0 - 1 gen_imgs = gen_imgs files.download ""rp_%d.png"" % epoch) write_losses self ax.legend ) save_models self",181
GAN_TS_recurrence,Time series simulation and analysis with GAN. Visualisation with recurrence plots.,"GAN, recurrence plot, time series, non stationarity, simulation",,GAN_TS_recurrence,5,numpy pandas matplotlib matplotlib rc matplotlib cm matplotlib.colors ListedColormap,9
GAN_TS_recurrence,Time series simulation and analysis with GAN. Visualisation with recurrence plots.,"GAN, recurrence plot, time series, non stationarity, simulation",,GAN_TS_recurrence,5,pandas numpy matplotlib matplotlib rc seaborn datetime datetime __init__ self path ts_image_and_rp_plot self n_rows n_cols epsilon  1 sequentialize the data  create a recurrence plot  reset the seqs_container Plot1  Hide the right and top spines  Only show ticks on the left and bottom spines,43
LDA-DTM_speech_xi_wordcloud,word cloud of the president of the People's Republic of China Xi Jinping's speech in 19th National Congress of the Communist Party of China,"LDA, word cloud, Xi Jinping, China, 19 Da",Xinwen Ni,LDA-DTM_Speech_XiJinping,2," coding:utf-8  please install the following module if you haven't yet. !pip install wordcloud !pip install jieba os path PIL Image numpy matplotlib wordcloud WordCloud os chnSegment  Please change the working directory to your path! os.chdir ""/Users/xinwenni/LDA-DTM/LDA-DTM_Speech_Xijiping"")   read the file   for Chinese    plotWordcloud.generate_wordcloud text)  Pass Text  store to file  to show the picture   ",52
LDA-DTM_speech_xi_wordcloud,word cloud of the president of the People's Republic of China Xi Jinping's speech in 19th National Congress of the Communist Party of China,"LDA, word cloud, Xi Jinping, China, 19 Da",Xinwen Ni,LDA-DTM_Speech_XiJinping,2," coding:utf-8 collections Counter os path jieba jieba.load_userdict path.join path.dirname __file__) 'userdict//userdict.txt')) # 导入用户自定义词典 word_segment text  计算每个词出现的频率，并存入txt文件  cut_all是分词模式，True是全模式，False是精准模式，默认False   fw.write ""%s""%dataDict)  返回分词后的结果  cut_all是分词模式，True是全模式，False是精准模式，默认False",21
CP2P_AAVEv1,Empirical analysis of on-chain data from Aave v1.,"CP2P, P2P Lending, Aave v1, Collateralized Loan, Blockchain",Ramona Merkl,CP2P_AAVEv1,2,!/usr/bin/env python  coding: utf-8  In[1]: pandas matplotlib matplotlib matplotlib.transforms Bbox numpy squarify seaborn matplotlib  In[2]:  In[3]:  In[4]:  In[5]: web3 Web3  In[6]:  In[7]:  In[8]:  number of coins borrwed per user  In[9]:  number of coins deposited per user  In[10]:  In[11]:  In[12]:  In[13]:  colormap for plots  coins by NUMBER  summarize small shares by 'other' coins by VALUE  summarize small shares by 'other'  In[14]: plt.title 'Borrowings by Count') squarify.plot loans_summary.Total  label=loans_summary.index  color=col  alpha=0.75) plt.savefig 'plots/popular_borrowings.png'  transparent=True) plt.close ) plt.title 'Borrowings by Value in USDT') squarify.plot loans_summary2.Total  label=loans_summary2.index  color=col  alpha=0.75) plt.savefig 'plots/popular_borrowings_vol.png'  transparent=True) plt.close )  In[15]: plot both plt.savefig 'plots/popular_borrowings.png'  transparent=True)  In[16]:  coins by NUMBER  summarize small shares by 'other' coins by VALUE  summarize small shares by 'other'  In[17]: plt.title 'Liquidations by Count') squarify.plot liqu_summary.Total  label=liqu_summary.index  color=col  alpha=0.75) plt.savefig 'plots/freq_liqu.png'  transparent=True) plt.close ) plt.title 'Liquidations by Value in USDT') squarify.plot liqu_summary2.Total  label=liqu_summary2.index  color=col  alpha=0.75) plt.savefig 'plots/freq_liqu_vol.png'  transparent=True) plt.close )  In[18]:  colormap for plots plot both plt.savefig 'freq_liquidations.png'  transparent=True)  In[19]:  coins by NUMBER  summarize small shares by 'other' coins by VALUE  summarize small shares by 'other'  In[20]: plt.title 'Deposits by Value in USDT') squarify.plot dep_summary2.Total  label=dep_summary2.index  color=col  alpha=0.75) plt.savefig 'plots/dep_vol.png'  transparent=True) plt.close )  In[21]:  colormap for plots plot both plt.savefig 'popular_deposits.png'  transparent=True)  In[22]:  In[23]: cax = ax.matshow PT1.T  interpolation='nearest'  cmap='seismic') cax = ax.matshow PT1.T  interpolation='nearest'  cmap='viridis') ax.xaxis.set_ticks_position 'bottom') ax.set_title 'Total Value in USDT')  In[24]: cax = ax.matshow PT2.T  interpolation='nearest'  cmap='seismic') cax = ax.matshow PT2.T  interpolation='nearest'  cmap='viridis') ax.xaxis.set_ticks_position 'bottom') ax.set_title 'Total Number of Liquidations') plt.savefig 'plots/freq_coin_combinations.png'  transparent=True)  In[25]: eth_liqu.head )  In[26]: plt.title '') fig.tight_layout )  plt.savefig '.png'  transparent=True)  In[27]:  Added block time  In[28]: fig  ax1 = plt.subplots figsize= 12 8)) plt.title '') fig.tight_layout )  plt.savefig 'plots/liqu_ETH_date_large.png'  transparent=True) plt.savefig 'plots/liqu_ETH_date_small.png'  transparent=True)  In[29]: price_eth = price[['ETH'  'USDC'  'USDT']] price_eth['ETH_USDT'] = price_eth.ETH/price_eth.USDT fig  ax1 = plt.subplots figsize= 16 9)) plt.title '') fig.tight_layout )  plt.savefig 'plots/dep_ETH_date_large.png'  transparent=True) plt.savefig 'plots/dep_ETH_date_small.png'  transparent=True)  In[30]: fig  ax1 = plt.subplots figsize= 16 9)) fig.tight_layout ) ,309
Regulatory_Complexity_Preprocessing,Preprocesses the scraped webpages into python dictionaries ready to use in the analysis.,"Regulatory_Complexity, preprocessing, html, string, regex, BeautifulSoup, scraper",Sabine Bertram,Regulatory_Complexity_Preprocessing,1,!/usr/bin/python  -*- coding: utf-8 -*- ###############################################################################  Preprocessing of HTM files  input:  - htm pages including the changes in the GBA  output: - pythonData: a dictionary of tuples with key: paragraphs  value:  date  list of sentences)          - modelData: a list of sentences for the vector learning process          - completeVersions: a dictionary of tuples with key: date  value: list of sentences ###############################################################################  imports os bs4 BeautifulSoup re datetime datetime translitcodec pickle natsort natsorted ###############################################################################  functions  clean test of h-class tags cleanH hList  sort paragraphs' subitems sortPar h1  h2  replace abbreviations such that splitting at . is possible cleanSents parList  abbrevs  repl print cleanWords  conduct sorting and cleaning of paragraph at once processPar h1  h2  abbrevs  repl  sort  remove numbering in beginning  split into sentences and clean words ###############################################################################  main  load pages and save paragraph #  date  list of sentences  paragraph number and date of enactment  exceptions  paragraphs that have been changed  print parNum  date  is this the oldest version?  if oldest version  save previous and new version  process paragraphs  save data  save modelData  if not  save only new version process paragraphs  save data  save modelData  paragraphs that have not been changed  save data  save modelData  reorder data to generate pythonData  reoder data by date  convert it to a sorted list  construct complete versions of the GBA  give up paragraphs  each version is only list of sentences  save data  generate word and sentence counts,231
normal_distribution_boxmuller,generate normal distribution using Box-Muller method,ERROR,Huong Vu 0856156,normal_distribution_boxmuller,2,!/usr/bin/env python  coding: utf-8  ## Drawing Normally-distributed samples with the Box-Muller method    Assume we have two independent standard Normal random Cartesian variables $X$ and $Y$ follows standard normal distribution. The joint distribution $p x y)$ is:  $$p x y) = p x)p y) = \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}\frac{1}{\sqrt{2\pi}}e^{-\frac{y^2}{2}} = \frac{1}{2\pi}e^{-\frac{x^2 + y^2}{2}} $$    In polar corordinates  $x^2 + y^2 = r^2$ where $x = r\cos \theta)$ $y = r\sin \theta)$    then $p x y) = \left   \frac{1}{2\pi} \right ) \left   e^{\frac{-r^2}{2}} \right )$    which is the product of two density functions  an exponential distribution over squared radii: $r^2 \sim Exp \frac{1}{2})$ and a uniform distribution over angles: $\theta \sim Unif 0 2\pi)$    Furthermore if we have:    $Exp \lambda) = \frac{-\log  Unif 0 1))}{\lambda}$    then $r \sim \sqrt{-2\log  Unif 0 1))}$    So in order to generate a normal distrubution we can do as follows:    1. Generate $u_1 u_2 \sim Unif 0 1)$  2. ransform the variables into radius and angle representation $r = \sqrt{-2\log u_1)}$   and $\theta = 2\pi u_2$   3. Transform radius and angle into Cartesian coordinates: $x = r \cos \theta)$  $y = r \sin \theta)$        In[4]: numpy random pylab show  In[5]:  transformation function gaussian u1 u2  In[8]:  uniformly distributed values between 0 and 1  In[11]:  plotting the values before and after the transformation,212
pyTSA_SimSARMA,"This Quantlet simulates ARMA(2,2) - autoregressive moving average process and draws the true ACF and PACF","time series,  stationarity, autocorrelation, PACF, ACF, simulation, stochastic process, ARMA, moving average, autoregression",ERROR,pyTSA_SimSARMA,1,pandas numpy statsmodels PythonTsa.plot_acf_pacf acf_pacf_fig matplotlib,6
P2P_Case_Study,Illustration of possible behaviour of LTV Ratios in a Crypto-based P2P Lending contract using historical Bitcoin prices,"bitcoin, crypto collateral, p2p lending, ltv ratio, collateralized loans",Ramona Merkl,P2P_Case_Study,2,!/usr/bin/env python  coding: utf-8  In[1]: pandas numpy matplotlib pylab plot  In[2]:  default='warn'  In[3]: btc_usd['Close'].plot )  In[8]: housing.loc[housing['TIME']=='2019-Q1']  In[9]: plt.plot housing['TIME'][56:]  housing['GER'][56:])  In[6]:  In[7]:  create subset  here for month of march  In[8]:  borrowed amount in USD  pre-agreed initial LTV  pre-agreed liquidation threshold  danger zone  BTC needed  in USD  In[9]:  calculate LTV ratios  In[11]:  In[13]:  In[14]:,54
DCA_manual_k-means,A manual k-means algorithm as alternative to the one provided by sklearn package,"manual, k-means, algorithm, clustering, data mining",Luisa Krawczyk,k-means-algorithms,1, -*- coding: utf-8 -*- ## k-means function as an alternative to the one available in sklearn   numpy numpy numpy linalg my_kmeans X  Y  niter_max  d dimensions  number of rows)  n observations  number of columns)  k clusters/ centroids  We will save the error  sum of squares) over the course of the algorithm  the following labels vector contains for each data point i the cluster that i is closest to  number of data points assigned to each cluster  computing the distance of i to each centroid:  increase of the sum of squares  computing the new centroids as means/ barycentres  I generate a random centroid matrix for initialization  run the algorithm,108
DEDA_Class_2017_MachineLearning,ERROR,ERROR,Junjie Hu,DEDA_Class_2017_MachineLearning,2,sklearn.datasets load_iris  More toy data sets from sklearn please refer:  http://scikit-learn.org/stable/datasets/ numpy sklearn tree graphviz sklearn.metrics accuracy_score  use this line in terminal if graphviz does not work: conda install python-graphviz   Glance the data set  name of features  data for the 4 features  name of the targets  data for the targets  randomly choose 1/3 of samples as testing data  training data  testing data  train the model  initial an decision tree classifier object with given arguments  A lot of arguments can be placed into the object  Refer docs: http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier  make prediction  visualizing the tree,92
DEDA_Class_2017_MachineLearning,ERROR,ERROR,Junjie Hu,DEDA_Class_2017_MachineLearning,2,"keras.datasets mnist keras.utils np_utils keras.models Sequential keras.layers.convolutional Convolution2D keras.layers Activation keras.layers.normalization BatchNormalization numpy time keras.layers SimpleRNN matplotlib keras.models load_model trainning_process model_history  Accuracy Increasing  Loss Decreasing  download the mnist to the path '~/.keras/datasets/' if it is the first time to be called  Parameter Specification  for reproducibility  data pre-processing  Normalize pixel data  Transform label set into binary representation  or so called ""One-hot encoding""  build RNN model  RNN model specification  Pool layer  Randomly deactivate neurons  Transform into 1 dimensional data  Full connected  Compile with defined parameters  training  Takes about half an hour to finish  The final cross validation accuracy is 99.22%  ============Load Trained Model=============",101
hedging_cc,Code infrastructure for paper Hedging Cryptocurrency Options,"BTC, hedging, cryptocurrency, cryptocurrency options",Jovanka Lili Matic,hedging_cc,34,!/usr/bin/env python  coding: utf-8  In[8]: qfin.utils bs_explicit_call numpy matplotlib  seed  model parameters  market parameters  implementation parameters integrate v0  path  strategy  rate=0.  dt=1/365 bs steps=100  dt=1/365  s0=1.  rate=0.  npaths=1  randomness  variance  calculate pnl,32
hedging_cc,Code infrastructure for paper Hedging Cryptocurrency Options,"BTC, hedging, cryptocurrency, cryptocurrency options",Jovanka Lili Matic,hedging_cc,34,"!/usr/bin/env python  coding: utf-8  In[1]: pandas numpy os datetime datetime matplotlib  # Concluding calibration remarks    - High volatility levels    - Indication for stochastic volatility    - Low jump indication    --------------------------------------    - Indication for infinite variation        ## Volatility comparison   In[2]:  In[3]:  In[4]:  In[5]:  In[6]:  In[7]: df = df[ df.index >= '2019-04-01') &  df.index < '2019-10-01')] df = df[ df.index >= '2019-10-01') &  df.index < '2020-02-01')] df = df[ df.index >= '2020-02-01') &  df.index < '2020-07-01')]  In[8]:  In[9]:  ### 3 separate pictures  In[10]:  In[11]:  In[12]:  In[13]:  In[14]:  Hide the right and top spines  Only show ticks on the left and bottom spines  In[15]: fig  ax = plt.subplots figsize= 10  4))  Hide the right and top spines  Only show ticks on the left and bottom spines  ## Volatility comparison BS vs MERTON  In[16]: sigmas.plot )  In[17]:  In[18]: sigmas['BLACK_SCHOLES'][df['BLACK_SCHOLES'].idxmax )] =np.mean df['BLACK_SCHOLES']) sigmas['MERTON'][df['MERTON'].idxmax )] =np.mean df['MERTON'])  In[19]:  In[20]:  ### ONE PICTURE   In[21]: sigmas['MERTON'].plot ax=ax  label=""Forecast""  linestyle='dashed' color='red' marker='*')  Hide the right and top spines  Only show ticks on the left and bottom spines     ### 3 PICTURES WITH DIFFERENT AXIS   In[22]: fig  ax = plt.subplots figsize= 10  4))  Hide the right and top spines  Only show ticks on the left and bottom spines  ## JUMP SIZE ANALYSIS $\lambda$  In[23]: 'MERTON'   In[24]:  In[25]:  In[26]:  ### ONE PICTURE   In[27]:  Hide the right and top spines  Only show ticks on the left and bottom spines  ### MULTIPLE PICTURES  In[28]:  In[29]:  Hide the right and top spines  Only show ticks on the left and bottom spines  In[30]:  Hide the right and top spines  Only show ticks on the left and bottom spines  ### RHO  In[31]:  In[32]:  In[33]:  In[34]:  In[35]:  In[36]: fig  ax = plt.subplots figsize= 10  4))  Hide the right and top spines  Only show ticks on the left and bottom spines  In[37]: fig  ax = plt.subplots figsize= 35  12))  Hide the right and top spines  Only show ticks on the left and bottom spines     In[38]: read_models models parameter Fix date index  Split list_split df  In[39]:  In[40]:  In[41]:  Hide the right and top spines  Only show ticks on the left and bottom spines     In[42]: fig  ax = plt.subplots figsize= 10  4))  Hide the right and top spines  Only show ticks on the left and bottom spines  In[43]:  In[44]:  In[45]:  In[46]:  In[47]:  In[48]:  In[49]:  In[50]: lambda_analysis['MERTON'].plot ax=ax  label=""Forecast""  linestyle='dashed' color='purple' marker='*')  Hide the right and top spines  Only show ticks on the left and bottom spines  In[51]:  In[52]: lambda_analysis['MERTON'].plot ax=ax  label=""Forecast""  linestyle='dashed' color='purple' marker='*')  Hide the right and top spines  Only show ticks on the left and bottom spines  In[53]:  In[56]:  Hide the right and top spines  Only show ticks on the left and bottom spines     In[ ]:  In[ ]:  In[ ]:  In[ ]:  In[ ]:",445
hedging_cc,Code infrastructure for paper Hedging Cryptocurrency Options,"BTC, hedging, cryptocurrency, cryptocurrency options",Jovanka Lili Matic,hedging_cc,34,!/usr/bin/env python  coding: utf-8  In[1]:  In[15]: pandas numpy matplotlib logging sys datetime datetime tqdm tqdm qfin.assets qfin.models BlackScholesModel qfin.hedges qfin.period Period qfin.volsurf VolatilitySurface qfin.utils bs_explicit_call  In[58]:  In[59]:  In[60]:  In[3]:  In[4]:  In[5]:  # Bullish  * 2 path models  * 7 hedge models  * 4 hedges  * 3 periods  * 3 strikes  * 2 maturities  In[7]:  'strikes': [4000]   'strikes': [8000]   'strikes': [10000]   In[8]: get_returns period  interval  In[9]:  # Calculate and store option prices  In[10]:  In[12]:  # Create hedges  In[13]:  model inputs  init kde model  init underlyings  # Initialize  load or generate) paths of all underlying used in hedging    start with spots  since the spot paths are required to generate the corresponding call paths  In[16]:  In[18]:  In[21]: multiprocessing  # Run the hedges  In[ ]: multiprocessing callback retval run hedge  # Single hedge separately  In[73]:  In[237]:  In[243]:  In[244]:  # Analyze  In[21]:  In[ ]:,139
hedging_cc,Code infrastructure for paper Hedging Cryptocurrency Options,"BTC, hedging, cryptocurrency, cryptocurrency options",Jovanka Lili Matic,hedging_cc,34,!/usr/bin/env python  coding: utf-8  In[9]: numpy pandas matplotlib datetime datetime scipy IPython.display display os tqdm tqdm  In[2]: qfin.models  In[3]:  In[4]:  plot style matters  In[5]:  In[6]:  SVCJ DeltaHedge  SVCJ DeltaGammaHedge  SVCJ DeltaGammaHedge  SVCJ MinimumVarianceHedge  KDE DeltaHedge  KDE DeltaGammaHedge  KDE DeltaGammaHedge  KDE MinimumVarianceHedge  In[19]: plot_hedge paths  models  hedge  period  strike  maturity  price  bounds= -2  2)      if fname[:-4] in bounds_dict:          bounds = bounds_dict[fname[:-4]]  In[20]:,61
hedging_cc,Code infrastructure for paper Hedging Cryptocurrency Options,"BTC, hedging, cryptocurrency, cryptocurrency options",Jovanka Lili Matic,hedging_cc,34,!/usr/bin/env python  coding: utf-8  In[1]: asyncio websockets json csv re datetime os  In[8]: pull_trades  call_api msg  while websocket.open:  do something with the response...  instrument parse pull_instruments  call_api msg  do something with the response... pull_book_summary  call_api msg  do something with the response...,41
hedging_cc,Code infrastructure for paper Hedging Cryptocurrency Options,"BTC, hedging, cryptocurrency, cryptocurrency options",Jovanka Lili Matic,hedging_cc,34,!/usr/bin/env python  coding: utf-8  In[22]: os pandas scipy stats numpy  In[54]:  In[65]:  In[66]:  In[46]:  In[ ]:,16
hedging_cc,Code infrastructure for paper Hedging Cryptocurrency Options,"BTC, hedging, cryptocurrency, cryptocurrency options",Jovanka Lili Matic,hedging_cc,34,"!/usr/bin/env python  coding: utf-8  In[4]: asyncio tardis_client TardisClient datetime datetime datetime timedelta os  In[5]:  messages as provided by Deribit real-time stream  sample ticker message  {    ""jsonrpc"": ""2.0""     ""method"": ""subscription""     ""params"": {      ""channel"": ""ticker.ETH-27DEC19-400-P.raw""       ""data"": {        ""underlying_price"": 181.88         ""underlying_index"": ""ETH-27DEC19""         ""timestamp"": 1569887997259         ""stats"": {          ""volume"": null           ""low"": null           ""high"": null        }         ""state"": ""open""         ""settlement_price"": 1.41         ""open_interest"": 16         ""min_price"": 1.145         ""max_price"": 1.307         ""mark_price"": 1.223004         ""mark_iv"": 109.02         ""last_price"": 1.195         ""interest_rate"": 0         ""instrument_name"": ""ETH-27DEC19-400-P""         ""index_price"": 181.19         ""greeks"": {          ""vega"": 0.17042           ""theta"": -0.10637           ""rho"": -0.91829           ""gamma"": 0.00198           ""delta"": -0.88711        }         ""bid_iv"": 0         ""best_bid_price"": 0.001         ""best_bid_amount"": 8         ""best_ask_price"": 0         ""best_ask_amount"": 0         ""ask_iv"": 0      }    }  }  In[10]: pull_daily_quotes start_date  end_date replay  pull_quotes ",106
hedging_cc,Code infrastructure for paper Hedging Cryptocurrency Options,"BTC, hedging, cryptocurrency, cryptocurrency options",Jovanka Lili Matic,hedging_cc,34,!/usr/bin/env python  coding: utf-8  In[1]: numpy pandas matplotlib datetime datetime scipy IPython.display display os tqdm tqdm  In[2]: qfin.models  In[3]:  In[13]:  In[14]:  SVCJ DeltaHedge  SVCJ DeltaGammaHedge  SVCJ DeltaGammaHedge  SVCJ MinimumVarianceHedge  KDE DeltaHedge  KDE DeltaGammaHedge  KDE DeltaGammaHedge  KDE MinimumVarianceHedge  In[34]: plot_hedge_boxplot paths  models  hedge  period  strike  maturity  price  bounds= -2  2)  In[35]:,50
hedging_cc,Code infrastructure for paper Hedging Cryptocurrency Options,"BTC, hedging, cryptocurrency, cryptocurrency options",Jovanka Lili Matic,hedging_cc,34,!/usr/bin/env python  coding: utf-8  In[ ]:  In[20]: math numpy scipy numpy pandas matplotlib scipy os scipy.interpolate InterpolatedUnivariateSpline qfin.utils bs_explicit_call  In[21]: integrate strategy  paths  initial_price  rate  dt=1/365  In[22]:  In[23]:  generate paths  # Delta hedging  In[27]:  setup initial price  integrate  liabilities  relative hedge  plot pnl  # Delta and gamma hedging  In[11]:  setup initial price  integrate  liabilities  relative hedge  plot pnl,58
hedging_cc,Code infrastructure for paper Hedging Cryptocurrency Options,"BTC, hedging, cryptocurrency, cryptocurrency options",Jovanka Lili Matic,hedging_cc,34,"!/usr/bin/env python  coding: utf-8  In[16]: pandas os matplotlib scipy datetime numpy  In[17]: qfin.models qfin.vs from_tickers  In[47]:  In[50]:  In[51]: obtain_market_data fname  model mean_confidence_interval data  confidence=0.95  In[52]:  In[56]: plot_parameters model  ax.set_title f""PARAMETERS {model.name}"")  In[58]: plot_rmse model  ax.set_title f""RMSE {model.name}"") cccccc"") cccccc"")  In[59]:",40
hedging_cc,Code infrastructure for paper Hedging Cryptocurrency Options,"BTC, hedging, cryptocurrency, cryptocurrency options",Jovanka Lili Matic,hedging_cc,34,"!/usr/bin/env python  coding: utf-8  In[1]: pandas numpy datetime datetime matplotlib arch  In[2]:  In[3]:  In[4]:  In[5]:  In[6]: print res.summary ))  In[7]: .set_index 'Date')  In[9]: df      = df.reset_index )  In[11]: ax.set_ylabel """")  In[ ]:  In[ ]:  In[ ]:",36
hedging_cc,Code infrastructure for paper Hedging Cryptocurrency Options,"BTC, hedging, cryptocurrency, cryptocurrency options",Jovanka Lili Matic,hedging_cc,34,!/usr/bin/env python  coding: utf-8  In[23]: pandas numpy scipy sklearn.neighbors KernelDensity matplotlib  In[2]:  In[3]:  In[20]:  In[5]:  In[6]:  In[13]:  In[22]:  In[21]:  Hide the right and top spines  Only show ticks on the left and bottom spines fig.savefig 'hd_kde.png' transparent=True)  In[ ]:,39
hedging_cc,Code infrastructure for paper Hedging Cryptocurrency Options,"BTC, hedging, cryptocurrency, cryptocurrency options",Jovanka Lili Matic,hedging_cc,34,"!/usr/bin/env python  coding: utf-8  In[1]:  In[2]: numpy matplotlib mpl_toolkits.mplot3d Axes3D matplotlib cm datetime datetime re os pandas scipy.interpolate scipy.optimize minimize qfin.volsurf VolatilitySurface qfin.utils bs_explicit_call qfin.volsurf.filters  # Load volatility surface and plot SVI fits  In[3]:  In[4]:  plot 3d  cutoff volsurf at the bottom at this ttm  ax.set_title f""DATE = {date_str}  PERIOD = {period_name}"")  # Calculate IV for all paths  In[ ]:  # Examine SVI fits  In[ ]:  In[ ]:  In[ ]:  In[ ]:",72
hedging_cc,Code infrastructure for paper Hedging Cryptocurrency Options,"BTC, hedging, cryptocurrency, cryptocurrency options",Jovanka Lili Matic,hedging_cc,34,!/usr/bin/env python  coding: utf-8  In[1]: numpy pandas  In[2]:  In[3]:  In[4]: df = df.loc[:  df.columns[3:]]  In[5]:  In[6]:  In[7]:  In[8]:  In[9]: for BS del df['count']  In[10]: for BS df_frames = [df1_sum['SIGMA'] df2_sum['SIGMA'] df3_sum['SIGMA']] df = pd.DataFrame df_frames  index=['BULLISH'  'CALM'  'COVID']) columns = df.columns df = pd.DataFrame np.round df_frames  2)  index=['BULLISH'  'CALM'  'COVID']  columns=columns) del df['count'] df  In[ ]:,56
hedging_cc,Code infrastructure for paper Hedging Cryptocurrency Options,"BTC, hedging, cryptocurrency, cryptocurrency options",Jovanka Lili Matic,hedging_cc,34,"!/usr/bin/env python  coding: utf-8  In[3]: pandas numpy os re  In[4]:  In[5]:  In[6]:  In[7]:  stats = ['mean'  'std'  'skew'  'kurt'  '1%'  '25%'  '50%'  '75%'  '99%']  df = df[df['maturity'] == '90']  In[12]:  ""  paths  period  ""\n\n"")      odf.columns = ['name'  'mean'  'std'  'skew'  'kurt'  '1\%'  '25\%'  '50\%'  '75\%'  '99\%']  In[ ]:",48
hedging_cc,Code infrastructure for paper Hedging Cryptocurrency Options,"BTC, hedging, cryptocurrency, cryptocurrency options",Jovanka Lili Matic,hedging_cc,34,!/usr/bin/env python  coding: utf-8  In[1]: pandas numpy os  In[2]:  In[3]: get_futures date  In[4]:  In[5]:  In[6]:  In[ ]:  In[ ]:  In[ ]:,21
hedging_cc,Code infrastructure for paper Hedging Cryptocurrency Options,"BTC, hedging, cryptocurrency, cryptocurrency options",Jovanka Lili Matic,hedging_cc,34,"!/usr/bin/env python  coding: utf-8  In[1]:  note that the package multiprocess is on purpose instead of multiprocessing   multiprocess is a fork which works with jupyter  In[2]:  In[3]: pandas matplotlib numpy math re multiprocess time os warnings logging io csv sys datetime datetime logging  In[4]: qfin.utils bs_iv qfin.models qfin.plot plot_smiles qfin.volsurf VolatilitySurface qfin.volsurf.filters qfin.calibration calibrate  In[5]:  In[99]:  # Calibration  In[103]:  In[180]:  KAPPA  RHO  V0  VBAR  XI  LAMBDA  MU_Y  SIGMA_Y  MU_V  In[174]:  dates = [""20200204""  ""20200318""]  In[175]: calibrate_day date  model  x0  reg  filters  output parameters  output errors  create rows  create df  In[ ]: callback retval  take other model parameters as initial value x0  take min IV for BS  take BS sigma**2 for HESTON parameters V0 and VBAR  take BS sigma for MERTON  take BS sigma for VARIANCE_GAMMA  take Heston parameters for SVJ  take SVJ parameters for SVCJ  In[61]:  market_vs.calibrate )      model_vs.calibrate )",139
SFERealDataExAR,Investigates time series of real financial data and finds an appropriate autoregressive model,"time series, AR, autoregressive model, real data, financial data, python","Bharathi Srinivasan, David Berscheid",SFERealDataExAR,1,AR_MSFT  os sys pandas pandas_datareader numpy statsmodels statsmodels statsmodels scipy arch arch_model matplotlib matplotlib  raw adjusted close prices  log returns tsplot y  lags=None  figsize= 10  8)  style='bmh' mpl.rcParams['font.family'] = 'Ubuntu Mono'  Select best lag order for MSFT returns,38
VISAoutliers,ERROR,ERROR,Jane Hsieh (Hsing-Chuan Hsieh),VISAoutliers,1,"!/usr/bin/env python3  -*- coding: utf-8 -*- numpy pandas matplotlib os change your current working directory  parameters -----------------------------------#- 'Japan'  'Australia'  'USA'          #-  ----------------------------------------------#-  ====================================  0. Input data: FRM prices ==================================================== 0.1 Reorganize data ## reset Datetime index from earliest to latest date  daily)  in case of missing dates ## Check if any missing date  missing data) df['Tran Count'].plot figsize= 30 10)) plt.show )  ====================================  1. Anomaly Detection: Find Anomalies Using Mean +/- 1.96*SD ====================================================  1.1 Calculate the past statistics  mean  std)  with a window size  i.e.  past window days [including current day] of data would be summarized)  Note: to reduce lag effect  instead of using 'past window'  you can use the window centored at current day  center=True)  parameters -----------------------------------#- - True # or False              #- -  ----------------------------------------------#- <<<<<<<<<<<<<<<<<<<<<<< combine the column-wise data into df   upper bound of 95% confidence interval lower bound of 95% confidence interval # Find possiblee anomalies by Mean +/- 1.96*SD Is_Anomaly x  outlier  normal  1.2. Visualization of the results ----------------------------------------------------------------- legend = ax.legend loc=""upper right""  edgecolor=""black"") legend.get_frame ).set_alpha None) legend.get_frame ).set_facecolor  0  0  0  0)) ax.set_title f'{country} - TS plot detecting anomalies with windowsize {window}  center={str centered)})') <<<<<<<<<<<<<<<<<<<<<<<  ====================================  2. Anomaly Detection: Find Anomalies Using Median +/- 1.96*MAD ====================================================  1.1 Calculate the past statistics  median  MAD)  with a window size  i.e.  past window days [including current day] of data would be summarized)  Note: to reduce lag effect  instead of using 'past window'  you can use the window centored at current day  center=True)  parameters -----------------------------------#- -  ----------------------------------------------#- scipy stats x = df[:31] stats.median_abs_deviation x['Tran Count']) combine the column-wise data into df   upper bound of robust 95% confidence interval lower bound of robust 95% confidence interval # Find possiblee anomalies by Mean +/- 1.96*SD Is_Anomaly_MAD x  outlier  normal  1.2. Visualization of the results ----------------------------------------------------------------- legend = ax.legend loc=""upper right""  edgecolor=""black"") legend.get_frame ).set_alpha None) legend.get_frame ).set_facecolor  0  0  0  0)) ax.set_title f'{country} - TS plot detecting anomalies with windowsize {window}  center={str centered)})') <<<<<<<<<<<<<<<<<<<<<<<",323
DEDA_Class_SS2018_GoalsperTeam_in_WC,"Create Choropleth WorldMap to depict sum of all World Cup Goals by each team that ever participated in a WorldCup. Using plotly, a dropdown menu is created depicting for each World Cup the amount of goals shot by each team participating .","Choropleth, geopandas, plotly, dropdown",Alexander Munz,WorldMaps,1, -*- coding: utf-8 -*- pandas plot Upload necessary files Load information about the World Cup  rounds  teams etc.) Read all World Cup matches Read location of Countries participants per WC Delete countries from DataFrame which we have no data for Sum up Goals by Team for each World Cup Sum up overall Goals per Team List all Teams  Include wcvears Series to DataFrame include Teams to DataFrame Merge DataFrame Fill DataFrame with missing rows so that all countries are included for all World Cups Specify datasets for each Worldcup Define Choroplath map that are going to be plotted Define Dropdown Menu         Set layout for plotting Define data and layout for the fig and plot fig,115
DEDA_Class_SS2018_app_input,"An web application built in Python. From the interface written in HTML, user input of a Berlin housing offer in doubt is parsed and entered into the prediction model built from existing data. A probability of how similar the offer is to scams is returned on the submit page.","Berlin, housing, expats, classification, advertisement, deposit, scam, rent",Jessie Hsieh,WebApp,1,pandas numpy warnings sklearn.ensemble RandomForestClassifier textblob_de TextBlobDE flask Flask random randint clean_text text data_prep_to_predict eintrag  freiab  freibis  mitgliedseit  miete  groesse  area  text  features to prepare:  popular_area and miete_delta days_to_freiab days_to_rent polarity_de new_user keyword features train_model  index  submit   All these try-except statements are to make sure entries are input correctly. groesse = int request.form['groesse'])  up to this  all inputs are parsed correctly  then  .values.reshape 1 -1) to be added if the features is a DF 	app.debug = True,77
pyTSA_SimLLM,"This Quantlet plots monthly time series of returns of Procter and Gamble from 1961 to 2016 and  their ACF and PACF (Example, 2.4 Figures 2.8-2.9 in the book)","time series, autocorrelation, returns, ACF, PACF, plot, visualisation","Huang Changquan, Alla Petukhina",pyTSA_SimLLM,1,numpy matplotlib PythonTsa.RandomWalk RandomWalk_with_drift PythonTsa.plot_acf_pacf acf_pacf_fig,6
Polar_coordinate_exponential,Create a function that can generate exponential distribution from polar coordinate. The basic idea is from muller-box transformation.,"polar coordinate, exponential distribution, muller-box transform, normal distribution, generate",Muhaimin 20201113,Polar_coordinate_exponential,1, -*- coding: utf-8 -*- numpy matplotlib scipy stats generate_exponential n brenchmark line plot to compare with brenchmark density plot compare with brenchmark KStest,23
SFEAdfPower,Simulation study for the power and level of the Augmented Dickey-Fuller Test for different AR- and MA-parameters as well as different number of lags.,"ADF, time series, stationary, Augmented Dickey-Fuller, unit root, test, power, level, simulation","Lucas Umann, Julian Schneider",SFM_AdfPower,1, Required Packages numpy matplotlib prettytable PrettyTable statsmodels.tsa.stattools adfuller statsmodels.tsa.arima_process arma_generate_sample    ~~~~~Input parameters~~~~~  number of simulations:  length of time series:  Alpha-coefficients for AR 1)  Beta-coefficients for MA 1)  Minimum and maximum of lags  Significance level of test  Type of ADF-test        'c'     ----> constant only        'ct'    ----> constant and trend        'ctt'   ----> constant  linear and quadratic trend        'nc'    ----> no constant  no trend  Function that generates the specified number of simulations for an ARMA-Process of the specified length  alpha and beta are the coefficients for the autoregressive and moving average processes generate_arma_simulations alpha  beta  Check if inputs are valid  Simulate data  Function that calculates the p-values for the simulated data calculate_adf data  lag  Function that calculates the probability of the rejection test-decision calculate_rejection_probability data  lag  Creates the array of results create_results_table   Extracts only the results for the first and last lag summary_first_last_lag complete_table  Call main functions for computation of results  Create and visualize tables of results  Create plots of results  Creates the plot for level of test  Creates the plot for power of test,172
FRM_2_SteadyCoinStudy,"Selecting the coins for analysis
 1) Analysis period: 2017 Jan - 2018 Nov
 2) Input data: weekly market capitalization data of the top 200 coins
 3) Selecting criteria:
 3.1) Top 15 coins according to the average market cap
 Result: 15 coins ['BTC' 'ETH' 'XRP' 'BCH' 'LTC' 'EOS' 'ADA' 'XLM' 'MIOTA' 'TRX' 'BSV' 'USDT' 'DASH' 'BNB' 'NEO']
 Market Cap share of the selected coins: 0.899
 3.2) Consistent appearance in the top 200 chart based on the market cap
 Result: 28 coins ['BCN' 'BTC' 'BTS' 'DASH' 'DCR' 'DGB' 'DGD' 'DOGE' 'ETC' 'ETH' 'FCT' 'GNT' 'LSK' 'LTC' 'MAID' 'NEO' 'REP' 'SC' 'STEEM' 'STRAT' 'USDT' 'WAVES' 'XEM' 'XLM' 'XMR' 'XRP' 'XZC' 'ZEC']
 Market Cap share: 0.849
 3.3) coins satidfy 1) and 2) above
 Result: 8 coins  ['BTC' 'DASH' 'ETH' 'LTC' 'NEO' 'USDT' 'XLM' 'XRP']
 Market Cap share of the selected coins: 0.795
 4) Final Data Input
 ['BTC' 'ETH' 'XRP' 'BCH' 'LTC' 'EOS' 'XMR' 'NEO' 'MIOTA' 'DASH' 'ETC' 'ZEC']
 Market Cap share of the selected coins: 0.88","Cryptocurrency, Market Capitalization, Coin Selection, weekly Capitalizations, Plotly animation","Qi Wu, Seokhee Moon",FRM_2_SteadyCoinStudy,2,"!/usr/bin/env python  coding: utf-8  ## Step 1. Get the data set: Montly market cap   In[32]: numpy pandas  note: len date=151 weeks)  finalPr = pd.DataFrame columns=[""date""  ""coins""  ""MktCap""])  for i in range len date)):      data_y = pd.DataFrame columns=[""date""  ""coins""  ""MktCap""])      data_y[""date""] = np.tile date[i] data.shape[0])      data_y[""coins""] = data.iloc[: 2*i]      data_y[""MktCap""] = data.iloc[: 2*i+1]      data_y = data_y[1:]      finalPr = finalPr.append data_y  ignore_index = True)  finalPr['mon'] = final.mon  finalPrMon = finalPr.groupby ['mon'  'coins']).mean )  See the market share of the coins in the graph below. We can check the first few coins represent a good share of the total market.  In[33]: plotly  In[47]:  save animation to html file plotly  ## Step 2. Selecting coins for analysis  ### 2 - 1) Based on average Market Cap  Here 2017 Jan-2018 Nov)  Select 'n' coins according to the average market share. Here we tried with n = 15  30  50  and the result is as belows.  In[88]:  check the market share of the Steadycoins1 for the test period  In[50]:  In[56]:  save animation to html file plotly  ### 2 - 2) Based on the number of  appearance in the top 200 marketCap chart  Here 2017 Jan-2018 Nov)  In[91]:  based on number of appearance in the top 200 marketCap chart  In[92]:  In[93]: plotly  ### 2 - 3) The coins that satisfy the both of the conditions above  In[95]:  compare two steady coins  In[96]:  In[97]: plotly  ## Our data  In[99]:  data we have",234
pyTSA_ReturnsPG,"This Quantlet plots monthly time series of returns of Procter and Gamble from 1961 to 2016 and  their ACF and PACF (Example, 2.4 Figures 2.8-2.9 in the book)","time series, autocorrelation, returns, ACF, PACF, plot, visualisation","Huang Changquan, Alla Petukhina",pyTSA_SimSBM,1,pandas matplotlib PythonTsa.SimulSBM simulSBM  if seed is fixed  the same SBM is reproduced.  if fig = True  a SBM plot is automatically generated.,23
PDRPC_stock_data,"Empirical results for stock data. Clustering based on risk measuring variables, PCA or Factor Analysis. Select best asset from each cluster and calculate Portfolio Allocation","stocks, clustering, PCA, Factor Analysis, Portfolio Allocation",Judith Bender,PDRPC_stock_data,2,"!/usr/bin/env python  coding: utf-8  #Install and import packages  In[ ]:  In[ ]:  In[ ]:  In[ ]:  In[ ]:  navigate to atalaia directory  get modifications made on the repo  install packages requirements !pip install -r requirements.txt  install package  In[ ]:  In[ ]:  Import packages numpy pandas scipy scipy.stats norm sklearn sklearn.linear_model LinearRegression sklearn.cluster KMeans sklearn.preprocessing StandardScaler sklearn metrics sklearn.metrics.cluster adjusted_rand_score sklearn.decomposition PCA matplotlib mpl_toolkits.mplot3d Axes3D nolds hurst_rs random seaborn levy factor_analyzer pypfopt pypfopt EfficientFrontier  # Preparation  In[ ]: Features number of features   monthly update 2360:20=118 Feature Matrix  In[ ]:  In[ ]:  In[ ]:  In[ ]: number of stocks  including market average) number of timepoints  In[ ]: rolling window of logreturns and logreturns_np Features CAPM beta autocorrelation coefficient Hurst using nolds package fit levy stable distribution alpha stable gamma stable  # Video of Correlation Matrix  In[ ]:  Data ohne EURO bzw Sharpe und ohne market_avg  # PCA / FA and Clustering  In[ ]: Goal: find best k stocks with k is number of clusters number of clusters  In[ ]: Assigned clusters Matrix: for every model and every stock and every time: to which cluster belongs the stock? without market average num_stocks-1  because we do not want market_avg  In[ ]: For evaluation Silhouette coefficient: between -1  bad) and 1  good). Close to zero: overlapping clusters Calinski-Harabasz Index = Variance Ratio Criterion: higher better Davies-Bouldin-Index: close to zero is best  In[ ]:  Data without Sharpe Ratio and without market_avg Perform PCA Perform FA with varimax FA with 2 factors and Varimax rotation FA with 3 factors and Varimax rotation Plots for Video Clustering for selecting column Ass_Clusters[: model_num k] = kmeans.predict Input) Evaluation Evaluation  # Take best stock from each cluster based on Sharpe Ratio  In[ ]: One Weight Matrix for every Asset-Allocation = for every Portfolio without market_avg  In[ ]: Ass_Clusters.shape = 961  24  118 Data_mon.shape = 961  10  118 number of clusters  +1 because Clusters are named 0 1 2 3... only clusters with 5 or more elements are considered we need copy because see wrong code below no Sharpe is below -100000 Choose stock with maximum Sharpe from each cluster  # Weights of different strategies  In[ ]: Weights of equal weight strategy  In[ ]:  In[ ]: Weights of Max Sharpe portfolio  ""Markowitz"")  In[ ]: first: get relevant part of logreturns_np.  logreturns_np.shape =  961  2610) append False because logreturns_np includes market_avg  but Weights_ones doesn't  # Portfolio based on clustering according to criterions above  In[ ]:  because of: 4 Inputs x 2  kmeans  agg)) for km_PC2'  'ac_PC2'  'km_PC3'  'ac_PC3'  'km_FA2'  'ac_FA2'  'km_FA3'  'ac_FA3'  We have already calculated the optimal weights for each strategy Saved in Weights_MV  Weights_MS  Weights_EW  each has dimensions:  num_stocks-1  num_updates  num_models) just select weights from these according to criterions Criterions saved in Sil_coef  VarRatC  Dav_Bould with dim= num_models  num_updates) optimal number of clusters: save here argmax  i.e. [0 1 2] instead of [3 5 10] Weights for criteria and strategies Equal weights Mean Variance Max Sharpe for km_PC2'  'ac_PC2'  'km_PC3'  'ac_PC3'  'km_FA2'  'ac_FA2'  'km_FA3'  'ac_FA3'  range  such that we choose indices of all numbers of clusters i.e.[3 5 10] for each of the above models that means: ran =[0 8 16 24...]  ran= [1 9 17 25 ...] ... ran=[7 15 23 31 ...] Weights  In[ ]:  In[ ]:  #Figure: Best number of clusters according to criterions  In[ ]: New figure controls default text size fontsize of the title fontsize of the x and y labels fontsize of the x tick labels fontsize of the y tick labels fontsize of the legend ax[0].legend ['Silhouette'  'Variance Ratio'  'Davies Bouldin']  bbox_to_anchor= 1 1)) ax[4].legend ['Silhouette'  'Variance Ratio'  'Davies Bouldin']  bbox_to_anchor= 1 1))  # Cumulative Portfolio Wealth  In[ ]:  In[ ]: Cumulative Portfolio Wealth For equal-weights strategy Minimum volatility strategy Initial CPW is 1 Initial CPW is 1 Initial CPW is 1 Timeseries of sums of weighted logreturns i.e. t from 0 to 117*20+19 = 2359. Note: CPW[0]=1  In[ ]: timesteps km/agg and PC23/FA23: 8 sil  vrc  db EW  MV  MS Initial CPW is 1 0 0 = sil  EW 1 0 = vrc  EW 2 0 = db  EW 0 1 = sil  MV 1 1 = vrc  MV 2 1 = db  MV 0 2 = sil  MS 1 2 = vrc  MS 2 2 = db  MS  # Portfolio Diversification Index  In[ ]: WeightedLogret WeightMat  logret  In[ ]: WL_MV = WeightedLogret Weights_MV  logreturns_np) WL_optclust_db_mv = WeightedLogret Weights_optclust_db_mv  logreturns_np)  In[ ]: PDI WL PCA-Input: X.shape: n_samples  n_features  i.e. here: 2359  960   In[ ]:  #Figures  In[ ]: CPW to dataframe   In[ ]: Overview New Figure  In[ ]: EW cycler cycler ax[0 0].legend ['market avg'  'PC2' 'PC3' 'FA2' 'FA3'])  In[ ]: MV ax[0 0].legend ['market avg'  'PC2' 'PC3' 'FA2' 'FA3'])  In[ ]: MS ax[0 0].legend ['market avg'  'PC2' 'PC3' 'FA2' 'FA3'])  In[ ]: ax[0 0].legend ['3 clusters'  '4 clusters'  '5 clusters'  '6 clusters'  '7 clusters'                  '8 clusters'  '9 clusters'  '10 clusters'  '15 clusters'  '20 clusters'  'mark_avg'])  In[ ]: ax[0 0].legend ['3 clusters'  '4 clusters'  '5 clusters'  '6 clusters'  '7 clusters'                 '8 clusters'  '9 clusters'  '10 clusters'  '15 clusters'  '20 clusters'  'mark_avg'])  In[ ]: ax[0 0].legend ['3 clusters'  '4 clusters'  '5 clusters'  '6 clusters'  '7 clusters'                 '8 clusters'  '9 clusters'  '10 clusters'  '15 clusters'  '20 clusters'  'mark_avg'])  In[ ]:  In[ ]:  In[ ]:  # Sharpe Ratio  In[ ]:  # Certainty Equivalent  In[ ]: Certainty Equivalent: mean- variance*0.5*lambda  lambda fuer Risikoaversion  we take lambda=1  # Adjusted Sharpe Ratio  In[ ]: Adjusted Sharpe Ratio  In[ ]:  In[ ]:",902
CBR-FRD,Explainable ML method Case-Based Reasoning for financial risk detection,"Data-driven CBR System, Financial Risk Detection, Explainable AI, Clustering Analysis, Cause Induction in Discrimination","Wei Li, Georgios Sermpinis, Florentina Paraschiv",CBR-FRD,3,!/usr/bin/env python3  -*- coding: utf-8 -*- sklearn.ensemble ExtraTreesClassifier sklearn.feature_selection chi2 skrebate ReliefF get_importance X_train  y_train  name,16
CBR-FRD,Explainable ML method Case-Based Reasoning for financial risk detection,"Data-driven CBR System, Financial Risk Detection, Explainable AI, Clustering Analysis, Cause Induction in Discrimination","Wei Li, Georgios Sermpinis, Florentina Paraschiv",CBR-FRD,3,!/usr/bin/env python3  -*- coding: utf-8 -*- numpy pandas sklearn.preprocessing RobustScaler math time random sklearn.linear_model LogisticRegression sklearn.svm SVC sklearn.neighbors KNeighborsClassifier sklearn.tree DecisionTreeClassifier sklearn.ensemble RandomForestClassifier sklearn.model_selection train_test_split sklearn.metrics precision_score sklearn.model_selection KFold warnings timeit default_timer collections Counter numba jit numba cuda numba pyswarms logging  only show error sklearn preprocessing sklearn.preprocessing MinMaxScaler measure_others measure_others get_importance get_importance dist_computation w  p  p1  m  r q get_result k  dist  y_train  y_val  from sklearn.linear_model import Lasso compute_sim w  p  p1  m  ref  query  k  dist test w p p1 m q r get_result1 k  dist  y_train  y_test ############################################################################################## Importing the data ############################################################################################## ##############################################################################################  amount of fraud classes      Shuffle dataframe rows ############################################################################################ ############################################################################################ ############################################################################################## ############################################################################################## ############################################################################################# ############################################################################################## scale_imp importances ################################################################################################## #################################################################################################### get_f params pso  Dimension of X  Dimension of X opt_func X  number of particles ############################################################################################### ###############################################################################################    ############################################################################################### ############################################################################################### #################################################################################################   ##################################################################################################,132
CBR-FRD,Explainable ML method Case-Based Reasoning for financial risk detection,"Data-driven CBR System, Financial Risk Detection, Explainable AI, Clustering Analysis, Cause Induction in Discrimination","Wei Li, Georgios Sermpinis, Florentina Paraschiv",CBR-FRD,3,!/usr/bin/env python3  -*- coding: utf-8 -*-  Classifier Libraries sklearn.linear_model LogisticRegression sklearn.svm SVC sklearn.neighbors KNeighborsClassifier sklearn.tree DecisionTreeClassifier sklearn.ensemble RandomForestClassifier sklearn.naive_bayes GaussianNB sklearn.metrics confusion_matrix scipy.stats uniform sklearn.model_selection cross_val_score sklearn.metrics precision_score  Use GridSearchCV to find the best parameters. sklearn.model_selection GridSearchCV measure_others X_train  y_train X_test y_test  Logistic Regression   We automatically get the logistic regression with the best parameters.  KNears best estimator  k nearest   Support Vector Classifier  SVC best estimator  DecisionTree Classifier  tree best estimator ####################################################################################### ##############################################################################################     ####################################################################################### ,73
SDA_2020_St_Gallen_04_Forecasting,Using a sentiment indicator build from SNB monetary policy assessments to forecast Switzerlands GDP,"Linear Regression, Forecasting, ElasticNet, RandomForestRegressor, GridSearch","Stefan Diener, Jan Scheidegger",SDA_2020_St_Gallen_04_Forecasting,1,"pickle pandas numpy matplotlib pyplot pandas_datareader datetime sklearn.ensemble RandomForestRegressor sklearn.linear_model ElasticNet sklearn.metrics mean_squared_error sklearn.model_selection train_test_split  %% Download macro data -------------------------------------------------------------------------------------  import data from fred  sentiment indicator starts from 2000  Gross Domestic Product for Switzerland  CPMNACSAB1GQCH)  Registered Unemployment Rate for Switzerland  LMUNRRTTCHQ156N)  OECD based Recession Indicators for Switzerland from the Period following the Peak through the Trough    CHEREC)  Imports of Goods and Services in Switzerland  CHEIMPORTQDSMEI)  Exports of Goods and Services in Switzerland  CHEEXPORTQDSMEI)  Government Final Consumption Expenditure in Switzerland  CHEGFCEQDSMEI)  Private Final Consumption Expenditure in Switzerland  CHEPFCEQDSMEI)  import the sentiment indicator  transform date into quarter  Construct final DataFrame  remove NA caused by growth rate calculation  plot data  %% Model Prep  split data into X / Y  Split data into train / test  %% Model Train  specify models and parameters  gridsearch + predictions from best model  evaluate models  calculate scores print ""Score: ""  grid.score y_test  y_pred))  %% Plot results",148
Web Crawling and Latent Dirichlet Alocation (LDA) into seven topics,Collect >=130 abstracts from http://www.wiwi.hu-berlin.de/de/forschung/irtg/results/discussion-papers,ERROR,"FB, FS, JH, WKH, Dwilaksana",Web Crawling and Latent Dirichlet Alocation (LDA) into seven topics,1,"requests  take the website source code back to you urllib  some useful functions to deal with website URLs bs4 BeautifulSoup  a package to parse website source code numpy  all the numerical calculation related methods re  regular expression package itertools  a package to do iteration works pickle  a package to save your file temporarily pandas  process structured data os cwd_dir = sub_dir if os.path.exists sub_dir) else os.getcwd )  # the path you save your files  This link can represent the domain of a series of websites  abs_folder = cwd_dir + 'Abstracts/'  os.makedirs abs_folder  exist_ok=True)  get source code  parse source code print e) print link_list[5])  if paper[3][-3:] == 'txt':  if paper[3][-3:] == 'pdf':      abstract_parsed = soup paper_abstract_page.content)      main_part = abstract_parsed.find_all 'body')[0].text.strip )  with open abs_folder + f""{re.sub '[^a-zA-Z0-9 ]'  ''  paper[0])}.txt""  'w'  encoding='utf-8') as abs_f:      abs_f.write main_part) print e) print paper[3]) random re nltk os path nltk.stem WordNetLemmatizer nltk.stem.porter PorterStemmer nltk.corpus stopwords nltk.corpus stopwords nltk.stem.wordnet WordNetLemmatizer string gensim gensim corpora matplotlib pandas numpy d = os.getcwd ) doc_l.pop )[0]  Regex cleaning  keep only letters  numbers and whitespace  apply regex  apply regex  lower case  nltk clean doc  Importing Gensim  Creating the term dictionary of our courpus  where every unique term is assigned an index.  Converting list of documents  corpus) into Document Term Matrix using dictionary prepared above.  Creating the object for LDA model using gensim library  Running and Trainign LDA model on the document term matrix. print ldamodel.print_topics num_topics=3  num_words=5)) Change the number of topic  20 need to modify to match the length of vocabulary  print  df) print  zz) plt.imshow zz  cmap='hot'  interpolation='nearest') plt.show )  plt.title ""heatmap xmas song"") plt.savefig ""heatmap_abstract.png""  transparent = True  dpi=400)",272
Probability_Upper_Cutoff_Point,Calculate probability of the data that will lie outside of the upper cutoff point,"probability, cutoff, outliner, normal distribution, quantile",Susan,Probability_Upper_Cutoff_Point,2,!/usr/bin/env python  coding: utf-8  In[1]: matplotlib numpy pylab  In[2]:  Make the figures big enough for the optically challenged. convert data frame to array for plotting  In[4]: scipy stats  Calculate cdf with x=w  Calculate probablity for outside the cutpoint of w  In[ ]:,42
SC_topics_unlabelled,"This Quantlet is dedicated to clustering of the unlabelled open source Ethereum Smart Contracts. First, we encode our abstracts with Distilbert embeddings and using UMAP we reduce dimensionality reduction to perform K-Means, finally again using UMAP reduce dimensionality to 2, to be able to plot it.","DistilBERT, topic modelling, clustering, UMAP, K-Means, comments, source code, Solidity","Elizaveta Zinovyeva, Raphael Constantin Georg Reule",SC-topics-unlabelled,2,"!/usr/bin/env python  coding: utf-8  In[1]:  In[2]: numpy pandas nltk.tokenize word_tokenize nltk.corpus stopwords nltk.stem WordNetLemmatizer string punctuation collections Counter collections OrderedDict matplotlib sklearn.cluster KMeans sklearn.metrics silhouette_score sklearn.mixture GaussianMixture sentence_transformers SentenceTransformer umap matplotlib re seaborn sklearn.feature_extraction.text CountVectorizer  In[3]:  In[4]:  In[5]:  In[6]:  remove thouse without comments  In[7]:  Code based on the blogpost https://towardsdatascience.com/topic-modeling-with-bert-779f7db187e6    In[8]:  In[9]:  In[10]:  In[11]:  In[12]: plt.title ""The Silhouette coefficient method \nfor determining number of clusters\n"" fontsize=16) plt.show )  In[13]: plt.title ""The Davies-Bouldin score \nfor determining number of clusters\n""  fontsize=16) plt.show )  In[14]:  In[15]:  7     105  11     97  1      95  6      77  0      76  8      72  2      66  3      52  5      49  4      44  13     36  12     29  10     26  9      15  In[16]:  Prepare data  Visualize clusters  In[17]:  In[18]:  In[19]: c_tf_idf documents  m  ngram_range= 1  1)  In[20]: extract_top_n_words_per_topic tf_idf  count  docs_per_topic  n=10  In[21]:  In[22]:  In[23]:  In[24]:  Set y-axis label  In[ ]:",140
xinbei_bike_opendata,Extract open data of Xinbei bike,ERROR,Huong Vu 0856156,xinbei_bike_opendata,2,!/usr/bin/env python  coding: utf-8  In[1]: requests json pprint matplotlib numpy pandas  In[2]: requests json pprint  In[5]:  In[6]:  In[ ]:,19
SFEvolgarchest,"Reads the date, DAX index values, stock prices of 20 largest companies at Frankfurt Stock Exchange (FSE), FTSE 100 index values and stock prices of 20 largest companies at London Stock Exchange (LSE) and estimates the volatility of the DAX and FTSE 100 daily return processes from 1998 to 2007 using a GARCH(1,1) model.","data visualization, graphical representation, plot, financial, asset, stock-price, returns, time-series, dax, ftse100, index, descriptive-statistics, volatility, garch, index",Andrija Mihoci,QID-1430-SFEvolgarchest,1,numpy pandas matplotlib arch arch arch_model arch.univariate GARCH statsmodels.tsa.arima_model ARMA datetime log_returns df __init__ self feed_data self data fit_ARCH self data):#fit ARCH q return is scaled in order to facilitate convergence fit_ARMA_GARCH self data fit the GARCH with AR term,40
pyTSA_USbill,"This Quantlet simulates ARMA(2,2) - autoregressive moving average process and draws the true ACF and PACF","time series,  stationarity, autocorrelation, PACF, ACF, simulation, stochastic process, ARMA, moving average, autoregression",ERROR,pyTSA_USbill,1,numpy pandas matplotlib PythonTsa.plot_acf_pacf acf_pacf_fig statsmodels PythonTsa.Selecting_arma choose_arma statsmodels.tsa.arima_model ARIMA PythonTsa.ModResidDiag plot_ResidDiag scipy stats  leave the last 6 items for forecast comparison  ARIMA requires 'strings' for column names.  the const is significantly zero. typ = 'levels' means: predict the levels of the variable 'ly'. typ = 'linear' to predict 'dly',50
SFM_Class_2018_Clustering_of_SContracts,Clustering Ethereum Smart Contracts,"hash, algorithm, crix, cryptocurrency, bitcoin, neural network, trading, fintech, Ethereum","Raphael Constantin Georg Reule, Elizaveta Zinovyeva, Rui Ren, Marvin Gauer",SFM_Class_2018_Clustering_of_SContracts,2,!/usr/bin/env python  coding: utf-8  # Clustering of Smart Contracts  Place ./contracts_source_codes.csv' created in SFM_Class_2018_filter_SContracts to the root folder of this quantlet  ### Import modules  In[ ]: json pprint pprint pandas numpy keras.preprocessing.text Tokenizer keras.preprocessing.sequence pad_sequences matplotlib pyplot scipy.cluster.hierarchy dendrogram scipy scipy.cluster.hierarchy fcluster sklearn.decomposition PCA sklearn.neighbors DistanceMetric sklearn.manifold TSNE sklearn.cluster KMeans nltk.corpus stopwords  additional set up  ### Read Data Set and Prepare  In[ ]:  set to true if testing  In[ ]: clean_text text # Remove stop words  ### Clustering on comments of the source code  In[ ]:  text preprocessing  In[ ]:  ### Visualisation of Clusters using tSNE  t-distributed stochastic neighbor embedding)  In[ ]:  In[ ]:  In[ ]: print df.head 20)) plt.legend loc='upper center'  bbox_to_anchor= 1.45  0.8)  prop={'size': 10})  ### Clustering on source code  In[ ]: clean_text text # Remove stop words  text preprocessing  In[ ]:  In[ ]:  In[ ]: print df.head 20)) plt.legend loc='upper center'  bbox_to_anchor= 1.45  0.8)  prop={'size': 10}),149
SDA_2020_St_Gallen_01_Data_Collection,Scraping all the Swiss National Bank Monetary Policy Assessments from 2000 to 2020. Reading in the pdf files and save as excel file,"Webscraping, Textmining","Stefan Diener, Jan Scheidegger",SDA_2020_St_Gallen_01_Data_Collection,1, %% Setup os pandas numpy time datetime bs4 BeautifulSoup urllib requests pdf2image convert_from_path cv2 xlsxwriter  required dependency pytesseract  Notes - OCR installation:  1. tesseract must to be installed <-- download from https://github.com/UB-Mannheim/tesseract/wiki  2. german training data must be added during installation      can also manually be downloaded from https://github.com/tesseract-ocr/tessdata/blob/master/deu.traineddata     and be manually moved in folder: C:\ProgramData\Anaconda3\envs\*env_name*\Library\bin\tessdata)  Set working directory to file location  %% Main main   ***** 1) Scrape PDFs *****  parse the SNB website  or 'lxml'  preferred) or 'html5lib'  if installed ')  extract all links  only keep links with /en/mmr/reference/pre_  specify names of files  only take the last part of url for the name  remove the points  add pdf ending to name  specifiy download path  download the file to specific path downloadFile url  fileName  ***** 2) Read PDFs  ~1h runtime) *****  i) specify cols for final df  ii) loop through pdf files  iii) loop page-images and apply OCR  scale images to increase OCR accuracy  requires installation as specified at start of file  cut out 8 digit date  save final df  %% Run file,172
SDA_2020_St_Gallen_03_Sentiment_Analysis,Building a sentiment indicator with counts of positive and negative words of the SNB monetary policy assessments,"Textmining, Dictionary-based Sentiment Analysis, Classification","Stefan Diener, Jan Scheidegger",SDA_2020_St_Gallen_03_Sentiment_Analysis,1,"pandas os re numpy pickle matplotlib matplotlib  Set working directory to file location  1: FED Financial Stability dictionary 2017)  Cite as: Correa  R. Garud  K. Londono  J  and Mislang  N.  2017). ""Sentiment in central bank's financial stability reports""  https://www.federalreserve.gov/econres/notes/ifdp-notes/constructing-a-dictionary-for-financial-stability-20170623.htm  2: negate dictionary  import from 03_Cleaning_EDA the cleaned data df  Dictionary tone assessment will compare them by Index  need the numbers back)  Make 'date' column as the index of Data negated word tone_count_with_negation_check dict  article  %% Count the positive and negative words using dictionary lm_dict or fed_dict  use lm_dict otherwise  Sentiment Score normalized by the number of words  %%  Plot Sentiment analysis -------------------------------------------------------------------------------------------------------  save the sentiment indicator  every year  every month  format the ticks",113
SDA_2020_St_Gallen_02_Simulations,Plotting Results from the Monte Carlo Simulation,"QLearning, Trading Bot, BTC, Bitcoin, Timeseries, Technical Indicators","Tobias Mann, Tim Graf",SDA_2020_St_Gallen_02_Simulations,8,pandas numpy simulator strategies  create simulator initialize decision maker  read in data  start simulation --> This will take some time!!!  retrieve Portfolio performance over simulated time,26
SDA_2020_St_Gallen_02_Simulations,Plotting Results from the Monte Carlo Simulation,"QLearning, Trading Bot, BTC, Bitcoin, Timeseries, Technical Indicators","Tobias Mann, Tim Graf",SDA_2020_St_Gallen_02_Simulations,8,numpy pandas simulator __init__ self  environment make_decision self  row  The BTC == 0 restriction ensures no additional BTC will be bought after the initial buying decision is made __init__ self  environment change_critical_deviation self  new make_decision self  row  sell at market  buy at market __init__ self  environment  function for calculating a moving average with numpy arrays moving_average self  array  periods make_decision self  row  calculate the moving averages  sell at market  buy at market __init__ self  environment ExpMovingAverage self  values  window computeMACD self  x  slow  fast  signal  compute the MACD  Moving Average Convergence/Divergence) using a fast and slow exponential moving avg'  return value is emaslow  emafast  macd which are len x) arrays make_decision self  row  calculate macd only if we have enough values  calculate signal line only when we have enough macd values  sell at market  buy at market __init__ self  environment make_decision self  row  sell at market  buy at market,149
SDA_2020_St_Gallen_02_Simulations,Plotting Results from the Monte Carlo Simulation,"QLearning, Trading Bot, BTC, Bitcoin, Timeseries, Technical Indicators","Tobias Mann, Tim Graf",SDA_2020_St_Gallen_02_Simulations,8,numpy __init__ self set_space self  high=None  low=None __init__ self add_observation self  observation min_observations self low self high self __init__ self  a n self __init__ self  observationspace  actionspace step self  action  new state  reward  done  _ reset self __init__ self  env  could be improved to be feature dependent get_discrete_state self  state  treatment of dauf find_action self  discretestate learn self  reward  At this time the agent has already made a new observation  but did not act on it  thus -2  and -1) for observations   actions)  print  self.env.observationspace.states[-1])  print self.get_discrete_state  self.env.observationspace.states[-1] ) ) observe self  observation act self new_data self ready_to_learn self is_ready self epsilon self epsilon_decay_value self,105
SDA_2020_St_Gallen_02_Simulations,Plotting Results from the Monte Carlo Simulation,"QLearning, Trading Bot, BTC, Bitcoin, Timeseries, Technical Indicators","Tobias Mann, Tim Graf",SDA_2020_St_Gallen_02_Simulations,8,qlearning numpy  define features __init__ self  lag calculate self  observations __init__ self  lag calculate self  observations __init__ self  lag calculate self  observations __init__ self  periods calculate self  observations  lag is here defined as the short moving average  and long_ma is = 2 * lag __init__ self  lag calculate self  observations  we return ma_short / ma_long as opposed to returning the features seperately __init__ self  lag ExpMovingAverage self  values  window calculate self  observations  we return signal / macd_memory as opposed to returning the features seperately,84
SDA_2020_St_Gallen_02_Simulations,Plotting Results from the Monte Carlo Simulation,"QLearning, Trading Bot, BTC, Bitcoin, Timeseries, Technical Indicators","Tobias Mann, Tim Graf",SDA_2020_St_Gallen_02_Simulations,8,"multiprocessing pandas numpy simulator qlearning smartstrategies features tqdm tqdm perform_mc_simulation env  data  repetitions = 100  output=""./lastmontecarlosimulation.csv""  the q_table is asigned with the random starting values when the agent is initialized   thus the same ql.environment can be reused in the simulation but the agent needs to be initialized each time  define function for one simulation simple_simulation i  env  data  return_dict  each simulation enforces the process to use a different seed  otherwise the random numbers in use will be the same for each simulation  actionspace  observationspace",84
SDA_2020_St_Gallen_02_Simulations,Plotting Results from the Monte Carlo Simulation,"QLearning, Trading Bot, BTC, Bitcoin, Timeseries, Technical Indicators","Tobias Mann, Tim Graf",SDA_2020_St_Gallen_02_Simulations,8,"pandas numpy tqdm tqdm warnings __init__ self  memorize = False  The all_orders dict associates an order with its id  Each order itself is a dict with keys  - type: {market  limit}  - status:{active  filled  canceled}  - is_buy:{True  False}  - id:{self reference in the order book}  - and for limit orders with a limit value __assign_id__ self __new__order__ self orders_by_status self  status canceled_orders self change_order_status self  id  status new_marketorder self  quantity  is_buy = True new_limitorder self  price  quantity  is_buy = True __init__ self  memorize = False  The Transaction book is based on a dictionary of transactions  each transaction contains a quantity {int}  price:{decimal}  time:{datetime}  is_buy{True  False}  id: {int}  origin{int} __assign_id__ self process self  transaction  origin __init__ self initialize_decisionmaker self  decisionmakerclass random_market_price self  open  high  low  close process_orders self  time  ohlc  comission_rate=0  Check for open orders to be filled  A filled order needs to change its status from active to filled   aditionally an according transaction is added to the transactions book  update the transactionbook for trades  buy at market  buy at limit  sell at market  sell at limit  update the orderbook for trades performance self  Add later -> function to retrive realtime performance evaluation simulate_on_aggregate_data self  data  verbose=True  because the decision maker is initialized it can access the simulators orderbook  the function can take additional inputs __init__ self  usd = 10**6  btc = 0 value self  price buy self  time  quantity  price sell self  time  quantity  price raise Exception ""Using laverage  bitcoin order exceeds btc funds"") __update__ self  time  price portfolio_repricing self  data tearsheet self  data  summary[""Sharpe ratio""] =  repriced.returns.mean ) - repriced.price.pct_change ).mean )) /  repriced.returns.std ) - repriced.price.pct_change ).std )) current_ratio self  price _sanity self ratio self portfolio_over_time self usd self btc self __init__ self __init__ self  environment The decisiomaker class is a generic class so its functions are meant to be overwritten by actual trading logic make_decision self",309
SDA_2020_St_Gallen_02_Simulations,Plotting Results from the Monte Carlo Simulation,"QLearning, Trading Bot, BTC, Bitcoin, Timeseries, Technical Indicators","Tobias Mann, Tim Graf",SDA_2020_St_Gallen_02_Simulations,8,numpy pandas simulator qlearning __init__ self  environment assign_agent self  agent memory self ratios self features self balance self  ratio make_decision self  row  sell btc  buy btc,26
SDA_2020_St_Gallen_02_Simulations,Plotting Results from the Monte Carlo Simulation,"QLearning, Trading Bot, BTC, Bitcoin, Timeseries, Technical Indicators","Tobias Mann, Tim Graf",SDA_2020_St_Gallen_02_Simulations,8,"numpy pandas pandas DataFrame simulator strategies os os listdir os.path isfile  Additional imports for qlearning in the second part qlearning features smartstrategies tqdm tqdm multiprocessing os  Import Monte Carlo for last part montecarlo  SETUP FOR SIMPLE TRADING STRATEGIES -------------------------------------------------  Read in data  Define variables  Define stregies to be tested  SIMULATE MULTIPLE SIMPLE TRADING STRATEGIES ------------------------------------------------- dataframebycolumn column  GENERAL FUNCTIONS -------------------------------------------------  get files with a .csv ending find_csv_filenames path_to_dir  suffix="".csv""  merge function to get NaN at the beginning merge_basedonlength df1  df2  column_name  merge dfs merge_dfs path  filenames  column ## SAVE SHEETS FOR LATER -------------------------------------------------  Tearsheets: Save single sheets save_tearsheets  folder_time_name  Tearsheets: Merge and save all in one  Porfolios: Save signle sheets save_portfolios  folder_time_name  Porfolios: Merge and save all in one  Strategies: save merged df save_strategies  name  get values  merge dataframes and save them  save dataframe ## Simulations with a QLearning model ### -------------------------------------------------  set random seed to allow reproducable results  define actions  define small observationspace  define bigger observationspace  define features for small obsverationspace  define features for bigger obsverationspace  Build q-environment  Build agents  setup simulator  link simulator to smartbalancer  assign agent to smart balancer  start simulator  Show Portfolio Performance  save Portfolios Performance and Tearsheet of Q-Learning Agents: save_df df  folder_path  name  Perform Montecarlo Simulation",202
CSC_Dapp_plotting,Plotting DApps tSNE,ERROR,"Elizaveta Zinovyeva, Raphael Constantin Georg Reule",CSC_Dapp_plotting,2,!/usr/bin/env python  coding: utf-8  In[1]: numpy matplotlib os pandas sklearn.model_selection StratifiedKFold random sklearn metrics collections Counter argparse sklearn.feature_extraction.text TfidfVectorizer sklearn.preprocessing LabelEncoder sklearn.manifold TSNE sklearn.decomposition PCA seaborn  In[2]:  In[3]:  In[4]:  In[5]: stop_words    = 'english'   max 144018  t-SNE on 2-dimensions  Plot data with dimensionality reduction PCA F7F6F7')) plt.scatter tsne_X[: 0]  tsne_X[: 0]  c=y_color  marker='o') F7F6F7'),53
pyTSA_AirTempChange,"This Quantlet simulates ARMA(2,2) - autoregressive moving average process and draws the true ACF and PACF","time series,  stationarity, autocorrelation, PACF, ACF, simulation, stochastic process, ARMA, moving average, autoregression",ERROR,pyTSA_Trend,1,numpy matplotlib PythonTsa.RandomWalk RandomWalk_with_drift PythonTsa.plot_acf_pacf acf_pacf_fig  burnin: the number of observation at the beginning of the sample to drop.  Used to reduce dependence on initial values.,26
SFM_GEV_distribution,"Simulates the following GEV Distributions: Weibull, Frechet, Gumbel.","Weibull, Frechet, Gumbel, GEV, simulation",Daniel Traian Pele,SFM_GEV_Distribution,2,!/usr/bin/env python  coding: utf-8  In[83]: scipy.stats genextreme scipy.stats weibull_min numpy matplotlib  In[ ]:  In[ ]:,15
DEDA_Class_2018WS_Berlin_Property_Analysis_Clustering_Universities,"Use HDBSCAN clustering algorithm to cluster spatially heterogeneous university points in lat, lng space within Berlin, DE.","HDBSCAN, Clustering, Spatial Analysis, Real Estate",Alex Truesdale,DEDA_Class_2018WS_Berlin_Property_Analysis_Clustering_Universities,2, -*- coding: utf-8 -*- os time gmplot pandas clustering_module  Introduction.  Search for all university data; return as list of objects; create coordinate matrix of points.  First pass of clustering  greedy).  Find centremost points of clusters; reorder into new matrix.  Second pass of clustering to cluster the clusters.  Find centremost points of new clusters.  Identify non-clustered clusters.  Define lat long coordinate lists.  Change dir. to visualisation dir.  Initialise map.  Plot scatter points. 6A5ACD'  size = 130  marker = False) 07B3BC'  size = 230  marker = False)  Trigger write / compile method to create final map.,94
DEDA_Class_2018WS_Berlin_Property_Analysis_Clustering_Universities,"Use HDBSCAN clustering algorithm to cluster spatially heterogeneous university points in lat, lng space within Berlin, DE.","HDBSCAN, Clustering, Spatial Analysis, Real Estate",Alex Truesdale,DEDA_Class_2018WS_Berlin_Property_Analysis_Clustering_Universities,2, -*- coding: utf-8 -*- hdbscan pandas numpy shapely.geometry MultiPoint clusterer objects_in  collection_in  min_cluster_size  min_samples  first_run  Run algorithm with haversine distance and ball-tree algorithm.  Count clusters and organise as pd.Series.  Store cluster locations in external list for proofing against original data. get_centermost_point cluster,42
SFM_Week_effect_bitcoin,"Hypothesis: The bitcoin prices are likely to fall on Monday, i.e. the closing price of Monday is less than the bitcoin price of the previous Friday.","Bitcoin, Finance, Week effect, EMH anomaly",Dragos Cioata,SFM_Week_effect_bitcoin,2,!/usr/bin/env python  coding: utf-8  In[2]: Weekend Effect for Bitcoin Market Hipothesis: The bitcoin prices are likely to fall on Monday. Means the closing price of Monday is less than the bitcoin price of previous Friday.  Author: Dragos Cioata Submitted: 04 Mar 2019 import pandas numpy scipy stats warnings matplotlib datetime astropy.table Table statsmodels.stats.multicomp pairwise_tukeyhsd read csv data kepp de date and de close price make date a datetime object extract the name of the day print 5 rows  plot the evolution of the bitcoin close price print the  test restult for the hipothesis  In[ ]:,95
pyTSA_ARMA2,"This Quantlet simulates ARMA(2,2) - autoregressive moving average process and draws the true ACF and PACF","time series,  stationarity, autocorrelation, PACF, ACF, simulation, stochastic process, ARMA, moving average, autoregression",ERROR,pyTSA_ARMA2,1,numpy pandas matplotlib statsmodels statsmodels.tsa.arima_process arma_generate_sample PythonTsa.plot_acf_pacf acf_pacf_fig PythonTsa.Selecting_arma choose_arma statsmodels.tsa.arima_model ARMA PythonTsa.LjungBoxtest plot_LB_pvalue scipy stats  check stationarity  check invertibility  It is always a good idea to make the data a Series!,32
pyTSA_NaoARMA,"This Quantlet plots  the monthly mean of North Atlantic Oscillation (NAO) index since January 1950 till December 2019, ots ACF and PACF, further fits ARMA model and performs Ljung-Box statistics","time series,  stationarity, autocorrelation, PACF, ACF, ARMA, moving average, autoregression,  Ljung-Box",ERROR,pyTSA_NaoARMA,1,pandas matplotlib statsmodels PythonTsa.plot_acf_pacf acf_pacf_fig statsmodels.tsa.arima_model ARMA PythonTsa.LjungBoxtest plot_LB_pvalue  Define the path to the dataset  automatically become a Series  see below  noestimatedcoef = number of estimated coefficients  nolags = max number of added terms in LB statistic,37
Modeling_LSTM,Define a parameter grid for the LSTM and choose the optimal set of parameters that leads to a minimal root-mean-squared error.,"LSTM, parameter tuning, RMSE","Georg Velev, Iliyana Pekova",Modeling_LSTM,1,keras.callbacks EarlyStopping train_predict_evaluate params data train_test_split sklearn.model_selection ParameterGrid,8
SFM_EMH_tests,"Implements the statistics tests for the Efficient Market Hypothesis: Bartell Test, Ljung-Box Test, Variance Ratio Test, Von-Newmann Test.","Bartell, EMH, random walk, Variance Ratio Test, Ljung-Box, Von-Newmann",Daniel Traian Pele,SFM_EMH_Tests,2,!/usr/bin/env python  coding: utf-8  In[1]: pandas datetime numpy  We will look at stock prices over the past year  starting at January 1  2016 yahoo_fin.stock_info  This line is necessary for the plot to appear in a Jupyter notebook%matplotlib inline  Control the default size of figures in this Jupyter notebook  Change the size of plots  Plot the adjusted closing price of AAPL VRTest x k  Estimate the mean  Estimate the variance for the 1st order difference  Estimate the variance for the q-th order difference  The raw value of the VRT  Change the size of plots  In[ ]:,95
LLE_moebius,Plotting the Moebius band,ERROR,Elizaveta Zinovyeva,LLE_moebius,2,!/usr/bin/env python  coding: utf-8  In[1]: pandas  In[9]: numpy matplotlib mpl_toolkits.mplot3d Axes3D sklearn.manifold locally_linear_embedding sklearn.decomposition PCA seaborn os random sklearn.model_selection train_test_split sklearn.model_selection StratifiedKFold time lightgbm sklearn.metrics log_loss sklearn.neighbors KNeighborsClassifier gc sklearn.linear_model LogisticRegression math  In[10]:  seed function seed_everything seed = 42  set seed  In[11]:  Möbius band is based on the following code https://github.com/gt11799/mobiusband/blob/master/mobiusband.py  In[20]:  number of sample func_x v  u func_y v  u func_z v  u mobiusband  longthen z_axes  In[21]:  In[41]:  In[42]:  In[43]:  PCA on 2-dimensions  Plot data with dimensionality reduction PCA  In[45]:  In[46]:  LLE  Plot data with dimensionality reduction PCA  In[ ]:,91
SFEReturns,"Computes the first order auto correlation of the returns, squared returns and absolute returns as well as the skewness, kurtosis and Jarque-Bera test statistic for German blue chips, 1974 - 1996.",ERROR,ERROR,QID-3148-SFEReturns,1,"numpy pandas matplotlib statsmodels.stats.stattools jarque_bera msg1 = ""This program calculates the first order auto correlation of returns  squared returns and absolute returns and skewness  kurtosis and the Bera Jarque statistic for german blue chips  1974 - 1996"" print msg1) log_returns series corr_coeff df distr_properties df calculate skewness  kurtosis  value of JB  JB p-value Calculate Correlation Coefficients",56
pyTSA_USUnempGDP,"This Quantlet plots monthly time series of returns of Procter and Gamble from 1961 to 2016 and  their ACF and PACF (Example, 2.4 Figures 2.8-2.9 in the book)","time series, autocorrelation, returns, ACF, PACF, plot, visualisation","Huang Changquan, Alla Petukhina",pyTSA_USUnempGDP,1,numpy pandas matplotlib PythonTsa.plot_multi_ACF multi_ACFfig PythonTsa.plot_multi_Q_pvalue MultiQpvalue_plot,7
SFM_Implied_vola,"Estimates the implied volatility for the Black-Scholes formula, using the Newton-Raphson method.","Black-Scholes, Newton-Rapshon, volatility, implied",Daniel Traian Pele,SFM_Implied_vola,2,!/usr/bin/env python  coding: utf-8  In[17]: scipy.stats norm pandas numpy datetime see also http://www.codeandfinance.com/finding-implied-vol.html find_vol target_value  call_put  S  K  tau  r  f x) print  i  sigma  diff)  f x) / f' x)  value wasn't found  return best guess so far bs_price cp_flag S K tau r v q=0.0 bs_vega cp_flag S K tau r v q=0.0 market value strike price start date of the option end date of the option time to maturity asset price risk free rate  call option  In[ ]:,81
SDA_2020_Giessen_Bird_Fourier,Wiggly Animal,"optimisation, portfolio allocation, asset allocation, machine learning, graph theory, hierarchical risk parity, markowitz, minimum variance, risk based investing",Fabio Martin,SDA_2020_Giessen_WigglyAnimal,2,!/usr/bin/env python  coding: utf-8  In[ ]: matplotlib matplotlib animation numpy append matplotlib  elephant parameters  last one is the eye fourier t  C elephant t  p init_plot   draw the body of the elephant  create trunk move_trunk i  move trunk to new position  but don't move eye stored at end or array)  initial the elephant body  initialize trunk  In[1]:  In[2]:  In[ ]:  In[ ]:,62
Group_6_Project_HMM_Bitcoin_model_error,Calculation of the prediction error of the constructed model,"HMM, Bitcoin, prediction, RMSE, states","Changjie Jin, Ng Zhi Wei Aaron, You Pengxin, Chio Qi Jun, Zhang Yuxi",Group_6_Project_HMM_Bitcoin_model_error,1," -*- coding: utf-8 -*- os warnings logging itertools pandas numpy matplotlib hmmlearn.hmm GaussianHMM sklearn.model_selection train_test_split tqdm tqdm  Change plot style to ggplot  for better and more aesthetic visualisation) Read Bitcoin data  Compute the fraction change in close  high and low prices  which would be used a feature Set constants for model Construct logging  Set up the model Split the data for training and testing Report the log Prepare model input Report the log Fit the model Output the states  Compute all possible outcomes predict close prices for days get most probable outcome Predict close price     print ""Root Mean Square Error: ""  rmse)     print ""Normalised Root Mean Square Error: ""  nrmse)     print ""Mean Error: ""  mean_err)     print ""Normalised Mean Error: ""  mean_err/mean_price)",121
DCA_finding_highest_centroid_values,Evaluation of cluster coherence for a manual k-means algorithm by returning the 5 highest centroid values per cluster,"k-means, algorithm, clustering, data mining, centroids, quantlet",Luisa Krawczyk,manual_k-means_centroids,1, -*- coding: utf-8 -*- numpy numpy numpy linalg  d dimensions  number of rows)  n observations  number of columns)  specify the number of clusters  specify the centroid initialization  specify the number of iterations  computing the distances of i to each centroid max L  function returning the the higher element of a list and its index top L  function returning a list's the elements and its indexes in descending order  returning the 5 highest centroid values and the corresponding terms,78
DEDA_Class_SS2018_Worldcup_Predictions,"Use Multinomial Logistic Regression (LogReg) to predict the Outcome of the Games of the World Cup Matches 2018. Predictions are based on historical results since 1993. Predictions were adapted after the Group Stages with the teams that actually passed on to the Knockout Stages, including all the games played so far in the World Cup into the Training/Test data. The same was done after the Quarterfinals.","Scikit-learn, LogisticRegression, train-test-split",Alexander Munz,WorldCupPredictions,3, -*- coding: utf-8 -*- Import necessary packages pandas datetime sklearn.model_selection train_test_split sklearn.linear_model LogisticRegression random Disable warning Load necessary files Load information about the World Cup  rounds  teams etc.) Create Worldcup Groups Create list with World Cup Teams Load current FIFA Rankings Load historic FIFA Rankings Create a column monthyear for later modification Create another FIFA Ranking dataframe for later modification Establish Winner Filter games with World Cup Teams Transform date column into datetime Filter Data to games from Aug 1993 to recent matches Create column monthyear for later modification Add year and year_weight to include discount factor add game_weight to include for importance factor Drop columns that are not needed for the multinomial logit regression Add column that turns winning/draw/losing into integer  home perspective) Add column that turns winning/draw/losing into integer  away perspective) Data cleanin: Replace team names so that dataframes are compatible Drop matches against opponent without ranking index Define dataframes of home team and away team perspective Set goals scored and goales conceded for both dataframes Add Ranking for each country depending on date of the game Merge those Dataframes and add ranking difference Calculate weighted_goals scored and conceded  home) Calculte weighted_goals scored and conceded  away) Calculate weighted goal difference to account for home advantage  hypothesis: home_advantage shows no influence) filter both dataframes regarding games with World Cup Teams  home perspective) Insert expected goal scores  home) Drop unnecessary columns  home) filter both dataframes regarding games with World Cup Teams  away perspective) Insert expected goal scores  away) Drop unnecessary columns  away) Bring both dataframes together Create Dummies to apply Logistic Regression  Separate X and y sets  Separate train and test sets Train algorithm with data and test 5t Sh6w accuracy of training and test set Store Group Stage Games  We only need the group stage games  so we have to slice the dataset Reference Set to know the games and the respective teams Ranking difference as a predictor for game outcome Expected goal difference for game outcome  Loop to add teams to new prediction dataset  Get dummy variables and drop winning_team column  Add missing columns compared to the model's training dataset  Remove winning team column Group matches and add points based on results Create Dataframe for Teams and their respective Points in the Groups Filter Dataframe to Teams that pass on to the Knockout Rounds  highest Points) Add Group Rank to each tean Prepare new dataset for predictions Get rid of unnecessary data Add rankings to Teams expected goal difference for game outcome Reference set Prepare new predicition container  Loop to add teams to new prediction dataset based on the ranking position of each team  Get dummy variables and drop winning_team column  Add missing columns compared to the model's training dataset  Remove winning team column  Create container for winners group matches and add points based on results add winner column to dataset Prepare dataset for quarterfinals Add Teams into quarterfinal Add rankings to dataset expected goal difference for game outcome Build reference Set Prepare new predicition container  Loop to add teams to new prediction dataset based on the ranking position of each team  Get dummy variables and drop winning_team column  Add missing columns compared to the model's training dataset  Remove winning team column  Create container to store Quarterfinal Winners group matches and add points based on results add winner column to dataset Prepare dataset for quarterfinals Aad winning teams to semifinal games Add rankings to dataset expected goal difference for game outcome Build reference Set Prepare new predicition container  Loop to add teams to new prediction dataset based on the ranking position of each team  Get dummy variables and drop winning_team column  Add missing columns compared to the model's training dataset  Remove winning team column  Build container to store winner and loser group matches and add points based on results add winner and loser column to dataset Prepare dataset for Finals and Playoff 3rd Place Get teams in both finals and playoff from the Semifinals and add them to the Worldcup final datasets Add rankings to dataset expected goal difference for game outcome Build reference Set Prepare new predicition container  Loop to add teams to new prediction dataset based on the ranking position of each team  Get dummy variables and drop winning_team column  Add missing columns compared to the model's training dataset  Remove winning team column group matches and add points based on results group matches and add points based on results,733
DEDA_Class_SS2018_Worldcup_Predictions,"Use Multinomial Logistic Regression (LogReg) to predict the Outcome of the Games of the World Cup Matches 2018. Predictions are based on historical results since 1993. Predictions were adapted after the Group Stages with the teams that actually passed on to the Knockout Stages, including all the games played so far in the World Cup into the Training/Test data. The same was done after the Quarterfinals.","Scikit-learn, LogisticRegression, train-test-split",Alexander Munz,WorldCupPredictions,3, -*- coding: utf-8 -*- Import necessary packages pandas datetime sklearn.model_selection train_test_split sklearn.linear_model LogisticRegression random Disable warning Load necessary files Load updated information about the World Cup  rounds  teams etc.) Create Worldcup Groups Create list with World Cup Teams Load current FIFA Rankings Load historic FIFA Rankings Create a column monthyear for later modification Create another FIFA Ranking dataframe for later modification Establish Winner Filter out games with World Cup Teams Transform date column into datetime Filter Data to games from Aug 1993 to recent matches Create column monthyear for later modification Add year and year_weight to include discount factor add game_weight to include for importance factor Drop columns that are not needed for the multinomial logit regression Add column that turns winning/draw/losing into integer  home perspective) Add column that turns winning/draw/losing into integer  away perspective) Data cleaning: Replace team names so that dataframes are compatible Drop matches against opponent without ranking index Define dataframes of home team and away team perspective Set goals scored and goales conceded for both dataframes Add Ranking for each country depending on date of the game Merge those Dataframes and add ranking difference Calculate weighted_goals scored and conceded  home) Calculte weighted_goals scored and conceded  away) Calculate weighted goal difference to account for home advantage  hypothesis: home_advantage shows no influence) filter both dataframes regarding games with World Cup Teams  home perspective) Insert expected goal scores  home) Drop unnecessary columns  home) filter both dataframes regarding games with World Cup Teams  away perspective) Insert expected goal scores  away) Drop unnecessary columns  away) Bring both dataframes together Create Dummies to apply Logistic Regression  Separate X and y sets  Separate train and test sets Train algorithm with data and test 5t Sh6w accuracy of training and test set  We only need the group stage games  so we have to slice the dataset Add rankings to dataset expected goal difference for game outcome Build reference Set Prepare new predicition container  Loop to add teams to new prediction dataset based on the ranking position of each team  Get dummy variables and drop winning_team column  Add missing columns compared to the model's training dataset  Remove winning team column  Build container to store winner and loser group matches and add points based on results add winner and loser column to dataset Prepare dataset for Finals and Playoff 3rd Place Get teams in both finals and playoff from the Semifinals and add them to the Worldcup final datasets Add rankings to dataset expected goal difference for game outcome Build reference Set Prepare new predicition container  Loop to add teams to new prediction dataset based on the ranking position of each team  Get dummy variables and drop winning_team column  Add missing columns compared to the model's training dataset  Remove winning team column group matches and add points based on results group matches and add points based on results,472
DEDA_Class_SS2018_Worldcup_Predictions,"Use Multinomial Logistic Regression (LogReg) to predict the Outcome of the Games of the World Cup Matches 2018. Predictions are based on historical results since 1993. Predictions were adapted after the Group Stages with the teams that actually passed on to the Knockout Stages, including all the games played so far in the World Cup into the Training/Test data. The same was done after the Quarterfinals.","Scikit-learn, LogisticRegression, train-test-split",Alexander Munz,WorldCupPredictions,3, -*- coding: utf-8 -*- Import necessary packages pandas datetime sklearn.model_selection train_test_split sklearn.linear_model LogisticRegression random Disable warning Load necessary files Load updated information about the World Cup  rounds  teams etc.) Create Worldcup Groups Create list with World Cup Teams Load current FIFA Rankings Load historic FIFA Rankings Create a column monthyear for later modification Create another FIFA Ranking dataframe for later modification Establish Winner Filter out games with World Cup Teams Transform date column into datetime Filter Data to games from Aug 1993 to recent matches Create column monthyear for later modification Add year and year_weight to include discount factor add game_weight to include for importance factor Drop columns that are not needed for the multinomial logit regression Add column that turns winning/draw/losing into integer  home perspective) Add column that turns winning/draw/losing into integer  away perspective) Data cleaning: Replace team names so that dataframes are compatible Drop matches against opponent without ranking index Define dataframes of home team and away team perspective Set goals scored and goales conceded for both dataframes Add Ranking for each country depending on date of the game Merge those Dataframes and add ranking difference Calculate weighted_goals scored and conceded  home) Calculte weighted_goals scored and conceded  away) Calculate weighted goal difference to account for home advantage  hypothesis: home_advantage shows no influence) filter both dataframes regarding games with World Cup Teams  home perspective) Insert expected goal scores  home) Drop unnecessary columns  home) filter both dataframes regarding games with World Cup Teams  away perspective) Insert expected goal scores  away) Drop unnecessary columns  away) Bring both dataframes together Create Dummies to apply Logistic Regression  Separate X and y sets  Separate train and test sets Train algorithm with data and test 5t Sh6w accuracy of training and test set  We only need the group stage games  so we have to slice the dataset Add rankings to dataset expected goal difference for game outcome Build reference Set Prepare new predicition container  Loop to add teams to new prediction dataset based on the ranking position of each team  Get dummy variables and drop winning_team column  Add missing columns compared to the model's training dataset  Remove winning team column  Create container to store Quarterfinal Winners group matches and add points based on results add winner column to dataset Prepare dataset for quarterfinals Aad winning teams to semifinal games Add rankings to dataset expected goal difference for game outcome Build reference Set Prepare new predicition container  Loop to add teams to new prediction dataset based on the ranking position of each team  Get dummy variables and drop winning_team column  Add missing columns compared to the model's training dataset  Remove winning team column  Build container to store winner and loser group matches and add points based on results add winner and loser column to dataset Prepare dataset for Finals and Playoff 3rd Place Get teams in both finals and playoff from the Semifinals and add them to the Worldcup final datasets Add rankings to dataset expected goal difference for game outcome Build reference Set Prepare new predicition container  Loop to add teams to new prediction dataset based on the ranking position of each team  Get dummy variables and drop winning_team column  Add missing columns compared to the model's training dataset  Remove winning team column group matches and add points based on results group matches and add points based on results,554
BTC_ANA_data_exploration,"Exploration of Scraped Addresses, Transaction Flow (Sankey Diagram), Transaction and Category Details, Time Series Chart and Transaction Categories. As well as analyzing the impact of different transaction categories on the Bitcoin Price through different metrics","Bitcoin, Blockchain, Transactions, Data Exploration, Data Visualization, Metrics, Sankey Diagram","David Steiner, Minh Nguyen",BTC_ANA_data_exploration,1," -*- coding: utf-8 -*- pandas seaborn matplotlib requests io  =============================================================================  LABELS / WALLETS  ============================================================================= Statistics to all scraped wallets Addresses per owner Statistics to all relevant wallets  scraped and predicted) Addresses per category known Addresses per category predicted Addresses per category all  =============================================================================  Transactions                  ============================================================================= merge_tnx_wallets tnx  wallets 1019673 Preprocessing 1.029 mio addresses 121023 transactions  =============================================================================  Remove Self Transactions  ============================================================================= self transactions 43484 self transactions Scatterplot  Date  Dollar  BTC)  =============================================================================  Sankey diagram  ============================================================================= pySankey sankey tnx = pd.read_csv ""../data/tmp/transactions_100BTC_labeled.csv""  index_col=False) #1019673 tnx = tnx.fillna 'unknown')  mode=""100BTC"" filt = 'unknown' Sankey diagram per entity n_transactions = 6.92 mio Sankey diagram per category  =============================================================================  Exploration  ============================================================================= Total Transactions per category Valid Transactions per category Dollar value per category Receiver transactions per owner Distribution of transactions dollar value Distribution of transactions market cap value Scatterplot  Date  Dollar  BTC) plt.xlim tnx_valid['block_timestamp'].min )  tnx_valid['block_timestamp'].max ))  =============================================================================  Transaction type  ============================================================================= preprocess_transaction_types df  category tnx_valid.columns.values Addresses per category   scatterplot by transaction type scatterplots by transaction type  =============================================================================  Price data  ============================================================================= get_price_data   =============================================================================  Time Series Chart  =============================================================================  =============================================================================  Analysis  Metrics  ============================================================================= datetime timedelta analytics df  price  mode='normal'",179
RL_CoinFigures,Draws figures of all coin prices used in the Thesis experiments relative to Bitcoin (from 07/2015 to 10/2018).,"reinforcement learning, neural network, machine learning, portfolio management, cryptocurrency",Ilyas Agakishiev,RL_CoinFigures,1,"pandas matplotlib matplotlib  Import from ""Database""",6
ActivationFunction,Train with keras several MLP with one hidden layer estimating a sinus function to compare activation functions and layer size (neurons number) effect.,"Dense, ANN, MLP, deep learning, neural network, activation function, hidden layer",Bruno Spilak,ANNSineWave,1,matplotlib matplotlib numpy keras keras.models Sequential keras.layers Dense from matplotlib.ticker import FormatStrFormatter  specify the sample size  specify the hyperparameters of the neural networks  activation functions list and sizes list of the hidden layer  neurons)  specify the hyperparameter for training the neural networks  The sample is interval in [0  2*pi]  Take sinus  For a mean squared error regression problem  we use RMSPROP algorithm  Train the model  iterating on the data in batches of 128 samples  uncomment to show the plot while running plt.show ),83
Hedging Effectiveness Plots,Hedging effectiveness results of optimal hedge ratio generated by minimizing expected shortfall 95% evaluated in different risk measures.,"Optimal hedge ratio, risk measures, BTC future, BTC, futures contract",Francis Liu,Hedging-Effectiveness-Plots,15,copulae1 KDEs toolbox json argparse matplotlib scipy scipy stats scipy.stats norm pandas numpy seaborn  from statsmodels.distributions.empirical_distribution import ECDF # don't use that os  Data source  Calibration Method  MM or MLE  moment conditions for MM  Parameters  Risk Measure Specification  Absolute risk aversion for exponential risk measure  Quantile level for expected shortfall  Quantile level for Value at Risk  To clip the h  Gaussian  t_Copula  Clayton  Frank  Gumbel  Plackett  Gaussian mix Indep  fix the maringals!  # Get List of csv files  ls = os.listdir data_path + 'train/')  ls = [l for l in ls if l.endswith '.csv')]  Placeholders for results  load paras  Get Best h  create csv for further combine with NIG factor copula,111
Hedging Effectiveness Plots,Hedging effectiveness results of optimal hedge ratio generated by minimizing expected shortfall 95% evaluated in different risk measures.,"Optimal hedge ratio, risk measures, BTC future, BTC, futures contract",Francis Liu,Hedging-Effectiveness-Plots,15,!/usr/bin/env python  coding: utf-8  In[1]: os  In[2]:  In[3]:  In[4]:  In[ ]:,11
Hedging Effectiveness Plots,Hedging effectiveness results of optimal hedge ratio generated by minimizing expected shortfall 95% evaluated in different risk measures.,"Optimal hedge ratio, risk measures, BTC future, BTC, futures contract",Francis Liu,Hedging-Effectiveness-Plots,15,numpy scipy stats scipy functools partial scipy integrate scipy.special gamma statsmodels.distributions.empirical_distribution ECDF scipy dill pathlib Path __init__ self  nu  Sigma  for cdf approximation use pdf self  *argv  mean = \vec{0} cdf_ref self  upper  reference computation of cdf  cdf approximation by http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.554.9917&rep=rep1&type=pdf equation 2; The equation was originally from Tong  1990) eq. 9.4.1 rvs self  size  Sample  Mean  correct as per paper Normal Inverse Gaussian Distributions and Stochastic Volatility Modelling __init__ self  delta  gamma rvs self  size pdf self  x __init__ self  alpha  beta  mu  delta  transformation=None  transoformation: aX + b pdf self  x cdf self  y mean self  Analytical var self std self skewness self kurtosis self normalise self  Standardised NIG for CF approximation use  normalise  Cumulants of the standardised NIG distribtion for later approximation use ppf_approx self  Zq  level 0  level 1  level 2  level 3 ppf_sampling_approx self  q_arr  size=5000000 rvs self  size ppf self  q ppf_approx2 self  q_arr  The main idea of this function is to leverage the numpy ability to to quick array operation.  First step: calculate the pdf using an array with equally spaced elements with values ranging from a min to a max.  Next step: calculate the pdf array to calculate percentage points using trapezoidal rule and nu.cumsum.  Third step: for each q in q_arr  search for the nearest percentage point and return the corresponding element in x_arr  create an array  search for the corresponding x_arr as quantile MGF self  z CGF self  z  Cumulants Generating Function CF self  z  Characteristic Function empirical_lambda u_arr  v_arr  q ERM_weight k  s __init__ self  alpha  beta  mu  sigma log_phi self  t pdf self  x rvs self  size          if removenan:              return Y[~np.isnan Y)][:size/3]          else:              return Y[:size/3] Variance rh ERM_estimate_trapezoidal k  rh ES q  rh VaR q  rh wrapper rs  rf  h  risk_measure optimize_h C  k_arr  q_arr_ES  q_arr_VaR risk_measures k_arr  q_arr_ES  q_arr_VaR  rh rh_PnL rh  def hedging_effectiveness h_arr  spot  future  k_arr  q_arr):  	results = np.ones  len h_arr)  1 + len k_arr) + len q_arr)))  	for i  h in enumerate h_arr):  		rh = spot - h * future  		results[i  :] = 1 - risk_measures k_arr  q_arr  rh) / risk_measures k_arr  q_arr  spot)  	return np.array [results[i  i] for i in range len h_arr))]) clip_h h  _min  _max  Usage columns = best_h_results_pd.columns  for c in columns:       best_h_results_pd.loc[: c] = best_h_results_pd.loc[: c].apply cap) __init__ self  data __call__ self  x,385
Hedging Effectiveness Plots,Hedging effectiveness results of optimal hedge ratio generated by minimizing expected shortfall 95% evaluated in different risk measures.,"Optimal hedge ratio, risk measures, BTC future, BTC, futures contract",Francis Liu,Hedging-Effectiveness-Plots,15,"numpy pandas scipy scipy stats scipy.stats norm scipy integrate scipy.special beta scipy  from statsmodels.distributions.empirical_distribution import ECDF matplotlib seaborn functools partial tqdm tqdm toolbox __init__ self H self  w  h  r_h  a helper function to compute the input to F_RF g self  w  h  r_h  to stablize the futher calulation F_RH self  h  r_h f_RH self  h  r_h _lambda self  q __init__ self  paras  Law_RS  Law_RF  Dependence Parameter  Marginal Distribution of Spot  Marginal Distribution of Future  Mean  COV C self  u  v  Copula Function c self  u  v  copula density          n = len u)          u[u==0] = 0.5/n          v[v==0] = 0.5/n          u[u==1] = 1-0.5/n          v[v==1] = 1-0.5/n          X1 = norm.ppf u)          X2 = norm.ppf v)          _c = 1/np.sqrt 1-self.rho**2)*np.exp - self.rho**2* X1**2+X2**2)-2*self.rho*X1*X2)/ 2-2*self.rho**2))          return _c D1C self  w  h  r_h l_fn self  rho  u  v dependency_likelihood self  u  v canonical_calibrate self  u  v  Mean  COV sample self  n sample_uv self  n):  # sample only the copula  Assuming uniform distribution of marginals tau self spearman_rho self Fisher_information rho mm_loss self  paras  u  v  q_arr  Mean  COV mm_calibrate self  u  v  q_arr  Mean  COV moment_conditions self  paras  u  v  q_arr  Mean  COV  Restore old paras  Mean  COV __init__ self  paras  Law_RS  Law_RF  nu_lowerbound  Dependence Parameter  Degree of Freedom  Marginal Distribution of Spot  Marginal Distribution of Future  DF  COV  inner  lowerest value permitted for degree of freedom C self  u  v  Copula Function c self  u  v  copula density D1C self  w  h  r_h  D1Operator l_fn self  rho  nu  u  v  nu_lowerbound=2  Likelihood Function  DF  COV  inner dependency_likelihood self  u  v canonical_calibrate self  u  v  DF  COV sample self  n Fisher_information rho  nu  I_rho  I_nu  I_rhonu tau self mm_loss self  paras  u  v  q_arr mm_calibrate self  u  v  q_arr moment_conditions self  paras  u  v  q_arr __init__ self  paras  Law_RS  Law_RF  Dependence Parameter  Marginal Distribution of Spot  Marginal Distribution of Future phi self  t phi_inverse self  t d_phi self  t d_phi_inverse self  t C self  u  v c self  u  v  copula density D1C self  w  h  r_h canonical_calibrate self  u  v l_fn self  theta  u  v  log dependency likelihood dependency_likelihood self  u  v sample self  n Fisher_information theta  Clayton  rho theta)  Trigamma is a special case of Huritz zeta function  Trigamma is a special case of Huritz zeta function tau self  Example 5.4 in Nelsen's Book mm_loss self  paras  u  v  q_arr mm_calibrate self  u  v  q_arr moment_conditions self  paras  u  v  q_arr __init__ self  paras  Law_RS  Law_RF  Dependence Parameter  Marginal Distribution of Spot  Marginal Distribution of Future phi self  t phi_inverse self  t d_phi self  t d_phi_inverse self  t D1C self  w  h  r_h C self  u  v c self  u  v):  # copula density  wiki missed the negative sign in front of theta l_fn self  theta  u  v  log dependency likelihood dependency_likelihood self  u  v canonical_calibrate self  u  v tau self  theta=None  Statistical modeling of joint probability distribution using copula: Application to peak  and permanent displacement seismic demands  Exercise 5.9 in Nelsen D1C_inv self  u  v  D1C's inverse sample self  size  prevent u2 > 1 mm_loss self  paras  u  v  q_arr mm_calibrate self  u  v  q_arr moment_conditions self  paras  u  v  q_arr __init__ self  paras  Law_RS  Law_RF  Dependence Parameter  Marginal Distribution of Spot  Marginal Distribution of Future phi self  t phi_inverse self  t d_phi self  t d_phi_inverse self  t C self  u  v c self  u  v  p.142 of Joe D1C self  w  h  r_h Gumbel_copula self  u  v  theta  Copula function for calibration l_fn self  theta  u  v  verbose=False  turn u==1 to a slightly smaller number to avoid inf dependency_likelihood self  u  v canonical_calibrate self  u  v sample self  size  samples of stable contain NaNs; Draw more samples and discard NaNs until len V) = size tau self mm_loss self  paras  u  v  q_arr mm_calibrate self  u  v  q_arr moment_conditions self  paras  u  v  q_arr C self  u  v c self  u  v Gumbel_copula self  u  v  theta  Copula function for calibration l_fn self  theta  u  v  verbose=False  the rotation is in effect in this function  no rotation is needed  turn u==1 to a slightly smaller number to avoid inf sample self  size  samples of stable contain NaNs; Draw more samples and discard NaNs until len V) = size _lambda self  q __init__ self  paras  Law_RS  Law_RF  Dependence Parameter  Marginal Distribution of Spot  Marginal Distribution of Future  from Joe's book  p. 141  Family B2) C self  u  v c self  u  v D1C_original self  u  v D1C self  w  h  r_h get_theta self  u  v sample self  size dependency_likelihood self  u  v canonical_calibrate self  u  v l_fn self  theta  u  v  log dependency likelihood  From Appendix C.7. of ""Extreme in Nature"" spearman_rho self mm_loss self  paras  u  v  q_arr mm_calibrate self  u  v  q_arr moment_conditions self  paras  u  v  q_arr __init__ self  paras  Law_RS  Law_RF  Marginal Distribution of Spot  Marginal Distribution of Future C self  u  v c self  u  v D1C self  w  h  r_h l_fn self  rho  p  u  v dependency_likelihood self  u  v canonical_calibrate self  u  v sample self  size spearman_rho self mm_loss self  paras  u  v  q_arr mm_calibrate self  u  v  q_arr moment_conditions self  paras  u  v  q_arr __init__ self  paras  Law_RS  Law_RF  Marginal Distribution of Spot  Marginal Distribution of Future C self  u  v c self  u  v l_fn self  rho1  rho2  p  u  v dependency_likelihood self  u  v canonical_calibrate self  u  v sample self  size spearman_rho self",891
Hedging Effectiveness Plots,Hedging effectiveness results of optimal hedge ratio generated by minimizing expected shortfall 95% evaluated in different risk measures.,"Optimal hedge ratio, risk measures, BTC future, BTC, futures contract",Francis Liu,Hedging-Effectiveness-Plots,15,"!/usr/bin/env python  coding: utf-8  In[1]: pandas numpy os matplotlib  ## Crix Calculation    From the published paper:  $$\text{CRIX}_t k  \beta) = \frac{\sum_{i=1}^k  \beta_{i 0}P_{it}Q_{i t_l^-}}  {\text{Divisor}_{t_l^-} k  \beta)}$$    $$  \text{Divisor}_{0} k  \beta) = \frac{\sum_{i=1}^k  \beta_{i 0}P_{i0}Q_{i 0}}  {\text{Starting Value}}   $$    $\text{Divisor}_{t_l^-} k  \beta)$ is not described in the original paper.       ## Self-financing CRIX  We use the ""weight"" given by Ilyas to calculate a self-financing CRIX.   The starting value is 1 000  meaning the initial investment to the portfolio of cryptos is 1 000 USD.   The 1 000 USD is used to purchase constitutents with the opening price of the period.           In[2]:  weights = weights.loc[ weights.index >= '20180530') &  weights.index <= '20210530')  :] # start day = 20180601 and end day = 20210530  In[3]:  In[4]:  In[5]: get_monthly_price df  y m  In[6]:  In[7]: read_price path  coin                  print 'already tz aware')  In[8]: starting value  In[10]:  In[571]:  In[538]:  In[539]:  In[525]:  In[524]: matplotlib  In[526]:  In[529]:  In[533]: os",151
Hedging Effectiveness Plots,Hedging effectiveness results of optimal hedge ratio generated by minimizing expected shortfall 95% evaluated in different risk measures.,"Optimal hedge ratio, risk measures, BTC future, BTC, futures contract",Francis Liu,Hedging-Effectiveness-Plots,15,matplotlib scipy scipy stats scipy.stats norm pandas numpy seaborn  from statsmodels.distributions.empirical_distribution import ECDF toolbox random K_Uniform u K_Triangle u K_Epanechnikov u K_Gaussian u sample_Uniform size  data  h sample_Triangle size  data  h sample_Epanechnikov size  data  h sample_Gaussian size  data  h __init__ self  data  kernel_name  bw=None cdf self  x  KDE's cdf should be a cdf of a mixture... pdf self  x      def pdf self  x):          s = np.sum self.kernel  x - self.data)/self.h_brot))          return s/ len self.data)*self.h_brot) rvs self  size ppf self  q plot_density self,82
Hedging Effectiveness Plots,Hedging effectiveness results of optimal hedge ratio generated by minimizing expected shortfall 95% evaluated in different risk measures.,"Optimal hedge ratio, risk measures, BTC future, BTC, futures contract",Francis Liu,Hedging-Effectiveness-Plots,15,!/usr/bin/env python  coding: utf-8  In[1]: matplotlib scipy scipy stats scipy.stats norm pandas numpy seaborn statsmodels.distributions.empirical_distribution ECDF copulae1 KDEs toolbox warnings itertools os  In[2]:  Gaussian  t_Copula  Clayton  Frank  Gumbel  Plackett  Gaussian mix Indep  fix the maringals!  In[3]:  fix the maringals!  In[4]: fn rho  In[5]: fn theta  In[6]: fn theta  In[7]: fn theta  In[8]:  In[10]:  force the Gaussian component to have rho_S=0.9  the corresponding para  In[11]:  plt.plot x_arr  f3  label=Copulae_names[2])  plt.plot x_arr  f4  label=Copulae_names[3])  plt.plot x_arr  f5  label=Copulae_names[4])  plt.plot x_arr  f6  label=Copulae_names[5])  In[25]:  In[59]:  axs[1 1].plot x_arr  f2  label=Copulae_names[1])  axs[1 2].plot x_arr  f3  label=Copulae_names[2])  axs[1 3].plot x_arr  f4  label=Copulae_names[3])  axs[1 4].plot x_arr  f5  label=Copulae_names[4])  axs[1 5].plot x_arr  f6  label=Copulae_names[5])  axs[1 6].plot x_arr  f7  label=Copulae_names[6])  In[12]:  plt.plot x_arr  np.log np.array f1))  label=Copulae_names[0])  plt.plot x_arr  np.log np.array f2))  label=Copulae_names[1])  plt.plot x_arr  np.log np.array f4))  label=Copulae_names[3])  plt.plot x_arr  np.log np.array f6))  label=Copulae_names[5])  plt.plot x_arr  np.log np.array f7))  label=Copulae_names[6])  In[62]:  plt.plot x_arr  np.log np.array f2))  label=Copulae_names[1])  plt.plot x_arr  np.log np.array f3))  label=Copulae_names[2])  plt.plot x_arr  np.log np.array f4))  label=Copulae_names[3])  plt.plot x_arr  np.log np.array f5))  label=Copulae_names[4])  plt.plot x_arr  np.log np.array f6))  label=Copulae_names[5])  In[14]:  In[15]:  In[16]:  In[17]:  In[ ]:,180
Hedging Effectiveness Plots,Hedging effectiveness results of optimal hedge ratio generated by minimizing expected shortfall 95% evaluated in different risk measures.,"Optimal hedge ratio, risk measures, BTC future, BTC, futures contract",Francis Liu,Hedging-Effectiveness-Plots,15,copulae1 KDEs toolbox json argparse matplotlib scipy scipy stats scipy.stats norm pandas numpy seaborn statsmodels.distributions.empirical_distribution ECDF os  Data source  Calibration Method  MM or MLE  moment conditions for MM  Load parameters  Path for reading OHR results and storing HE results  Get List of csv files  ls = os.listdir data_path + 'train/')  ls = [l for l in ls if l.endswith '.csv')]  read OHR  Combine OHR with that of NIG factor hedging_effectiveness rm  rs  rf  h wrapper_HE rm  file  h  insample=True,79
Hedging Effectiveness Plots,Hedging effectiveness results of optimal hedge ratio generated by minimizing expected shortfall 95% evaluated in different risk measures.,"Optimal hedge ratio, risk measures, BTC future, BTC, futures contract",Francis Liu,Hedging-Effectiveness-Plots,15,,0
Hedging Effectiveness Plots,Hedging effectiveness results of optimal hedge ratio generated by minimizing expected shortfall 95% evaluated in different risk measures.,"Optimal hedge ratio, risk measures, BTC future, BTC, futures contract",Francis Liu,Hedging-Effectiveness-Plots,15,copulae1 KDEs toolbox os json argparse  Data source  Calibration Method  MM or MLE  moment conditions for MM  Gaussian  t_Copula  Clayton  Frank  Gumbel  Plackett  Gaussian mix Indep  Get List of csv files  Placeholders for results  Calibration  Prettify the result  Save results as html for display  Save files to designated directories,49
RVOLaccur,Checks the predicting accuracy of various models through Diebold Mariano tests.,"cryptocurrency, RNN, LSTM, GRU, DM, test, Diebold, Mariano, HAR, FNN",Ivan Mitkov,RVOLaccur,2,!/usr/bin/env python3  -*- coding: utf-8 -*- warnings  Make sure that all of the modules are already installed in Anaconda. If not    the following comands should be ran in Terminal  Improting the packages pickle pandas pandas Series matplotlib numpy statsmodels.graphics.tsaplots plot_acf statsmodels statsmodels.tsa.stattools adfuller statsmodels.tsa.stattools adfuller statsmodels.stats.diagnostic itertools sys pylab scipy scipy.stats kurtosis scipy.stats skew scipy.stats norm scipy.stats jarque_bera numpy.random normal math sklearn.preprocessing MinMaxScaler sklearn.metrics mean_squared_error keras.layers keras optimizers keras.constraints NonNeg keras.layers.advanced_activations,71
RVOLaccur,Checks the predicting accuracy of various models through Diebold Mariano tests.,"cryptocurrency, RNN, LSTM, GRU, DM, test, Diebold, Mariano, HAR, FNN",Ivan Mitkov,RVOLaccur,2,!/usr/bin/env python3  -*- coding: utf-8 -*- dm_test actual_lst  prediction_1  prediction_2  horizon = 1  power = 2  Import libraries scipy.stats t collections pandas numpy  Initialise lists  convert every value of the lists into real values  Length of lists  as real numbers)  construct d according to crit  Mean of d          Find autocovariance and construct DM test statistics autocovariance Xi  N  k  Xs  0  1  2  Find p-value  Construct named tuple for return  Actual analysis warnings  Set wordking directiory os  Please note that the code works without any errors only if you run the whole script at once  If you run partial codes then you have to replace manually the working directory with your path  os.chdir r'/hier/comes/your/path/link')  Impot of packages dalib.packages  In order to keep constant outputs  avoiding different weights initialization numpy.random seed  1. High volatility time longer training data set  Predictions  Visualization  2. High volatility time longer training data set  Predictions  Visualization  3. Low volatility time longer training data set  Predictions  Visualization  4. Low volatility time longer training data set  Predictions  Visualization,170
Localizing_Multivariate_CAViaR,"Estimating a Multivariate Conditional Auto-Regressive Value at Risk model (MV-CAViaR) requires adaptation to possibly time varying parameter. Following the strategy of Spokoiny (2009) one can consider a sequence of included time intervals with the same end point, testing each of them agains the largest included one for homogeneity. Performing the tests subsequently one can choose the largest interval that is not rejected for estimation in order to have the least variance with moderately small bias. The standard way to test homogeneity is via Change Point detection. The critical values can be estimated using Multiplier Bootstrap procedure Spokoiny, Zhilova (2013). Based on a joint work with Xiu Xu and Wolfgang Härdle we implement interval homogeneity test with bootstrap-simulated critical values.","CAViaR, interdependence, tail-events, bootstrap, time series, value at risk, autoregression",Yegor Klochkov,mvcaviar,8,numpy math isnan scipy.stats norm generate_random_weights num  SHOULD ONLY BE APPLIED IN MAIN PROCESS!  let p > q and p + q = 1 then P X = 1 - \sqrt{q/p}) = p and P X = 1 + \sqrt{p/q}) = q  ensures that E X) = 1 and Var X) = 1 and X > 0 a.s.  moreover  taking p = 4/5 and q = 1/5 we have X \in [1 - 1/2  1 + 2] = [0.5  3] trans x dict_of_pars kernel  recurrent  bias pars_diff pars1  pars2 pars_sum pars1  pars2 pars_left_right pars_l  pars_r get_left_pars pars get_right_pars pars clean_vec_from_nans values two_moment_quantile x  tau,104
Localizing_Multivariate_CAViaR,"Estimating a Multivariate Conditional Auto-Regressive Value at Risk model (MV-CAViaR) requires adaptation to possibly time varying parameter. Following the strategy of Spokoiny (2009) one can consider a sequence of included time intervals with the same end point, testing each of them agains the largest included one for homogeneity. Performing the tests subsequently one can choose the largest interval that is not rejected for estimation in order to have the least variance with moderately small bias. The standard way to test homogeneity is via Change Point detection. The critical values can be estimated using Multiplier Bootstrap procedure Spokoiny, Zhilova (2013). Based on a joint work with Xiu Xu and Wolfgang Härdle we implement interval homogeneity test with bootstrap-simulated critical values.","CAViaR, interdependence, tail-events, bootstrap, time series, value at risk, autoregression",Yegor Klochkov,mvcaviar,8,numpy mvcaviar matplotlib __init__ self  pars  breaks what_par self  t get_realisation self  T y = np.matmul A  np.abs Y[:  [t-1]])) z = np.matmul B  sigmas[:  t-1]) theta0 = np.array [ 0.8  -0.3                      0.0   0.0                      #                     .5  0.05  0.0                      1.0  0.0  0.05]),41
Localizing_Multivariate_CAViaR,"Estimating a Multivariate Conditional Auto-Regressive Value at Risk model (MV-CAViaR) requires adaptation to possibly time varying parameter. Following the strategy of Spokoiny (2009) one can consider a sequence of included time intervals with the same end point, testing each of them agains the largest included one for homogeneity. Performing the tests subsequently one can choose the largest interval that is not rejected for estimation in order to have the least variance with moderately small bias. The standard way to test homogeneity is via Change Point detection. The critical values can be estimated using Multiplier Bootstrap procedure Spokoiny, Zhilova (2013). Based on a joint work with Xiu Xu and Wolfgang Härdle we implement interval homogeneity test with bootstrap-simulated critical values.","CAViaR, interdependence, tail-events, bootstrap, time series, value at risk, autoregression",Yegor Klochkov,mvcaviar,8,sys numpy pandas read_csv scipy.stats norm matplotlib seaborn math ceil estimator misc adaptation ###############################################################################      the complete reproduction of the simulations is done in three steps:   1) simulate the data by simulations.py      2) produce the bootstrap values using adaptation_full; script.py helps to run independent tasks in separate processes   3) collect all the bvals  decide the interval lenght and conduct the one step ahead prediction using estimate_with_window; script_ahead.py helps to do it in parallel adaptation_full dump=False estimate_with_window ,75
Localizing_Multivariate_CAViaR,"Estimating a Multivariate Conditional Auto-Regressive Value at Risk model (MV-CAViaR) requires adaptation to possibly time varying parameter. Following the strategy of Spokoiny (2009) one can consider a sequence of included time intervals with the same end point, testing each of them agains the largest included one for homogeneity. Performing the tests subsequently one can choose the largest interval that is not rejected for estimation in order to have the least variance with moderately small bias. The standard way to test homogeneity is via Change Point detection. The critical values can be estimated using Multiplier Bootstrap procedure Spokoiny, Zhilova (2013). Based on a joint work with Xiu Xu and Wolfgang Härdle we implement interval homogeneity test with bootstrap-simulated critical values.","CAViaR, interdependence, tail-events, bootstrap, time series, value at risk, autoregression",Yegor Klochkov,mvcaviar,8,"numpy pandas read_csv matplotlib subprocess scipy.stats norm math ceil sys misc run operation bvals_clean = clean_vec_from_nans bvals) print ""{} vs {}: {};  {}"".format len1  len2  res  comp))  Turn off tick labels ax.set_yticklabels []) ax.set_xticklabels [])",35
Localizing_Multivariate_CAViaR,"Estimating a Multivariate Conditional Auto-Regressive Value at Risk model (MV-CAViaR) requires adaptation to possibly time varying parameter. Following the strategy of Spokoiny (2009) one can consider a sequence of included time intervals with the same end point, testing each of them agains the largest included one for homogeneity. Performing the tests subsequently one can choose the largest interval that is not rejected for estimation in order to have the least variance with moderately small bias. The standard way to test homogeneity is via Change Point detection. The critical values can be estimated using Multiplier Bootstrap procedure Spokoiny, Zhilova (2013). Based on a joint work with Xiu Xu and Wolfgang Härdle we implement interval homogeneity test with bootstrap-simulated critical values.","CAViaR, interdependence, tail-events, bootstrap, time series, value at risk, autoregression",Yegor Klochkov,mvcaviar,8,tensorflow numpy collections namedtuple misc qr_loss y  f  tau  sample_weights=None predict_mvcaviar x_train  prev_quantile  pars  Do gradient descent step err = answers - outputs loss = tf.reduce_mean tf.maximum tau * err   tau - 1) * err))  Do gradient descent step  Do gradient descent step,43
Localizing_Multivariate_CAViaR,"Estimating a Multivariate Conditional Auto-Regressive Value at Risk model (MV-CAViaR) requires adaptation to possibly time varying parameter. Following the strategy of Spokoiny (2009) one can consider a sequence of included time intervals with the same end point, testing each of them agains the largest included one for homogeneity. Performing the tests subsequently one can choose the largest interval that is not rejected for estimation in order to have the least variance with moderately small bias. The standard way to test homogeneity is via Change Point detection. The critical values can be estimated using Multiplier Bootstrap procedure Spokoiny, Zhilova (2013). Based on a joint work with Xiu Xu and Wolfgang Härdle we implement interval homogeneity test with bootstrap-simulated critical values.","CAViaR, interdependence, tail-events, bootstrap, time series, value at risk, autoregression",Yegor Klochkov,mvcaviar,8,subprocess,1
Localizing_Multivariate_CAViaR,"Estimating a Multivariate Conditional Auto-Regressive Value at Risk model (MV-CAViaR) requires adaptation to possibly time varying parameter. Following the strategy of Spokoiny (2009) one can consider a sequence of included time intervals with the same end point, testing each of them agains the largest included one for homogeneity. Performing the tests subsequently one can choose the largest interval that is not rejected for estimation in order to have the least variance with moderately small bias. The standard way to test homogeneity is via Change Point detection. The critical values can be estimated using Multiplier Bootstrap procedure Spokoiny, Zhilova (2013). Based on a joint work with Xiu Xu and Wolfgang Härdle we implement interval homogeneity test with bootstrap-simulated critical values.","CAViaR, interdependence, tail-events, bootstrap, time series, value at risk, autoregression",Yegor Klochkov,mvcaviar,8,"numpy collections namedtuple scipy.stats norm  local estimator misc cheat_estimation train_func  is_separated  *args  **kwargs _train_mvcaviar *args  **kwargs _train_mvcaviar_w_shift *args  **kwargs _train_mvcaviar_separated *args  **kwargs __init__ self  Y  tau  breaks  message=""X""  prev_quantile=None do_test self init_pars=pars_left_right self.res_joint.pars  self.res_joint.pars)  __call__ self  weights bstrap_cp_test Y  tau  breaks  SIM_NUM  message=""X""  prev_quantile=None  cheat_pars=None select_homogeneity_interval_with_single_break Y  tau  lengths  SIM_NUM=400",50
Localizing_Multivariate_CAViaR,"Estimating a Multivariate Conditional Auto-Regressive Value at Risk model (MV-CAViaR) requires adaptation to possibly time varying parameter. Following the strategy of Spokoiny (2009) one can consider a sequence of included time intervals with the same end point, testing each of them agains the largest included one for homogeneity. Performing the tests subsequently one can choose the largest interval that is not rejected for estimation in order to have the least variance with moderately small bias. The standard way to test homogeneity is via Change Point detection. The critical values can be estimated using Multiplier Bootstrap procedure Spokoiny, Zhilova (2013). Based on a joint work with Xiu Xu and Wolfgang Härdle we implement interval homogeneity test with bootstrap-simulated critical values.","CAViaR, interdependence, tail-events, bootstrap, time series, value at risk, autoregression",Yegor Klochkov,mvcaviar,8, -*- coding: utf-8 -*-,4
AMLBitcoin,"Working on the Elliptic labeled Bitcoin transaction dataset, an attempt is made to detect illicit activities using different Machine Learning models","Bitcoin, Elliptic, AML, illicit activities, Cryptocurrency, Random Forest, XGBoost, Neural Network ",Muhammad Najati Al-imam,Detecting-Crypto-Money-Laundering,2,"!/usr/bin/env python  coding: utf-8  #<h1 align=""center""><font color=#b00020><b>Detecting Money Laundering Activities on Crypto Exchanges - Muhammad Najati Alimam - DEDA_SoSe_2021</b></font></h1>  #<h2 align=""left""><font color=#2196F3>Introduction</h2>  <b>Content</b>  <p>  This anonymized data set is a transaction graph collected from the Bitcoin blockchain. A node in the graph represents a transaction  an edge can be viewed as a flow of Bitcoins between one transaction and the other. Each node has 166 features and has been labeled as being created by a ""licit""  ""illicit"" or ""unknown"" entity.  </p>    <b>Nodes and edges</b>  <p>  The graph is made of 203 769 nodes and 234 355 edges. Two percent  4 545) of the nodes are labelled class1  illicit). Twenty-one percent  42 019) are labelled class2  licit). The remaining transactions are not labelled with regard to licit versus illicit.  </p>    <b>Features</b>  <p>  There are 166 features associated with each node. Due to intellectual property issues  we cannot provide an exact description of all the features in the dataset. There is a time step associated to each node  representing a measure of the time when a transaction was broadcasted to the Bitcoin network. The time steps  running from 1 to 49  are evenly spaced with an interval of about two weeks. Each time step contains a single connected component of transactions that appeared on the blockchain within less than three hours between each other; there are no edges connecting the different time steps.  </p>    <p>  The first 94 features represent local information about the transaction – including the time step described above  number of inputs/outputs  transaction fee  output volume and aggregated figures such as average BTC received  spent) by the inputs/outputs and average number of incoming  outgoing) transactions associated with the inputs/outputs. The remaining 72 features are aggregated features  obtained using transaction information one-hop backward/forward from the center node - giving the maximum  minimum  standard deviation and correlation coefficients of the neighbour transactions for the same information data  number of inputs/outputs  transaction fee  etc.).  </p>      <b>License</b>  <p>  This data set is distributed under the Creative CommonsAttribution-NonCommercial-NoDerivatives License 4.0 International.  <p>Summary</p>  <ul>  <li>Licit: exchanges  wallet providers  miners  licit services  etc.  <li>Ilicit: scams  malware  terrorist  organization  ransomware  Ponzi shcemes  etc. A given transaction is licit if the entity that generated it was licit.    <li>Nodes and Edges: 203 769 node transactions and 234 355 directed edge payments flows. 2% are ilicit  Class 1)  21% are licit  Class 2)  <li>Features: Each node has associated 166 features. 94 represent local information  timestep  number of inputs/outputs  transaction fee  output volume and aggregated figures) and 72 features represent aggregated features  obtained by aggregating transaction information such as maximum  minimum  standard deviation  correlation coefficients of neighbor transactions).  <li>Temporal Information: A time step is associated with each node  representing an stimated of the time when the transaction is confirmed. There are 49 distinct timesteps evenly spaced with an interval of 2 weeks.  <li>elliptic_txs_edgelist.csv contains graph edges information;  <li>elliptic_txs_classes.csv contains information about legality of transactions;  <li>elliptic_txs_features.csv contains information about transaction features;  #<h2 align=""left""><font color=#2196F3>Setting Path</h2>  In[1]:  Create a global variable to idicate whether the notebook is run in Colab sys os  Configure variables pointing to directories and stored files  google.colab drive    #<h2 align=""left""><font color=#2196F3>Importing General Use Libraries</h2>  In[2]: pathlib Path math pandas numpy pickle pprint pprint sklearn metrics scipy stats matplotlib seaborn plotly plotly.subplots make_subplots  removes warning messages pd.set_option 'display.max_rows'  None)  Setting seed for results to be re-producable  #<h2 align=""left""><font color=#2196F3>Loading Datasets - Visualisation - Merging</h2>  In[3]:  In[4]: change class unknown to 3  In[5]:  In[6]:  Class 1 means that transaction is illicit  '2' means that transaction is licit and most transaction aren't labeled.  In[7]:  renaming columns  In[8]:  merge with classes  First we pick the class 1 or 2 transactions only  In[9]:  Merge Class and features  In[10]:  drop class  text id and time step  in this case  class 2 corresponds to licit transactions  we chang this to 0 as our interest is the ilicit transactions  #<h2 align=""left""><font color=#2196F3>Split</h2>  In[11]: sklearn.model_selection train_test_split  Split train/test data. By test here I mean validation.  In[12]:  A random state is assigned for results to be repeatable  #<h2 align=""left""><font color=#2196F3>Random Forest Classifier</h2>  In[31]:  In[13]: sklearn.ensemble RandomForestClassifier sklearn.metrics precision_recall_fscore_support sklearn.metrics roc_auc_score sklearn.metrics confusion_matrix sklearn.metrics plot_roc_curve  In[32]:  In[33]:  #<h2 align=""left""><font color=#2196F3>RF - Hyperparameter Tuning</h2>  In[ ]:  Hyperparameter tuning for all models was not performed in this notebook due to time constraint  but a sample on how to tune a RF model using Randomized Search is found below  In[ ]:  In[ ]: sklearn.model_selection RandomizedSearchCV  Number of trees in random forest  Number of features to consider at every split  Maximum number of levels in tree  Minimum number of samples required to split a node  Minimum number of samples required at each leaf node  Create the random grid  In[ ]: ##### WARNING: this might take a really long time depending on your CPU\n# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrfc_tuning = RandomForestClassifier )\n# Random search of parameters  using 3 fold cross validation  \n# search across 50 different combinations  and use all available cores\nrfc_tuning_random = RandomizedSearchCV estimator = rfc_tuning  param_distributions = random_grid  n_iter = 50  cv = 3  verbose=1  random_state=666  n_jobs = -1)\n# Fit the random search model\nrfc_tuning_random.fit X_train y_train)\nrfc_tuning_random_predict = rfc_tuning_random.predict X_test)\n\nprec rec f1 num = precision_recall_fscore_support y_test  rfc_tuning_random_predict)\n\nprint ""Random Forest Classifier"")\nprint ""Precision:%.3f \\nRecall:%.3f \\nF1 Score:%.3f""% prec[1] rec[1] f1[1]))')  #<h2 align=""left""><font color=#2196F3>XGBoost</h2>  In[19]: xgboost XGBClassifier  In[20]:  In[21]:  XGBoost AUC score shows a different number from sklearn's roc_auc_score  find out why: https://github.com/dmlc/xgboost/issues/2064  A small clarification of the above: https://discuss.analyticsvidhya.com/t/what-is-the-difference-between-predict-and-predict-proba/67376/3  #<h2 align=""left""><font color=#2196F3>XGBoost - Hyperparameter Tuning</h2>  In[22]:  After finding the best parameters  which was not done in this notebook due to time constraints)  Aguide to do that can be found here: https://blog.cambridgespark.com/hyperparameter-tuning-in-xgboost-4ff9100a3b2f  For now  we will pretend to have found the optimal parameters and set them manually.  In[23]:  In[24]:  In[25]:  #<h2 align=""left""><font color=#2196F3>Feed Foward Neural Network</h2>  In[26]: from tensorflow import keras # instead of importing the whole framework  we can import individual modules that we need keras.models Sequential keras.models Model keras.layers Dense keras.layers concatenate keras.layers Input regularization keras.layers Dropout keras regularizers  In[ ]:  In[27]:  define and fit the model get_model X_train  y_train  define model  compile model  fit model  In[28]:  In[29]:  predict probabilities for test set  predict crisp classes for test set  reduce to 1d array   accuracy:  tp + tn) /  p + n)  precision tp /  tp + fp)  recall: tp /  tp + fn)  f1: 2 tp /  2 tp + fp + fn)  #<h2 align=""left""><font color=#2196F3>Plotting ROC</h2>  In[37]:  RF and XGBoost  In[ ]:  Neural Network  #<h2 align=""left""><font color=#2196F3>Sources</h2>  sources  <ul>  <li>https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74<li>https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/random_forest_explained/.ipynb_checkpoints/Improving%20Random%20Forest%20Part%202-checkpoint.ipynb  <li>https://www.kaggle.com/artgor/elliptic-data-eda  <li>https://www.kaggle.com/smlopezza/elliptic-data-set-eda-graphs-random-forest  <li> https://www.kaggle.com/georsara1/95-auc-score-in-train-sample-with-neural-nets</li>  <li>https://towardsdatascience.com/fine-tuning-xgboost-in-python-like-a-boss-b4543ed8b1e</li>  </ul>  A lot of research was done. I have probably missed some resources  but they all relate to code and code optimization. Please refer to the slides for a more comprehensive source citation  #<h1 align=""center""><font color=#b00020><b>Thank You</b></font></h1>",1118
pyTSA_MacroUS,"This Quantlet plots monthly time series of returns of Procter and Gamble from 1961 to 2016 and  their ACF and PACF (Example, 2.4 Figures 2.8-2.9 in the book)","time series, autocorrelation, returns, ACF, PACF, plot, visualisation","Huang Changquan, Alla Petukhina",pyTSA_MacroUS,1,numpy pandas matplotlib statsmodels.tsa.api VAR statsmodels.tsa.api VARMAX statsmodels PythonTsa.plot_multi_ACF multi_ACFfig PythonTsa.plot_multi_Q_pvalue MultiQpvalue_plot  log  difference and then drop NAN  leave the last three data for forecasting comparison  here select p = 3 for VAR p).  Cannot fix individual autoregressive parameters when  ‘enforce_stationarity = True‘. In this case  must either  fix all autoregressive parameters or none.,54
DEDA_WebScrapingInCryptocurrencies,Extract sentiment from Bitcoin related news and use sentiment to forecast price,ERROR,Alex,DEDA_WebScrapingInCryptocurrencies,7, -*- coding: utf-8 -*- bs4 BeautifulSoup requests pandas bitcoinfunc  Scrap title time link tag and content from every article  Using the function defined previously with certain input arguments  Get the content of each article,34
DEDA_WebScrapingInCryptocurrencies,Extract sentiment from Bitcoin related news and use sentiment to forecast price,ERROR,Alex,DEDA_WebScrapingInCryptocurrencies,7,bs4 BeautifulSoup requests datetime os pandas pickle  Using the requests module to get source code from the url bitcoin_news_scraping page=1  refresh=False  Argument page equals 1 by default  Visit the home page if page equals 1  Change url by different argument  connect to the website if the webpage source code file is not exist of we need to refresh it  save the request object after scraping  else  we open the source code file from local disk to save time  Using BeautifulSoup to parse webpage source code  Finding all the <li> tag content  initial empty list to load the data  Using .find_all ) method to search tag name and attribute  For more than only 0 1)!  Loop in the <a> tag  strip ) method can remove specific chars at the head and tail,131
DEDA_WebScrapingInCryptocurrencies,Extract sentiment from Bitcoin related news and use sentiment to forecast price,ERROR,Alex,DEDA_WebScrapingInCryptocurrencies,7,numpy collections Counter matplotlib  Only the most frequent 10 words are observed  Sort the values by decending order  Rearrange your data  Plot the results,24
DEDA_WebScrapingInCryptocurrencies,Extract sentiment from Bitcoin related news and use sentiment to forecast price,ERROR,Alex,DEDA_WebScrapingInCryptocurrencies,7,collections Counter wordcloud WordCloud PIL Image wordcloud text  image,9
DEDA_WebScrapingInCryptocurrencies,Extract sentiment from Bitcoin related news and use sentiment to forecast price,ERROR,Alex,DEDA_WebScrapingInCryptocurrencies,7,nltk  Clean txt from Bitcoin magazin  split into words nltk.tokenize word_tokenize  remove all tokens that are not alphabetic  convert to lower case  filter out stopwords nltk.corpus stopwords  Additional stopwords from McDonald  Stopwords consiting of dates and numbers  More stopwords from McDonald,41
DEDA_WebScrapingInCryptocurrencies,Extract sentiment from Bitcoin related news and use sentiment to forecast price,ERROR,Alex,DEDA_WebScrapingInCryptocurrencies,7,pandas datetime matplotlib math numpy  xlrd must be downloaded first xlrd  df = pd.read_excel 'LoughranMcDonald_MasterDictionary_2014.xlsx')  Rearrange the negativ/positive words of the McDonalds Textfile  Find all negatives in Bitcoin Magazin articles:  Find all positivs in Bitcoin Magazin articles  Bullish/Bearish indicators:  Rearrange the negativ/positive words of the dictionary #Extract sentiment  Returns all words found in btc_articles that match dictionary_Chen  Average over sentiments for single articles  Add sentiment to bitcoin dataframe:  Add btc_prices to bitcoin dataframe:  Group by day of sentiment,78
DEDA_WebScrapingInCryptocurrencies,Extract sentiment from Bitcoin related news and use sentiment to forecast price,ERROR,Alex,DEDA_WebScrapingInCryptocurrencies,7, -*- coding: utf-8 -*- os quandl numpy pandas plotly plotly datetime quandl  Define a function to pull the data from one exchange: crypto_data chart_exchange  Download and cache Quandl dataseries  First chart of Bitcoin timeseries from Bitstamp: 30BEHF'))  Pull pricing data for more BTC exchanges and safe them in a dictionary  Merch into one single dataframe combine_dataframes dataframes  labels  col  Plotting the results: 17BECF')  18EFGH')  23HFTP')  30BEHF')   Readjust the plot and drop values of 0  Average over the weighted prices of all 4 exchanges and plot the result 14DETF')  opacity=0.8),89
pyTSA_ReturnsIBM,"This Quantlet plots monthly time series of returns of Procter and Gamble from 1961 to 2016 and  their ACF and PACF (Example, 2.4 Figures 2.8-2.9 in the book)","time series, autocorrelation, returns, ACF, PACF, plot, visualisation","Huang Changquan, Alla Petukhina",pyTSA_ReturnsIBM,1,pandas import numpy as np matplotlib statsmodels PythonTsa.plot_acf_pacf acf_pacf_fig PythonTsa.LjungBoxtest plot_LB_pvalue arch arch_model PythonTsa.Selecting_arma choose_arma statsmodels.graphics.api qqplot,17
SDA_2019_Machine_Learning_Asset_Allocation_RHP,Implementation of three different portfolio optimisation methods. The classical Markowitz Minimum Variance technique is compared to the Inverse Variance Portfolio and a new approach called Hierarchical Risk Parity (HRP). HRP combines graph theory and machine learning techniques to determine the optimal allocation to assets based on the information contained in the covariance matrix. Strategies are applied to Crypto Currency Assets and Stocks and evaluated on risk/return profile.,"optimisation, portfolio allocation, asset allocation, machine learning, graph theory, hierarchical risk parity, markowitz, minimum variance, risk based investing",Julian Woessner,SDA_2019_St_Gallen_Hierarchical_Risk_Parity,1,"!/usr/bin/env python3  -*- coding: utf-8 -*-  In[1]:   Load modules os  Set Working directory here  Import modules for Datastructuring and calc. pandas numpy scipy stats warnings tqdm tqdm  Modules for RHP algorithm matplotlib scipy  Modules for the network plot networkx networkx.convert_matrix from_numpy_matrix  Modules for Markowitz optimization cvxopt cvxopt  suppress warnings in clustering  In[2]:   define functions for HRP and IVP getIVP cov **kargs  Compute the inverse-variance portfolio getClusterVar cov  cItems  Compute variance per cluster  matrix slice getQuasiDiag link  Sort clustered items by distance  number of original items  make space  find clusters  item 1  item 2  re-sort  re-index getRecBipart cov sortIx  Compute HRP alloc  initialize all items in one cluster  bi-section  parse in pairs  cluster 1  cluster 2  weight 1  weight 2 correlDist corr  A distance matrix based on correlation  where 0<=d[i j]<=1   This is a proper distance metric  distance matrix plotCorrMatrix path  corr  labels=None  Heatmap of the correlation matrix  reset pylab  In[3]:   define function for MinVar portfolio  The MIT License  MIT)  Copyright  c) 2015 Christian Zielinski  Permission is hereby granted  free of charge  to any person obtaining a copy  of this software and associated documentation files  the ""Software"")  to deal  in the Software without restriction  including without limitation the rights  to use  copy  modify  merge  publish  distribute  sublicense  and/or sell  copies of the Software  and to permit persons to whom the Software is  furnished to do so  subject to the following conditions:  The above copyright notice and this permission notice shall be included in all  copies or substantial portions of the Software.  THE SOFTWARE IS PROVIDED ""AS IS""  WITHOUT WARRANTY OF ANY KIND  EXPRESS OR  IMPLIED  INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM  DAMAGES OR OTHER  LIABILITY  WHETHER IN AN ACTION OF CONTRACT  TORT OR OTHERWISE  ARISING FROM   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE  SOFTWARE.    min_var_portfolio cov_mat  allow_short=False  Constraints Gx <= h  x >= 0  Constraints Ax = b  sum x) = 1  Solve  Put weights into a labeled series  In[4]:   Define functions for network graphs Function to plot Network plots plotNetwork path corr  Transform it in a links data frame links=corr.stack ).reset_index ) Build graph  remove self-loops  replace values that are below threshold  create undirected graph from adj_matrix  set names to crypots  raise text positions  In[5]:  Loading and structuring crypto data sets load csv  Deleting empty rows  Removing timestamp Price_data_univ = pd.merge SP500  Crypto  on='Date'  how='inner')#rename column  define Date  as index  Calculating returns  calculate daily returns  Calculating covariance matrix  Covariance matrix of the return matrix  Correlation matrix of the return matrix  In[6]:  Heatmap and network analysis of corr. matrix  Plotting Correlation matrix heatmap  network plot of correlation matrix  Sort correlation matrix  recover labels   reorder  Plot sorted correlation matrix  Plot dendogram of the constituents 2) Cluster Data  reset pylab  In[7]: Function to calculate the HRP portfolio weights HRPportf cov corr 1) Cluster covariance matrix  recover labels 2) Allocate capital according to HRP  In[8]:  Compute the weights for the Markowitz MinVar and the HRP portfolio and the   IVP portfolio  Latex table output  In[9]:  Backtesting the three optimisation methods for Crypto dataset  Function to calculate the weigths in sample and then test out of sample Backtest_Crypto returns  rebal = 30  rebal = 30 default rebalancing after 1 month  Number of iterations without first set to train  Check for rebalancing date  In[10]:      Calculate the backtested portfolio returns  Calculate index  reset pylab  Calculate portfolio return and portfolio variance  Daily Standard deviation  Sharpe ratios  Latex table output  In[11]:  Analysis for the SP500 universe from 2015 onwards  Data import load csv Deleting empty columns  define Date as index  Drop columns with incomplete data  Calculating returns and deleting columns that contain 0 calculate daily returns Return_data_univ = Return_data_univ.fillna 0)  Calculating covariance matrix  Covariance matrix of the return matrix  Correlation matrix of the return matrix  In[12]:  Plotting Correlation matrix heatmap  Sort correlation matrix  recover labels   reorder  Plot sorted correlation matrix  Plot dendogram of the constituents 2) Cluster Data  font size for the x axis labels)  reset pylab  In[13]:  Backtesting function for stocks data Backtest_SP500 returns  rebal = 30  rebal = 30 default rebalancing after 1 month  Number of iterations without first set to train  Check for rebalancing date  In[14]:    Calculate the backtested portfolio returns  Change rebalancing frequency  Calculate index  Calculate portfolio return and portfolio variance  Daily average return  Daily Standard deviation  Sharpe ratios  Latex table output  In[15]:  Analysis for the SP500 universe from 2000 onwards: Data import load csv Deleting empty columns SP500 = SP500[[col for col in SP500.columns if SP500.loc[: col].notna ).any )]]   define Date  as index  Calculating returns and deleting columns that contain 0 calculate daily returns  Calculating covariance matrix  Covariance matrix of the return matrix  Correlation matrix of the return matrix  In[16]:  Calculate the backtested portfolio returns  Change rebalancing frequency  Calculate index  Calculate portfolio return and portfolio variance  Daily average return  Daily Standard deviation  Sharpe ratios  Latex table output  In[17]:  Changing the rebalancing frequency  Monthly  half-year  yearly Print indices  Calculate index  Calculate index  Calculate index  Calculate performance figures  Daily Standard deviation  Sharpe ratios  Latex table output  Calculate performance figures  Daily Standard deviation  Sharpe ratios  Latex table output  Calculate performance figures  Daily Standard deviation  Sharpe ratios  Latex table output  In[18]:  Create Mixed portfolios with stocks and crypto  Data import load csv  randomly select 10 stocks from SP500 rename column  define Date  as index  Calculating returns and deleting columns that contain 0 calculate daily returns  Calculating covariance matrix  Covariance matrix of the return matrix  Correlation matrix of the return matrix  In[19]:  Plotting Correlation matrix heatmap  Sort correlation matrix  recover labels   reorder  Plot sorted correlation matrix  Plot dendogram of the constituents  Cluster Data  reset pylab  In[20]:  Calculate the backtested portfolio returns  Changing the rebalancing frequency  Monthly  half-year  yearly  Monthly Print indices  Calculate index  Calculate index  Calculate index  Calculate index  Calculate performance figures  Daily Standard deviation  Sharpe ratios  Latex table output  Calculate performance figures  Daily Standard deviation  Sharpe ratios  Latex table output  Calculate performance figures  Daily Standard deviation  Sharpe ratios  Latex table output  Calculate performance figures  Daily Standard deviation  Sharpe ratios  Latex table output  In[ ]:",1018
KEC,"A novel clustering algorithm based on expectiles. The cluster centres are defined as multivariate expectiles and clusters are searched via greedy algorithm by minimising within cluster ""tau-variance"". Applications and simulations are provided.",ERROR,Bingling Wang,KEC,8,!/usr/bin/env python  coding: utf-8  In[10]: sklearn datasets matplotlib pandas sklearn.cluster KMeans mpl_toolkits mplot3d numpy time copy math sklearn.decomposition PCA itertools sklearn.cluster SpectralClustering sklearn.cluster AgglomerativeClustering sklearn metrics sklearn.manifold TSNE scipy.stats nct scipy.stats chi2  In[123]: sim1_fun p k n_samples sim2_fun p k n_samples tau = np.zeros p*k) sim3_fun p k n_samples sim4_fun p k n_samples sim5_fun p k n_samples sim6_fun p k n_samples sim7_fun p k n_samples  In[32]: sim8_fun p k1 k2 k3 mu1 = np.zeros  k p)) mu1 = np.zeros  k p)) for j in range len n_samples)): sample1[j-1:j :] = sample[j-1:j :] + 2* j-1) mu1[ j-1) :] = mu[p* j-1):p*j] + 2* j-1)  In[117]:  In[35]: co_fun tau s  In[125]: def sim_clu n_samples p k): y = blobs[1] print tau) X = PCA n_components=20).fit_transform X_ini) X = TSNE n_components=3).fit_transform X) k-expectile k-means spectral hierarcicel quantile  In[90]: Euclidean distance dist a  b Tau-distance dist_fun a m tau Expectile estimation expectile_fun group  tau Estimate optimal taus tau_fun points  mu  Define K expectile clustering  k_expectile_new X  k  Initialize cluster centers as K means cluster centers  To store the value of centroids when it updates  Initialize tau = 0.5  Error func. - Distance between new centroids and old centroids  Loop will run till the error  while error >= 0.5:  Assigning each value to its closest cluster  Storing the old centroid values  Finding the new centroids and tau  Updating taus print tau_list)  In[124]: matching cluster index itertools cluster_index clusters  target  k print index)  In[126]: X = blobs[0] y = blobs[1] y[ n_samples+1):n_samples* k-1)]=1 y[ n_samples* k-1)+1):n_samples*k]=2 y =y+1 C1  c5 = k_expectile_new X  3) cluster_index clusters  target  k print index)  In[46]:  In[47]:,267
KEC,"A novel clustering algorithm based on expectiles. The cluster centres are defined as multivariate expectiles and clusters are searched via greedy algorithm by minimising within cluster ""tau-variance"". Applications and simulations are provided.",ERROR,Bingling Wang,KEC,8,!/usr/bin/env python  coding: utf-8  In[2]: sklearn datasets matplotlib pandas sklearn.cluster KMeans mpl_toolkits mplot3d numpy time copy math sklearn.decomposition PCA itertools  In[3]:  In[5]:  Anisotropicly distributed data skewed norm  In[4]: Euclidean distance dist a  b Tau-distance dist_fun_vtau a m tau Expectile estimation expectile_fun group  tau Estimate optimal taus tau_fun points  mu  In[7]:  K expectile clustering with unknown taus k_expectile_vtau X  k  Initialize cluster centers as K means cluster centers  To store the value of centroids when it updates  Initialize tau = 0.5  Initialize var = var  Error func. - Distance between new centroids and old centroids  Loop will run till the error   Assigning each value to its closest cluster  Storing the old centroid values  Finding the new centroids and tau  Updating taus  In[6]:,121
SDA_2019_St_Gallen_PD_with_ML_and_Sentiment_Analysis_Textual_Analysis,"Collect textual data from companies annual reports, perform sentiment analysis, report polarity scores.","textual analysis, sentiment extraction, polarity scores, stopwords, annual reports ",ERROR,Textual_Analysis,2, -*- coding: utf-8 -*- os csv pandas numpy glob  Set working directory,12
SDA_2019_St_Gallen_PD_with_ML_and_Sentiment_Analysis_Textual_Analysis,"Collect textual data from companies annual reports, perform sentiment analysis, report polarity scores.","textual analysis, sentiment extraction, polarity scores, stopwords, annual reports ",ERROR,Textual_Analysis,2," -*- coding: utf-8 -*- Import libraries needed os pandas numpy requests sqlite3 sqlalchemy create_engine bs4 BeautifulSoup urllib.request urlopen re nltk pickle nltk nltk.tokenize RegexpTokenizer set working directory   function to get the text from the html links via BeautifulSoup url_to_text url Function to clean the dataset clean_text_round2 text  convert to lower case  remove \xa0 which is non-breaking space from ISO 8859-1  remove newline feeds  \n) following hyphens  remove hyphens preceded and followed by a blank space  replace 'and/or' with 'and or'  tow or more hypens  periods  or equal signs  possiblly followed by spaces are removed  all underscores are removed  3 or more spaces are replaced by a single space  three or more line feeds  possibly separated by spaces are replaced by two line feeds  remove hyphens before a line feed  replace hyphens preceding a capitalized letter with a space  remove capitalized or all capitals for the months  remove years  remove words million and company   remove line feeds replace single line feed \n with single space Initiate the years Look into the time frame wanted Pass via sqlite3 as is more efficient Go get the required data from the EDGAR index file After having passed by SQL  save it to a csv  Take only the 10-K files we are interested in  rearange the dataset  save in csv Transform data into a list Create a dataset only with the paths  Initialise the loop to extract the links  Inspect element to find how the 10K files are stored - in <div> and then find type and the link right before that this contains the actual 10 K file Save all the links to the 10-K files into a csv file  Get the texts files from the urls save them into a csv file if needed Loading stop words dictionary for removing stop words Tokenizeing module and filtering tokens using stop words list  removing punctuations tokenizer text Based on the dictionary of Loughran and McDonald  2016)  Calculating positive score  positive_word_LM text  Calculating Negative score negative_word_LM text Based on the dictionary of Bing Liu   Calculating positive score  positive_word_B text  Calculating Negative score negative_word_B text  Calculating polarity score polarity_score positiveScore  negativeScore  Function to count the words total_word_count text  Calculating Average sentence length   It will calculated using formula --- Average Sentence Length = the number of words / the number of sentences average_sentence_length text Import the negative and positive dictionaries  #### Words from Loughran and McDonald  negative   positive #### Words from Bing Liu  negative  positive  If the process of collection and textual analysis were different  here dowload the urls  urls = pd.read_csv PATH + r""\urls.csv""  index_col = 0  names=[""url""])[""url""].values.tolist )[1:]  Retrive all the cik numbers from the urls   Pickle the data to save them on the computer Open the pickle files previously saved on the computer                  Change to a pd.DataFrame  Create the variables for the textual analysis ",469
SDA_2019_Machine_Learning_Asset_Allocation_ParsePriceData,Web scrape data for Crypto Assets and SP500 constituents from the internet. The tickers are scraped from wikipedia and timeseries data is downloaded.,"web scraping, wikipedia, timeseries, price data",Julian Woessner,SDA_2019_St_Gallen_Webscraping_Timeseries,2,!/usr/bin/env python3  -*- coding: utf-8 -*-  In[1]:  Import packages bs4 os pandas_datareader pickle requests  In[2]:  Define Function to parse tickers of SP500 companies save_sp500_tickers   In[3]:  Parse tickers and timeseries from 2015 to 2019  then save as csv  Define start and end data  Save as .csv file  In[4]:  Parse tickers and timeseries from 2015 to 2019  then save as csv  Define start and end data  Save as .csv file  In[ ],70
SDA_2019_Machine_Learning_Asset_Allocation_ParsePriceData,Web scrape data for Crypto Assets and SP500 constituents from the internet. The tickers are scraped from wikipedia and timeseries data is downloaded.,"web scraping, wikipedia, timeseries, price data",Julian Woessner,SDA_2019_St_Gallen_Webscraping_Timeseries,2,!/usr/bin/env python3  -*- coding: utf-8 -*-  In[1]:  Import required packages os numpy pandas pickle quandl datetime datetime time pandas DataFrame plotly plotly plotly  In[2]:  Define Quandl Helper Function get_quandl_data quandl_id  Pull Kraken BTC price exchange data  In[3]:  Define Function to merge columns into dataframe merge_dfs_on_column dataframes  labels  col  In[4]:  Define json data Helper Function get_json_data json_url  cache_path  In[5]:  Parse crypto data and save to csv  get data from the start of 2015 in seconds  up until today  pull daily data  86 400 seconds per day) get_crypto_data poloniex_pair  Calculate USD Price as a new column in each altcoin dataframe Merge USD price of each altcoin into single dataframe   Add BTC price to the dataframe  Save as .csv file  In[ ],119
RL_Experiment1Performance,Outputs the performance of the algorithm for the test period for different target drawdowns as a plot. Additionally plots performance of (modified) CRIX.,"reinforcement learning, neural network, machine learning, portfolio management, cryptocurrency",Ilyas Agakishiev,RL_Experiment1Performance,1,pandas datetime datetime matplotlib matplotlib.dates DateFormatter,6
Clustering of Cryptopunks NFT,Clustering 9999 cryptopunks into 200 clusters based on Hierarchical Clustering,"Hierarchical, Clustering, Cryptopunks, NFTs, Prices","Muhammad Azka, Mariia Semenenko",Cryptopunks-image-clustering,5,"!/usr/bin/env python  coding: utf-8  In[4]: os selenium webdriver selenium.webdriver.common.keys Keys bs4 BeautifulSoup PIL Image requests datetime re time urllib urllib.request Request pandas numpy hdr = {'User-Agent': 'Mozilla/5.0'}  In[5]: req = Request url headers=hdr) web_r = urlopen req) selsoup = BeautifulSoup web_r  ""html.parser"")  In[20]:  In[18]:  In[19]:  In[24]:  In[25]:  In[11]:  In[12]:  In[15]:  In[16]:  In[32]:",52
Clustering of Cryptopunks NFT,Clustering 9999 cryptopunks into 200 clusters based on Hierarchical Clustering,"Hierarchical, Clustering, Cryptopunks, NFTs, Prices","Muhammad Azka, Mariia Semenenko",Cryptopunks-image-clustering,5,"!/usr/bin/env python  coding: utf-8  In[5]:  for loading/processing the images   keras.preprocessing.image load_img keras.preprocessing.image img_to_array keras.applications.vgg16 preprocess_input  models  keras.applications.vgg16 VGG16 keras.models Model  clustering and dimension reduction sklearn.cluster KMeans sklearn.decomposition PCA  for everything else os numpy matplotlib random randint pandas pickle tqdm  In[2]:  In[4]:  In[6]:  change the working directory to the path where the images are located  this list holds all the image filename  creates a ScandirIterator aliased as files  loops through each file in the directory  adds only the image files to the flowers list  In[6]:  load model  remove the output layer  In[297]:  In[147]:  load model  remove the output layer  reshape the data for the model reshape num_of_samples  dim 1  dim 2  channels)  prepare image for model  get the feature vector  In[296]:  In[151]:  load model  remove the output layer  reshape the data for the model reshape num_of_samples  dim 1  dim 2  channels)  prepare image for model  get the feature vector  In[7]: extract_features file  model  load the image as a 224x224 array  convert from 'PIL.Image.Image' to numpy array  reshape the data for the model reshape num_of_samples  dim 1  dim 2  channels)  prepare image for model  get the feature vector  In[8]:  lop through each image in the dataset  try to extract the features and update the dictionary  if something fails  save the extracted features as a pickle file  optional)  In[124]:  In[126]:  In[127]:  In[128]:  In[132]: matplotlib pyplot plt.show ) red  In[130]: matplotlib pyplot plt.show ) green  In[131]: matplotlib pyplot plt.show ) blue  In[61]:  get a list of the filenames  get a list of just the features  reshape so that there are 210 samples of 4096 vectors  In[84]:  In[133]: matplotlib  150 components explain almost 95% of the variance  In[73]:  In[236]: sklearn.cluster DBSCAN  In[197]: sklearn.cluster AgglomerativeClustering  In[288]: sklearn.cluster SpectralClustering  In[286]:  In[219]:  In[289]:  In[229]:  In[241]:  holds the cluster id and the images { id: [images] } post_process_clustering  model  In[290]:  In[291]: PIL Image print ""Group""  i) display img)  In[292]: PIL Image print ""Group""  i) display img)  In[284]:  In[285]:  In[ ]:",321
Clustering of Cryptopunks NFT,Clustering 9999 cryptopunks into 200 clusters based on Hierarchical Clustering,"Hierarchical, Clustering, Cryptopunks, NFTs, Prices","Muhammad Azka, Mariia Semenenko",Cryptopunks-image-clustering,5,os math random initColors  findLargestElement rows  cols  lengthArray  matrix  Loop through each row  Loop through each column  Sort the length matrix so that we can find the element with the longest length  Store that length createMatrix rows  cols  matrixToWorkOn  matrix  Loop through each row  Append a row to matrixToWorkOn for each row in the matrix passed in  Loop through each column  Add a each column of the current row  in string form) to matrixToWorkOn makeRows rows  cols  largestElementLength  rowLength  matrixToWorkOn  finalTable  color  Loop through each row  Initialize the row that will we work on currently as a blank string  Loop trhough each column  If we are using colors then do the same thing but as without  below)  Only add color if it is in the first column or first row  If we are not using colors  or j != 0 or i != 0) just add a space and the element that should be in that position to a variable which will store the current element to work on  If the raw element is less than the largest length of a raw element  raw element is just the unformatted element passed in)  If we are using colors then add the amount of spaces that is equal to the difference of the largest element length and the current element  minus the length that is added for the color)  * The plus two here comes from the one space we would normally need and the fact that we need to account for a space that tbe current element already has  If it is not the first column or first row than it doesn't need to subtract the color length  If we are not using color just do the same thing as above when we were using colors for when the row or column is not the first each time  If the raw element length us equal to the largest length of a raw element then we don't need to add extra spaces  Now add the current element to the row that we are working on  When the entire row that we were working on is done add it as a row to the final table that we will print  If we are using color then the length of each row  each row will end up being the same length) equals to the length of the last row  again each row will end up being the same length) minus the length the color will inevitably add if we are using colors  Otherwise  we are not using colors) the length of each row will be equal to the length of the last row  each row will end up being the same length) createWrappingRows rowLength  finalTable  Here we deal with the rows that will go on the top and bottom of the table  look like -> +--------------+)  we start by initializing an empty string  Then for the length of each row minus one  have to account for the plus that comes at the end  not minus two because rowLength doesn't include the | at the beginning) we add a -  Add a plus at the beginning  Add a plus at the end  Add the two wrapping rows createRowUnderFields largestElementLength  cols  finalTable  Initialize the row that will be created   Loop through each column  For each column add a plus  Then add an amount of -'s equal to the length of largest raw element and add 2 for the 2 spaces that will be either side the element  Then add the current element  there will be one for each column) to the final row that will be under the fields  Add a final plus at the end of the row  Insert this row under the first row printRowsInTable finalTable  For each row - print it printTable matrix  useFieldNames=False  color=None  Rows equal amount of lists inside greater list  Cols equal amount of elements inside each list  This is the array to sort the length of each element  This is the variable to store the vakye of the largest length of any element This is the variable that will store the length of each row  This is the matrix that we will work with throughout this program  main difference between matrix passed in and this matrix is that the matrix that is passed in doesn't always have elements which are all strings) This the list in which each row will be one of the final table to be printed,740
LSTM_Performance_No_Parameter_Tuning,Training the LSTM on the data of one of the Coins (ETH) and analysing the results on the test set without parameter tuning.,"LSTM, ETH, results","Georg Velev, Iliyana Pekova",LSTM_Performance_No_Parameter_Tuning,1,matplotlib visualize_results the_model,3
pyTSA_ReturnsPG,"This Quantlet plots monthly time series of returns of Procter and Gamble from 1961 to 2016 and  their ACF and PACF (Example, 2.4 Figures 2.8-2.9 in the book)","time series, autocorrelation, returns, ACF, PACF, plot, visualisation","Huang Changquan, Alla Petukhina",pyTSA_ReturnsPG,1,pandas matplotlib PythonTsa.plot_acf_pacf acf_pacf_fig statsmodels.tsa.stattools acf  get the position of the maximum Timestamp '1998-10-31 00:00:00'  freq = 'M')  get the position of the minimum Timestamp '2000-03-31 00:00:00'  freq = 'M'),30
pyTSA_ARMA3,This Quantlet demonstrates the fitting of the ARMA model to simulated time series,"time series,  stationarity, autocorrelation, PACF, ACF, simulation, Box Jenkins Anylsis, ARMA, KPSS",ERROR,pyTSA_ARMA3,1,numpy pandas matplotlib statsmodels statsmodels.tsa.arima_process arma_generate_sample PythonTsa.plot_acf_pacf acf_pacf_fig PythonTsa.Selecting_arma choose_arma statsmodels.tsa.arima_model ARMA PythonTsa.LjungBoxtest plot_LB_pvalue scipy stats  check stationarity  check invertibility  It is always a good idea to make the data a Series! NormaltestResult statistic = 0.48049991217901883                  pvalue = 0.786431263213941),39
EmbeddingPortfolio,"We provide results for ""Does non-linear factorization of financial returns help build better portfolio?"", Spilak, WK Härdle (2022). Please refer to github Wiki for a detailed description on how to use the code!","autoencoder, portfolio, tail-risk, trading strategy, cryptocurrency, deep learning, machine learning, econometrics",Bruno Spilak,EmbeddingPortfolio,11,!/usr/bin/env python  coding: utf-8  # Import libraries  In[1]: pandas matplotlib numpy dl_portfolio.data load_data dl_portfolio.backtest plot_perf pickle os sklearn.neighbors KernelDensity seaborn sklearn metrics dl_portfolio.sensitivity plot_sensitivity warnings  # Some useful functions  In[2]: plot_final_backtest_performance returns  benchmark  save=False  save_path=None  hedge=False plot_target_pred_proba target  proba  max_xticks  save_path=None  figsize= 20 10) classification_metrics cv_signal: dict  target load_all_backtest_result ae_dir  nmf_dir  dataset  markowitz_dir  m=0  Load Market budget  Load markowitz and robust M results  Leverage  # Save directory  In[3]:  # Load tail events data  In[4]:  Load tail events data  Handle R renaming of columns...  to be safe  to be safe  ## Activation map  In[5]:  ## Linear activations  In[6]:  # Backtest  In[7]:  In[8]:  ## Backtest stats dataset 1  In[9]:  stats1.loc[['SP500'  'Russel2000'  'EuroStoxx50']  'TTO'] = np.nan  In[10]:  ## Backtest stats dataset 2  In[11]:  In[12]:  dataset1_stats.loc[['SP500'  'Russel2000'  'EuroStoxx50']  'TTO'] = np.nan  In[13]:  ## Classification metrics dataset 1  ### Labels  In[14]:  In[15]:  # ROC Curve  In[16]:  ## Classification metrics dataset 2  ### Labels  In[17]:  ## ROC curve  In[18]:  # Exceedance and AUC  In[19]:  In[20]:  # Proba plot  ## Dataset1  In[21]:  ## Dataset 2  In[22]:  In[ ]:,171
EmbeddingPortfolio,"We provide results for ""Does non-linear factorization of financial returns help build better portfolio?"", Spilak, WK Härdle (2022). Please refer to github Wiki for a detailed description on how to use the code!","autoencoder, portfolio, tail-risk, trading strategy, cryptocurrency, deep learning, machine learning, econometrics",Bruno Spilak,EmbeddingPortfolio,11,os sys pandas dl_portfolio.data load_data dl_portfolio.utils get_linear_encoder dl_portfolio.logger LOGGER create_linear_features base_dir ae_config  Load test results argparse,16
EmbeddingPortfolio,"We provide results for ""Does non-linear factorization of financial returns help build better portfolio?"", Spilak, WK Härdle (2022). Please refer to github Wiki for a detailed description on how to use the code!","autoencoder, portfolio, tail-risk, trading strategy, cryptocurrency, deep learning, machine learning, econometrics",Bruno Spilak,EmbeddingPortfolio,11,"os pickle pandas numpy matplotlib joblib Parallel dl_portfolio.data load_data dl_portfolio.hedge hedged_portfolio_weights_wrapper dl_portfolio.backtest cv_portfolio_perf_df dl_portfolio.logger LOGGER dl_portfolio.constant METHODS_MAPPING argparse  ------------------------------------------------ input ------------------------------------------------  dataset1  -------------------------------------------------------------------------------------------------------  Get necessary data  Define paths  Load data  Define paths  Load data  Reorder series  Now parse cv portfolio weights and train weights  if port not in [""equal""  ""equal_classs""]  Get portfolio returns  Format final results",56
EmbeddingPortfolio,"We provide results for ""Does non-linear factorization of financial returns help build better portfolio?"", Spilak, WK Härdle (2022). Please refer to github Wiki for a detailed description on how to use the code!","autoencoder, portfolio, tail-risk, trading strategy, cryptocurrency, deep learning, machine learning, econometrics",Bruno Spilak,EmbeddingPortfolio,11,"!/usr/bin/env python  coding: utf-8  In[1]: os pandas pickle numpy matplotlib seaborn dl_portfolio.data load_data dl_portfolio.utils load_result sys itertools sklearn.neighbors KernelDensity scipy.stats pearsonr warnings  # Some useful functions  In[2]: plt_features features   start  end  dataset  savepath=None  Start the arrow at the origin 0 for PC1 1 for PC2  width = 0.01  # float  default: 0.001  Start the arrow at the origin  0 for PC1 1 for PC2  width = 0.01  # float  default: 0.001  Add a unit circle for scale get_corr_factor_asset test_data  features  corr=""pearson"" intrepretation_plot test_data  features  dataset  labels  corr=""pearson""  savepath=None  # Input: set datatset name  ""dataset1"" or ""dataset2"")  In[3]:  # Save directory  In[4]:  # Load results  In[ ]:  Load evaluation  Load results ae_config nmf_config  # Factors  ## Explainability  In[6]:  cmap='Reds')  cmap='Reds')  ## Interpretation  In[7]:  In[8]:  ## Factors distribution  In[9]:  In[ ]:",129
EmbeddingPortfolio,"We provide results for ""Does non-linear factorization of financial returns help build better portfolio?"", Spilak, WK Härdle (2022). Please refer to github Wiki for a detailed description on how to use the code!","autoencoder, portfolio, tail-risk, trading strategy, cryptocurrency, deep learning, machine learning, econometrics",Bruno Spilak,EmbeddingPortfolio,11,setuptools setup,2
EmbeddingPortfolio,"We provide results for ""Does non-linear factorization of financial returns help build better portfolio?"", Spilak, WK Härdle (2022). Please refer to github Wiki for a detailed description on how to use the code!","autoencoder, portfolio, tail-risk, trading strategy, cryptocurrency, deep learning, machine learning, econometrics",Bruno Spilak,EmbeddingPortfolio,11,dl_portfolio.run run_ae dl_portfolio.logger LOGGER joblib Parallel os dl_portfolio.constant LOG_DIR dl_portfolio.data load_data argparse dl_portfolio.config ae_config dl_portfolio.config nmf_config,16
EmbeddingPortfolio,"We provide results for ""Does non-linear factorization of financial returns help build better portfolio?"", Spilak, WK Härdle (2022). Please refer to github Wiki for a detailed description on how to use the code!","autoencoder, portfolio, tail-risk, trading strategy, cryptocurrency, deep learning, machine learning, econometrics",Bruno Spilak,EmbeddingPortfolio,11,"datetime logging os pickle sys matplotlib numpy pandas seaborn sklearn metrics dl_portfolio.backtest bar_plot_weights dl_portfolio.cluster get_cluster_labels dl_portfolio.evaluate average_prediction dl_portfolio.logger LOGGER dl_portfolio.constant BASE_FACTOR_ORDER_DATASET2 argparse  os.makedirs f""{save_dir}/cv_plots/"")  Load paths ae_config nmf_config  Load Market budget  market_budget = market_budget.drop 'CRIX')  Main loop to get results  ['aerp'  'aeerc'  'ae_rp_c']  Get average weights for AE portfolio across runs  Build dictionary for cv_portfolio_perf  if port not in ['equal'  'equal_classs']  Get portfolio weights time series  port_weights = {}  for p in PORTFOLIOS:      if p not in ['equal'  'equal_class']:          port_weights[p] = get_ts_weights cv_results  port=p)  Get average perf across runs  Plot excess return  # Plot one cv weight  CV = 0  plt.figure figsize= 14  7))  plt.bar ASSETS  port_weights['hrp'].iloc[CV].values  label='hrp')  plt.bar ASSETS  port_weights['aerp'].iloc[CV].values  label='aerp')  plt.legend )  plt.ylim [0  0.9])  x = plt.xticks rotation=45)  if args.save:      plt.savefig f""{save_dir}/weights_hrp_aerp.png""  bbox_inches='tight'  transparent=True)  plt.figure figsize= 14  7))  plt.bar ASSETS  port_weights['hrp'].iloc[CV].values  label='hrp')  plt.bar ASSETS  port_weights['ae_rp_c'].iloc[CV].values  label='ae_rp_c')  plt.legend )  plt.ylim [0  0.9])  x = plt.xticks rotation=45)  if args.save:      plt.savefig f""{save_dir}/weights_hrp_aeerc_cluster.png""  bbox_inches='tight'  transparent=True)  plt.figure figsize= 14  7))  plt.bar ASSETS  port_weights['hcaa'].iloc[CV].values  label='hcaa')  plt.bar ASSETS  port_weights['aeerc'].iloc[CV].values  label='aeerc')  plt.legend )  plt.ylim [0  0.9])  x = plt.xticks rotation=45)  if args.save:      plt.savefig f""{save_dir}/weights_hcaa_aeerc.png""  bbox_inches='tight'  transparent=True)  Get statistics #########################  Model evaluation  Average prediction across runs for each cv  Compute pred metric  scaler = preprocessing.MinMaxScaler feature_range= -1  1))  scaler.fit returns[cv])  same_ret = pd.DataFrame scaler.transform returns[cv])  index=returns.index  columns=returns.columns)  same_pred = pd.DataFrame scaler.transform pred[cv])  index=returns.index  columns=returns.columns)  Average prediction across runs  Compute pred metric  loading analysis  loading over cv folds  Correlation  Ex factor correlation cv = 0  Ex pred correlation cv = 0  Cluster analysis  Plot heatmap  Plot heatmap of average rand  Consensus matrix  Save final result",258
EmbeddingPortfolio,"We provide results for ""Does non-linear factorization of financial returns help build better portfolio?"", Spilak, WK Härdle (2022). Please refer to github Wiki for a detailed description on how to use the code!","autoencoder, portfolio, tail-risk, trading strategy, cryptocurrency, deep learning, machine learning, econometrics",Bruno Spilak,EmbeddingPortfolio,11,!/usr/bin/env python  coding: utf-8  In[1]: pandas matplotlib numpy dl_portfolio.data load_data dl_portfolio.backtest plot_perf pickle os sklearn.neighbors KernelDensity seaborn  # Some useful functions  In[2]: plot_weights weights  savepath=None  x_step=2 plot_final_backtest_performance returns  benchmark  save=False  save_path=None  hedge=False load_all_backtest_result ae_dir  nmf_dir  markowitz_dir  dataset  Load Market budget  market_budget = market_budget.drop 'CRIX')  Load markowitz and robust M results  Leverage  # Save directory  In[3]:  # Load backtest result  In[4]:  # Dataset 1  ## Backtest performance  In[5]:  ## Backtest stats  In[6]:  ## Weights  In[7]:  # Dataset 2  ## Backtest performance  In[8]:  ## Backtest stats  In[9]:  ## Weights  In[10]:  In[ ]:,90
CV05_Preprocessing,"Merges all the components i.e. tweets, trends and market data that have been scraped into one file and applies sentiment analysis on the tweets","merge, consolidation, final, components, preprocessing, sentiment",Fabian Schmidt,CV05_Preprocessing,5,!/usr/bin/env python  coding: utf-8  In[1]: pandas numpy datetime datetime matplotlib re nltk nltk.tokenize word_tokenize string punctuation nltk.corpus stopwords SentimentAnalyser SentimentAnalyser nltk.tokenize.treebank TreebankWordDetokenizer  ## Read in data   ### Market data  In[2]:  #### Convert unix code time to more readable datetime format  In[3]:  In[4]:  ### Volatility data  In[5]:  In[6]:  In[7]:  btc_volatility.drop columns=['Unnamed: 0'  'n']  inplace=True)  btc_volatility = btc_volatility[ btc_volatility['date'] >= '2020-01-01') &  btc_volatility['date'] < '2020-0-01')]  btc_volatility  In[8]:  ### Trend  In[9]:  In[10]:  ### Tweets  In[11]:  sentiment analyser class  In[12]:  stopwords for cleaning  In[13]:  cleaning method for tweets processTweet tweet  convert text to lower-case  remove URLs  remove usernames  [^\s]+)'  r'\1'  tweet) # remove the # in #hashtag  remove repeated characters  helloooooooo into hello)  In[14]: processTweets chunk  clean tweet  perform sentiment analysis on cleaned tweet using Sentiment Analyser  returns dictionary   add to respective lists  In[15]:  read the large csv file with specified chunksize   add nrows=100 for testing df_chunk = pd.read_csv 'data/by_day/tweets_btc_2020-05-12.csv'  chunksize=500)  In[16]:  append each chunk df here   Each chunk is in df format  drop columns where text contains nan  init empty sentiment columns   perform data filtering   set filtered text and sentiments  Once the data filtering is done  append the chunk to list  concat the list into dataframe   In[17]:  In[23]:  In[24]:  In[25]:  check if duplicate tweets exist  In[26]:  drop all rows with duplicate tweet id  check again if duplicate tweets exist  In[27]:  In[28]:  ### Merge two data frames  In[29]:  first merge tweets with hourly trend data  In[30]:  In[31]:  In[32]:  In[33]:  In[34]:  In[36]:  ### Aggregation by day  In[37]:  In[38]:  In[39]:  In[40]:  In[42]:,246
CV05_Preprocessing,"Merges all the components i.e. tweets, trends and market data that have been scraped into one file and applies sentiment analysis on the tweets","merge, consolidation, final, components, preprocessing, sentiment",Fabian Schmidt,CV05_Preprocessing,5,"vaderSentiment.vaderSentiment SentimentIntensityAnalyzer __init__ self sentiment_analyzer_scores self  sentence print ""{:-<40} {}"".format sentence  str score)))",13
CV05_Preprocessing,"Merges all the components i.e. tweets, trends and market data that have been scraped into one file and applies sentiment analysis on the tweets","merge, consolidation, final, components, preprocessing, sentiment",Fabian Schmidt,CV05_Preprocessing,5,!/usr/bin/env python  coding: utf-8  In[1]: pandas numpy datetime datetime matplotlib re nltk nltk.tokenize word_tokenize string punctuation nltk.corpus stopwords SentimentAnalyser SentimentAnalyser nltk.tokenize.treebank TreebankWordDetokenizer  ## Read in data   ### Market data  In[2]:  #### Convert unix code time to more readable datetime format  In[3]:  In[4]:  ### Volatility data  In[5]:  In[6]:  In[7]:  btc_volatility.drop columns=['Unnamed: 0'  'n']  inplace=True)  btc_volatility = btc_volatility[ btc_volatility['date'] >= '2020-01-01') &  btc_volatility['date'] < '2020-0-01')]  btc_volatility  In[8]:  ### Trend  In[9]:  In[10]:  ### Tweets  In[11]:  sentiment analyser class  In[12]:  stopwords for cleaning  In[13]:  cleaning method for tweets processTweet tweet  convert text to lower-case  remove URLs  remove usernames  [^\s]+)'  r'\1'  tweet) # remove the # in #hashtag  remove repeated characters  helloooooooo into hello)  In[14]: processTweets chunk  clean tweet  perform sentiment analysis on cleaned tweet using Sentiment Analyser  returns dictionary   add to respective lists  In[15]:  read the large csv file with specified chunksize   add nrows=100 for testing df_chunk = pd.read_csv 'data/by_day/tweets_btc_2020-05-12.csv'  chunksize=500)  In[16]:  append each chunk df here   Each chunk is in df format  drop columns where text contains nan  init empty sentiment columns   perform data filtering   set filtered text and sentiments  Once the data filtering is done  append the chunk to list  concat the list into dataframe   In[17]:  In[18]:  In[19]:  In[20]:  In[21]:  check if duplicate tweets exist  In[22]:  drop all rows with duplicate tweet id  check again if duplicate tweets exist  In[23]:  In[24]:  ### Merge two data frames  In[27]:  first merge tweets with hourly trend data  In[28]:  In[29]:  In[30]:  In[31]:  In[32]:  In[33]:  ### Aggregation by day  In[34]:  In[35]:  In[36]:  In[37]:,246
pyTSA_ReturnsPG,"This Quantlet plots monthly time series of returns of Procter and Gamble from 1961 to 2016 and  their ACF and PACF (Example, 2.4 Figures 2.8-2.9 in the book)","time series, autocorrelation, returns, ACF, PACF, plot, visualisation","Huang Changquan, Alla Petukhina",pyTSA_CointegrationOil,1,numpy pandas matplotlib statsmodels.tsa.stattools coint statsmodels.tsa.vector_ar.vecm VECM coint_johansen  PythonTsa.plot_multi_ACF multi_ACFfig PythonTsa.plot_multi_Q_pvalue MultiQpvalue_plot,12
A-Shape_CRIX_VCRIX,Several plots of Alpha Shapes over the scatter plot 0x=CRIX vs. 0y=VCRIX for different values of Alpha,"Alpha shape (a-shape), CRIX, VCRIX",Elena Ivanova,A-Shape_CRIX_VCRIX,2,!/usr/bin/env python  coding: utf-8  In[ ]: pandas numpy matplotlib inline datetime matplotlib scipy.spatial Delaunay numpy alpha_shape points  alpha  only_outer=True add_edge edges  i  j  already added  if both neighboring triangles are in shape  it's not a boundary edge  Loop over triangles:  ia  ib  ic = indices of corner points of the triangle  Computing radius of triangle circumcircle  www.mathalino.com/reviewer/derivation-of-formulas/derivation-of-formula-for-radius-of-circumcircle matplotlib.pyplot  Computing the alpha shape   label='alpha')  ax.legend loc='best'),65
pyTSA_BTC,"This Quantlet produces plots of closing daily prices, log-returns and their ACF for Time series of BTC daily prices from June 23, 2017 to June 22, 2018.","time series, BTC, Bitcoin, autocorrelation, log-returns",ERROR,pyTSA_BTC,1,"pandas numpy matplotlib PythonTsa.plot_acf_pacf acf_pacf_fig delete ""NaN""",7
SFM_Sim_Pareto_like,SFM_Sim_pareto simulates Y=-X ~ Pareto distribution.,"Pareto, density, simulation",Daniel Traian Pele,SFM_Sim_Pareto_like,2,!/usr/bin/env python  coding: utf-8  In[6]: numpy scipy.stats pareto matplotlib  In[ ]:  In[ ]:  In[ ]:  In[ ]:  In[ ]:,19
LSTM,Use machines learning algorithms to predict daily price move in Bitcoin and perform day trading,"Bitcoin, SVM, Random Forest, Neural Network, LSTM","Hong Yuxi, Lin Jinyu, Sun Rongsheng, Zhou Xiaoqian, Zou Yutong",LSTM_,4,!/usr/bin/env python  coding: utf-8  In[10]: import needed packages numpy matplotlib pandas datetime datetime math itertools sklearn preprocessing datetime operator itemgetter sklearn.metrics mean_squared_error math sqrt keras.models Sequential keras.layers.core Dense keras.layers.recurrent LSTM matplotlib.pyplot MultipleLocator os os.path dirname  In[11]: read in Bitcoin dataframe  In[3]: drop out unwanted data  In[4]: normalizing the data sklearn.preprocessing MinMaxScaler  In[76]: set the amount of feature set the window of prediction the prediction is based on all the information observed within a window of 5 days turn the datafram into array transform data according to window and features LSTM expect our data to be in a specific format  usually a 3D array. split data into training data and testing data you may change the cut to test different period  In[77]: the shape of training data and testing data  In[78]: LSTM model construction initialize the neural network  add the Long Short-Term Memory layer add dropout layers that prevent overfitting  add the Long Short-Term Memory layer add dropout layers that prevent overfitting  add a densely connected neural network layer  compile model using the popular adam optimizer and set the loss as the mean_squarred_error  In[79]: LSTM model summary  In[80]: model prediction outcome  In[81]: invert scaling for forecast testing data  In[82]: invert scaling for actual testing data  In[99]: plot the original and predict testing data  In[84]:  invert scaling for forecast training data  In[85]:  invert scaling for actual training data  In[98]: plot the original and predict training data  In[87]: score the result by MSE MAE sklearn.metrics mean_absolute_error sklearn.metrics mean_squared_error math mape y_true  y_pred  In[88]: the accuracy rate of price movements  In[ ]:,258
LSTM,Use machines learning algorithms to predict daily price move in Bitcoin and perform day trading,"Bitcoin, SVM, Random Forest, Neural Network, LSTM","Hong Yuxi, Lin Jinyu, Sun Rongsheng, Zhou Xiaoqian, Zou Yutong",LSTM_,4,!/usr/bin/env python  coding: utf-8  In[1]: import needed packages numpy matplotlib pandas datetime datetime math itertools sklearn preprocessing datetime operator itemgetter sklearn.metrics mean_squared_error math sqrt keras.models Sequential keras.layers.core Dense keras.layers.recurrent LSTM matplotlib.pyplot MultipleLocator os os.path dirname  In[2]: read in Bitcoin dataframe  In[3]: drop out unwanted data  In[4]: normalizing the data sklearn.preprocessing MinMaxScaler  In[5]: set the amount of feature set the window of prediction the prediction is based on all the information observed within a window of 5 days turn the datafram into array transform data according to window and features LSTM expect our data to be in a specific format  usually a 3D array. split data into training data and testing data  In[71]: LSTM model construction initialize the neural network  add the Long Short-Term Memory layer add dropout layers that prevent overfitting  add the Long Short-Term Memory layer add dropout layers that prevent overfitting  add a densely connected neural network layer  compile model using the popular adam optimizer and set the loss as the mean_squarred_error  In[72]: LSTM model summary  In[73]: model prediction outcome  In[74]: obtain prediction signals: 1=buy 0=sell  In[75]:  In[84]: trading strategy:1 long;0 short calculate trading strategy performance'  In[80]:  calculate trading strategy volatility and Sharpe ratio  annualised) 10Y bond yield  In[ ]:,202
LSTM_Architecture,Definition of the architecture of the LSTM that is used for the time-series prediction of the hourly returns.,"LSTM, architecture, time-series, prediction","Georg Velev, Iliyana Pekova",LSTM_Architecture,1,Definition of a custom error-function keras root_mean_squared_error y_true  y_pred Definition of the architecture of LSTM: multi_step_LSTM n_steps_in=24  n_features=1 n_steps_out=24 nr_neurons=100 lr=0.01 epochs=1 tensorflow keras keras optimizers keras.models Sequential keras.layers Dense keras.layers.advanced_activations LeakyReLU,32
SFEplotma1,"Plots two realizations of an MA(1) (moving average) process with MA coefficient = beta, random normal innovations and n=n1 (above) and n=n2 (below).","moving-average, stationary, linear, discrete, simulation, time-series, process, stochastic-process, stochastic, plot, graphical representation","Joanna Tomanek, Lasse Groth, WK Härdle",QID-1284-SFEplotma1,1,pandas numpy statsmodels.graphics.tsaplots plot_acf matplotlib statsmodels.tsa.arima_process ArmaProcess  parameter settings  lag value  Obtain MA 1) sample by sampling from a ARMA ) model with no AR coefficient title='Sample ACF of an MA 1) Process'),33
pyTSA_CorticoSales,This Quantlet builds plots time series and ACF of Chinese quarterly GDP from 1992 to 2017,"time series, autocorrelation, Chinese GDP, plot, visualisation","Huang Changquan, Alla Petukhina",pyTSA_CorticoSales,1,pandas numpy statsmodels matplotlib PythonTsa.plot_acf_pacf acf_pacf_fig PythonTsa.ModResidDiag plot_ResidDiag  [1  1  1  0  1] means ma.L4 = 0 by default.,19
RVOLharfnn,Forecasts the realized volatility of a portfolio from cryptocurrencies by the means of the Heterogeneous autoregressive (HAR) model and a hybrid between HAR and a simple feedforward neural network (FNN-HAR).,"cryptocurrency, RV, HAR, FNN, realized, volatility",Ivan Mitkov,RVOLharfnn,2,!/usr/bin/env python3  -*- coding: utf-8 -*- warnings  Make sure that all of the modules are already installed in Anaconda. If not    the following comands should be ran in Terminal  Improting the packages pickle pandas pandas Series matplotlib numpy statsmodels.graphics.tsaplots plot_acf statsmodels statsmodels.tsa.stattools adfuller statsmodels.tsa.stattools adfuller statsmodels.stats.diagnostic itertools sys pylab scipy scipy.stats kurtosis scipy.stats skew scipy.stats norm scipy.stats jarque_bera numpy.random normal math sklearn.preprocessing MinMaxScaler sklearn.metrics mean_squared_error keras.layers keras optimizers keras.constraints NonNeg keras.layers.advanced_activations,71
RVOLharfnn,Forecasts the realized volatility of a portfolio from cryptocurrencies by the means of the Heterogeneous autoregressive (HAR) model and a hybrid between HAR and a simple feedforward neural network (FNN-HAR).,"cryptocurrency, RV, HAR, FNN, realized, volatility",Ivan Mitkov,RVOLharfnn,2,!/usr/bin/env python3  -*- coding: utf-8 -*- train_test_data dataframe  fraction  RNN pandas numpy numpy sqrt  Split the targets into training/testing sets train_test_var dataframe  n_rows  RNN pandas numpy numpy sqrt  Split the targets into training/testing sets naive_erros train_Y  test_Y  NAIVE_train  NAIVE_test pandas math sqrt sklearn.metrics mean_squared_error __init__ self  RV_type  trainX  trainY  testX  testY train self matplotlib numpy sklearn linear_model keras.models Sequential keras.layers Dense keras optimizers  Create linear regression object  Train the model using the training sets predict self pandas  Make predictions using the testing set evaluate self  train = False  test = False pandas math sqrt sklearn.metrics mean_squared_error  Some Error Measures print self.model.evaluate self.trainX  self.trainY  verbose=0))  Some Error Measures error_df self pandas math sqrt sklearn.metrics mean_squared_error  Actual analysis warnings  Set wordking directiory os  Please note that the code works without any errors only if you run the whole script at once  If you run partial codes then you have to replace manually the working directory with your path  os.chdir r'/hier/comes/your/path/link')  Impot of packages dalib.packages  In order to keep constant outputs  avoiding different weights initialization numpy.random seed  Slicing the data frame  long version  short version          long version  short version                  # Scenario high volatiolity time  longer training  Import data  Train-Test-Data  Naïve model  HAR-RV  FNN-HAR  Save the predictions and errors  # Scenario high volatiolity time  shorter training  Scenario high volatiolity time  longer training  Import data  Train-Test-Data  Naïve model  HAR-RV  FNN-HAR  Save the predictions and errors  # Scenario low volatiolity time  longer training  Import data  Train-Test-Data  Naïve model  HAR-RV  FNN-HAR  Save the predictions and errors  # Scenario low volatiolity time  shorter training  Scenario high volatiolity time  longer training  Import data  Train-Test-Data  Naïve model  HAR-RV  FNN-HAR  Save the predictions and errors,274
pyTSA_EconUS,"This Quantlet plots monthly time series of returns of Procter and Gamble from 1961 to 2016 and  their ACF and PACF (Example, 2.4 Figures 2.8-2.9 in the book)","time series, autocorrelation, returns, ACF, PACF, plot, visualisation","Huang Changquan, Alla Petukhina",pyTSA_EconUS,1,pandas matplotlib statsmodels PythonTsa.plot_acf_pacf acf_pacf_fig PythonTsa.LjungBoxtest plot_LB_pvalue PythonTsa.plot_multi_ACF multi_ACFfig statsmodels.tsa.statespace.sarimax SARIMAX arch arch_model  the residuals are of Model  8.16b) and so noestimatedcoef = 2.  mean = 'Zero' means no mean equation in GARCH models.,34
pyTSA_causality,This Quantlet simulates and plots causal and noncausal AR(5) - autoregressive processes as describe in Example 3.9 in the book,"time series,  stationarity, autocorrelation,  simulation, stochastic process, ARMA, moving average, autoregression, causality",ERROR,pyTSA_Causality,1,numpy pandas matplotlib statsmodels.tsa.arima_process arma_generate_sample  AR model  i)  AR model  ii),11
MPT_Model_Benchmark,"Random Benchmarks to evaluate the trained Email Processing Time model. 3 different Benchmarks were applied. A stratified random classifier, a classifier always selecting the most frequent class and a classifier selecting the two most frequent classes based on their probabilities.","Machine Learning (ML), NLP, Textual Analysis, Word Embeddings, SIF Embedding, FastText, Dummy Model Performance Benchmark",Marvin Gauer,MPT_Model_Benchmark,1,os sys sklearn.dummy DummyClassifier pandas numpy.random choice sklearn.metrics f1_score  load the true labels F1 y_true y_pred  type = None  Create Graph  Classify all as either the most or second most frequent class -> 0  Prob: 0.504) or 2  Prob: 0.496,40
BTC_ANA_data_collection,Scraping and preprocessing of labeled btc addresses from differen websites. Collection of specified transactions from a full bitcoin blockchain node hosted on Google Big Query,"Scraping, Bitcoin, Blockchain, Transactions, BigQuery","David Steiner, Minh Nguyen",BTC_ANA_data_collection,7," -*- coding: utf-8 -*- os pandas matplotlib google.cloud bigquery pip install google-cloud-bigquery https://cloud.google.com/docs/authentication/getting-started  =============================================================================  Helpers  ============================================================================= get_atts obj  filter="""" estimate_gigabytes_scanned query  bq_client  =============================================================================  get transactions with max transaction value  ============================================================================= get_all_tx_over_value btc btc in satoshi      =============================================================================  get all transactions for a list of addresses  =============================================================================    get_all_tx_from_address list_addresses",47
BTC_ANA_data_collection,Scraping and preprocessing of labeled btc addresses from differen websites. Collection of specified transactions from a full bitcoin blockchain node hosted on Google Big Query,"Scraping, Bitcoin, Blockchain, Transactions, BigQuery","David Steiner, Minh Nguyen",BTC_ANA_data_collection,7, -*- coding: utf-8 -*- pandas requests bs4 BeautifulSoup,8
BTC_ANA_data_collection,Scraping and preprocessing of labeled btc addresses from differen websites. Collection of specified transactions from a full bitcoin blockchain node hosted on Google Big Query,"Scraping, Bitcoin, Blockchain, Transactions, BigQuery","David Steiner, Minh Nguyen",BTC_ANA_data_collection,7," -*- coding: utf-8 -*- pandas requests bs4 BeautifulSoup  =============================================================================  Scraper  =============================================================================  =============================================================================  Data Cleaning  ============================================================================= get_owner address_full check_wallet_type row elif row['number_ins'] >= 200:     return ""EXCHANGE"" elif row['balance'] >= 10000:     return ""WHALE"" else:     return ""BIG_FISH""  =============================================================================  Export  =============================================================================",37
BTC_ANA_data_collection,Scraping and preprocessing of labeled btc addresses from differen websites. Collection of specified transactions from a full bitcoin blockchain node hosted on Google Big Query,"Scraping, Bitcoin, Blockchain, Transactions, BigQuery","David Steiner, Minh Nguyen",BTC_ANA_data_collection,7, -*- coding: utf-8 -*- pandas  =============================================================================  Load Data Sources  address  owner  category  =============================================================================  =============================================================================  Recategorize specific categories  ============================================================================= change specific owner and category values Change duplicate name Remove Historic  =============================================================================  Export data to CSV  ============================================================================= Export to csv,38
BTC_ANA_data_collection,Scraping and preprocessing of labeled btc addresses from differen websites. Collection of specified transactions from a full bitcoin blockchain node hosted on Google Big Query,"Scraping, Bitcoin, Blockchain, Transactions, BigQuery","David Steiner, Minh Nguyen",BTC_ANA_data_collection,7, -*- coding: utf-8 -*- pandas numpy  =============================================================================  Scrape labeled addresses  !Please run scripts manually due to very long runtime!  ============================================================================= WalletExplorer.com  20 mio addresses for categories Exchange  Service  Pool  Gambling  Historic) --> sraper_walletexplorer.py Bitinfocharts.com  --> scraper_bitinfocharts.py  top 10.000 BTC adresses) --> scraper_bitinfocharts_tor.py  scrape specific addresses one by one. TOR Browser neccessary) Cryptoground --> Scrape specific addresses for Mt.Gox Exchange  =============================================================================  Preprocess and merge all addresses from different data sources  !Please run scripts manually due to very long runtime!  ============================================================================= -->wallet_preprocessing.py  =============================================================================  Get all transactions over a specific value of btc  ============================================================================= bigquery_btc_node get_all_tx_over_value  =============================================================================  Get complete transaction history for samples of addresses per category  !GOOGLE DEVELOPER CREDENTIALS NECCESSARY!  ============================================================================= bigquery_btc_node get_all_tx_from_address  =============================================================================  Get complete transaction history for unknown addresses  Addresses will be processed in bulk for memory issues  !GOOGLE DEVELOPER CREDENTIALS NECCESSARY!  =============================================================================,133
BTC_ANA_data_collection,Scraping and preprocessing of labeled btc addresses from differen websites. Collection of specified transactions from a full bitcoin blockchain node hosted on Google Big Query,"Scraping, Bitcoin, Blockchain, Transactions, BigQuery","David Steiner, Minh Nguyen",BTC_ANA_data_collection,7, -*- coding: utf-8 -*- pandas requests bs4 BeautifulSoup re threading time traceback lxml.html fromstring itertools cycle sqlalchemy create_engine importlib DB connection proxies = pd.read_csv 'proxies.txt'  names=['ips']  header=None  index_col=False) #https://proxyscrape.com/free-proxy-list proxies =  proxies.ips.values.tolist ) scrape_owner owner  category_name  proxy df_all_wallets = df_all_wallets.append df)  =============================================================================   Proxies  ============================================================================= get_proxies  Grabbing IP and corresponding PORT find_working_proxy url get_response proxy  url  =============================================================================  START      ============================================================================= if counter == 1 or counter == 2 or counter == 3:,70
BTC_ANA_data_collection,Scraping and preprocessing of labeled btc addresses from differen websites. Collection of specified transactions from a full bitcoin blockchain node hosted on Google Big Query,"Scraping, Bitcoin, Blockchain, Transactions, BigQuery","David Steiner, Minh Nguyen",BTC_ANA_data_collection,7, -*- coding: utf-8 -*- pandas numpy bs4 BeautifulSoup threading time random torrequest TorRequest for windows https://github.com/erdiaker/torrequest/issues/2  =============================================================================  Scraper  =============================================================================       scrape_owner df  =============================================================================  Export to csv  ============================================================================= remove_digits address,28
SFEWNSimu,Investigates time series of white noise,"time series, white noise, WN, TS, stationarity, python","Bharathi Srinivasan, David Berscheid",SFEWNSimu,1,white_noise  os sys pandas pandas_datareader numpy statsmodels statsmodels statsmodels scipy arch arch_model matplotlib matplotlib  raw adjusted close prices  log returns tsplot y  lags=None  figsize= 10  8)  style='bmh' mpl.rcParams['font.family'] = 'Ubuntu Mono'  plot of discrete white noise,36
SL_20190404_Wordcloud,"Creates a word cloud for one of the three given texts (an Austrian novel - ""Der Mann ohne Eigenschaften"" by Robert Musil, a songtext - ""99 Luftballons"" by Nena or a poem - ""Gemeinsam"" by Hanna Schnyders) in the shape of a man with hat, a balloon or a star. A word cloud is a data visualization technique used for representing text data in which the size of each word indicates its frequency or importance.","Wordcloud, nltk, stopwords, mask, numpy, Python, text data, frequency, data visualization, natural language processing","Anna Pacher, Patrick Plum, Kathrin Spendier",SL_20190404_Wordcloud,1," -*- coding: utf-8 -*-  importing the necessery modules  wordcloud WordCloud matplotlib csv os os path PIL Image numpy nltk nltk.corpus stopwords  Please change the working directory to your path! import german stopwords  these are words that will be excluded from the wordcloud  for instance articles and prepositions)  file object is created   here it can be chosen which text should be displayed file_ob = open path.join root_path  '99Luftbalons.txt')  encoding='utf-8')  file_ob = open path.join root_path  'gedicht.txt')  encoding='utf-8')   1. text: Chapter one and two of Musil's book ""Der Mann ohne Eigenschaften"" taken form http://musilonline.at/musiltext/  2. https://www.songtexte.com/songtext/nena/99-luftballons-63dcfa57.html  3. taken from http://www.gedichte-zitate.com/gedichte.html  reader object is created   contents of reader object is stored .   data is stored in list of list format.   empty string is declare   iterating through list of rows   iterating through words in the row   concatenate the words   Mask balloon = np.array Image.open 'balloon.jpeg')) star = np.array Image.open path.join root_path  ""star.jpeg""))) ########################################  remove stopwords from WordCloud   show  200 words in the wordcloud .  mask = balloon  mask= star   plot the WordCloud image    store to file wordcloud.to_file ""99luftballons.png"") wordcloud.to_file ""poem.png"") ######################################## mask = balloon  mask= star   plot the WordCloud image  ",185
Regulatory_Complexity_Plots,Plots the different complexity measures and their yearly aggregates over time,"Regulatory_Complexity, complexity-measure, plot, plotly, relative-change, percentage-change",Sabine Bertram,Regulatory_Complexity_Plots,1,!/usr/bin/python  -*- coding: utf-8 -*- ###############################################################################  Plots the different complexity measures over time  input: complexity measures  mean  std  iqr) in /Regulatory_Complexity_Distances/output  output: folder 'plots' with plots of          1. relative change of the complexity over time  relative to the initial value          2. percentage change of the complexity over time ###############################################################################  imports os glob datetime plotly plotly numpy pandas ###############################################################################  functions  preprocessing txt files with complexity measures processing filename  time ###############################################################################  main  set import path  create directory for plot output  create time series  read data  convert data to dataframe  plot relative change in datapoints  plot percentage change in datapoints  generate yearly aggregates  plot relative change in datapoints  plot percentage change in datapoints,110
pyTSA_MarkovReturnsSP500,"This Quantlet plots monthly time series of returns of Procter and Gamble from 1961 to 2016 and  their ACF and PACF (Example, 2.4 Figures 2.8-2.9 in the book)","time series, autocorrelation, returns, ACF, PACF, plot, visualisation","Huang Changquan, Alla Petukhina",pyTSA_MarkovReturnsSP500,1,numpy pandas statsmodels matplotlib PythonTsa.plot_acf_pacf acf_pacf_fig PythonTsa.LjungBoxtest plot_LB_pvalue statsmodels.tsa.regime_switching.tests.test_markov_regression areturns,10
LLE_Firm_Reports,Plotting the 2 company reports using LLE,ERROR,Elizaveta Zinovyeva,LLE_Firm_Reports,2,!/usr/bin/env python  coding: utf-8  In[4]: numpy pandas sklearn.manifold TSNE matplotlib pyplot  In[5]:  com1 = com1[-com1.token.str.contains '\ |\)|\.|\ ')]  com2 = com2[-com2.token.str.contains '\ |\)|\.|\ ')]  In[6]:  In[7]:  In[8]:  In[9]:  In[10]:  In[11]:  In[12]:  In[13]:  prepare for plotting  In[14]:  In[16]:  In[ ]:,39
CSC_Dapp_classification,Classification of smart contracts using traditional and deep learning methods,ERROR,"Elizaveta Zinovyeva, Raphael Constantin Georg Reule",CSC_Dapp_classification,8,!/usr/bin/env python  coding: utf-8  In[1]: numpy matplotlib os pandas sklearn.pipeline Pipeline sklearn.model_selection GridSearchCV GradientBoostingClassifier sklearn.model_selection StratifiedKFold sklearn.linear_model LogisticRegression sklearn.naive_bayes GaussianNB random sklearn metrics collections Counter argparse sklearn.model_selection cross_validate sklearn.metrics roc_auc_score sklearn.feature_extraction.text TfidfVectorizer sklearn.linear_model SGDClassifier sklearn.model_selection ParameterGrid lightgbm seaborn sklearn.preprocessing LabelEncoder re tqdm tqdm sklearn.manifold TSNE mpl_toolkits.mplot3d Axes3D  In[2]:  In[3]:  In[4]:  In[5]:  make sure in the test set we do not have our DApps SCs  In[6]:  In[7]: prep row  In[8]:  In[9]:  In[10]: stop_words    = 'english'   max 144018  In[11]:  In[12]:  In[13]:  TRAIN ON WHOLE DATA AND PREDICT ON TEST  In[14]:  In[15]:  In[16]:  In[17]: D3D5D2'  258AD1'  CA2C19'  CF58EC'  EA8F34'})  In[28]: D3D5D2'  258AD1'  CA2C19'  CF58EC'  EA8F34'}  In[35]:  In[43]:  In[41]:  Hide grid lines  Hide axes ticks  In[ ]:,113
CSC_Dapp_classification,Classification of smart contracts using traditional and deep learning methods,ERROR,"Elizaveta Zinovyeva, Raphael Constantin Georg Reule",CSC_Dapp_classification,8,"numpy pandas keras keras.layers Embedding keras.layers Dense keras.layers Bidirectional keras.layers LSTM keras.layers Conv1D keras.layers Dense keras.models Model keras.optimizers RMSprop keras keras.engine.topology Layer keras.models load_model keras.preprocessing text keras initializers sklearn.model_selection StratifiedKFold sklearn.metrics roc_auc_score sklearn.metrics precision_score nltk tokenize nltk ############################################ ####           ATTENTION              ###### ############################################  Code for attention is based on the following implementation https://gist.github.com/cbaziotis/7ef97ccf71cbc14366835198c09809d2 dot_product x  kernel  Code for attention is based on the following implementation https://gist.github.com/cbaziotis/7ef97ccf71cbc14366835198c09809d2 build self  input_shape compute_mask self  input  input_mask=None call self  x  mask=None  Cast the mask to floatX to avoid float64 upcasting in theano compute_output_shape self  input_shape ############################################ ####              MODELS              ###### ############################################   gru_keras max_features  maxlen  bidirectional  dropout_rate  embed_dim  rec_units mtype='GRU'  reduction = None  Code for hierarchical attention network is based on the following implementation https://github.com/richliao/textClassifier/blob/master/textClassifierHATT.py    make_hat max_sent_len  max_sent_amount  max_features  embed_dim  rec_units  dropout_rate cnn_keras max_features  maxlen  dropout_rate  embed_dim  num_filters=300 dl_model model_type='BGRU'  max_features=40000  embed_dim=50  rec_units=150  dropout_rate=0.25  maxlen=400  max_sent_len=100  max_sent_amount=4 ############################################ ####            TRAINING              ###### ############################################ clean_str string string = string.replace "" ""  ""."").replace "";""  ""."").replace "":""  ""."").replace ""-""  ""."") tok_sentence s TRAIN VAL for threshold in [0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9]: clean_str string string = string.replace "" ""  ""."").replace "";""  ""."").replace "":""  ""."").replace ""-""  ""."") tok_sentence s  FULL TRAIN Test",194
CSC_Dapp_classification,Classification of smart contracts using traditional and deep learning methods,ERROR,"Elizaveta Zinovyeva, Raphael Constantin Georg Reule",CSC_Dapp_classification,8,"numpy pandas keras keras.layers Embedding keras.layers Dense keras.layers Bidirectional keras.layers LSTM keras.layers Conv1D keras.layers Dense keras.models Model keras.optimizers RMSprop keras keras.engine.topology Layer keras.models load_model keras.preprocessing text keras initializers sklearn.model_selection StratifiedKFold sklearn.metrics roc_auc_score sklearn.metrics precision_score nltk tokenize nltk ############################################ ####           ATTENTION              ###### ############################################  Code for attention is based on the following implementation https://gist.github.com/cbaziotis/7ef97ccf71cbc14366835198c09809d2 dot_product x  kernel  Code for attention is based on the following implementation https://gist.github.com/cbaziotis/7ef97ccf71cbc14366835198c09809d2 build self  input_shape compute_mask self  input  input_mask=None call self  x  mask=None  Cast the mask to floatX to avoid float64 upcasting in theano compute_output_shape self  input_shape ############################################ ####              MODELS              ###### ############################################   gru_keras max_features  maxlen  bidirectional  dropout_rate  embed_dim  rec_units mtype='GRU'  reduction = None  Code for hierarchical attention network is based on the following implementation https://github.com/richliao/textClassifier/blob/master/textClassifierHATT.py    make_hat max_sent_len  max_sent_amount  max_features  embed_dim  rec_units  dropout_rate cnn_keras max_features  maxlen  dropout_rate  embed_dim  num_filters=300 dl_model model_type='BGRU'  max_features=40000  embed_dim=50  rec_units=150  dropout_rate=0.25  maxlen=400  max_sent_len=100  max_sent_amount=4 ############################################ ####            TRAINING              ###### ############################################ clean_str string string = string.replace "" ""  ""."").replace "";""  ""."").replace "":""  ""."").replace ""-""  ""."") tok_sentence s TRAIN VAL for threshold in [0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9]: clean_str string string = string.replace "" ""  ""."").replace "";""  ""."").replace "":""  ""."").replace ""-""  ""."") tok_sentence s  FULL TRAIN Test",194
CSC_Dapp_classification,Classification of smart contracts using traditional and deep learning methods,ERROR,"Elizaveta Zinovyeva, Raphael Constantin Georg Reule",CSC_Dapp_classification,8,!/usr/bin/env python  coding: utf-8  In[1]: numpy matplotlib os pandas sklearn.pipeline Pipeline sklearn.model_selection GridSearchCV GradientBoostingClassifier sklearn.model_selection StratifiedKFold sklearn.linear_model LogisticRegression sklearn.naive_bayes GaussianNB random sklearn metrics collections Counter argparse sklearn.model_selection cross_validate sklearn.metrics roc_auc_score sklearn.feature_extraction.text TfidfVectorizer sklearn.linear_model SGDClassifier sklearn.model_selection ParameterGrid lightgbm seaborn sklearn.preprocessing LabelEncoder  In[2]:  In[3]:  In[4]:  In[5]:  In[6]:  # Training on code  no comments)  ## Check for possible meta-parameters  In[7]:  In[8]:  In[9]:  In[10]:  ### Ridge  In[11]: stop_words    = 'english'   max 144018  In[12]:  8000 AUC ROC 0.91 AUC PRC 0.621  20000 AUC ROC 0.913 AUC PRC 0.632  40000 AUC ROC  0.904) 0.914 AUC PRC 0.64  0.607)  60000 AUC ROC  0.904) 0.913 AUC PRC 0.641  0.607)  80000 AUC ROC  0.904) 0.913 AUC PRC 0.641  0.607)  In[13]:  use best C  In[14]:  In[15]:  In[16]:  In[17]:  In[18]:  In[19]:  TRAIN ON WHOLE DATA AND PREDICT ON TEST  In[20]:  In[21]:  In[22]:  ### Random Forest  In[23]:  max 144018  In[24]:  In[25]:  In[26]:  TRAIN ON WHOLE DATA AND PREDICT ON TEST  In[27]:  In[28]:  In[29]:  ### SVM  In[30]:  best parameters  print performance  In[31]:  In[32]:  In[33]:  In[34]:  TRAIN ON WHOLE DATA AND PREDICT ON TEST  In[35]:,171
CSC_Dapp_classification,Classification of smart contracts using traditional and deep learning methods,ERROR,"Elizaveta Zinovyeva, Raphael Constantin Georg Reule",CSC_Dapp_classification,8,!/usr/bin/env python  coding: utf-8  In[302]: pandas re numpy math  In[303]:  In[304]:  less than 60 cases -> other  In[305]: prep row  In[306]:  In[307]:  In[308]:  In[309]:  In[310]:  In[311]:  In[312]:,27
pyTSA_GlobalTemperature,This Quantlet builds plots time series and ACF of Chinese quarterly GDP from 1992 to 2017,"time series, autocorrelation, Chinese GDP, plot, visualisation","Huang Changquan, Alla Petukhina",pyTSA_GlobalTemperature,1,"numpy pandas matplotlib math statsmodels PythonTsa.plot_acf_pacf acf_pacf_fig PythonTsa.LjungBoxtest plot_LB_pvalue statsmodels.tsa.arima_model ARMA  transform gtemts into annual mean data  a 432*6 zero matrix  tim is a time variable similar to the function 'time' in R.  tim is standardized for reducing computation error.  adding the constant term.  exog should not include a constant  specifying this in the ""fit"".",55
DEDA_Class_SS2018_DEDA_Sentiment_Regression,Calculates forecasts based on historical data and Sentiment of Stock News,"sentiment, news, text mining, lexicon, forecasting, stocks",Marvin Gauer,DEDA_Class_SS2018_Stock_News_Sentiment,1, Import of packages used bs4 BeautifulSoup re csv requests pandas time datetime datetime quandl plotly plotly plotly matplotlib numpy statsmodels PIL Image wordcloud WordCloud sys warnings  The api key is linked to quandl account  https://www.quandl.com)  It would be necessary for each user to apply an account and use its corresonding api  Functions Words filename = None  header = False  column = 0 plot_figures figures  nrows = 1  ncols=1  name = 'Wordcloud.png' Downloader stock = None  symbol = None  quandl_api_key = QUANDL_API_KEY  List containing all the article links  Temporary list to extract the article links per page  Counter  Create soup using the user input  Search all news article links on the website  Extract all article links  Download Article Texts  Create empty Dictonairy   Set counter  Loop to extract articles  Create temporary soup to extract articles from previous links  Search for scripts  Extract the article  start and end day of the articles  extract historics quotes  incorporate quotes into the articles dataframe  extend doc with the qotes  create empty DataFrame and fill with nested dict values and keys shifter dates = None  dataset = None  column = None  FUN = np.mean  tops = False RegressionTrainer data = None  split = 0.8  model = None  alpha = [0 0.05 0.1 0.15 0.2 0.25 0.3]  intercept = True  For Training and Testing  For returnin in results DataValueGetter df = None  data = None  col_df = None  col_raw = None  mean_return = 0  Scaling = False  Check for new articles  Search all news article links on the website  Extract all article links  Set counter  Loop to extract articles  Create temporary soup to extract articles from previous links  Search for scripts  Extract the article  start and end day of the new articles incl some old days  extract historics quotes  create empty DataFrame and fill with nested dict values and keys  Incorporate Score Columns  remove Links and Special Characters  Get rid of punctuation  Get rid of special characters  Get rid of stopwords  Calculate positive & negative words per article and assign them to DataFrame Plotter forecast = False  data1 = None   model = None  prediction = None   Plotting  Create List of lists with texts raw_data.Text != np.nan  Main  Data Preperation  Please type in the name as described in the presentation  corresponding to the link structure of finanzen.net)  Symbol in terms of the Quandl-EOD dataset  Load the LM-Wordlists in English and German  Download articles  Incorporate Score Columns  incorporate stopwords  Define Mask for stopwords  remove Links and Special Characters  Get rid of punctuation  Get rid of special characters  Get rid of stopwords  Calculate positive & negative words per article and assign them to DataFrame  Create WordCloud for pos and neg words  Graphics  Link for the graphic,445
SFEvolnonparest,"Reads the date, DAX index values, stock prices of 20 largest companies at Frankfurt Stock Exchange (FSE), FTSE 100 index values and stock prices of 20 largest companies at London Stock Exchange (LSE) and estimates the volatility of the DAX and FTSE 100 daily return processes from 1998 to 2007.","conditional mean, conditional variance, interpolation, kernel, Nonparametric, nonparametric estimation, estimation, quadratic kernel, smoothing, volatility, plot, graphical representation, data visualization, dax, ftse100, financial, returns, index, asset, stock-price, time-series","Andrija Mihoci, Maria Osipenko, Awdesch Melzer",QID-3263-SFEvolnonparest,1,numpy pandas matplotlib arch arch_model arch.univariate GARCH statsmodels.tsa.arima_model ARMA datetime log_returns df stocks log returns date observations time steps dax returns  ftse returns  bandwidth  bandwidth quadk x compute quadratic kernel true:1  false:0 lpregest X Y p h estimate conditional first moment estimate conditional second moment conditional variance interpolation ############################ FTSE ######################## estimate conditional second moment conditional variance interpolation,58
gensimLDA,Contains applications based on Python gensim to train and use Latent Dirichlet Allocation (LDA) and Dynamic Topic Models (DTM),"Latent Dirichlet Allocation, Dynamic Topic Model, topic models, text mining, gensim, preprocessing, stop words",Jerome Bau,LDA_DTM,3,logging os gensim corpora gensim.models.wrappers.dtmmodel DtmModel numpy matplotlib matplotlib.legend_handler HandlerLine2D gensimLDA  dependency on gensimLDA.py  folder that contains all the news articles  direct path to the C-Code compiled form David Blei's git   https://github.com/blei-lab/dtm)  should be adapted to the source of articles and content  I named articles per {$month}+{article number}  such as 304 for the 5th selected article in March  number of documents for each month get_texts self __len__ self  collect probabilities for chosen keyterms  plot,74
gensimLDA,Contains applications based on Python gensim to train and use Latent Dirichlet Allocation (LDA) and Dynamic Topic Models (DTM),"Latent Dirichlet Allocation, Dynamic Topic Model, topic models, text mining, gensim, preprocessing, stop words",Jerome Bau,LDA_DTM,3,stop_words get_stop_words gensim corpora pandas __init__ self  Preprocessing  Corpora  Model  Query  Paths define_filenames_and_foldername_source self  filenames  foldername define_filenames_and_foldername_save self  prefix  foldername define_filename_and_foldername_query self  filename  foldername  Preprocessing change_operating_language self  lang  Corpus Creation generate_stop_words self  numbers='include'  custom=[] load_raw_corpus self create_frequency_count self remove_once self save_dict self save_mm self load_corpus_mm self load_corpus_dict self load_corpus_dict_from_text self train_lda_model self  topics=100  chunksize=1000  passes = 3 save_lda_model self create_single_line_query self  query create_new_query_from_raw self transform_query_to_dict self search_query self query_top_topics self  topics,71
gensimLDA,Contains applications based on Python gensim to train and use Latent Dirichlet Allocation (LDA) and Dynamic Topic Models (DTM),"Latent Dirichlet Allocation, Dynamic Topic Model, topic models, text mining, gensim, preprocessing, stop words",Jerome Bau,LDA_DTM,3,gensimLDA  saved all query articles in a folder called query  filter meta_topics and stop word-like topics dist a b transform_to_non_sparse_vector  create_matches ,21
USC_Clustering,Clustering & Classifying Ethereum Smart Contracts,"hash, algorithm, crix, cryptocurrency, bitcoin, neural network, trading, fintech, Ethereum","Raphael Constantin Georg Reule, Elizaveta Zinovyeva",SC_Clustering,8,!/usr/bin/env python  coding: utf-8  In[1]: os numpy pandas re  In[2]:  In[3]:  In[4]:  0x9a60e133d8382904563a7ce9490ef53f92ff4c01  0x9482a18ed523b1a2f097b480d38852dfff83e0b9  0x3f8a7f9ccd72f8b81c02c4cd3cfdfa30f1be5463  0x651aa5ea257af3d6fd08aedca8c5a446edb7b7a6  In[5]:  derivative and future  derivative and forward  derivative and option  derivative and stock  In[6]:  In[32]: json pprint pprint  In[7]:  In[ ]:,37
USC_Clustering,Clustering & Classifying Ethereum Smart Contracts,"hash, algorithm, crix, cryptocurrency, bitcoin, neural network, trading, fintech, Ethereum","Raphael Constantin Georg Reule, Elizaveta Zinovyeva",SC_Clustering,8,!/usr/bin/env python  coding: utf-8  In[45]:  import modules json pprint pprint pandas numpy keras.preprocessing.text Tokenizer keras.preprocessing.sequence pad_sequences matplotlib pyplot scipy.cluster.hierarchy dendrogram scipy scipy.cluster.hierarchy fcluster sklearn.decomposition PCA sklearn.neighbors DistanceMetric sklearn.manifold TSNE  additional set up nltk.corpus stopwords  In[17]:  In[18]:  In[20]:  In[75]: clean_text text # Remove stop words  In[78]:  text preprocessing  In[79]:  In[80]:  In[81]:  In[99]: sklearn.cluster KMeans numpy  In[100]: print df.head 20)) plt.legend loc='upper center'  bbox_to_anchor= 1.45  0.8)  prop={'size': 10})  In[101]:  the same but with source code  In[103]: clean_text text # Remove stop words  text preprocessing  In[104]:  In[105]: sklearn.cluster KMeans numpy  In[106]: print df.head 20)) plt.legend loc='upper center'  bbox_to_anchor= 1.45  0.8)  prop={'size': 10})  In[ ]:,101
USC_Clustering,Clustering & Classifying Ethereum Smart Contracts,"hash, algorithm, crix, cryptocurrency, bitcoin, neural network, trading, fintech, Ethereum","Raphael Constantin Georg Reule, Elizaveta Zinovyeva",SC_Clustering,8,!/usr/bin/env python  coding: utf-8  # Parse source codes to a Pandas Data Frame  In[1]: os numpy pandas re  In[2]:  In[41]:  Load list from existing dataset  In[4]:  functions contract_name_extract data extract contract name function_name_extract data extract function names and join to one string comments_extract data extract contract comments and join to one text  find all occurance streamed comments  /*COMMENT */) from string  find all occurance singleline comments  //COMMENT\n ) from string code_no_punct data  In[5]:  In[6]:  In[48]:  In[50]:,76
USC_Clustering,Clustering & Classifying Ethereum Smart Contracts,"hash, algorithm, crix, cryptocurrency, bitcoin, neural network, trading, fintech, Ethereum","Raphael Constantin Georg Reule, Elizaveta Zinovyeva",SC_Clustering,8,"!/usr/bin/env python  coding: utf-8  ###   In[22]:  ""Powered by Etherscan.io APIs""  ### Import Modules  In[2]: requests pandas numpy sys time  ### Load List of Contracts  In[3]:  Load list from Italian research  prepare HASHES for the scraping  In[4]:  Load list from existing dataset  In[15]:  In[16]: split_date_year row split_date_month row  In[17]:  ### Constants    In[18]:  In[19]:  ### Function to call the API  In[25]: scrape_ether_contract_and_write adress_array  API_Token  we can do 5 GET/POST requests per sec  save full GET request print f""contract {data['result'][0]['ContractName']} downloaded"")    save solidity source code  save ABI compiled version  ### Function Call  In[26]:",90
MnistDeepLearning,"Create and train four neural networks corresponding to different architectures (Dense, RNN, GRU and LSTM) on Fashion Mnist dataset.","Dense, ANN, MLP, RNN, LSTM, deep learning, mnist, fashion, neural network",Bruno Spilak,mnistDeepLearning,1,sys numpy numpy keras keras.models Sequential keras.layers LSTM keras.datasets fashion_mnist keras.utils to_categorical matplotlib matplotlib encode data __init__ self  model_type  n_layers = 1  n_epochs = 10  Classifier  timesteps to unroll  timesteps to unroll  hidden LSTM units  rows of 28 pixels  an mnist img is 28x28)  mnist classes/labels  0-9)  Size of each batch  Internal __create_model self __load_data self train self  save_model=False evaluate self  model=None,62
Crypto_LSTM_Prediction,use LSTM and GRU to optimize prediction,"crypto, LSTM, GRU, prediction","Wang Wenyi, Liu Zekai, Tang Jiayun, Zhang Lin, Chen Jing",Crypto_LSTM_Prediction,2,!/usr/bin/env python  coding: utf-8  In[2]: tensorflow numpy pandas matplotlib os sklearn.metrics mean_absolute_error tensorflow.keras.layers LSTM tensorflow.keras Sequential tensorflow.keras.layers Dense tensorflow.keras activations  ## Read in data  In[3]:  ## Process BTC data  In[8]: sklearn.preprocessing MinMaxScaler  ## LSTM model for BTC  In[9]:  In[10]:  In[11]:  ## Process ETH data  In[4]: sklearn.preprocessing MinMaxScaler  ## LSTM model for ETH  In[5]:  In[6]:  In[7]:  ## Process BNB Data  In[12]: sklearn.preprocessing MinMaxScaler  ## LSTM model for BNB  In[13]:  In[16]:  In[15]:,70
CSC_BigQuery,"BigQuery on Kaggle platform to obtain dataset with ethereum hashes, contracts",ERROR,"Elizaveta Zinovyeva, Raphael Constantin Georg Reule",CSC_BigQuery,4,"!/usr/bin/env python  coding: utf-8  ### Please use this code as a notebook at Kaggle https://www.kaggle.com/bigquery/ethereum-blockchain  In[1]: google.cloud bigquery pandas  In[2]:  Create a ""Client"" object  Construct a reference to the ""crypto_ethereum"" dataset   API request - fetch the dataset  List all the tables in the ""crypto_ethereum"" dataset  Print names of all tables in the dataset  there's only one!)  In[3]:  Transform the rows into a nice pandas dataframe  Look at the first 10  In[4]:",71
CSC_BigQuery,"BigQuery on Kaggle platform to obtain dataset with ethereum hashes, contracts",ERROR,"Elizaveta Zinovyeva, Raphael Constantin Georg Reule",CSC_BigQuery,4,"!/usr/bin/env python  coding: utf-8  ### Please use this code as a notebook at Kaggle https://www.kaggle.com/bigquery/ethereum-blockchain  In[1]: google.cloud bigquery pandas  In[2]:  Create a ""Client"" object  Construct a reference to the ""crypto_ethereum"" dataset   API request - fetch the dataset  List all the tables in the ""crypto_ethereum"" dataset  Print names of all tables in the dataset  there's only one!)  In[3]:  Transform the rows into a nice pandas dataframe  Look at the first 10  To make querying BigQuery datasets even easier on Kaggle  we have also written some helper functions that are packaged in the [BigQueryHelper module] https://github.com/SohierDane/BigQuery_Helper/blob/master/bq_helper.py) available in Kernels. I'll replicate the code above using the wrapper functions below. Because our query results are cached by default  we don't need to worry about spending more quota by executing the query twice.  In[4]:",130
DEDA_WebScrapingIntro,UNIT 3 and UNIT 4 of DEDA. Introduce webscraping and object-oriented programming,ERROR,"Junjie Hu, Cathy YH Chen, Isabell Fetzer",DEDA_WebScrapingIntro,4,"!/usr/bin/env python  coding: utf-8  # **DEDA Unit 4**: Web Scraping Example - South China Morning Post  In[25]:  Load moduls  requests bs4 BeautifulSoup datetime datetime  needed to retrieve the date of publication   Receiving source code from the South China Morning Post website   In[26]:  Returns the content of the response   …  … in bytes  … in unicode  Using BeautifulSoup to parse webpage source code  ‘html.parser’ also possible   In[27]:  create empty list   For loop iterates over every line in text   create empty dictionary where we gradually attach items to   1) Filter title  link and text content   print filtered_part1[0])   2) Refilter to obtain date  for date   print filtered_part2[0])  Error handling    1) Filter title and link  find title item  find title item  adjust link print news_title) print news_link)  Scanning scraped text  filtered_part1) for right titles   find text   print news_link)  Correcting title errors   add news_title to the dictionary  add news_link to the dictionary   2) Filter date   cuts off the time   print news_date)  add news_date to the dictionary  attach dictionary to list  In[28]:  Load moduls  pandas os  Calling DataFrame constructor on our list   Exporting to .csv file   # Object-Oriented Programming  OOP)    In[29]:  The class Person is inherited from class object  Using __init__ to initialize a class to take arguments  self is default argument that points to the instance __init__ self  first  last  gender  age  In[30]:  The class Student inherited from class Person __init__ self  first  last  gender  age  school  super ) method allows us to handle the arguments from parent class without copying  Child class can also be added new arguments describe self  describe is a method of class Student  In[31]:  student1 is an instance of class Student  Using the attributes in the object student1  Using the methods in the object student1  In[32]:  !pip install feedparser feedparser __init__ self  url __eq__ self  other  __eq__ ) is a magic method that enables comparison two object with == __repr__ self  __repr__ ) is a magic method that enables customization default printed format get_titles self get_specificitem self  item_name  In[33]:  ReadRSSClass is the file name of the module code  Here we use == to validate if two responses of two url are equal  Print out the titles  ## **Application** On Scraping Daily Weather Report of China Cities  In[34]:  Import all the packages you need  always remember that you can find 99% packages you need in python requests  take the website source code back to you urllib  some useful functions to deal with website URLs bs4 BeautifulSoup  a package to parse website source code numpy  all the numerical calculation related methods re  regular expression package itertools  a package to do iteration works pickle  a package to save your file temporarily pandas  process structured data  the path you save your files  This link can represent the domain of a series of websites  In[35]: city_collection   get source code  parse source code  find the items with tag named 'dt'  iterate within all the items  get name of the province  get link to all the cities in the province  save dict in the dict  iterate with the province link to find all the cities  use the urllib package to join relative links in the proper way  In[36]: weather_collection link  print tr_item)  daily_weather = dict )  month_weather.append daily_weather)  In[ ]:  Nice way to get a date string with certain format  '02d' means 2 digits   ==== We have already download the links to all the cities=====   ==== Otherwise  uncomment the function below to retrieve provinces information ======  initialize a dictionary to hold provinces information  This dictionary includes 'province_link' which is the links to find the cities for each province and the 'cities' contains city names and links  provinces_info = city_collection )  # Use this function to retrieve links to all the cities  This is called context management  with open can close the document automatically when the  write  change 'rb' -> 'wb'  pickle.dump provinces_info  cities_file)  # write  The structure is dict in dict  first layer keyword is province name  In each province you can find the cities  In each city  you can find the date  in the date  you can find weather record  In[ ]:  Iterate over different provinces  Iterate cities within each provinces  Iterate over different months  Exercise: Try to convert the ""json""-like format to pandas DataFrame",699
DEDA_WebScrapingIntro,UNIT 3 and UNIT 4 of DEDA. Introduce webscraping and object-oriented programming,ERROR,"Junjie Hu, Cathy YH Chen, Isabell Fetzer",DEDA_WebScrapingIntro,4,!/usr/bin/env python  coding: utf-8  # **DEDA Unit 3:** Introduction of Web Scraping in Python   #### **Reading XML Data Online**  In[13]: requests xml  #### **Reading JSON Data Online**  In[15]: requests json pandas  #### **Webpage with RSS Feed**  In[12]: feedparser  list all titles,41
MLvsGARCH,"We provide results for ""Tail-risk protection: Machine Learning meets modern Econometrics"", Spilak, WK Härdle (2020). Please refer to README2.md for a detailed description on how to use the code.","tail-risk, trading strategy, cryptocurrency, Value-At-Risk, deep learning, machine learning, econometrics, extreme value theory, exceedance probability",Bruno Spilak,MLvsGARCH,7,pandas numpy constant QS_NAME get_dl_perf_df path  [w + '_' + 'q0.%s_' % q + c for c in qdf.columns],19
MLvsGARCH,"We provide results for ""Tail-risk protection: Machine Learning meets modern Econometrics"", Spilak, WK Härdle (2020). Please refer to README2.md for a detailed description on how to use the code.","tail-risk, trading strategy, cryptocurrency, Value-At-Risk, deep learning, machine learning, econometrics, extreme value theory, exceedance probability",Bruno Spilak,MLvsGARCH,7,,0
SL_20200410_LDA,LDA analysis of the abstracts of a set of scientifique papers to understand main topics that are handled in those papers.,"LDA, heat-map, topic-modelling",Bousbiat Hafsa,SL_20200410_LDA,1,"!/usr/bin/env python3  -*- coding: utf-8 -*- random os re nltk os path nltk.stem WordNetLemmatizer nltk.stem.porter PorterStemmer nltk.corpus stopwords nltk.corpus stopwords nltk.stem.wordnet WordNetLemmatizer string gensim gensim corpora matplotlib pandas numpy  Please change the working directory to your path! os.chdir ""/Users/xinwenni/LDA-DTM/xmas_song"")  doc_l.pop )[0]  Regex cleaning  keep only letters  numbers and whitespace  apply regex  apply regex  lower case  nltk clean doc  Importing Gensim  Creating the term dictionary of our courpus  where every unique term is assigned an index.  Converting list of documents  corpus) into Document Term Matrix using dictionary prepared above.  Creating the object for LDA model using gensim library  Running and Trainign LDA model on the document term matrix. print ldamodel.print_topics num_topics=3  num_words=5))  20 need to modify to match the length of vocabulary  plt.imshow zz  cmap='hot'  interpolation='nearest') plt.show )  plt.title ""heatmap xmas song"")",131
DEDA_Class_2018WS_Berlin_Property_Analysis_GWR,"Geographically Weighted Regression (GWR) for analysis of correlation between real estate listing prices and social / location-based features for real estate within Berlin, DE, taking spatial colinearity into consideration.","Spatial Analysis, Social Data, Real Estate, Geographically Weighted Regression, GWR",Alex Truesdale,DEDA_Class_2018WS_Berlin_Property_Analysis_GWR,1, -*- coding: utf-8 -*- os csv sys time numpy pandas io StringIO mgwr.gwr GWR mgwr.sel_bw Sel_BW  Introduction.  Read in data.  Feature subset definition.  listings = listings[['balcony'  'built_in_kitchen'  'cellar'  'courtage'  'garden'                        'is_vacant'  'lift'  'living_space'  'number_rooms'  'private_seller'                        'lat'  'lng'  'price']]  Define predictors and target values.  GWR __enter__ self __exit__ self  *args  Feature subset cuts.  gwr_stats_meta = output[31:33] + output[36:47]  gwr_stats_features = [output[50]] + output[52:63] Full features cuts.,65
ONSM,"An integration of social media characteristics into an econometric framework requires modeling a high dimensional dynamic network with dimensions of parameter $\Theta$ typically much larger than the number of observations. To cope with this problem we impose two structural assumptions onto the singular value decomposition of $\Theta = U D V^{\top}$. Firstly, the matrix with probabilities of connections between the nodes of a network has a rank much lower than the number of nodes. Therefore, there is limited amount of non-zero elements on the diagonal of $D$ and the whole operator admits a lower dimensional factorisation. Secondly, in observed social networks only a small portion of users are highly-affecting, leading to a sparsity regularization imposed on singular vectors $V.$
Using a novel dataset of 1069K messages from 30K users posted on the microblogging platform StockTwits during a 4-year period (01.2014-12.2018) and quantifying their opinions via natural language processing, we model their dynamic opinions network and further separate the network into communities. With a sparsity regularization, we are able to identify important nodes in the network.","network, autoregression, high-dimensional inference, sentiment weight, stocktwits",Yegor Klochkov,ONSM,5,numpy scipy.stats norm get_missing_probabilities mat  conf=0.95  tol=0.1 missing_var X  delta missing_covar X  Y  delta_x  delta_y,15
ONSM,"An integration of social media characteristics into an econometric framework requires modeling a high dimensional dynamic network with dimensions of parameter $\Theta$ typically much larger than the number of observations. To cope with this problem we impose two structural assumptions onto the singular value decomposition of $\Theta = U D V^{\top}$. Firstly, the matrix with probabilities of connections between the nodes of a network has a rank much lower than the number of nodes. Therefore, there is limited amount of non-zero elements on the diagonal of $D$ and the whole operator admits a lower dimensional factorisation. Secondly, in observed social networks only a small portion of users are highly-affecting, leading to a sparsity regularization imposed on singular vectors $V.$
Using a novel dataset of 1069K messages from 30K users posted on the microblogging platform StockTwits during a 4-year period (01.2014-12.2018) and quantifying their opinions via natural language processing, we model their dynamic opinions network and further separate the network into communities. With a sparsity regularization, we are able to identify important nodes in the network.","network, autoregression, high-dimensional inference, sentiment weight, stocktwits",Yegor Klochkov,ONSM,5,read_data missing numpy matplotlib seaborn alternating matrix_competition  SET NUMBER OF CLUSTERS plt.show ),13
ONSM,"An integration of social media characteristics into an econometric framework requires modeling a high dimensional dynamic network with dimensions of parameter $\Theta$ typically much larger than the number of observations. To cope with this problem we impose two structural assumptions onto the singular value decomposition of $\Theta = U D V^{\top}$. Firstly, the matrix with probabilities of connections between the nodes of a network has a rank much lower than the number of nodes. Therefore, there is limited amount of non-zero elements on the diagonal of $D$ and the whole operator admits a lower dimensional factorisation. Secondly, in observed social networks only a small portion of users are highly-affecting, leading to a sparsity regularization imposed on singular vectors $V.$
Using a novel dataset of 1069K messages from 30K users posted on the microblogging platform StockTwits during a 4-year period (01.2014-12.2018) and quantifying their opinions via natural language processing, we model their dynamic opinions network and further separate the network into communities. With a sparsity regularization, we are able to identify important nodes in the network.","network, autoregression, high-dimensional inference, sentiment weight, stocktwits",Yegor Klochkov,ONSM,5,numpy matplotlib cvxopt matrix cvxopt.solvers qp seaborn scipy.linalg sqrtm import read_data import missing kmeans_greedy gauss_var1_process theta  sigma  T  variance of innovations is sigma * I lasso_from_covariance D  c  alpha  minimizes 1/2 x^{T}Dx - c^{T} x + \alpha \| v \|_1 random_basis n  k  orthogonal normalization of columns u_random n  k  cluster_num  index=None v_step_normalized D0  D1  alpha_v  u  make sure u^{\T}u = I! u_step_normalized cluster_num  D1  v  ind_old=None check this stupid function!!! <- apparently works... __init__ self  theta  u  v  index  loss  to implement: index choice; initial index + initial u  matrix competition  index competition?) alternating cluster_num  k  D0  D1  alpha_v  epochs=10  u_init=None  index_init=None matrix_competition repeat_num  *args  **kwargs simu  ax.set_xticklabels ax.get_xticklabels )  rotation=-90  fontsize=8) ax.set_yticklabels ax.get_yticklabels )  rotation=0  fontsize=8)  ax.set_xticklabels ax.get_xticklabels )  rotation=-90  fontsize=8)  ax.set_yticklabels ax.get_yticklabels )  rotation=0  fontsize=8) simu ),130
ONSM,"An integration of social media characteristics into an econometric framework requires modeling a high dimensional dynamic network with dimensions of parameter $\Theta$ typically much larger than the number of observations. To cope with this problem we impose two structural assumptions onto the singular value decomposition of $\Theta = U D V^{\top}$. Firstly, the matrix with probabilities of connections between the nodes of a network has a rank much lower than the number of nodes. Therefore, there is limited amount of non-zero elements on the diagonal of $D$ and the whole operator admits a lower dimensional factorisation. Secondly, in observed social networks only a small portion of users are highly-affecting, leading to a sparsity regularization imposed on singular vectors $V.$
Using a novel dataset of 1069K messages from 30K users posted on the microblogging platform StockTwits during a 4-year period (01.2014-12.2018) and quantifying their opinions via natural language processing, we model their dynamic opinions network and further separate the network into communities. With a sparsity regularization, we are able to identify important nodes in the network.","network, autoregression, high-dimensional inference, sentiment weight, stocktwits",Yegor Klochkov,ONSM,5,numpy get_index_matrix cluster_num  ind proj_from_ind k  ind find_new_home func  k  S  ind  i assert  np.shape S) ==  n  n)) greedy_update func  k  S  ind assert  np.shape S) ==  n  n)) __init__ self  index  proj  message kmeans_greedy func  k  S  iter_limit=10000  init_index=None assert  np.shape S) ==  n  n)),47
ONSM,"An integration of social media characteristics into an econometric framework requires modeling a high dimensional dynamic network with dimensions of parameter $\Theta$ typically much larger than the number of observations. To cope with this problem we impose two structural assumptions onto the singular value decomposition of $\Theta = U D V^{\top}$. Firstly, the matrix with probabilities of connections between the nodes of a network has a rank much lower than the number of nodes. Therefore, there is limited amount of non-zero elements on the diagonal of $D$ and the whole operator admits a lower dimensional factorisation. Secondly, in observed social networks only a small portion of users are highly-affecting, leading to a sparsity regularization imposed on singular vectors $V.$
Using a novel dataset of 1069K messages from 30K users posted on the microblogging platform StockTwits during a 4-year period (01.2014-12.2018) and quantifying their opinions via natural language processing, we model their dynamic opinions network and further separate the network into communities. With a sparsity regularization, we are able to identify important nodes in the network.","network, autoregression, high-dimensional inference, sentiment weight, stocktwits",Yegor Klochkov,ONSM,5,numpy pandas read_csv csv math sqrt find_homo_Be x  scale=4  res=20 check_interval l  r read_SIFI name_to_sifi read_stock_twits_user_sentiment name_to_twits  min_delta=0.1  min_days=200 avoid_empty = lambda x  err_t: x[0] if np.size x) > 0 else err_t read_crix_returns name_to_crix_price read_crypto_sentiment name_to_crypto_sentiment  top_num=50,37
Polar_coordinate_normal,Create a function that can generate normal distribution from polar coordinate. The basic idea is from muller-box transformation.,"polar coordinate, exponential distribution, muller-box transform, normal distribution, generate",Muhaimin 20201113,Polar_coordinate_normal,1, -*- coding: utf-8 -*- numpy matplotlib scipy stats generate_normal n brenchmark line plot to compare with brenchmark density plot compare with brenchmark KStest for x KStest for y,28
CRIXbtcBB,Compute Bollinger bands indicator for btc.,"Bollinger bands, Bitcoin, cryptocurrency",SPILAK Bruno,CRIXbtcBB,1,pandas matplotlib Bollinger bands graph bollinger close  window = 20  width = 2  Get index values for the X axis for facebook DataFrame  Plot Adjust Closing Price and Moving Averages,30
P2P_Simulation,Simulation of Bitcoin prices and LTV ratios using a stochastic volatility with correlated jumps model and computation of the expected value of an exemplary crypto-collateralized P2P lending contract.,"bitcoin, crypto collateral, p2p lending, ltv ratio, SVCJ",Ramona Merkl,P2P_Simulation,2,!/usr/bin/env python  coding: utf-8  In[1]: math numpy pandas scipy numpy scipy matplotlib numpy seaborn math sqrt pylab plot  ## Simulation of BTC and LTV Process with SVCJ Model  In[2]:  SVCJ parameters  from Hou et al. v0      = 0.19**2  In[3]:  time horizon in days  dt  number of paths  In[4]:  In[5]:  In[6]:  liquidization threshold  danger zone bitcoin value on 12/06/2020  fix collateral size for 1 BTC for calculation of LTV ratios  set starting values  In[7]:  computation of the processes  In[8]: plt.savefig 'fig1.png'  transparent=True)  In[9]: plt.savefig 'fig2.png'  transparent=True)  In[10]: plt.savefig 'fig3.png'  transparent=True)  In[11]:  In[12]:  example to show inverse movement plt.savefig 'fig4.png'  transparent=True)  In[13]: plt.savefig 'fig5.png'  transparent=True)  In[14]:  vola process of this path plt.savefig 'fig6.png'  transparent=True)  In[15]:  LTV and price  when collateral is sold  In[16]:  simulated BTC price range  ## Determine Value of Lending Contract  In[17]:  function for calculating the payoff of contract k at time t payoff S  LTV  B0  K  z  t  T  r  k=0  total repayment  collateral size  amount of bitcoin needed  find time points where LTV hits threshold  In[18]: ## contract parameters ###  borrowed amount in USD  pre-agreed interest rate  annual discounting factor  time point of view  in days)  In[19]:  calculate payoff for all contracts  In[20]:  empirical mean of all contracts  empirical variance  In[21]:  In[22]:  which path gives maximum payoff  liquidization day of this particular path  In[23]:  In[24]:  therefore  the expected value of the contract is  discounted value of the pre-agreed payback  contract value  present value of repayment  In[25]:  how many lenders lose money or gain more than the repayment amount?  In[26]:  how many borrowers lose their collateral?  In[ ]:,260
SC_KMeansVsSC,This code is modified and simpler version of original code available on scikit learn python library. The codes creates a dataset of noisy circles and performs clustering using K-means and SC,"Graph theory, spectral clustering, clustering, plot, animation, laplacian, graph, eigenvalues, K means, noisy circles, scikit learn",Kainat Khowaja,SC_KMeansVsSC,2,"!/usr/bin/env python  coding: utf-8  In[1]:  This code is simpler version of the built-in examples of python library scikit learning provided on  https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html#sphx-glr-auto-examples-cluster-plot-cluster-comparison-py  In[15]:  In[16]: time warnings numpy matplotlib sklearn cluster sklearn.neighbors kneighbors_graph sklearn.preprocessing StandardScaler itertools cycle  In[17]:  In[18]:  ============  Set up cluster parameters  ============  update parameters with dataset-specific values  normalize dataset for easier parameter selection  estimate bandwidth for mean shift  connectivity matrix for structured Ward  make connectivity symmetric  ============  Create cluster objects  ============  catch warnings related to kneighbors_graph 377eb8'  '#ff7f00'  '#4daf4a'  f781bf'  '#a65628'  '#984ea3'  999999'  '#e41a1c'  '#dede00'])   add black color for outliers  if any) 000000""])",95
pyTSA_TemperatureSH,"This Quantlet plots time series of monthly temperature volatility for the southern hemisphere, its first order difference and its ACF for the period from January 1850 to December 2007","simulation, random walk, Gaussian, normal","Huang Changquan, Alla Petukhina",pyTSA_TemperatureSH,1,pandas matplotlib PythonTsa.plot_acf_pacf acf_pacf_fig statsmodels.tsa.stattools kpss read_table is deprecated  use read_csv instead.  the first differencing savefig 'TSP_TemperatureSH_fig3-3.png'),17
pyTSA_SimNormRW,This Quantlet generates three simulated paths of a standard Gaussian random walk,"simulation, random walk, Gaussian, normal","Huang Changquan, Alla Petukhina",pyTSA_SimNormRW,1,numpy pandas numpy.random normal matplotlib,5
CP2P_smoothfit,"Solution to the attempt of finding an optimal exercise strategy for a CP2P contract based on Aave´s lending pool mechanism. Plotting the function shows, that there is no root and therefore no optimal exercise strategy, if continuous price paths are considered.","CP2P, P2P Lending, American Options, Smooth-Fit, Variational Inequalities, HJB",Ramona Merkl,CP2P_smoothfit,2,"!/usr/bin/env python  coding: utf-8  In[1]: numpy matplotlib  In[2]:  coins Ni L  strike price Ki L  payoff pi L  x  price threshold aH L  In[3]:  smoot-fit function g L  b  In[4]:  contract interest  risk-free rate  dividend; default: coins do not pay dividends r_tilde = r - gamma  volatility  paramteres for smooth-fit function lambda1 =   - r_tilde + sigma**2/2) + np.abs r_tilde + sigma**2/2))/sigma**2 lambda2 =   - r_tilde + sigma**2/2) - np.abs r_tilde + sigma**2/2))/sigma**2  threshold LTV  liquidation bonus  initial coins  initial debt  liquidation threshold  initial LTV  chosen by borrower)  starting coin price S 0)  initial strike price  In[5]: b = np.linspace a+1  11000  20000)  number of liquidations  In[6]:  In[7]: plt.ylim 0 100) plt.xlim a 4000)  In[8]: plt.ylabel r""$f' b)-N_{i-1}$"") plt.savefig 'g_fct_0.png'  transparent=True)  In[9]: C1_C2 L  a  b f L  a  b f_prime L  a  b  In[10]: L = 1 plt.axhline y=0  color='r'  linestyle='-')  In[11]:  In[12]:  root of f_prime  In[13]: plt.ylabel r""$f' b)-N_{i-1}$"") plt.savefig 'g_fct_drift.png'  transparent=True)  In[14]: plt.ylim -0.1 0.2) plt.xlim a 4000)",161
DEDA_Class_2017_WordCloudandSentimentAnalysiswithYelpReviews,"Crawl user-published reviews on Yelp and apply text mining and machine learing techniques using Python. Web Scraping and Word Cloud, Sentiment Analysis using Textblob, Sentiment Classification using SVM",ERROR,Xun Gong,DEDA_WordCloudAndSentimentAnalysiswithYelpReviews,4, further information about wordcloud go to https://github.com/amueller/word_cloud wordcloud WordCloud pandas matplotlib  lower max_font_size,13
DEDA_Class_2017_WordCloudandSentimentAnalysiswithYelpReviews,"Crawl user-published reviews on Yelp and apply text mining and machine learing techniques using Python. Web Scraping and Word Cloud, Sentiment Analysis using Textblob, Sentiment Classification using SVM",ERROR,Xun Gong,DEDA_WordCloudAndSentimentAnalysiswithYelpReviews,4,"requests bs4 BeautifulSoup json pandas os  from utils import ThreadWorker  returns a list of yelp restaurant names based on search page yelp_get_biz_names find_loc = 'Berlin'  cflt = 'Chinese'  page = 0 #Default test args  Argument page equals 1 by default  Visit the home page if page equals 0  Change url by different argument  Define the function to read the first page of reviews of each biz yelp_get_biz_reviews biz  get single biz url  Using BeautifulSoup to parse webpage source code  Finding all the <script type=""application/ld+json""> tag content  Deserialized the content to python dictionary  initial empty list to load the data  get all the reviews in the content dict  extract useful attributes in each review and add to review_list  get the biz names of the first two pages",126
DEDA_Class_2017_WordCloudandSentimentAnalysiswithYelpReviews,"Crawl user-published reviews on Yelp and apply text mining and machine learing techniques using Python. Web Scraping and Word Cloud, Sentiment Analysis using Textblob, Sentiment Classification using SVM",ERROR,Xun Gong,DEDA_WordCloudAndSentimentAnalysiswithYelpReviews,4,os numpy pandas matplotlib time sklearn.model_selection train_test_split sklearn.feature_extraction.text TfidfVectorizer sklearn svm sklearn.metrics classification_report sklearn.metrics confusion_matrix  get only reviews text and rating stars  print reviews_df.head ))  print reviews_df['Review'][0])  print reviews_df['Review Rating'][0])  print reviews_df['Review Rating'].value_counts ))  encode the rating to dummies  1: 5 stars  0: 1 and 2 stars  split train/test subset  define label list  Vectorization TF-IDF)  Perform classification with SVM  kernel=rbf  Perform classification with SVM  kernel=linear  print the prediction results,69
DEDA_Class_2017_WordCloudandSentimentAnalysiswithYelpReviews,"Crawl user-published reviews on Yelp and apply text mining and machine learing techniques using Python. Web Scraping and Word Cloud, Sentiment Analysis using Textblob, Sentiment Classification using SVM",ERROR,Xun Gong,DEDA_WordCloudAndSentimentAnalysiswithYelpReviews,4,pandas matplotlib textblob TextBlob itertools matplotlib rcParams  print reviews_df.shape)  print reviews_df.head ))  get only reviews text and rating stars  print reviews_df['Review'][0])  print reviews_df['Review Rating'][0])  details = TextBlob reviews_df['Review'][0])  print details.sentiment),30
SDA_2020_Cryptocurrencies_Clustering,Clustering cryptocurrencies using K-Means after having reduced the information with a PCA and Factor Analysis and using rolling windows of 14 days,"K-Means Clustering, Cryptocurrencies, Principal Components Analysis, Factor Analysis, Rolling Window","Michael Bader, Pierre-Matthias Drianne, Romaine Kohni",Cryptocurrencies_Clustering,3,!/usr/bin/env python  coding: utf-8  In[1]: pycoingecko CoinGeckoAPI pandas numpy seaborn statsmodels.tsa.stattools adfuller statsmodels.graphics.tsaplots plot_acf sklearn.decomposition FactorAnalysis sklearn.preprocessing MinMaxScaler sklearn.decomposition PCA factor_analyzer FactorAnalyzer sklearn.cluster KMeans matplotlib sklearn.linear_model LinearRegression factor_analyzer.factor_analyzer calculate_bartlett_sphericity factor_analyzer.factor_analyzer calculate_kmo  In[2]:  Calling Coin Gecko API # Import the data cg = CoinGeckoAPI ) coins_market = cg.get_coins_markets 'usd') df_coins_market = pd.DataFrame coins_market)  In[3]: # Get the prices for each crypto for 365 days  01.01.19 - 31.12.19) prices = [] for i in df_coins_market['id']:     a = cg.get_coin_market_chart_range_by_id i  'usd'  1546300800  1577750400)     b = a['prices']     c = []    for j in range len b)):         c.append b[j][1])     prices.append [i  c]) del a  b  c  i  j # Creating the prices  returns and market caps dataframes frame = pd.DataFrame prices) coins = frame[1].apply pd.Series) frame = coins.set_index frame[0]) frame = frame.dropna axis=0).transpose ) frame.to_csv 'CC_Prices.csv' index=True header=True)  Returns returns_data = np.log frame) - np.log frame.shift 1)) returns_data = returns_data[1:] returns_data.to_csv 'CC_LogReturns.csv'  index=True  header=True)  To ensure working with same figures  we will not generate above df each time.   Then  we saved them into csv to import them just below.   Importing different df  Prices  Returns  Market caps  In[4]:  Exploratory Analysis  1) Plotting the prices over the period of various combinations of coins  2) CCs returns and market caps  3) Stationarity test  4) Autocorrelation test  5) Statistical summary of cc  6) Scatterplot of correlated cc  7) Explorative Clustering with K-Means  In[5]:  Exploratory Analysis  1) Plotting the prices over the period of various combinations of coins  In[6]:  Exploratory Analysis  2) CCs logreturns and prices/market caps  2.A) CCs logreturns  mean of 2019 log returns for each cc  cumulative log returns in 2019 for each cc  Plotting mean and cumulative returns of 2019 for each cc  2.B) Market Caps consideration with mean returns and prices  2.B.i) Scatter plots of MarketCap-MeanReturns and MarketCaps-Prices  with bitcoin)  2.B.ii) Scatter plots of MarketCap-MeanReturns and MarketCaps-Prices  without bitcoin)  In[7]:  Exploratory Analysis  3) Stationarity Test  initialization  columns names of returns db  ≠ cryptos)  number of cc  testing stationarity  from https://www.hackdeploy.com/augmented-dickey-fuller-test-in-python/  implementing a class of functions to test stationarity __init__ self  significance=.05 ADF_Stationarity_Test self  timeseries  printResults = True Dickey-Fuller test:  Lags Used' '# Observations Used']) Add Critical Values  creating a matrix which will give if cc are stationary or not  converting it to a panda df  exporting it to a csv  deleting useless variables  In[8]:  printing last df of interest: all cryptos seem to be stationary = good news  In[9]:  Exploratory Analysis  4) Autocorrelation Test  In this section  the autocorrelation of each cc is computed with 20 lags   Autocorrelation plots will be shown for autocorrelated cc plot_acf returns_data['bitcoin']  lags = 20  alpha = 0.05) ac_bitcoin = acf returns_data['bitcoin'] nlags=20) for i in range len ac_bitcoin)):    if  ac_bitcoin[i]<0.035):       print ac_bitcoin[i])  useful for below  creating autocorrelation matrix  adding cryptos names  adding lags names  creating a list to add the autocorrelated cryptos  making a loop to determine which cryptos are autocorrelated and add them to the list  autocorrelation plots for cryptos in the list  In[10]:  Exploratory Analysis  5) Statistical summary of cc  statistical summary of cryptos  plotting the histogram of logreturns mean for all cc  plotting the histogram of logreturns std for all cc  In[11]:  Exploratory Analysis  6) Scatterplot of correlated cc  correlation matrix of all cryptos logreturns  exporting it to csv and excel  useful for reporting)  scatter plot between highly  negatively or positively) correlated cryptos  just a for loop to determine what are the correlated cryptos sns.regplot x=abc y=dcb ci = None data=ret_cc)  In[12]:  In[13]: Preparing the data change here to get other diagram #### Creating the Clusters #plotting the result  In[14]:  Clustering  1) Rolling windows computation for each variable  2) PCA: Principal Components Analysis followed by a k-means clustering algorithm on its components  2.a) Application on a single window  2.b) Application on all rolling windows  3) LFM: Linear Factor Model followed by a k-means clustering algorithm on its factors  3.a) Application on a single window  3.b) Application on all rolling windows  In[15]:  Clustering  1) Rolling windows computation for each variable  determining the size of the rolling windows we are going to compute  Mean  Variance  Skewness  IMPORTANT: need to take rolling window of 3 to calculate it)  Kurtosis  IMPORTANT: need to take rolling window of 4 to calculate it)  Quantile 0.05  Quantile 0.10  Quantile 0.15  Quantile 0.85  Quantile 0.90  Quantile 0.95  In[16]:  Clustering  2) PCA: Principal Components Analysis followed by a k-means clustering algorithm on its components  2.a) Application on a single window  Initialization: Df creation for the first window  final dataframe of the different variables  moments + quantiles) for each cc  In[17]:  Clustering  2) PCA: Principal Components Analysis followed by a k-means clustering algorithm on its components  2.a) Application on a single window  PCA is firstly made  calling PCA function  from these variances  the number of components is determined as the third component still explains more than 10% of the variance  inspired by https://reneshbedre.github.io/blog/pca_3d.html  projecting log returns on two first components  Then  the k-means algorithm may be applied on this time-step  To check the number of clusters needed  defining the function kmeans with the   In[18]:  clustering the first rolling window print label) Plotting the results  In[19]:  In[20]:  Clustering  2) PCA: Principal Components Analysis followed by a k-means clustering algorithm on its components  2.b) Application on all rolling windows  defining the function kmeans with the   optimal number of clusters found in last part  Loop on the windows  creating a labels df for later  time indices  3D matrix for the PCA components at each time-step  days*nb_cc*nb_pc)  matrix for labels= in which cluster is a cc df = frames_norm  computation of the rolling windows through time  all rolling windows are created  the pca is initialized  pca is done  clusters labels are assigned to each cc  instead of analyzing each time-step  let's analyze the distribution of clusters assignation for each cc  printing table of interest  In[21]:  Plotting the results for some rolling windows Plotting the results  In[22]:  In[23]:  Distribution of cc log returns in clusters  In[44]:  Clustering  3) LFM: Linear Factor Model followed by a k-means clustering algorithm on its factors  3.a) Application on a single window  Initialization: Firstly  test if it makes sense to make a LFM  It checks whether variables are correlated or not. If significant  ok. If not  a factor  analysis should not be done.   It seems to be statistically significant so we can continue.   It computes a score of the suitability of the data to a Factor Analysis  0.715 seems ok  Creating factor analysis object and perform factor analysis  Checking eigen values: it will show us the number of significant factors  Computing Eigenvalues  Creating a scree plot to display the different eigen values  In[50]:  In[51]:  In[53]:  Linear Factor model estimation  Application on the first rolling window  Then  the k-means algorithm may be applied on this time-step  To check the number of clusters needed. The criteria is the sse  sum of squared errors).   Searching for the smallest one  a tradeoff needs to be found between a small sse but a small number of kmeans.  In[56]:  Now we know the number of clusters we want  let's try it for the first rolling window.   initialize kmeans class object  defining the function kmeans predict the labels of clusters. Getting unique labels filter rows of original data: it separates the data into the different clusters Plotting the results  In[57]:  Clustering  3) LFM: Linear Factor Model followed by a k-means clustering algorithm on its factors  3.b) Application on all rolling windows  Timed linear factor model  3D matrix for the projection of cc returns on two factors  output matrix with the clusters labels for each cc  In[58]:  Plotting the results for some rolling windows Plotting the results  In[ ]:  Distribution of cc log returns in clusters  In[ ]:,1263
SDA_2020_Cryptocurrencies_Clustering,Clustering cryptocurrencies using K-Means after having reduced the information with a PCA and Factor Analysis and using rolling windows of 14 days,"K-Means Clustering, Cryptocurrencies, Principal Components Analysis, Factor Analysis, Rolling Window","Michael Bader, Pierre-Matthias Drianne, Romaine Kohni",Cryptocurrencies_Clustering,3,!/usr/bin/env python  coding: utf-8  In[1]: pycoingecko CoinGeckoAPI pandas numpy seaborn statsmodels.tsa.stattools adfuller statsmodels.graphics.tsaplots plot_acf sklearn.decomposition FactorAnalysis sklearn.preprocessing MinMaxScaler sklearn.decomposition PCA factor_analyzer FactorAnalyzer sklearn.cluster KMeans matplotlib sklearn.linear_model LinearRegression factor_analyzer.factor_analyzer calculate_bartlett_sphericity factor_analyzer.factor_analyzer calculate_kmo  In[2]:  Calling Coin Gecko API # Import the data cg = CoinGeckoAPI ) coins_market = cg.get_coins_markets 'usd') df_coins_market = pd.DataFrame coins_market)  In[3]: # Get the prices for each crypto for 365 days  01.01.19 - 31.12.19) prices = [] for i in df_coins_market['id']:     a = cg.get_coin_market_chart_range_by_id i  'usd'  1546300800  1577750400)     b = a['prices']     c = []    for j in range len b)):         c.append b[j][1])     prices.append [i  c]) del a  b  c  i  j # Creating the prices  returns and market caps dataframes frame = pd.DataFrame prices) coins = frame[1].apply pd.Series) frame = coins.set_index frame[0]) frame = frame.dropna axis=0).transpose ) frame.to_csv 'CC_Prices.csv' index=True header=True)  Returns returns_data = np.log frame) - np.log frame.shift 1)) returns_data = returns_data[1:] returns_data.to_csv 'CC_LogReturns.csv'  index=True  header=True)  To ensure working with same figures  we will not generate above df each time.   Then  we saved them into csv to import them just below.   Importing different df  Prices  Returns  Market caps  In[4]:  Exploratory Analysis  1) Plotting the prices over the period of various combinations of coins  2) CCs returns and market caps  3) Stationarity test  4) Autocorrelation test  5) Statistical summary of cc  6) Scatterplot of correlated cc  7) Explorative Clustering with K-Means  In[5]:  Exploratory Analysis  1) Plotting the prices over the period of various combinations of coins  In[6]:  Exploratory Analysis  2) CCs logreturns and prices/market caps  2.A) CCs logreturns  mean of 2019 log returns for each cc  cumulative log returns in 2019 for each cc  Plotting mean and cumulative returns of 2019 for each cc  2.B) Market Caps consideration with mean returns and prices  2.B.i) Scatter plots of MarketCap-MeanReturns and MarketCaps-Prices  with bitcoin)  2.B.ii) Scatter plots of MarketCap-MeanReturns and MarketCaps-Prices  without bitcoin)  In[7]:  Exploratory Analysis  3) Stationarity Test  initialization  columns names of returns db  ≠ cryptos)  number of cc  testing stationarity  from https://www.hackdeploy.com/augmented-dickey-fuller-test-in-python/  implementing a class of functions to test stationarity __init__ self  significance=.05 ADF_Stationarity_Test self  timeseries  printResults = True Dickey-Fuller test:  Lags Used' '# Observations Used']) Add Critical Values  creating a matrix which will give if cc are stationary or not  converting it to a panda df  exporting it to a csv  deleting useless variables  In[8]:  printing last df of interest: all cryptos seem to be stationary = good news  In[9]:  Exploratory Analysis  4) Autocorrelation Test  In this section  the autocorrelation of each cc is computed with 20 lags   Autocorrelation plots will be shown for autocorrelated cc plot_acf returns_data['bitcoin']  lags = 20  alpha = 0.05) ac_bitcoin = acf returns_data['bitcoin'] nlags=20) for i in range len ac_bitcoin)):    if  ac_bitcoin[i]<0.035):       print ac_bitcoin[i])  useful for below  creating autocorrelation matrix  adding cryptos names  adding lags names  creating a list to add the autocorrelated cryptos  making a loop to determine which cryptos are autocorrelated and add them to the list  autocorrelation plots for cryptos in the list  In[10]:  Exploratory Analysis  5) Statistical summary of cc  statistical summary of cryptos  plotting the histogram of logreturns mean for all cc  plotting the histogram of logreturns std for all cc  In[11]:  Exploratory Analysis  6) Scatterplot of correlated cc  correlation matrix of all cryptos logreturns  exporting it to csv and excel  useful for reporting)  scatter plot between highly  negatively or positively) correlated cryptos  just a for loop to determine what are the correlated cryptos sns.regplot x=abc y=dcb ci = None data=ret_cc)  In[12]:  In[13]: Preparing the data change here to get other diagram #### Creating the Clusters #plotting the result  In[14]:  Clustering  1) Rolling windows computation for each variable  2) PCA: Principal Components Analysis followed by a k-means clustering algorithm on its components  2.a) Application on a single window  2.b) Application on all rolling windows  3) LFM: Linear Factor Model followed by a k-means clustering algorithm on its factors  3.a) Application on a single window  3.b) Application on all rolling windows  In[15]:  Clustering  1) Rolling windows computation for each variable  determining the size of the rolling windows we are going to compute  Mean  Variance  Skewness  IMPORTANT: need to take rolling window of 3 to calculate it)  Kurtosis  IMPORTANT: need to take rolling window of 4 to calculate it)  Quantile 0.05  Quantile 0.10  Quantile 0.15  Quantile 0.85  Quantile 0.90  Quantile 0.95  In[16]:  Clustering  2) PCA: Principal Components Analysis followed by a k-means clustering algorithm on its components  2.a) Application on a single window  Initialization: Df creation for the first window  final dataframe of the different variables  moments + quantiles) for each cc  In[17]:  Clustering  2) PCA: Principal Components Analysis followed by a k-means clustering algorithm on its components  2.a) Application on a single window  PCA is firstly made  calling PCA function  from these variances  the number of components is determined as the third component still explains more than 10% of the variance  inspired by https://reneshbedre.github.io/blog/pca_3d.html  projecting log returns on two first components  Then  the k-means algorithm may be applied on this time-step  To check the number of clusters needed  defining the function kmeans with the   In[18]:  clustering the first rolling window print label) Plotting the results  In[19]:  In[20]:  Clustering  2) PCA: Principal Components Analysis followed by a k-means clustering algorithm on its components  2.b) Application on all rolling windows  defining the function kmeans with the   optimal number of clusters found in last part  Loop on the windows  creating a labels df for later  time indices  3D matrix for the PCA components at each time-step  days*nb_cc*nb_pc)  matrix for labels= in which cluster is a cc df = frames_norm  computation of the rolling windows through time  all rolling windows are created  the pca is initialized  pca is done  clusters labels are assigned to each cc  instead of analyzing each time-step  let's analyze the distribution of clusters assignation for each cc  printing table of interest  In[21]:  Plotting the results for some rolling windows Plotting the results  In[22]:  In[23]:  Distribution of cc log returns in clusters  In[44]:  Clustering  3) LFM: Linear Factor Model followed by a k-means clustering algorithm on its factors  3.a) Application on a single window  Initialization: Firstly  test if it makes sense to make a LFM  It checks whether variables are correlated or not. If significant  ok. If not  a factor  analysis should not be done.   It seems to be statistically significant so we can continue.   It computes a score of the suitability of the data to a Factor Analysis  0.715 seems ok  Creating factor analysis object and perform factor analysis  Checking eigen values: it will show us the number of significant factors  Computing Eigenvalues  Creating a scree plot to display the different eigen values  In[50]:  In[51]:  In[53]:  Linear Factor model estimation  Application on the first rolling window  Then  the k-means algorithm may be applied on this time-step  To check the number of clusters needed. The criteria is the sse  sum of squared errors).   Searching for the smallest one  a tradeoff needs to be found between a small sse but a small number of kmeans.  In[56]:  Now we know the number of clusters we want  let's try it for the first rolling window.   initialize kmeans class object  defining the function kmeans predict the labels of clusters. Getting unique labels filter rows of original data: it separates the data into the different clusters Plotting the results  In[57]:  Clustering  3) LFM: Linear Factor Model followed by a k-means clustering algorithm on its factors  3.b) Application on all rolling windows  Timed linear factor model  3D matrix for the projection of cc returns on two factors  output matrix with the clusters labels for each cc  In[58]:  Plotting the results for some rolling windows Plotting the results  In[ ]:  Distribution of cc log returns in clusters  In[ ]:,1263
SVM,Use machines learning algorithms to predict daily price move in Bitcoin and perform day trading,"Bitcoin, SVM, Random Forest, Neural Network, LSTM","Hong Yuxi, Lin Jinyu, Sun Rongsheng, Zhou Xiaoqian, Zou Yutong",SVM,4,"!/usr/bin/env python  coding: utf-8  In[1]: pandas matplotlib numpy seaborn sns.set style=""whitegrid""  color_codes=True) warnings sklearn.preprocessing MinMaxScaler sklearn.metrics accuracy_score sklearn.model_selection KFold sklearn.metrics roc_curve sklearn.metrics matthews_corrcoef sklearn metrics sklearn.model_selection train_test_split sklearn.model_selection TimeSeriesSplit sklearn.model_selection GridSearchCV sklearn.model_selection TimeSeriesSplit sklearn svm sklearn.svm SVC  In[2]: __init__ self  n_splits get_n_splits self  X  y  groups split self  X  y=None  groups=None  In[3]: train_model data get data Split the data to train and test Below uses method to find the best parameters For Kernel = ""linear""  Performance of linear model on test data For Kernel = ""poly"" and ""sigmod"" sklearn.svm SVC For Kernel = ""rbf""   print accuracy rate print 'Accuracy Score:' accuracy_score y_validation  y_pred  normalize=True)*100.0)  In[ ]:",106
SVM,Use machines learning algorithms to predict daily price move in Bitcoin and perform day trading,"Bitcoin, SVM, Random Forest, Neural Network, LSTM","Hong Yuxi, Lin Jinyu, Sun Rongsheng, Zhou Xiaoqian, Zou Yutong",SVM,4,"!/usr/bin/env python  coding: utf-8  # SVM BTC  In[13]: pandas matplotlib numpy seaborn sns.set style=""whitegrid""  color_codes=True) warnings sklearn.preprocessing MinMaxScaler sklearn.metrics accuracy_score sklearn.model_selection KFold sklearn.metrics roc_curve sklearn.metrics matthews_corrcoef sklearn metrics sklearn.model_selection train_test_split sklearn.model_selection TimeSeriesSplit sklearn.model_selection GridSearchCV sklearn.model_selection TimeSeriesSplit os os.path dirname  In[14]: define total rolling period  roll over every 5 days trading strategy:1 long;0 short calculate trading strategy performance  In[15]: plot figure  In[16]:  calculate trading strategy volatility and Sharpe ratio  annualised) 1-y treasury yield  In[ ]:",74
DEDA_Class_2017_Statistics&Finance,ERROR,ERROR,"Junjie Hu, Yves Hilpisch",DEDA_Class_2017_Statistics&Finance,7,numpy  Giving the seed of random number generator  Generate a 20*20 matrix with uniform distributed random integers between 0 to 50 matplotlib  Create a window to draw  Create axes  Plot the dots  using circle  Set figure title  Save figure to a high quality  clear the figure  Another scatter plot  plt also provide a quick way to draw  Inverse matrix  Matrix times operation  Generate a 20*20 identity matrix  Using .allclose ) function to evaluate two matrices are equal within tolerance  True PIL Image numpy.fft fft  Open then image by using Python Imaging Library PIL)  Decoding and encoding image to float number  Processing Fourier transform  Filter the lower frequency  Inverse Fourier transform  Keep the real part  Output the image,117
DEDA_Class_2017_Statistics&Finance,ERROR,ERROR,"Junjie Hu, Yves Hilpisch",DEDA_Class_2017_Statistics&Finance,7,scipy matplotlib numpy  Generate a normal distributed random variable  First  generate x axis from 0.1% - 99.9% of quantile  The random variable with mean=10  sigma=3  Calculate the pdf  Then  Calculate the CDF  Plot the cdf and pdf in 1 figure  upper plot is pdf   rows  columns  which one)  lower plot is cdf  Save,53
DEDA_Class_2017_Statistics&Finance,ERROR,ERROR,"Junjie Hu, Yves Hilpisch",DEDA_Class_2017_Statistics&Finance,7, Monte Carlo valuation of European call option  in Black-Scholes-Merton model  bsm_mcs_euro.py numpy  Parameter Values  initial index level  strike price  time-to-maturity  riskless short rate  volatility  number of simulations  Valuation Algorithm  pseudorandom numbers  index values at maturity  inner values at maturity  Monte Carlo estimator  Result Output  Valuation of European call options in Black-Scholes-Merton model  incl. Vega function and implied volatility estimation  bsm_functions.py  Analytical Black-Scholes-Merton  BSM) Formula bsm_call_value S0  K  T  r  sigma math log scipy stats  stats.norm.cdf --> cumulative distribution function                     for normal distribution  Vega function bsm_vega S0  K  T  r  sigma math log scipy stats  Implied volatility function bsm_call_imp_vol S0  K  T  r  C0  sigma_est  it=100,105
DEDA_Class_2017_Statistics&Finance,ERROR,ERROR,"Junjie Hu, Yves Hilpisch",DEDA_Class_2017_Statistics&Finance,7,scipy matplotlib matplotlib plot_normal mu  sigma  size  Create a function to receive normal distribution parameters  The hist ) method not only draw the histogram  but also return the info of the drawing  Use bins returned from hist ) to draw a wrap line  Draw 3 plots with different parameters in the same figure  Set the legend of the figure  Like LATEX  Use $ and \to mark mathematical symbols,68
DEDA_Class_2017_Statistics&Finance,ERROR,ERROR,"Junjie Hu, Yves Hilpisch",DEDA_Class_2017_Statistics&Finance,7,numpy  A basic Wiener Process  Standard normal distribution  Generalized form of Wiener Process,13
DEDA_Class_2017_Statistics&Finance,ERROR,ERROR,"Junjie Hu, Yves Hilpisch",DEDA_Class_2017_Statistics&Finance,7, Monte Carlo valuation of European call option numpy  Parameter Values  initial index level  strike price  time-to-maturity  risk-free short-term rate  volatility  number of simulations  Valuation Algorithm  A random vector with normal distribution  Geometric Brownian Motion simulation of price at time T  index values at maturity  Option values at maturity  Discount average option values to time 0  Result Output  Valuation of European call options in Black-Scholes-Merton model  incl. Vega function and implied volatility estimation  bsm_functions.py  Black-Scholes-Merton model  Analytical Black-Scholes-Merton  BSM) Formula bsm_call_value S0  K  T  r  sigma math log scipy stats  stats.norm.cdf --> cumulative distribution function                     for normal distribution  Vega function bsm_vega S0  K  T  r  sigma math log scipy stats  Implied volatility function bsm_call_imp_vol S0  K  T  r  C0  sigma_est  it=100 matplotlib pandas datetime  asset price  interest rate  tolerance level for moneyness  Read VSTOXX futures and options data  Altering timestamp to datetime  new column for implied volatilities  iterating over all options quotes  picking the right futures value  only for options with moneyness within tolerance  VSTOXX value  estimate for implied volatility  select data for this maturity,174
DEDA_Class_2017_Statistics&Finance,ERROR,ERROR,"Junjie Hu, Yves Hilpisch",DEDA_Class_2017_Statistics&Finance,7,numpy matplotlib scipy.stats norm sklearn.neighbors KernelDensity  Code reference: http://scikit-learn.org/stable/auto_examples/neighbors/plot_kde_1d.html  Create 2 normal distributed data set  Create x axis range  Create linear combination of 2 normal distributed random variable  Plot the real distribution  Use 3 different kernel to estimate  Initial an object to use kernel function to fit data  bandwidth will affect the result  Evaluate the density model on the data  Add text on the plot  position argument can be arbitrary  Plot the random points  squeeze them into narrow space  Set x-axis y-axis limit to adjust the figure,87
pyTSA_GDPChina,This Quantlet builds plots time series and ACF of Chinese quarterly GDP from 1992 to 2017,"time series, autocorrelation, Chinese GDP, plot, visualisation","Huang Changquan, Alla Petukhina",pyTSA_GDPChina,1,pandas matplotlib PythonTsa.plot_acf_pacf acf_pacf_fig,4
SFM_Quant_fin,"Analysis of the evolution of prices of shares of AAPL, AMZN, GOOG, FB, NFLX, MSFT, SPY between 2013 and 2019.","evolution, daily returns, cumulative returns, bollinger bands, volatility, random walk, risk","Denisa-Maria Ilie, Claudia-Andreea Toma",SFM_Quant_fin,2,"!/usr/bin/env python  coding: utf-8  Quantitative techniques for financial analysis     Students: Ilie Denisa-Maria & Toma Claudia Andreea    Master of Applied Statistics and Data Science     Bucharest    2018 - 2019    In[12]:  Import libraries requests pandas datetime time matplotlib numpy seaborn math matplotlib style  In[2]:  Function for reading data from Alpha Vantage API get_data ticker_symbol  company  In[3]:  Data extraction call  In[4]:  In[5]:  Slicing function of the dataframe for 2013+ slice_dataframe df  In[6]:  Slicing function call   In[7]:  In[8]:  Create formatted dataframe - data & symbols  In[9]:  In[10]:  Graphical representation of the evolution of stock prices plot_data df  title=""Stock prices""  In[13]:  In[14]:  Check missing values  In[15]:  Fill missing values through Forward and Backward fill  In[16]:  In[17]:  Testing the random-walk hypothesis statsmodels.tsa.stattools adfuller  In[18]:  Compute daily returns compute_daily_return df  In[19]:  In[20]:  In[62]: ax.legend loc='lower left')  In[63]: ax.legend loc='lower left')  In[52]: ax.legend loc='lower left')  In[64]: ax.legend loc='lower left')  In[65]: ax.legend loc='lower left')  In[66]: ax.legend loc='lower left')  In[22]:  In[23]:  Histogram of daily returns of shares  Calcul Kurtosis  daca este mai mare decat zero -> fat tails  In[24]:  Calcul Kurtosis  daca este mai mare decat zero -> fat tails  In[25]:  Calcul Kurtosis  daca este mai mare decat zero -> fat tails  In[67]:  Calcul Kurtosis  daca este mai mare decat zero -> fat tails  In[68]:  Calcul Kurtosis  daca este mai mare decat zero -> fat tails  In[69]:  Calcul Kurtosis  daca este mai mare decat zero -> fat tails  In[26]:  Combined histograms of daily returns on shares  In[70]:  In[71]:  In[72]:  In[73]:  In[74]:  In[75]:  In[27]:  In[28]:  Check the existence of correlations between stock prices through the scatter plot  degree 1: y = b*x + a  In[29]:  Check the existence of correlations between stock prices through the correlation matrix  In[30]:  In[31]:  degree 1: y = b*x + a  In[76]:  degree 1: y = b*x + a  In[77]:  degree 1: y = b*x + a  In[78]:  degree 1: y = b*x + a  In[79]:  degree 1: y = b*x + a  In[80]:  degree 1: y = b*x + a  In[32]:  degree 1: y = b*x + a  In[33]:  Compute cumulative daily returns compute_cumulative_return df  In[34]:  In[35]:  In[36]:  In[37]:  Calculul statisticilor descriptive pentru pretul actiunilor  In[38]:  Compute rolling averages  rolling standard deviations and Bollinger bands get_rolling_mean df  window get_rolling_std df  window get_bollinger_bands rm  rstd  In[39]:  In[40]: bollinger_bands df  symbol   1) rolling averages   2) rolling standard deviations   3) Bollinger band - 2 standard deviations from the mean  both positive and negative)  Store new columns  Graphical Representation  In[53]:  In[54]:  In[55]:  In[56]:  In[57]:  In[58]:  In[59]:  In[42]:  In[43]:  In[44]:  In[45]: export_csv ticker_symbol  df  In[46]:  In[97]:  Annualized daily returns  Annual daily returns for 252 trading days  100 -> %)  In[85]: quandl  Importing an example of non-risky return  interest on investing in a treasury bail)  In[83]:  In[102]:  Most recent value  In[98]:  In[99]:  In[100]:  In[103]:  In[104]:  In[109]:  In[110]:  In[111]:  Compute Sharp Ratio  In[108]:  Bibliography     Python Analysis of Stocks https://ntguardian.wordpress.com/2018/07/17/stock-data-analysis-python-v2/    ALPHA VANTAGE Preprocessed Free APIs in JSON and CSV formats https://www.profitaddaweb.com/2018/07/alpha-vantage-preprocessed-free-apis-in.html    Machine Learning for Trading https://eu.udacity.com/course/machine-learning-for-trading--ud501 ",482
High_Frequency_TFT,Using TFT to do high frequency prediction of stock index futures,"High frequency, CSI 300, TFT, Xgboost, MLP","Cheng Tuoyuan, Wang Duyue, Wang Wenbo, Zheng Zhongyi",High_Frequency_TFT,1,pandas sklearn.metrics roc_auc_score  load data copy pathlib Path warnings tensorflow tensorboard numpy pandas pytorch_lightning pytorch_lightning.callbacks EarlyStopping pytorch_lightning.loggers TensorBoardLogger torch pytorch_forecasting Baseline pytorch_forecasting.data GroupNormalizer pytorch_forecasting.metrics SMAPE pytorch_forecasting.models.temporal_fusion_transformer.tuning optimize_hyperparameters  keep encoder length long  as it is in the validation set)  create validation set  predict=True) which means to predict the last max_prediction_length points in time  for each series  create dataloaders for model  set this between 32 to 128  configure network and trainer  clipping gradients is a hyperparameter and important to prevent divergance  of the gradient for recurrent neural networks  not meaningful for finding the learning rate but otherwise very important  most important hyperparameter apart from learning rate  number of attention heads. Set to up to 4 for large datasets  between 0.1 and 0.3 are good values  set to <= hidden_size  7 quantiles by default  reduce learning rate if no improvement in validation loss after x epochs  configure network and trainer  log the learning rate  logging results to a tensorboard  coment in for training  running valiation every 30 batches  fast_dev_run=True   # comment in to check that networkor dataset has no serious bugs  7 quantiles by default  uncomment for learning rate finder and otherwise  e.g. to 10 for logging every 10 batches  load the best model according to the validation loss   given that we use early stopping  this is not necessarily the last epoch)  calcualte mean absolute error on validation set  raw predictions are a dictionary from which all kind of information including quantiles can be extracted  plot 10 examples  train test split  fill missing value ## train xgb model xgboost XGBClassifier ## train neural network mlp model tensorflow.keras.models Sequential tensorflow.python.keras.layers Dense tensorflow,268
SDA_Final Project_LSTM,Final Project of SDA I Class: Modelling The Effect of Confirmed Number COVID-19 Ratio to The Exchange Rate of Euro to Rupiah,ERROR,"Samingun Handoyo, Andreas Rony Wijaya",SDA_Final Project_LSTM,5,pandas numpy statsmodels.tsa.api VAR statsmodels.tsa.stattools adfuller statsmodels.tools.eval_measures rmse statsmodels.tsa.stattools grangercausalitytests statsmodels.tsa.vector_ar.vecm coint_johansen matplotlib cointegration_test df  alpha=0.05 adjust val  length= 6): return str val).ljust length  Summary covid_ratio data  read data  Decorations plot for the Exchange rates  Decorations,36
SDA_Final Project_LSTM,Final Project of SDA I Class: Modelling The Effect of Confirmed Number COVID-19 Ratio to The Exchange Rate of Euro to Rupiah,ERROR,"Samingun Handoyo, Andreas Rony Wijaya",SDA_Final Project_LSTM,5,numpy matplotlib pandas math sqrt keras.regularizers l2 keras.models Sequential keras.layers Dense keras.layers LSTM sklearn.preprocessing MinMaxScaler sklearn.metrics mean_squared_error scipy.stats pearsonr define_model0 hyparam train_X  split into train and test sets  reshape input to be 3D [samples  timesteps  features]  plot history  make a prediction  invert scaling for forecast  invert scaling for actual  calculate RMSE model_performance A F,54
SDA_Final Project_LSTM,Final Project of SDA I Class: Modelling The Effect of Confirmed Number COVID-19 Ratio to The Exchange Rate of Euro to Rupiah,ERROR,"Samingun Handoyo, Andreas Rony Wijaya",SDA_Final Project_LSTM,5,"numpy matplotlib pandas math sqrt keras.regularizers l2 keras.models Sequential keras.layers Dense keras.layers LSTM sklearn.preprocessing MinMaxScaler sklearn.metrics mean_squared_error scipy.stats pearsonr define_model0 hyparam train_X colnames=[""MYR-1"" ""MYR-2"" ""MYR-3"" ""USD-1"" ""USD-2"" ""USD-3"" ""EUR-1"" ""EUR-2"" ""EUR-3"" ""EURO""] DF.columns=colnames  split into train and test sets  reshape input to be 3D [samples  timesteps  features]  plot history  make a prediction  invert scaling for forecast  invert scaling for actual  calculate RMSE model_performance A F",65
SDA_Final Project_LSTM,Final Project of SDA I Class: Modelling The Effect of Confirmed Number COVID-19 Ratio to The Exchange Rate of Euro to Rupiah,ERROR,"Samingun Handoyo, Andreas Rony Wijaya",SDA_Final Project_LSTM,5,"numpy matplotlib pandas math sqrt keras.regularizers l2 keras.models Sequential keras.layers Dense keras.layers LSTM sklearn.preprocessing MinMaxScaler sklearn.metrics mean_squared_error scipy.stats pearsonr define_model0 hyparam train_X colnames=[""MYR-1"" ""MYR-2"" ""MYR-3"" ""USD-1"" ""USD-2"" ""USD-3"" ""EUR-1"" ""EUR-2"" ""EUR-3"" ""EURO""] DF.columns=colnames  split into train and test sets  reshape input to be 3D [samples  timesteps  features]  plot history  make a prediction  invert scaling for forecast  invert scaling for actual  calculate RMSE model_performance A F",65
SDA_Final Project_LSTM,Final Project of SDA I Class: Modelling The Effect of Confirmed Number COVID-19 Ratio to The Exchange Rate of Euro to Rupiah,ERROR,"Samingun Handoyo, Andreas Rony Wijaya",SDA_Final Project_LSTM,5,numpy matplotlib pandas math sqrt keras.regularizers l2 keras.models Sequential keras.layers Dense keras.layers LSTM sklearn.preprocessing MinMaxScaler sklearn.metrics mean_squared_error scipy.stats pearsonr define_model0 hyparam train_X  split into train and test sets  reshape input to be 3D [samples  timesteps  features]  plot history  make a prediction  invert scaling for forecast  invert scaling for actual  calculate RMSE model_performance A F,54
pyTSA_USBoysGirls,"This Quantlet produces time series plot, the bar charts and lag plots to vizualize a data of the numbers of boys and girls born in USA per year from 1940 to 2002.","time series, bar chart, vizualization, lag plot",ERROR,pyTSA_USBoysGirls,1,pandas matplotlib pandas.plotting lag_plot,4
FRM_4_Data_Analysis,Analyze the price/return of the 12 coins and the 2 macro-economic indices,"data, analysis, normalization, standardization, log transformation","Qi Wu, Seokhee Moon",FRM_4_Data_Analysis,2,"!/usr/bin/env python  coding: utf-8  In[1]: numpy pandas plotly plotly  ## 1. Price   ### 1. - 1) Raw price  In[2]:  fig = go.Figure )  for coin in price.columns:      fig.add_trace go.Scatter x=price.index  y=price[coin] name = coin))  fig.update_layout title=""Hourly Price"")  plotly.offline.plot fig filename=""Hourly Price.html"")  ### 1. - 2) Log price   In[3]:  fig = go.Figure )  for coin in LogPrice.columns:      fig.add_trace go.Scatter x=LogPrice.index  y=LogPrice[coin] name = coin))  fig.update_layout title=""Log Hourly Price"")  plotly.offline.plot fig filename=""Log Hourly Price.html"")  ### 1. - 3) Standardized price   In[4]:  fig = go.Figure )  for coin in StdPrice.columns:      fig.add_trace go.Scatter x=StdPrice.index  y=StdPrice[coin] name = coin))  fig.update_layout title=""Standardized Hourly Price"")  plotly.offline.plot fig filename=""Standardized Hourly Price.html"")  #### Export the graph as a png file  In[6]: matplotlib pyplot matplotlib matplotlib.dates MonthLocator matplotlib  fig.savefig 'Price data analysis.png')  ## 2. Return  ### 2. - 1) Raw return   In[7]:  fig = go.Figure )  for coin in returns.columns:      fig.add_trace go.Scatter x=returns.index  y=returns[coin] name = coin))  fig.update_layout title=""Hourly Return"")  plotly.offline.plot fig filename=""Hourly Return.html"")  ### 2. - 2) Standardized return   In[8]:  fig = go.Figure )  for coin in returnsStd.columns:      fig.add_trace go.Scatter x=returnsStd.index  y=returnsStd[coin] name = coin))  fig.update_layout title=""Standardized Hourly Return"")  plotly.offline.plot fig filename=""Standardized Hourly Return.html"")  #### Export the graph as a png file  In[9]:  fig.savefig 'Return data analysis.png')  ## 3. Macroeconomic index  ### 3. - 1) Raw macro index  In[10]:  fig = go.Figure )  for i in macro.columns:      fig.add_trace go.Scatter x=macro.index  y=macro[i] name = i))  fig.update_layout title=""Macroeconomic variables"")  plotly.offline.plot fig filename=""Macroeconomic variables.html"")  ### 3. - 2) Standardized macro index  In[14]:  fig = go.Figure )  for i in macroStand.columns:      fig.add_trace go.Scatter x=macroStand.index  y=macroStand[i] name = i))  fig.update_layout title=""Standardized Macroeconomic variables"")  plotly.offline.plot fig filename=""Standardized Macroeconomic variables.html"")  #### Export the graph as a png file  In[12]:  fig.savefig 'Macro-economic data analysis.png')  ## 4. Macroeconomic index Return  ### 3. - 1) Raw macro index Return  In[26]:  fig = go.Figure )  for i in macroRT.columns:      fig.add_trace go.Scatter x=macroRT.index  y=macroRT[i] name = i))  fig.update_layout title=""Macroeconomic variables Return"")  plotly.offline.plot fig filename=""Macroeconomic variables Return.html"")  ### 3. - 2) Standardized macro index Return  In[27]:  fig = go.Figure )  for i in macroRTStand.columns:      fig.add_trace go.Scatter x=macroRTStand.index  y=macroRTStand[i] name = i))  fig.update_layout title=""Standardized Macroeconomic variables Return"")  plotly.offline.plot fig filename=""Standardized Macroeconomic variables Return.html"")  #### Export the graph as a png file  In[33]:",359
SFEpacfar2,Plots the partial autocorrelation function of an AR(2) (autoregressive) process.,"acf, partial, PACF, autocorrelation, autoregressive, discrete, graphical representation, linear, plot, process, simulation, stationary, stochastic, stochastic-process, time-series",Joanna Tomanek,QID-599-SFEpacfar2,1,pandas numpy statsmodels.graphics.tsaplots plot_pacf statsmodels.tsa.arima_process arma_generate_sample matplotlib  parameter settings  lag value  add zero-lag and negate alphas,16
SMSclus8pd,"employs the single linkage method using simple Euclidean distance and squared Euclidean distance matrices to perform a cluster analysis on an 8 points example. Three plots are generated. First plot shows the 8 points, second plot the dendrogram for squared Euclidean distance and single linkage while the third plot presents the dendrogram for Euclidean distance using single linkage algorithm","cluster-analysis, dendrogram, distance, euclidean, euclidean distance matrix, euclidean-norm, linkage, single linkage, squared",ERROR,SMSclus8pd,1,numpy matplotlib scipy.cluster hierarchy,4
SFEacfma1,Plots the autocorrelation function of an MA(1) (moving average) process.,"acf, autocorrelation, discrete, graphical representation, linear, moving-average, plot, process, simulation, stationary, stochastic, stochastic-process, time-series",Joanna Tomanek,QID-3434-SFEacfma1,1,pandas numpy statsmodels.graphics.tsaplots plot_acf statsmodels.tsa.arima_process ArmaProcess matplotlib  parameter settings  lag value sampled values  Obtain MA 1) sample by sampling from a ARMA ) model with no AR coefficient,28
SFM_Class_2018_Scrape_API_SContracts,Scrape the Etherscan API to get source code of smart contracts given the list of their hashes,"hash, algorithm, crix, cryptocurrency, scrape, trading, fintech, Ethereum, API, Solidity","Raphael Constantin Georg Reule, Elizaveta Zinovyeva, Rui Ren, Marvin Gauer",SFM_Class_2018_Scrape_API_SContracts,2,"!/usr/bin/env python  coding: utf-8  # Scrape API of etherscan to load source codes of smart contracts  # ""Powered by Etherscan.io APIs""  Please place API_token for https://etherscan.io/apis into the root folder as API_token.txt   Data set with hash addresses of contracts as Ethereum.csv in the root folder.   Datasets structure: four columns: timestamd  hash  amount of transactions and ethereum connceted to it.  ### Import Modules  In[ ]: requests pandas numpy sys time os  ### Functions  In[ ]: split_date_year row split_date_month row  ### Constants and paths    In[ ]:  set to true if only for testing or code check  ### Check whether folders exist  ##### if not create them  In[ ]:  ### Load List of Contracts  In[ ]:  Load list from existing dataset  Datasets structure: four columns: timestamd  hash  amount of transactions and ethereum connceted to it  Take a sample  ### Function to call the API  In[ ]: scrape_ether_contract_and_write adress_array  API_Token  check whether the address is already in the folder  we can do 5 GET/POST requests per sec  save full GET request print f""contract {data['result'][0]['ContractName']} downloaded"")    save solidity source code  save ABI compiled version  ### Function Call  In[ ]:",184
SC_Dapp_scraping,"This Quantlet is dedicated to gathering data on the open source source codes of Solidity Smart Contracts. First, the Ethereum Dapps names are obtained using the API https://www.stateofthedapps.com/. For each DApp the list of smart contract hashes were obtained if available (Also through the API of state of the Dapps). And followingly using the API of https://etherscan.io/, the source codes (if verified) are obtained for the open source Dapps. Finally, the codes are parsed and preprocessed to extract comments and merged with the category from the State of the Dapps and stored as a csv file.","API, parsing, scraping, regex, Ethereum, Solidity, Etherscan, smart contracts, stateofthedapps","Elizaveta Zinovyeva, Raphael Constantin Georg Reule",SC-Dapp-scraping,8,!/usr/bin/env python  coding: utf-8  In[ ]: pandas numpy  In[ ]:  In[ ]:  In[ ]:  In[ ]:  In[ ]: df.groupby by=[df.created.dt.month  df.created.dt.year]).mean )  In[ ]:  In[ ]:  In[ ]:,28
SC_Dapp_scraping,"This Quantlet is dedicated to gathering data on the open source source codes of Solidity Smart Contracts. First, the Ethereum Dapps names are obtained using the API https://www.stateofthedapps.com/. For each DApp the list of smart contract hashes were obtained if available (Also through the API of state of the Dapps). And followingly using the API of https://etherscan.io/, the source codes (if verified) are obtained for the open source Dapps. Finally, the codes are parsed and preprocessed to extract comments and merged with the category from the State of the Dapps and stored as a csv file.","API, parsing, scraping, regex, Ethereum, Solidity, Etherscan, smart contracts, stateofthedapps","Elizaveta Zinovyeva, Raphael Constantin Georg Reule",SC-Dapp-scraping,8,!/usr/bin/env python  coding: utf-8  In[ ]: pandas requests re numpy ast time datetime date  ### We need to get the list of all hashes first  In[ ]:  In[ ]:  we have the maximum of 20 pages  if the limit set to 100 apps per page  In[ ]:  In[ ]:  In[ ]:  ### For each dapp  here or each slug  retrieve a list of hashes  In[ ]:  In[ ]:  In[ ]:  In[ ]:  In[ ]: hashes.to_csv f'data/hashes_with_licenses_{today}.csv'  index=False),77
SC_Dapp_scraping,"This Quantlet is dedicated to gathering data on the open source source codes of Solidity Smart Contracts. First, the Ethereum Dapps names are obtained using the API https://www.stateofthedapps.com/. For each DApp the list of smart contract hashes were obtained if available (Also through the API of state of the Dapps). And followingly using the API of https://etherscan.io/, the source codes (if verified) are obtained for the open source Dapps. Finally, the codes are parsed and preprocessed to extract comments and merged with the category from the State of the Dapps and stored as a csv file.","API, parsing, scraping, regex, Ethereum, Solidity, Etherscan, smart contracts, stateofthedapps","Elizaveta Zinovyeva, Raphael Constantin Georg Reule",SC-Dapp-scraping,8,"!/usr/bin/env python  coding: utf-8  In[ ]:  Used Etherscan.io APIs""  In[ ]: requests pandas numpy sys time os matplotlib tqdm tqdm  In[ ]: ## Load List of Contracts  In[ ]:  In[ ]:  In[ ]:  In[ ]:  In[ ]:  In[ ]:  contants  In[ ]:  In[ ]:  ### Create Folders  In[ ]:  In[ ]:  In[ ]:  In[ ]: scrape_ether_contract_and_write address_array  API_Token  time.sleep 0.01) # we can do 5 GET/POST requests per sec  save solidity source code  In[ ]:",75
SC_Dapp_scraping,"This Quantlet is dedicated to gathering data on the open source source codes of Solidity Smart Contracts. First, the Ethereum Dapps names are obtained using the API https://www.stateofthedapps.com/. For each DApp the list of smart contract hashes were obtained if available (Also through the API of state of the Dapps). And followingly using the API of https://etherscan.io/, the source codes (if verified) are obtained for the open source Dapps. Finally, the codes are parsed and preprocessed to extract comments and merged with the category from the State of the Dapps and stored as a csv file.","API, parsing, scraping, regex, Ethereum, Solidity, Etherscan, smart contracts, stateofthedapps","Elizaveta Zinovyeva, Raphael Constantin Georg Reule",SC-Dapp-scraping,8,"!/usr/bin/env python  coding: utf-8  In[ ]: re pandas numpy pickle os tqdm tqdm  In[ ]: https://stackoverflow.com/questions/2319019/using-regex-to-remove-comments-from-source-files https://github.com/HektorLin/HU-IRTG/blob/master/Partition%20Codes%20and%20Comments.ipynb remove_comments string  first group captures quoted strings  double or single)  second group captures comments  //single-line or /* multi-line */) _replacer match  if the 2nd group  capturing comments) is not None   it means we have captured a non-quoted  real) comment string.  so we will return empty to remove the comment  otherwise  we will return the 1st group  captured quoted-string following the example above  distinguish group 1): """" or'' and group 2 true comments): \*.*\ or // return when a true comment is located leave_only_comments string  In[ ]:  In[ ]:  In[ ]:  In[ ]:",109
DEDA_class2019_SYSU_Peppa_Pig,"This Quantlet provides the code to draw PeppaPig by using the library named turtle. The focus lies on the configuration of the pen size, the drawing speed and the fill of the shape. The project can be used in stimulate interest of children in python. Reference:https://blog.csdn.net/weixin_33701632/article/details/111928743","turtle,  draw, peppa pig, fill, pen, picture, fun",Ruting Rainy WANG,DEDA_class2019_SYSU_Peppa_Pig,1," coding:utf-8 turtle  set the pen size 设置画笔的大小   set the color of GBK 设置GBK颜色范围为0-255  set the pen color pink) and fill it 设置画笔颜色和填充颜色 pink)  set the size of main window 设置主窗口的大小为840*500  set the speed of drawing 设置画笔速度为10  draw Peppa Pig's nose  use the pen 提笔  go to  -100  100) 画笔前往坐标 -100 100)  begin to draw 下笔  set the angle of the pen  -30°) 笔的角度为-30°  the signal to fill the figure 外形填充的开始标志  preset the step length  turn left 3 degrees向左转3度  go forward step ""a"" 向前走a的步长  fill the color 依据轮廓填充  draw Peppa Pig's nostrils  use the pen提笔  set the angle of the pen 90°) 笔的角度为90度  go forward 25 steps 向前移动25  set the angle of the pen 0°)转换画笔的角度为0  go forward 10 steps 向前移动10  set the pen color pink) 设置画笔颜色  Draw a circle with radius 5 画一个半径为5的圆  set the pen color brown)设置画笔和填充颜色  fill the figure  the second nostril  draw Peppa Pig's head   Draw a circle clockwise with radius of 300 and center anle of 30 顺时针画一个半径为300 圆心角为30°的园   Draw a circle clockwise with radius of 300 and center anle of 60  set the angle of the pen 161°)  repeat 15-25 向左转3度 向前走a的步长  Draw Pig's ears  Draw Pig's eyes  Draw Pig's gill  Mouth  Body  Hands  Feet  Tail  save ",201
DEDA_WS201819_MARKOV_Chains_LSTM_Trump_Tweets,"Uses Markov Chains and LSTMs to generate new text in the style of some input corpus. Includes the weights for a pretrained LSTM model for Donald Trump tweets. Both techniques can in principle be used for any kind of natural language. The LSTM model should also be able to deal with highly structured text such as computer code - but that might require more nodes per layer and more training time. If you get an error, make sure you have both tensorflow installed and numpy upgraded to the latest version.","LSTM, Markov Chains, text generation, Twitter, NLP",Justin Engelmann,DEDA_Class_2018WS_Markov_LSTM_Trump_Twitter,2,!/usr/bin/env python  coding: utf-8  In[19]: random os sys tensorflow numpy keras.models load_model keras.models Sequential keras.layers Embedding keras.callbacks LambdaCallback keras.optimizers RMSprop collections defaultdict  In[13]:  In[17]: # import and prepare text  Import Text > can be replaced with your own text  but then you need to train the model from scratch. taken from http://www.trumptwitterarchive.com/ drop half the text to speed up everything.  adapted from https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py  Create a list of Chars  cut the text in semi-redundant sequences of maxlen characters vectorise  In[21]:  Markov chain 'Training' taken from: https://eli.thegreenplace.net/2018/elegant-python-code-for-a-markov-chain-text-generator/   generation  you can add your own see text here.  In[4]:  build the model: a two-layer LSTM load weights  In[9]: sample preds  temperature=1.0  helper function to sample an index from a probability array gen_text  adapted from https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py  In[10]:  In[ ]:  define print out call back on_epoch_end epoch  _  Function invoked at end of each epoch. Prints generated text. sentence = starter_seq[:]  train model some more # if you're not a GPU  this will take a very  very long time  model.fit x  y             batch_size=1024             epochs=10             verbose=1             callbacks=[print_callback tf.keras.callbacks.ModelCheckpoint '{epoch:02d}-{loss:.2f}.hdf5'  monitor='loss'  period=2)]),174
SFEpacfma2,Plots the partial autocorrelation function of an MA(2) (moving average) process.,"acf, partial, PACF, autocorrelation, moving-average, discrete, graphical representation, linear, plot, process, simulation, stationary, stochastic, stochastic-process, time-series","Christian M. Hafner, Ying Chen",QID-600-SFEpacfma2,1,pandas numpy statsmodels.graphics.tsaplots plot_pacf matplotlib statsmodels.tsa.arima_process arma_generate_sample  parameter settings  lag value  add zero-lag and negate alphas,16
sentiment_analysis_1,Project_Group8 file,"NUS, FE5225, sentiment analysis, stopwords, first step",Wang Yuzhou,sentiment_analysis_1,2,os jieba collections OrderedDict numpy pandas csv  read manual dictionary seg_word sentence 去除停用词      calculate_score_by_list word_list score sentence  read data  This part is to build dictionary manually  whose name is manual_dict.csv  word count  enter value by the order of the count of key  save manual dictionary  calculate score_list  scoring,48
sentiment_analysis_1,Project_Group8 file,"NUS, FE5225, sentiment analysis, stopwords, first step",Wang Yuzhou,sentiment_analysis_1,2,collections defaultdict os re jieba codecs from snownlp import SnowNLP seg_word sentence 读取停用词文件  去除停用词      读取情感词典（需要根据词频统计进行扩充+网络词典的删减） create_sendic  读取国立台湾的大学情感词典 sen_file3 = open 'sum_positive.txt'  'r+'  encoding='utf-8') sen_file4 = open 'sum_negative.txt'  'r+'  encoding='utf-8') sen_list3 = sen_file3.read ).splitlines ) sen_list4 = sen_file4.read ).splitlines ) print len sen_list3)) print len sen_list4)) 读取Boson网络情感词典 整合字典 对每一行内容根据空格分隔，索引0是情感词，1是情感分值 读取否定词文件并创建列表 create_notlist  获得每条评论的最终评分 caculate_score word_list sen_dict not_word_list,55
SC_QuantletData,"This quantlet was originally writen by Eliza Zinovyeva which is also available on Quantlet repository under HClustering. The current code is modified for application of Spectral Clustering on the same data using the same methodology.
Generally, the code uses the keywords from Quantlet data, tokenizes the words to vectors and applies spectral clustering using scikit python library to cluster the quantlets into different number of clusters. It also uses PCA and TSNE for visualisation of the clusters","Quantlet, Graph theory, spectral clustering, clustering, plot, animation, laplacian, graph, eigenvalues, K means, noisy circles, scikit learn, PCA, TSNE",Kainat Khowaja,SC_QuantletData,2,!/usr/bin/env python  coding: utf-8  In[6]:  Installing Keras module conda install -c conda-forge keras !pip install -r requirements.txt  In[7]:  import modules json pprint pprint pandas numpy keras.preprocessing.text Tokenizer keras.preprocessing.sequence pad_sequences matplotlib pyplot sklearn.decomposition PCA sklearn.neighbors DistanceMetric sklearn.manifold TSNE sklearn.metrics.pairwise cosine_distances sklearn.metrics pairwise_distances sklearn.cluster SpectralClustering sklearn.metrics.pairwise cosine_similarity sklearn.preprocessing StandardScaler sklearn.decomposition PCA sklearn.metrics silhouette_score  additional set up  suppress scientific float notation  constants and parameters  load data   In[8]:  create data frame  In[9]:  extract the columns we need  In[10]:  In[11]:  text preprocessing labels = [i.split '/')[1] for i in df.name]  In[12]: Calculating cosine similarity affinity matrix  In[8]:  prepare for plotting  In[13]: Making 10 clusters using spectral clustering as an example  In[14]: Adding cluster labels to the datafram  In[15]: Dropping the 'metainfo.txt' from quantlet names to avoid redundancies Viewing quantlet names with cluster lables  In[17]: ## Looking at the names of quantlets in a single cluster  more options can be specified also  In[13]:  In[14]:  In[16]:  In[17]:  In[32]: print df.head 20))  In[33]:  In[34]: print df.head 20)) plt.legend loc = 'upper left'  prop={'size': 10})  In[ ]:,168
Crawl_cryptocurrencies,"Crawling cryptocurrencies data from coinmarketcap.com using beautifulsoup library from python language programming, and store it into csv file","web crawling, cryptocurrencies, blockchain, CRIX analysis, coinmarketcap",Muhaimin 20201107,Crawl_cryptocurrencies,1, -*- coding: utf-8 -*- import library bs4 BeautifulSoup requests pandas json time declare the website link check the cryptocurrencies might be available begin crawl the data Set up the date period when we want crawl the data. Caution! The data might be expired or not available yet and made the results empty. yyyymmdd yyyymmdd create dataframe   preprocess the timestamp save to directory,62
RL_CoinReturnsFigures,"Draws figures of all monthly coin returns (relative to Bitcoin) for all coins used in the Thesis experiments (from 07/2015 to 10/2018, if available).","reinforcement learning, neural network, machine learning, portfolio management, cryptocurrency",Ilyas Agakishiev,RL_CoinReturnsFigures,1,"pandas matplotlib matplotlib  Import from ""Database""    ",6
Step1. Generate Fair and Loaded Dice Sample,"Generate random sample – Observations & Hidden states of dice (with Faif and Loaded dice used interchangeably), sample size is set as N",ERROR,ERROR,Step1. Generate Fair and Loaded Dice Sample,1,!/usr/bin/env python3  -*- coding: utf-8 -*- os numpy pandas matplotlib 0. ================================================ Functions =============================================== InitialState pi_0=0.5 Default: replace=True EmitObs S_t  Dice  pF  pL Fair dice state with prob = pF Loaded dice state with prob = pL TransitState S_t  p01  p10 Fair dice state with prob = pF Loaded dice state with prob = pL GenerateFLDice N  p01  p10  pL  pF=[1/6 for i in range 6)]  Dice = [i+1 for i in range 6)]  pi_0=0.5  seed=None  Generate initial hidden state S_t & the Observed state O_t out of it Generate New S_t at next time  i.e.  t+1) point according to previous mechanism of S_t;  and O_t out of New S_t 1. =============== Set parameters for sampling & those of Hidden Markov Model HMM) ============================= # Sampling-related parameters sample size # HMM-related parameters  initial prob of hiddent state p S_0 = 0)  [0: fair dice; 1: Loaded dice]; p S_0=1) = 1-pi_0  Transition prob from 0 F) to 1 L)); p00 =  1- p01  Transition prob from 1 L) to 0 F); p11 = 1-p10 ## Emission probs: pF vs pL are distributions of Fair vs Loaded Dice  with point = [1 2 ... 6]  respectively ## Note: length of Dice  pF  pL must be the same 2. ============================= Generate sequence of {O_t}  {S_t} from t=0 ... N-1 ====================  # To reproduce the experiment  seed is fixed; o.w. you can set seed = None for fully random gereration or equal to below: Sample_Hid  Sample_Obs = GenerateFLDice  N=N  p01=p01  p10=p10  pL=pL  seed=seed  pi_0 = pi_0)  Output data # Data Visualization -------------------------------------------------------------------------------------------- plot_timeseries axes  x  y  color  xlabel  ylabel  linestyle='-'  marker='None',268
SFM_ES_Pareto,This quantlet estimates Tail Entropy Expected Shortfall for a Pareto Distribution.,ERROR,Daniel Traian Pele,SFM_ES_Pareto,2,!/usr/bin/env python  coding: utf-8  In[11]: numpy pandas scipy.stats pareto scipy.stats norm scipy.stats linregress scipy.stats multinomial matplotlib statsmodels statsmodels entropy hist  bit_instead_of_nat=True frange start  stop  step probability of bins plt.legend loc='upper center' bbox_to_anchor= 0.5  -0.15)  fancybox=True  shadow=True)  In[13]: plt.legend loc='upper center' bbox_to_anchor= 0.5  -0.15)  fancybox=True  shadow=True)  In[ ]:  In[ ]:,49
DEDA_HClustering_image_example_km,Motivation example on image segmentation with k-means clustering,"Computer vision, image segmentation, k-means, cluster analysis",Elizaveta Zinovyeva,DEDA_HClustering_image_example_km,2,!/usr/bin/env python  coding: utf-8  In[ ]:  import modules numpy matplotlib matplotlib cv2 sklearn.feature_extraction image  In[ ]:  contants and parameters  In[ ]:  read image from folder  In[ ]:  reshape and pre-process data  In[ ]:  define criteria  Now convert back into uint8  and make original image,44
SFEbitreePDiv,Computes European/American option prices using a binomial tree for assets with dividends as a percentage of the stock price amount.,"binomial, tree, asset, call, put, option, option-price, european-option, dividends, financial, black-scholes",ERROR,QID-1452-SFEbitreePDiv,1,numpy calc_parameters T  N  sigma  r  div  P up movement) set_factor t t_div div calc_price S0  K  u  d  N  r  dt  q  disc_div  disc_div_t  option  calculate the values at maturity T  factor: 1-div for t > t_div  else: 1 Using the recursion formula for pricing in the CRR model:   from  T-dt  T-2*dt  ....  dt  0) ###### MAIN ################  current stock price  strike price  time to maturity  volatility  interest rate  continuous dividend   div  t). after t: S t)= 1-div_1)*S t)  steps in tree  calculate price ,85
SFM_Sim_pareto,SFM_Sim_pareto simulated a Pareto distribution.,"Pareto, density, simulation",Daniel Traian Pele,SFM_Sim_Pareto,2,!/usr/bin/env python  coding: utf-8  In[27]: numpy scipy.stats pareto matplotlib  In[ ]:  In[ ]:  In[ ]:,15
RFC,Use machines learning algorithms to predict daily price move in Bitcoin and perform day trading,"Bitcoin, SVM, Random Forest, Neural Network, LSTM","Hong Yuxi, Lin Jinyu, Sun Rongsheng, Zhou Xiaoqian, Zou Yutong",RFC,4,!/usr/bin/env python  coding: utf-8  In[1]: pandas numpy matplotlib sklearn.ensemble RandomForestClassifier sklearn.model_selection train_test_split sklearn.model_selection RandomizedSearchCV sklearn.metrics classification_report sklearn.metrics pandas read_csv pandas ExcelFile IPython.display Image sklearn.tree export_graphviz subprocess call  In[2]: train_model data get data Split the data to train and test Below uses K-Fold Validation method to find the best parameters  Number of trees in random forest  Number of features to consider at every split  Maximum number of levels in tree  Minimum number of samples required to split a node  Minimum number of samples required at each leaf node  Method of selecting samples for training each tree  Create the random grid print accuracy rate print OOB score  R^2 for out-of-bag sample set) print accuracy rate print OOB score  R^2 for out-of-bag sample set) print report metrics print feature significance Below is an example of decision tree  Export as dot file  Convert to png using system command  requires Graphviz)  Display in jupyter notebook,150
RFC,Use machines learning algorithms to predict daily price move in Bitcoin and perform day trading,"Bitcoin, SVM, Random Forest, Neural Network, LSTM","Hong Yuxi, Lin Jinyu, Sun Rongsheng, Zhou Xiaoqian, Zou Yutong",RFC,4,!/usr/bin/env python  coding: utf-8  In[1]: pandas numpy matplotlib sklearn.ensemble RandomForestClassifier sklearn.model_selection train_test_split sklearn.model_selection RandomizedSearchCV sklearn.metrics classification_report sklearn.metrics pandas read_csv pandas ExcelFile datetime os os.path dirname  In[2]: define total rolling period  roll over every 1 week trading strategy:1 long;0 short calculate trading strategy performance  In[8]: plot figure  In[9]:  calculate trading strategy volatility and Sharpe ratio  annualised) 1Y treasury yield  In[32]:  In[ ]:,61
CrypOpt_HistoricalDensity,Uses Monte Carlo sampling on historical returns to estimate historical density of underlying of an option.,"sampling, crypto options, cryptocurrency, historical density, physical density, gif",Franziska Wehrmann,CrypOpt_HistoricalDensity,2,matplotlib  turn on when want to create GIF   otherwise will show all plots os math numpy matplotlib pyplot pandas imageio historical_density sampling  ------------------------------------------------------------------- LOAD DATA  -------------------------------------------------------------------- SETTINGS  ------------------------------------------------------------ COMPUTE AND PLOT  ----------------------------------------------------------------- TIME SERIES  ----------------------------------------------------------- DIFFERENT KERNELS  Use 3 different kernel to estimate  Scatter plot of data samples and histogram  -------------------------------------------------------------------- GIF PLOT density_plot tau_day  S0  M=10000  h=0.1  kernel='epanechnikov'  Used to return the plot as an image rray  draw the canvas  cache the renderer  create gif  ----------------------------------------------------------------- SINGLE PLOT plot_HD tau_day  S0  M=10000  h=0.1  kernel='epanechnikov',85
CrypOpt_HistoricalDensity,Uses Monte Carlo sampling on historical returns to estimate historical density of underlying of an option.,"sampling, crypto options, cryptocurrency, historical density, physical density, gif",Franziska Wehrmann,CrypOpt_HistoricalDensity,2,numpy sklearn.neighbors KernelDensity sampling data  target  tau_day  S0  M=10000 density_estimation sample  S  h  kernel='epanechnikov',14
BERT,Generate plots to illustrate the BERT result and importance of words,"NLP, word annotation, deep neural networks, transformers, self attention, text classification",Francis Liu,BERT,2,"!/usr/bin/env python  coding: utf-8    # Bidirectional Encoder Representation from Transformer  BERT)  This is a notebook of using BERT to distinguish whether a Reddit comment under the Bitcoin subreddit is posted in June 2016 or Dec 2017  where Bitcoin's price was stabilized in June 2016 and extremely volatile in Dec 2017.     Ktrain  a lightweight wrapper for Keras  is used in this notebook. It allows users to train and deploy BERT easily.     DO NOT RUN THIS NOTEBOOK LOCALLY UNLESS YOUR LOCAL MACHINE CONTAINS A NVIDIA GPU BETTER OR EQUIVALENT TO A GTX1060.     If you still want to run this notebook  you can upload this notebook to Google Colab. Google provides a free-to-use GPU  and TPU as well).   In[1]:  Create file structure   !tree --filelimit 10  In[ ]: numpy pandas os tqdm tqdm  In[3]: google.colab auth  https://cloud.google.com/resource-manager/docs/creating-managing-projects Put your project ID here  In[4]:  Check how many data we have  In[ ]:  Make balance dataset  In[ ]:  Seperate train test dataset  In[ ]:  Write files according to the ktrain package requirment write_txt_files T='train'  D='dec2017'  pd2write = dec2017_bal  id_ = train_id  In[8]:  In[9]: ktrain ktrain text  In[10]:  In[11]:  In[17]:  To get a trained model  For Unix based OS  e.g. Mac  Linux  Ubuntu etc.)  For windows machine  slower)  import urllib.request  url = 'https://box.hu-berlin.de/f/1760795d78e141c7aa58/?dl=1'  urllib.request.urlretrieve url  ""mymodel"")  In[ ]:  In[19]:  In[ ]:  In[21]:  In[ ]:  DON'T RUN THIS IF YOU DO NOT HAVE A GPU MACHINE  In[22]:  If you run the code correctly  you will get the below result  <img src=""text1.jpg"">  In[23]:  If you run the code correctly  you will get the below result  <img src=""text2.jpg"">  ",258
SDA_2020_St_Gallen_03_VisualizeSimulations,Plotting various perfomance graphes for simulated technical trading strategies. This quantlet should ideally be executed after SDA_2020_St_Gallen_01_DataImport.,"QLearning, Trading Bot, BTC, Bitcoin, Timeseries, Technical Indicators","Tobias Mann, Tim Graf",SDA_2020_St_Gallen_03_VisualizeSimulations,1,"pandas numpy matplotlib matplotlib matplotlib.dates DateFormatter matplotlib matplotlib.pylab date2num mplfinance.original_flavor candlestick_ohlc os  DATA IMPORT  --------------------------  set the Time Period and Output path  define the inputs  feature engineering  Set overall Plots ---------------------  FUNCTIONS -------------------- moving_average x  n  type='simple' find_csv_filenames path_to_dir  suffix="".csv""  SIMPLE MOVING AVERAGES --------------------------  Create signals  1.0 = Signal  0.0 = No signal  Generate trading orders  0 = do nothing  1 = Buy  2 = Sell  Initialize the plot figure  Add a subplot and label for y-axis ax1.margins x=-0.4  y=--0.4)  Plot the closing price  Plot the short and long moving averages  Plot the buy signals  Plot the sell signals  Show and save the plot  MACD --------------------------  Create signals  1.0 = Signal  0.0 = No signal  Generate trading orders  0 = do nothing  1 = Buy  2 = Sell  Upper Subplot  Lower Subplot  PRICE PLOT ---  Plot the Buy Signals  Plot the sell signals  MACD PLOT ---  Plot the Buy Signals  Plot the sell signals  plot and save  RSI -----------------------------------  Configure variables  Define variables  Set up plot  Create a new Axes instance with an invisible x-axis and an independent y-axis positioned opposite to the original one  i.e. at right)  Plot the Price  Plot the MA  Plot the volume  dollar volume in millions  sets secondary axis = 0  ax2t.set_ylabel ""Volume"")  plot the rsi  add legend  show and save plot  MEAN REVERSION -------------------------------------------  Get nested list of date  open  high  low and close prices  Upper Subplot ax1.plot df.index  df['close']  color='black')  Lower Subplot  ALL TOGETHER PLOTS ----------------------------------------------  Create figure and set axes for subplots  create axis  Create a new Axes instance with an invisible x-axis and an independent y-axis positioned opposite to the original one  i.e. at right)  Get nested list of date  open  high  low and close prices  Plot candlestick chart  Create signals  1.0 = Signal  0.0 = No signal  Generate trading orders  Plot MACD  Plot the Buy Signals  Plot the sell signals  Plot RSI  Above 70% = overbought  below 30% = oversold  Plot z-score  Show volume in millions  ax_vol.set_ylabel "" Million)"")  Format x-axis ticks as dates  Show and plot  PLOTTING THE PORTFOLIOS OF SIMPLE TRADING STRATEGIES ----------------------------------------------  Please change the input variables to get the wanted Porfolio  initilaize the plot  create a color palette  format x axis  plot the values ax.plot df_pfs.time  df_pfs.QL2  label='Q-Learning' )  Show and plot  create folder  save plot  PLOTTING THE Q_LEARNING PORTFOLIO ----------------------------------------------  Please change the input variables to get the wanted Porfolio  initilaize the plot  create a color palette  format x axis  plot the values  Show and plot",411
DEDA_2020SS_CRIX_vs_Other_Indices,Comparison of the performance of CRIX against other cryptocurreny indices and the total crypto market.,"CRIX, total market, values, returns, correlation","Davide Balistreri, Vafa Ganbarova",DEDA_2020SS_CRIX_vs_Other_Indices,2,!/usr/bin/env python  coding: utf-8  In[1]:  Importing data pandas  In[2]: numpy pandas Series scipy scipy stats  In[3]:  Descriptive Statistics  In[4]:  In[5]:  Plotting matplotlib  In[6]:  In[7]:  In[8]:  Standardization of index values sklearn preprocessing sklearn.preprocessing StandardScaler  In[9]:  Plotting standardized values against total market  In[10]:  In[11]:  In[12]:  In[13]:  In[14]:  In[15]:  In[16]:  Pairplots of index values seaborn  In[17]:  In[18]:  In[19]:  Correlations of standardized index values  In[20]:  In[21]:  In[22]:  In[23]:  In[24]:  Computation of log returns  In[25]:  In[26]:  Plotting log returns  In[27]:  In[28]:  In[29]:  Pairplots of the log returns  In[30]:  In[31]:  Correlations of log returns  In[32]:  In[33]:  In[34]:  Volatility  In[35]:  In[36]:  In[37]: matplotlib matplotlib.colors Normalize numpy.random rand  In[38]: 009ACD'  '#ADD8E6'  '#63D1F4'  '#0EBFE9'  '#C1F0F6'  '#0099CC']  In[39]:  In[ ]:,110
pyTSA_ReturnsIBM,"This Quantlet plots monthly time series of returns of Procter and Gamble from 1961 to 2016 and  their ACF and PACF (Example, 2.4 Figures 2.8-2.9 in the book)","time series, autocorrelation, returns, ACF, PACF, plot, visualisation","Huang Changquan, Alla Petukhina",pyTSA_TrendIBM,1,numpy pandas matplotlib PythonTsa.plot_acf_pacf acf_pacf_fig PythonTsa.LjungBoxtest plot_LB_pvalue,7
WebScraping_Oil_News_Headline_and_Clustering,Project_Group11 file,"NUS, FE5225, oil, news, headline, webscraping","Peng Wenhui, Gui Yilin",WebScraping_Oil_News_Headline,2,"!/usr/bin/env python  coding: utf-8  In[1]: requests bs4 BeautifulSoup pandas numpy urllib.request urlopen  In[3]: os os path PIL Image numpy matplotlib wordcloud WordCloud nltk.corpus stopwords  In[4]: url='https://www.investing.com/commodities/crude-oil-news/'+'2' https://www.reuters.com/news/archive/oil?view=page&page=2&pageSize=10 print r) print url) print response) icontents = site.find_all class_=""js-external-link-wrapper articleItem"") print contents) print news_title) print text)  In[5]: all_text=all_text+text     stopword=stopword.append 'hours')  In[6]:  In[27]: os re nltk os path nltk.stem WordNetLemmatizer nltk.stem.porter PorterStemmer nltk.corpus stopwords nltk.corpus stopwords nltk.stem.wordnet WordNetLemmatizer string gensim gensim corpora matplotlib pandas numpy  In[28]:  Regex cleaning  keep only letters  numbers and whitespace  apply regex  lower case   In[29]: nltk clean doc  In[30]:                      columns[2]:""""                      columns[3]:""""                     columns[4]:""""                                                                                                      plt.imshow zz  cmap='hot'  interpolation='nearest') plt.show )  In[31]: SequenceSelection dictionary  length  startindex = 0  Test input  In[32]: preprocess text re string punctuation os  In[33]: hieratical nltk.tokenize word_tokenize nltk.corpus stopwords nltk.stem.snowball SnowballStemmer sklearn.feature_extraction.text TfidfVectorizer sklearn.metrics.pairwise cosine_similarity sklearn.manifold MDS matplotlib pandas scipy.cluster.hierarchy ward os  In[34]: tokenize_and_stem text_file  declaring stemmer and stopwords language  In[35]:  In[36]: sklearn.cluster KMeans  ax.legend numpoints=1)  In[37]: sklearn.manifold LocallyLinearEmbedding  In[38]:  In[ ]:  In[ ]:",156
CRIXportfolio,"Build three cryptocurrencies portfolios based on price movements forecasts with MLP, RNN and LSTM neural networks in order to outperform CRIX. We apply three different weight strategies: price, marketcap and equally weighted portfolios. We use the predictions to build quarterly rebalanced buy and hold portfolios and also long-short portfolios. We compare the performance with the baseline models.","neural networks, deep learning, cryptocurrency, CRIX, portfolio, prediction, time series, stock price forecast",SPILAK Bruno,CRIXportfolio,1,dldata.portfolio_tools os pandas matplotlib To know how these tables are created look at basic_result function ####################  Quarterly returns # #################### ######################## Crix ##################  load prediction Baseline LSTM RNN TDNN Final models LSTM RNN  MLP ############################################### ########## Marketcap weighted ################# ############################################### ################ Long portfolio True portfolio Baseline LSTM RNN  MLP ############## Final models LSTM RNN  MLP ################# long-short portfolio True portfolio ##############  Baseline LSTM RNN  MLP ############## Final models LSTM RNN  MLP ######################################### ########### Equally weighted ############ ######################################### ################ Long portfolio  True portfolio ##############  Baseline LSTM RNN  MLP ############## Final models LSTM RNN  MLP ################ Long-short portfolio True portfolio ##############  Baseline LSTM RNN  MLP ##############  Final model LSTM RNN  MLP ####################################### ########### Price weighted ############ ####################################### ################ Long portfolio True portfolio ############## Baseline LSTM RNN  MLP ############## Final models LSTM RNN  MLP ################# Long short portfolio ############## True portfolio ############## Baseline LSTM RNN  MLP ############## Final models LSTM RNN  MLP ######################################### ######## Performance comparison ######### ######################################### Performance long strategy Performance long short strategy,162
DEDA_class2019_SYSU_Abstract_LDA_Crawler,Collect >=130 abstracts from http://www.wiwi.hu-berlin.de/de/forschung/irtg/results/discussion-papers,"web scraper, regular expression, abstract analysis, LDA preparation, textmining","FB, FS, JH, WKH",DEDA_class2019_SYSU_Abstract_LDA_Crawler,1,"requests  take the website source code back to you urllib  some useful functions to deal with website URLs bs4 BeautifulSoup  a package to parse website source code numpy  all the numerical calculation related methods re  regular expression package itertools  a package to do iteration works pickle  a package to save your file temporarily pandas  process structured data os  the path you save your files  This link can represent the domain of a series of websites  abs_folder = cwd_dir + 'Abstracts/'  os.makedirs abs_folder  exist_ok=True)  get source code  parse source code  if paper[3][-3:] == 'txt':  if paper[3][-3:] == 'pdf':      abstract_parsed = soup paper_abstract_page.content)      main_part = abstract_parsed.find_all 'body')[0].text.strip )  with open abs_folder + f""{re.sub '[^a-zA-Z0-9 ]'  ''  paper[0])}.txt""  'w'  encoding='utf-8') as abs_f:      abs_f.write main_part)",121
CMBbubbles,"Visualizes a collection of computers clustered by various means using D3.js and CoffeeScript. On hovering brief information of the specific computer appears (including producer, model name and picture). A live example can be found at cm.wiwi.hu-berlin.de/viz. Extended upon original work by Jim Vallandingham.","data, web, html, interactive, data visualization, graphical representation, visualization, plot, clustering","Torsten van den Berg, Sophie Burgard",CMBbubbles,1,!/usr/bin/env python  -*- coding: utf-8 -*-  CMBbubbles.py:  A simple wrapper for CMBbubbles.html for Quantnet compatibility.  Set HTML header  utf-8)  Open actual HTML  Output header and actual HTML content,28
pyTSA_MarkovReturnsDAX,"This Quantlet plots monthly time series of returns of Procter and Gamble from 1961 to 2016 and  their ACF and PACF (Example, 2.4 Figures 2.8-2.9 in the book)","time series, autocorrelation, returns, ACF, PACF, plot, visualisation","Huang Changquan, Alla Petukhina",pyTSA_MarkovReturnsDAX,1,pandas statsmodels matplotlib PythonTsa.plot_acf_pacf acf_pacf_fig PythonTsa.LjungBoxtest plot_LB_pvalue,7
SFEtimeret,"Reads the date, DAX index values, stock prices of 20 largest companies at Frankfurt Stock Exchange (FSE), FTSE 100 index values and stock prices of 20 largest companies at London Stock Exchange (LSE) and plots the time series of the DAX and FTSE 100 daily returns from 1998 to 2007.",ERROR,Andrija Mihoci,QID-3381-SFEtimeret,1,numpy pandas matplotlib datetime read data calculate log returns Prepare Plots Create Plots,13
RL_Experiment2Performance,Outputs the performance of the algorithm for the test period as a plot. Additionally plots performance of (modified) CRIX and the algorithm without leverage.,"reinforcement learning, neural network, machine learning, portfolio management, cryptocurrency",Ilyas Agakishiev,RL_Experiment2Performance,1,pandas datetime datetime matplotlib matplotlib.dates DateFormatter,6
TS_CRIX_VCRIX,Time Series of daily CRIX data in the period 2 May 2016 to 10 Nov 2018; all together 923 data points Time Series of the VCRIX (CRIX volatility),"Time Series (TS), CRIX, VCRIX",Elena Ivanova,TS_CRIX_VCRIX,2,!/usr/bin/env python  coding: utf-8  In[1]: pandas numpy matplotlib inline datetime matplotlib  In[2]:  In[ ]: one needs the package seaborn installed  seaborn  In[ ]: seaborn  In[ ]:  In[ ]:,28
DEDA_SVM_Spiral,Determines and plots the decision boundaries of a SVM classifier with RBF kernel for spiral data,"Support vector machines, SVM, classification, radial basis functions",Georg Keilbar,DEDA_SVM_Spiral,1,numpy matplotlib sklearn svm Read data plot_svm_decision X  y  model_class  **model_params Fit model Define grid Prediction on grid Contour + scatter plot,22
SFEmersenne_twist,Demonstrates Mersenne-Twister in Python framework for random number generation,"‘normal-distribution, plot, random, random-number-generation, simulation’","Alisa Kolesnikova, Ramona Ottow",SFEMersenneTwist,1,random Basic functions System is constantly running the RN generation Example of statistical bootstrapping using resampling with replacement to  estimate a confidence interval for the mean of a sample of size five statistics mean random choices Another usecase random expovariate statistics mean Plotting random numbers  path) matplotlib random Scatterplot,49
DEDA_Class_2018_StGallen_NeuralNetworks,"This Quantlet contains one of two analyses that explore the relationship between weather data and stock returns. TrainNeuralNetwork.py sequentially runs different configurations of neural networks, aiming to predict stock returns from weather data.",ERROR,ERROR,DEDA_Class_2018_StGallen_NeuralNetworks,1," -*- coding: utf-8 -*- os pickle pandas numpy sklearn preprocessing  local imports from utils folder utils utils utils  making a directory to save plots: main   loading the data  weather_path = ""./data/Weather_ALL.csv""  stock_path = ""./data/StockIndices.csv""  Loading the data  Descriptive Statistics for weather data  only makes sense with more data  de.temp_descriptive weather_per_city)  Creating and saving Histograms of returns of all indices  for index in return_data_list:     de.plot_histogram return_data_list[index]['Return']  index)  train_NN_on_prices stock_path  weather_path) save_to_pickle data  filename train_NN_on_prices stock_path  weather_path  Neural Net Data  LSTM  step over the 5 000 in jumps of 200  grab from i to i + 200 train_NN_on_returns stock_path  weather_path  Neural Net Data  LSTM  step over the 5 000 in jumps of 200  grab from i to i + 200 calculate_returns price_data_list",120
NLP Japanese text from Yahoo,ERROR,ERROR,"Junjie Hu, Dwilaksana",NLP Japanese text from Yahoo,1,os pandas sudachipy tokenizer  pip3 install SudachiPy if package not found  CorePackage: pip install https://object-storage.tyo2.conoha.io/v1/nc_2520839e1f9641b08211a5c85243124a/sudachi/SudachiDict_core-20191030.tar.gz  More information: https://github.com/WorksApplications/SudachiPy wordcloud WordCloud matplotlib TokenCleanText tker  mode  text  Read and Pre-process Data  Define Stopping Words  Define stop words and punctuation  not perfect!  Tokenize Text  Create WordCloud,43
Step3. HMM Analysis,"Use standard HMM package (i.e., hmmlearn) to analyze the data with multinomial (discrete) emissions – in Casino case of fair and loaded dice data",ERROR,ERROR,Step3. HMM Analysis,2,!/usr/bin/env python3  -*- coding: utf-8 -*- numpy pandas matplotlib ====================== 0. Input: Observation O_t  and true Hidden states  for comparison) =================================== Generate_Dice_Sample 0.1.---------------- Set parameters for sampling & those of Hidden Markov Model HMM) --------------------------------  # Sampling-related parameters  integer or None  i.e.  random) sample size # HMM-related parameters  initial prob of hiddent state p S_0 = 0)  [0: fair dice; 1: Loaded dice]; p S_0=1) = 1-pi_0  Transition prob from 0 F) to 1 L)); p00 =  1- p01  Transition prob from 1 L) to 0 F); p11 = 1-p10 ## Emission probs: pF vs pL are distributions of Fair vs Loaded Dice  with point = [1 2 ... 6]  respectively ## Note: length of Dice  pF  pL must be the same 2.0.------------------------ Generate sequence of {O_t}  {S_t} from t=0 ... N-1 ------------------------ # To reproduce the experiment  seed is fixed; o.w. you can set seed = None for fully random gereration Obs.to_csv 'Sample_Obs.csv') Hid.to_csv 'Sample_Hid.csv') # Data Visualization -------------------------------------------------------------------------------------------- plot_timeseries axes  x  y  color  xlabel  ylabel  linestyle='-'  marker='None' ======================================== 1. Model Training and Prediction ================================================== hmmlearn hmm #load model– MultinomialHMM which is Hidden Markov Model with multinomial  discrete) emissions {hidden states} #Training and Prediction of Hidden states log-probability of sequence Obs # To make consistent of the symbols of Hid_pre with Hid  transform the symbols of Hid_pre as follows: #Monitoring convergence of EM algorithm #Does the algorithm converge? True  ---------------------------------------------------------------------------------------------------------------------------------------  --------------------------- Problem 1. [Evaluation] Given a known model what is the likelihood of sequence Obs happening? ---------------------- math # Suppose new seq. X  compute its prob.  ---------------------------------------------------------------------------------------------------------------------------------------  --------------------------- Problem 2. [Decoding] Given a known model and sequence Obs  what is the optimal hidden state sequence? ------------ # Suppose new seq. X  compute its prob. # Check prediction accuracy ## confusion matrix Notice:  check if the labels from true hidden seq.  Hid) are consistent with those form predicted seq.  Hid_pre) If yes: o.w.  as below: Now 0=Loaded dice  1=Fair dice ## acc sklearn.metrics accuracy_score ???????????????????????????????????????????????????? temp.plot linestyle='None'  markersize = 10.0) plt.show )  --------------------------------------------------------------------------------------------------------------------------------------- ---------------------------- Problem 3. [Learning] Given sequence Obs and number of hidden states  what is the optimal model which maximizes the probability of O?------- Initial state occupation distribution  Matrix of transition probabilities between states. dim=  n_components  n_components) Probability of emitting a given symbol when in each state. dim=   n_components  n_features)),382
Step3. HMM Analysis,"Use standard HMM package (i.e., hmmlearn) to analyze the data with multinomial (discrete) emissions – in Casino case of fair and loaded dice data",ERROR,ERROR,Step3. HMM Analysis,2,!/usr/bin/env python3  -*- coding: utf-8 -*- os numpy pandas matplotlib 0. ================================================ Functions =============================================== InitialState pi_0=0.5 Default: replace=True EmitObs S_t  Dice  pF  pL Fair dice state with prob = pF Loaded dice state with prob = pL TransitState S_t  p01  p10 Fair dice state with prob = pF Loaded dice state with prob = pL GenerateFLDice N  p01  p10  pL  pF=[1/6 for i in range 6)]  Dice = [i+1 for i in range 6)]  pi_0=0.5  seed=None  Generate initial hidden state S_t & the Observed state O_t out of it Generate New S_t at next time  i.e.  t+1) point according to previous mechanism of S_t;  and O_t out of New S_t,110
PAC_CV_GCV,Fitting the bandwidth of Nadaraya-Watson estimator to simulated data on an interval using cross-validation and generalised cross-validation in R and Python,"smoothing, Nadaraya-Watson, cross-validation, generalised cross-validation, empirical error","Anna Shchekina, Lili Matic, Wolfgang Karl Härdle",PAC_CV_GCV,2,!/usr/bin/env python  coding: utf-8    # PAC learning : Bias-variance trade-off   $L_{emp}$  $L_{CV}$ and $L_{GCV}$     ## Load packages    In[1]: numpy scipy scipy.stats norm sklearn.neighbors KernelDensity matplotlib os  # Data generation process  In[2]:  Fix some general setup variables  sample size n   Monte Carlo iterations J  number  of repetitions for the CV exercise J_cv   noise variance  Define colors  ## 1. Generate  X$\sim U [0 1]$  In[3]:  ## 2. Generate $y_i = \sin^3 2\pi x_i^3) + \varepsilon_i   i\in [1 n]$  where $\varepsilon_i \sim N 0 \sigma_\varepsilon^2)$ and $\sigma^2_\varepsilon = 0.3$      2a) Generate $\varepsilon_i \sim N 0 \sigma_\varepsilon^2)$ with $\sigma^2_\varepsilon $ to be determined  In[4]:  Only for illustration - regenerated in next block!  2b) Generate $y_i = \sin^3 2\pi x_i^3) + \varepsilon_i$ for $i\in 1:n$  In[5]: f x gendata f  sigma  n  ## Data plot  In[6]:  # Kernel regression smoothing with NW kernel estimator  ## Smoothing with Gaussian Kernel    In this task we choose $K_h$ to be a Gaussian Kernel. The Naradaya Watson  NW)  estimator is given by  \begin{align}  \hat{f}_h t):=\sum_{i=1}^n\frac{K_h t-x_i)}{\sum_{i=1}^nK_h t-x_i)}y_i  \end{align}    In[7]:  Naradaya-Watson at points t  a mesh of grid points nw_inner t  x  y  h  f_h estimation  fh ts  x  y  h  In[8]:  trial bandwidth for Gaussian kernel  note that in terms of Uni  box car) kernel   this has to be multiplied with 1.35/0.77  i.e. we for h=0.01 are averaging over the window  t - 0.0175  t + 0.0175)  LM change the NW smoother !!!!   NW estimation  In[9]:  # Bias-variance decomposition    \begin{equation}  \begin{aligned}  \mathrm{E}\left[ y-\hat{f})^{2}\right] &=\mathrm{E}\left[y^{2}+\hat{f}^{2}-2 y \hat{f}\right]=\mathrm{E}\left[y^{2}\right]+\mathrm{E}\left[\hat{f}^{2}\right]-\mathrm{E}[2 y \hat{f}] \\  &=\operatorname{Var}[y]+\mathrm{E}[y]^{2}+\operatorname{Var}[\hat{f}]+\mathrm{E}[\hat{f}]^{2}-2 f \mathrm{E}[\hat{f}]=\operatorname{Var}[y]+\operatorname{Var}[\hat{f}]+ f-\mathrm{E}[\hat{f}])^{2} \\  &=\operatorname{Var}[y]+\operatorname{Var}[\hat{f}]+\mathrm{E}[f-\hat{f}]^{2}=\sigma^{2}+\operatorname{Var}[\hat{f}]+\operatorname{Bias}[\hat{f}]^{2}  \end{aligned}  \end{equation}    Source: Wikipedia    Estimation for some bandwidths e.g. $h \in [0.01  0.02  \dots  0.1]$  In[10]:  ## Find $\hat{f}$   Kernel regression for all bandwidths    Bias: $\mathrm{E}[\hat{f} - f] =  \mathrm{E}[\hat{f}]  - f $    Variance: $E\left[\hat{f}-\mathrm{E} \hat{f}\right]^{2}$  In[11]:  Repeat data generation J times  collects the CVs over simulations  collects the emp error over simulations  In[12]:   Computation of bias  end h loop  end J loop  now calc bias  end h loop  In[13]:  In[14]:  end h loop  end J loop    In[15]:  Plot bias-variance-tradeoff  In[16]:  In[17]:  ## Cross-validation    Estimate   \begin{equation}  \hat{f}_{h -i\ } x_i) = \sum_{j\ne i} \frac{K_h x_i-x_j)}{{\sum_{k\ne i} K_h x_i-x_k)}}\ y_j  \end{equation}  In[18]:  end l loop over obs  end h loop  end J loop  In[19]: plt.plot h L_emp) plt.plot h [sigma**2]*len h))  In[20]:  end h loop  end J loop    ## Generalized Cross-validation  \begin{equation}  L^\mathrm{GCV} \hat{f}_h) = GCV h)=  \frac{1}{\left[1-\mathrm{tr}\{S h)\}/n  \right]^2}\ L_\mathrm{emp} \hat{f}_h) \rightarrow \min_{h\in \mathbb{R}}   \end{equation}    for the hat matrix $$\mathrm{tr}\{S h)\} = \sum_{i=1}^n s_{ii}= n^{-1} K_h 0) \sum_{i=1}^n\{n^{-1} {\sum_{k=1}^n K_h x_i-x_k)}\}^{-1}$$  We know that   $\mathrm{tr}\{S h)\} \approx h^{-1} K 0)$      In[21]:  tr  S h)/n )  In[22]:,432
Portfolio_NetworkDegree,"Portfolio constructed by sorting the network attention proxy, network degree. S&P500 constituents are sorted into quintile, and each quintile portfolio is rebalanced at the end of each month by the preceding monthly network degree. Portfolios can be value-weighted or equal-weighted.","Textual Analysis, Network Analysis, Network Degree, Portfolio Management",Junjie Hu,Portfolio_Strategy,1,os datetime pandas numpy matplotlib statsmodels.formula api calendar pickle portfolio_characters_overtime portfolios_overtime  character  num_port  shift_period=1  avg_weight=False  strategic_returns_overtime = [np.multiply periodic_returns  positions) for periodic_returns in weighted_values_overtime]  shift the periods plot_3portfolios_cumreturns portfolios_cumsum  benchmark_cumsum  lb=' EW)'  ifbenchmark=True convert_month_int_index_2_datetime df read_ff_rf freq='daily'  Read FF factors  Risk free return portfolio_ts_analysis portfolio_returns_ts  newey_west=False  ifstd=False  ff_freq='daily'  adj_r2=False  portfolio_returns_ts = portfolio_returns_ew_m1['p_3'].to_frame )  FF3 regression  p_ff3[['p_1'  'ex_mkt'  'SMB'  'HML']].cumsum axis=0).plot )  FF5 regression  p_ff5[['p_1'  'ex_mkt'  'SMB'  'HML'  'RMW'  'CMA']].cumsum axis=0).plot )  ff5_rsqr = reg_ff5_fit.rsquared_adj  mean and alphas are in bps  mean =  mean * 100 * np.sqrt 255)).round 2)  percentage  annualized sharpe ratio  factor2sort stats  MV Pct stats  B/M stats  Liquidity stats,102
VisualOverfittingExemp,Uses a decision tree regression to give a visual example of overfitting,"DecisionTree, Decision Tree Regression, Overfitting, visualisation, random data, data generation",Jerome Bau,VisualOverfittingExemp,1,numpy pandas math matplotlib sklearn.tree DecisionTreeRegressor generate_funct_cloud size=100  x_range=[-10  10]  sigma=400 funct x random_float a  b decisiontree_through_cloud df  n=100,19
CV04_Merging_Tweets,Creates the csv that will be read into create_final_df. All daily tweets will be aggregated monthly and then concatted into one csv file,"Tweets, Trends, merge, aggregation, consolidation",Fabian Schmidt,CV04_Merging_Tweets,1,os glob pandas json csv   '02'  '03'  '04'  '05'  ]#'06'  '07'  '08'  '09'  '10'  '11'  '12']  Concat all tweets from different days into one file Dump the consolidated file with the format combine all files in the list export to csv,41
SFM_Ext_ret,This program tests the hypothesis that the DJIA log-returns are drawn from a Normal distribution and estimates the probability of extreme negative returns.,"Normal distribution, extreme returns, DIJA",Dragos Cioata,SFM_Ext_ret,2,!/usr/bin/env python  coding: utf-8  In[4]:  This program tests the hypothesis that the DJIA log-returns are drawn from  a Normal distribution and estimates the probability of extreme negative returns Author: Dragos Cioata Submitted: 04 Mar 2019 import numpy numpy pandas scipy scipy stats pylab seaborn scipy.stats norm matplotlib scipy.stats ttest_ind astropy.table Table read csv data make de column Date a datetime type extract the year of the date keep only the 1987 year or bellow make a empty list calculate the Log-price=log price) calculate the log-return the formula is  logprice t)=logprice t-1)-logprice t) where t is the time year) drop the first element  because he dosent have a log return plot log-returns make log-return a dataframe print  a descriptive statistics for log-return print the quantiles for log-return print the biggest 10 log-return print the littlest 10 log-return Probability of extreme negative returns;  In[ ]:  In[ ]:  In[ ]:,147
CV07_Modeling,Models for the crypto volatility analysis,"Models, Sentiments, Predictions, Trends, Cryptocurrencies, Time Series",Fabian Schmidt,CV07_Modeling,2,!/usr/bin/env python  coding: utf-8  In[ ]:  placeholder,7
RL_Experiment2Leverage,Outputs the leverage for each time period as a plot.,"reinforcement learning, neural network, machine learning, portfolio management, cryptocurrency",Ilyas Agakishiev,RL_Experiment2Leverage,1,pandas datetime datetime matplotlib matplotlib.dates DateFormatter,6
SFM_Block_maxima_GEV,This program applies the Block Maxima method to annual losses of DJIA.,"Extreme Value Theory, Block Maxima, log-returns, losses.",Daniel Traian Pele,SFM_Block_maxima_GEV,2,!/usr/bin/env python  coding: utf-8  In[38]: This program applies the Block Maxima method to annual losses of DJIA pandas datetime numpy matplotlib scipy.stats genextreme scipy stats scipy.stats kstest quandl plot the fit x.plot t  pdf)  In[ ]:  In[ ]:,38
SoNIC_AAPL_BTC_benchmark,"Comparison of SoNIC with VAR and l1 penalized VAR on AAPL and BTC datasets. We also make comparison with zero Theta, which corresponds to no causality.",ERROR,Yegor Klochkov,SoNIC_AAPL_BTC_benchmark,5,numpy scipy.stats norm get_missing_probabilities mat  conf=0.95  tol=0.1 missing_var X  delta  make_positive=True missing_covar X  Y  delta_x  delta_y,16
SoNIC_AAPL_BTC_benchmark,"Comparison of SoNIC with VAR and l1 penalized VAR on AAPL and BTC datasets. We also make comparison with zero Theta, which corresponds to no causality.",ERROR,Yegor Klochkov,SoNIC_AAPL_BTC_benchmark,5,"numpy matplotlib cvxopt matrix cvxopt.solvers qp seaborn scipy.linalg sqrtm math sklearn linear_model import read_data common common get_index_matrix cluster_num  ind  normalize=True  print ""WARNING!!!"") index_dist cluster_num  int1  int2 gauss_var1_process theta  sigma  T  variance of innovations is sigma * I lasso_from_covariance D  c  alpha  minimizes 1/2 x^{T}Dx - c^{T} x + \alpha \| v \|_1 random_basis n  k  orthogonal normalization of columns u_random n  k  cluster_num  index=None v_step D0  D1  alpha_v  z  make sure u^{\T}u = I! z_step cluster_num  D1  v  ind_old=None check this stupid function!!! <- apparently works... __init__ self  theta  u  v  index  loss  to implement: index choice; initial index + initial u  matrix competition  index competition?) alternating cluster_num  D0  D1  alpha_v  epochs=10  index_init=None print ""loss : {}"".format loss)) v_step_from_data x_train  y_train  alpha_v  z  make sure u^{\T}u = I! direct cluster_num  D0  D1  alpha_v  index_init=None loss print ""loss : {}"".format loss)) direct_from_data cluster_num  x_train  y_train  alpha_v  index_init=None  loss  print ""loss : {}"".format loss)) alternating_from_data cluster_num  x_train  y_train  alpha_v  epochs=100  index_init=None _func1 v  ind _func0 ind print ""K={}: epochs {}/{}"".format cluster_num  e  epochs)) matrix_competition type  repeat_num   *args  **kwargs simu n  c_num  s  T  pmin=1.0  define true index define true theta generate the time series include missing observations ax.set_xticklabels ax.get_xticklabels )  rotation=-90  fontsize=8) ax.set_yticklabels ax.get_yticklabels )  rotation=0  fontsize=8)  ax.set_xticklabels ax.get_xticklabels )  rotation=-90  fontsize=8)  ax.set_yticklabels ax.get_yticklabels )  rotation=0  fontsize=8) simu )",217
SoNIC_AAPL_BTC_benchmark,"Comparison of SoNIC with VAR and l1 penalized VAR on AAPL and BTC datasets. We also make comparison with zero Theta, which corresponds to no causality.",ERROR,Yegor Klochkov,SoNIC_AAPL_BTC_benchmark,5,numpy math ceil common common common.alternating matrix_competition get_sonic_theta Y_train  deltas  num_clusters  lmbd get_prediction Y_test  theta  deltas  SET NUMBER OF CLUSTERS  theoretical lambda #############################  we compare four methods:  1) SONIC  2) VAR + lasso = SONIC with K = N  3) VAR = SONIC with K = N and lambda = 0  4) theta = 0  no causality),57
SoNIC_AAPL_BTC_benchmark,"Comparison of SoNIC with VAR and l1 penalized VAR on AAPL and BTC datasets. We also make comparison with zero Theta, which corresponds to no causality.",ERROR,Yegor Klochkov,SoNIC_AAPL_BTC_benchmark,5,numpy  func takes ind! find_new_home func  k  ind  i greedy_update func  k  ind __init__ self  index  value  message  nochange kmeans_greedy func  k  n  iter_limit=10000  init_index=None,25
SoNIC_AAPL_BTC_benchmark,"Comparison of SoNIC with VAR and l1 penalized VAR on AAPL and BTC datasets. We also make comparison with zero Theta, which corresponds to no causality.",ERROR,Yegor Klochkov,SoNIC_AAPL_BTC_benchmark,5,numpy pandas read_csv csv math sqrt find_homo_Be x  scale=4  res=20 check_interval l  r read_SIFI name_to_sifi read_stock_twits_user_sentiment name_to_twits  min_delta=0.1  min_days=200 avoid_empty = lambda x  err_t: x[0] if np.size x) > 0 else err_t read_crix_returns name_to_crix_price read_crypto_sentiment name_to_crypto_sentiment  top_num=50,37
pyTSA_ReturnsPG,"This Quantlet plots monthly time series of returns of Procter and Gamble from 1961 to 2016 and  their ACF and PACF (Example, 2.4 Figures 2.8-2.9 in the book)","time series, autocorrelation, returns, ACF, PACF, plot, visualisation","Huang Changquan, Alla Petukhina",pyTSA_SpuriousRegression,1,numpy pandas matplotlib statsmodels.tsa.stattools adfuller statsmodels.tsa.arima_process arma_generate_sample statsmodels PythonTsa.plot_acf_pacf acf_pacf_fig,10
Augmento_visualization,"Visualizing distribution of Augmento.ai topics, correlation matrices, and autocorrelation of BTC price","sentiments, cryptocurrency, bitcoin, market analysis, data visualization, correlation matrix, autocorrelation","Gleb Zhidkov, Anna Shchekina, Vanessa Guarino",Augmento_visualization,2,!/usr/bin/env python  coding: utf-8  In[1]: pandas numpy matplotlib  In[2]:  In[3]: sentiments = sentiments.reindex sentiments.corr ).mean ).sort_values ascending=True).index  axis=1)  In[4]:  Visualize distribution of sentiments  daily data):  In[5]:  The lowest and the highest correlations:  In[6]:  Correlation plot for daily data  In[7]:  Correlation plot for hourly data  In[8]:  Correlation plot for hourly data  twitter only)  In[9]: matplotlib.ticker ScalarFormatter  In[10]:  In[11]:  Plot autocorrelation,59
BTC_ANA_data_preprocessing,"Data Cleaning, Merging and Preprocessing and application of the common input heuristic for of all transactions in order to get additional labels for unknown addresses. Feature Engineering for Multi Class Address Classification","Bitcoin, Blockchain, Transactions, Feature Engineering, Data Preprocessing","David Steiner, Minh Nguyen",BTC_ANA_data_preprocessing,4, -*- coding: utf-8 -*- pandas threading numpy requests io feature_engineering df Calculate Balance Lifetime and inputs keep inputs transaction value usd category count  paypack rate  address in input and output) Address counts per transaction balance  totql address input and output transaction value adr transaction value  one address) total transaction value  all addresses) ratio of adr value to tx value percent marketcap drop unneccessary columns handle_threads list_address  counter get_features tx  n_threads = 100 Add Dollar Price        Multithrading,76
BTC_ANA_data_preprocessing,"Data Cleaning, Merging and Preprocessing and application of the common input heuristic for of all transactions in order to get additional labels for unknown addresses. Feature Engineering for Multi Class Address Classification","Bitcoin, Blockchain, Transactions, Feature Engineering, Data Preprocessing","David Steiner, Minh Nguyen",BTC_ANA_data_preprocessing,4, -*- coding: utf-8 -*- pandas random string randomString x check_owner owner aggregate_most_common df https://stackoverflow.com/questions/15222754/groupby-pandas-dataframe-and-select-most-common-value group_transactions df  unique=False regroup df add_category wallets  labeled_tnx merge_tnx_wallets tnx  wallets_subset  labeled_wallets Merge trnsactions with wallet labels,31
BTC_ANA_data_preprocessing,"Data Cleaning, Merging and Preprocessing and application of the common input heuristic for of all transactions in order to get additional labels for unknown addresses. Feature Engineering for Multi Class Address Classification","Bitcoin, Blockchain, Transactions, Feature Engineering, Data Preprocessing","David Steiner, Minh Nguyen",BTC_ANA_data_preprocessing,4, -*- coding: utf-8 -*- pandas io requests  =============================================================================  Merge Data  ============================================================================= data_merging merge_data Import data sources https://coinmetrics.io/community-data-dictionary/ Combine all data sources  =============================================================================  Apply common input ownership heuristic   https://en.bitcoin.it/wiki/Common-input-ownership_heuristic  VERY LONG RUNTIME  ~10-20 houres)  ============================================================================= common_input_heuristic merge_tnx_wallets data_merging add_new_wallets extract only neccessarry labeled addresses unknown=13.3mio ; known=2.3mio Common input heuristic algorithm Add cateogory to owners Add new addresses to btc_wallets  =============================================================================  Filter transactions for min dollar or min percent of total marcetcap and year  ============================================================================= data_merging merge_data Filter remove self transaction  =============================================================================  Get list of unknown addresses  for scraping and feature engineering)  ============================================================================= data_merging get_unknown_wallets  =============================================================================  Feature engineering trainingset  VERY LONG RUNTIME  ~5-10 houres)  ============================================================================= feature_engineering get_features Import csv with adr details create new features append  Export csv with features Export all features  =============================================================================  Feature engineering unknown dataset  VERY LONG RUNTIME  ~24-48 houres)  ============================================================================= Import csv with adr details create new features Export csv with features append  Export all features,149
BTC_ANA_data_preprocessing,"Data Cleaning, Merging and Preprocessing and application of the common input heuristic for of all transactions in order to get additional labels for unknown addresses. Feature Engineering for Multi Class Address Classification","Bitcoin, Blockchain, Transactions, Feature Engineering, Data Preprocessing","David Steiner, Minh Nguyen",BTC_ANA_data_preprocessing,4, -*- coding: utf-8 -*- pandas merge_data btc_price_data  tnx  wallets Preprocess transactions Merge trnsactions with wallet labels Merge with price data filter_data data  filter_type  value  year_start=2013  year_end=2020 get_unknown_wallets df get addresses that have no label add_new_wallets wallets  labeled_wallets,37
UMAP_sim_ts,"Simulates time series data (AR1, sine curve, white noise) and applies the UMAP VizTech to reflect local and global structure of the data. ","UMAP, VizTech, AR1, Dimension Reduction, Time Series",Lucas Valentin Umann,Dimension-Reduction-Techniques,2,"!/usr/bin/env python  coding: utf-8  # A simulated example: UMAP and time series data  ### Load required packages and configuration of figure size  In[1]: numpy statsmodels sklearn.model_selection train_test_split sklearn.preprocessing StandardScaler matplotlib seaborn pandas umap math random  ### Function to simulate ARIMA time series  In[2]: ARIMA phi = np.array [0])  theta = np.array [0])  d = 0  t = 0  mu = 0  sigma = 1  n = 20  burn = 10  add ""theta_0"" = 1 to theta  set max lag length AR model  set max lag length MA model  simulate n + q error terms  create array for returned values  initialize first time series value  add unit roots  create temp array  ### Configure window for sliding window approach  In[3]: Window width  Number of columns) Stride Start time Desired rows Length of matrix  ### Generate AR 1) process  In[4]:  AR part using 1 lag  MA part using 1 lag  number of simulated values  to get comparable results  simulate time series  In[5]:  ### Generate sine curve  In[6]:  In[7]:  ## First scenario: Only look at sine curve  In[8]:  In[9]:  In[10]:  In[11]:  In[12]:  In[13]:  In[14]:  In[15]:  In[16]:  ## Second scenario: Sine curve mixed with AR 1)-process  In[17]:  In[18]:  In[19]:  In[20]:  In[21]:  In[22]:  In[23]:  In[24]:  In[25]:  ## Third scenario: Pure AR 1)-process  In[26]:  In[27]:  In[28]:  In[29]:  In[30]:  In[31]:  In[32]:  In[33]:  In[34]:  ## Fourth scenario: White Noise  In[35]:  In[36]:  In[37]:  In[38]:  In[39]:  In[40]:",226
LLE_Quantlet,Quantlet data reduction with PCA and LLE,ERROR,Elizaveta Zinovyeva,LLE_Quantlet,2,!/usr/bin/env python  coding: utf-8  In[73]:  import modules json pprint pprint pandas numpy keras.preprocessing.text Tokenizer keras.preprocessing.sequence pad_sequences matplotlib pyplot scipy.cluster.hierarchy dendrogram scipy scipy.cluster.hierarchy fcluster sklearn.decomposition PCA sklearn.neighbors DistanceMetric sklearn.manifold TSNE sklearn.metrics.pairwise cosine_distances sklearn.metrics pairwise_distances os random collections  additional set up  suppress scientific float notation  constants and parameters  load data   In[74]:  create data frame  In[75]:  extract the columns we need    if c % 100 == 0:        print c)  In[76]:  text preprocessing labels = [i.split '/')[1] for i in df.name]  In[77]:  prepare for plotting  In[78]:  In[79]: colors_assignment row  In[80]:  In[81]:  In[95]: blue =   lle[: 1]>0.16)) red =   lle[: 0]>0.02))  In[90]:  In[56]:  prepare for plotting  In[ ]:,103
SVCJ_MC,Simulation of the Euler-discretized Stochastic Volatility with Correlated Jumps model and Monte Carlo Option Pricing,"Euler discretization, Monte Carlo Option Pricing, SVCJ, Darrel Duffie, Cryptocurrencies",Jovanka Lili Matic,SVCJ_MC,2,!/usr/bin/env python  coding: utf-8  In[79]: math numpy pandas scipy numpy scipy matplotlib numpy datetime scipy optimize plt.style.use 'seaborn')  # sets the plotting style  In[80]:  SVCJ parameters  In[81]:  dt  time horizon in days trialrun dt     = 1/10 n      = 1000 m      = int 10* 1/dt)/10)  In[82]:  In[83]:  In[84]:  initial CRIX level  p. 20  In[85]:  In[86]:  In[87]:  In[88]:  Option pricing   For the price process to be a martingale  =risk-neutral)  this should be approximately $S_0$  6500).  In[89]:  Implied volatility  In[90]: callprice S K T sigma r callprice 100 100 1 0.2 0.05)  In[91]: Implied volatility for given strike solve sigma  cp  k  T result.x[0] callprice s0  k  T  result.x[0]  mu)  Normalise everything to $S_0=100$  In[92]:  In[93]: call_and_vol K  T  T maturity in years [call_and_vol 120 0.5)  callprice 100  120  0.5  0.98  r)] call_and_vol 120 0.5)  From here on we do everything on $S_0=100$  In[94]: strike.shape  In[95]:  European call & put option data  3 maturities) h5.close ) type data) data.info ) data.head )  In[102]:  In[ ]: # restrict to ATM options  In[ ]:  In[ ]: compute Black-Scholes calibrated parameter solve_bs sigma scipy.optimize root  In[ ]: mpl_toolkits.mplot3d Axes3D  set up canvas for 3D plotting  creates 3D plot,192
RCVJ_DataOperation,"This quantlet includes mainly the data processing part for the RCVJ_Forecasting project. Read and write data, clean the old CRIX high frequency data and functions to retrieve data from DYOS.","Data Analysis, Data Processing, Cryptocurrency",Junjie Hu,RCVJ_DataOperation,10,dynamica_data_loader.DynamicaDataLoader DynamicaDataLoader Code.tokens_cryptos blackchain_token pandas datetime os form_df coin_data  coin_data = btc_data  df['time'] = [dt.datetime.fromtimestamp float timestamp)  tz=nyc_tz) for timestamp in df['timestamp']]  Combine constitutes,24
RCVJ_DataOperation,"This quantlet includes mainly the data processing part for the RCVJ_Forecasting project. Read and write data, clean the old CRIX high frequency data and functions to retrieve data from DYOS.","Data Analysis, Data Processing, Cryptocurrency",Junjie Hu,RCVJ_DataOperation,10,pandas numpy os matplotlib datetime round_time_values row_values clean_crix   Encode timestamp into New York City time  UTC-5  plt.savefig 'dropped_plot.jpg'  dpi=300)  Round index to 5 min to closest 5-min  Drop duplicated entries and take the first one  Check daily periods larger or equal to 280  hf_ak_dropped_grouped = hf_ak.groupby by=hf_ak.index.date  axis=0).count )  Take only the days with more than 280 periods data  Calculate time difference in each day  see if they are all 5-min frequency  all 5-min frequency,75
RCVJ_DataOperation,"This quantlet includes mainly the data processing part for the RCVJ_Forecasting project. Read and write data, clean the old CRIX high frequency data and functions to retrieve data from DYOS.","Data Analysis, Data Processing, Cryptocurrency",Junjie Hu,RCVJ_DataOperation,10,pandas numpy os datetime matplotlib ======================================================================================,6
RCVJ_DataOperation,"This quantlet includes mainly the data processing part for the RCVJ_Forecasting project. Read and write data, clean the old CRIX high frequency data and functions to retrieve data from DYOS.","Data Analysis, Data Processing, Cryptocurrency",Junjie Hu,RCVJ_DataOperation,10,pandas numpy os datetime matplotlib plot_hf data,7
RCVJ_DataOperation,"This quantlet includes mainly the data processing part for the RCVJ_Forecasting project. Read and write data, clean the old CRIX high frequency data and functions to retrieve data from DYOS.","Data Analysis, Data Processing, Cryptocurrency",Junjie Hu,RCVJ_DataOperation,10,datetime os numpy pandas Code.GlobalParams data_dir read_coin_return coin  freq  refresh  tz_lag  if not backup_data:  logreturn = logreturn_hf.copy deep=True) read_dyos_clean coin  tz_lag=0  Calculate the high frequency return in each day since we don't have  Access the New HF data from Dynamica company  close_price['Time'] = [dt.datetime.fromtimestamp float timestamp)  tz=timezone) for timestamp in close_price['timestamp']]  close_price.set_index keys='Time'  drop=True  inplace=True) lower_freq ts_df  freq='10min' read_clean_gemini coin='gemini_BTC'  freq='5min'  refresh=False  tz_lag=0  coin_full = coin_full.tz_localize 'UTC').tz_convert timezone)  coin_full = coin_full.tz_convert timezone)  Weird data on 2018.8.23  drop them  coin_2018['Unix Timestamp'] = [stamp if len str stamp)) ==10 else int stamp/1000) for stamp in coin_2018['Unix Timestamp']]  coin_full['timestamp'] = [float dt.datetime.timestamp datetime)) for datetime in coin_full.index]  coin_full['Time'] = [dt.datetime.fromtimestamp timestamp  tz=timezone) for timestamp in                       coin_full['Unix Timestamp']]  coin_full.set_index 'Time'  inplace=True)  plt.figure figsize= 16  5))  plt.plot coin_full['Volume'])  accumulate trading volume  volume = volume[volume.index.date >=dt.date 2015 12 31)]  select 5-min samples of close  open  high  low  Concat  Log-return panel_to_multits panel_df  ind_col_name  variable_name read_indices_full_RV   Transform panel data to RV ts  open price ts and close price ts read_indices_trading_rv_bpv ,162
RCVJ_DataOperation,"This quantlet includes mainly the data processing part for the RCVJ_Forecasting project. Read and write data, clean the old CRIX high frequency data and functions to retrieve data from DYOS.","Data Analysis, Data Processing, Cryptocurrency",Junjie Hu,RCVJ_DataOperation,10,"requests time pymongo datetime __init__ self  start_block_hash  save block chain in a list block_visiting self  block_hash append_blocks self block_roller self  Initial the block  start with reading the database and get the latest block's prev block hash  Visit the previous block  Append the previous block and get ready  {self.block_current[""height""]} initialed')  time.sleep 10)  {_height} is finished')  time.sleep 10) write_block self read_block self parse_block self  block_info",63
RCVJ_DataOperation,"This quantlet includes mainly the data processing part for the RCVJ_Forecasting project. Read and write data, clean the old CRIX high frequency data and functions to retrieve data from DYOS.","Data Analysis, Data Processing, Cryptocurrency",Junjie Hu,RCVJ_DataOperation,10,matplotlib Code.GlobalParams pandas,3
RCVJ_DataOperation,"This quantlet includes mainly the data processing part for the RCVJ_Forecasting project. Read and write data, clean the old CRIX high frequency data and functions to retrieve data from DYOS.","Data Analysis, Data Processing, Cryptocurrency",Junjie Hu,RCVJ_DataOperation,10,pandas os numpy datetime parse_datetime x  CLEAN the DATA,9
RCVJ_DataOperation,"This quantlet includes mainly the data processing part for the RCVJ_Forecasting project. Read and write data, clean the old CRIX high frequency data and functions to retrieve data from DYOS.","Data Analysis, Data Processing, Cryptocurrency",Junjie Hu,RCVJ_DataOperation,10,datetime os numpy pandas Code.GlobalParams data_dir read_coin_return coin  freq  refresh  tz_lag  if not backup_data:  logreturn = logreturn_hf.copy deep=True) read_dyos_clean coin  tz_lag=0  Calculate the high frequency return in each day since we don't have  Access the New HF data from Dynamica company  close_price['Time'] = [dt.datetime.fromtimestamp float timestamp)  tz=timezone) for timestamp in close_price['timestamp']]  close_price.set_index keys='Time'  drop=True  inplace=True) lower_freq ts_df  freq='10min' read_clean_gemini coin='gemini_BTC'  freq='5min'  refresh=False  tz_lag=0  coin_full = coin_full.tz_localize 'UTC').tz_convert timezone)  coin_full = coin_full.tz_convert timezone)  Weird data on 2018.8.23  drop them  coin_2018['Unix Timestamp'] = [stamp if len str stamp)) ==10 else int stamp/1000) for stamp in coin_2018['Unix Timestamp']]  coin_full['timestamp'] = [float dt.datetime.timestamp datetime)) for datetime in coin_full.index]  coin_full['Time'] = [dt.datetime.fromtimestamp timestamp  tz=timezone) for timestamp in                       coin_full['Unix Timestamp']]  coin_full.set_index 'Time'  inplace=True)  plt.figure figsize= 16  5))  plt.plot coin_full['Volume'])  accumulate trading volume  volume = volume[volume.index.date >=dt.date 2015 12 31)]  select 5-min samples of close  open  high  low  Concat  Log-return panel_to_multits panel_df  ind_col_name  variable_name read_indices_full_RV   Transform panel data to RV ts  open price ts and close price ts read_indices_trading_rv_bpv ,162
RCVJ_DataOperation,"This quantlet includes mainly the data processing part for the RCVJ_Forecasting project. Read and write data, clean the old CRIX high frequency data and functions to retrieve data from DYOS.","Data Analysis, Data Processing, Cryptocurrency",Junjie Hu,RCVJ_DataOperation,10,pandas  import mysql.connector peewee os,5
DEDA_Class_SS2018_DictionaryForTurkishSentiment,Demonstrating getting data from various websites (web scraping) and analyzing the election rally speeches in Turkey (24.06.2018) by constructing a bridge between Turkish and English through tokenization and translation.,"web scraping, Turkish sentiment, Turkish word frequency, election speeches, speech analysis",Cemre Ünal,DEDA_Class_SS2018_DictionaryForTurkishSentiment,8,"!/usr/bin/env python3  -*- coding: utf-8 -*- bs4 BeautifulSoup requests string nltk.corpus stopwords matplotlib wordcloud WordCloud PIL Image googletrans Translator 1"" Removing the punctuation and making the words lowercase print text.translate translator))  Creating a dictionary for the words in the text Checking the listed stopwords in NLTK package stop_words = stopwords.words 'turkish') Importing the extended stopwords list Resource: https://github.com/ahmetax/trstop/blob/master/dosyalar/derlemtr2016-10000.txt print stopwords) Removing stopwords filtered_words = [i.replace 'i̇'  'i') for i in filtered_words] SequenceSelection dictionary  length  startindex = 0  Visualizing the frequent words in Turkish n = range len dictshow)) plt.bar n  dictshow.values )  align='center') plt.xticks n  dictshow.keys )  rotation = 45) plt.title ""Erdogan Speech Most Frequent Words"") plt.tight_layout ) plt.savefig ""2_RTE FrequentWords.png""  transparent = True  dpi=1000) Translating most frequest words into English googletrans Translator  Removing duplicates - happens due to differences in the language remove_duplicates values  If value has not been encountered yet   ... add it to both list and set.  Visualising the most frequent words in English  Creating the word cloud of filtered words in Turkish numpy os path os  Creating the word cloud of filtered words in english googletrans Translator     print eng_fil_words)    eng_fil_words filtered_WC_eng = filtered_WC.replace 'i̇'  'i')",189
DEDA_Class_SS2018_DictionaryForTurkishSentiment,Demonstrating getting data from various websites (web scraping) and analyzing the election rally speeches in Turkey (24.06.2018) by constructing a bridge between Turkish and English through tokenization and translation.,"web scraping, Turkish sentiment, Turkish word frequency, election speeches, speech analysis",Cemre Ünal,DEDA_Class_SS2018_DictionaryForTurkishSentiment,8,"!/usr/bin/env python3  -*- coding: utf-8 -*- string matplotlib wordcloud WordCloud PIL Image googletrans Translator print text.translate translator)) #CHANGE THE VARIABLE NAME ""agg"" to ""ankara""  ""yalova"" or ""rize"" to get the individual word clouds in slides 29-31  filtered_words = [i.replace 'i̇'  'i') for i in filtered_words] SequenceSelection dictionary  length  startindex = 0 googletrans Translator remove_duplicates values  If value has not been encountered yet   ... add it to both list and set.  Visualising the most frequent words in English numpy os path os  Creating the word cloud of filtered words in english googletrans Translator     print eng_fil_words)    eng_fil_words filtered_WC_eng = filtered_WC.replace 'i̇'  'i')",100
DEDA_Class_SS2018_DictionaryForTurkishSentiment,Demonstrating getting data from various websites (web scraping) and analyzing the election rally speeches in Turkey (24.06.2018) by constructing a bridge between Turkish and English through tokenization and translation.,"web scraping, Turkish sentiment, Turkish word frequency, election speeches, speech analysis",Cemre Ünal,DEDA_Class_SS2018_DictionaryForTurkishSentiment,8,"!/usr/bin/env python3  -*- coding: utf-8 -*- string matplotlib wordcloud WordCloud PIL Image googletrans Translator print text.translate translator)) #CHANGE THE VARIABLE NAME ""agg"" to ""ist""  ""gazi"" etc to get the plots of individual cities filtered_words = [i.replace 'i̇'  'i') for i in filtered_words] SequenceSelection dictionary  length  startindex = 0 googletrans Translator remove_duplicates values  If value has not been encountered yet   ... add it to both list and set.  Visualising the most frequent words in English numpy os path os  Creating the word cloud of filtered words in english googletrans Translator     print eng_fil_words)    eng_fil_words filtered_WC_eng = filtered_WC.replace 'i̇'  'i')",97
DEDA_Class_SS2018_DictionaryForTurkishSentiment,Demonstrating getting data from various websites (web scraping) and analyzing the election rally speeches in Turkey (24.06.2018) by constructing a bridge between Turkish and English through tokenization and translation.,"web scraping, Turkish sentiment, Turkish word frequency, election speeches, speech analysis",Cemre Ünal,DEDA_Class_SS2018_DictionaryForTurkishSentiment,8,"!/usr/bin/env python3  -*- coding: utf-8 -*- bs4 BeautifulSoup requests string matplotlib wordcloud WordCloud PIL Image googletrans Translator 1"" Removing the punctuation and making the words lowercase print text.translate translator))  Creating a dictionary for the words in the text Checking the listed stopwords in NLTK package stop_words = stopwords.words 'turkish') Importing the extended stopwords list Resource: https://github.com/ahmetax/trstop/blob/master/dosyalar/derlemtr2016-10000.txt print stopwords) Removing stopwords filtered_words = [i.replace 'i̇'  'i') for i in filtered_words] SequenceSelection dictionary  length  startindex = 0  Visualizing the frequent words in Turkish Translating most frequest words into English googletrans Translator  Removing duplicates - happens due to differences in the language remove_duplicates values  If value has not been encountered yet   ... add it to both list and set.  Visualising the most frequent words in English  Creating the word cloud of filtered words in Turkish ##### Face of erdogan as a mask IN PROGRESS ###### numpy os path os plt.axis ""off"") plt.title title + "" - "" + date_stamp)  Creating the word cloud of filtered words in english googletrans Translator     print eng_fil_words)    eng_fil_words filtered_WC_eng = filtered_WC.replace 'i̇'  'i')",175
DEDA_Class_SS2018_DictionaryForTurkishSentiment,Demonstrating getting data from various websites (web scraping) and analyzing the election rally speeches in Turkey (24.06.2018) by constructing a bridge between Turkish and English through tokenization and translation.,"web scraping, Turkish sentiment, Turkish word frequency, election speeches, speech analysis",Cemre Ünal,DEDA_Class_SS2018_DictionaryForTurkishSentiment,8,"!/usr/bin/env python3  -*- coding: utf-8 -*- string matplotlib wordcloud WordCloud PIL Image googletrans Translator print text.translate translator)) #CHANGE THE VARIABLE NAME ""agg"" to ""ist""  ""gazi"" etc to get the plots of individual cities filtered_words = [i.replace 'i̇'  'i') for i in filtered_words] SequenceSelection dictionary  length  startindex = 0 googletrans Translator remove_duplicates values  If value has not been encountered yet   ... add it to both list and set.  Visualising the most frequent words in English numpy os path os  Creating the word cloud of filtered words in english googletrans Translator     print eng_fil_words)    eng_fil_words filtered_WC_eng = filtered_WC.replace 'i̇'  'i')",97
DEDA_Class_SS2018_DictionaryForTurkishSentiment,Demonstrating getting data from various websites (web scraping) and analyzing the election rally speeches in Turkey (24.06.2018) by constructing a bridge between Turkish and English through tokenization and translation.,"web scraping, Turkish sentiment, Turkish word frequency, election speeches, speech analysis",Cemre Ünal,DEDA_Class_SS2018_DictionaryForTurkishSentiment,8,"!/usr/bin/env python3  -*- coding: utf-8 -*- pandas bs4 BeautifulSoup requests string nltk.corpus stopwords matplotlib matplotlib.pyplot rcParams wordcloud PIL Image googletrans Translator 1"" Removing the punctuation and making the words lowercase print text.translate translator))  Creating a dictionary for the words in the text Checking the listed stopwords in NLTK package stop_words = stopwords.words 'turkish') Importing the extended stopwords list Resource: https://github.com/ahmetax/trstop/blob/master/dosyalar/derlemtr2016-10000.txt print stopwords) Removing stopwords filtered_words = [i.replace 'i̇'  'i') for i in filtered_words] SequenceSelection dictionary  length  startindex = 0  Visualizing the frequent words in Turkish Translating most frequest words into English googletrans Translator  Removing duplicates - happens due to differences in the language remove_duplicates values  If value has not been encountered yet   ... add it to both list and set.  Visualising the most frequent words in English  Creating the word cloud of filtered words in Turkish ##### Face of erdogan as a mask IN PROGRESS ###### numpy os path os plt.axis ""off"") plt.title title + "" - "" + date_stamp)  Creating the word cloud of filtered words in english googletrans Translator     print eng_fil_words)    eng_fil_words filtered_WC_eng = filtered_WC.replace 'i̇'  'i')",179
DEDA_Class_SS2018_DictionaryForTurkishSentiment,Demonstrating getting data from various websites (web scraping) and analyzing the election rally speeches in Turkey (24.06.2018) by constructing a bridge between Turkish and English through tokenization and translation.,"web scraping, Turkish sentiment, Turkish word frequency, election speeches, speech analysis",Cemre Ünal,DEDA_Class_SS2018_DictionaryForTurkishSentiment,8,"!/usr/bin/env python3  -*- coding: utf-8 -*- pandas bs4 BeautifulSoup requests string nltk.corpus stopwords matplotlib re wordcloud WordCloud PIL Image googletrans Translator  Extracting links print soup.find 'div'  {""id"" : ""divContentList""}).text) print links) ##NOTE### The links in the links list are chronologically ordered. Change the index of the list to assign the speech you'd like to analyse  Accessing speech texts  Extracting Title and date of the speech Translating speech title in English Removing the punctuation and making the words lowercase print text.translate translator))  Creating a dictionary for the words in the text Checking the listed stopwords in NLTK package stop_words = stopwords.words 'turkish') Importing the extended stopwords list Resource: https://github.com/ahmetax/trstop/blob/master/dosyalar/derlemtr2016-10000.txt print stopwords) Removing stopwords SequenceSelection dictionary  length  startindex = 0  Visualizing the frequent words in Turkish Translating most frequest words into English  Visualising the most frequent words in English  Creating the word cloud of filtered words in Turkish ##### Face of erdogan as a mask IN PROGRESS ###### numpy os path os plt.axis ""off"")  Creating the word cloud of filtered words in english     print eng_fil_words)    eng_fil_words filtered_WC_eng = filtered_WC.replace 'i̇'  'i')",179
DEDA_Class_SS2018_DictionaryForTurkishSentiment,Demonstrating getting data from various websites (web scraping) and analyzing the election rally speeches in Turkey (24.06.2018) by constructing a bridge between Turkish and English through tokenization and translation.,"web scraping, Turkish sentiment, Turkish word frequency, election speeches, speech analysis",Cemre Ünal,DEDA_Class_SS2018_DictionaryForTurkishSentiment,8,"!/usr/bin/env python3  -*- coding: utf-8 -*- pandas bs4 BeautifulSoup requests string nltk.corpus stopwords matplotlib wordcloud WordCloud PIL Image googletrans Translator Removing the punctuation and making the words lowercase print text.translate translator))  Creating a dictionary for the words in the text Checking the listed stopwords in NLTK package stop_words = stopwords.words 'turkish') Importing the extended stopwords list Resource: https://github.com/ahmetax/trstop/blob/master/dosyalar/derlemtr2016-10000.txt print stopwords) Removing stopwords filtered_words = [i.replace 'i̇'  'i') for i in filtered_words] SequenceSelection dictionary  length  startindex = 0  Visualizing the frequent words in Turkish Translating most frequest words into English googletrans Translator  Visualising the most frequent words in English  Creating the word cloud of filtered words in Turkish ##### Face of erdogan as a mask IN PROGRESS ###### numpy os path os plt.axis ""off"")  Creating the word cloud of filtered words in english googletrans Translator     print eng_fil_words)    eng_fil_words filtered_WC_eng = filtered_WC.replace 'i̇'  'i')",142
AOBDL_lexicon,Antisocial Online behaivor detection. This part is dedicated to lexicon-based approaches. The list is taken from http://www.bannedwordlist.com/,ERROR,Elizaveta Zinovyeva,AOBDL_lexicon,2,!/usr/bin/env python  coding: utf-8  In[ ]: os numpy pandas gc  In[ ]: 2. Get the file  GOOGLE COLAB SETUP  Load the Drive helper and mount google.colab drive  This will prompt for authorization.  ### TOXIC WIKIPEDIA  In[ ]: 3. Read file as panda dataframe  CHANGE TRAIN AND TEST  MIX TO GET SIMILAR DISTRIBUTION sklearn.model_selection train_test_split  In[41]:  In[ ]:  In[ ]: toxicity text  dict_toxic  In[ ]: tqdm tqdm  In[45]:  In[ ]: sklearn.preprocessing MinMaxScaler  In[ ]: sklearn.metrics roc_auc_score tensorflow.keras backend sklearn.metrics roc_auc_score sklearn.metrics precision_score  In[48]:  ### Facebook  In[ ]: 3. Read file as panda dataframe sklearn.model_selection train_test_split  In[50]: sklearn.preprocessing MinMaxScaler  In[58]:  ### Twitter  In[ ]: 3. Read file as panda dataframe  In[ ]:  In[ ]: sklearn.model_selection train_test_split  In[100]: sklearn.preprocessing MinMaxScaler  In[101]:  ### Formspring  In[ ]:  In[ ]:  In[ ]: sklearn.model_selection train_test_split  In[87]: sklearn.preprocessing MinMaxScaler  In[96]:  In[ ]:,134
NewsNetwork,Construct news network out of the identified tickers of S&P500 stocks,"Textual Analysis, Network Analysis",Junjie Hu,NewsNetwork_Formation,1,pandas networkx datetime itertools matplotlib pickle os ast re numpy math concurrent time calendar count_weighted_edges edges_list  Count edges frequency adj_row_normalize adj_df networks_construction_periodic g_spx_w  nodes_list  dir_label  start  end  Weighted Undirected Graph  Keep only the firms that are in the current spx constituents  Weighted Directed Graph  Keep only the firms that are in the current spx constituents OneTypeNetworkConstruction network_fullsample  dir_label  full_nodes  specify the years in the sample  Full Sample Network  Round the time frame. Count the articles before 9am on day t+1 to t.  Equivalently  shifting the time frame 9-hours back,89
preprocessing_text_abstract,"Preprocessing data which is text data. it contains case folding, tokenizing, lemmatizer, and stemming.",ERROR,"Muhaimin, Rasyid, Aziz, Maya, 07-12-2020",preprocessing_text_abstract,1, -*- coding: utf-8 -*-  Install Library pandas numpy io nltk gensim gensim corpora nltk.corpus stopwords nltk.tokenize word_tokenize nltk.stem PorterStemmer nltk.stem LancasterStemmer nltk.stem WordNetLemmatizer nltk pos_tag pandas DataFrame from gensim.models import HdpModel from gensim.models.coherencemodel import CoherenceModel string re  Import data  Import stopwords  Preprocess function preprocess_text text  stopwords  additional_word  Case folding  Lemmatizer  Stemming  Tokenizing  Run preprocess,54
SFEkurgarch,"Computes and plots the kurtosis function of a GARCH(1,1) (generalised autoregressive conditional heteroscedasticity) process for different parameters.",ERROR,ERROR,QID-3165-SFEkurgarch,1,numpy pandas matplotlib mpl_toolkits mpl_toolkits mplot3d  number of grid points  in the book = 31)  computing grid  kurtosis  formula from the book SFE: Fourth moment of a GARCH 1 1) process kurtosis a b,34
pyTSA_GDPChinaDiff,This Quantlet produces and plot time series of seasonally and firstly differenced Chinese quarterly GDP and its ACF for the period from 1992 to 2017.,"time series, seasonality, stationarity, autocorrelation, KPSS, ACF",ERROR,pyTSA_GDPChinaDiff,1,pandas matplotlib PythonTsa.plot_acf_pacf acf_pacf_fig statsmodels.tsa.stattools kpss  seasonal differencing  ms means marker size InterpolationWarning: p-value is greater than the indicated p-value  0.20253789040706957  0.1  12   {'10%': 0.347  '5%': 0.463  '2.5%': 0.574  '1%': 0.739}),31
BitcoinPricingKernels,"Bitcoin Pricing Kernels are inferred using a novel data set from Deribit, one of the largest Bitcoin derivatives exchanges. This enables arbitrage-free pricing of various instruments. State Price Densities are estimated nonparametrically. The underlying asset process is viewed through the lens of a Stochastic Volatility with Correlated Jumps (SVCJ) framework. Shape invariant pricing kernels are reported. Market inefficiencies are assessed based on the shape of the pricing kernels. A trading strategy that exploits these inefficiencies is evaluated.","Bitcoin, Pricing Kernel, Bitcoin Option, Option, Derivative, Pricing, State Price Density, Rookley",Julian Winkel,BitcoinPricingKernels,15,sys __init__ self  *files write self  obj  If you want the output to be visible immediately flush self,18
BitcoinPricingKernels,"Bitcoin Pricing Kernels are inferred using a novel data set from Deribit, one of the largest Bitcoin derivatives exchanges. This enables arbitrage-free pricing of various instruments. State Price Densities are estimated nonparametrically. The underlying asset process is viewed through the lens of a Stochastic Volatility with Correlated Jumps (SVCJ) framework. Shape invariant pricing kernels are reported. Market inefficiencies are assessed based on the shape of the pricing kernels. A trading strategy that exploits these inefficiencies is evaluated.","Bitcoin, Pricing Kernel, Bitcoin Option, Option, Derivative, Pricing, State Price Density, Rookley",Julian Winkel,BitcoinPricingKernels,15,numpy matplotlib pandas rpy2 rpy2.robjects.packages importr  https://rdrr.io/cran/kedd/man/h.ccv.html estimate_bandwidth self  tau_day  rets estimate_density self __init__ self gausskernel self  u newton_raphson_f self  h  n newton_raphson_f_deriv self  h  n estimate_bandwidth self  n  error = 0.01  Guess initial value: 0.01  0.01 # try to fulfill second condition  Mean Value Theorem: Check if zero exists  First condition  Second Condition  Confirm proposed h  use indicator for florens-zmirou florens_zmirou self  @Todo: check kernel input  dimensions should be false! h = hd.estimate_bandwidth 10000),75
BitcoinPricingKernels,"Bitcoin Pricing Kernels are inferred using a novel data set from Deribit, one of the largest Bitcoin derivatives exchanges. This enables arbitrage-free pricing of various instruments. State Price Densities are estimated nonparametrically. The underlying asset process is viewed through the lens of a Stochastic Volatility with Correlated Jumps (SVCJ) framework. Shape invariant pricing kernels are reported. Market inefficiencies are assessed based on the shape of the pricing kernels. A trading strategy that exploits these inefficiencies is evaluated.","Bitcoin, Pricing Kernel, Bitcoin Option, Option, Derivative, Pricing, State Price Density, Rookley",Julian Winkel,BitcoinPricingKernels,15,sshtunnel pymongo pprint datetime pandas os  For Deribit asyncio websockets json  Avg IV per Day bson.json_util dumps  dump Output  Todo: connection doesnt stop atm __init__ self self.server_started = self._start ) if self.server_started: self._mean_iv do_sample = False  write_to_file = False) _server self _start self self.server = self._server ) self.server.start ) _stop self self.server.stop ) _filter_by_timestamp self  starttime  endtime _generate_stats self  Get first and last element: synth_btc self  do_sample  write_to_file  Save Output as JSON  Dump each element in a dict and save as JSON _mean_iv self  do_sample = False  write_to_file = False  Try to Subset / WHERE Statement print list avg_iv_per_day))  Save Output as JSON  Dump each element in a dict and save as JSON  Deribit create_msg self  _tshigh  _tslow  retrieves constant interest rate for time frame call_api self  do something with the response... _run self  starttime  endtime  download_interest_rates  download_historical_iv server_started = self._start ) .sort 'timestamp') print doc)  Got to change this to subtr. d = pd.DataFrame dat) d.to_csv 'data/orderbooks_test.csv'),158
BitcoinPricingKernels,"Bitcoin Pricing Kernels are inferred using a novel data set from Deribit, one of the largest Bitcoin derivatives exchanges. This enables arbitrage-free pricing of various instruments. State Price Densities are estimated nonparametrically. The underlying asset process is viewed through the lens of a Stochastic Volatility with Correlated Jumps (SVCJ) framework. Shape invariant pricing kernels are reported. Market inefficiencies are assessed based on the shape of the pricing kernels. A trading strategy that exploits these inefficiencies is evaluated.","Bitcoin, Pricing Kernel, Bitcoin Option, Option, Derivative, Pricing, State Price Density, Rookley",Julian Winkel,BitcoinPricingKernels,15, Set US locale for datetime.datetime.strptime  Conflict with MAER/MAR) platform locale pandas datetime statsmodels matplotlib numpy time sys pdb pickle gc csv os statsmodels.nonparametric.kernel_regression KernelReg scipy stats scipy.integrate simps scipy interpolate mpl_toolkits.mplot3d Axes3D pathlib Path src.brc BRC src.tee Tee src.strategies IronCondor rpy2 robjects src.helpers save_dict rpy2  Can only read profit using pickle!! gausskernel x create_kernelmatrix regressors decompose_instrument_name _instrument_names  tradedate  round_tau_digits = 4  call == 1  put == 0 in is_call  Calculate Tau; being time to maturity Error here: time data '27MAR20' does not match format '%d%b%y'  Funny Error: datetime does recognize MAR with German AE instead of A  always 8 o clock list map lambda x: x.dt.date  tradedate))#tradedate.dt.date # Round to date  else the taus are all unique and the rounding creates different looking maturities list map lambda x: x - reference_date  maturitydate)) Tdiff/365 #list map lambda x: x.days/365  Tdiff)) # else: Tdiff/365  Strike must be float  Add maturitydate for trading simulation create_regressors _newm  _tau  m_0  t_0  SET m_0 and t_0 as index variables for a loop!  Create matrix of regressors m_0 = min _newm) t_0 = min _tau)  as on page 181 trisurf x  y  z  xlab  ylab  zlab  filename  blockplots  This is the data we have: vola ~ tau + moneyness  https://stackoverflow.com/questions/9170838/surface-plots-in-matplotlib x  y = np.meshgrid x  y) plt.show block = blockplots) plot_volasmile m2  sigma  sigma1  sigma2  mindate  plot_ident  blockplots  Shrink current axis by 20%   Put a legend to the right of the current axis plt.show block = blockplots) plot_spd spd  mindate  tau  plot_ident plt.show block = blockplots) gaussian_kernel M  m  h_m  T  t  h_t epanechnikov M  m  h_m  T  t  h_t k X  d epanechnikov2 M  m  h_m  T  t  h_t new_epanechnikov X smoothing_rookley df  m  t  h_m  h_t  kernel=gaussian_kernel  extend = False  M = np.array df.M)  Before  After Poly extension np.array df.tau)  the kernel lays on M_j - m  T_j - t ker = new_epanechnikov X[: 5]) test = gausskernel X[: 5])  Compare Kernels  This kernel gives too much weight on far-away deviations plt.scatter M  ker  color = 'green') plt.scatter M  X[: 5]  color = 'red') plt.vlines m  ymin = 0  ymax = 1) plt.show )  This is our estimated vs real iv  iv_est = np.dot X  beta) plt.scatter df.moneyness  df.mark_iv  color = 'red') plt.scatter df.moneyness  iv_est  color = 'green') plt.vlines m  ymin = 0  ymax = 1) plt.title 'est vs real iv and current location') plt.show ) extend_polynomial x  y plt.plot xnew ynew xnew) x y 'o') plt.title 'interpolated smile') plt.show ) rookley_unique_tau df  h_m  h_t=0.01  gridsize=149  kernel='epak'  gridsize is len of estimated smile tau = df.tau.iloc[0]  if all taus are the same  empty sequence for tau precision = 3  fill  TODO: speed up with tensor instead of loop / np.std df.moneyness) / np.std df.moneyness) plt.plot df.moneyness  df.mark_iv  'ro'  ms=3  alpha=0.3  color = 'green') plt.title 'moneyness vs iv: base for interpolation') plt.show ) plt.plot M_std  smile  'ro'  ms=3  alpha=0.3  color = 'red') plt.plot M_std  first) plt.plot M_std  second) plt.title 'no interpolation') plt.show ) plt.plot M_std  smile) M_std  smile = extend_polynomial M_std  smile) M_std  first = extend_polynomial M_std  first) M_std  second = extend_polynomial M_std  second) rookley df  h_m  h_t=0.01  gridsize=149  kernel='epak'  gridsize is len of estimated smile tau = df.tau.iloc[0]  if all taus are the same  fill  TODO: speed up with tensor instead of loop / np.std df.moneyness) / np.std df.moneyness) plt.plot df.moneyness  df.mark_iv  'ro'  ms=3  alpha=0.3  color = 'green') plt.plot df.moneyness  smile  'ro'  ms=3  alpha=0.3  color = 'red') plt.plot df.moneyness  first) plt.plot df.moneyness  second) plt.show ) compute_spd sigma  sigma1  sigma2  tau  s  m2  r_const  mindate  plot_ident  SPDBL  Scalars tau = np.mean tau) s = np.mean s) r = np.mean r) r = 0  now start spdbl estimation  error should be here in the length of m  First derivative of d1  First derivative of d2 term  Second derivative of d1 term  Second derivative of d2 term  f2 = dnorm d1  mean = 0  sd = 1) * d12 - d1 * dnorm d1  mean = 0  sd = 1) *  d11^2) -  1/ ert * m) * dnorm d2  mean = 0  sd = 1) * d22) +   dnorm d2  mean = 0  sd = 1) * d21)/ ert * m^2)) +  1/ ert * m) * d2 * dnorm d2  mean = 0  sd = 1) *  d21^2)) -  2 * pnorm d2  mean = 0  sd = 1)/ ert *  m^3))) +  1/ ert *  m^2)) * dnorm d2  mean = 0  sd = 1) * d21)  recover strike price  Calculate the quantities of interest print '\ndelta: '  delta  '\ngamma:'  gamma) plt.plot fstar) plt.show )  Moneyness plot_spd spd  mindate  tau  plot_ident) spdbl _df  mindate  maxdate  tau  r_const  blockplots  Subset  Only calls  tau in [0  0.25] and fix one day  bc looking at intra day here)  &  _df['maturity'] == mat)]# _df['tau'] > 0) &  _df['tau'] < 0.03)]  @Todo: tau should always be > 0  else check! sub['tau'] = round sub['tau']  2) print 'Only unique transactions!')  Isolate vars m = float sub['index_price']/sub['strike'] # Spot price corrected for div divided by k   s-d)/k); moneyness of an option; div always 0 here  Forward price  k capital rookley sub  h)   Empirical Vola Smile  @Todo: Conflict with new tau being just zeros trisurf newm  tau  vola  'moneyness'  'tau'  'vola'  'pricingkernel/plots/empirical_vola_smile_' + plot_ident  blockplots)  Projected Vola Smile  tau is too long here!! classify_options dat verify_density s  Check if integral is 1 option_pnl underlying_at_maturity  strikes  premium_per_strike  call = True  long = True hist_iv df o = sub.groupby ['maturitydate_char'  'instrument_name'  'date']).mean )['mark_iv'] plot_atm_iv_term_structure df mindate  Set names  Rescale   nimm einfach mal n tau ueber das man die iv plotten kann plt.axis )  Shrink current axis by 20%   Put a legend to the right of the current axis plt.show block = False) real_vola df iv_vs_real_vola realized_vola  historical_iv  Redirect Output to log file  Collect Trading Summary  Initiate R Objects a  b = r.gen_physical_density 10  0.01  10000)  Read final prices pd.read_csv 'data/BTCUSDT.csv')  ascending  Initiate BRC instance to query data. First and Last day are stored. datetime.datetime 2020  3  21  0  0  0)#brc.first_day #datetime.datetime 2020  9  6  0  0  0)#  Simulation Methods  ['brownian'  'svcj']  Tau-maturitydate combination dict  make sure days are properly set  Debug curr_day_starttime = datetime.datetime 2020  4  5  0  0  0) curr_day_endtime = datetime.datetime 2020  4  5  23  59  59)  Convert dates  utc  assumption here!  To check Results after trading   Calculate mean instrument price  Prepare for moneyness domain restriction  0.8 < m < 1.2)  Select Tau and Maturity  Tau is rounded  prevent mix up!)  Save Tau-Maturitydate combination tau_maturitydate[curr_day.strftime '%Y-%m-%d')] =  unique_taus )  As taus are ascending  once we do not find one instrument for a specific taus it is unlikely to find one for the following  as the SPDs degenerate with higher taus.  Only continue if spd fulfills conditions of a density  @Todo: Check if startprice is ok!!  last one on day which we observed  need at least one day for the physical density  which is fixed in there!  Todo: Compare svcj results to old hd results  append if already exists  make a new file if not Pickling Pickling,1153
BitcoinPricingKernels,"Bitcoin Pricing Kernels are inferred using a novel data set from Deribit, one of the largest Bitcoin derivatives exchanges. This enables arbitrage-free pricing of various instruments. State Price Densities are estimated nonparametrically. The underlying asset process is viewed through the lens of a Stochastic Volatility with Correlated Jumps (SVCJ) framework. Shape invariant pricing kernels are reported. Market inefficiencies are assessed based on the shape of the pricing kernels. A trading strategy that exploits these inefficiencies is evaluated.","Bitcoin, Pricing Kernel, Bitcoin Option, Option, Derivative, Pricing, State Price Density, Rookley",Julian Winkel,BitcoinPricingKernels,15,pandas matplotlib numpy pdb src.helpers save_dict __init__ self  s  gdom  gstar  tau  date  dat  tdat  time_to_maturity_in_days  simmethod  tradedirection = 'long'  make this as long as the prices!  If this occurs  then probably tdat is not updated to the extent of our orderbooks pdb.set_trace )  need this for dict key  Alternatively: choose self.dat.settlement_price * self.dat.index_price plot_densities_vs_strategy self  m  Sort by x to prevent overlapping lines m = m.sort_values 'x')  Build Plot  Shrink current axis by 20%   Put a legend to the right of the current axis plot_densities_vs_payoff self  m  moneyness_lim = [0.5  1.5]  Payoff Function for Plot self.moneyness_per_state.append self.settlement_price/state)  For Payoff Plot settlement_moneyness = self.settlement['Adj.Close'].iloc[-1] / self.settlement_states m_min = max min settlement_moneyness)  0.5) m_max = min max settlement_moneyness)  1.5) self.settlement_moneyness = np.arange m_min  m_max  len self.payoff_per_state))  set order in order to prevent wiggly lines  Build Plot plt.xlim = moneyness_lim  Make sure we get no spelling error  days / day)  Shrink current axis by 20%   Put a legend to the right of the current axis build_strategy self  m m.spdy[m.spdy != 0].quantile 0.005)  IST DER STRIKE FUER DIE PUTS HOEHER ALS DER CALL??  Long Call  Short Call  Long Put  Short Put find_instruments_in_range self  df  strategy_range  posi .head self.allowed_position_size) spread_payoff self  settlement_price  call spread and put spread  dont use self.settlement_price as default because we wanna have a complete payoff function for different settlement states print direction)  Reverse long and short instruments run self  Based on the Density Comparison  create a trading rule  Also: Find regions to long/short the instruments  Long Call where SPD < HD and right side of center  Short Call where SPD > HD and right side of center  Long Put where SPD < HD and left side of center  Short Put where SPD > HD and left side of center  Combine in order to compare which density is larger at which points  Prevent fuzzy graphs  Getting a bunch of NAs from the merge  gotta replace with zeros  So that its usable in global_plots.py  Fix the probbos where they arise pdb.set_trace )   &  self.dat.days_to_maturity == self.time_to_maturity_in_days)  Construct payoff function  Select instruments and Buy for the mean on a particular day.  Pf log  Actual Payoff  Density Comparison and Strategy,356
BitcoinPricingKernels,"Bitcoin Pricing Kernels are inferred using a novel data set from Deribit, one of the largest Bitcoin derivatives exchanges. This enables arbitrage-free pricing of various instruments. State Price Densities are estimated nonparametrically. The underlying asset process is viewed through the lens of a Stochastic Volatility with Correlated Jumps (SVCJ) framework. Shape invariant pricing kernels are reported. Market inefficiencies are assessed based on the shape of the pricing kernels. A trading strategy that exploits these inefficiencies is evaluated.","Bitcoin, Pricing Kernel, Bitcoin Option, Option, Derivative, Pricing, State Price Density, Rookley",Julian Winkel,BitcoinPricingKernels,15,with open 'mycsvfile.csv'  'wb') as f:  # You will need 'wb' mode in Python 2.x     w = csv.DictWriter f  my_dict.keys ))     w.writeheader )     w.writerow my_dict) csv  keynames in w.fieldnames  You will need 'wb' mode in Python 2.x save_dict moneyness  gdom  gstar  spd  curr_day  tau  maturitydate_char  fname load_dict fname save_a a  This worked below!  Save pickle  load pickle  This is the fucking right imported one!!,64
BitcoinPricingKernels,"Bitcoin Pricing Kernels are inferred using a novel data set from Deribit, one of the largest Bitcoin derivatives exchanges. This enables arbitrage-free pricing of various instruments. State Price Densities are estimated nonparametrically. The underlying asset process is viewed through the lens of a Stochastic Volatility with Correlated Jumps (SVCJ) framework. Shape invariant pricing kernels are reported. Market inefficiencies are assessed based on the shape of the pricing kernels. A trading strategy that exploits these inefficiencies is evaluated.","Bitcoin, Pricing Kernel, Bitcoin Option, Option, Derivative, Pricing, State Price Density, Rookley",Julian Winkel,BitcoinPricingKernels,15,pandas pickle matplotlib '),4
BitcoinPricingKernels,"Bitcoin Pricing Kernels are inferred using a novel data set from Deribit, one of the largest Bitcoin derivatives exchanges. This enables arbitrage-free pricing of various instruments. State Price Densities are estimated nonparametrically. The underlying asset process is viewed through the lens of a Stochastic Volatility with Correlated Jumps (SVCJ) framework. Shape invariant pricing kernels are reported. Market inefficiencies are assessed based on the shape of the pricing kernels. A trading strategy that exploits these inefficiencies is evaluated.","Bitcoin, Pricing Kernel, Bitcoin Option, Option, Derivative, Pricing, State Price Density, Rookley",Julian Winkel,BitcoinPricingKernels,15, Calculate IV vs Real Vola and visualize matplotlib matplotlib json collections datetime numpy pandas pymongo.uri_parser _TLSINSECURE_EXCLUDE_OPTS brc BRC compute_vola x  Annualizing daily SD should be ok aswell  trading days in rolling windoww 252  # trading days per year  Var Swap  returns are not demeaned)  Classical  returns are demeaned  dof=1) df['real_var'] = df['log_rtn'].rolling window).var ) * ann_factor df['real_vol'] = np.sqrt df['real_var'])  Execute  Order dict by keys and then plot IV  Vola over Time  @Todo: Filter for volume  Dump each element in a dict and save as JSON dates.append currkey) * 100  Display only Monthly Dates  every month  Specify the format - %b gives us Jan  Feb... plt.ylabel 'IV')  Shrink current axis by 20%   Put a legend to the right of the current axis  Only Months  Specify formatter,126
BitcoinPricingKernels,"Bitcoin Pricing Kernels are inferred using a novel data set from Deribit, one of the largest Bitcoin derivatives exchanges. This enables arbitrage-free pricing of various instruments. State Price Densities are estimated nonparametrically. The underlying asset process is viewed through the lens of a Stochastic Volatility with Correlated Jumps (SVCJ) framework. Shape invariant pricing kernels are reported. Market inefficiencies are assessed based on the shape of the pricing kernels. A trading strategy that exploits these inefficiencies is evaluated.","Bitcoin, Pricing Kernel, Bitcoin Option, Option, Derivative, Pricing, State Price Density, Rookley",Julian Winkel,BitcoinPricingKernels,15, Reconstruct trading from nohup.out csv re matplotlib scipy.stats gaussian_kde numpy extract_performance line  Collect info and subrtract delivery fee for options  0.015% - this fee can never be higher than 12.5% of the option's value. print line)  Reconstruct overall payoff from string  ['overall payoff:  99.11172356292109'] --> taking right part  overall payoff fees = delivery_fee *  abs call_spread_payoff) + abs put_spread_payoff))  @todo: how to get relative returns?  Reconstruct date and tau of instrument  First date then tau so we can order according to date  real performance only  complete picture  PNL Distribution plt.show ) plt.show )  Risk free rate = 0  You will need 'wb' mode in Python 2.x,106
BitcoinPricingKernels,"Bitcoin Pricing Kernels are inferred using a novel data set from Deribit, one of the largest Bitcoin derivatives exchanges. This enables arbitrage-free pricing of various instruments. State Price Densities are estimated nonparametrically. The underlying asset process is viewed through the lens of a Stochastic Volatility with Correlated Jumps (SVCJ) framework. Shape invariant pricing kernels are reported. Market inefficiencies are assessed based on the shape of the pricing kernels. A trading strategy that exploits these inefficiencies is evaluated.","Bitcoin, Pricing Kernel, Bitcoin Option, Option, Derivative, Pricing, State Price Density, Rookley",Julian Winkel,BitcoinPricingKernels,15,numpy scipy.stats invwishart scipy.stats beta math floor pdb pandas datetime matplotlib numpy scipy  https://github.com/QuantLet/SVCJ/tree/master/ __init__ self  prices  n  N  Prior distribution of hyperparameter values  prior for mu   nrow = 2  ncol = 1) # prior for alpha and beta diag 2) # prior for alpha and beta  prior for sigma2_v  prior for sigma2_v  prior for mu_v  prior for mu_v  prior for mu_y  prior for sigma2_y  prior for sigma2_y  prior for lambda  prior for lambda vector ) annualized_returns self  prices test_rho_acceptance self  rho  p  i  rhoprop  acceptsumrho  rhosum  rho2sum rho = 0 # init print 'trying to update rhosum') print 'rhoprop: '  rhoprop) print 'p: '  p) print 'pot_x vs u: '  pot_x  u)  += bug with ndarray test_simple_acceptance self  i  mV  mVsum  mV2sum propose_rho self  rho  rhoprop  Y  Z  m  J  X  s2V  alpha  V0tmp  V  beta  XV  T ** T  Check if p5 really should be the same as p3 pdb.set_trace ) update_XV self  J  Jindex  V  V0  alpha  beta  Y  Z  m  mJ  mV  X  rho  s2V  rhoJ  s2J  XV  @Todo check index  so truncnorm should only be able to deliver positive results  but apparently it must be overwritten by loc  pdb.set_trace )  @Todo see comment above  result must be nonnegative but is overwritten  https://stackoverflow.com/questions/53125437/fitting-data-using-scipy-truncnorm print 'overwriting XV[t] due to negativity') update_X self  Jindex  X  V  alpha  beta  Y  Z  m  mJ XV  rho  rhoJ  s2J  s2V for t in Jindex:  @Todo check if this t index is correctly iterated over print 'current t: '  t) pdb.set_trace ) draw_v self  i  dfV  V  Y  Vsum2  stdevV  mean of t dist with parameter dfV  Variance of t dist  test_j self  Y  Z  m  X  V  V0tmp  alpha  beta  XV  lda  rho  s2V fit self  prices  N self.ones = np.ones len self.annualized_returns))  Starting values  m=mu  kappa=-alpha/beta  the theta in equation 1  alpha in equation 3  beta in eq.3  sigma_v in eq.3  the relation between w1 ad w2 0  0  mu_v  the param in expoential distr. of Z_v   jump size in variance)  mu_y  the mean of jump size in price Z_y  sigma_Y  the variance of jump size in price Z_y   rho param in the jump size of price  jump intensity  Initial values for variance_t  J = data 2:end 3);  the jump size in volatility  Z_t^y rexp length Y)  rate = 1/mV)  the jump size in price rnorm n = length Y)  mean =  mJ+XV*rhoJ)  sd = s2J**0.5) rep 0 len V)) rep 1  len Y))  Param Matrix  Draw m i+1)  Expected Return  Expected Variance pdb.set_trace )   test = np.matrix [1/V0tmp**0.5  V0tmp**0.5])  np.dot test np.transpose test)) np.array 1/V0tmp**0.5  V0tmp**0.5) np.invert self.B)  Rho must be Rho[0] if it is ndarray or shapes are going to misalign  s2V  Check out how almost the same is being done for rho. Put this in a function  @todo: check if p and q are supposed to be the same here  Rho  Draw a candidate for rho  i + 1)  draw rhoc from a t distribution with 8 df and std of 0.26666 print 'updating rho: '  rho) print 'rhosum: '  rhosum) if isinstance rho  np.ndarray):     print 'here')     rho = rho[0]  mV  Ds becomes negative  then invwishart fails print 'ds  Ds: '  ds  Ds) if sum XV) < 0):     print 'sum XV): '  sum XV))     pdb.set_trace )  mJ  s2Y  rhoJ  lambda  J print 'p: '  p) print 'compare_p: '  compare_p) pdb.set_trace )  print 'watching p: '  p) @Todo aaaaha X is none because J is fucked because p is just a list of nan... u < p print 'XV: '  XV) if XV < 0:     print 'xv < 0')     pdb.set_trace ) print 'x is none')  @Todo check len of X to be replaced here pdb.set_trace )  Draw V  p1  p2...  Found Error: X is None  because in update_X Jindex is an empty array  @Todo: check if p2 is integer pdb.set_trace ) print 'j: '  j)  Collect Results  @Todo: Check parameter evolution in test print test[i ])  @ Todo: Probbo in _sd: mJ2sum is a negative number to the power of 0.5 --> NAN  Summary  Plot Parameter Evolution np.shape test)[1] - 1  skip the zeros  Activate and Move Legend   Put a legend to the right of the current axis extract_param self  params  paramname simulate self  params  s0  startdate   prices  parameters  ndays  N  startvalue  SVCJ parameters params['mean'][ params['params'] == 'mu')][0] #0.042 -0.0492 2.061  0.0515 0.0102 -0.188 0.275 0.007 -0.210 0.709 0.19**2   dt  time horizon in days s0     = 6500  initial CRIX level  p. 20 plt.show ) plt.show ) p = np.random.normal size = 10000) print p)  pd.DataFrame p).to_csv 'random_prices_svcj.csv') params = pd.read_csv 'svcjparams.csv')  Startdate for algo  First date we accept in the BTCUSDT data prices_raw = pd.read_csv 'data/BTC_USD_Quandl.csv') prices_raw['date'] = prices_raw['Timestamp'].apply lambda x: datetime.datetime.fromtimestamp x/1000))    price_raw['date'] = prices_raw['Date'] p = prices_raw['Adj.Close'][prices_raw['date'] <= startdate]  Reverse prices... print 'prices: '  len p)) p = p[2001:] p = pr.reindex index=pr.index[::-1])  Calculate Vola per day in order to get V0 params  prices  vola = s.fit prices = p  N = _N) pdb.set_trace ),819
BitcoinPricingKernels,"Bitcoin Pricing Kernels are inferred using a novel data set from Deribit, one of the largest Bitcoin derivatives exchanges. This enables arbitrage-free pricing of various instruments. State Price Densities are estimated nonparametrically. The underlying asset process is viewed through the lens of a Stochastic Volatility with Correlated Jumps (SVCJ) framework. Shape invariant pricing kernels are reported. Market inefficiencies are assessed based on the shape of the pricing kernels. A trading strategy that exploits these inefficiencies is evaluated.","Bitcoin, Pricing Kernel, Bitcoin Option, Option, Derivative, Pricing, State Price Density, Rookley",Julian Winkel,BitcoinPricingKernels,15,math tau scipy optimize  Optimize Thetas pandas numpy helpers load_estimated_densities functools reduce  To merge list of data frames matplotlib  R rpy2 robjects rpy2 update_parameters self  Update each of the four parameters  Integrate the following expression ... K_t self  theta_vary initialize_K0 self  linearly moved u k_out = [] for i in range len u)):     k_out.append self.PK[0] u_lin)) return sum k_out) *  1/T) __init__ self  taurange load_estimated_densities allowed_taurange=[0.0163  0.0165])#load_estimated_densities allowed_taurange=[0.018  0.02])  Discrete Time Steps List of Pricing Kernels for each Point in Time self.d = load_dict 'out/estimated_densities.csv')  Parameter Initialization  Vector over length of T  Theta1 and Theta2 are initially 1  the others are 0 as in  Haerdle and Marron 1990  @ Todo: Filter input for similar taus! _normalize_parameters self Normalization: Mean of all Parameters Theta_ti must be 1 _w self  u  a  b  i  theta2  theta3 out = [] for _u in u[i]:#range len u)):  Indicator Function out.append prel)  Append one so that the product does not vanish out.append 1)  Take product of all constituents p = 1 for i in range len out)):     p = p * i return p initialize_K0 self  theta2  theta3  Shift the domain  For the initialization  this is 1 * u + 0  Smoothen up  How to use h.select from r?  Assign Y hat sm_reg['estimate']  Mean Kt Curve is initial Reference Curve K0  names are still missing  and still got to calculate the mean!  Rowwise Mean  @Todo exclude irrelevant columns for the mean calculation!! lt.plot df['M']  df['mean']) plt.show ) mse self  thetas  K0  i  Unpack to-be-optimized argument  Ensure restrictions theta2 = theta2 theta4 = theta4 sm.t =  t.reference - theta2)/theta3  # time adjustment  Get The Difference to K0** r-1)  @Todo check if mean is correct here  Mean Squared Error du = df['M'].diff )# impute NAs! #np.diff ).tolist ) * w #* du #  du) missing here optimize_theta self  thetas  K0  i  Minimize MSE with respect to Thetas  initial guess    theta1  theta2  theta3  theta4; initial guess proabbaly excluded from rest of the arguments  Before vs afterwards          2D Stuff: Use to show different SPDs/EPKs on a single day pricingkernel_plot self pdb.set_trace ) currdate = date[i] currtau = self.tau[i] currmaturity = maturitydate[i]  Shrink current axis by 20%   Put a legend to the right of the current axis plt.show ) ara self  K  first observation is lost due to diff function _run self  If there are only two curves  just subtract the Kernel from the interpolation  Output Collection  Initial K0 must be looped over all PKs and M    basically Mean Kernel  Optimize Parameters  Collect Parameters  Normalize Parameters theta3/sum theta3) theta4/sum theta4)#  Collect Output  Restart  Update K0** j-1)  like initialize_K0  but with updated thetas  Absolute Risk Aversion self.ara K0_tracker[-1])  Check K0tracker  Plot Convergence of PKs lab = str 'Tau: ') + str np.unique self.tau)[0])  Shrink current axis by 20%   Put a legend to the right of the current axis,467
BitcoinPricingKernels,"Bitcoin Pricing Kernels are inferred using a novel data set from Deribit, one of the largest Bitcoin derivatives exchanges. This enables arbitrage-free pricing of various instruments. State Price Densities are estimated nonparametrically. The underlying asset process is viewed through the lens of a Stochastic Volatility with Correlated Jumps (SVCJ) framework. Shape invariant pricing kernels are reported. Market inefficiencies are assessed based on the shape of the pricing kernels. A trading strategy that exploits these inefficiencies is evaluated.","Bitcoin, Pricing Kernel, Bitcoin Option, Option, Derivative, Pricing, State Price Density, Rookley",Julian Winkel,BitcoinPricingKernels,15, Merton Jump Diffusion Model  1976  Yves Hilpisch - Python for Finance p. 285 ff.  Stochastic Differential Equation on page 285  Euler Discretization Scheme page 286 numpy matplotlib __init__ self _simulate self  We need three sets of independent random numbers in order to   simulate the jump diffusion  Input: tdat  r  startvalue  days  sigma)  Maturity # Default: 50  Number of Paths # Default: 10000 _run self  plot = True  Histogram  Paths,69
BitcoinPricingKernels,"Bitcoin Pricing Kernels are inferred using a novel data set from Deribit, one of the largest Bitcoin derivatives exchanges. This enables arbitrage-free pricing of various instruments. State Price Densities are estimated nonparametrically. The underlying asset process is viewed through the lens of a Stochastic Volatility with Correlated Jumps (SVCJ) framework. Shape invariant pricing kernels are reported. Market inefficiencies are assessed based on the shape of the pricing kernels. A trading strategy that exploits these inefficiencies is evaluated.","Bitcoin, Pricing Kernel, Bitcoin Option, Option, Derivative, Pricing, State Price Density, Rookley",Julian Winkel,BitcoinPricingKernels,15, Calculate IV vs Real Vola and visualize matplotlib matplotlib json collections datetime numpy pandas pymongo.uri_parser _TLSINSECURE_EXCLUDE_OPTS brc BRC  Order dict by keys and then plot IV  Vola over Time  @Todo: Filter for volume  Dump each element in a dict and save as JSON dates.append currkey) df.to_csv 'out/synth_btc_index_per_minute.csv')  Display only Monthly Dates  every month  Specify the format - %b gives us Jan  Feb... plt.plot dates  underlying) plt.title 'Synthetic BTC Index')  Only Months  Specify formatter,73
BitcoinPricingKernels,"Bitcoin Pricing Kernels are inferred using a novel data set from Deribit, one of the largest Bitcoin derivatives exchanges. This enables arbitrage-free pricing of various instruments. State Price Densities are estimated nonparametrically. The underlying asset process is viewed through the lens of a Stochastic Volatility with Correlated Jumps (SVCJ) framework. Shape invariant pricing kernels are reported. Market inefficiencies are assessed based on the shape of the pricing kernels. A trading strategy that exploits these inefficiencies is evaluated.","Bitcoin, Pricing Kernel, Bitcoin Option, Option, Derivative, Pricing, State Price Density, Rookley",Julian Winkel,BitcoinPricingKernels,15,os pickle save_dict moneyness  gdom  gstar  spd  curr_day  tau  maturitydate_char  fname load_dict fname load_estimated_densities fname = 'out/estimated_densities.csv'  allowed_taurange = [0.01  0.02]  only_taus = False  Extract Date and Tau from Dict Key  Day when instrument is traded  Time to maturity as Decimal  Maturitydate of Instrument,44
BitcoinPricingKernels,"Bitcoin Pricing Kernels are inferred using a novel data set from Deribit, one of the largest Bitcoin derivatives exchanges. This enables arbitrage-free pricing of various instruments. State Price Densities are estimated nonparametrically. The underlying asset process is viewed through the lens of a Stochastic Volatility with Correlated Jumps (SVCJ) framework. Shape invariant pricing kernels are reported. Market inefficiencies are assessed based on the shape of the pricing kernels. A trading strategy that exploits these inefficiencies is evaluated.","Bitcoin, Pricing Kernel, Bitcoin Option, Option, Derivative, Pricing, State Price Density, Rookley",Julian Winkel,BitcoinPricingKernels,15, 3D Stuff: Use to track a single instrument over time os matplotlib from mpl_toolkits.mplot3d import Axes3D helpers load_dict numpy pdb  2D Stuff: Use to show different SPDs/EPKs on a single day pricingkernel_plot x  pk  date  tau  idx plt.plot x[i]  s[i]  label = str 'spd') + str tau[i])) plt.plot x[i]  z[i]  label = str 'hd') + str tau[i])) pdb.set_trace ) pdb.set_trace ) plt.ylim 0  0.1)  Shrink current axis by 20%   Put a legend to the right of the current axis plt.draw ) plot_pk_over_time x  pk  date  tau  idx  maturitydate plt.plot x[i]  s[i]  label = str 'spd') + str tau[i])) plt.plot x[i]  z[i]  label = str 'hd') + str tau[i])) pdb.set_trace )   + maturitydate[i] currdate = date[i] currtau = tau[i]  maturitydate is all the same  Shrink current axis by 20%   Put a legend to the right of the current axis plt.savefig 'pricingkernel/plots/pk_dynamics' + str maturitydate) + '.png') plt.draw ) 'out/estimated_densities.csv'  pricingkernel  Extract Date and Tau from Dict Key  Day when instrument is traded  Time to maturity as Decimal  Maturitydate of Instrument  Filter for different dates and run this whole thing  Tracking all Maturities which are traded on one day  Find the index where all maturitydates are the same  So that we can compare instruments which have equal Maturity    Meaning we can track one fixed instrument over time,215
pyTSA_ReturnsPG,"This Quantlet plots monthly time series of returns of Procter and Gamble from 1961 to 2016 and  their ACF and PACF (Example, 2.4 Figures 2.8-2.9 in the book)","time series, autocorrelation, returns, ACF, PACF, plot, visualisation","Huang Changquan, Alla Petukhina",pyTSA_UnitRoot,1,numpy pandas statsmodels.tsa.arima_process arma_generate_sample matplotlib from PythonTsa.plot_acf_pacf import acf_pacf_fig,9
SC_classification,"This Quantlet is dedicated to classification of the Solidity source code using machine learning methods. We differentiate between traditional machine learning methods and deep learning. There are three modes for training these methods: on the source code, on the source code without comments and on only comments. Moreover, deep pre-trained transformer BERT - the state-of-the-art approach in the NLP were used on the comments mode. Furthermore, additional feature extraction of the most important block names of the Solidity code was made and the best performing ML algorithm was trained on these features.","machine learning, source code, ML, DL, transformers, BERT","Elizaveta Zinovyeva, Raphael Constantin Georg Reule",SC-classification,9,!/usr/bin/env python  coding: utf-8  In[1]: pandas numpy numpy.random seed tensorflow random os nltk tokenize nltk sklearn.model_selection StratifiedKFold sklearn.model_selection StratifiedKFold sklearn.metrics roc_auc_score sklearn.metrics precision_score sklearn.preprocessing LabelEncoder tensorflow.keras.layers Dense tensorflow.keras.optimizers RMSprop tensorflow.keras.models Model tensorflow.keras.callbacks ModelCheckpoint transformers traitlets matplotlib seaborn tqdm.notebook tqdm tokenizers BertWordPieceTokenizer tensorflow.keras backend gc  In[2]:  Create strategy from tpu Transformer implementation is based on the following code: https://www.kaggle.com/miklgr500/jigsaw-tpu-bert-with-huggingface-and-keras  In[3]:  In[4]:  In[5]: other x  In[6]:  In[7]:  In[8]:  In[9]:  In[10]:  ## Only The Comments  In[11]:  In[12]:  In[13]:  First load the real tokenizer  Save the loaded tokenizer locally  Reload it with the huggingface tokenizers library  In[14]: fast_encode texts  tokenizer  chunk_size=256  maxlen=512 build_model transformer  loss='categorical_crossentropy'  max_len=512  lr=lr x = tf.keras.layers.Dropout 0.35) cls_token)  In[15]:  #### BERT  In[16]:  In[17]:  In[18]:  In[19]:  In[20]:  In[21]:  In[21]:,118
SC_classification,"This Quantlet is dedicated to classification of the Solidity source code using machine learning methods. We differentiate between traditional machine learning methods and deep learning. There are three modes for training these methods: on the source code, on the source code without comments and on only comments. Moreover, deep pre-trained transformer BERT - the state-of-the-art approach in the NLP were used on the comments mode. Furthermore, additional feature extraction of the most important block names of the Solidity code was made and the best performing ML algorithm was trained on these features.","machine learning, source code, ML, DL, transformers, BERT","Elizaveta Zinovyeva, Raphael Constantin Georg Reule",SC-classification,9,numpy pandas tensorflow tensorflow keras tensorflow.compat.v1.keras.layers Embedding tensorflow.compat.v1.keras.layers Dense tensorflow.compat.v1.keras.layers Bidirectional tensorflow.compat.v1.keras.layers CuDNNLSTM tensorflow.compat.v1.keras.layers Conv1D tensorflow.compat.v1.keras.layers Dense tensorflow.compat.v1.keras.models Model tensorflow.compat.v1.keras.optimizers RMSprop tensorflow tensorflow.compat.v1.keras.models load_model tensorflow.compat.v1.keras.preprocessing text tensorflow.keras initializers sklearn.model_selection StratifiedKFold sklearn.metrics roc_auc_score sklearn.metrics precision_score nltk tokenize nltk ############################################ ####              MODELS              ###### ############################################   gru_keras max_features  maxlen  bidirectional  dropout_rate  embed_dim  rec_units mtype='GRU'  reduction = None  classes=4  lr=0.001 cnn_keras max_features  maxlen  dropout_rate  embed_dim  num_filters=300  classes=4  lr=0.001 dl_model model_type='BGRU'  max_features=40000  embed_dim=50  rec_units=150  dropout_rate=0.25  maxlen=400  classes=4  lr=0.001 ############################################ ####            TRAINING              ###### ############################################ for threshold in [0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9]:,89
SC_classification,"This Quantlet is dedicated to classification of the Solidity source code using machine learning methods. We differentiate between traditional machine learning methods and deep learning. There are three modes for training these methods: on the source code, on the source code without comments and on only comments. Moreover, deep pre-trained transformer BERT - the state-of-the-art approach in the NLP were used on the comments mode. Furthermore, additional feature extraction of the most important block names of the Solidity code was made and the best performing ML algorithm was trained on these features.","machine learning, source code, ML, DL, transformers, BERT","Elizaveta Zinovyeva, Raphael Constantin Georg Reule",SC-classification,9,"!/usr/bin/env python  coding: utf-8  In[1]: numpy matplotlib os pandas sklearn.pipeline Pipeline sklearn.model_selection GridSearchCV GradientBoostingClassifier sklearn.model_selection StratifiedKFold sklearn.linear_model LogisticRegression sklearn.naive_bayes GaussianNB random sklearn metrics collections Counter argparse sklearn.model_selection cross_validate sklearn.metrics roc_auc_score sklearn.feature_extraction.text TfidfVectorizer sklearn.linear_model SGDClassifier sklearn.model_selection ParameterGrid lightgbm seaborn sklearn.preprocessing LabelEncoder collections  In[2]: other x  In[3]:  In[4]:  In[5]:  In[6]:  In[7]:  In[8]:  In[9]:  In[10]:  In[11]:  In[12]:  In[13]:  ## Full code  ### Ridge Regression  In[14]:  In[15]:  use best C  In[16]:  In[17]:  Features: {FEATURE} C:{c_p} AUC ROC: {round metrics_cv.loc[:  metrics_cv.columns.str.contains 'aucroc')].mean axis=0).mean )  3)} ###  AUC PRC: {round metrics_cv.loc[:  metrics_cv.columns.str.contains 'aucprc')].mean axis=0).mean )  3)}"")  In[18]:  In[19]:  In[20]:  In[21]:  TRAIN ON WHOLE DATA AND PREDICT ON TEST  In[22]:  In[23]:  In[24]:  ### Lasso Regression  In[25]:  use best C  In[26]:  In[27]:  Features: {FEATURE} C:{c_p} AUC ROC: {round metrics_cv.loc[:  metrics_cv.columns.str.contains 'aucroc')].mean axis=0).mean )  3)} ###  AUC PRC: {round metrics_cv.loc[:  metrics_cv.columns.str.contains 'aucprc')].mean axis=0).mean )  3)}"")  In[28]:  In[29]:  In[30]:  In[31]:  TRAIN ON WHOLE DATA AND PREDICT ON TEST  In[32]:  In[33]:  In[34]:  ### Random Forest  In[35]: ##  AUC PRC: {round metrics_cv.loc[:  metrics_cv.columns.str.contains 'aucprc')].mean axis=0).mean )  3)}"")  In[36]:  In[37]:  TRAIN ON WHOLE DATA AND PREDICT ON TEST  In[38]:  In[39]:  In[40]:  ### SVM  In[41]:  best parameters ##  AUC PRC: {round metrics_cv.loc[:  metrics_cv.columns.str.contains 'aucprc')].mean axis=0).mean )  3)}"")  In[42]:  In[43]:  In[44]:  In[45]:  TRAIN ON WHOLE DATA AND PREDICT ON TEST  In[46]:  ### Only comments  ### Ridge Regression  In[47]:  use best C  In[48]:  In[49]:  Features: {FEATURE} C:{c_p} AUC ROC: {round metrics_cv.loc[:  metrics_cv.columns.str.contains 'aucroc')].mean axis=0).mean )  3)} ###  AUC PRC: {round metrics_cv.loc[:  metrics_cv.columns.str.contains 'aucprc')].mean axis=0).mean )  3)}"")  In[50]:  In[51]:  In[52]:  In[53]:  TRAIN ON WHOLE DATA AND PREDICT ON TEST  In[54]:  In[55]:  In[56]:  ### Lasso Regression  In[57]:  use best C  In[58]:  In[59]:  Features: {FEATURE} C:{c_p} AUC ROC: {round metrics_cv.loc[:  metrics_cv.columns.str.contains 'aucroc')].mean axis=0).mean )  3)} ###  AUC PRC: {round metrics_cv.loc[:  metrics_cv.columns.str.contains 'aucprc')].mean axis=0).mean )  3)}"")  In[60]:  In[61]:  In[62]:  In[63]:  TRAIN ON WHOLE DATA AND PREDICT ON TEST  In[64]:  In[65]:  In[66]:  ### Random Forest  In[67]: ##  AUC PRC: {round metrics_cv.loc[:  metrics_cv.columns.str.contains 'aucprc')].mean axis=0).mean )  3)}"")  In[68]:  In[69]:  TRAIN ON WHOLE DATA AND PREDICT ON TEST  In[70]:  In[71]:  In[72]:  ### SVM  In[73]:  best parameters ##  AUC PRC: {round metrics_cv.loc[:  metrics_cv.columns.str.contains 'aucprc')].mean axis=0).mean )  3)}"")  In[74]:  In[75]:  In[76]:  In[77]:  TRAIN ON WHOLE DATA AND PREDICT ON TEST  In[78]:  ## WITHOUT Comments  only code  ### Ridge Regression  In[79]:  use best C  In[80]:  In[81]:  Features: {FEATURE} C:{c_p} AUC ROC: {round metrics_cv.loc[:  metrics_cv.columns.str.contains 'aucroc')].mean axis=0).mean )  3)} ###  AUC PRC: {round metrics_cv.loc[:  metrics_cv.columns.str.contains 'aucprc')].mean axis=0).mean )  3)}"")  In[82]:  In[83]:  In[84]:  In[85]:  TRAIN ON WHOLE DATA AND PREDICT ON TEST  In[86]:  In[87]:  In[88]:  ### Lasso Regression  In[89]:  use best C  In[90]:  In[91]:  Features: {FEATURE} C:{c_p} AUC ROC: {round metrics_cv.loc[:  metrics_cv.columns.str.contains 'aucroc')].mean axis=0).mean )  3)} ###  AUC PRC: {round metrics_cv.loc[:  metrics_cv.columns.str.contains 'aucprc')].mean axis=0).mean )  3)}"")  In[92]:  In[93]:  In[94]:  In[95]:  TRAIN ON WHOLE DATA AND PREDICT ON TEST  In[96]:  In[97]:  In[98]:  ### Random Forest  In[99]: ##  AUC PRC: {round metrics_cv.loc[:  metrics_cv.columns.str.contains 'aucprc')].mean axis=0).mean )  3)}"")  In[100]:  In[101]:  TRAIN ON WHOLE DATA AND PREDICT ON TEST  In[102]:  In[103]:  In[104]:  ### SVM  In[105]:  best parameters ##  AUC PRC: {round metrics_cv.loc[:  metrics_cv.columns.str.contains 'aucprc')].mean axis=0).mean )  3)}"")  In[106]:  In[107]:  In[108]:  In[109]:  TRAIN ON WHOLE DATA AND PREDICT ON TEST  In[110]:",508
SC_classification,"This Quantlet is dedicated to classification of the Solidity source code using machine learning methods. We differentiate between traditional machine learning methods and deep learning. There are three modes for training these methods: on the source code, on the source code without comments and on only comments. Moreover, deep pre-trained transformer BERT - the state-of-the-art approach in the NLP were used on the comments mode. Furthermore, additional feature extraction of the most important block names of the Solidity code was made and the best performing ML algorithm was trained on these features.","machine learning, source code, ML, DL, transformers, BERT","Elizaveta Zinovyeva, Raphael Constantin Georg Reule",SC-classification,9,!/usr/bin/env python  coding: utf-8  In[1]: numpy pandas nltk.tokenize word_tokenize nltk.corpus stopwords nltk.stem WordNetLemmatizer sklearn.feature_extraction.text TfidfVectorizer string punctuation collections Counter collections OrderedDict matplotlib sklearn.cluster KMeans sklearn.metrics silhouette_score sklearn.mixture GaussianMixture umap matplotlib re seaborn sklearn.feature_extraction.text CountVectorizer  In[2]:  In[3]: 2A702C'  F20C05'  05F2C0'  9505F2'  050CF2'  A7B2B6'}  In[4]:  Prepare data  In[6]:  Visualize clusters,48
SC_classification,"This Quantlet is dedicated to classification of the Solidity source code using machine learning methods. We differentiate between traditional machine learning methods and deep learning. There are three modes for training these methods: on the source code, on the source code without comments and on only comments. Moreover, deep pre-trained transformer BERT - the state-of-the-art approach in the NLP were used on the comments mode. Furthermore, additional feature extraction of the most important block names of the Solidity code was made and the best performing ML algorithm was trained on these features.","machine learning, source code, ML, DL, transformers, BERT","Elizaveta Zinovyeva, Raphael Constantin Georg Reule",SC-classification,9,!/usr/bin/env python  coding: utf-8  In[1]: pandas numpy numpy.random seed tensorflow random os nltk tokenize nltk sklearn.model_selection StratifiedKFold sklearn.model_selection StratifiedKFold sklearn.metrics roc_auc_score sklearn.metrics precision_score sklearn.preprocessing LabelEncoder  In[2]:  In[3]:  In[4]: sys  In[5]: models  In[6]: other x  In[7]:  In[8]:  In[9]:  In[10]:  In[11]:  ## FULL  In[12]:  In[13]:  ### BGRU  In[14]:  In[15]:  In[16]:  In[17]:  ### GRU  In[18]:  In[19]:  In[20]:  In[21]:  In[21]:  ### CNN  In[22]:  In[23]:  In[24]:  In[25]:  In[25]:  ## Only The Source Code  In[26]:  In[27]:  ### BGRU  In[28]:  In[29]:  In[30]:  In[31]:  ### GRU  In[32]:  In[33]:  In[34]:  In[35]:  In[35]:  ### CNN  In[36]:  In[37]:  In[38]:  In[39]:  In[39]:  ## Only The Comments  In[40]:  In[41]:  ### BGRU  In[42]:  In[43]:  In[44]:  In[45]:  ### GRU  In[46]:  In[47]:  In[48]:  In[49]:  ### CNN  In[50]:  In[51]:  In[52]:  In[53]:,114
"Automated Cryptocurrency Portfolios: Portfolio Optimization, an Empirical Study.","Markowitz (Global Minimum Variance, maximum Sharpe), Hierarchical Risk Parity and three simple portfolios: equally weighted, inverse volatility and inverse variance in the novel asset class of cryptocurrencies. The CRyptocurrency IndeX CRIX is used as benchmark. Portfolio optimization is computed using 120 days of daily historical data with portfolio rebalancing taking place every 7 days and 30 days.","portfolio optimization, mean-variance optimization, Markowitz, modern portfolio theory, random matrix theory, eigenvalue-clipping, hierarchical risk parity, cryptocurrencies, CRIX CRyptocurrency IndeX","Morishige Takane, Guillermo Masayuki",Automated-Cryptocurrency-Portfolios,12,math pandas numpy datetime timedelta datetime datetime pypfopt EfficientFrontier pypfopt risk_models pypfopt expected_returns mlfinlab.portfolio_optimization.hrp HierarchicalRiskParity first row of zeros Set to zero the weighted returns of the dates before the start of the portfolio Set the first weights  remove first row rebalancing and returns calculation All the cryptos not in the weight names set to 0 Returns calculation  Returns calculation Save the dataframes to Excel file,66
"Automated Cryptocurrency Portfolios: Portfolio Optimization, an Empirical Study.","Markowitz (Global Minimum Variance, maximum Sharpe), Hierarchical Risk Parity and three simple portfolios: equally weighted, inverse volatility and inverse variance in the novel asset class of cryptocurrencies. The CRyptocurrency IndeX CRIX is used as benchmark. Portfolio optimization is computed using 120 days of daily historical data with portfolio rebalancing taking place every 7 days and 30 days.","portfolio optimization, mean-variance optimization, Markowitz, modern portfolio theory, random matrix theory, eigenvalue-clipping, hierarchical risk parity, cryptocurrencies, CRIX CRyptocurrency IndeX","Morishige Takane, Guillermo Masayuki",Automated-Cryptocurrency-Portfolios,12,math pandas numpy datetime timedelta datetime datetime pypfopt EfficientFrontier pypfopt risk_models pypfopt expected_returns first row of zeros Set to zero the weighted returns of the dates before the start of the portfolio Set the first weights  remove first row risk free rate - 0.02  in this case = 0) rebalancing and returns calculation All the cryptos not in the weight names set to 0 Returns calculation  Returns calculation Save the dataframes to Excel file,74
"Automated Cryptocurrency Portfolios: Portfolio Optimization, an Empirical Study.","Markowitz (Global Minimum Variance, maximum Sharpe), Hierarchical Risk Parity and three simple portfolios: equally weighted, inverse volatility and inverse variance in the novel asset class of cryptocurrencies. The CRyptocurrency IndeX CRIX is used as benchmark. Portfolio optimization is computed using 120 days of daily historical data with portfolio rebalancing taking place every 7 days and 30 days.","portfolio optimization, mean-variance optimization, Markowitz, modern portfolio theory, random matrix theory, eigenvalue-clipping, hierarchical risk parity, cryptocurrencies, CRIX CRyptocurrency IndeX","Morishige Takane, Guillermo Masayuki",Automated-Cryptocurrency-Portfolios,12,math pandas numpy datetime timedelta datetime datetime pypfopt EfficientFrontier pypfopt risk_models pypfopt expected_returns first row of zeros Set to zero the weighted returns of the dates before the start of the portfolio Set the first weights  remove first row risk free rate - 0.02  in this case = 0) Make the weights matrix sum = 1 divisor = weights.sum ) weights=weights.divide divisor) weight_matrix = weights.to_frame ) rebalancing and returns calculation All the cryptos not in the weight names set to 0 Returns calculation  Make the weights matrix sum = 1 divisor = weights.sum ) weights = weights.divide divisor) weights = weights.to_frame ) scaling factor k  making the sum of all short-long positions = 1  Returns calculation Save the dataframes to Excel file,122
"Automated Cryptocurrency Portfolios: Portfolio Optimization, an Empirical Study.","Markowitz (Global Minimum Variance, maximum Sharpe), Hierarchical Risk Parity and three simple portfolios: equally weighted, inverse volatility and inverse variance in the novel asset class of cryptocurrencies. The CRyptocurrency IndeX CRIX is used as benchmark. Portfolio optimization is computed using 120 days of daily historical data with portfolio rebalancing taking place every 7 days and 30 days.","portfolio optimization, mean-variance optimization, Markowitz, modern portfolio theory, random matrix theory, eigenvalue-clipping, hierarchical risk parity, cryptocurrencies, CRIX CRyptocurrency IndeX","Morishige Takane, Guillermo Masayuki",Automated-Cryptocurrency-Portfolios,12,math pandas numpy datetime timedelta datetime datetime pypfopt EfficientFrontier pypfopt risk_models pypfopt expected_returns first row of zeros Set to zero the weighted returns of the dates before the start of the portfolio Set the first weights  remove first row risk free rate - 0.02  in this case = 0) rebalancing and returns calculation All the cryptos not in the weight names set to 0 Returns calculation  Returns calculation Save the dataframes to Excel file,74
"Automated Cryptocurrency Portfolios: Portfolio Optimization, an Empirical Study.","Markowitz (Global Minimum Variance, maximum Sharpe), Hierarchical Risk Parity and three simple portfolios: equally weighted, inverse volatility and inverse variance in the novel asset class of cryptocurrencies. The CRyptocurrency IndeX CRIX is used as benchmark. Portfolio optimization is computed using 120 days of daily historical data with portfolio rebalancing taking place every 7 days and 30 days.","portfolio optimization, mean-variance optimization, Markowitz, modern portfolio theory, random matrix theory, eigenvalue-clipping, hierarchical risk parity, cryptocurrencies, CRIX CRyptocurrency IndeX","Morishige Takane, Guillermo Masayuki",Automated-Cryptocurrency-Portfolios,12,math pandas numpy pyRMT datetime timedelta datetime datetime pypfopt EfficientFrontier pypfopt risk_models pypfopt expected_returns first row of zeros Set to zero the weighted returns of the dates before the start of the portfolio Set the first weights  remove first row risk free rate - 0.02  in this case = 0) rebalancing and returns calculation All the cryptos not in the weight names set to 0 Returns calculation  Make the weights matrix sum = 1 divisor = weights.sum ) weights = weights.divide divisor)  Returns calculation Save the dataframes to Excel file,90
"Automated Cryptocurrency Portfolios: Portfolio Optimization, an Empirical Study.","Markowitz (Global Minimum Variance, maximum Sharpe), Hierarchical Risk Parity and three simple portfolios: equally weighted, inverse volatility and inverse variance in the novel asset class of cryptocurrencies. The CRyptocurrency IndeX CRIX is used as benchmark. Portfolio optimization is computed using 120 days of daily historical data with portfolio rebalancing taking place every 7 days and 30 days.","portfolio optimization, mean-variance optimization, Markowitz, modern portfolio theory, random matrix theory, eigenvalue-clipping, hierarchical risk parity, cryptocurrencies, CRIX CRyptocurrency IndeX","Morishige Takane, Guillermo Masayuki",Automated-Cryptocurrency-Portfolios,12,math pandas numpy datetime timedelta datetime datetime pypfopt EfficientFrontier pypfopt risk_models pypfopt expected_returns first row of zeros Set to zero the weighted returns of the dates before the start of the portfolio Set the first weights  remove first row risk free rate - 0.02  in this case = 0) Make the weights matrix sum = 1 divisor = weights.sum ) weights=weights.divide divisor) rebalancing and returns calculation All the cryptos not in the weight names set to 0 Returns calculation  Make the weights matrix sum = 1 divisor = weights.sum ) weights = weights.divide divisor)  Returns calculation Save the dataframes to Excel file,102
"Automated Cryptocurrency Portfolios: Portfolio Optimization, an Empirical Study.","Markowitz (Global Minimum Variance, maximum Sharpe), Hierarchical Risk Parity and three simple portfolios: equally weighted, inverse volatility and inverse variance in the novel asset class of cryptocurrencies. The CRyptocurrency IndeX CRIX is used as benchmark. Portfolio optimization is computed using 120 days of daily historical data with portfolio rebalancing taking place every 7 days and 30 days.","portfolio optimization, mean-variance optimization, Markowitz, modern portfolio theory, random matrix theory, eigenvalue-clipping, hierarchical risk parity, cryptocurrencies, CRIX CRyptocurrency IndeX","Morishige Takane, Guillermo Masayuki",Automated-Cryptocurrency-Portfolios,12,math pandas numpy pyRMT datetime timedelta datetime datetime pypfopt EfficientFrontier pypfopt risk_models pypfopt expected_returns first row of zeros Set to zero the weighted returns of the dates before the start of the portfolio Set the first weights  remove first row risk free rate - 0.02  in this case = 0) Make the weights matrix sum = 1 divisor = weights.sum ) weights=weights.divide divisor) weight_matrix = weights.to_frame ) rebalancing and returns calculation All the cryptos not in the weight names set to 0 Returns calculation  Make the weights matrix sum = 1 divisor = weights.sum ) weights = weights.divide divisor) weights = weights.to_frame ) scaling factor k  making the sum of all short-long positions = 1  Returns calculation Save the dataframes to Excel file,123
"Automated Cryptocurrency Portfolios: Portfolio Optimization, an Empirical Study.","Markowitz (Global Minimum Variance, maximum Sharpe), Hierarchical Risk Parity and three simple portfolios: equally weighted, inverse volatility and inverse variance in the novel asset class of cryptocurrencies. The CRyptocurrency IndeX CRIX is used as benchmark. Portfolio optimization is computed using 120 days of daily historical data with portfolio rebalancing taking place every 7 days and 30 days.","portfolio optimization, mean-variance optimization, Markowitz, modern portfolio theory, random matrix theory, eigenvalue-clipping, hierarchical risk parity, cryptocurrencies, CRIX CRyptocurrency IndeX","Morishige Takane, Guillermo Masayuki",Automated-Cryptocurrency-Portfolios,12,pypfopt CLA mlfinlab math pandas numpy pyRMT matplotlib datetime timedelta datetime datetime pypfopt EfficientFrontier pypfopt risk_models pypfopt expected_returns mlfinlab.portfolio_optimization.mean_variance MeanVarianceOptimisation mlfinlab.portfolio_optimization.hrp HierarchicalRiskParity  first row of zeros  Set to zero the weighted returns of the dates before the start of the portfolio  Set the first weights  remove first row  risk free rate - 0.02  in this case = 0) k = weight_matrix.abs ) k = k.sum axis=1) weight_matrix = weight_matrix.divide k  axis=0) cla.min_volatility ) Plot Efficient Frontier plot Covariance Plots Covariance Matrix after the Eigenvalue Clipping Plot HRP,87
"Automated Cryptocurrency Portfolios: Portfolio Optimization, an Empirical Study.","Markowitz (Global Minimum Variance, maximum Sharpe), Hierarchical Risk Parity and three simple portfolios: equally weighted, inverse volatility and inverse variance in the novel asset class of cryptocurrencies. The CRyptocurrency IndeX CRIX is used as benchmark. Portfolio optimization is computed using 120 days of daily historical data with portfolio rebalancing taking place every 7 days and 30 days.","portfolio optimization, mean-variance optimization, Markowitz, modern portfolio theory, random matrix theory, eigenvalue-clipping, hierarchical risk parity, cryptocurrencies, CRIX CRyptocurrency IndeX","Morishige Takane, Guillermo Masayuki",Automated-Cryptocurrency-Portfolios,12,math pandas cvxpy numpy datetime timedelta datetime datetime pypfopt risk_models pypfopt expected_returns minimum_variance means  cov  fully-invested  lower bound for weights  upper bound for weights  first row of zeros  Set to zero the weighted returns of the dates before the start of the portfolio  Set the first weights  remove first row  risk free rate  Constraint the weights to have leverage of 1  Check if there should be rebalancing  Calculate returns  All the cryptos not in the weight names set to 0 return  Returns calculation  Calculate weights  Constraint the weights to have leverage of 1  Returns calculation when no rebalancing is needed  Save the dataframes to Excel file,106
"Automated Cryptocurrency Portfolios: Portfolio Optimization, an Empirical Study.","Markowitz (Global Minimum Variance, maximum Sharpe), Hierarchical Risk Parity and three simple portfolios: equally weighted, inverse volatility and inverse variance in the novel asset class of cryptocurrencies. The CRyptocurrency IndeX CRIX is used as benchmark. Portfolio optimization is computed using 120 days of daily historical data with portfolio rebalancing taking place every 7 days and 30 days.","portfolio optimization, mean-variance optimization, Markowitz, modern portfolio theory, random matrix theory, eigenvalue-clipping, hierarchical risk parity, cryptocurrencies, CRIX CRyptocurrency IndeX","Morishige Takane, Guillermo Masayuki",Automated-Cryptocurrency-Portfolios,12,math pandas numpy pyRMT datetime timedelta datetime datetime pypfopt EfficientFrontier pypfopt risk_models pypfopt expected_returns mlfinlab.portfolio_optimization.hrp HierarchicalRiskParity first row of zeros Set to zero the weighted returns of the dates before the start of the portfolio Set the first weights  remove first row rebalancing and returns calculation All the cryptos not in the weight names set to 0 Returns calculation  Returns calculation Save the dataframes to Excel file,67
"Automated Cryptocurrency Portfolios: Portfolio Optimization, an Empirical Study.","Markowitz (Global Minimum Variance, maximum Sharpe), Hierarchical Risk Parity and three simple portfolios: equally weighted, inverse volatility and inverse variance in the novel asset class of cryptocurrencies. The CRyptocurrency IndeX CRIX is used as benchmark. Portfolio optimization is computed using 120 days of daily historical data with portfolio rebalancing taking place every 7 days and 30 days.","portfolio optimization, mean-variance optimization, Markowitz, modern portfolio theory, random matrix theory, eigenvalue-clipping, hierarchical risk parity, cryptocurrencies, CRIX CRyptocurrency IndeX","Morishige Takane, Guillermo Masayuki",Automated-Cryptocurrency-Portfolios,12,math pandas numpy scipy stats datetime timedelta datetime datetime scipy.stats norm seaborn matplotlib statistics statsmodels pypfopt EfficientFrontier pypfopt risk_models pypfopt expected_returns mlfinlab.portfolio_optimization.hrp HierarchicalRiskParity first row of zeros Descriptive Statistics of all the logarithmic returns of Cryptocurrencies shapiro = st.shapiro ret) print  shapiro) sh = shapiro[1] print  sh) desc.append sh) 'Shapiro-Wilk test p-value' ignore_index=False Calculate Portfolio Statistics with Prices + timedelta days=1) 95% confidence level shapiro = st.shapiro ret) print  shapiro) sh = shapiro[1] print  sh) desc.append sh) 'Shapiro-Wilk test p-value' ignore_index=False Statistics of Portfolios cc = pd.read_excel r'/Users/gmmtakane/Desktop/Thesis/PR_norm.xlsx') cc = pd.read_excel r'/Users/gmmtakane/Desktop/Thesis/CC_norm.xlsx') cc = pd.read_excel r'/Users/gmmtakane/Desktop/Thesis/CCRET.xlsx') start_p = start_analysis + timedelta days=1) 95% confidence level shapiro = st.shapiro ret) print  shapiro) sh = shapiro[1] print  sh)  desc.append sh) 'Shapiro-Wilk test p-value' ignore_index=False Save the dataframes to Excel file returns.to_excel r'/Users/gmmtakane/Desktop/Thesis/ret performance2' + '.xlsx'),133
"Automated Cryptocurrency Portfolios: Portfolio Optimization, an Empirical Study.","Markowitz (Global Minimum Variance, maximum Sharpe), Hierarchical Risk Parity and three simple portfolios: equally weighted, inverse volatility and inverse variance in the novel asset class of cryptocurrencies. The CRyptocurrency IndeX CRIX is used as benchmark. Portfolio optimization is computed using 120 days of daily historical data with portfolio rebalancing taking place every 7 days and 30 days.","portfolio optimization, mean-variance optimization, Markowitz, modern portfolio theory, random matrix theory, eigenvalue-clipping, hierarchical risk parity, cryptocurrencies, CRIX CRyptocurrency IndeX","Morishige Takane, Guillermo Masayuki",Automated-Cryptocurrency-Portfolios,12,math pandas cvxpy numpy datetime timedelta datetime datetime pypfopt risk_models pypfopt expected_returns maximum_Sharpe means  cov  fully-invested first row of zeros Set to zero the weighted returns of the dates before the start of the portfolio Set the first weights  remove first row risk free rate y expected_mean expected_vol k weights = maximum_Sharpe mu_excess  Sigma) Constraint the weights to have leverage of 1 Check if there should be rebalancing Calculate returns All the cryptos not in the weight names set to 0 return Returns calculation Calculate weights y  expected_mean  expected_vol  k  weights = maximum_Sharpe mu_excess  Sigma)  Constraint the weights to have leverage of 1  Returns calculation when no rebalancing is needed Save the dataframes to Excel file,116
Group_6_Project_HMM_Bitcoin_model_comparison,Comparison of the constructed model with alternative methods,"HMM, Bitcoin, prophet, LSTM, ARIMA","Changjie Jin, Ng Zhi Wei Aaron, You Pengxin, Chio Qi Jun, Zhang Yuxi",Group_6_Project_HMM_Bitcoin_model_comparison,1," -*- coding: utf-8 -*- pip install pystan conda install -c conda-forge fbprophet -y  pip install keras  pip install tensorflow pip install statsmodels os warnings logging itertools pandas numpy matplotlib hmmlearn.hmm GaussianHMM sklearn.model_selection train_test_split tqdm tqdm  Change plot style to ggplot  for better and more aesthetic visualisation) Read Bitcoin data  Compute the fraction change in close  high and low prices  which would be used a feature #################################################################################  Prophet Part pip install pystan conda install -c conda-forge fbprophet -y Prophet fbprophet Prophet pandas numpy matplotlib fbprophet Prophet fbprophet.diagnostics cross_validation fbprophet.diagnostics performance_metrics fbprophet.plot plot_cross_validation_metric warnings FB data: pd.DataFrame 年周期性 周周期性      m.add_seasonality name='monthly'  period=30.5  fourier_order=5  prior_scale=0.1)#月周期性 m.add_country_holidays country_name='CN')#中国所有的节假日     预测时长 Plot the predicted pricespro_  fig = plt.figure )  axes = fig.add_subplot 111)  axes.plot plotdays  actual_close_prices  'bo-'  label=""actual"")  axes.plot plotdays  pro_predicted_close_prices  'r+-'  label=""predicted"")  axes.set_title 'BTC-USD')  fig.autofmt_xdate )  plt.legend )  plt.show )  compare RMSE & ME sklearn.metrics mean_squared_error sklearn.metrics mean_absolute_error ################################################################################### LSTM Part  pip install keras  pip install tensorflow keras.models Sequential keras.layers Dense keras.layers.normalization BatchNormalization keras.optimizers Adam keras.callbacks EarlyStopping sklearn.preprocessing StandardScaler 历史数量 预测数量  n_test = 1 split_data x  y  n_test: int build_train train  n_in  n_out build_lstm n_in: int  n_features: int model_fit x_train  y_train  x_val  y_val  n_features minmaxscaler data: pd.DataFrame  fig = plt.figure )  plt.plot predict[""Predict""] )  plt.plot predict[""Actual""] )  compare RMSE & ME sklearn.metrics mean_squared_error sklearn.metrics mean_absolute_error ################################################################################  ARIMA Part pip install statsmodels statsmodels.graphics.tsaplots plot_acf 平稳性检测 statsmodels.tsa.arima_model ARIMA matplotlib pyplot  plot residual errors  pyplot.plot Arima_Predict)  pyplot.plot Arima_Actual )  pyplot.show ) sklearn.metrics mean_squared_error sklearn.metrics mean_absolute_error ################################################################################  HMM model Set constants for model Construct logging  Set up the model Split the data for training and testing Report the log Prepare model input Report the log Fit the model Output the states  Compute all possible outcomes predict close prices for days get most probable outcome Predict close price Plot the predicted prices",292
DEDA_SVM_Linear,Determines and plots the decision boundaries of a linear SVM classifier for different regularization parameters C.,"Support vector machines, SVM, classification",Georg Keilbar,DEDA_SVM_Linear,1,numpy matplotlib sklearn svm simulation of 40 data points plot_svm_linear X  Y  penalty fit the model get the separating hyperplane get the margin and the parallels plot the separating plane and the parallels plot the data points mark the support vectors,41
SFM_Class_2018_filter_SContracts,"From Source codes of Ethereum Smart Contracts create a data frame with features such as comments, source code, functions","parse, text mining, source code, Solidity, trading, fintech, Ethereum","Raphael Constantin Georg Reule, Elizaveta Zinovyeva, Rui Ren, Marvin Gauer",SFM_Class_2018_filter_SContracts,2,"!/usr/bin/env python  coding: utf-8  # Parse source codes to a Pandas Data Frame  Please see the quantlet SFM_Class_2018_Scrape_API_SContracts and place the created there folder ""./sol_source""  into root folder of this quanlet.  As well as Ethereum.csv  In[ ]: os numpy pandas re  In[ ]:  In[ ]:  Load list from existing dataset  In[ ]:  functions contract_name_extract data extract contract name function_name_extract data extract function names and join to one string comments_extract data extract contract comments and join to one text  find all occurance streamed comments  /*COMMENT */) from string  find all occurance singleline comments  //COMMENT\n ) from string code_no_punct data  In[ ]:  In[ ]:  In[ ]:  In[ ]:",106
CRIXrnn,Build a cryptocurrency portfolio based on price movements forecasts with recurrent neural networks in order to outperform CRIX,"recurrent neural networks, deep learning, cryptocurrency, CRIX, portfolio, prediction, time series, stock price forecast",SPILAK Bruno,CRIXrnn,1,pandas sklearn.metrics classification_report dldata.dldata_tools series_to_supervised_multi data Save object so we do not have to recreate the tables for different testing clean data results of different models Model diagnosis Accuracies are good  F1 score more constrated Model performance Plots clean data results of different models Model diagnosis Accuracies are good  F1 score more constrated Model performance Plots,56
SC_over_time,This Quantlet is dedicated to visualization of time-series of the amount of Smart contracts created since the invention of Ethereum,"Kaggle, BigQuery, visualization, time-series, source code, Solidity, smart contracts","Elizaveta Zinovyeva, Raphael Constantin Georg Reule",SC-over-time,4,!/usr/bin/env python  coding: utf-8  In[ ]: pandas numpy matplotlib seaborn  In[ ]:  In[ ]:  In[ ]: ef476f' ax1.set_xlabel 'date')  instantiate a second axes that shares the same x-axis 19751C'  we already handled the x-label with ax1 14213d'  In[ ]:  In[ ]:  In[ ]: 118ab2' 073b4c',45
SC_over_time,This Quantlet is dedicated to visualization of time-series of the amount of Smart contracts created since the invention of Ethereum,"Kaggle, BigQuery, visualization, time-series, source code, Solidity, smart contracts","Elizaveta Zinovyeva, Raphael Constantin Georg Reule",SC-over-time,4,!/usr/bin/env python  coding: utf-8  In[1]: numpy pandas matplotlib  In[2]: datetime date  In[3]: bq_helper BigQueryHelper  Helper object for BigQuery Ethereum dataset  In[4]:  In[5]:  Estimate total data scanned for query  in GB)  In[6]:  Store the results into a Pandas DataFrame  In[7]:  Convert strings into datetime objects and resample the data to monthly totals  In[8]:  title='New Ethereum Smart Contracts  monthly totals)'   In[9]:,59
LPA_simulations,"Creates simulation data for different scenarios mimicking the real life scenarios, applies the local parametric approach to detect change points, regime shifts and structural breaks and then compares the estimates from fixed rolling window estimates and LPA approach using RMSE. It also provides all the plots along with the confidence intervals ","mergers & acquisitions, mergers, acquisitions, deals, poisson, time series, decomposition, forecasting, prediction, economics, estimation, adaptive parameters",Kainat Khowaja,LPA_Simulations,2,!/usr/bin/env python  coding: utf-8  In[27]: Importing packages pandas io math sqrt matplotlib numpy datetime datetime scipy stats math sklearn.metrics mean_squared_error decimal Decimal timeit # Setting defaults for plots  In[28]:  Defining important functions generate_random_weights num R x w  To store the required sum   Add factorial of all the elements  s += math.log math.factorial x[i]))*w[i];  Likelihood theta x w = None n if weights equal 1  sum otherwise  - R x w) MLE_estimator x  w = None  For stepwise function plot mystep x y z  ax=None  **kwargs # New functions poisson yi  theta /math.factorial yi) l = stats.poisson.pmf k=yi  mu=theta) weights_matrix num  length weights for whole interval likelihoods y  theta Simplifying numpy notation plus a b c=0 d=0 e=0 h=0 i=0 j=0 k=0  l=0 neg a product a b c=1 d=1 e=1 h=1 i=1 j=1 simulate data simulate types  means  lengths Scales the data to positive and integerize it  simulated dates  initialise data of lists.   Creates pandas DataFrame.  Function for Confidence Interval norm_CI data  alpha=0.95  In[3]: # Defining various simulation scenerios  In[45]: Same simulation repeated multiple times select how many times you want to run the same simulation scenerio select the simulation scenerio here plt.xlabel 'Year') plt.ylabel 'Count') Making filename  Initializing starting from final time from theory Assuming homogeneity in first three month define how many bootstraps  starting at a new point for each iteration Arithmetic increase K = int io/n_0)   Geometric increase Calculated this analytically started from 1 because 0th interval already homo and K+1 because last point is not inclusive Arithmetic n_k =  k+1)*n_0 n_k_plus1 =  k+2)*n_0 Geometric increase New intervals and their estimators  test statistics for this breakpoint and adding it to the list # Bootstrap procedure   Getting lengths of intervals length of A_k length of B_k generating weights and calculating estimators for the bootstrap intervals Test statistic for bootstrap for this breakpoint  since there are no changepoints  we provide artifical values to make the algo jump to the next interval # Saving data to CSV file  # Combining all the files  In[46]:  Getting names of all csv files  Making a dataframe by concatinating name Deleting the duplicated date column  Sorting the dataframe aphabatically  # Constructing confidence interval  In[47]:  In[48]: CI plot Getting CIs for MLEs Fetching MLE data from dataframe plt.fill_between dates lower  upper  color='r'  alpha=.10)  In[49]:  Fetching windows data from dataframe Getting CIs CI plot  In[50]: Making a new column of MLEs in data   In[51]: number of periods ahead to forecast  DT_N.N.plot label='Number of M&A') DT_N.MLEs.plot label='MLEs') DT_N.N.plot label='Number of M&A' color = 'k' ) plt.xlabel 'Year')  Calculating moving averages using rolling window with time period 12 -we use mean because the max likelihood  estimator of poisson is sample mean   Calculating moving averages plt.legend )  In[52]: Idea: Take the last point  13th from the last) as estimate and use it as prediction of next n months  In[ ]:,471
Boosting_example,Predict housing price using the Boston housing data set. Then calculate SHAP values and plot the respective figures.,"Shapley, Boosting, SHAP, Interpretability, Explainable AI",Ratmir Miftachov,Boosting_example,2,!/usr/bin/env python  coding: utf-8  In[1]: numpy numpy matplotlib matplotlib keras tensorflow tensorflow keras tensorflow.keras layers tensorflow.keras.models Sequential tensorflow.keras.layers Dense SHAP shap xgboost pandas sklearn.datasets make_regression  In[2]: Import data  explain the model's predictions using SHAP  In[3]:  visualize the first prediction's explanation plt.figure figsize= 6.4 4.8)  dpi= 200) # 6.4  4.8) baseline plt.savefig 'boost_1.png'  transparent=True)  In[4]:  visualize the first prediction's explanation with a force plot # screenshot  In[5]:  summarize the effects of all the features plt.savefig 'boost_2.png'  transparent=True)  In[6]: plt.savefig 'boost_3.png'  transparent=True)  In[7]:  create a dependence scatter plot to show the effect of a single feature across the whole dataset plt.savefig 'boost_4.png'  transparent=True)  In[12]:  In[ ]:,104
loadcrix,Load crix and vcrix data from a CSV file.,ERROR,Jovanka Lili Matic,loadcrix,2,!/usr/bin/env python  coding: utf-8  In[16]:  Import packages numpy pandas matplotlib seaborn  In[25]:   skiprows=1)   skiprows=1)  In[ ]:,16
Quantlet_Grading,Grading of all Quantlets within one GitHub repository with the use of the classes modules/QUANTLET.py and modules/METAFILE.py.,"Quantlet, Quantlet grading, text analysis, text evaluation, yaml debugging",Marius Sterling,Quantlet_Evaluation,3, -*- coding: utf-8 -*- os  Change working directory to the repository path  Loading QUANTLET class modules.QUANTLET QUANTLET  Add github token  if you try to access a private repository or   to have a higher access limit.  set the user name   set name of repository  Quantlets are downloaded  Quantlets are graded.,49
Quantlet_Grading,Grading of all Quantlets within one GitHub repository with the use of the classes modules/QUANTLET.py and modules/METAFILE.py.,"Quantlet, Quantlet grading, text analysis, text evaluation, yaml debugging",Marius Sterling,Quantlet_Evaluation,3, -*- coding: utf-8 -*- itertools numpy nltk.corpus stopwords __init__ self  file  repo  content  commits FIXME: no general error exceptions                 [i for i in file_type if i not in ['png' 'jpg' 'jpeg' 'pdf']] pre_clean c yaml_debugger x clean_keys d  import copy; d = copy.deepcopy tmp)  [{k:v} for k v in d.items ) if '[' in k]:  TODO check with string distance which field is meant and combine list_to_string self create_keyword_list self __grading self content  quality of metainfo file  number of keywords  number of words in discription  number of words without stopwords in discription  indication why grade worse than A was given  number of pictures in   submission year _refs = [c.name.split '.')[0] for c in _contents if c.name.split '.')[1] in ['png' 'jpg' 'jpeg']] _pdfs = [c for c in _pdfs if c.name.split '.')[0] in _refs],133
Quantlet_Grading,Grading of all Quantlets within one GitHub repository with the use of the classes modules/QUANTLET.py and modules/METAFILE.py.,"Quantlet, Quantlet grading, text analysis, text evaluation, yaml debugging",Marius Sterling,Quantlet_Evaluation,3," -*- coding: utf-8 -*- numpy pandas datetime github Github sklearn.feature_extraction.text CountVectorizer collections Counter scipy.sparse csc_matrix nltk.corpus stopwords nltk.stem.wordnet WordNetLemmatizer gensim.models Phrases gensim.models.phrases Phraser gensim.corpora Dictionary gensim.matutils corpus2dense sklearn.decomposition TruncatedSVD sklearn.pipeline make_pipeline sklearn.preprocessing Normalizer sklearn.cluster KMeans sklearn.metrics pairwise sklearn.manifold MDS tqdm tqdm gensim.sklearn_api TfIdfTransformer matplotlib time sleep modules.METAFILE METAFILE __init__ self  github_token=None  user=None stop_until_rate_reset self at_least_remaining=None rate = Github self.github_token).get_rate_limit ).rate rate = Github self.github_token).get_rate_limit ).core __download_metafiles_from_repository self  repo  server_path='.'  override=None  get repo content from directory server_path update_existing_metafiles self repos2update=None print k) remove_deleted_repos self repos_del update_all_metafiles self since=None download_metafiles_from_user self  repo_name=None  override=True save self  filepath load filepath grading self  save_path=None grades_equals=None get_last_commit self get_recently_changed_repos self  since create_readme self repos=None __readme_template name_of_quantlet  metainfo_original  pics  quantlet # [<img src=""https://github.com/QuantLet/Styleguide-and-FAQ/blob/master/pictures/qloqo.png"" alt=""Visit QuantNet"">] http://quantlet.de/) **') ## [%s Code: %s] %s)\n""% lang.upper ) i i)) ## [%s Code: %s] %s)\n""% lang.upper ) i i)) ## %s Code\n```%s' %  lang.upper )  lang))  see https://developer.github.com/v3/git/trees/#create-a-tree                     element = InputGitTreeElement '/'.join b.path.split '/')[:-1]) +'/'+'README.md'  '100644'  'blob'  readme.decode ))                         element = InputGitTreeElement v.directory +'/'+'README.md'  '100644'  'blob'  readme) text_preprocessing text \$\%\&\' \)\*\+\.\/\:\<\=\>\?\@\[\\\]\^\`\{\|\}\~\-_\ \;]""  re.UNICODE)  creating cleaned  see text_preprocessing) texts  excluding empty texts  {k:v for k v in docs_clean.items ) for i in v if len i)>15}  Add bigrams and trigrams to docs_clean  only ones that appear 10 times or more). txt_to_list txt  Filter out words that occur in less than leave_tokens_out_with_less_than_occurence  documents   or more than leave_tokens_out_with_ratio_of_occurence of the documents.  get bag-of-words representation of each quantlet sys.stdout = open os.devnull  'w') sys.stdout = sys.__stdout__ get_document_term_matrix self  corpus  dictionary get_SVD_explained_variance_ratio self  tdm  with_normalize=False get_corpus_tfidf self corpus dictionary lsa_model self  corpus  dictionary  num_topics=10 get_lsa_matrix self  lsa  corpus  dictionary cl_kmeans self  X  n_clusters cl_spectral self X n_clusters dist_metric='euclidean' From scikit - learn: [‘cityblock’  ‘cosine’  ‘euclidean’  ‘l1’  ‘l2’  ‘manhattan’].These metrics support sparse matrix inputs. From scipy.spatial.distance: [‘braycurtis’  ‘canberra’  ‘chebyshev’  ‘correlation’  ‘dice’  ‘hamming’  ‘jaccard’  ‘kulsinski’   ‘mahalanobis’  ‘matching’  ‘minkowski’  ‘rogerstanimoto’  ‘russellrao’  ‘seuclidean’  ‘sokalmichener’  ‘sokalsneath’  ‘sqeuclidean’  ‘yule’]  See the documentation for scipy.spatial.distance for details on these metrics.These metrics do not support sparse matrix inputs. cl_agglomerative self X  n_clusters  dist_metric='euclidean'  linkage= 'ward' cl_dbscan_n_cluster self X n_cluster dist_metric='euclidean' maxIter = 100  verbose = False lower=0 upper=40 half_find lower upper count=0 maxIter=maxIter verbose=verbose cl_dbscan self X eps dist_metric='euclidean' cl_dbscan_grid self  X  eps_grid  dist_metric='euclidean' n_cluster=None topic_labels self cl document_topic_matrix lsa top_n=5 take_care_of_bigrams=True clustering self  n_clusters top_n_words=5 tfidf=True cluster_algo='kmeans' dist_metric='euclidean' linkage=None directory='data/' file_ending n cluster_algo dist_metric linkage  need to be executed such that d.id2token is computed create_datanames_file self directory='' save_qlet_repo_file self  cluster_label file_name_ending  directory=''  TODO: Safe as JSON and read from file!!! correct_book pub __create_template v cluster id  TODO combine all print k) tsne self X cluster_labels n_iter=5000 dist_metric='euclidean' DPI=150  save_directory='' save_ending='' file_type='pdf'",428
vietnam_news_webscap,simple web-scraping on vietnam.net,ERROR,Huong Vu 0856156,vietnam_news_webscrap,2,!/usr/bin/env python  coding: utf-8  In[1]: requests bs4 BeautifulSoup pandas numpy  In[2]: os os path PIL Image numpy matplotlib wordcloud WordCloud  In[3]:  In[4]:  In[ ]:,24
DEDA_Introduction,"UNIT 1 of DEDA. Introduce basic syntax, such as numeric and string, and basic data structure, like list, tuple, set and dict in Python",ERROR,Junjie Hu and Isabell Fetzer and Lucas Umann,DEDA_Introduction,2,"!/usr/bin/env python  coding: utf-8  # **DEDA Unit1** : Python Installation  IDE and Basic Syntax  ### **Numerics**  In[ ]:  This is a one-line comment in Python. It extends to the end of the physical line.  initiates integer variable a which the value 5 is assigned to  another integer variable b which the value 3 is assigned to  the old value of b is overwritten by 5  initialising two variables at once: a takes the value  7; b takes 3  short for a = a+1 so again a new value is assigned to a  ### **Functions**  In[ ]:  the fct. print ) prints out the value of a; the output is 8  the fct. round ) rounds 8/3 and assigns the output to c  Function can also take arguments  which allow us further specifications.  ndigits specifies the number of decimal places  output is 2.6667  arguments do not need to be spelled out  if placed correctly  output is 2.667  ### **Strings**  In[ ]:  initiates string variables by single quotation marks  … or by double quotation marks  HelloWorld  numbers can also be assigned as strings too  concatenates strings so output is ‘2010’  the fct. int ) converts strings to integers  f is 30 now  formatting strings  using f '….'  string  you can write variable names inside the brackets {}  directly.  Hello Python-World.  Hello WORLD.  ### **Comparison**  In[ ]:  checks if a equals 5; output will be True  checks if a is not equal to 7; output will be False  checks if a is smaller or equal 7; output will be True  False  old integer value of a  which is 7) is overwritten by string value ‘A’  old integer value of b  which is 3) is overwritten by string value ‘B’  False  True  ### **Lists**  In[ ]:  initiates an empty list  which we choose to call “my_list”  adds elements to the list  output: [2  3  5  7  11]  indexing  the first element of my_list is called  which is 2  the last element of my_list is called  which is 11  slicing  calls the first two elements of my_list [2  3]  calls the last three elements [5  7  11]  appending  [2  3  5  7  11  13]  [2  3  5  7  11  13  17  19]  In[ ]:  adds three string elements to the list  Welcome  to  [‘Welcome'  'to'  ‘Python']  using for loop to iterate all elements in the list:  word we choose randomly  any other name works too  In[ ]:  to print it out in one line we need to apply the fct join ) to our list:  sets * as separators  Welcome*to*Python  sets white spaces as separators  Welcome to Python  Try to execute this code and see what happens.  slices string by indices  Welcome to P.  Try changing the indices to negative.  WELCOME TO PYTHON  ### **Operations**  In[ ]:  by using dir ) function  or help ) function; you may see all ops on the str object  In[ ]:  or a str instance  In[ ]:  likewise   ### **Dictionaries**  In[ ]:  {'name': 'DEDA'  'unit': 0}  alternative  accessing  0  assigning  get keys  ['name'  'unit']  ['DEDA'  1]  adding values  {'name': 'DEDA'  'unit': 1  'lecturers': ['Chen'  'Härdle']}  ### **If/elif/else**  In[ ]:  initialising a numeric variable x  checking for the value of x  note the four additional level of indentation  4 spaces)  In[ ]:  Conditions can be combined or altered with: and  or  not  is  is not  initialising a list p  True  False  True  Empty/Missing Values can be initialised by the term “None”  initialising an empty variable x  alternatively: if not y is None: ….  ### **For loop**  In[ ]:  output: 2 4 6 8 10  iterates from 0 to 5  skip 3*2 in output  output:  0 2 4 8 10  output: D E D A  a has value 1  b has value 2  ### **While eloop**  In[ ]:  Fibonacci series: sum of two preceding numbers defines next number  1 1 2 3 5 8 13 21 34 55 89  In[ ]:  a list called fib  appends the sum of the last 2 elements of our list to our new list  [0  1  1  2  3  5  8  13  21  34  55  89  144]  ### **Functions**  In[ ]: my_first_fct   remember the four indentation  call the function  another function with two parameter m and n my_second_fct m  n  call the function and pass the variables m = 4 and n = 3  call the function again and pass other variables  In[ ]: square_numeric x  Squares numeric x square_iterable x  Squares numerics in iterable x square_iterabel_short x  Squares numerics in iterable x  [1  4  9  16  25]  [1  4  9  16  25]  # **Application 1: Wiggling Elephant Trunk**  Wiggling Elephant Trunk:    vonNeuman_elephant.py      ""With four parameters I can fit an elephant          and with five I can make him wiggle his trunk.""      Original Versions:        Author[1]: Piotr A. Zolnierczuk  zolnierczukp at ornl dot gov)      Retrieved on 14 September 2011 from      http://www.johndcook.com/blog/2011/06/21/how-to-fit-an-elephant/    Modified to wiggle trunk:        2 October 2011 by David Bailey  http://www.physics.utoronto.ca/~dbailey)      Author[2]:      Advanced Physics Laboratory      https://www.physics.utoronto.ca/~phy326/python/  Based on the paper:        ""Drawing an elephant with four complex parameters""  by      Jurgen Mayer  Khaled Khairy  and Jonathon Howard       Am. J. Phys. 78  648  2010)  DOI:10.1119/1.3254017    The paper does not specify how the wiggle parameter controls the    trunk  so a guess was made.    Inspired by John von Neumann's famous quote  above) about overfitting data.        Attributed to von Neumann by Enrico Fermi  as quoted by      Freeman Dyson in ""A meeting with Enrico Fermi"" in      Nature 427  22 January 2004) p. 297    Python Version: 3.6    Modified based on author[2]'s work    Author: Junjie Hu    Overfiting problem in trading strategy stated:        Bailey  D.  Borwein  J.  Lopez de Prado  M.  & Zhu  Q.  2014).      Pseudo-mathematics and financial charlatanism: The effects of backtest overfitting on out-of-sample performance.    In[ ]:  import matplotlib  matplotlib.use 'TKAgg') matplotlib animation numpy append matplotlib IPython.display HTML  PLEASE NOTE IN SPYDER YOU SHOULD DISABLE THE ACTIVE SUPPORT in PREFs  elephant parameters  patrick's happy spermwhale  parameters = [30 - 10j  20 + 20j  40 + 10j  20 - 50j  -40 + 10j]  philipp's flying swan   parameters = [1 - 2j  9 + 9j  1 - 2j  9 + 9j  0 + 0j]  kathrin's hungry animal   parameters = [50 - 50j  30 + 10j  5 - 2j  -5 - 6j  20 + 20j]  anna’s happy hippo  parameters = [50 - 15j  5 + 2j  -10 - 10j  -14 - 60j  5 + 30j]  fabio’s bird with right wing paralysis  parameters = [50 - 15j  5 + 2j  -1 - 5j  -14 - 60j  18 - 40j]  for pea shooter see code below  fourier t  C elephant t  p init_plot   draw the body of the elephant & create trunk move_trunk i  move trunk to new position  but don't move eye stored at end or array)  initial the elephant body  initialize trunk  Uncomment if you would like to save video externally  # **Application 2: Wentian’s Pea Shooter**  Wentian’s pea shooter:    vonNeuman_elephant.py        ""With four parameters I can fit an elephant       and with five I can make him wiggle his trunk.""    Original Versions:        Author[1]: Piotr A. Zolnierczuk  zolnierczukp at ornl dot gov)      Retrieved on 14 September 2011 from      http://www.johndcook.com/blog/2011/06/21/how-to-fit-an-elephant/    Modified to wiggle trunk:        2 October 2011 by David Bailey  http://www.physics.utoronto.ca/~dbailey)      Author[2]:      Advanced Physics Laboratory      https://www.physics.utoronto.ca/~phy326/python/    Based on the paper:        ""Drawing an elephant with four complex parameters""  by      Jurgen Mayer  Khaled Khairy  and Jonathon Howard       Am. J. Phys. 78  648  2010)  DOI:10.1119/1.3254017  The paper does not specify how the wiggle parameter controls the  trunk  so a guess was made.    Inspired by John von Neumann's famous quote  above) about overfitting data.      Attributed to von Neumann by Enrico Fermi  as quoted by      Freeman Dyson in ""A meeting with Enrico Fermi"" in      Nature 427  22 January 2004) p. 297          Python Version: 3.6    Modified based on author[2]'s work    Author: Junjie Hu    Overfiting problem in trading strategy stated:        Bailey  D.  Borwein  J.  Lopez de Prado  M.  & Zhu  Q.  2014).      Pseudo-mathematics and financial charlatanism: The effects of backtest overfitting on out-of-sample performance.  """"""  In[ ]:  for peashooter:   import matplotlib  matplotlib.use 'TKAgg') matplotlib animation numpy append matplotlib fourier t  C peashooter t  p init_plot  move_trunk i  Video will be externally saved  plt.show )",1337
lda_analysis,perform LDA Analysis on given dataset,ERROR,Huong Vu 0856156,lda_analysis,2,!/usr/bin/env python  coding: utf-8  In[31]: random os re nltk os path nltk.stem WordNetLemmatizer nltk.stem.porter PorterStemmer nltk.corpus stopwords nltk.corpus stopwords nltk.stem.wordnet WordNetLemmatizer string gensim gensim corpora matplotlib pandas numpy  In[32]:  keep only letters  numbers and whitespace  apply regex  apply regex  lower case  nltk  Creating the term dictionary of our courpus  where every unique term is assigned an index.  Converting list of documents  corpus) into Document Term Matrix using dictionary prepared above. clean doc  Creating the term dictionary of our courpus  where every unique term is assigned an index.  Converting list of documents  corpus) into Document Term Matrix using dictionary prepared above.  In[33]:  Creating the object for LDA model using gensim library  Running and Trainign LDA model on the document term matrix.  In[34]:  In[35]:,122
DEDA_HClustering_quantlet,Hierarchical clustering on qunatlets,"Quantlets, hierarchical, k-means, cluster analysis",Elizaveta Zinovyeva,DEDA_HClustering_quantlets,2,!/usr/bin/env python  coding: utf-8  In[ ]:  install modules if they are not installed  In[ ]:  import modules json pprint pprint pandas numpy keras.preprocessing.text Tokenizer keras.preprocessing.sequence pad_sequences matplotlib pyplot scipy.cluster.hierarchy dendrogram scipy scipy.cluster.hierarchy fcluster sklearn.decomposition PCA sklearn.neighbors DistanceMetric sklearn.manifold TSNE sklearn.metrics.pairwise cosine_distances sklearn.metrics pairwise_distances  additional set up  suppress scientific float notation  constants and parameters  load data   In[ ]:  create data frame  In[ ]:  extract the columns we need  In[ ]:  add HClustering  In[ ]:  text preprocessing labels = [i.split '/')[1] for i in df.name]  In[ ]:  In[ ]:  In[ ]:  In[ ]:  In[ ]:  In[ ]: Z = shc.linkage train_X  method='average'  metric='cosine') clusters = fcluster Z  max_d  criterion='maxclust') print df.head 20))  In[ ]:  prepare for plotting  In[ ]:  In[ ]:  In[ ]:  In[ ]:  In[ ]: print df.head 20)) plt.legend loc = 'upper left'  prop={'size': 10})  In[ ]: print df.head 20)) plt.legend loc = 'upper left'  prop={'size': 10}) plt.legend loc='upper center'  bbox_to_anchor= 1.3  0.8)  prop={'size': 10}),155
AOBDL_data_preparation,"Antisocial Online behaivor detection. Contains data preparation procedure for traditional, deep learning models, including hierarchical attention model",ERROR,Elizaveta Zinovyeva,AOBDL_data_preparation,2,"!/usr/bin/env python  coding: utf-8  In[1]:  MODULES IMPORT  numpy pandas pdb os  In[2]:  GOOGLE COLAB SETUP  Load the Drive helper and mount google.colab drive  This will prompt for authorization.  In[3]:  VARIABLES AND PATHS 3. Read file as panda dataframe  In[11]:  Create target Directory if don't exist  In[4]: Combine labels and test set  In[5]:  Clean data completely  substitute_repeats_fixed_len text  nchars  ntimes=3 substitute_repeats text  ntimes=3 text_to_wordlist text  remove_stop_words=True  stem_words=False  with_punct_sent=False from string import punctuation re  not only but stopword anyway  r""'m""  'am')   or 'is' but both are stopwords  or 'would' but both are stopwords  Clean the text  with the option to remove stop_words and to stem words. print t) text = re.sub t[0]  t[1]  text)      Clean the text text               = re.sub r""What's""  """"  text) if with_punct_sent==False:     pass text = ''.join [c for c in text if c not in '!""#$%&\' )*+ -./:;<=>?@[\\]^_`{|}~']) else:     text = ''.join [c for c in text if c not in '!.?'])  Optionally  remove stop words  Optionally  shorten words to their stems  In[6]:  In[7]:  In[8]:  In[9]:",167
Blockchain_mechanism_plotting,"Use distributional characteristics such as fourier power spectrum, moments, quantiles, global we optimums, as well as the measures for long term dependencies, risk and noise to summarise the information from crypto time series and conduct clustering via spectral clustering","Cryptocurrency, Blockchain mechanism , Distributional characteristics, Clustering, Box plot","Kainat Khowaja , Min-Bin Lin,",Blockchain_mechanism_plotting,1," import all required packages matplotlib.lines Line2D seaborn pandas matplotlib matplotlib.lines Line2D os glob functools reduce  define a method to extract data for a given crypto get_crypto crypto  df_list  col_names  make date as index  col names  define a method for plotting time series heat map time_heatmap data  names  colors  plot_title  boxplot ""horizontal""  1*18) horizontal_boxplot  data  startTime  endTime  refine the data  extract data by desired   get rid of data col  handle na and data type  take log   #df = np.log df).replace -np.inf  0)  # df = df.apply lambda x: x/1000000)  plotting  melt the dataframe  set  figure layout  iterate over boxes  iterate over whiskers and median lines  plt.savefig 'boxplot_ver.png'  transparent=False  dpi=500)  boxplot ""vertical""  3*6) vertical_boxplot  data  startTime  endTime  n_rows  n_cols  refine the data  extract data by desired  get rid of data col  handle na and data type  take log   set up figure layout  plotting  marker style  interate over  plt.savefig 'boxplot_36.png'  transparent=False  dpi=500)  data loading  name reference  path to the directory of the all csv files  file  parameter) names  load data  day-based): list of dataframes  each dataframe for each variable)  give name attribute for each dataframe in the list",185
Understanding_Transaction_Graphs,Transaction graph analysis and temporal community detection for cryptocurriencies,"Transaction graph, crypto, network, temporal louvain, stable community, communities, bitcoin, dogecoin, litecoin, crytocurrencies",Rebecca Staniscia Koprik and Maria-Sophie Liess,Transaction-Graph,2,"!/usr/bin/env python  coding: utf-8  # Transaction data      https://chain.so/  # BITCOIN EXTRACTION  In[ ]: #################### ##### BITCOIN ###### #################### ### EXTRACTION ##### ####################  import packages requests tabulate pandas networkx numpy datetime  683093 - 683200: 11/05/2021   range block  replace as needed  transaction info  range transaction  transaction link  everything went swimmingly                                                                                                                     parse the response as JSON  transaction id  range  transaction link  everything went swimmingly parse the response as JSON  transaction_id  input leg  output leg  date  filling for different len  as data frame  clean data  obmit nulldata & reindex  fill NaN with previous data for all NaN  None  convert epoch time  add date_form to df  drop old column and rename  NOTE! We need to be aware of the following different transaction types:  Newly generated coins  identifiable via the participating address ""coinbase!  # BITCOIN INDICATORS OVER TIME  In[ ]: #################### ##### BITCOIN ###### #################### ### INDICATORS ##### #################### matplotlib  summarizes df  creates new var with vertices  consolidated edges and vertices data   plot  rename ticks    legend loc  save  In[ ]: #################### ##### BITCOIN ###### #################### ## INDICATORS II ### ####################  creates new col with edge to vert ratio  creates new col with to from ratio  creates dataframe  plot  rename ticks  legend loc  save  In[ ]: #################### ##### BITCOIN ###### #################### ## INDICATORS III ## ####################  density  plot  rename ticks  save  In[ ]: #################### ##### BITCOIN ###### #################### ## INDICATORS IV ### ####################  consolidated data with ratios  # BTC EXTRACTION - NETWORKX  In[ ]: #################### ##### BITCOIN ###### #################### #### NETWORKX ###### ####################  import packages requests tabulate pandas networkx numpy datetime  683093 - 683200: 11/05/2021   range block  replace as needed  transaction info  range transaction  transaction link  everything went swimmingly                                                                                                                     parse the response as JSON  transaction id  range  transaction link  everything went swimmingly parse the response as JSON  transaction_id  input leg  output leg  date  filling for different len  as data frame  clean data  obmit nulldata & reindex  fill NaN with previous data for all NaN  None  convert epoch time  add date_form to df  drop old column and rename  # BTC NETWORKX INDICATORS  In[ ]: #################### ##### BITCOIN ###### #################### #### NETWORKX ###### ####################  Prep for NetworkX  creates tuple list  create an empty graph  adds edges  plots  In[ ]: #################### ##### BITCOIN ###### #################### #### NETWORKX ###### #################### ## INDICATORS I #### ####################  Get information on the graph level  Number of vertices  Number of edges  Connectedness  A graph is connected if every node can be reached from any other node  Density  Transitivity  relating the number of triangles with the number of triples in the graph  Clustering coefficient  Average degree  average of all vertex degrees in the graph  summary table  In[ ]: #################### ##### BITCOIN ###### #################### #### NETWORKX ###### #################### ## INDICATORS II ### ####################  Degree distribution   degree of a node in a network is the number of connections it has to other   nodes and the degree distribution is the probability distribution of these   degrees over the whole network.  Remove plot frame line on the top   Remove plot frame line on the right  Remove ticks on the bottom  Remove the ticks on the left 75846b""  bins='auto');   TRY IT OUT WITH 7!  only very few vertices with high degrees  and many with relatively small degrees   -- a typical results for social networks. In such cases  we say that the degree   distribution is heavy tailed.  In[ ]: #################### ##### BITCOIN ###### #################### #### NETWORKX ###### #################### ## INDICATORS III ## ####################  Betweeness Centrality  a vertex is important when it connects two large communities  which would   remain unconnected if the vertex was not there.  Remove plot frame line on the top   Remove plot frame line on the right  Remove ticks on the bottom  Remove the ticks on the left 75846b""  bins='auto');  TRY IT OUT WITH BINS = 7  In[ ]: #################### ##### BITCOIN ###### #################### #### NETWORKX ###### #################### ## INDICATORS IV ### ####################  Get information on the vertex level  Neighborhood  neighborhood of vertex  vi  is the set of all vertices that are adjacent to  vi  Connectedness  Two vertices are connected of there is a path between then  Clustering coefficient  The clustering coefficient of a single vertex informs us about how well the   neighbors of the vertex are themselves connected. The maximum amount of   clustering is achieved if all neighbors of the vertex are neighbores as well  Degree   The degree of  vi  is just the number of adjacent vertices.  # DOGECOIN EXTRACTION  In[ ]: #################### ##### DOGECOIN ##### #################### ### EXTRACTION ##### #################### requests  3802040 - 3802147: 07/07/2021   range block  replace as needed  transaction info  range transaction  transaction link  everything went swimmingly  parse the response as JSON  transaction id  range  transaction link  everything went swimmingly parse the response as JSON  transaction_id  input leg  output leg  date  filling for different len  as data frame  clean data  obmit nulldata & reindex  fill NaN with previous data for all NaN  None  convert epoch time  add date_form to df  drop old column and rename  # DOGECOIN INDICATORS OVER TIME  In[ ]: #################### ##### DOGECOIN ##### #################### ### INDICATORS ##### #################### matplotlib  summarizes df  creates new var with vertices  consolidated edges and vertices data   plot  rename ticks    legend loc  In[ ]: #################### ##### DOGECOIN ##### #################### ## INDICATORS II ### ####################  creates new col with edge to vert ratio  creates new col with to from ratio  creates dataframe  plot  rename ticks    legend loc  In[ ]: #################### ##### DOGECOIN ##### #################### # INDICATORS III ### ####################  density  plot  rename ticks    legend loc  In[ ]: #################### ##### DOGECOIN ##### #################### ## INDICATORS IV ### ####################  consolidated data with ratios  # DOGECOIN EXTRACTION - NETWORKX  In[ ]: #################### ##### DOGECOIN ##### #################### #### NETWORKX ###### ####################  import packages requests tabulate pandas networkx numpy datetime  3802140 - 3802147: 07/07/2021   range block  replace as needed  transaction info  range transaction  transaction link  everything went swimmingly                                                                                                                     parse the response as JSON  transaction id  range  transaction link  everything went swimmingly parse the response as JSON  transaction_id  input leg  output leg  date  filling for different len  as data frame  clean data  obmit nulldata & reindex  fill NaN with previous data for all NaN  None  convert epoch time  add date_form to df  drop old column and rename  # DOGECOIN NETWORKX INDICATORS  In[ ]: #################### ##### DOGECOIN ##### #################### #### NETWORKX ###### ####################  Prep for NetworkX  creates tuple list  tuple  create an empty graph  adds edges  plots  In[ ]: #################### ##### DOGECOIN ##### #################### #### NETWORKX ###### #################### ## INDICATORS I #### ####################  Get information on the graph level  Number of vertices  Number of edges  Connectedness  A graph is connected if every node can be reached from any other node  Density  Transitivity  relating the number of triangles with the number of triples in the graph  Clustering coefficient  Average degree  average of all vertex degrees in the graph  summary table  In[ ]: #################### ##### DOGECOIN ##### #################### #### NETWORKX ###### #################### ## INDICATORS II ### ####################  Degree distribution   degree of a node in a network is the number of connections it has to other   nodes and the degree distribution is the probability distribution of these   degrees over the whole network.  Remove plot frame line on the top   Remove plot frame line on the right  Remove ticks on the bottom  Remove the ticks on the left 75846b""  bins='auto');  plt.xlim [0  10])  only very few vertices with high degrees  and many with relatively small degrees   -- a typical results for social networks. In such cases  we say that the degree   distribution is heavy tailed.  In[ ]: #################### ##### DOGECOIN ##### #################### #### NETWORKX ###### #################### ## INDICATORS III ## ####################  Betweeness Centrality  a vertex is important when it connects two large communities  which would   remain unconnected if the vertex was not there.  Remove plot frame line on the top   Remove plot frame line on the right  Remove ticks on the bottom  Remove the ticks on the left 75846b""  bins='auto');  In[ ]: #################### ##### DOGECOIN ##### #################### #### NETWORKX ###### #################### ## INDICATORS IV ### ####################  Get information on the vertex level  Neighborhood  neighborhood of vertex  vi  is the set of all vertices that are adjacent to  vi  Connectedness  Two vertices are connected of there is a path between then  Clustering coefficient  The clustering coefficient of a single vertex informs us about how well the   neighbors of the vertex are themselves connected. The maximum amount of   clustering is achieved if all neighbors of the vertex are neighbores as well  Degree   The degree of  vi  is just the number of adjacent vertices.  # LITECOIN EXTRACTION  In[ ]: ##################### ##### LITECOIN ###### ##################### #### EXTRACTION ##### #####################  import packages requests tabulate pandas networkx numpy datetime  2082158 - 2082265: 07/07/2021   range block  replace as needed  transaction info  range transaction  transaction link  everything went swimmingly  parse the response as JSON  transaction id  range  transaction link  everything went swimmingly parse the response as JSON  transaction_id  input leg  output leg  date  filling for different len  as data frame  clean data  obmit nulldata & reindex  fill NaN with previous data for all NaN  None  convert epoch time  add date_form to df  drop old column and rename  # LITECOIN INDICATORS OVER TIME  In[ ]: #################### ##### LITECOIN ##### #################### ### INDICATORS ##### #################### matplotlib  summarizes df  creates new var with vertices  consolidated edges and vertices data   plot  rename ticks    legend loc  In[ ]: #################### ##### LITECOIN ##### #################### ## INDICATORS II ### ####################  creates new col with edge to vert ratio  creates new col with to from ratio  creates dataframe  plot  rename ticks    legend loc  In[ ]: #################### ##### LITECOIN ##### #################### # INDICATORS III ### ####################  density  plot  rename ticks    legend loc  In[ ]: #################### ##### LITECOIN ##### #################### ## INDICATORS IV ### ####################  consolidated data with ratios  # LITECOIN EXTRACTION - NETWORKX  In[ ]: #################### #### LITECOIN ###### #################### #### NETWORKX ###### ####################  import packages requests tabulate pandas networkx numpy datetime  2082258 - 2082265: 07/07/2021   range block  replace as needed  transaction info  range transaction  transaction link  everything went swimmingly                                                                                                                     parse the response as JSON  transaction id  range  transaction link  everything went swimmingly parse the response as JSON  transaction_id  input leg  output leg  date  filling for different len  as data frame  clean data  obmit nulldata & reindex  fill NaN with previous data for all NaN  None  convert epoch time  add date_form to df  drop old column and rename  # LITECOIN NETWORKX INDICATORS  In[ ]: #################### #### LITECOIN ###### #################### #### NETWORKX ###### ####################  creates tuple list  tuple  create an empty graph  adds edges  plots  In[ ]: #################### #### LITECOIN ###### #################### #### NETWORKX ###### #################### ## INIDICATORS I ### ####################  Get information on the graph level  Number of vertices  Number of edges  Connectedness  A graph is connected if every node can be reached from any other node  Density  Transitivity  relating the number of triangles with the number of triples in the graph  Clustering coefficient  Average degree  average of all vertex degrees in the graph  summary table  In[ ]: #################### #### LITECOIN ###### #################### #### NETWORKX ###### #################### # INIDICATORS II ### ####################  Degree distribution   degree of a node in a network is the number of connections it has to other   nodes and the degree distribution is the probability distribution of these   degrees over the whole network.  Remove plot frame line on the top   Remove plot frame line on the right  Remove ticks on the bottom  Remove the ticks on the left 75846b""  bins='auto');  only very few vertices with high degrees  and many with relatively small degrees   -- a typical results for social networks. In such cases  we say that the degree   distribution is heavy tailed.  In[ ]: #################### #### LITECOIN ###### #################### #### NETWORKX ###### #################### # INIDICATORS III ## ####################  Betweeness Centrality  a vertex is important when it connects two large communities  which would   remain unconnected if the vertex was not there.  Remove plot frame line on the top   Remove plot frame line on the right  Remove ticks on the bottom  Remove the ticks on the left 75846b""  bins='auto');  In[ ]: #################### #### LITECOIN ###### #################### #### NETWORKX ###### #################### # INIDICATORS IV ### ####################  Get information on the vertex level  Neighborhood  neighborhood of vertex  vi  is the set of all vertices that are adjacent to  vi  Connectedness  Two vertices are connected of there is a path between then  Clustering coefficient  The clustering coefficient of a single vertex informs us about how well the   neighbors of the vertex are themselves connected. The maximum amount of   clustering is achieved if all neighbors of the vertex are neighbores as well  Degree   The degree of  vi  is just the number of adjacent vertices.  # BITCOIN EXTRACTION - TEMPORAL COMMUNITY  In[ ]: #################### ##### BITCOIN ###### #################### ### EXTRACTION ##### #################### ### COMM FORMAT #### ####################  import packages requests tabulate pandas networkx numpy datetime  682950 - 683238: 11/05/2021   range block  transaction info  range transaction  transaction link  everything went swimmingly                                                                                                                     parse the response as JSON  transaction id  range  transaction link  everything went swimmingly parse the response as JSON  transaction_id  input leg  output leg  date  filling for different len  as data frame  clean data  obmit nulldata & reindex  fill NaN with previous data for all NaN  None  rename columns to keep  drop unecessary columns  dictionary for convertion to int  to_id range range  creates key id  as dataframe  from_id range range  creates key id  as dataframe  merges dfs  drop columns and rename to keep  factorise timestamps  as int  re order  save  # BITCOIN STABLE COMMUNITY  In[ ]: #################### ##### BITCOIN ###### #################### ### STABLE COM ##### ####################  mounts drive google.colab drive  In[ ]:  https://github.com/VeryLargeGraph/TSCAN  runs run.py - TSCAN  In[ ]:  prints stable communities - STATIC ONES  In[ ]:  Represent Stable Community networkx  importing matplotlib.pyplot matplotlib  [46  1086  1600  1601  47]\n   [2257  2530  2531  2551  2553]\n [68  229  208  209  69  102]\n  drawing in spring layout 75846b"")  In[ ]:  # BITCOIN TEMPORAL LOUVAIN  In[ ]: #################### ##### BITCOIN ###### #################### ### TEMP MEAS ###### ####################  imports teneto teneto teneto TemporalNetwork  define tnet  rename columns to keep  temporal louvain  as dataframe  In[ ]:  score for stable community  as int  summarizes df  summary_temp_louv_bc = summary_temp_louv_bc.drop columns = 'same')  number of stable communities  # DOGECOIN EXTRACTION - TEMPORAL COMMUNITY  In[ ]: #################### ##### DOGECOIN ##### #################### ### EXTRACTION ##### #################### ### COMM FORMAT #### ####################  import packages requests tabulate pandas networkx numpy datetime  3802140 - 3802147: 07/07/2021   range block  replace as needed  transaction info  range transaction  transaction link  everything went swimmingly                                                                                                                     parse the response as JSON  transaction id  range  transaction link  everything went swimmingly parse the response as JSON  transaction_id  input leg  output leg  date  filling for different len  as data frame  clean data  obmit nulldata & reindex  fill NaN with previous data for all NaN  None  rename columns to keep  drop unecessary columns  dictionary for convertion to int  to_id range range  creates key id  as dataframe  from_id range range  creates key id  as dataframe  merges dfs  drop columns and rename to keep  factorise timestamps  as int  re order  save  # DOGECOIN TEMPORAL LOUVAIN  In[ ]: #################### ##### DOGECOIN ##### #################### ### TEMP MEAS ###### ####################  imports teneto teneto teneto TemporalNetwork  define tnet  rename columns to keep  temporal louvain  as dataframe  In[ ]:  score for stable community  as int  summarizes df  number of stable communities  # DOGECOIN STABLE COMMUNITY  In[ ]: #################### ##### DOGECOIN ##### #################### ### STABLE COM ##### ####################  mounts drive google.colab drive  In[ ]:  https://github.com/VeryLargeGraph/TSCAN  runs run.py - TSCAN  In[ ]:  prints stable communities - STATIC ONES  In[ ]:  Represent Stable Community networkx  importing matplotlib.pyplot matplotlib  [46  1086  1600  1601  47]\n   [2257  2530  2531  2551  2553]\n  drawing in spring layout 75846b"")  # LITECOIN EXTRACTION - TEMPORAL COMMUNITY    In[ ]: #################### ##### LITECOIN ##### #################### ### EXTRACTION ##### #################### ### COMM FORMAT #### ####################  import packages requests tabulate pandas networkx numpy datetime  2082258 - 2082265: 07/07/2021   range block  replace as needed  transaction info  range transaction  transaction link  everything went swimmingly                                                                                                                     parse the response as JSON  transaction id  range  transaction link  everything went swimmingly parse the response as JSON  transaction_id  input leg  output leg  date  filling for different len  as data frame  clean data  obmit nulldata & reindex  fill NaN with previous data for all NaN  None  rename columns to keep  drop unecessary columns  dictionary for convertion to int  to_id range range  creates key id  as dataframe  from_id range range  creates key id  as dataframe  merges dfs  drop columns and rename to keep  factorise timestamps  as int  re order  save  # LITECOIN STABLE COMMUNITY  In[ ]: #################### ##### LITECOIN ##### #################### ### STABLE COM ##### ####################  mounts drive google.colab drive  In[ ]:  https://github.com/VeryLargeGraph/TSCAN  runs run.py - TSCAN  In[ ]:  prints stable communities - STATIC ONES  In[ ]:  Represent Stable Community networkx  importing matplotlib.pyplot matplotlib  [2257  2530  2531  2551  2553]\n  drawing in spring layout 75846b"")  # LITECOIN TEMPORAL LOUVAIN  In[ ]: #################### ##### LITECOIN ##### #################### ### TEMP MEAS ###### ####################  imports teneto teneto teneto TemporalNetwork  define tnet  rename columns to keep  temporal louvain  IndexError: index 408424201934 is out of bounds for axis 0 with size 1006740  as dataframe  In[ ]:  score for stable community  as int  summarizes df  number of stable communities  # NETWORK GRAPH - REPRESENTATION  In[ ]: numpy matplotlib networkx matplotlib.animation FuncAnimation  number of nodes  generate graph  generating input frames here  since my data is too big  its important that the frames go as input and is not generated  on the fly  random ndarray between 0 and 5  length and number of frames = number of nodes in the graph  draw the topology of the graph  what changes during animation  is just the color  pass frames to funcanimation via update function  this is where I get stuck  since I cannot break  out of the loop  neither can I read every array of  the ndarray without looping over it explicitly update i  for i in range len frame)):  instead of giving frame as input  if I randomly generate it  then it works  np.random.randint 2  size=200)  output animation; its important I save it  In[ ]:  Network graph networkx  importing matplotlib.pyplot matplotlib  drawing in spring layout 75846b"")",2958
pyTSA_SimAR2,This Quantlet simulates and plots AR(2) time series and its ACF and PACF.,"time series,  stationarity, autocorrelation, PACF, ACF, simulation, stochastic process, ARMA",ERROR,pyTSA_SimAR2,1,numpy pandas matplotlib statsmodels.tsa.arima_process arma_generate_sample PythonTsa.plot_acf_pacf acf_pacf_fig pandas.plotting lag_plot  ma = [1] means no ma part in the model,19
CV03_Scraping_Market,Scrapes market data for bitcoin from cryptocompare.com using API access,"scraping, bitcoin, time series, crypto, api",Fabian Schmidt,CV03_Scraping_Market,1,requests argparse pandas parse_args ,4
CrypOpt_RNDandHD,Compares the risk neutral density and the historical density of bitcoin prices. For more detailed description of estimation methods of densities see Quantlets CrypOpt_RiskNeutralDensity and CrypOpt_HistoricalDensity.,"crypto options, cryptocurrency, historical density, physical density, risk neutral density, state price density, Rookley, pricing kernel",Franziska Wehrmann,CrypOpt_RNDandHD,5,numpy scipy.stats norm rnd_appfinance M  S  K  o  o1  o2  r  tau,12
CrypOpt_RNDandHD,Compares the risk neutral density and the historical density of bitcoin prices. For more detailed description of estimation methods of densities see Quantlets CrypOpt_RiskNeutralDensity and CrypOpt_HistoricalDensity.,"crypto options, cryptocurrency, historical density, physical density, risk neutral density, state price density, Rookley, pricing kernel",Franziska Wehrmann,CrypOpt_RNDandHD,5,os pandas numpy matplotlib pyplot matplotlib.pyplot cm smoothing locpoly_smoothing risk_neutral_density rnd_appfinance expand expand_X historical_density sampling  ------------------------------------------------------------------------ MAIN  ---------------------------------------------------------- LOAD DATA ---- RND  filter out Moneyness bigger than 1.3  filter out Moneyness small than 0.7  ---------------------------------------------------------------- LOAD DATA HD  --------------------------------------------------------------------- 3D PLOT  -------------------------------------------------------------- SPD NORMAL  ---------------------------------------------------------------- B-SPLINE  derivatives  ---------------------------------------------------------------------- HD  --------------------------------------------------------------------- 2D PLOT plot_2d d  day  tau_day  -------------------------------------------------------------- SPD NORMAL  ------------------------------------------------------ EXTENSION B-SPLINE  derivatives  ---------------------------------------------------------------------- HD  red: RND  blue: HD,69
CrypOpt_RNDandHD,Compares the risk neutral density and the historical density of bitcoin prices. For more detailed description of estimation methods of densities see Quantlets CrypOpt_RiskNeutralDensity and CrypOpt_HistoricalDensity.,"crypto options, cryptocurrency, historical density, physical density, risk neutral density, state price density, Rookley, pricing kernel",Franziska Wehrmann,CrypOpt_RNDandHD,5,numpy scipy.stats norm scipy  B-Spline _gaussian_kernel M  m  h_m  T  t  h_t _epanechnikov M  m  h_m  T  t  h_t _local_polynomial df  m  t  h_m  h_t  kernel=_gaussian_kernel locpoly_smoothing df  tau  h_m  h_t=0.05  gridsize=50  kernel='epak' bspline M  smile  sections  degree=3,38
CrypOpt_RNDandHD,Compares the risk neutral density and the historical density of bitcoin prices. For more detailed description of estimation methods of densities see Quantlets CrypOpt_RiskNeutralDensity and CrypOpt_HistoricalDensity.,"crypto options, cryptocurrency, historical density, physical density, risk neutral density, state price density, Rookley, pricing kernel",Franziska Wehrmann,CrypOpt_RNDandHD,5,numpy math expand_spline spline  edge  degree  left_x  middle_x  right_x expand_X X  add_left  add_right expand smile  first  second  M  S  K  edge=0.4  Moneyness  first  first  second,25
CrypOpt_RNDandHD,Compares the risk neutral density and the historical density of bitcoin prices. For more detailed description of estimation methods of densities see Quantlets CrypOpt_RiskNeutralDensity and CrypOpt_HistoricalDensity.,"crypto options, cryptocurrency, historical density, physical density, risk neutral density, state price density, Rookley, pricing kernel",Franziska Wehrmann,CrypOpt_RNDandHD,5,numpy sklearn.neighbors KernelDensity sampling data  target  tau_day  S0  M=10000 density_estimation sample  S  h  kernel='epanechnikov',14
HedgingCrix_models_paths,"Simulation of Euler discretized hedge models including the Geometric Brownian Motion (Black-Scholes), Merton Jump Diffusion and the dynamics under Heston's Stochastic Volatility Model.","Black Scholes, Merton jump diffusion, Stochastic Volatility, Heston Model, volatility, process, Euler discretization, jumps",Jovanka Lili Matic,HedgingCRIX,2,"!/usr/bin/env python  coding: utf-8  In[127]: install pandas install pandas math numpy scipy scipy integrate numpy scipy matplotlib numpy datetime pandas mpl_toolkits.mplot3d Axes3D numpy.fft seaborn seaborn.set_style ""ticks"")  In[256]:  In[258]: Geometric Brownian Motion  In[259]: Merton Jump Diffusion Model  In[261]:  In[262]:  In[263]: Heston stochastic volatility   In[264]:  In[266]:  In[ ]:  In[ ]:  In[ ]:",50
spatial_analysis_geoplot_transport,Map geographic linestring representations of public transport lines in the city of Berlin (data from VBB),"Spatial Mapping, Geospatial Analysis, Berlin City Data, GeoPlot, GeoPandas","Alex Truesdale, Susan",spatial_analysis_geoplot_transport,2,!/usr/bin/env python  coding: utf-8  # Read Data  In[1]: geoplot geoplot geopandas pandas contextily matplotlib matplotlib shapely shapely wkt warnings  ## PLZ Data  In[2]:  ## Transport Data  In[3]:  ## Bounding Data  In[4]:  # Define Colourmap  In[5]: matplotlib cm __init__ self  cmap_name  start_val  stop_val get_rgb self  val  In[6]:  In[7]:  In[8]:  # Plot Maps  In[9]:  In[10]:  In[11]:  In[12]:,55
Crypto_Data_Collection,use binance api to collect crypto currency data,"crypto, request, binance","Wang Wenyi, Liu Zekai, Tang Jiayun, Zhang Lin, Chen Jing",Crypto_Data_Collection,2,!/usr/bin/env python  coding: utf-8  In[121]: requests time pandas  maximum enquiry per request  means 29 x 1000 = 29000 hours ; in 3 years we have around 365x3x24=26280 hours  2021/3/22 00:00:00 UTC  2021/3/22 08:00:00 Singapore time) initialize  store the enquiry result  reset start time and end time store the csv file and dataframe  choose the data from 2018-03-22 00:00:00 to 2021-03-22 00:00:00  In[123]:  In[124]:  In[126]:  In[ ]:,66
LLE_AOBDL,Plotting the Antisocial Online Behavior using LLE,ERROR,Elizaveta Zinovyeva,LLE_AOBDL,2,!/usr/bin/env python  coding: utf-8  In[48]: numpy pandas pdb os keras.preprocessing.text Tokenizer keras.preprocessing.sequence pad_sequences sklearn.manifold TSNE sklearn.decomposition PCA sklearn.feature_extraction.text TfidfVectorizer matplotlib pyplot  In[49]:  In[50]:  In[51]: Combine labels and test set  In[52]:  In[53]:  In[54]:  In[55]:  In[56]:  prepare for plotting  In[57]:  In[58]: colors row  In[59]:  In[60]:  In[61]:  prepare for plotting  In[62]:  In[63]:  In[64]:  In[ ]:,52
PDRPC_simulation_study,"Simulation Study: Simulate Time Series, calculate 12 risk measuring variables and use them as input for PCA/Factor Analysis. Cluster based on Principal Components/Factors and calculate the Adjusted Rand Index to see whether the method works.","simulation, clustering, time series, PCA, Factor Analysis, Adjusted Rand Index",Judith Bender,PDRPC_simulation_study,2,!/usr/bin/env python  coding: utf-8  # Install and load packages  In[ ]:  In[ ]:  In[ ]:  In[ ]:  navigate to atalaia directory  get modifications made on the repo  install packages requirements !pip install -r requirements.txt  install package  In[ ]:  In[ ]:  Import packages pandas numpy scipy scipy.stats norm sklearn sklearn.linear_model LinearRegression sklearn.cluster KMeans sklearn.preprocessing StandardScaler sklearn metrics sklearn.metrics.cluster adjusted_rand_score sklearn.decomposition PCA matplotlib mpl_toolkits.mplot3d Axes3D random seaborn factor_analyzer nolds hurst_rs levy  # Preparation  In[ ]: Number of Simulations time  i.e. number of observations number of time series number of features  number of clusters: in simulation we know the true value. With real world data we have to try different ones. number of models  In[ ]: To store the features Adj_Rand: to save the Adj_Rand_Scores  In[ ]: km model  X  y agc X  y  # Simulate data and calculate features  In[ ]: n+1 because we need market proxy TS loc=mean  scale=standard deviation market proxy TS: average over all Calculate features CAPM beta Data.loc[j 'alpha']=reg.intercept_ #alpha only if necessary  'autocor'  'Hurst'  'stab_a'  'stab_g' autocorrelation coefficient Hurst using nolds package fit levy stable distribution alpha stable gamma stable  # Compute PCA/Factor Analysis and calculate Adjusted Rand Index  In[ ]: Scale the data Clustering PCA  in this case necessary to use scaled data)  then Clustering Factor Analysis  then clustering Factor Analysis with 2 factors Factor Analysis with 3 factors FA with 2 factors and Varimax rotation FA with 3 factors and Varimax rotation,237
drawing_animal,draw moving animal,ERROR,Huong Vu 0856156,drawing_animal,2,!/usr/bin/env python  coding: utf-8  In[1]: matplotlib matplotlib animation numpy append matplotlib  In[5]: parameters fourier t  C len f) = len t) hippo t  p x: concatenate fourier t  Cy) series  with len t)) and a number [p[4].imag]  so len x)=t+1; same as to y  For function–animation.FuncAnimation init_plot   draw the body part and save len x)=len y) = 1000+1  y series are still the same after updating  since sin float 0))=0 trunk: the initialized plot  predefined); trunk.set_data x  y): updated plot by drawing  x  y) move_nose i  In[6]: IPython.display HTML  initialize trunk frames: frame number = i Delay between frames in milliseconds; total time = frames* interval/1000)=100  plt.show )  In[7]:  In[ ]:,111
ChristmasTree,A pretty Christmas Tree Gif made in Python,"Quantlet, Christmas, Gif, Quantinar, Holidays",Raul Bag,ChristmasTree,2,logging os turtle random gif_creator GIFCreator draw_circle_image image  size  x  y  method 2: use class  If you want to create a class  just define your draw function  and then record it.  optional draw self,34
ChristmasTree,A pretty Christmas Tree Gif made in Python,"Quantlet, Christmas, Gif, Quantinar, Holidays",Raul Bag,ChristmasTree,2,turtle tkinter typing Callable pathlib Path re os sys functools PIL PIL.ImageFile ImageFile PIL EpsImagePlugin init **options  https://anzeljg.github.io/rin2/book2/2405/docs/tkinter/cap-join-styles.html  change the default style of the line that made of two connected line segments  default is ROUND  # https://anzeljg.github.io/rin2/book2/2405/docs/tkinter/create_line.html make_gif image_list: List[Path]  output_path: Path  **options  int  if 0  then loop forever. Otherwise  it means the loop number.  The time gap that you pick image after another on the recording. i.e.  If the value is low  then you can get more source image  so your GIF has higher quality.  millisecond.  # 1000 / FPS __init__ self  name  temp_dir: Path = None  duration: int = None  **options  True  it's ok when parents is not exists name self duration self temp_dir self configure self  **options  type check record self  draw_func: Callable = None  **options wrap   https://blog.csdn.net/lingyu_me/article/details/105400510  Does a turtle.clear ) and then resets this turtle's state  i.e. direction  position etc.)  init start the recording  start immediately convert_eps2image self make_gif self  output_name=None  **options  open the output folder _start self _stop self _save self  print self.__counter)  0001.eps  0002.eps ...  trigger only once  so we need to set it again.,182
loadcrix,Load BTC options from a CSV file and plot in multiple colors.,ERROR,Jovanka Lili Matic,LoadBTC,2,!/usr/bin/env python  coding: utf-8  In[4]: numpy pandas matplotlib  In[5]:  In[6]:  In[23]:  In[50]:  In[49]:  In[ ]:,15
CV02_Scraping_Trends,Scrapes google trends data for keywords bitcoin and BTC using API access,"scraping, bitcoin, time series, crypto, api, google, trends",Fabian Schmidt,CV02_Scraping_Trends,1, Since pytrends is returning a DataFrame object  we need pandas: pandas  Import of pytrends  needs to be pip installed first): pytrends.request TrendReq,22
DEDA_Class_2019SS_Crypto_News_Sentiment_Sentiment_Scoring,Sentiment scoring tool which takes the output of the news source scraping tool and applies to them the unsupervised VADER and TextBlob sentiment methods.,"Sentiment Analysis, VADER, TextBlob, Cryptocurrency",Alex Truesdale,DEDA_Class_2019SS_Crypto_News_Sentiment_Sentiment_Scoring,3,sys requirements sentiment_scorer SentimentScorer main ,5
DEDA_Class_2019SS_Crypto_News_Sentiment_Sentiment_Scoring,Sentiment scoring tool which takes the output of the news source scraping tool and applies to them the unsupervised VADER and TextBlob sentiment methods.,"Sentiment Analysis, VADER, TextBlob, Cryptocurrency",Alex Truesdale,DEDA_Class_2019SS_Crypto_News_Sentiment_Sentiment_Scoring,3,sentiment_base pandas nltk numpy nltk.sentiment.vader SentimentIntensityAnalyzer textblob TextBlob __init__ self sentiment_scorer self  stories_sentences scorer_handler self  frame  name save_frames self,19
DEDA_Class_2019SS_Crypto_News_Sentiment_Sentiment_Scoring,Sentiment scoring tool which takes the output of the news source scraping tool and applies to them the unsupervised VADER and TextBlob sentiment methods.,"Sentiment Analysis, VADER, TextBlob, Cryptocurrency",Alex Truesdale,DEDA_Class_2019SS_Crypto_News_Sentiment_Sentiment_Scoring,3,pandas time file_reader source story_filter frame,6
EPF,Hybrid model for Electricity Price Forecasting EPF,"Electricity Price Forecasting, Machine Learning models, Statistical methods, data decomposition, Accuracy",Souhir Ben Amor,EPF,3,"!/usr/bin/env python  coding: utf-8  <a href=""https://colab.research.google.com/github/QuantLet/EPF/blob/main/Multivariate_Random_Forest.ipynb"" target=""_parent""><img src=""https://colab.research.google.com/assets/colab-badge.svg"" alt=""Open In Colab""/></a>  In[ ]: pandas matplotlib numpy keras Sequential keras.layers Dense tensorflow keras sklearn.preprocessing MinMaxScaler sklearn.preprocessing LabelEncoder sklearn.metrics mean_squared_error sklearn.ensemble RandomForestRegressor keras.models Sequential keras.layers Dense keras.layers LSTM keras.layers Conv1D keras.layers GRU numpy  In[ ]:  df = df.sort_values df['datetime']  ascending=[True])  In[ ]: table2lags table  max_lag  min_lag=0  separator='_'  In[ ]:  In[ ]:  In[ ]:  In[ ]:  In[ ]:  In[ ]:  In[ ]:  In[ ]:  In[ ]:  In[ ]:  In[ ]:  In[ ]:  In[ ]:  In[ ]:  In[ ]:  In[ ]:  In[ ]:  In[ ]:  In[ ]: sklearn.model_selection GridSearchCV  Number of trees in random forest  n_estimators = [int x) for x in np.linspace start = 200  stop = 2000  num = 10)]  Number of features to consider at every split  max_features = ['auto'  'sqrt']  Maximum number of levels in tree  max_depth = [int x) for x in np.linspace 10  110  num = 11)]  max_depth.append None)  Minimum number of samples required to split a node  min_samples_split = [2  5  10]  Minimum number of samples required at each leaf node  min_samples_leaf = [1  2  4]  Method of selecting samples for training each tree  bootstrap = [True  False]  Create the random grid  print gridsearch)  {'bootstrap': [True]    'max_depth': [10  50  250 1000]    'max_features': [2  'sqrt']    'min_samples_leaf': [1  4]    'min_samples_split': [2  5  10]    'n_estimators': [200  1000  2000]}  In[ ]:  Random search of parameters  using 3 fold cross validation    search across 100 different combinations  and use all available cores  In[ ]:  In[ ]:  In[ ]:  In[27]:  In[28]:  In[29]:  In[30]:  In[31]:  In[32]:  In[47]:  In[48]:  In[49]:  In[51]:  np.mean prediciton['% error'])  In[52]:  In[53]: numpy inf  In[54]:  In[56]:  In[57]:  In[58]:  In[59]:  In[60]:  In[64]:  Save prediction as CSV pandas pandas read_csv csv",277
EPF,Hybrid model for Electricity Price Forecasting EPF,"Electricity Price Forecasting, Machine Learning models, Statistical methods, data decomposition, Accuracy",Souhir Ben Amor,EPF,3,[1] auto_ml Predictor from auto_ml.utils import get_boston_dataset auto_ml.utils_models load_ml_model PyEMD EEMD pip install PeakUtils sklearn.preprocessing MinMaxScaler sklearn.model_selection train_test_split sklearn.metrics r2_score matplotlib os scipy.integrate odeint keras keras.models keras.layers keras.optimizers keras.callbacks keras backend [2] series_to_supervised data  n_in  n_out  dropnan=True  input sequence  t-n  ... t-1)  forecast sequence  t  t+1  ... t+n)  put it all together  drop rows with NaN values [3]  pandas numpy matplotlib matplotlib [4] numpy [5] numpy [6] [7] Scale the Data [8] [9]  before you do the EMD  cut out the out of sample part so that the EMDs are not constructed with those future values and information contained within them [10] [11] eemd for in-sample [12] eemd for in-sample [13],111
SDA_2021_NUS_Markov_Extract,Generate the extract using Markov,"NUS, FE5225, Markov, HMM",Sun Rongsheng,SDA_2021_NUS_Markov_Extract,2,"!/usr/bin/env python  coding: utf-8  In[2]:  * present here an example showing generation of arXiv article titles and abstract from existing ones. The end result could fool the laymen  as we will see.  In[3]: numpy pandas markovify ready-to-use text Markov chain  In[4]: concatenating titles and abstract in new column  In[5]: number of words defining a state in the text Markov chain generating a model for all the text and one only for titles  We picked states composed of two words. It's a compromise between variance  ""creativity"" of the word generation  and the requirement of a sensible output  respectively small and large state_size.  # Generating random article titles + abtracts  In[6]: findnth  str  char="" ""  n=2  In[7]: some make_sentence calls raise a KeyError exception for misunderstood reasons first generating a title generating abstract from the end of the tile concatenating both  In[ ]:",141
StockCorr_Data_Downloader,Uses python script to download NASDAQ and NYSE mega/large/mid/small cap stock daily price data from Yahoo!Finance,"stock, data, downloader, correlation, clustering, concurrency","Li Yilin, Mei Yuxin, Sun Qingwei, Xie Chuda, Zhang Yingxin",StockCorr_Data_Downloader,2,requests re json time run symbol  initial  final downloader symbol  start  end  s = ['^VIX'  '^GSPC'  '^HSI'  '^IXIC'  '^VIX3M'  '^VXN'  '^VXO'],21
StockCorr_Data_Downloader,Uses python script to download NASDAQ and NYSE mega/large/mid/small cap stock daily price data from Yahoo!Finance,"stock, data, downloader, correlation, clustering, concurrency","Li Yilin, Mei Yuxin, Sun Qingwei, Xie Chuda, Zhang Yingxin",StockCorr_Data_Downloader,2,time logging downloader os argparse concurrent futures read_symbols symfile download_one symbol  logging.info 'already downloaded  skip') download_many symbols: list[str] main ,19
Data_Preprocessing,Preprocessing the cryptocurrency data in such a way that it fits the requirements of LSTM,"cryptocurrency, data, preprocessing","Georg Velev, Iliyana Pekova",Data_Preprocessing,1,numpy Spurce: https://machinelearningmastery.com/how-to-develop-lstm-models-for-time-series-forecasting/  split a univariate sequence into samples split_sequence sequence  n_steps_in  n_steps_out  find the end of this pattern  check if we are beyond the sequence  gather input and output parts of the pattern Example Sequence:  ,36
CoinGeckoCrawler,"Scrapes Data on Market Capitalization, Price and Volume of all coins listed at CoinGecko.com, using the API provided by CoinGecko","Scraper, Crawler, CoinGecko, Cryptocurrencies, Marketcapitalization","Konstantin Häusler, Junjie Hu, Vanessa Emmanuela Guarino",CoinGeckoCrawler,1,pycoingecko CoinGeckoAPI pandas time datetime datetime functools reduce __init__ self coin_api self  tries  delay  backoff  *args get_coins_name_id dataframe_coin create_dataframe coin  coin_id  minimum_date  maximum_date main ,24
Ms_Th_Predict_Poll_From_Emotions,Plots the descriptive statistics of the Master Thesis : Predicting Poll trends with facial emotions. Open with Jupyter Notebook.,"Neural Network, Multi Layer Perceptron, French Presidential Election 2017, Microsoft Emotion API",Victor Cluzel,Ms_Th_Predict_Poll_From_Emotions,2,!/usr/bin/env python  coding: utf-8  In[20]: ###################################################################################### ### 4_Descriptive statistics ###################################################################################### ## This code allows to generate the plots for the descriptive statistics ###################################################################################### ## RUN WITH PYTHON 3 ### ## Just change the directory for loading the file at the beginning of code ### os numpy matplotlib  In[21]: here change directory  In[22]:  In[23]: generate the graph for all emotions in the database graph_emo db_emo  the x locations for the groups  the width of the bars  In[24]:  In[25]: generate the graph for main emotions in the database graph_emo_main db_emo  the x locations for the groups  the width of the bars  In[26]:  In[29]: pandas  In[30]:  In[39]: generate the graph of all emotions for a specific candidate graph_emo_cand db_emo  the x locations for the groups  the width of the bars  In[32]:  In[33]:  In[34]:  In[35]:  In[36]:  In[37]: generate the graph comparing all emotions of all candidates graph_emo_cand_all db_emo i  the x locations for the groups  the width of the bars  In[38]:  In[40]: generate the graph of main emotions for a specific candidate graph_emo_main_cand db_emo  the x locations for the groups  the width of the bars  In[41]:  In[42]:  In[43]:  In[44]:  In[45]:  In[46]: generate the graph comparing main emotions of all candidates graph_emo_main_cand_all db_emo i  the x locations for the groups  the width of the bars  In[47]:  In[49]:  values  14th column as index  ie the sign of the poll trend  In[51]: plot according to the poll trends : upward trend  main emotions  the x locations for the groups  the width of the bars  In[52]: plot according to the poll trends :downward trend  main emotions  the x locations for the groups  the width of the bars  In[ ]:  In[53]: plot according to the poll trends : upward trend  all emotions  the x locations for the groups  the width of the bars  In[54]: plot according to the poll trends : downward trend  all emotions  the x locations for the groups  the width of the bars,317
pyTSA_SimGauss,"This Quantlet draws 1000 random numbers  from the normal  distribution, produces histogram and time series plot","random, distribution, Gaussian, normal","Huang Changquan, Alla Petukhina",pyTSA_SimGauss,1,numpy pandas matplotlib,3
SDA_20201031_hw4_Cutoffpoint,Calculating the probability of a “normal (distribution)” to be outside the upper cutoff point,ERROR,Andreas Rony Wijaya,SDA_20201031_hw4_Cutoffpoint,1,!/usr/bin/env python  coding: utf-8  In[1]: matplotlib numpy pylab  In[2]:  In[3]:  In[4]:  In[5]:  In[6]:  In[7]:  In[8]:  In[9]:  In[10]: scipy stats  In[11]:  In[ ]:,22
LOBDeepPP_MSE_computation_L,"Joint Limit order book ask and bid price predition for multiple predicton horizons with a neural networks. This script computes the MSE loss for the trained model for the fixed time lag application for training, validation and test dataset.","Limit order book, finance, forecasting, prediction, MSE, evaluation",Marius Sterling,LOBDeepPP_MSE_computation_L,1,!/usr/bin/env python3  -*- coding: utf-8 -*- os json_tricks numpy sys LOB.LOB_keras_train_class LOB_keras_train_class  %%  %%  %%,15
SFEtimewn,Plots the time series of a Gaussian white noise.,"distribution, normal, normal-distribution, simulation, stochastic-process, stochastic, process, white noise, gaussian, time-series, plot, graphical representation",Joanna Tomanek,QID-603-SFEtimewn,1,numpy matplotlib set seed to have reproducible results sample 1000 normally distributed noise terms create a plot,17
Financial Report Analysis BERT W2V GLOVE,To extract and via word embedding methods to visualise financial reports.,"BERT, w2v, GLOVE, text analysis, LLE, dimension reduction",Francis Liu,BERT_financialReportAnalysis,10,"typing Any torch transformers BertTokenizer nltk nltk.tokenize sent_tokenize numpy pandas  OPTIONAL: if you want to have more information on what's happening  activate the logger as follows logging  logging.basicConfig level=logging.INFO) matplotlib  % matplotlib inline  Load pre-trained model tokenizer  vocabulary)  max_sent_length = 512 # BERT Base load_model   Load pretrain model of BERT text_tokening corporate_profile_text  tokenizer=tokenizer  verbose=0  Sentence tokenization  Remove unwanted characters  this function should be factored out later)  for checking  Turn index back to tokens  Print out token and id pair for checking  loop over each sentence  loop over each tokens and ids company2vector sent_token_id  model  average_over_sent=True  Turn corporate Profile to vector  Take the last hidden layer outputs and average over sentence and words  to numpy vector_of_each_whole_token corporate_profile_tokens  result  Indexing subwords  Sentence level  Token level  [second output][sent][the tokens] #""):  Taking mean of sub words  For each sentence in last layer  For each token in sentences  Taking mean over sub words  Create Pandas DataFrame",151
Financial Report Analysis BERT W2V GLOVE,To extract and via word embedding methods to visualise financial reports.,"BERT, w2v, GLOVE, text analysis, LLE, dimension reduction",Francis Liu,BERT_financialReportAnalysis,10,!/usr/bin/env python  coding: utf-8  In[1]:  In[129]:  In[ ]: PyPDF2 re  In[ ]: f_correction text  Correct the error made by pdf extractor  In[ ]: get_corporate_profile file_name  Take the text in the cooperate profile page  Seach for the first page with the word 'company profile'  print p)  In[133]:  In[ ]:,48
Financial Report Analysis BERT W2V GLOVE,To extract and via word embedding methods to visualise financial reports.,"BERT, w2v, GLOVE, text analysis, LLE, dimension reduction",Francis Liu,BERT_financialReportAnalysis,10,"PyPDF2 re f_correction text  Correct the error about ""fl"" and ""fi"" made by pdf extractor get_corporate_profile file_name  Take the text in the cooperate profile page  TCregex = r""table[\s{ 3}|\n]of[\s{ 3}|\n]content""  Seach for the first page with the word 'company profile which is not table of content'  need to check whether there is enough content  print p)",56
Financial Report Analysis BERT W2V GLOVE,To extract and via word embedding methods to visualise financial reports.,"BERT, w2v, GLOVE, text analysis, LLE, dimension reduction",Francis Liu,BERT_financialReportAnalysis,10,!/usr/bin/env python  coding: utf-8  In[ ]:  In[ ]: torch transformers BertTokenizer nltk nltk.tokenize sent_tokenize numpy  OPTIONAL: if you want to have more information on what's happening  activate the logger as follows logging logging.basicConfig level=logging.INFO) matplotlib  % matplotlib inline  Load pre-trained model tokenizer  vocabulary)  max_sent_length = 512 # BERT Base  In[7]: google.colab drive  In[8]:  In[ ]:  In[ ]:  Load Testing Data  In[ ]:  s.insert 0  '[CLS]')  s.insert -1  '[SEP]')  In[ ]:  In[13]:  For more information  In[ ]:  In[16]:  In[ ]:  In[20]:  In[ ]:  In[ ]:  In[57]:  In[ ]:  In[ ]: seaborn matplotlib numpy  In[ ]:  In[70]:  In[ ]:,97
Financial Report Analysis BERT W2V GLOVE,To extract and via word embedding methods to visualise financial reports.,"BERT, w2v, GLOVE, text analysis, LLE, dimension reduction",Francis Liu,BERT_financialReportAnalysis,10,!/usr/bin/env python  coding: utf-8  In[2]: numpy pandas sklearn.manifold TSNE matplotlib pyplot  In[4]:  Download Two Reports and their word embeddings by BERT  Download Pretrained W2V  In[5]:  In[18]:  com1 = com1[com1.punc==False]  com1 = com1[com1.stopwords==False]  com2 = com2[com2.punc==False]  com2 = com2[com2.stopwords==False]  In[7]:  In[8]:  In[10]:  In[11]:  In[12]:  prepare for plotting  In[13]:  In[14]: gensim.models.word2vec Word2Vec gensim  In[15]:  In[16]:  In[17]:  In[18]:  In[19]:  In[20]:  In[21]: gensim.models.keyedvectors KeyedVectors  model = Word2Vec.load 'GoogleNews-vectors-negative300.bin.gz')  In[22]:  In[23]:  In[32]:  In[33]:  In[34]:  In[ ]:,71
Financial Report Analysis BERT W2V GLOVE,To extract and via word embedding methods to visualise financial reports.,"BERT, w2v, GLOVE, text analysis, LLE, dimension reduction",Francis Liu,BERT_financialReportAnalysis,10,!/usr/bin/env python  coding: utf-8  In[1]:  In[ ]:  In[3]:  In[4]:  In[5]: companytoVector_BERT corporate_profile_extrator google.colab files os glob pandas string nltk.corpus stopwords nltk  In[ ]:  In[7]:  In[ ]:  for debugging  Remove punctuation  Remove stopwords  Save  np.save './outputs/'+name  result)  In[ ]:  In[ ]:  In[ ]:  Remove punctuation  In[ ]:  Remove stopwords  In[ ]:  In[ ]:  In[ ]:  In[ ]:  In[ ]:  Check time needed to run  In[ ]:,65
CrypOpt_RiskNeutralDensity,Uses Rookley Method to estimate risk neutral density. Based on data of crypto options traded on Deribit.,"Rookley, crypto options, cryptocurrency, risk neutral density, state price density, local polynomial, locpoly",Franziska Wehrmann,CrypOpt_RiskNeutralDensity,4,os pickle matplotlib pyplot  ---------------------------------------------------------------------- SMILES  ------------------------------------------------------------------------ RNDs  ----------------------------------------------------------------- DERIVATIVES  ----------------------------------------------------------------- TAU PROCESS,13
CrypOpt_RiskNeutralDensity,Uses Rookley Method to estimate risk neutral density. Based on data of crypto options traded on Deribit.,"Rookley, crypto options, cryptocurrency, risk neutral density, state price density, local polynomial, locpoly",Franziska Wehrmann,CrypOpt_RiskNeutralDensity,4,numpy scipy.stats norm rnd_appfinance M  S  K  o  o1  o2  r  tau,12
CrypOpt_RiskNeutralDensity,Uses Rookley Method to estimate risk neutral density. Based on data of crypto options traded on Deribit.,"Rookley, crypto options, cryptocurrency, risk neutral density, state price density, local polynomial, locpoly",Franziska Wehrmann,CrypOpt_RiskNeutralDensity,4,numpy scipy.stats norm scipy  B-Spline _gaussian_kernel M  m  h_m  T  t  h_t _epanechnikov M  m  h_m  T  t  h_t _local_polynomial df  m  t  h_m  h_t  kernel=_gaussian_kernel locpoly_smoothing df  tau  h_m  h_t=0.05  gridsize=50  kernel='epak',33
CrypOpt_RiskNeutralDensity,Uses Rookley Method to estimate risk neutral density. Based on data of crypto options traded on Deribit.,"Rookley, crypto options, cryptocurrency, risk neutral density, state price density, local polynomial, locpoly",Franziska Wehrmann,CrypOpt_RiskNeutralDensity,4,os pandas pickle numpy smoothing locpoly_smoothing risk_neutral_density rnd_appfinance  ------------------------------------------------------------------------ MAIN  ------------------------------------------------------------------- LOAD DATA  filter out Moneyness bigger than 1.3  filter out Moneyness small than 0.7  --------------------------------------------------------------- SMOOTHING  ----------------------------------------------------------- CALCULATE SPD  plotting in separate script,34
SFM_Hurst_Exponent,"Downloads BTC/USDT transaction data from Binance API aggregated per minute from 2017-01-01 until 2020-01-30. Selects close prices and divides them into 1000 subsets. For each subset, Hurst Exponent is calculated and an AR Model is fitted. The close price for the following 7 periods is made and MSE is calculated. One-sided t-test indicates that large Hurst Exponents are related to a smaller MSE, hence indicates prediction quality.","Hurst Exponent, Hurst, Autoregressive Model, AR, Random Walk, Bitcoin, BTC, Prediction, Predictability, Prediction Quality, Time Series Classification",Julian Winkel,SFM_Hurst_Exponent,3,pandas numpy matplotlib scipy hurst compute_Hc SFM_Hurst_Exponent.AR ar os  Execute Download  from SFM_Hurst_Exponent import binance_download  Read and select Close price  Header: Timestamp  OHLCV  ...)  Get back to simple returns around 0  Evaluate Hurst equation for complete data set  Use Autoregressive Model to predict price for periods of differing H.  Would expect high H periods  e.g. > 0.5) to have a smaller Mean Squared Error than other periods  Then use some hypothesis test on the mean difference.  Use t-test for mse_l small when H_l large  One sided test  Hurst Exponent vs MSE  Compensate ax labels  Raw Bitcoin Close Prices  Autocorrelation of BTC  same as [2:]  dropna) but way faster!  Plot Bitcoin Returns per Minute  Bitcoin Autocorrelation  Plot for subsets  add H as horizontal line   Plot Hurst R/S,126
SFM_Hurst_Exponent,"Downloads BTC/USDT transaction data from Binance API aggregated per minute from 2017-01-01 until 2020-01-30. Selects close prices and divides them into 1000 subsets. For each subset, Hurst Exponent is calculated and an AR Model is fitted. The close price for the following 7 periods is made and MSE is calculated. One-sided t-test indicates that large Hurst Exponents are related to a smaller MSE, hence indicates prediction quality.","Hurst Exponent, Hurst, Autoregressive Model, AR, Random Walk, Bitcoin, BTC, Prediction, Predictability, Prediction Quality, Time Series Classification",Julian Winkel,SFM_Hurst_Exponent,3,pandas read_csv matplotlib pyplot statsmodels.tsa.ar_model AR sklearn.metrics mean_squared_error hurst compute_Hc pandas ar series  train autoregression  walk forward over time steps in test,22
SFM_Hurst_Exponent,"Downloads BTC/USDT transaction data from Binance API aggregated per minute from 2017-01-01 until 2020-01-30. Selects close prices and divides them into 1000 subsets. For each subset, Hurst Exponent is calculated and an AR Model is fitted. The close price for the following 7 periods is made and MSE is calculated. One-sided t-test indicates that large Hurst Exponents are related to a smaller MSE, hence indicates prediction quality.","Hurst Exponent, Hurst, Autoregressive Model, AR, Random Walk, Bitcoin, BTC, Prediction, Predictability, Prediction Quality, Time Series Classification",Julian Winkel,SFM_Hurst_Exponent,3, requires dateparser package dateparser pytz datetime datetime datetime timedelta binance.client Client  pip install python-binance time json re pandas csv os date_to_milliseconds date_str  get epoch value in UTC  parse our date string  if the date is not timezone aware apply UTC timezone  return the difference in time interval_to_milliseconds interval get_historical_klines symbol  interval  start_str  end_str=None  create the Binance client  no need for api key  init our list  setup the max limit  convert interval to useful value in seconds  convert our date strings to milliseconds  if an end time was passed convert it  it can be difficult to know when a symbol was listed on Binance so allow start time to be before list date  fetch the klines from start_ts up to max 500 entries or the end_ts if set  handle the case where our start date is before the symbol pair listed on Binance  append this loops data to our output data  update our start timestamp using the last value in the array and add the interval timeframe  it wasn't listed yet  increment our start date  check if we received less than the required limit and exit the loop  exit the while loop  sleep after every 3rd call to be kind to the API __datetime date_str  Wrap start and end time around signal  open a file with filename including symbol  interval and start and end converted to milliseconds  set file write mode,230
TXTnlp,"Shows basic functionalities of natural language processing: sentence and word tokenization, part-of-speech tagging, lemmatization, stopword removal and word counting. POS-tags are converted to WordNet compatible tags such that the WordNet lemmatizer can be used. Please install required Python packages before usage: os, io, types, collections, textblob, nltk.","text mining, data mining, counts",Elisabeth Bommes,TXTnlp,1, Proper prior cleaning of text might improve results!  Source of textfile: http://www.nasdaq.com/aspx/stockmarketnewsstoryprint.aspx?storyid=3-disastrous-mistakes-mcdonalds-should-regret-cm406246  Please install packages if not installed yet! os io types TextBlob stopwords nltk.stem.wordnet WordNetLemmatizer ## Functions  Function to convert Penn-Treebank POS tags to simplified  WordNet) POS tags posWN posTB  POS function pos blob  Tokenizer function token blob  Lemmatizer function lem article nltk.stem.wordnet WordNetLemmatizer  Counter and stopwords remover cntr lems  stpw collections Counter  Set working directory  Read text file ## Basics  Print first sentence  Print first word  Print first part of speech tag ## Tokens and POS tags ## WordNet lemmatizer  Create namespace  Assign parts of namespace  Lemmatize  Show part of results ## Count words and remove stopwords  Show 10 most frequent lemmas,114
GAN_Gamma_Distribution,"Training a GAN on dataset of 1000 samples of a Gamma distribution. Input of the generator is a std. normal distribution, the GAN is trained for 4000 epochs. Every 100 epochs histograms of generated and training data are plotted.","GAN, distribution, transformation, simulation, generation, neural network",marius sterling,GAN_Gamma_Distribution,2,matplotlib scipy stats numpy,4
GAN_Gamma_Distribution,"Training a GAN on dataset of 1000 samples of a Gamma distribution. Input of the generator is a std. normal distribution, the GAN is trained for 4000 epochs. Every 100 epochs histograms of generated and training data are plotted.","GAN, distribution, transformation, simulation, generation, neural network",marius sterling,GAN_Gamma_Distribution,2,sys torch torch nn torch.autograd.variable Variable torch manual_seed torchvision transforms torch matplotlib pandas numpy tqdm tqdm __init__ self has the same properties as the torch.nn.module Modules will be added to it in the order they are passed in the constructor forward self  x __init__ self  is used when net is setup  forward self  x  Noise noise size ones_target size zeros_target size real_data_target size fake_data_target size pretrain_discriminator optimizer real_data print error_real.grad) train_discriminator optimizer  real_data  fake_data  Reset gradients  1.1 Train on Real Data  Calculate error and backpropagate  1.2 Train on Fake Data  Calculate error and backpropagate  1.3 Update weights with gradients  Return error and predictions for real and fake inputs train_generator optimizer  fake_data  2. Train Generator  Reset gradients  Sample noise and generate fake data  Calculate error and backpropagate  Update weights with gradients  Return error  Optimizers lr_decay epoch  Loss function  Number of steps to apply to the discriminator  In Goodfellow et. al 2014 this variable is assigned to 1  Number of epochs population_sample n  1. Train Discriminator	    Generate fake data detach means do not compute gradients  Train D  2. Train Generator  Generate fake data  Train G fig.suptitle 'Generator and Discriminator Error' fontsize=12),189
DEDA_Class_2018WS_Berlin_Property_Analysis_Gmap_Plotter,"Example Quantlet for charting spatial data collected for this project. This file plots all transit data for the city of Berlin, DE.","Spatial Analysis, Transit Data, Real Estate, Data Visualisation",Alex Truesdale,DEDA_Class_2018WS_Berlin_Property_Analysis_Gmap_Plotter,1, -*- coding: utf-8 -*- os sys openpyxl pandas time gmplot  Introduction.  Define lat / lng collections  tuples).  Change dir. to visualisation dir.  Initialise raw map with api key.  key_02 = 'xxxxxxxxxxxxxx'  raw_map_all = gmplot.GoogleMapPlotter 52.5112264  13.415641  10.81  apikey = key_02)  Initialise raw maps.  Plot scatter points for bus points. 2E8B57'  size = 75  marker = False)  Trigger write / compile method to create final map.  Plot scatter points for tram points. 8FBC8F'  size = 140  marker = False)  Trigger write / compile method to create final map.  Plot scatter points for ubahn points. 4682B4'  size = 190  marker = False)  Trigger write / compile method to create final map.  Plot scatter points for sbahn points. FF7F50'  size = 190  marker = False)  Trigger write / compile method to create final map.  Plot scatter points for all transit types. 8FBC8F'  size = 140  marker = False) FF7F50'  size = 190  marker = False) 4682B4'  size = 190  marker = False) 2E8B57'  size = 75  marker = False)  Trigger write / compile method to create final map.,175
DEDA_class2019_SYSU_LSTMPrediction,"Use LSTM to predict stock price.The main steps are: 1. import tensorflow 2. transform data format 3. create training data and testing data 4. use LSTM to fit the model 5. predict 6. plot results (blue: omitted, orange: train set, green: test set, red: prediction) note that the evaluation of the LSTM fit depends on the training interval. For fun we trained from the ""future"" in the 2nd train valley plot.","LSTM, prediction, stock, training, tensorflow, plot, KERAS","Rainy Wang, WK Härdle, Justin Hellermann",DEDA_class2019_SYSU_LSTMPrediction,1,os pandas numpy sklearn.preprocessing MinMaxScaler keras.models Sequential keras.layers Dense matplotlib matplotlib.pylab rcParams keras backend __init__ self load data minmax_scale_close_price self a b select_train_period self start end select_test_period self start end assemble_data self seq_len sequentialize both test and train set and store x and y values Assemble Train Set Assemble Test Set Assemble Test Set __init__ self units fit self x_data y_data epochs batch_size predict self x_data calc_rmse self therefore rescale plot_results self  Load and Prepare the data for the LSTM 	  Feed data to LSTM  train  predict test sample,88
SMSclus8psc,Employs the spectral clustering algorithm on an 8 points example,"cluster-analysis, spectral, plot, graphical representation, eigenvalues, eigenvectors","Awdesch Melzer, Simon Trimborn",SMSclus8psc,1,numpy scipy random sklearn.cluster SpectralClustering matplotlib matplotlib.patches Ellipse,8
Feature_Engineering,Feature_Engineering,"lightgbm, XGBoost, feature engineering, model encoder, AUC, credit card default, feature selection","Cao Yi, Chen Chi, Ma Haochun, Xu Xiaoxuan, Ji Xiongzhang",Feature_Engineering,6,"!/usr/bin/env python  coding: utf-8  # Preprocessing  In[3]:  In[4]:  In[5]:  import packages pandas numpy category_encoders.count CountEncoder sklearn.preprocessing LabelEncoder sklearn.model_selection StratifiedKFold sklearn.decomposition TruncatedSVD sklearn.feature_extraction.text CountVectorizer sklearn.preprocessing MinMaxScaler lightgbm tqdm tqdm tqdm tqdm_notebook gc warnings toad matplotlib  In[6]:  In[7]:  test_label never show up to get rid of leakage problem!  In[8]:  In[9]:  In[8]:  concatenate base and label  del train_base  test_base  concatenate train and test dataset of operation and transaction  del train_op  test_op  train_df  test_df  train_trans  test_trans  # Feature Engineering 1  In[9]:  base = toad.detector.detect data)  base  In[10]:  Since this feature have too many null values  so we drop this feature  In[11]:  Transfer into integer  In[12]:  In[13]:  Interactive item  In[14]:  data.dtypes  In[15]:  using Label Encoder to deal with categorical features  In[16]:  using Count Encoder to categorical features  del train_base  test_base  del df df_category df_category_nunique  In[17]:  In[18]:  generate features from experience  In[19]:  In[20]:  Feature Engineering on trans.csv  In[21]: transform variable tm_diff transform_time x  extract elements from tm_diff  timestamp means how many seconds passed after the start point  using timestamp to reorganize the sequence of each user's behaviour  In[22]:  use hour to group the behaviours in trans and op  Features about transcation amount  In[23]:  define a function that calculate statistics about transaction amount gen_user_amount_features df  extract features about transcation amount  In[24]:  define unique value of each user gen_user_nunique_features df  value  prefix  extract features from trans dataframe  transaction amount per day  transcation amount each time  In[25]:  define a function to get the transcation amount with each group gen_user_group_amount_features df  value  group by platform  gourp by type1  group by type2  group by time group by week  In[26]:  add time axis  define a function to calculate users' transcation amount linked with feature 'days' gen_user_window_amount_features df  window  extract amount feature within transcations after 7 days  extract amount feature within transcations after 14 days  extract amount feature within transcations after 21 days  extract amount feature within transcations after 28 days  In[27]:  define a function to calculate users' transcation amount linked with feature 'hours' gen_user_window_amount_features df  window  extract amount feature within transcations after 6 a.m.  extract amount feature within transcations after 12 p.m.  extract amount feature within transcations after 18 p.m.  In[28]:  for some features  we consider that null comes from some reason  so we generate features about missing value gen_user_null_features df  value  prefix  extract missing value and ratio within ip address  In[29]:  extract the first apperance of ""type1 == 45a1168437c708ff""  In[30]:  define the transaction amount of each users per day and hour per_hour_amt df value1 value2  extract transcation per day and hour features  In[31]:  define a function to calculate each user's transcation amount group by tunnel_in and tunnel_out tunnel_in_out_amt df value1 value2  extract trascation amount group by tunnel_in and tunnel_out  In[32]:  define a function to calculate each user's transcation amount group by ""ip"" day_ip_amt df value1 value2  extract trascation amount group by ""ip""  In[33]:  define a function to find the gap between transcations of each user for each hour amt_gap df value1 value2  extract features from the amount of transcation group by time gap  In[34]:  define a function to calculate the sum of gap ratio of amount for each user gap_amt_rate df value1 value2  extract the gap ratio feature  In[35]:  generate the amount of transaction per day per_day_amt df value  extract features using the function  In[36]:  define a function to find the gap between transcations of each user for each transcation day day_amt_gap df value  extract amount gap per day  In[37]:  define a function to calculate the gap ratio per day day_gap_amt_rate df value  generate amount gap ratio per day  In[38]:  define a function to calculate the type1 transcation per day day_type_amt df value1 value2  extract type1 transcation amount per day  In[39]:  define a function to calculate the type2 transcation per day day_type2_amt df value1 value2  extract type2 transcation amount per day  In[40]:  Word2Vec  In[41]: gensim.models Word2Vec multiprocessing w2v_feat data_frame  feat  mode  generate word2vec features  In[42]:  In[43]:  using Count Encoder to categorical features in df_trans category_encoders.count CountEncoder  extract frequncy features for features which have more than five unique values  print len A_cnt_features))  In[44]:  Extract features from op.csv  In[45]:  In[46]: gen_user_tfidf_features df  value  use SVD method to reduce the dimension of this sparse matrix  In[47]: gen_user_countvec_features df  value  In[48]: gensim.models Word2Vec multiprocessing w2v_feat data_frame  feat  mode 生成word2vec特征  In[49]:  using Counter Encoder to generate features from op_df category_encoders.count CountEncoder  In[50]:  encode the frequency of opeartion and calculate statistics  In[51]:  define a function to calculate the possible values of each feature for each user gen_user_nunique_features df  value  prefix  extract number of unique values of each user  In[52]:  define a function to count how many times of one operation type happens group by day gen_user_window_op_features df  window  extract operation type counting after 5 days  extract operation type counting after 10 days  In[53]:  define a function to count how many times of one operation type happens group by hour gen_op_window_hour_features df  window  extract operation type counting after 6  extract operation type counting after 12  extract operation type counting after 18  In[54]:  define a function to count how many times of one operation type happens group by time i.e morning  afternoon  evening and night) gen_user_group_op_features df  value  extrate operation type counts group by time  In[55]:  counting how many times each user operated the app  del op_count  calculate how many times of operation happened per day  In[56]:  define a function to calculate the statistics of the counting of operations of each user in per hour per day day_per_hour_cnt df value1 value2  In[57]:  define a function to calculate the statistics of the counting of operations of each user in per day  In[58]:  counting the opeartions happened in the morning for each user  In[59]:  counting the opeartions happened in the afternoon for each user  In[60]:  counting the opeartions happened in the evening for each user  In[61]:  counting the opeartions happened at night for each user  In[62]:  calculate the statistics of the operation counting between two days  In[63]:  calculate the statistics of the operation counting between two hours  In[64]:  calculate the ratio of the number of the operations between two days  In[65]:  calculate the ratio of the number of the operations between two hours  In[66]:  calculate the statistics of the number of operatins of each user happened per second  In[67]:  define a function to calculate the statistics of the number of operatins of each user happened per minutes  In[68]:  calculate the statistics of the number of different devices one may use per day  In[69]:  calculate the statistics of the number of different devices one may use per hour  In[70]:  calculate the statistics of the number of different ip one may use per day  In[71]:  calculate the statistics of the number of different ip one may use per hour  In[72]:  calculate the statistics of the number of different ip one may use per day per minute  Done!  In[73]:  In[74]:  save the generate features  # Feature Engineering 2  In[75]: warnings numpy pandas matplotlib seaborn scipy stats datetime datetime collections Counter math sklearn.linear_model LinearRegression sklearn.preprocessing LabelEncoder sklearn.feature_extraction.text CountVectorizer sklearn.decomposition TruncatedSVD sklearn.preprocessing MinMaxScaler sklearn.model_selection StratifiedKFold sklearn.metrics roc_auc_score sklearn.feature_selection SelectPercentile gensim.models Word2Vec lightgbm tqdm tqdm gc os  In[76]:  read_data  In[77]: parse_time tm  In[78]:  # Define embedding functions  In[79]: w2v_emb df  f1  f2  prefix  In[80]: tfidf_emb df  f1  f2  prefix  In[81]: countvec_emb df  f1  f2  In[82]: add_trend_feature arr  abs_values=False  # Features from operation  In[83]:  In[84]:  In[85]:  In[86]:  In[87]:  # Features from transaction  In[88]:  In[89]:  In[90]:  In[91]:  In[92]:  In[93]:  # Features from base  In[94]:  In[95]:  In[96]:  In[97]:  In[98]:  欺诈率 stat df  df_merge  group_by  agg statis_feat df_know  df_unknow  In[99]:  In[100]:  In[101]:  In[102]:  In[103]:  save the generate features",1237
Feature_Engineering,Feature_Engineering,"lightgbm, XGBoost, feature engineering, model encoder, AUC, credit card default, feature selection","Cao Yi, Chen Chi, Ma Haochun, Xu Xiaoxuan, Ji Xiongzhang",Feature_Engineering,6,!/usr/bin/env python  coding: utf-8  In[108]:  pip install lux-api  In[136]: pandas seaborn matplotlib gc  In[110]:  In[112]:  In[122]:  In[116]:  In[119]:  In[120]:  In[129]:  In[134]:  In[137]: transform variable tm_diff transform_time x  using timestamp to reorganize the sequence of each user's behaviour  In[138]:  In[140]:  In[141]:  In[143]:  In[146]: 用行和列标签绘制  绘制x-y-z的热力图，比如 年-月-销量 的热力图 设置坐标字体方向  In[ ]:,49
NoiseDifference,"Draws figures for White, Pink and Blue noise in time-domain and frequency-domain, ACF, PCF and applied Fourier transform","White nose, pink noise, blue noise, time-domain, frequency-domain, Fourier transform",Junjie Hu,SFE-NoiseDifference,1,wave matplotlib matplotlib numpy statsmodels scipy.signal periodogram statsmodels.tsa.stattools adfuller scipy.stats.mstats normaltest extract_signal file_name  num_frames=-1  dtype='int16'  Open signal file  decoding as int16 plot_signal signal  color='b'  name=None  plt.title name) plot_acf_pacf signal  lags=None  name=None plot_periodgram signal  color=None  name=None plot_decomposition   f=0.1  f=0.6  f=1.2  sampling from the audio  plot time series of signals  plot acf and pacf of signals  plot periodogram of signals  adf test  normality test  histogram plot,64
SDA_2020_St_Gallen_04_VisualizeMonteCarlo,"This quantlet is part of other quantlets and should ideally be executed after the quantlets before (see parent folders). Here the monte carlo simulations performed in the quantlet 'SDA_2020_St_Gallen_02_Simulations' are plotted. First, a plot showing the performance of the q-learning algorithm with 100 randomly generated initial q-tables. Second, a plot showing the performance of the q-learning algorithm in comaprison to a simple Buy and Hold and considers the confidence intervals.","QLearning, Monte Carlo, Trading Bot, BTC, Bitcoin, Timeseries, Technical Indicators","Tobias Mann, Tim Graf",SDA_2020_St_Gallen_04_VisualizeMonteCarlo,2,os pandas numpy scipy stats matplotlib matplotlib matplotlib.dates DateFormatter plotting test_average_performance paths  data  threshold = .01  verbose = False  This functin calculates the avgerage cumulative BTC return  and calculates the p-Value that the true cumulative simulation return is equal or below to BTC performance  MONTE CARLO SIMULATION PLOT ----------------------------------------------  change the folder where the input is  initiliaze figure  create a color palette  format x axis  plot every X  show and plot,71
SDA_2020_St_Gallen_04_VisualizeMonteCarlo,"This quantlet is part of other quantlets and should ideally be executed after the quantlets before (see parent folders). Here the monte carlo simulations performed in the quantlet 'SDA_2020_St_Gallen_02_Simulations' are plotted. First, a plot showing the performance of the q-learning algorithm with 100 randomly generated initial q-tables. Second, a plot showing the performance of the q-learning algorithm in comaprison to a simple Buy and Hold and considers the confidence intervals.","QLearning, Monte Carlo, Trading Bot, BTC, Bitcoin, Timeseries, Technical Indicators","Tobias Mann, Tim Graf",SDA_2020_St_Gallen_04_VisualizeMonteCarlo,2,"pandas numpy matplotlib matplotlib.patches Polygon create_mc_dist_plot paths  data  quantiles  output=""./Images/LastMonteCarloDistribution.png""  title=""Qlearning Monte Carlo Simulation vs BTC""  Center Confidence interval",19
SC-classification-significance,This Quantlet is dedicated to test of the difference in performance of different training modes. The wilcoxon test is used.,"machine learning, source code, significance testing, nonparametrics, wilcoxon","Elizaveta Zinovyeva, Raphael Constantin Georg Reule",SC-classification-significance,2,!/usr/bin/env python  coding: utf-8  In[26]: numpy matplotlib os pandas sklearn.model_selection StratifiedKFold sklearn.linear_model LogisticRegression random sklearn metrics collections Counter argparse sklearn.metrics roc_auc_score sklearn.feature_extraction.text TfidfVectorizer seaborn sklearn.preprocessing LabelEncoder collections tqdm scipy.stats mannwhitneyu  In[2]: other x  In[3]:  In[4]:  In[5]:  In[6]:  In[7]:  In[8]:  In[9]:  In[10]:  In[11]:  In[12]:  In[13]:  In[14]:  ## Full code  ### Ridge Regression  In[15]:  In[16]:  In[17]:  In[18]:  In[19]:  In[20]:  In[21]:  In[23]:  In[47]: re contract_name_extract data extract contract name function_name_extract data extract function names and join to one string interface_name_extract data extract function names and join to one string event_name_extract data extract function names and join to one string library_name_extract data extract function names and join to one string  In[48]:  In[50]:  ### Hypotheses testing  In[54]:  In[52]:  In[41]:  In[43]:  In[45]:  In[56]:  In[57]:  In[58]:  In[59]:  In[61]:  In[62]:  In[63]:  In[65]:  In[66]:  In[67]:  In[68]:  #### Average Precision  In[73]:  In[74]:  In[75]:  In[76]:  In[77]:  In[78]:  In[79]:  In[80]:  In[81]:  In[82]:  In[83]:  In[84]:  In[85]:  In[86]:,144
SFEBarrier_Pricing_MC,"Computes Barrier option prices using Monte Carlo method for assets with/without continuous dividends. barrier option types: up-and-out, up-and-in, down-and-out, down-and-in","binomial, Monte-Carlo, asset, option, option-price, barrier-option, up-and-out, up-and-in, down-and-out, down-and-in",Franziska Wehrmann,SFM_BarrierPricing_MonteCarlo,1,"numpy scipy.stats bernoulli matplotlib matplotlib.lines Line2D calc_parameters T  N  sigma  r  div  P up movement) sample_paths S0  N  u  d  q  M  M sample paths at once  time steps valid_barrier_type paths  barrier_type  B  keep ""always smaller than barrier""  True=1  False=0  where always False  it was never > B  keep ""always bigger than barrier""  True=1  False=0  where always False  it was never > B  keep ""at least once bigger than barrier""  True=1  False=0  where at least once True  keep ""at least once smaller than barrier""  True=1  False=0  where at least once True split_paths paths  B  K  barrier_type  option  valid  not knocked out by barrier_type with B  invalid  complement of p_valid  option is exercised  subset of p_valid calc_price p_counts  K  r  T  M  option  not possible to exercise any option sample_and_plot S0  K  B  T  N  u  d  q  M  barrier_type  plt.savefig 'up-and-out_call.png'  transparent=True)  plotting  The following will plot the root mean square error  plt.savefig 'MonteCarlo_variation.png'  transparent=True) ###### MAIN ################  current stock price  strike price  time to maturity  volatility  interest rate  dividend  steps in tree  barrier  number of paths for Monte Carlo  calculate all prices  plot sample paths  plot Monte Carlo variations  runs for 10.000  20.000  ... 500.000 paths  Value from SFEBarrier_Pricing_Tree",200
pyTSA_Chaos,This Quantlet produces Chaos like a White Noise and saves in csv file,"time series, White noise, ACF, PACF, simulation","Huang Changquan, Alla Petukhina",pyTSA_Chaos,1,pandas numpy  start value,4
DCA_sum_of_squares_for_k-means,Plotting the sum of squared distances for each k of a k-means clustering,"k-means, sum of squares, clustering, bar plot, visualisation",Luisa Krawczyk,Sum_of_squares,1, -*- coding: utf-8 -*-  sum of squares as a function of k  decide for k=15 or k=18 or k=9  22 or 26 sklearn.cluster KMeans matplotlib,25
LOBDeepPP_MSE_visualisation_L,"Joint Limit order book ask and bid price predition for multiple predicton horizons with a neural networks. This script creates plots meant for analysis of the fixed time lag application for training, validation and test dataset.","Limit order book, finance, forecasting, prediction, MSE, evaluation, plotting",Marius Sterling,LOBDeepPP_MSE_visualisation_L,1,"!/usr/bin/env python3  -*- coding: utf-8 -*- os json_tricks numpy matplotlib  %%  %%  %% Extracting MSE from errors  %% MSE for ask and bid for train  test and validation data     ax.legend loc='center left'  bbox_to_anchor= 1  0.5))  %% MSE vs prediction horizon with order levels for ask and bid seperated     plt.ylim [0.005  0.01])     plt.ylim [0.0035  0.02])     plt.ylim [0.0035  0.01])     plt.savefig f'{path_save}/ph_mse_ol_{ds}_zoom.pdf'                  bbox_inches='tight'  dpi=300)  %% MSE vs prediction horizon with order levels for ask and bid seperated         plt.ylim [0.0035  0.01])         plt.savefig f'{path_save}/ph_mse_ol_{ds}_{""bid"" if bid else ""ask""}_zoom.pdf'                      bbox_inches='tight'  dpi=300)",85
pyTSA_MacroDE,"This Quantlet plots monthly time series of returns of Procter and Gamble from 1961 to 2016 and  their ACF and PACF (Example, 2.4 Figures 2.8-2.9 in the book)","time series, autocorrelation, returns, ACF, PACF, plot, visualisation","Huang Changquan, Alla Petukhina",pyTSA_MacroDE,1,numpy pandas matplotlib statsmodels.tsa.api VAR PythonTsa.plot_multi_ACF multi_ACFfig PythonTsa.plot_multi_Q_pvalue MultiQpvalue_plot  Extension name of the data is dat  not csv or txt.  argument sep = '\s+' needed.  leave the last four data for forecasting comparison  ic = None since the order has been selected.  transferred to 'numpy.ndarray' class  for forecast  the data needs to belong to the class 'ndarray',57
Crypto_GRU_Prediction,use LSTM and GRU to optimize prediction,"crypto, LSTM, GRU, prediction","Wang Wenyi, Liu Zekai, Tang Jiayun, Zhang Lin, Chen Jing",Crypto_GRU_Prediction,2,!/usr/bin/env python  coding: utf-8  In[1]: tensorflow numpy pandas matplotlib os sklearn.metrics mean_absolute_error tensorflow.keras.layers LSTM tensorflow.keras Sequential tensorflow.keras.layers Dense tensorflow.keras activations  ## Read in data  In[2]:  ## Process BTC data  In[7]: sklearn.preprocessing MinMaxScaler  ## GRU model for BTC  In[8]:  In[9]:  In[10]:  ## Process ETH data  In[3]: sklearn.preprocessing MinMaxScaler  ## GRU model for ETH  In[4]:  In[5]:  In[6]:  ## Process BNB Data  In[8]: sklearn.preprocessing MinMaxScaler  ## GRU model for BNB  In[9]:  In[10]:  In[11]:,70
ANNregression,Gradient descent implementation from scratch in order to train an artificial neural network to fit a regression of city-cycle fuel consumption in miles per gallon.,"Dense, ANN, MLP, deep learning, neural network",Bruno Spilak,ANNregression,1,matplotlib matplotlib numpy  cf: https://github.com/antoine-eon/olga/tree/master/Algorithme/Algo%20Reseau%20Neuronal%20Regression%20Lineaire%20EX  Load the data and create the data matrices X and Y  This creates a feature vector X with a column of ones  bias)  and a column of car weights.  The target vector Y is a column of MPG values for each car.  Standardization  Two weights  bias and feature)  Batch gradient descent  size eta  We iterate over each data point for one epoch  Update the weights  Plot the data and best fit line   label = t)  Plot the data and best fit line,87
SFM_Pareto_tail,Estimates the parameters of Pareto-like distribution for the left tail of log-returns.,"Pareto, tail, log-return",Daniel Traian Pele,SFM_Pareto_Tail,2,!/usr/bin/env python  coding: utf-8  In[9]: pandas datetime numpy matplotlib scipy.stats pareto scipy.stats norm  We will look at stock prices overtime end = datetime.date.today ) pandas_datareader data powerlaw print mu) print sigma) print q) ### xmin*alpha/ alpha-1),36
Export_Predictions_to_MATLAB,"Split the entire dataset in two parts. the first one consists of all observations without the last 24 ones. It is used for the training process of LSTM with the optimal parameters. The second subset consists of the last 24 observations used for the prediction of the hourly returns in the next 24 hours. Once the predictions are generated for each of the coins, they are exported from python in order to be imported in MATLAB.","LSTM, predictions, export","Georg Velev, Iliyana Pekova",Export_Predictions_to_MATLAB,1,numpy generate_predictions data google.colab files,5
SFEirsValuation,"Calculates the value of an IRS based on a Bond, FRA and Forward Rate approach.","finance, derivative, interest rates, swaps","Marvin Gauer, Laureen Lake",SFEirsValuation,1, SFM1 - Project -> Compare the three different ways to value an IRS numpy pandas prettytable PrettyTable ####### 1.Input Parameters  Fixed Rate  Principal ####### 2. Valuation ####### 2.1. Helper Functions Bfix yields  Coupon_Anually  P  Calculates a Bondprice with fixed Coupon  D calculates the Discount factors  Bond Price is the single Price and NPVs are the Present Values of the Bond payments FRA Rfix  yields  Si  Ti  P Calculates the Value of an FRA  Si < Ti  DSi is the discount factor of time point Si. Same holds for Ti and DTi  Price of the FRA  FRA = Value of the FRA ForwardRates yields  Si  Ti  Calculates the Forward rate implicit in the yield curve  Si < Ti  DSi is the discount factor of time point Si. Same holds for Ti and DTi  FR is the Forward Rate for Period Si to Ti ####### 2.2. Valuation in Terms of Bond Prizes ####### 2.3. Valuation in Terms of FRA Prizes ####### 2.4. Valuation in Terms of Valuation in Terms of discounted Cash Flows  DCF are the discounted Cash Flows ####### 3. Results  Creation of the Columns of the printed table  Creates the table based on the columns defined above,197
DEDA_Class_SS2018_BitcoinReddit,Analysing Sentiment of Subreddits,"Sentiment, Wordcloud, Word Frequency, Web Scraping",Constantin Hackober,DEDA_Class_SS2018_Reddit_Sentiment,3,"Bitcoin   Import Reddit API Praw  praw  More imports pandas nltk nltk.tokenize word_tokenize nltk.corpus stopwords nltk tokenize nltk.sentiment.vader SentimentIntensityAnalyzer nltk.stem WordNetLemmatizer textblob TextBlob seaborn matplotlib wordcloud WordCloud random PIL Image numpy IPython display math pprint pprint  Sign in Reddit App   Choose Subreddit and Category   Define text   Prepare text   Clean text print clean_words)  Word Frequency Dictionary valueSelection dictionary  length  startindex = 0  Create plot  Create wordcloud  Sentiment Analysis Submissions  nltk.sentiment.vader SentimentIntensityAnalyzer  Sentiment Analysis 2017 and 2018 serialize post Lower case remove punctuation top_2018['title'] = top_2018['title'].str.replace '[^\w\s]' '') remove stopwords top_2018['title'] = top_2018['title'].apply lambda x: "" "".join x for x in x.split ) if x not in stopwords)) spelling correction TextBlob sentiment print top_2018[['title' 'sentiment']])  Create plot 2017 word count Lower case remove punctuation top_2017['title'] = top_2017['title'].str.replace '[^\w\s]' '') remove stopwords top_2017['title'] = top_2017['title'].apply lambda x: "" "".join x for x in x.split ) if x not in stopwords)) spelling correction TextBlob sentiment print top_2017[['title' 'sentiment']]) creating plot",155
DEDA_Class_SS2018_BitcoinReddit,Analysing Sentiment of Subreddits,"Sentiment, Wordcloud, Word Frequency, Web Scraping",Constantin Hackober,DEDA_Class_SS2018_Reddit_Sentiment,3,import Reddit API Praw  praw more imports pandas nltk nltk.tokenize word_tokenize nltk.corpus stopwords nltk tokenize nltk.sentiment.vader SentimentIntensityAnalyzer nltk.stem WordNetLemmatizer textblob TextBlob seaborn matplotlib wordcloud WordCloud PIL Image numpy IPython display math pprint pprint Sign in Reddit App  choose subreddit and category   define text   prepare text  clean text print clean_words)  word frequency valueSelection dictionary  length  startindex = 0  create plot create wordcloud sentiment analysis submissions  nltk.sentiment.vader SentimentIntensityAnalyzer sentiment analysis 2017 and 2018 serialize post word count Lower case remove punctuation remove stopwords spelling correction TextBlob sentiment creating plot 2017 word count Lower case remove punctuation remove stopwords spelling correction TextBlob sentiment creating plot,102
DEDA_Class_SS2018_BitcoinReddit,Analysing Sentiment of Subreddits,"Sentiment, Wordcloud, Word Frequency, Web Scraping",Constantin Hackober,DEDA_Class_SS2018_Reddit_Sentiment,3,import Reddit API Praw  praw more imports pandas nltk nltk ne_chunk nltk.corpus stopwords nltk tokenize nltk.sentiment.vader SentimentIntensityAnalyzer nltk.stem WordNetLemmatizer textblob TextBlob seaborn matplotlib wordcloud WordCloud random PIL Image numpy IPython display math pprint pprint Sign in Reddit App  choose subreddit and category   define text  print  text)  prepare text  clean text print clean_words)  word frequency valueSelection dictionary  length  startindex = 0  create plot create wordcloud sentiment analysis submissions  nltk.sentiment.vader SentimentIntensityAnalyzer sentiment analysis 2017 and 2018 serialize post word count Lower case remove punctuation remove stopwords spelling correction TextBlob sentiment creating plot 2017 word count Lower case remove punctuation remove stopwords spelling correction TextBlob sentiment creating plot,105
pyTSA_MacroUS2,"This Quantlet plots monthly time series of returns of Procter and Gamble from 1961 to 2016 and  their ACF and PACF (Example, 2.4 Figures 2.8-2.9 in the book)","time series, autocorrelation, returns, ACF, PACF, plot, visualisation","Huang Changquan, Alla Petukhina",pyTSA_ML,1,numpy pandas matplotlib sklearn.preprocessing MinMaxScaler tensorflow.keras.models Sequential tensorflow.keras.layers GRU tensorflow.keras.callbacks EarlyStopping sklearn.metrics mean_absolute_percentage_error PythonTsa.plot_acf_pacf acf_pacf_fig  Let the input be a vector of the previous 24 hours of the load values.  one-step-ahead prediction  equal to y_train = train_shifted[y_col].to_numpy ),38
AOBDL_DL,"Antisocial Online behaivor detection. Contains benchmarking on different models deep learning models: GRU, LSTM, BGRU (bidirectional), BLSTM, BGRU + attention, BGRU + global average pooling, BGRU + global maximum pooling. Furthermore contains HAN and psHAN",ERROR,Elizaveta Zinovyeva,AOBDL_DL,3,!/usr/bin/env python  coding: utf-8  In[ ]: pandas numpy numpy.random seed tensorflow set_random_seed tensorflow random os nltk tokenize nltk  In[ ]: 2. Get the file  GOOGLE COLAB SETUP  Load the Drive helper and mount google.colab drive  This will prompt for authorization.  In[ ]: sys  In[ ]: models models_no_gpu  In[ ]: 3. Read file as panda dataframe  CHANGE TRAIN AND TEST  MIX TO GET SIMILAR DISTRIBUTION sklearn.model_selection train_test_split  In[ ]:  # LSTM  In[ ]:  In[ ]:  # GRU  In[ ]:  In[ ]:  # CNN  In[ ]:  In[ ]:  # BGRU  In[ ]:  In[ ]:  # BLSTM  In[ ]:  In[ ]:  # BGRU + Max Pooling  In[ ]:  In[ ]:  # BGRU + attention  In[ ]:  In[ ]:  # BGRU + Average Pooling  In[ ]:  In[ ]:  # psHAN  In[ ]:  In[ ]:  # HAN  In[ ]: 3. Read file as panda dataframe  CHANGE TRAIN AND TEST  MIX TO GET SIMILAR DISTRIBUTION sklearn.model_selection train_test_split  In[ ]:  In[ ]:  In[ ]:,157
AOBDL_DL,"Antisocial Online behaivor detection. Contains benchmarking on different models deep learning models: GRU, LSTM, BGRU (bidirectional), BLSTM, BGRU + attention, BGRU + global average pooling, BGRU + global maximum pooling. Furthermore contains HAN and psHAN",ERROR,Elizaveta Zinovyeva,AOBDL_DL,3,"numpy pandas keras keras.layers Embedding keras.layers Dense keras.layers Bidirectional keras.layers CuDNNLSTM keras.layers Conv1D keras.layers Dense keras.models Model keras.optimizers RMSprop keras keras.engine.topology Layer keras.models load_model keras.preprocessing text keras initializers sklearn.model_selection StratifiedKFold sklearn.metrics roc_auc_score sklearn.metrics precision_score nltk tokenize nltk ############################################ ####           ATTENTION              ###### ############################################  Code for attention is based on the following implementation https://gist.github.com/cbaziotis/7ef97ccf71cbc14366835198c09809d2 dot_product x  kernel  Code for attention is based on the following implementation https://gist.github.com/cbaziotis/7ef97ccf71cbc14366835198c09809d2 build self  input_shape compute_mask self  input  input_mask=None call self  x  mask=None  Cast the mask to floatX to avoid float64 upcasting in theano compute_output_shape self  input_shape ############################################ ####              MODELS              ###### ############################################   gru_keras max_features  maxlen  bidirectional  dropout_rate  embed_dim  rec_units mtype='GRU'  reduction = None  Code for hierarchical attention network is based on the following implementation https://github.com/richliao/textClassifier/blob/master/textClassifierHATT.py    make_hat max_sent_len  max_sent_amount  max_features  embed_dim  rec_units  dropout_rate cnn_keras max_features  maxlen  dropout_rate  embed_dim  num_filters=300 dl_model model_type='BGRU'  max_features=40000  embed_dim=50  rec_units=150  dropout_rate=0.25  maxlen=400  max_sent_len=100  max_sent_amount=4 ############################################ ####            TRAINING              ###### ############################################ clean_str string string = string.replace "" ""  ""."").replace "";""  ""."").replace "":""  ""."").replace ""-""  ""."") tok_sentence s TRAIN VAL for threshold in [0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9]: clean_str string string = string.replace "" ""  ""."").replace "";""  ""."").replace "":""  ""."").replace ""-""  ""."") tok_sentence s  FULL TRAIN Test",194
wordcloud_abstract_covid19,Create a wordcloud about covid19 using abstract that crawled from sciencedirect,ERROR,"Muhaimin, 07-12-2020",wordcloud_abstract_covid19,1, -*- coding: utf-8 -*-  Import library pandas numpy wordcloud WordCloud PIL Image numpy random matplotlib  Function for coloring word into black and white  Import data abstract after preprocessing  Import mask/shape  Create wordcloud model  Plot the wordcloud  Save the wordcloud,39
SDA_2020_St_Gallen_Polarity_Based_Sentiment_Analysis,Using Polarity Based Sentiment Analysis and linear regression to find the effect of Reddit Submissions on Bitcoin and Ethereum,"Linear Regression, OLS, Sentiment Analysis, GridSearch, Polarity, ","Daniil Bulat, Luca Riboni",SDA_2020_St_Gallen_Polarity_Based_Sentiment_Analysis,2,============================================================================= =============================================================================              BTC and ETH Predictions using Reddit Submissions                           Smart Data Analytics                          Universität St. Gallen                         Daniil Bulat 14-607-055                          Luca Riboni 14-619-878 ============================================================================= ============================================================================= ============================= Import Packages =============================== re pandas numpy yfinance os collections os path PIL Image wordcloud WordCloud matplotlib statistics mean statsmodels =============================== Functions =================================== IsNotNull value negated word polarity_check dict  article  of positive words:'  pos_count)  of negative words:'  neg_count) data_handle data  currency  df_currency clouds data  Create and generate a word cloud image: count_graph title data ================================= Dictionary ================================ # Positive Words Dictionary # Negative Words Dictionary # Complete Dictionary ============================ Sentiment Analysis ============================= ============================== Word Clouds =================================== # BTC  Create and generate a word cloud image: # ETH  Create and generate a word cloud image: graphs ================================= OLS ======================================= ## Bitcoin # Weekly Time Series of Polarity Scores # Monthly Time Series of Polarity Scores # Add constant # OLS Model ## Ethereum # Weekly Time Series of Polarity Scores # Monthly Time Series of Polarity Scores # Add constant # OLS Model,165
SDA_2020_St_Gallen_Polarity_Based_Sentiment_Analysis,Using Polarity Based Sentiment Analysis and linear regression to find the effect of Reddit Submissions on Bitcoin and Ethereum,"Linear Regression, OLS, Sentiment Analysis, GridSearch, Polarity, ","Daniil Bulat, Luca Riboni",SDA_2020_St_Gallen_Polarity_Based_Sentiment_Analysis,2,============================================================================= =============================================================================                               Reddit Reader                           Smart Data Analytics                          Universität St. Gallen                         Daniil Bulat 14-607-055                          Luca Riboni 14-619-878 ============================================================================= ============================================================================= requests json csv datetime numpy =================================  FUNCTIONS  =============================== getPushshiftData query  after  before  sub collectSubData subm  keys = ['title'  'url'  'id'  'score'  ' created_utc'  'num_comments']  list_ = [subm[i] if i in subm.keys ) else np.nan for i in keys] list to store data points text = subm['selftext'] 1520561700.0 updateSubs_file   'reddit_bitcoin.csv =================================    SCRAPING   =============================  comment; submission; subreddit  'bitcoin'  for timestamp  dates) use: https://www.unixtimestamp.com/index.php  01.12.2020  01.01.2015  'bitcoin' =================================  SUBMISSIONS  =============================  Calls getPushshiftData ) with the created date of the last submission,96
SFEacfar1,Plots the autocorrelation function of an AR(1) (autoregressive) process.,"acf, autocorrelation, autoregressive, discrete, graphical representation, linear, plot, process, simulation, stationary, stochastic, stochastic-process, time-series","Joanna Tomanek, WK Härdle",QID-597-SFEacfar1,1,pandas numpy statsmodels statsmodels.graphics.tsaplots plot_acf statsmodels.tsa.arima_process ArmaProcess matplotlib matplotlib  parameter settings  lag value  Obtain MA 1) sample by sampling from a ARMA ) model with no AR coefficient,28
SC_TheoreticalExample,This code provides a step-wise example of Spectral Clustering and uses simple linear algebra to perform SC on a hand-made simple theoretical example,"Graph theory, spectral clustering, clustering, eigenvalues, eigenvectors, laplacians, matrices",Kainat Khowaja,SC_TheoreticalExample,2,!/usr/bin/env python  coding: utf-8  In[42]: Importing important libraries numpy scipy.linalg fractional_matrix_power  In[46]:  In[47]:  degree matrix  laplacian matrix  In[54]:  eigenvalues  eigenvectors  In[ ]:,22
DEDA_ConditionIteration,UNIT 2 of DEDA. Introduce the conditional execution and how to iterate by using,ERROR,Isabell Fetzer and Junjie Hu,DEDA_ConditionIteration,2,!/usr/bin/env python  coding: utf-8  # **DEDA Unit 2** : Data Structure and Commonly Used Operations  ### **List**  In[1]: ']  counts items of the list. Output will be 3  4 items  string characters) in second item of the list   adds one element at the end of the list   ['C++'  'Java'  'C#' 'JavaScript']  adds one element at arbitrary place  ['Python'  'C++'  'Java'  'C#' 'JavaScript']  adds multiple elements at the end of the list  ['Python'  'C++'  'Java'  'C#'  'JavaScript'  'Python 2.7'  'Python 3.6']  In[2]: ') # removes specific element from list  ['Python'  'C++'  'Java'  'JavaScript'  'Python 2.7'  'Python 3.6']  removes the last element of the list …  ['Python'  'C++'  'Java'  'JavaScript'  'Python 2.7']  … and this does so too  ['Python'  'C++'  'Java'  'JavaScript']  converts list to string  Python; C++; Java; JavaScript  converts string to list   ['Python'  'C++'  'Java'  'JavaScript']  In[3]:  basic search in list  0  95  112  1  reorder list  [0  95  2  11  4]  [0  2  4  11  95]  [95  11  4  2  0]  [95  11  4  2  0]  formatting numbers of list to 2 digits  ['95'  '11'  '04'  '02'  '00']  ### **Tuple**  In[6]:  empty tuple  tuple is implicitly created  tuple vs. list   In[9]:  accessing tuples works like accessing lists  however …   … tuple do not support item assignment while lists do  TypeError! Python stops.   In[10]:  ['G.v. Rossum'  65  'Dutch']  ### **Set and Frozenset**  In[11]:  empty set  also a set  empty dict  empty frozenset  also a frozenset  again: a frozenset  four sets  sets will drop duplicated elements automatically  merge 2 sets into 1 w/o duplicates  {'string'  'tuple'  'list'}  In[12]:  a | b  {'frozenset'  'dictionary'  'string'  'tuple'}  a & b  {'string'  'tuple'}  a - b  {'frozenset'  'dictionary'}  b - a  {'list'}  a ^ b   {'frozenset'  'dictionary'  'list'}  False  ### **Dict**  In[13]:  find the value by using the key  Guido von Rossum  get all the keys and put into a list   ['name'  'birth'  'gender'  ‘height']  get all values and put into a list  ['Guido von Rossum'  '31-01-1956  'male'  1.73]  get all key-value pairs and put into a list  [ 'name'  'Guido von Rossum')   'birth'  '31-01-1956')   'gender'  'male')   'height'  1.73)]  add a key with value  change value of a key  alter multiple keys & values  ### **Conditional Execution**  In[14]:  True  False  reassigning   True  True  False  0  empty sequence will be evaluated as False  equals: if bool_smb == TRUE  ### **Iteration**  In[17]:   1) inefficient and not pythonic!  i iterates over the two list of positions  0 1 2)  In[16]:   2) better  still not beautiful!  In[18]:   3) beautiful Python code!  for even more elegance:  print f'{unit}: {repo}')  In[19]:   4) even more elegant code!  In[21]:  example 1  ['Introduction Chapter'  'WebScrapingIntro Chapter']  example 2  stops whole loop  rejects all remaining statements and continues with next iteration   In[24]:  a loop embed in another loop  In[25]:  infinite loop  ### **Import Packages**  In[26]:   1) import <module_name>   for the import of the whole module  numpy   2) import <module_name> as <self-chosen_name>   for the import of the whole module with an alias numpy   3) from <module_name> import <name s)>   for the import of an specific object of the module  numpy array numpy  imports everything from the module numpy   4) from <module_name> import <name s)> as <self-chosen_name>  for the import of an specific object of the module with an alias numpy array  ### **Read & Write Files**  In[27]: removes leading and trailing whitespaces   In[28]:  simplified:   'r' and 'utf-8' are set by default  no need to call readlines ) explicitly  it’s the default  In[33]:  shakespeare.close ) will be called automatically and is redundant      In[34]:  BY WILLIAM SHAKESPEARE will be added to last line of the .txt file   # **Application** : Apple Stocks Data   **Input and Output**  In[35]: pandas  AAPL.csv file can be found in GitHub Repository of this Unit  <class 'pandas.core.frame.DataFrame'>  In[49]:  Getting the data for year 2021  In[50]:  shape of DataFrame  In[51]:  sorting by value  In[52]:  check monotonicity increasing through time  In[53]:  sorting by index  In[54]:  reset index as numeric  In[55]:  check null values  In[56]:  remove null values  In[57]:  set a column as index  In[ ]:  duplicates timestamp operations  In[58]:  operation  way1  potential risk  way2  drop by index  row operation. ~  take inverse  In[59]:  In[ ]:  a simple build-in plot function of pandas  In[60]:  Save the new data as json format,695
Augmento_data_import,Data request via Augmento.ai API (both daily and hourly topic values for BTC for 1 Nov 2016 – 30 Sep 2019). Preparation of data for further analysis by merging it with BTC price and volume data (the latter for daily data only). The output files are then saved in the parent folder.,"sentiments, cryptocurrency, bitcoin, market analysis, API request, data preparation","Gleb Zhidkov, Anna Shchekina, Vanessa Guarino",Augmento_data_import,2,"!/usr/bin/env python  coding: utf-8  In[1]: requests json pandas  In[2]:  make selection of months  last month in list won't be requested)  micro choice of months for testing months =  ""2019-07""  ""2019-08""  ""2019-09"")  In[3]:  get topic names from Augmento  In[4]:  prepare request of data from Augmento  bin_size = 24H / 1H request_and_clean source  bin_size=""24H""  coin='bitcoin'  months=months  initialize dataframe  loop over months and request data  unnest topic counts from requested data  change col names  clean data a bit  In[5]:  request and clean data for bin_size = 24H  default)  sum by position  dataframes are identical)  In[6]:  request and clean data for bin_size = 1H  is this required for further steps ???)  sum by position  dataframes are identical)  In[ ]:  check if all good: data_wide data_wide.describe ) data_wide.dtypes  In[7]:  prepare volume and price data 24h  source: https://coinmarketcap.com/currencies/bitcoin/historical-data/?start=20130429&end=20200129  omit rows w/ missing volume  join datasets  In[40]:  prepare price data 1h  source: http://www.cryptodatadownload.com/cdd/Coinbase_BTCUSD_h.csv  via http://www.cryptodatadownload.com/data/northamerican/   merge topics and prices  sort_index )  In[41]:  In[43]:  save data",158
DEDA_Class_2018WS_Quantlet_Scraper,Scraping tool to gather raw code from all available .R scripts in the Quantlet repository.,"Web Scraping, Tom Foolery",Alex Truesdale,DEDA_Class_2018WS_Quantlet_Scraper,1, -*- coding: utf-8 -*- requests_html HTMLSession bs4 BeautifulSoup time html sys csv re os  Introduction.  Initilise HMTL requests session.  Open source file as .csv and read in lines.  Define URL replacement pairings.  Compile RegEx pattern for 'or' replacement statement; test on dummy URL.  Initilise aggregate list container; loop through source URLs  find absolute links for .R files.  Redefine links_aggregate as set of unique URLs.  Save raw code links as .csv file.  Create dictionary pairing URLs to their raw contents.  Try except block to skip over  and identify) files with encoding problems  6 of ~1500... negligable.  Define aggregate_code_path; loop through pairwise dict. and write individual files  Append code to aggregate file.,110
Cryptopunks-Transactions-Analysis,"Trying to discover links between Cryptopunk prices and their attributes, using mainly Linear Regression","Cryptopunks, Collectible, NFT, Pricing, Feature Engineering",Danyal Ahmed and Kalina Sperber,Cryptopunks-Transactions-Analysis,18,!/usr/bin/env python  coding: utf-8  In[1]: pandas numpy seaborn matplotlib datetime  In[211]: helper numConcat num1  num2  In[2]:  In[3]:  In[4]: Fix amount values in eth and dollar  In[5]:  #EDA  In[220]:  # Dummy Encoding  In[221]:  In[222]:  In[224]:  In[225]:,35
Cryptopunks-Transactions-Analysis,"Trying to discover links between Cryptopunk prices and their attributes, using mainly Linear Regression","Cryptopunks, Collectible, NFT, Pricing, Feature Engineering",Danyal Ahmed and Kalina Sperber,Cryptopunks-Transactions-Analysis,18,!/usr/bin/env python  coding: utf-8  # Exploring Relationship of time and prices  In this Notebook we look into the hype of cryptopunks beginning in July / August 2020.   In[ ]: pandas numpy seaborn matplotlib datetime  In[ ]:  In[ ]:  In[ ]: Fix amount values in eth and dollar  In[ ]:  In[ ]:  In[ ]:  # Dates  In[ ]:  In[ ]: Join aggregate sales Join aggregate bids  In[ ]:  In[ ]:  In[ ]: pylab rcParams statsmodels  In[ ]:  In[ ]:  In[ ]:  In[ ]:,82
Cryptopunks-Transactions-Analysis,"Trying to discover links between Cryptopunk prices and their attributes, using mainly Linear Regression","Cryptopunks, Collectible, NFT, Pricing, Feature Engineering",Danyal Ahmed and Kalina Sperber,Cryptopunks-Transactions-Analysis,18,"!/usr/bin/env python  coding: utf-8  <a href=""https://colab.research.google.com/github/kalinakalina/DEDA_Cryptopunks/blob/main/EDA_accessoires.ipynb"" target=""_parent""><img src=""https://colab.research.google.com/assets/colab-badge.svg"" alt=""Open In Colab""/></a>  In[ ]:  imports pandas requests requests get bs4 BeautifulSoup numpy seaborn matplotlib  In[ ]:  helper functions fix_eth x  In[ ]:  import transaction   In[ ]:  import accessoires   In[ ]:  In[ ]:  # Types & Attributes  In[ ]: ')"":'number'   # Feature explanation  * number: the amount of crpytopunk that have this feature  * avail: amount of crpytopunks currently available for sale  * avg_sale: avg sale over the last 90 days  * cheapest: cheapest punktwith this attribute that is currently for sale  * more example: dropped as it is not readable like this  In[ ]:  scraping attributes data and transforming it ')"":'number'   # Graphical discovery for attributes  In[ ]:  In[ ]:  In[ ]:  In[ ]:  In[ ]: number of punks that have the same feature compared to average sale price over time  In[ ]:  # Dummy Encode  In[ ]:  In[ ]:  In[ ]:  In[ ]:",153
Cryptopunks-Transactions-Analysis,"Trying to discover links between Cryptopunk prices and their attributes, using mainly Linear Regression","Cryptopunks, Collectible, NFT, Pricing, Feature Engineering",Danyal Ahmed and Kalina Sperber,Cryptopunks-Transactions-Analysis,18,!/usr/bin/env python  coding: utf-8  In[1]: pandas numpy seaborn matplotlib plotly datetime scipy stats requests requests get bs4 BeautifulSoup xgboost sklearn.model_selection RepeatedKFold sklearn.model_selection cross_val_score numpy absolute sklearn.model_selection train_test_split sklearn.linear_model LinearRegression sklearn metrics linearmodels PooledOLS statsmodels linearmodels PanelOLS linearmodels RandomEffects sklearn.decomposition PCA prince mca sklearn.model_selection cross_val_score sklearn.model_selection RepeatedKFold sklearn.linear_model Lasso numpy mean numpy std numpy absolute sklearn.linear_model LassoCV numpy arange statsmodels.tools.eval_measures rmse statsmodels.tools.eval_measures meanabs statsmodels  In[2]:  In[3]: fix_df x  In[4]:  In[5]:  In[6]:  In[7]:  In[8]:  In[9]:  In[10]:  In[11]:  In[12]:  In[13]:  In[14]:  In[15]:  In[16]:  In[41]:  In[42]:  In[43]:  # Last Sale Reg with Selected Columns  In[34]:  In[35]:  In[36]:  In[40]:,94
Cryptopunks-Transactions-Analysis,"Trying to discover links between Cryptopunk prices and their attributes, using mainly Linear Regression","Cryptopunks, Collectible, NFT, Pricing, Feature Engineering",Danyal Ahmed and Kalina Sperber,Cryptopunks-Transactions-Analysis,18,!/usr/bin/env python  coding: utf-8  In[1]: pandas numpy seaborn matplotlib plotly datetime scipy stats  In[2]:  In[3]:  In[4]: Fix amount values in eth and dollar  In[5]:  In[6]:  In[7]:  # Bids and Sales  In[8]:  In[9]: Filter df with bids only Drop rows with withdrawn bids Sum the amount of bids in eth for each punk  In[10]:  In[11]: Select top 15 punks with highest amount in bids  In[12]:  In[13]:  In[14]: Select top 15 punks with highest amount in sales  In[15]: Create dataframe with counts of each transaction type for each punk  In[16]:  In[17]:  In[18]: Cryptopunks with no bids or sales  In[19]:  In[20]: Cryptopunks with zero sales  In[21]: Punks with the highest amount of bids but no sales  In[22]: Creating seperate df for amount of sales/transactions against each cryptopunk  # User Activity  In[23]: Looking at the 'from' column  In[24]:  Interestingly  the account page for user 0x78b58e shows no records of the bids or transaction made by this account. But we can verify all their bids and purchases through the transaction history data. Example - Punk 550.    Same for user 0x6611fe below  i.e punk 382.  In[25]:  In[26]: Looking at the 'to' column  In[27]:  # Dates  In[28]:  In[29]: Join aggregate sales Join aggregate bids  In[30]: mask =  date_df.index > '2017-06-27') &  date_df.index <= '2021-05-16') new_date_df = date_df.loc[mask]  In[31]:  In[32]:  In[264]: pylab rcParams statsmodels,216
Cryptopunks-Transactions-Analysis,"Trying to discover links between Cryptopunk prices and their attributes, using mainly Linear Regression","Cryptopunks, Collectible, NFT, Pricing, Feature Engineering",Danyal Ahmed and Kalina Sperber,Cryptopunks-Transactions-Analysis,18,!/usr/bin/env python  coding: utf-8  ## Install  Imports and Helper Functions  In[1]:  In[2]: pandas matplotlib matplotlib_venn venn3 scipy stats linearmodels PooledOLS linearmodels PanelOLS linearmodels RandomEffects sklearn metrics sklearn.linear_model LinearRegression sklearn.linear_model Lasso sklearn.linear_model LassoCV sklearn.model_selection train_test_split sklearn.model_selection cross_val_score sklearn.model_selection RepeatedKFold sklearn.metrics mean_squared_error sklearn.decomposition PCA numpy numpy mean numpy std numpy absolute numpy arange statsmodels statsmodels.tools.eval_measures rmse statsmodels.tools.eval_measures meanabs mlxtend.feature_selection SequentialFeatureSelector mlxtend.plotting plot_sequential_feature_selection lightgbm prince  In[3]:  Helper Function fix_df x outlier_truncation x  factor=1.5  Calculate IQR  Define upper/lower bound  Truncation  # Data Imports  In[4]:  Read Transactions  Read Accessoires Read Dummy Encoded Accessoires  In[5]:  # Data Preperation        In[6]: Average Sale Price for all Punks # Join accessoires and prices X = sm.add_constant X)  In[7]: Average Sale Price Without Punks that have 0 as average price  unsold) X_na = sm.add_constant X_na)  In[8]:  # Linear Regression including all data  Trained and tested on data that only non sales that average at 0.  In[9]:  In[16]:  In[ ]:  In[12]:  # Linear Regression without avg 0 Sales  In[15]:  In[19]:  In[ ]:  # Linear Regression with Lasso Regularization  In[20]:  In[23]:  In[24]:  # Step Wise Regression  In[25]: sfs.subsets_  In[ ]: 10 features  20 features  30 features  60 features  In[ ]:  20 features  Picture Export  print 'best combination  r2: %.3f): %s\n' %  sfsbackfit_20.k_score_  sfsbackfit_20.k_feature_names_))  fig_20 = plot_sfs sfsbackfit_20.get_metric_dict )  kind='std_err')  plt.title 'Sequential Forward Selection with 20 features  w. StdDev)')  plt.grid )  plt.show )  # Visualization Feature Selection  In[ ]:  In[ ]:  In[ ]:  print coeff_lasso_top_20)  print sfsbackfit_20.k_feature_names_)  print sigf)  # PCA  https://www.datacamp.com/community/tutorials/principal-component-analysis-in-python  In[ ]: sklearn.decomposition PCA  # MCA  In[ ]: mca_acc.explained_inertia_  In[ ]:  In[ ]:  In[ ]:  In[ ]:  # LGBM Regessor Gradient Boosting  Trained an testes on data that only has non null sales  In[ ]:  # Linear Regression without avg 0 sales and with outlier truncation  In[ ]:  In[ ]:  In[ ]: # Join accessoires and prices,296
Cryptopunks-Transactions-Analysis,"Trying to discover links between Cryptopunk prices and their attributes, using mainly Linear Regression","Cryptopunks, Collectible, NFT, Pricing, Feature Engineering",Danyal Ahmed and Kalina Sperber,Cryptopunks-Transactions-Analysis,18,!/usr/bin/env python  coding: utf-8  In[178]: requests bs4 BeautifulSoup pandas datetime  In[ ]:  In[180]:  In[181]:  In[182]:  In[183]: ':''}  regex=True)  In[184]: Join Prices Split Prices into two columns  In[185]: Remove special characters  In[186]: Convert strings into numbers  In[187]: Join Dates  In[189]:,39
Cryptopunks-Transactions-Analysis,"Trying to discover links between Cryptopunk prices and their attributes, using mainly Linear Regression","Cryptopunks, Collectible, NFT, Pricing, Feature Engineering",Danyal Ahmed and Kalina Sperber,Cryptopunks-Transactions-Analysis,18,!/usr/bin/env python  coding: utf-8  In[2]:  In[2]: pandas numpy seaborn matplotlib plotly datetime scipy stats sklearn.model_selection RepeatedKFold sklearn.model_selection cross_val_score numpy absolute sklearn.model_selection train_test_split sklearn.linear_model LinearRegression sklearn metrics linearmodels PooledOLS statsmodels linearmodels PanelOLS linearmodels RandomEffects sklearn.decomposition PCA sklearn.model_selection cross_val_score sklearn.model_selection RepeatedKFold numpy mean numpy std numpy absolute numpy arange statsmodels.tools.eval_measures rmse statsmodels.tools.eval_measures meanabs  In[3]:  In[4]: fix_df x  In[5]: Average Sale Price  In[6]:  In[7]:  In[8]:  In[9]:  # Simple Linear Regression  In[10]:  In[11]:  In[12]:  In[13]:  # Linear Regression with Time Dummies  In[16]:  In[17]:  In[18]:  In[19]:  In[20]:  In[21]:  In[22]:  # Panel Data Models with Time Dummies  In[27]:  ### Random Effects  In[28]:  In[29]:  ### Panel OLS  In[ ]: To get a full rank matrix,108
Cryptopunks-Transactions-Analysis,"Trying to discover links between Cryptopunk prices and their attributes, using mainly Linear Regression","Cryptopunks, Collectible, NFT, Pricing, Feature Engineering",Danyal Ahmed and Kalina Sperber,Cryptopunks-Transactions-Analysis,18,!/usr/bin/env python  coding: utf-8  In[17]: requests requests get bs4 BeautifulSoup pandas numpy collections OrderedDict re time sleep random randint  In[27]: fix_dollar x fix_eth x  In[29]: For cryptopunks with no bidding history  split the empty amount column and fetch the df For cryptopunks with bidding history  plit the string and convert the digits to float  then fetch the df,58
DEDA_2020_Sentiment-Analysis-of-Cryptocurriencies,This Project uses Twitter API to analyse the sentiments of Cryptocurriencies ,"Sentiment-Analysis, Sentiment, CRIX, Bitcoin, cryptocurrency","Sukanya Bhattacharyya, Shubham Senger",DEDA_2020_Sentiment-Analysis-of-Cryptocurrencies,1,"Codes for Streaming  And Tokenizing) matplotlib sys tweepy textblob TextBlob tweepy Stream tweepy.streaming StreamListener Codes for saving Table on_data self data on_error self  status Codes for Crearing a Table prettytable PrettyTable prettytable from_csv xlsxwriter Codes for Creating a Wordcloud wordcloud WordCloud matplotlib PIL Image numpy Code for Bar graph sys matplotlib nltk Streaming Tweets and cleaning  os pandas matplotlib seaborn itertools collections tweepy nltk nltk.corpus stopwords re networkx warnings cryptocurrency -filter:retweets"" Code for Tokenization Codes for top 50 Tweets  Cleaned Tweets) remove_url txt Code for Generating Words with frequencies Codes for forming data frame with the most frequent words  Unigram) Codes for removing stop words Codes returning words after removing stop words Codes for the new words with the frequency Stop words removed) Codes for generating Bar Graph with the most frequent words Codes for dataframe after removing stopwords  Final Unigram) ",141
MPT_Fast_Text_Visualization_Example,"Visualization Example for the Fast Text Word Embeddings. Pretrained FastText word embeddings vectors in German language are used. Dimensionality reduction fromm 300 to 3 via PCA. One sees, that the semantics are nicely extracted and capitals respectively countries are positioned close to each other and could be seperated along the second principal component.","Machine Learning (ML), NLP, Textual Analysis, Word Embeddings, SIF Embedding, FastText, PCA",Marvin Gauer,MPT_Fast_Text_Visualization_Example,1,os sys gensim.models FastText sklearn.decomposition PCA matplotlib pyplot mpl_toolkits.mplot3d.proj3d proj_transform matplotlib.text Annotation mpl_toolkits.mplot3d.art3d Line3DCollection numpy urllib sh gunzip __init__ self  s  xyz  *args  **kwargs draw self  renderer annotate3D ax  s  *args  **kwargs load_pretrained_FastText embeding_model_filename = 'cc.de.300.bin'  Moving animation ax.figure.set_size_inches width  height) make_gif files  output  delay=100  repeat=True  **kwargs rotanimate ax  angles  output  **kwargs  Example Sentences respectively words,56
SFM_Year_effect_bitcoin,This anomaly describes the increase in the prices of the Bitcoin in the last week of December and the first half month of January.,"Bitcoin, Finance, Year effect, EMH anomaly",Dragos Cioata,SFM_Year_effect_bitcoin,2,!/usr/bin/env python  coding: utf-8  In[4]: This anomaly describes the increase in the prices of the Bitcoin in the last week of December and the first half month of January. Author: Dragos Cioata Submitted: 04 Mar 2019 import pandas numpy scipy stats warnings matplotlib datetime astropy.table Table statsmodels.stats.multicomp pairwise_tukeyhsd read csv data kepp de date and de close price make date a datetime object extract the month print 5 rows  plot the evolution of the bitcoin close price print the  test restult for the hipothesis  In[ ]:,86
CMBcpuscrap,Extracts year of introduction and transistor count for Intel microprocessors introduced between 1971 and 2008 from the Intel Microprocessor Quick Reference Guide using XPath and pattern matching.,"time-series, empirical, discrete, tree, distance, preprocessing, data adjustment, transformation, data, text, web scraping, web data, parser, pattern matching, regular expressions, selection, data set, conditional, frequencies, html, dom, xpath","Torsten van den Berg, Sophie Burgard",CMBcpuscrap,1," -*- coding: utf-8 -*- os re hashlib warnings pandas requests lxml Levenshtein  Set path  Intel Microprocessor Quick Reference Guide  MD5 hash of working page fetch_html page_url  check_md5=False  Compare current and tested MD5 hash to identify changeg HTML  Minimal cleaning  <br> and &nbsp; are meaningless  break parser and are thrown out md5_working_test md5_current get_values_from_table table  Select columns containing year and transistor count  Drop table if length mismatches best_match_position string  list_of_strings  python-Levenshtein is not ideal although performance is good  Alternative methods and modules could be tested  Get position of closest match  XPath does not use zero-based numbering  therefore shift by 1  Not very pretty  still outperforms np.argmax  enumerate and others get_column_values table  heading  Table header rows have a gray background  always begin with ""Processor"" E3E3E3"" or @bgcolor = ""#e3e3e3"") '  Header row  list)  Find position of most similar header column  Extract nth column using XPath  Drop header from list extract_transistors text  - Match grouped comma "" "" delimited  thousands separated digits  Remove commata  Match last occurence of digit - word combinations like 3.2 million  Set exponent by numeral  Multiply mantissa by 10^6 or 10^9  NA if nothing meaningful can be found  Drop results later using pd.dropna ) extract_year date_text  Use last two chars of string to obtain yy  Transform yy to yyyy  Fetch HTML from URL and create LXML tree  XPath to tables containing CPU specs  Merge all tables' data frames  Extract and tranform proper data from fields  Drop NAs and transform to int  Reset overlapping index  Write to data frame CSV",250
Understanding Smart Contracts: Hype or Hope?,This Quantlet is dedicated to visualization of time-series of energy consumption of Ethereum,"energy, visualization, time-series, Ethereum, smart contracts","Elizaveta Zinovyeva, Raphael Constantin Georg Reule",SC-energy-consumption,2,!/usr/bin/env python  coding: utf-8  In[ ]: numpy pandas matplotlib  In[ ]: DIV/0!')  In[ ]:  In[ ]:  In[44]: bdb2ff' 023047',19
MLPClassification,Use machines learning algorithms to predict daily price move in Bitcoin and perform day trading,"Bitcoin, SVM, Random Forest, Neural Network, LSTM","Hong Yuxi, Lin Jinyu, Sun Rongsheng, Zhou Xiaoqian, Zou Yutong",MLPClassification,2,!/usr/bin/env python  coding: utf-8  In[1]:  !pip install sklearn numpy matplotlib sklearn.model_selection RandomizedSearchCV datetime pandas sklearn.neural_network MLPClassifier sklearn.metrics accuracy_score os os.path dirname  ## Load data  In[2]:  ### Train test data preparation  Suppose we are interested in the following train & test scenario:    Train: before 20200101     Test: 20200101~20201231  In[214]:  In[215]:  one day after the test period  In[216]:  In[217]:  In[218]:  ### Randomized search  In[219]:  In[220]:  In[221]:  In[261]:  ### Model training   In[226]: model_train x_train  y_train  ### Trading P&L  On a rolling window of 5 days  In[247]:  In[228]: trading strategy:1 long;0 short calculate trading strategy performance  In[236]:  In[211]: plot figure  In[246]:  calculate trading strategy volatility and Sharpe ratio  annualised)  In[ ]:,106
2D_Normal_Polar,Generating 2D Normal Distribution with Polar Coordinates,"2D, normal, polar, exponential, box-muller",Susan,2D_Normal_Polar,2,!/usr/bin/env python  coding: utf-8  In[1]: seaborn matplotlib numpy scipy  generate two sets of U 0  1) random variables  generate *independent* normal random variables using Box-Muller  In[ ]:,27
LLE_S,Plotting the S and sphere and reduction using LLE,ERROR,Elizaveta Zinovyeva,LLE_S,4,!/usr/bin/env python  coding: utf-8  In[17]:  Taken from https://scikit-learn.org/stable/auto_examples/manifold/plot_manifold_sphere.html#sphx-glr-auto-examples-manifold-plot-manifold-sphere-py  and https://scikit-learn.org/stable/auto_examples/manifold/plot_compare_methods.html  In[15]: matplotlib mpl_toolkits.mplot3d Axes3D sklearn.decomposition PCA sklearn manifold sklearn.utils check_random_state numpy  Next line to silence pyflakes. This import is needed.  Create figure  Add 3d scatter plot  In[12]:  In[13]:  LLE  Plot data with dimensionality reduction PCA  In[16]:  PCA on 2-dimensions  Plot data with dimensionality reduction PCA  In[21]:  In[27]:  Create our sphere.  Sever the poles from the sphere.  Plot our dataset.  In[32]:  In[35]:  PCA on 2-dimensions  Plot data with dimensionality reduction PCA  In[36]:  In[37]:  LLE  Plot data with dimensionality reduction PCA  In[ ]:,91
LLE_S,Plotting the S and sphere and reduction using LLE,ERROR,Elizaveta Zinovyeva,LLE_S,4,!/usr/bin/env python  coding: utf-8  In[6]: matplotlib mpl_toolkits.mplot3d Axes3D sklearn.decomposition PCA sklearn manifold sklearn.utils check_random_state numpy pandas  In[9]: plotly  In[10]:  Create our sphere.  Sever the poles from the sphere.  In[11]:  In[12]:  In[ ]:,32
SFElshill,"Reads the date, DAX index values, stock prices of 20 largest companies at Frankfurt Stock Exchange (FSE), FTSE 100 index values and stock prices of 20 largest companies at London Stock Exchange (LSE) and computes the Least Square (LS) and the Hill estimators of the tail index for all 42 analysed return processes.","data visualization, graphical representation, plot, financial, asset, stock-price, returns, time-series, dax, ftse100, index, descriptive-statistics, hill-estimator, least-squares, tail, index","Andrija Mihoci, Awdesch Melzer",QID-1955-SFElshill,1,numpy pandas sklearn sklearn.linear_model LinearRegression  date  S t)  r t) = log S t)) - log S t-1))   sample size  columns or r  Right tail index regression and Hill estimator  m1 largest observations  m2 largest observations Sort each column loop over the columns of the return data frame and fit regression reshape data  fit the regression model  append coefficients to list assemble coefficients to a data frame,67
Modelling_Baseline,Modelling_Baseline,"lightgbm, XGBoost, feature engineering, model encoder, AUC, credit card default, feature selection","Cao Yi, Chen Chi, Ma Haochun, Xu Xiaoxuan, Ji Xiongzhang",Modelling_Baseline,4,"!/usr/bin/env python  coding: utf-8  # LightGBM  In[112]:  pip install xgboost  In[17]: pandas sklearn.model_selection train_test_split lightgbm xgboost hyperopt fmin sklearn.preprocessing LabelEncoder gc matplotlib seaborn functools partial pprint pprint numpy hyperopt.pyll scope plotly express plotly graph_objects plotly offline sklearn.metrics make_scorer sklearn.model_selection cross_val_score sklearn.utils check_random_state  In[44]:  label encoder  In[19]:  In[20]:  prepare the test set for the final evaluation  test_X = data[data['label'].isnull )]  test_y = pd.read_csv 'test_label_new.csv'  index_col=0)  test = pd.merge test_X  test_y  on='user')  test.shape  In[21]:  In[22]:  In[23]:  # optimal parameters  {'feature_fraction': 0.59     'learning_rate': 0.08748742858194956     'max_depth': 14     'num_leaves': 25     'reg_alpha': 10     'reg_lambda': 13     'sub_sample': 0.56}  In[24]:  In[25]:  In[26]:  In[27]:  In[41]:  计算ROC的值 svm_threasholds为阈值  In[29]:  print df_importance_top100.iloc[: 0].tolist ))  In[30]:  feature importance visualization  sorted zip clf.feature_importances_  X.columns)  reverse=True)  feature_imp = pd.DataFrame sorted zip clf.feature_importances_ X.columns))  columns=['Value' 'Feature'])  plt.title 'LightGBM Features  avg over folds)')  plt.show )  # XGBoost  In[31]:                              tree_method='gpu_hist' \n                           random_state=seed)\n\ndf_oof = df_train[['user'  ycol]].copy )\ndf_oof['prob'] = 0\nprediction = df_test[['user']]\nprediction['prob'] = 0\ndf_importance_list = []\n\nkfold = StratifiedKFold n_splits=5  shuffle=True  random_state=seed)\nfor fold_id   trn_idx  val_idx) in enumerate \n        kfold.split df_train[feature_names]  df_train[ycol])):\n    X_train = df_train.iloc[trn_idx][feature_names]\n    Y_train = df_train.iloc[trn_idx][ycol]\n\n    X_val = df_train.iloc[val_idx][feature_names]\n    Y_val = df_train.iloc[val_idx][ycol]\n\n    print '\\nFold_{} Training ================================\\n'.format \n        fold_id + 1))\n\n    lgb_model = model.fit X_train \n                          Y_train \n                          eval_set=[ X_train  Y_train)   X_val  Y_val)] \n                          verbose=100 \n                          eval_metric='auc'  \n                          early_stopping_rounds=50)\n\n    pred_val = lgb_model.predict_proba X_val )[:  1]\n    df_oof.loc[val_idx  'prob'] = pred_val\n\n    pred_test = lgb_model.predict_proba df_test[feature_names])[:  1]\n    prediction['prob'] += pred_test / kfold.n_splits\n\n    df_importance = pd.DataFrame {\n        'column': feature_names \n        'importance': lgb_model.feature_importances_ \n    })\n    df_importance_list.append df_importance)\n\n    del lgb_model  pred_val  pred_test  X_train  Y_train  X_val  Y_val\n    gc.collect )"")  In[32]:  In[33]:  In[34]:  In[35]:  In[36]:  In[37]:  In[42]: sklearn.metrics make_scorer  计算ROC的值 svm_threasholds为阈值  In[39]:  print df_importance_top100.iloc[: 0].tolist ))  In[40]:  feature importance visualization  sorted zip clf.feature_importances_  X.columns)  reverse=True)  feature_imp = pd.DataFrame sorted zip clf.feature_importances_ X.columns))  columns=['Value' 'Feature'])  plt.title 'LightGBM Features  avg over folds)')  plt.show )  In[ ]:  In[ ]:  In[ ]:",290
Modelling_Encoding_Target,Modelling_Encoding,"lightgbm, XGBoost, feature engineering, model encoder, AUC, credit card default, feature selection","Cao Yi, Chen Chi, Ma Haochun, Xu Xiaoxuan, Ji Xiongzhang",Modelling_Encoding,6,"!/usr/bin/env python  coding: utf-8  # LightGBM  In[112]:  pip install xgboost  In[10]: pandas sklearn.model_selection train_test_split lightgbm xgboost hyperopt fmin sklearn.preprocessing LabelEncoder gc matplotlib seaborn functools partial pprint pprint numpy hyperopt.pyll scope plotly express plotly graph_objects plotly offline sklearn.metrics make_scorer sklearn.model_selection cross_val_score sklearn.utils check_random_state  In[5]:  label encoder  In[11]:  count encoder category_encoders  In[ ]:  target encoder category_encoders  In[12]:  In[30]:  prepare the test set for the final evaluation  test_X = data[data['label'].isnull )]  test_y = pd.read_csv 'test_label_new.csv'  index_col=0)  test = pd.merge test_X  test_y  on='user')  test.shape  In[13]:  In[14]:  In[172]:  # optimal parameters  {'feature_fraction': 0.59     'learning_rate': 0.08748742858194956     'max_depth': 14     'num_leaves': 25     'reg_alpha': 10     'reg_lambda': 13     'sub_sample': 0.56}  In[15]:  In[16]:  In[17]:  In[18]:  In[19]:  计算ROC的值 svm_threasholds为阈值  In[20]:  print df_importance_top100.iloc[: 0].tolist ))  In[21]:  feature importance visualization  sorted zip clf.feature_importances_  X.columns)  reverse=True)  feature_imp = pd.DataFrame sorted zip clf.feature_importances_ X.columns))  columns=['Value' 'Feature'])  plt.title 'LightGBM Features  avg over folds)')  plt.show )  # XGBoost  In[ ]:                              tree_method='gpu_hist' \n                           random_state=seed)\n\ndf_oof = df_train[['user'  ycol]].copy )\ndf_oof['prob'] = 0\nprediction = df_test[['user']]\nprediction['prob'] = 0\ndf_importance_list = []\n\nkfold = StratifiedKFold n_splits=5  shuffle=True  random_state=seed)\nfor fold_id   trn_idx  val_idx) in enumerate \n        kfold.split df_train[feature_names]  df_train[ycol])):\n    X_train = df_train.iloc[trn_idx][feature_names]\n    Y_train = df_train.iloc[trn_idx][ycol]\n\n    X_val = df_train.iloc[val_idx][feature_names]\n    Y_val = df_train.iloc[val_idx][ycol]\n\n    print '\\nFold_{} Training ================================\\n'.format \n        fold_id + 1))\n\n    lgb_model = model.fit X_train \n                          Y_train \n                          eval_set=[ X_train  Y_train)   X_val  Y_val)] \n                          verbose=100 \n                          eval_metric='auc'  \n                          early_stopping_rounds=50)\n\n    pred_val = lgb_model.predict_proba X_val )[:  1]\n    df_oof.loc[val_idx  'prob'] = pred_val\n\n    pred_test = lgb_model.predict_proba df_test[feature_names])[:  1]\n    prediction['prob'] += pred_test / kfold.n_splits\n\n    df_importance = pd.DataFrame {\n        'column': feature_names \n        'importance': lgb_model.feature_importances_ \n    })\n    df_importance_list.append df_importance)\n\n    del lgb_model  pred_val  pred_test  X_train  Y_train  X_val  Y_val\n    gc.collect )"")  In[170]:  In[143]:  In[161]:  In[162]:  In[163]:  In[164]:  In[167]:  计算ROC的值 svm_threasholds为阈值  In[ ]:  print df_importance_top100.iloc[: 0].tolist ))  In[ ]:  feature importance visualization  sorted zip clf.feature_importances_  X.columns)  reverse=True)  feature_imp = pd.DataFrame sorted zip clf.feature_importances_ X.columns))  columns=['Value' 'Feature'])  plt.title 'LightGBM Features  avg over folds)')  plt.show )  In[ ]:",296
Modelling_Encoding_Target,Modelling_Encoding,"lightgbm, XGBoost, feature engineering, model encoder, AUC, credit card default, feature selection","Cao Yi, Chen Chi, Ma Haochun, Xu Xiaoxuan, Ji Xiongzhang",Modelling_Encoding,6,"!/usr/bin/env python  coding: utf-8  # LightGBM  In[112]:  pip install xgboost  In[1]: pandas sklearn.model_selection train_test_split lightgbm xgboost hyperopt fmin sklearn.preprocessing LabelEncoder gc matplotlib seaborn functools partial pprint pprint numpy hyperopt.pyll scope plotly express plotly graph_objects plotly offline sklearn.metrics make_scorer sklearn.model_selection cross_val_score sklearn.utils check_random_state  In[2]:  cat boost encoder category_encoders  In[3]:  In[ ]:  In[6]:  prepare the test set for the final evaluation  test_X = data[data['label'].isnull )]  test_y = pd.read_csv 'test_label_new.csv'  index_col=0)  test = pd.merge test_X  test_y  on='user')  test.shape  In[7]:  In[12]:  In[172]:  # optimal parameters  {'feature_fraction': 0.59     'learning_rate': 0.08748742858194956     'max_depth': 14     'num_leaves': 25     'reg_alpha': 10     'reg_lambda': 13     'sub_sample': 0.56}  In[13]:  In[14]:  In[15]:  In[16]:  In[17]:  计算ROC的值 svm_threasholds为阈值  In[18]:  print df_importance_top100.iloc[: 0].tolist ))  In[19]:  feature importance visualization  sorted zip clf.feature_importances_  X.columns)  reverse=True)  feature_imp = pd.DataFrame sorted zip clf.feature_importances_ X.columns))  columns=['Value' 'Feature'])  plt.title 'LightGBM Features  avg over folds)')  plt.show )  # XGBoost  In[20]:                              tree_method='gpu_hist' \n                           random_state=seed)\n\ndf_oof = df_train[['user'  ycol]].copy )\ndf_oof['prob'] = 0\nprediction = df_test[['user']]\nprediction['prob'] = 0\ndf_importance_list = []\n\nkfold = StratifiedKFold n_splits=5  shuffle=True  random_state=seed)\nfor fold_id   trn_idx  val_idx) in enumerate \n        kfold.split df_train[feature_names]  df_train[ycol])):\n    X_train = df_train.iloc[trn_idx][feature_names]\n    Y_train = df_train.iloc[trn_idx][ycol]\n\n    X_val = df_train.iloc[val_idx][feature_names]\n    Y_val = df_train.iloc[val_idx][ycol]\n\n    print '\\nFold_{} Training ================================\\n'.format \n        fold_id + 1))\n\n    lgb_model = model.fit X_train \n                          Y_train \n                          eval_set=[ X_train  Y_train)   X_val  Y_val)] \n                          verbose=100 \n                          eval_metric='auc'  \n                          early_stopping_rounds=50)\n\n    pred_val = lgb_model.predict_proba X_val )[:  1]\n    df_oof.loc[val_idx  'prob'] = pred_val\n\n    pred_test = lgb_model.predict_proba df_test[feature_names])[:  1]\n    prediction['prob'] += pred_test / kfold.n_splits\n\n    df_importance = pd.DataFrame {\n        'column': feature_names \n        'importance': lgb_model.feature_importances_ \n    })\n    df_importance_list.append df_importance)\n\n    del lgb_model  pred_val  pred_test  X_train  Y_train  X_val  Y_val\n    gc.collect )"")  In[21]:  In[22]:  In[23]:  In[24]:  In[25]:  In[26]:  In[28]: sklearn.metrics make_scorer  计算ROC的值 svm_threasholds为阈值  In[29]:  print df_importance_top100.iloc[: 0].tolist ))  In[30]:  feature importance visualization  sorted zip clf.feature_importances_  X.columns)  reverse=True)  feature_imp = pd.DataFrame sorted zip clf.feature_importances_ X.columns))  columns=['Value' 'Feature'])  plt.title 'LightGBM Features  avg over folds)')  plt.show )  In[ ]:  In[ ]:",292
Modelling_Encoding_Target,Modelling_Encoding,"lightgbm, XGBoost, feature engineering, model encoder, AUC, credit card default, feature selection","Cao Yi, Chen Chi, Ma Haochun, Xu Xiaoxuan, Ji Xiongzhang",Modelling_Encoding,6,"!/usr/bin/env python  coding: utf-8  # LightGBM  In[112]:  pip install xgboost  In[1]: pandas sklearn.model_selection train_test_split lightgbm xgboost hyperopt fmin sklearn.preprocessing LabelEncoder gc matplotlib seaborn functools partial pprint pprint numpy hyperopt.pyll scope plotly express plotly graph_objects plotly offline sklearn.metrics make_scorer sklearn.model_selection cross_val_score sklearn.utils check_random_state  In[16]:  label encoder  In[15]:  count encoder category_encoders  In[36]:  target encoder category_encoders  In[43]:  cat boost encoder category_encoders  In[ ]:  In[30]:  prepare the test set for the final evaluation  test_X = data[data['label'].isnull )]  test_y = pd.read_csv 'test_label_new.csv'  index_col=0)  test = pd.merge test_X  test_y  on='user')  test.shape  In[17]:  In[22]:  In[172]:  # optimal parameters  {'feature_fraction': 0.59     'learning_rate': 0.08748742858194956     'max_depth': 14     'num_leaves': 25     'reg_alpha': 10     'reg_lambda': 13     'sub_sample': 0.56}  In[15]:  In[16]:  In[17]:  In[18]:  In[19]:  计算ROC的值 svm_threasholds为阈值  In[20]:  print df_importance_top100.iloc[: 0].tolist ))  In[21]:  feature importance visualization  sorted zip clf.feature_importances_  X.columns)  reverse=True)  feature_imp = pd.DataFrame sorted zip clf.feature_importances_ X.columns))  columns=['Value' 'Feature'])  plt.title 'LightGBM Features  avg over folds)')  plt.show )  # XGBoost  In[ ]:                              tree_method='gpu_hist' \n                           random_state=seed)\n\ndf_oof = df_train[['user'  ycol]].copy )\ndf_oof['prob'] = 0\nprediction = df_test[['user']]\nprediction['prob'] = 0\ndf_importance_list = []\n\nkfold = StratifiedKFold n_splits=5  shuffle=True  random_state=seed)\nfor fold_id   trn_idx  val_idx) in enumerate \n        kfold.split df_train[feature_names]  df_train[ycol])):\n    X_train = df_train.iloc[trn_idx][feature_names]\n    Y_train = df_train.iloc[trn_idx][ycol]\n\n    X_val = df_train.iloc[val_idx][feature_names]\n    Y_val = df_train.iloc[val_idx][ycol]\n\n    print '\\nFold_{} Training ================================\\n'.format \n        fold_id + 1))\n\n    lgb_model = model.fit X_train \n                          Y_train \n                          eval_set=[ X_train  Y_train)   X_val  Y_val)] \n                          verbose=100 \n                          eval_metric='auc'  \n                          early_stopping_rounds=50)\n\n    pred_val = lgb_model.predict_proba X_val )[:  1]\n    df_oof.loc[val_idx  'prob'] = pred_val\n\n    pred_test = lgb_model.predict_proba df_test[feature_names])[:  1]\n    prediction['prob'] += pred_test / kfold.n_splits\n\n    df_importance = pd.DataFrame {\n        'column': feature_names \n        'importance': lgb_model.feature_importances_ \n    })\n    df_importance_list.append df_importance)\n\n    del lgb_model  pred_val  pred_test  X_train  Y_train  X_val  Y_val\n    gc.collect )"")  In[170]:  In[143]:  In[161]:  In[162]:  In[163]:  In[164]:  In[167]:  计算ROC的值 svm_threasholds为阈值  In[ ]:  print df_importance_top100.iloc[: 0].tolist ))  In[ ]:  feature importance visualization  sorted zip clf.feature_importances_  X.columns)  reverse=True)  feature_imp = pd.DataFrame sorted zip clf.feature_importances_ X.columns))  columns=['Value' 'Feature'])  plt.title 'LightGBM Features  avg over folds)')  plt.show )  In[ ]:",301
CSC_contracts_over_time,Plotting contracts over time,ERROR,"Elizaveta Zinovyeva, Raphael Constantin Georg Reule",CSC_contracts_over_time,2,!/usr/bin/env python  coding: utf-8  In[1]: pandas numpy matplotlib seaborn  In[2]:  In[4]:  In[12]:  In[25]: DB3B28' ax1.set_ylabel 'New contracts'  color=color)  instantiate a second axes that shares the same x-axis ax2.set_ylabel 'Total contracts'  color=color)  # we already handled the x-label with ax1  otherwise the right y-label is slightly clipped plt.show )  In[ ]:,50
Group_6_Project_HMM_Bitcoin_model,"Construction, State analysis and back test of HMM model","HMM, Bitcoin, backtest, states, trading","Changjie Jin, Ng Zhi Wei Aaron, You Pengxin, Chio Qi Jun, Zhang Yuxi",Group_6_Project_HMM_Bitcoin_model,1," -*- coding: utf-8 -*- numpy pandas matplotlib cm hmmlearn.hmm GaussianHMM scipy warnings data from excel plot initial bitcoin prices hmm brute force method to get the best model using diag covariance  default) and full covariance get_best_hmm_model X  max_states  max_iter = 10000  General plots of hidden states plot_hidden_states hmm_model  data  X  column_price {0} hidden state"".format i)) futures returns plot i+1 {0} hidden state"".format i))  #futures return cumm sum {0} hidden state"".format i))  Statistical features of states mean_confidence_interval vals  confidence compare_hidden_states hmm_model  cols_features  conf_interval  iters = 10000  Samples generation  Feature params Statistical features  Create features  Plot features  Split the data on sets price data plot and visualise the hidden states compare the hidden states Run backtest exit at rand state backtest hmm_model  testdata  testset init portfolio states definition conditions calculation of PnL run model on test set plot OOS states Plot states plot for test set data plot initial bitcoin prices",149
SDA_2020_St_Gallen_01_DataImport,"Fetching, Preprocessing and Plotting data. This Quantle is part of other quantlets and represents the first step. The overall goal is to build a trading bot, which analyzes a single timeseries of crypto currency prices (BTC/USD) to identify optimal conditions for entry and exit trades in real-time. This quantlet retrieves data from a github repository, which is minutely data of BTC/USD from Binance of the years 2013 to 2019.","QLearning, Trading Bot, BTC, Bitcoin, Timeseries, Technical Indicators","Tobias Mann, Tim Graf",SDA_2020_St_Gallen_01_DataImport,1,pandas os datetime matplotlib numpy ## FETCH DATA FROM GITHUB  ------------------------------ https://github.com/Zombie-3000/Bitfinex-historical-data  Merge dataframes  check length and compare to theoretical value print 'Theoretical maximum obervations:'  60*24*365*7) print 'Actual observations of df:' len df_merged)) convert timestamp to datae  reset index and sort values according to Time print output  save the new file under the folder Data ## MAKE SUBSETS OF DATA  ------------------------------  create and save subsets of Decembers make_subset  df  start_window  end_window  name  create subsets ## MAKE PLOTS FROM THE DATA  ------------------------------  plot the full timeframe  plot the different timeframes  plot every year,92
SFERealDataExMA,Investigates time series of real financial data and finds appropriate moving average model,"time series, MA, moving average, real data, financial data, python","Bharathi Srinivasan, David Berscheid",SFERealDataExMA,1,MA3  os sys pandas pandas_datareader numpy statsmodels statsmodels statsmodels scipy arch arch_model matplotlib matplotlib  raw adjusted close prices  log returns tsplot y  lags=None  figsize= 10  8)  style='bmh' mpl.rcParams['font.family'] = 'Ubuntu Mono'  Fit MA 3) to SPY returns,37
Augmento_jumps,"Heuristical approach to analysis of Augmento.ai sentiments/topics (daily values) for states of extreme BTC price in- and decreases (referred as jumps) by comparing the mean value of each topic index in the positive or negative jump state and with the mean value in the whole dataset to identify the topics with the highest mean difference ratio. Two analysis are performed: based on real time data (realTime) and volume-adjusted time data (imagTime), for which the Augmento.ai dataset is updated for the new time measure as Volume_t/Volume^CMA_t, where CMA is the central moving average with a window of 300 days","sentiments, cryptocurrency, bitcoin, market analysis, outlier analysis, jumps, CMA, volume-adjusted time","Gleb Zhidkov, Anna Shchekina, Vanessa Guarino",Augmento_jumps,4,"!/usr/bin/env python  coding: utf-8  In[1]: pandas numpy matplotlib  In[2]:  Load data and sort chronologically  In[3]:  In[4]:   style=""."") ax = work.plot y=""BTC_Price_diff""  x=""Date""  label=""Relative difference in BTC Price""  figsize= sizePar*2  sizePar/2)) quantile .25)  In[5]:  In[6]: Comparison outl  data x =  outl.mean ) - data.mean )) / data.mean ) return x)  In[7]:  In[8]:",51
Augmento_jumps,"Heuristical approach to analysis of Augmento.ai sentiments/topics (daily values) for states of extreme BTC price in- and decreases (referred as jumps) by comparing the mean value of each topic index in the positive or negative jump state and with the mean value in the whole dataset to identify the topics with the highest mean difference ratio. Two analysis are performed: based on real time data (realTime) and volume-adjusted time data (imagTime), for which the Augmento.ai dataset is updated for the new time measure as Volume_t/Volume^CMA_t, where CMA is the central moving average with a window of 300 days","sentiments, cryptocurrency, bitcoin, market analysis, outlier analysis, jumps, CMA, volume-adjusted time","Gleb Zhidkov, Anna Shchekina, Vanessa Guarino",Augmento_jumps,4,"!/usr/bin/env python  coding: utf-8  In[1]: pandas numpy matplotlib  In[2]:  Load data and sort chronologically  In[3]: work = work[[""Date""  ""BTC_Volume""  ""BTC_""]]  first 24 dates are omitted work[""Vol_MA""] = work[""Vol-25""].rolling 100  min_periods=1).mean )  In[4]:  In[5]: work.plot x=""Date""  y=""ImagDay""  label=""Imaginary day""  figsize= sizePar  sizePar/2))  In[6]: imagData[""im""]  In[7]:  In[8]: ImagDataNew  In[9]:  In[10]:  In[11]: ax = ImagDataNew.reset_index ).plot y=""BTC_Price_diff""  x=""index""  label=""Relative difference in BTC Price""  figsize= sizePar*2  sizePar/2))   style=""."") quantile .25)  In[12]:  In[13]: Comparison outl  data x =  outl.mean ) - data.mean )) / data.mean ) return x)  In[14]:  In[15]:",85
spellcheck_seq2seq_3rditeration,Final Iteration- Final implementation,ERROR,"Huong Vu 0856156, Susan Ku 0856706",spellcheck_seq2seq,2,"!/usr/bin/env python  coding: utf-8  In[1]: __future__ unicode_literals io open unicodedata string re random time math torch torch torch optim torch torch.utils data matplotlib matplotlib numpy pandas json os system nltk.translate.bleu_score SmoothingFunction  In[2]: ----------Hyper Parameters----------# The number of vocabulary 29  torch.manual_seed 1236)  In[3]: asMinutes s timeSince since  percent  #### Show loss  In[4]: show_result score  loss  #### BLEU  In[5]: compute BLEU-4 score compute_bleu output  reference  #### Get data  In[6]: getData mode  ### Build vocabulary  In[7]: __init__ self  input the training data and build vocabulary build_vocab self  corpus  convert word in indices word2indices self  word  add_eos=False  add_sos=False  padding input of same target into same length  convert indices to word indices2word self  indices  In[8]:  ### Data Loader  In[9]: __init__ self  mode  vocab __len__ self __getitem__ self  index  convert  multi-input)+target into multi- input+target) pair convert_pair self  In[10]:  In[11]:  ### Encoder  Input  1  batch_size)  In[12]: __init__ self  input_size  hidden_size forward self  input  hidden initHidden self  batch_size=32  ### Decoder  Input  1  batch_size)  In[13]: __init__ self  hidden_size  output_size forward self  input  hidden initHidden self  batch_size=32  #### Train  In[14]:  transpose tensor from  batch_size  seq_len) to  seq_len  batch_size)  calculate number of time step ----------sequence to sequence part for encoder----------# ----------sequence to sequence part for decoder----------#  Teacher forcing: Feed the target as the next input  Teacher forcing  Without teacher forcing: use its own predictions as the next input  detach from history as input  #### Train iteration  In[15]:  Reset every print_every  Reset every plot_every  create dataloader  evaluate and save model  #### Evaluate  In[16]:  print the prediction and return the bleu score show_prediction inputs  prediction  targets  plot_pred  In[17]: evaluate encoder  decoder  vocab  batch_size=32  plot_pred=False  create dataloader      testset = SpellingLoader 'test'  vocab)  convert input&target into string  transpose tensor from  batch_size  seq_len) to  seq_len  batch_size)  calculate number of time step ----------sequence to sequence part for encoder----------# ----------sequence to sequence part for decoder----------#  detach from history as input  get predict indices  convert indices into string  calculate average BLEU score  #### main  In[18]:  In[19]: encoder = torch.load ""./models/encoder_0.8785.ckpt"") decoder = torch.load ""./models/decoder_0.8785.ckpt"")",325
SDA_2019_St_Gallen_PD_with_ML_and_Sentiment_Analysis_Outlier_detection_Oversampling_Feature_Selection,"Identify and remove outliers, perform oversampling, select the 44 most important features of the dataset","outlier detection, isolation forest, oversampling, random naive oversampling, synthetic minority oversampling, adaptive synthetic",ERROR,Outlier_detection_Oversampling_Feature_Selection,2, -*- coding: utf-8 -*- os pandas numpy sklearn.ensemble IsolationForest outlier decection using IsolationForest Find the number of anomalies and normal points here points classified -1 are anomalous  matching anomalies to y-dataset  save cleaned datasets,34
SDA_2019_St_Gallen_PD_with_ML_and_Sentiment_Analysis_Outlier_detection_Oversampling_Feature_Selection,"Identify and remove outliers, perform oversampling, select the 44 most important features of the dataset","outlier detection, isolation forest, oversampling, random naive oversampling, synthetic minority oversampling, adaptive synthetic",ERROR,Outlier_detection_Oversampling_Feature_Selection,2," -*- coding: utf-8 -*- os pandas numpy sklearn decomposition sklearn.model_selection train_test_split matplotlib mpl_toolkits.mplot3d Axes3D seaborn sklearn.ensemble ExtraTreesClassifier  Set working directory os.chdir r'C:\Users\KAT\Documents\Project\Oversampling') remove data from 2018 check if the dataset only contains unique combinations of cik and year check_unique df merge Dataset into one table check that the dataset do not contain nan or inf values reduce the components of the dataset reduce_components X  X_name  y  top_n  do_plot X_top =  X.iloc[: [list X_all.columns).index ""fyear"")] + [list X_all.columns).index ""cik"")] + [list X_all.columns).index ""conm"")] + list indices[:top_n])] create testing and training sets save the test sets plotting functions create_3Dplot X  y  angle  name  transformer = [] Create 3D Plot create_2Dplot X  y  name  transformer = [] Create 3D Plot plot the number of defaults vs non-defaults in the training dataset plot the number of defaults vs non-defaults in the test dataset Run a loop to Oversample different format of the data Random naive over-sampling imblearn.over_sampling RandomOverSampler Synthetic minority oversampling imblearn.over_sampling SMOTE Adaptive synthetic imblearn.over_sampling ADASYN",163
DEDA_SVM_Nonlinear,Determines and plots the decision boundaries of a SVM classifier for different kernels and regularization parameters C.,"Support vector machines, SVM, classification",Georg Keilbar,DEDA_SVM_Nonlinear,1,sklearn.datasets make_moons numpy matplotlib sklearn svm  Simulate data plot_svm_nonlinear x  y  model_class  **model_params Fit model Define grid Prediction on grid Contour + scatter plot,24
FRM_7_FRM_vs_BBW,Compare the weighted BBW of the total coins used in the analysis to FRM Crypto and get the boxplot of individual lambdas(individual FRM for each coin),"Cryptocurrency, Bollinger Bands Width, volatility of hourly price, risk measure, FRM Crypto,box plot","Qi Wu, Seokhee Moon",FRM_7_FRM_vs_BBW,2,"!/usr/bin/env python  coding: utf-8  In[1]: numpy pandas plotly plotly matplotlib  ## Compare weighted BBW and FRM ws. 48  In[2]:  In[14]:  In[43]:  fig = go.Figure )  fig.add_trace go.Scatter x=lambdas.index  y=lambdas.FRM[lambdas.FRM<2]  name = 'FRM'  opacity = 0.8                             line =dict color= 'rgb 1 1 1)')  width = 0.8)))  fig.add_trace go.Scatter x=BBW.time  y=BBW.iloc[: -1]  name = 'BBW_weighted'  opacity = 1                             line =dict color= 'rgb 255 0 0)')  width = 1.5)))  fig.update_layout go.Layout               paper_bgcolor='rgba 0 0 0 0)'               plot_bgcolor='rgba 0 0 0 0)'          ))  fig.update_layout title=""BBW vs FRM"")  plotly.offline.plot fig filename=""FRM with weighted BBW.html"")  ## boxplot of individual Lambdas  In[10]:  In[41]:  In[42]:  In[13]:  hours for a week  In[29]: 25 weeks  In[35]:  In[46]:  make a boxplot based on individual lambda value in a week  regard every individual lambdas from different coins indifferent   In[47]:  nr. of columns is the nr. of weeks  each column contains 2016 =12coins*24hours*7days) lambda values for a single week from 12 coins  In[48]:  In[49]:  cut extrem values for a better view",158
Augmento_lasso,"Applying OLS and Lasso regressions on Augmento.ai sentiments/topics data (daily values) to identify the topics that have an explanatory value in explaining the differences in the BTC price. Data is transformed to log ratios, such that the formula for the Lasso regression is log(P_t/P_t-1) = log(X_t/X_t-1) + log(X_t/X_t-2), where X denotes the set of explanatory variables (topics and BTC volume as control variable).","sentiments, cryptocurrency, bitcoin, market analysis, OLS, Lasso","Gleb Zhidkov, Anna Shchekina, Vanessa Guarino",Augmento_lasso,2,!/usr/bin/env python  coding: utf-8  In[2]: pandas numpy seaborn csv sklearn.linear_model LinearRegression statsmodels sklearn.linear_model LassoCV sklearn.linear_model Lasso mlxtend.feature_selection ExhaustiveFeatureSelector  In[9]:  approach with lagged variables  Load data 24h  Replace zeroes  Make lagged variables  Merge  remove lines with NA's  Define X and Y  Overview  In[16]:  In[17]:  In[18]:  In[19]:  In[20]:  In[21]:  In[22]:  In[29]:  In[30]:  In[31]:  In[32]:,52
SDA_2020_St_Gallen_Bitcoin_Sentiment_Analysis_Using_Tweets,"Analysis of Bitcoin fluctuations using sentiments from tweets that were scraped from Twitter. The script can scrape the last 7 days of tweets and minutely BTC prices, and display a cross-correlation measure between sentiment scores and measures (return and volatility). The script includes a simple user interface, where the user can choose to use sample data or scrape real time.","Twitter, Sentiment, Bitcoin, Crypto, Cryptocurrency, Scraping","Mihai Atimut, Sandor Lukacs, Arpad Gerber",SDA_2020_St_Gallen_Bitcoin_Sentiment_Analysis_Using_Tweets,1," IMPORTING PACKAGES pandas twython Twython vaderSentiment.vaderSentiment SentimentIntensityAnalyzer cryptocompare datetime datetime re time os sys math numpy matplotlib pyscagnostics scagnostics sklearn preprocessing seaborn  IMPORTANT REQUIREMENTS os.chdir """")  PLEASE SET WORKING DIRECTORY PRIOR TO THE RUN os.chdir """") ########################################################################### ########################### DEFINING FUNCTIONS ############################ ########################################################################### ExtractTweets sample=True  If the user chose to use the sample tweets  Setting up Twitter API  Setting up the extraction  Variables to keep track  Reset data buffers  one for each dataframe column)  If it is the first iteration  download the 100 most  recent tweets btc"" count='100'  lang=""en"")  If it is not the first iteration:  Pass the oldest tweets id to the max_id parameter  Download the 100 tweets before the max_id  After the first call we should have max_id from result   of previous call. Pass it in query. btc"" include_entities='true'   Counter to track the scraper extraction  Loop trough the results and extract the needed data  STEP 3: Get the next max_id  Parse the data returned to get max_id to   be passed in consequent call.  No more next pages  If API limit is reached the script waits for 15 minutes to reset limit  Function CountDown: Script to make a countdown timer  Arguments: t: seconds to count down CountDown t  Function CleanseTweets: Clean the text of the tweets to be ready for the sentiment analysis  Arguments: df: The dataframe containing the raw tweets from the ExtractTweets function CleanseTweets df  Removing hashtags and retweets ""  """").replace ""RT""  """") for tweet in df.Text]  Removing most URLs and user taggings  Function AnalyzeSentiment: Calculates the sentiment score for each tweet in the dataframe  Arguments: df AnalyzeSentiment df  Creating a local instance of the tweets  Initializing Vader class for analyzing sentiments  Creating containers for the results sys.stdout.write ""\rAnalyzed {0} scores out of {1}"".format counter                                                             len local_tweets))) sys.stdout.flush )  Function WeightedSentiment: Calculates the weighted average sentiment by retweet count  Arguments: df WeightedSentiment df  Creating a local instance of the tweets  Function ResampleDataframe: Calculates the minutely sentiment  Arguments: df ResampleDataframe df  Creating a local instance of the tweets  Function FetchCryptoprices: Gets the cryptocurrency  BTC) prices  Arguments: num: number of minutes to be fetched FetchCryptoprices df  sample  Function FinalizeData: Unifies minutely sentiment and prices  calculates returns and volatility  Arguments: sentiment  prices FinalizeData sentiment  prices  Function CreateCorrelplots: Creates plots for cross-correllation  Arguments: df CreateCorrelplots df  name  Function create_scagnostics: Creates scagnostics plots  Arguments: df create_scagnostics df  name  Function CreateDerivative: Create derivative of each column of a dataframe  Arguments: df CreateDerivative df ########################################################################### ############################## MAIN PROGRAM ############################### ###########################################################################  Clearing the terminal  STEP 1: Getting Tweets STEP 2: Cleansing Tweets STEP 3: Creating Sentiment STEP 4: Calculating minutely sentiment STEP 5: Fetching Crypto Prices STEP 6: Unify dataframes: tweets  sentiment  prices. And create features: return  vola  lagged return STEP 7: Creating Derivative Measures STEP 8: Scagnostics STEP 8: Defining an autocorrellaton measure",458
PDRPC_crypto_data,"Empirical results for crypto data. Clustering based on risk measuring variables, PCA or Factor Analysis. Select best crypto from each cluster and calculate Portfolio Allocation","cryptocurrency, clustering, PCA, Factor Analysis, Portfolio Allocation",Judith Bender,PDRPC_crypto_data,2,!/usr/bin/env python  coding: utf-8  #Install and import packages  In[ ]:  In[ ]:  In[ ]:  In[ ]:  In[ ]:  navigate to atalaia directory  get modifications made on the repo  install packages requirements !pip install -r requirements.txt  install package  In[ ]:  In[ ]:  Import packages numpy pandas scipy scipy.stats norm sklearn sklearn.linear_model LinearRegression sklearn.cluster KMeans sklearn.preprocessing StandardScaler sklearn metrics sklearn.metrics.cluster adjusted_rand_score sklearn.decomposition PCA matplotlib mpl_toolkits.mplot3d Axes3D nolds hurst_rs random seaborn levy factor_analyzer pypfopt pypfopt EfficientFrontier  # Preparation,75
lstmWeatherForecast,Training a simple LSTM model for weather forecasting task and comparing with a MLP. The data is downloaded automatically. To run just install the requirements with pip inside your environment.,"Dense, MLP, LSTM, deep learning, weather, forecasting, neural network",Bruno Spilak,lstmWeatherForecast,1," -*- coding: utf-8 -*-  Timeseries forecasting for weather prediction # Setup pandas matplotlib tensorflow tensorflow keras numpy zipfile ZipFile os batch_generator_2d ts_dataset show_raw_visualization data  save_path=None  show=False show_heatmap data  save_path=None  show=False normalize data  train_split visualize_loss history  title  save_path=None  show=False show_prediction plot_data  delta  title  save_path=None  show=False # Climate Data Time-Series  mbar  Download # Raw Data Visualization  mbar # Data Preprocessing  Training dataset # Validation dataset # Training"""""" # Prediction  Build MLP model for comparison"""""" # Training  We stopped at epoch 7 for LSTM. For illustration no need to compare further",88
SDA_2019_St_Gallen_SMART_Sentiment_Analysis_Forecasting,This Quantlet calculates a test sample to validate the predictions made with data from the web scraping Quantlet.,"Forecasting, Valuation, web scraping, automatic sentiment analysis, stock-market news",Oliver Kostorz,Forecasting,1,Import packages bs4 BeautifulSoup requests yfinance stop_words get_stop_words re datetime fuzzywuzzy process datetime pandas numpy math pickle os json ##Functions Clean html tags in string remove_html_tags text Get stock symbol getCompany text Rounds time downwards to earlier five minute interval roundtime time Loads relevant data from web scraping process ##Forecasting List containing relevant URLs for testing forecasts News must be at least 5 days old!!!  Unpickling Add additional links to news articles to use for training the algorithm Refer to Meta-Information for full guide and requirements Pickling Defines dataframe to collect relevant info Same steps as during web scraping to process news  variance until news  variance after news Predict return and volatility by words in news article Save result in table,121
DEDA_class2019_SYSU_Abstract_LDA_LDAAnalysis,Summarize 5 topics from 130 collected abstracts with LDA method,"LDA, topic modelling, abstract analysis","FB, FS, WKH 20200924",DEDA_class2019_SYSU_Abstract_LDA_LDAAnalysis,1," -*- coding: utf-8 -*- requests  take the website source code back to you urllib  some useful functions to deal with website URLs bs4 BeautifulSoup  a package to parse website source code numpy  all the numerical calculation related methods re  regular expression package itertools  a package to do iteration works pickle  a package to save your file temporarily pandas  process structured data os  This link can represent the domain of a series of websites  get source code  parse source code  if paper[3][-3:] == 'txt':  if paper[3][-3:] == 'pdf':      abstract_parsed = soup paper_abstract_page.content)      main_part = abstract_parsed.find_all 'body')[0].text.strip )  with open abs_folder + f""{re.sub '[^a-zA-Z0-9 ]'  ''  paper[0])}.txt""  'w'  encoding='utf-8') as abs_f:      abs_f.write main_part) random os re nltk os path nltk.stem WordNetLemmatizer nltk.stem.porter PorterStemmer nltk.corpus stopwords nltk.corpus stopwords nltk.stem.wordnet WordNetLemmatizer string gensim gensim corpora matplotlib pandas numpy declare the number of topics want to be generate and notated by K number of topics  Regex cleaning  keep only letters  numbers and whitespace  apply regex  apply regex  lower case  nltk clean doc  Importing Gensim  Creating the term dictionary of our courpus  where every unique term is assigned an index.  Converting list of documents  corpus) into Document Term Matrix using dictionary prepared above.  Creating the object for LDA model using gensim library  Running and Trainign LDA model on the document term matrix. print ldamodel.print_topics num_topics=3  num_words=5))  20 need to modify to match the length of vocabulary  plt.imshow zz  cmap='hot'  interpolation='nearest') plt.show )  plt.title ""heatmap xmas song"")",240
DEDA_Class_SS2018_MainWebScraping,Scarping from IRTG webpage to obtain researchers information for further analysis,"scraping, similarity, clustering",Natalie Habib,WebScraping,2,!/usr/bin/env python3  -*- coding: utf-8 -*-  import custom webscraping class 'scrape' from WebScraping.scrape WebScraping.scrape scrape  url: target website  scrape guest reserachers from wiwi.hu-berlin.de   save scraped data to csv-file  MainWebScraping.py,29
DEDA_Class_SS2018_MainWebScraping,Scarping from IRTG webpage to obtain researchers information for further analysis,"scraping, similarity, clustering",Natalie Habib,WebScraping,2,!/usr/bin/env python3  -*- coding: utf-8 -*- requests  libraries to download websites bs4 BeautifulSoup  handle html-code pandas  standard library for machine learning pandas DataFrame  libraries to access operating system information as HOME-path and systemdate os os.path expanduser datetime date __init__ self  current_url  former_url  target-websites which are scraped  agent for scraping  dataframes in which the retrieved data is stored   timestamp: current date for history of csv-files setHomePath self build filepath  request HOME path  build to new directories  download a given url input .. output    download self  url  num_retries=2  proxies=None  resp = requests.get url  headers=headers  proxies=proxies  timeout=5)     my_header: user agent with name and web-browser     retries: number of retries after timeout  define df_researcher input None column names: [id  position  origin  interests  contact] output empty df  getDataFrame self parse a html-text to a df  input html-text of a downloaded website assumption structure of information as described  output df of the website the columns list the variables  1) till  5) without  0) and each row represents  one researcher  HTML2df self  html_text  url parse one unstructured HTML-table  input HTML-table the table data picture shall be ignored in the scraping process expecting 4 table rows with the variables id  position  origin  and interests in four cases 5 rows with the fifth variable contact   0)identifying the image file over the img-tag -> ignore it  1)identifying the id over the h5-tag -> save it   2)identifying the position over the em-tag -> save it   3)now unique identifier of the origin -> save it  4)identifying interests over the keyword 'Interests' in a td-tag -> save it   5)identifying contact over the keyword 'Contact' in a td-tag -> ignore it  output: a row for the specified df  ParseHTMLTable self  data  index  Find table content and save it in a list   Firstly  define variables  id contains last name  fore name  studying period and mail-address  find all td-tags per table-row and save its content in the defined variables  1)identifying the id over the h5-tag -> save it   some scholars have multiple h5-tags with multiple information  exception: surename  forname and period is saved in a h2-tag  case1: the table row includes only one h5-tag -> surename  forname  and timeperiod is saved in one tag  case2: the table row includes two or more h5-tag  max are 3 -> surename  forname and two timeperiods are saved in multiple tags   2)identifying the position over the em-tag -> save it   3)no unique identifier of the origin -> save it  4)identifying interests over the keyword 'Interests' in a td-tag -> save it   initialize temporary dataframe with the column header 'id' 'position' 'origin' 'interests'  returns the content of a html-tag  input: td - the table data  from which the content shall be retrieved         ident_tag - the tag  which identify the content  note: the tags on the website have no unique identifier  so that web content is identified by their html-tag  output: the content  which is enclosed by the ident_tag           if the tag is empty  the function will return 'NA' getContent self  td  ident_tag  empty string counts as False  all other strings count as True   write a given dataframe to a CSV-file at the path HOME/IRTG/data/[name-of-the-file].csv   writeCSV self  df  name  set path to filename          export scraped data to csv-file to filename_irtg  merge two dataframes df1 and df2         concatCurrentFormerDf self scrape.py,536
DEDA_Class_2019SS_Crypto_News_Sentiment_Plots_Analysis,Plotting modules to graph rolling-mean time series data taken from derived sentiment scores. These are plotted against BTC prices in USD for visualisation of the relationship. Basic correlation and Granger causation are calculated as well.,"Plotting, Time Series, Cryptocurrency",Alex Truesdale,DEDA_Class_2019SS_Crypto_News_Sentiment_Plots_Analysis,7,DEDA_Class_2019SS_Crypto_News_Sentiment_Plots_Analysis AnalysisBase numpy pandas matplotlib plotter data_in_fn  data_in_month  file_key ## Run plotting functions ###,14
DEDA_Class_2019SS_Crypto_News_Sentiment_Plots_Analysis,Plotting modules to graph rolling-mean time series data taken from derived sentiment scores. These are plotted against BTC prices in USD for visualisation of the relationship. Basic correlation and Granger causation are calculated as well.,"Plotting, Time Series, Cryptocurrency",Alex Truesdale,DEDA_Class_2019SS_Crypto_News_Sentiment_Plots_Analysis,7,DEDA_Class_2019SS_Crypto_News_Sentiment_Plots_Analysis AnalysisBase numpy pandas matplotlib plotter data_in  file_key 4984AF') ## Run plotting functions ###,14
DEDA_Class_2019SS_Crypto_News_Sentiment_Plots_Analysis,Plotting modules to graph rolling-mean time series data taken from derived sentiment scores. These are plotted against BTC prices in USD for visualisation of the relationship. Basic correlation and Granger causation are calculated as well.,"Plotting, Time Series, Cryptocurrency",Alex Truesdale,DEDA_Class_2019SS_Crypto_News_Sentiment_Plots_Analysis,7,DEDA_Class_2019SS_Crypto_News_Sentiment_Plots_Analysis AnalysisBase numpy pandas matplotlib plotter data_in_vader  data_in_blob  file_key ## Run plotting functions ###,14
DEDA_Class_2019SS_Crypto_News_Sentiment_Plots_Analysis,Plotting modules to graph rolling-mean time series data taken from derived sentiment scores. These are plotted against BTC prices in USD for visualisation of the relationship. Basic correlation and Granger causation are calculated as well.,"Plotting, Time Series, Cryptocurrency",Alex Truesdale,DEDA_Class_2019SS_Crypto_News_Sentiment_Plots_Analysis,7,DEDA_Class_2019SS_Crypto_News_Sentiment_Plots_Analysis AnalysisBase statsmodels.tsa.stattools grangercausalitytests numpy pandas matplotlib plotter data_in_fn  data_in_month  file_key ## Run plotting functions ### ## Correlation of time series ### ## Correlation of time series ###,28
DEDA_Class_2019SS_Crypto_News_Sentiment_Plots_Analysis,Plotting modules to graph rolling-mean time series data taken from derived sentiment scores. These are plotted against BTC prices in USD for visualisation of the relationship. Basic correlation and Granger causation are calculated as well.,"Plotting, Time Series, Cryptocurrency",Alex Truesdale,DEDA_Class_2019SS_Crypto_News_Sentiment_Plots_Analysis,7,pandas numpy datetime sklearn.preprocessing MinMaxScaler __init__ self reader self date_range_determiner self date_range_slicer self vader_scaler self textblob_scaler self,17
DEDA_Class_2019SS_Crypto_News_Sentiment_Plots_Analysis,Plotting modules to graph rolling-mean time series data taken from derived sentiment scores. These are plotted against BTC prices in USD for visualisation of the relationship. Basic correlation and Granger causation are calculated as well.,"Plotting, Time Series, Cryptocurrency",Alex Truesdale,DEDA_Class_2019SS_Crypto_News_Sentiment_Plots_Analysis,7,DEDA_Class_2019SS_Crypto_News_Sentiment_Plots_Analysis AnalysisBase statsmodels.tsa.stattools grangercausalitytests numpy pandas matplotlib plotter data_in_fn  data_in_month  file_key ## Run plotting functions ### ## Correlation of time series ### ## Correlation of time series ###,28
DEDA_Class_2019SS_Crypto_News_Sentiment_Plots_Analysis,Plotting modules to graph rolling-mean time series data taken from derived sentiment scores. These are plotted against BTC prices in USD for visualisation of the relationship. Basic correlation and Granger causation are calculated as well.,"Plotting, Time Series, Cryptocurrency",Alex Truesdale,DEDA_Class_2019SS_Crypto_News_Sentiment_Plots_Analysis,7,DEDA_Class_2019SS_Crypto_News_Sentiment_Plots_Analysis AnalysisBase numpy pandas matplotlib,5
SoNIC_simulation_study,"stability simulations for SoNIC model with N=100; K=2, 5; T=100,200,500,1000,2000",ERROR,Yegor Klochkov,SoNIC_stability_simulation,6,numpy scipy.stats norm get_missing_probabilities mat  conf=0.95  tol=0.1 missing_var X  delta  make_positive=True missing_covar X  Y  delta_x  delta_y,16
SoNIC_simulation_study,"stability simulations for SoNIC model with N=100; K=2, 5; T=100,200,500,1000,2000",ERROR,Yegor Klochkov,SoNIC_stability_simulation,6,"numpy sys pandas DataFrame matplotlib math ceil common common do n  T  c_num  s  pmin  set up theta_star  define true theta  generate the time series  include missing observations  estimate missing probabilities  number of windows and their length  candidates cluster numbers  ans_theta = []  ans_loss = []  ans_theta.append [np.linalg.norm ress[0].theta - ress[j].theta) for j in range 1  sim_num)])  ans_loss.append ress[0].loss) get_yticks_my_way ymin  ymax draw n  T  c_num  s  pmin  set integral ticks  save png and low quality jpg  clean before doing next graph format_func value  tick_number  find number of multiples of pi/2 draw_together n  Ts  c_num  s  pmin  save png and low quality jpg  clean before doing next graph  python simustability.py ""n"" ""c_num"" ""s"" ""pmin"" optional  =1 by default)  T = int sys.argv[4]) Ts = [100  400  500] for T in Ts:     do n  T  c_num  s  pmin)",137
SoNIC_simulation_study,"stability simulations for SoNIC model with N=100; K=2, 5; T=100,200,500,1000,2000",ERROR,Yegor Klochkov,SoNIC_stability_simulation,6,"numpy matplotlib cvxopt matrix cvxopt.solvers qp seaborn scipy.linalg sqrtm math sklearn linear_model import read_data common common get_index_matrix cluster_num  ind  normalize=True  print ""WARNING!!!"") index_dist cluster_num  int1  int2 gauss_var1_process theta  sigma  T  variance of innovations is sigma * I lasso_from_covariance D  c  alpha  minimizes 1/2 x^{T}Dx - c^{T} x + \alpha \| v \|_1 random_basis n  k  orthogonal normalization of columns u_random n  k  cluster_num  index=None v_step D0  D1  alpha_v  z  make sure u^{\T}u = I! z_step cluster_num  D1  v  ind_old=None check this stupid function!!! <- apparently works... __init__ self  theta  u  v  index  loss  to implement: index choice; initial index + initial u  matrix competition  index competition?) alternating cluster_num  D0  D1  alpha_v  epochs=10  index_init=None print ""loss : {}"".format loss)) v_step_from_data x_train  y_train  alpha_v  z  make sure u^{\T}u = I! direct cluster_num  D0  D1  alpha_v  index_init=None loss print ""loss : {}"".format loss)) direct_from_data cluster_num  x_train  y_train  alpha_v  index_init=None  loss  print ""loss : {}"".format loss)) alternating_from_data cluster_num  x_train  y_train  alpha_v  epochs=100  index_init=None _func1 v  ind _func0 ind print ""K={}: epochs {}/{}"".format cluster_num  e  epochs)) matrix_competition type  repeat_num   *args  **kwargs simu n  c_num  s  T  pmin=1.0  define true index define true theta generate the time series include missing observations ax.set_xticklabels ax.get_xticklabels )  rotation=-90  fontsize=8) ax.set_yticklabels ax.get_yticklabels )  rotation=0  fontsize=8)  ax.set_xticklabels ax.get_xticklabels )  rotation=-90  fontsize=8)  ax.set_yticklabels ax.get_yticklabels )  rotation=0  fontsize=8) simu )",217
SoNIC_simulation_study,"stability simulations for SoNIC model with N=100; K=2, 5; T=100,200,500,1000,2000",ERROR,Yegor Klochkov,SoNIC_stability_simulation,6,numpy  func takes ind! find_new_home func  k  ind  i greedy_update func  k  ind __init__ self  index  value  message  nochange kmeans_greedy func  k  n  iter_limit=10000  init_index=None,25
SoNIC_simulation_study,"stability simulations for SoNIC model with N=100; K=2, 5; T=100,200,500,1000,2000",ERROR,Yegor Klochkov,SoNIC_stability_simulation,6,numpy pandas read_csv csv math sqrt find_homo_Be x  scale=4  res=20 check_interval l  r read_SIFI name_to_sifi read_stock_twits_user_sentiment name_to_twits  min_delta=0.1  min_days=200 avoid_empty = lambda x  err_t: x[0] if np.size x) > 0 else err_t read_crix_returns name_to_crix_price read_crypto_sentiment name_to_crypto_sentiment  top_num=50,37
SoNIC_simulation_study,"stability simulations for SoNIC model with N=100; K=2, 5; T=100,200,500,1000,2000",ERROR,Yegor Klochkov,SoNIC_stability_simulation,6,subprocess     N c_num  pmin  s = 1 always,8
Regulatory_risk_coindesk,Analysis the regulatory risk of cryptocurrency market using the scrapped news data from Coindesk.com,"LDA, topic modelling, regulatory risk, regulation, Cryptocurrency",Xinwen Ni,Regulatory_risk_coindesk,1,"!/usr/bin/env python3  -*- coding: utf-8 -*- pandas datetime datetime  from datetime import timedelta matplotlib  import numpy as np  import datetime  import calendar  import re datelist beginDate  endDate df1.columns = ['titles'  'authors'  'contents'  'releases'  'updates'                  'imgs'  'tags'  'bars']  find out the index of regulation news in the news data      tmp_time = tmp_stamp[11:19]     tmp_time = tmp_stamp[11:19]  df_news=pd.read_csv ""news_counts.csv"")  df_news=df_news.drop columns=['Unnamed: 0'])  df_regulation=pd.read_csv ""regulation_counts.csv"")  df_regulation=df_regulation.drop columns=['Unnamed: 0'])  first_date=datetime.strptime df_news.iloc[0].get 'Date') '%Y-%m-%d').date )  first_date=df_news.iloc[0].get 'Date')  last_date=df_news.iloc[-1].get 'Date')  Datelist=datelist first_date last_date)  df_temp=df_agg.drop columns=['Ratio_num' 'Ratio_len' 'Ratio_line' 'Ratio_words'])  df_temp.to_csv 'daily_count_news_regulation.csv')  convert daily to weekly  Getting week number  Getting year. Weeknum is common across years to we need to create unique index by using year and weeknum  generate the monthly data df_weekly = df_daily.groupby ['Year' 'Week_Number']).agg {'News_len': 'mean' 'News_line': 'mean' 'News_num': 'mean' 'News_words': 'mean' 'Reg_len': 'mean' 'Reg_line': 'mean' 'Reg_num': 'mean' 'Reg_words': 'mean' 'Ratio_len': 'mean' 'Ratio_line': 'mean' 'Ratio_num': 'mean' 'Ratio_words': 'mean'}) datetime  generate the weekly data   Grouping based on required values df_weekly = pd.concat [df_weekly  pd.DataFrame columns = ['News_len'])]) plot  fig = plt.figure ) ax1 = fig.add_subplot 111) ax1.scatter df_weekly.Date df_weekly.News_num c = 'r' marker = 'o') ax1.scatter df_weekly.Date df_weekly.Reg_num c = 'b' marker = 'o') plt.savefig 'Num_of_news_and_reg_weekly.png' dpi = 720 transparent=True) plt.show ) fig = plt.figure ) ax1 = fig.add_subplot 111) ax1.scatter df_monthly.Date df_monthly.News_num c = 'r' marker = 'o') ax1.scatter df_monthly.Date df_monthly.Reg_num c = 'b' marker = 'o') plt.savefig 'Num_of_news_and_reg_monthly.png' dpi = 720 transparent=True) plt.show ) fig = plt.figure ) ax1 = fig.add_subplot 111) ax1.scatter df_weekly.Date df_weekly.Ratio_num c = 'r' marker = 'o') ax1.plot df_weekly.Date df_weekly.Ratio_num 'b' linewidth=1  markersize=1) plt.savefig 'Weekly_num_ratio.png' dpi = 720 transparent=True) plt.show ) fig = plt.figure ) ax1 = fig.add_subplot 111) ax1.scatter df_weekly.Date df_weekly.Ratio_words c = 'r' marker = 'o') ax1.plot df_weekly.Date df_weekly.Ratio_words 'b' linewidth=1  markersize=1) plt.savefig 'Weekly_words_ratio.png' dpi = 720 transparent=True) plt.show ) fig = plt.figure ) ax1 = fig.add_subplot 111) ax1.scatter df_monthly.Date df_monthly.Ratio_num c = 'r' marker = 'o') ax1.plot df_monthly.Date df_monthly.Ratio_num 'b' linewidth=1  markersize=1) plt.savefig 'Monthly_num_ratio.png' dpi = 720 transparent=True) plt.show ) fig = plt.figure ) ax1 = fig.add_subplot 111) ax1.scatter df_monthly.Date df_monthly.Ratio_words c = 'r' marker = 'o') ax1.plot df_monthly.Date df_monthly.Ratio_words 'b' linewidth=1  markersize=1) plt.savefig 'Monthly_words_ratio.png' dpi = 720 transparent=True) plt.show ) nltk re numpy pandas pprint pprint  Gensim gensim gensim gensim.utils simple_preprocess gensim.models CoherenceModel gensim.matutils kullback_leibler  spacy for lemmatization spacy  Plotting tools pyLDAvis pyLDAvis  don't skip this matplotlib matplotlib inline  Enable logging for gensim - optional logging warnings nltk.corpus stopwords  Convert to list data = df_news.Content.values.tolist ) sent_to_words sentences  deacc=True removes punctuations  Define functions for stopwords  bigrams  trigrams and lemmatization remove_stopwords texts make_bigrams texts make_trigrams texts lemmatization texts  allowed_postags=['NOUN'  'ADJ'  'VERB'  'ADV'] clean_data data  Remove Emails  Remove new line characters  Remove distracting single quotes get_lemm data     print data_words[:1])  Build the bigram and trigram models  higher threshold fewer phrases.  Faster way to get a sentence clubbed as a trigram/bigram  See trigram example     print trigram_mod[bigram_mod[data_words[0]]])  Remove Stop Words  Form Bigrams  Initialize spacy 'en' model  keeping only tagger component  for efficiency)  python3 -m spacy download en  Do lemmatization keeping only noun  adj  vb  adv data_lemmatized = lemmatization data_words_bigrams  allowed_postags=['NOUN'  'ADJ'  'VERB'  'ADV'])     print data_lemmatized[:1])  change the data   Create Dictionary  Create Corpus  Term Document Frequency For example   0  1) above implies  word id 0 occurs once in the first document. Likewise  word id 1 occurs twice and so on. id2word[0] # Human readable format of corpus  term-frequency) [[ id2word[id]  freq) for id  freq in cp] for cp in corpus[:1]]  Print the Keyword in the 10 topics  Compute Perplexity  a measure of how good the model is. lower the better.  Compute Coherence Score  Hellinger distance  DTM  BasicCleanText raw_text  keep only letters  numbers and whitespace  apply regex  lower case   Tokenization  create English stop words list en_stop = get_stop_words 'en')  remove stop words from tokens stopped_tokens = [i for i in tokens if not i in en_stop]  Create p_stemmer of class PorterStemmer  stem token df_reg = pd.read_csv 'news_counts.csv' encoding=""ISO-8859-1"")  find out the time slice  drop the words only appers once collections OrderedDict  generate the dictionary  store the dictionary  for future reference Save vocabulary Prevent storing the words of each document in the RAM __iter__ self  assume there's one document per line  tokens separated by whitespace  load one vector into memory at a time DTM_topic_1=ldaseq.print_topic_times topic=1  top_terms=10) DTM_topic_2=ldaseq.print_topic_times topic=2  top_terms=10) DTM_topic_3=ldaseq.print_topic_times topic=3  top_terms=10) DTM_topic_4=ldaseq.print_topic_times topic=4  top_terms=10) topic_time DTM_topic time_stamps topic2_words_time=topic_time DTM_topic_1 time_stamps) topic3_words_time=topic_time DTM_topic_2 time_stamps) topic4_words_time=topic_time DTM_topic_3 time_stamps) topic5_words_time=topic_time DTM_topic_4 time_stamps) plot the dynamic movement of topic 1 plt.xlim  -1  2)) plt.ylim  0  0.02))",728
COVID_Risk_Narratives,"The supporting materials for paper ""COVID Risk Narratives - A Computational Linguistic Approach to the Econometric Identification of Narrative Risk During a Pandemic"". Techniques included - sentiment analysis (bag-of-words method), topic modelling (LDA), word embedding (word2vec), SIR epidemiological model, vector autoregression. Main use of the codes - 1) Identification of important/perennial economic narratives. 2) Monitor narratives of interest for financial forecasting.","narrative economics, COVID-19, natural language processing, tone analysis, early warning indicator","Yuting Chen, University College Dublin",COVID_Risk_Narratives_for_DFIN,8,"!/usr/bin/env python  coding: utf-8  In[1]: gensim.models Word2Vec gensim pandas numpy datetime datetime  In[2]: os select_path           print ""Using the Mac data path"")          print ""Using the AWS data path"")  In[3]:  load the trained embedding models  In[4]:  In[5]: matplotlib  list_sim_covid_crisis = []\nfor i in range len l_dates)):\n    date = l_dates[i]\n    model = l_models[i]\n    list_date.append date)\n    list_sim1.append model.wv.similarity \'coronavirus\'  \'recession\'))\n    list_sim2.append model.wv.similarity \'coronavirus\'  \'crisis\'))\n    list_sim3.append model.wv.similarity \'coronavirus\'  \'rollout\'))\n    list_sim4.append model.wv.similarity \'coronavirus\'  \'worsening\'))\n\n#     print ""Cosine similarity between \'coronavirus\' "" + \n#                ""and \'crisis\' - CBOW : ""  \n#     model.wv.similarity \'coronavirus\'  \'crisis\')) \n\n# print ""most similar words with coronavirus : ""  \n#       model1.wv.most_similar \'coronavirus\')[:10])\ndf_sim[\'date\'] = list_date\ndf_sim[\'recession\'] = list_sim1\ndf_sim[\'crisis\'] = list_sim2\ndf_sim[\'rollout\'] = list_sim3\ndf_sim[\'worsening\'] = list_sim4\n\ndf_sim = df_sim.set_index \'date\')\n# df_sim.plot figsize= 12 6))  \n\nfig  ax = plt.subplots figsize= 12  6))\n\ncrashstart = datetime 2020  2  21) \ncrashend = datetime 2020  3  23)\nax.axvspan crashstart  crashend  alpha=0.2  color=\'grey\')\n\n# Add x-axis and y-axis\nax.plot df_sim.index.values df_sim[df_sim.columns[0]] label=df_sim.columns[0] linewidth=1)\nax.plot df_sim.index.values df_sim[df_sim.columns[1]] label=df_sim.columns[1] linewidth=1)\nax.plot df_sim.index.values df_sim[df_sim.columns[2]] label=df_sim.columns[2] linewidth=1)\n# ax.plot df_sim.index.values df_sim[df_sim.columns[3]] label=df_sim.columns[3] linewidth=1)\nplt.legend )\n\n# Set title and labels for axes\nax.set xlabel=""Date"" \n       ylabel=\'Cosine Similarity with ""coronavirus""\')\n\nplt.show )\nfig.savefig \'smantic_similarity.png\' bbox_inches=None)\n\nprint df_sim.corr ))')  In[ ]: sklearn.manifold TSNE matplotlib display_closestwords_tsnescatterplot model  word  size      print close_words)      fig.savefig 'tsne_tweets.png' bbox_inches=None)  In[ ]:  plot wordcloud of the most similar words to coronavirus wordcloud WordCloud  In[ ]: PIL Image show_wordcloud data  title = None          max_font_size=40    chosen at random by flipping a coin; it was heads  to recolour the image      plt.imshow wordcloud.recolor color_func=image_colors))  In[ ]:  In[ ]:",234
COVID_Risk_Narratives,"The supporting materials for paper ""COVID Risk Narratives - A Computational Linguistic Approach to the Econometric Identification of Narrative Risk During a Pandemic"". Techniques included - sentiment analysis (bag-of-words method), topic modelling (LDA), word embedding (word2vec), SIR epidemiological model, vector autoregression. Main use of the codes - 1) Identification of important/perennial economic narratives. 2) Monitor narratives of interest for financial forecasting.","narrative economics, COVID-19, natural language processing, tone analysis, early warning indicator","Yuting Chen, University College Dublin",COVID_Risk_Narratives_for_DFIN,8,csv glob re string sys time  Modify to identify path for custom modules Load_MasterDictionary  User defined directory for files to be parsed  User defined file pointer to LM dictionary  User defined output file  Setup output  of alphabetic '  '# of digits '   of numbers '  'avg # of syllables per word '  'average word length '  'vocabulary'] main   drop all May month references  for this parse caps aren't informative so shift get_data doc  Note that \w+ splits hyphenated words  word count  drop punctuation within numbers for number count  Convert counts to %  Vocabulary,93
COVID_Risk_Narratives,"The supporting materials for paper ""COVID Risk Narratives - A Computational Linguistic Approach to the Econometric Identification of Narrative Risk During a Pandemic"". Techniques included - sentiment analysis (bag-of-words method), topic modelling (LDA), word embedding (word2vec), SIR epidemiological model, vector autoregression. Main use of the codes - 1) Identification of important/perennial economic narratives. 2) Monitor narratives of interest for financial forecasting.","narrative economics, COVID-19, natural language processing, tone analysis, early warning indicator","Yuting Chen, University College Dublin",COVID_Risk_Narratives_for_DFIN,8,!/usr/bin/python3  BDM : 201510 time load_masterdictionary file_path  print_flag=False  f_log=None  get_other=False  Load slightly modified nltk stopwords.  I do not use nltk import to avoid versioning errors.  Dropped from nltk: A  I  S  T  DON  WILL  AGAINST  Added: AMONG   clear line create_sentimentdictionaries _master_dictionary  _sentiment_categories  Create dictionary of sentiment dictionaries with count set = 0 __init__ self  cols  _stopwords  Full test program in /TextualAnalysis/TestPrograms/Test_Load_MasterDictionary.py,61
COVID_Risk_Narratives,"The supporting materials for paper ""COVID Risk Narratives - A Computational Linguistic Approach to the Econometric Identification of Narrative Risk During a Pandemic"". Techniques included - sentiment analysis (bag-of-words method), topic modelling (LDA), word embedding (word2vec), SIR epidemiological model, vector autoregression. Main use of the codes - 1) Identification of important/perennial economic narratives. 2) Monitor narratives of interest for financial forecasting.","narrative economics, COVID-19, natural language processing, tone analysis, early warning indicator","Yuting Chen, University College Dublin",COVID_Risk_Narratives_for_DFIN,8,"!/usr/bin/env python  coding: utf-8  In[1]: numpy pandas matplotlib yfinance  In[2]:  import the topic distribution for the model with k=20  tweet  news  history  aggregate the document-level data to daily data  can do it when necessary  set date as index first  rename the topic of interest  In[3]:  import price and vix  for illustration and reproducibility yfinance  ticker = '^VIX' # ^GSPC ^DJUSS ^DJI ^NDX ""FTSEMIB.MI"" Close  In[5]:  First plot  plot the intensity index with VIX in the history  weekly  plot a second graph on the same figure  the same but with a focus on 2020  plot a third graph on the same figure  the S&P 500 index  plot a fourth graph on the same figure  the S&P 500 index with a focus on 2020  to mark the crash period datetime datetime  to mark the testing period  to mark the focus period  first graph  prepare the dataframe  t stands for temporary  added these three lines  shadow the test period  prepare dataframe for the second and third graph  t stands for temporary  added these three lines  shadow the crash period   add one event  third graph  shadow the crash period   In[ ]:",187
COVID_Risk_Narratives,"The supporting materials for paper ""COVID Risk Narratives - A Computational Linguistic Approach to the Econometric Identification of Narrative Risk During a Pandemic"". Techniques included - sentiment analysis (bag-of-words method), topic modelling (LDA), word embedding (word2vec), SIR epidemiological model, vector autoregression. Main use of the codes - 1) Identification of important/perennial economic narratives. 2) Monitor narratives of interest for financial forecasting.","narrative economics, COVID-19, natural language processing, tone analysis, early warning indicator","Yuting Chen, University College Dublin",COVID_Risk_Narratives_for_DFIN,8,"!/usr/bin/env python  coding: utf-8  In[12]: pandas numpy seaborn re nltk nltk.corpus stopwords Load_MasterDictionary load_masterdictionary  from Generic_Parser.py string datetime datetime matplotlib  In[14]:  load processed file\npath_temp = ""Temp/""\n# impoort your data\ndf = pd.read_pickle path_temp+test_type+\'_Processed.pkl\')\ndf = df[[\'date\' \'CONTENT\']]\n\nif test_type == \'tweet\':\n    start_date = datetime.strptime ""01/01/2020""  \'%d/%m/%Y\')\n#     end_date = datetime.strptime ""26/05/2020""  \'%d/%m/%Y\')\n    df = df.loc[df[\'date\'] >= start_date]\n    df = df.reset_index )\nelse:\n#     df.columns = [\'time\' \'source\' \'text\' \'country\']\n    df[\'date\'] = pd.to_datetime df[\'date\']  format=\'%Y-%m-%d\')\n\n# preview data\nprint df.shape)\n\nprint ""The first piece of text \\n"" df[""CONTENT""][0])\n\ndf.head )')  In[15]:  define the tone analyzers  easy  for L&M lm_get_data doc  from ""Generic_Parser.py""  if import processed data  comment this line      tokens = re.findall '\w+'  doc)  # Note that \w+ splits hyphenated words  word count  Convert counts to % I don't need to      for i in range 3  10 + 1):          _odata[i] =  _odata[i] / _odata[2])  Vocabulary lm_sentiment_analyzer text          text = text.upper )  for vader vader_sentiment_analyzer text  for textblob textblob_sentiment_analyzer text  In[16]: ## text or text_first\n    odata = lm_sentiment_analyzer i)\n    data.append odata)\ndf_tones = pd.DataFrame data columns = [\'total\' \'positive\' \'negative\' \'uncertainty\' \'litigious\' \'weak_modal\' \'moderate_modal\' \'strong_modal\' \'constraining\'])\ndf_tones[\'date\'] = df[""date""]\nprint df_tones.head ))')  In[17]:",178
SMSclus8pc,Employs the centroid linkage using squared Euclidean distance matrices to perform a cluster analysis on an 8 points example,"cluster-analysis, dendrogram, euclidean, distance, linkage",Awdesch Melzer,SMSclus8pс,1,pandas numpy matplotlib matplotlib.patches Ellipse scipy.cluster hierarchy,7
SoNIC_AAPL_BTC,estimation of SoNIC model on AAPL and BTC sentiment weights,ERROR,Yegor Klochkov,SoNIC_AAPL_BTC,5,numpy scipy.stats norm get_missing_probabilities mat  conf=0.95  tol=0.1 missing_var X  delta  make_positive=True missing_covar X  Y  delta_x  delta_y,16
SoNIC_AAPL_BTC,estimation of SoNIC model on AAPL and BTC sentiment weights,ERROR,Yegor Klochkov,SoNIC_AAPL_BTC,5,"numpy matplotlib cvxopt matrix cvxopt.solvers qp seaborn scipy.linalg sqrtm math sklearn linear_model import read_data common common get_index_matrix cluster_num  ind  normalize=True  print ""WARNING!!!"") index_dist cluster_num  int1  int2 gauss_var1_process theta  sigma  T  variance of innovations is sigma * I lasso_from_covariance D  c  alpha  minimizes 1/2 x^{T}Dx - c^{T} x + \alpha \| v \|_1 random_basis n  k  orthogonal normalization of columns u_random n  k  cluster_num  index=None v_step D0  D1  alpha_v  z  make sure u^{\T}u = I! z_step cluster_num  D1  v  ind_old=None check this stupid function!!! <- apparently works... __init__ self  theta  u  v  index  loss  to implement: index choice; initial index + initial u  matrix competition  index competition?) alternating cluster_num  D0  D1  alpha_v  epochs=10  index_init=None print ""loss : {}"".format loss)) v_step_from_data x_train  y_train  alpha_v  z  make sure u^{\T}u = I! direct cluster_num  D0  D1  alpha_v  index_init=None loss print ""loss : {}"".format loss)) direct_from_data cluster_num  x_train  y_train  alpha_v  index_init=None  loss  print ""loss : {}"".format loss)) alternating_from_data cluster_num  x_train  y_train  alpha_v  epochs=100  index_init=None _func1 v  ind _func0 ind print ""K={}: epochs {}/{}"".format cluster_num  e  epochs)) matrix_competition type  repeat_num   *args  **kwargs simu n  c_num  s  T  pmin=1.0  define true index define true theta generate the time series include missing observations ax.set_xticklabels ax.get_xticklabels )  rotation=-90  fontsize=8) ax.set_yticklabels ax.get_yticklabels )  rotation=0  fontsize=8)  ax.set_xticklabels ax.get_xticklabels )  rotation=-90  fontsize=8)  ax.set_yticklabels ax.get_yticklabels )  rotation=0  fontsize=8) simu )",217
SoNIC_AAPL_BTC,estimation of SoNIC model on AAPL and BTC sentiment weights,ERROR,Yegor Klochkov,SoNIC_AAPL_BTC,5,common common numpy matplotlib seaborn common.alternating matrix_competition  SET NUMBER OF CLUSTERS,11
SoNIC_AAPL_BTC,estimation of SoNIC model on AAPL and BTC sentiment weights,ERROR,Yegor Klochkov,SoNIC_AAPL_BTC,5,numpy  func takes ind! find_new_home func  k  ind  i greedy_update func  k  ind __init__ self  index  value  message  nochange kmeans_greedy func  k  n  iter_limit=10000  init_index=None,25
SoNIC_AAPL_BTC,estimation of SoNIC model on AAPL and BTC sentiment weights,ERROR,Yegor Klochkov,SoNIC_AAPL_BTC,5,numpy pandas read_csv csv math sqrt find_homo_Be x  scale=4  res=20 check_interval l  r read_SIFI name_to_sifi read_stock_twits_user_sentiment name_to_twits  min_delta=0.1  min_days=200 avoid_empty = lambda x  err_t: x[0] if np.size x) > 0 else err_t read_crix_returns name_to_crix_price read_crypto_sentiment name_to_crypto_sentiment  top_num=50,37
upper_cutoff_point,Calculate the upper-cut-off of normal distribution.,ERROR,Huong Vu 0856156,upper_cutoff_point,2,!/usr/bin/env python  coding: utf-8  In[1]: scipy.stats norm numpy matplotlib  Creating a function to calculate w - non-parametric criterion for outlier identification  Wilkinson et al.) of the normal distribution: $w=q_{75}+1.5 q_{75}-q_{25})$  norm.ppf from spicy.stats can help us return the quantile of a Normal distribution  for example:  In[9]:  In[26]: calc_w mu sigma  After calculating $w$  we can define a function to calculate the probability that data points will lie outside our outlier identification: $P x>w)$ when our data follows normal distribution.   In[40]: calc_prob_outlier w mu sigma  In[43]:  In[68]:,86
SDA_2019_St_Gallen_SMART_Sentiment_Analysis_Data_visualization,This Quantlet takes the output of the web scraping and forecasting Quantlets to visualize the results as a WordCloud and a scatterplot for the prediction vs realized return and volatility of the stock in the test sample.,"Data visualization, WordCloud, scatterplot, automatic sentiment analysis, stock-market news",Oliver Kostorz,Data visualitation,1,Import packages pandas numpy os wordcloud WordCloud PIL Image matplotlib PIL Image Set working device ##Data visualisation Dictionary Wordcloud Scatterplot prediction-actual return Scatterplot prediction-actual volatility,25
pyTSA_AustralianLabour,"This Quantlet  plots time series and additive decomposition results of monthly employed total persons from February 1978 to November 2018 (in thousands) (example 2.5, Figures 2.13-2.16 from the Book).","time series, additive decomposition, smoothing, employment, seasonality, visualisation","Huang Changquan, Alla Petukhina",pyTSA_AustralianLabour,1,pandas matplotlib statsmodels.graphics.tsaplots month_plot statsmodels.tsa.seasonal seasonal_decompose PythonTsa.plot_acf_pacf acf_pacf_fig  Graph time series plot from 2013.1 to 2017.1 Plot seasonal plots,19
CSC_Scraping,Scraping source codes given hashes,ERROR,"Elizaveta Zinovyeva, Raphael Constantin Georg Reule",CSC_Scraping,2,"!/usr/bin/env python  coding: utf-8  ###   In[1]:  Used Etherscan.io APIs""  ### Import Modules  In[2]: requests pandas numpy sys time os matplotlib  ### Load List of Contracts  In[3]:  Load list from existing dataset  In[4]:  In[5]:  In[6]:  In[7]:  In[8]:  In[9]:  In[10]:  ### Constants    In[11]:  In[12]:  In[13]:  ### Create Folders  In[14]:  In[69]:  ### Function to call the API  In[34]:  In[49]: scrape_ether_contract_and_write address_array  API_Token  time.sleep 0.01) # we can do 5 GET/POST requests per sec  save full GET request  save solidity source code  save ABI compiled version  ### Function Call  In[102]:",86
pyTSA_ReturnsPG,"This Quantlet plots monthly time series of returns of Procter and Gamble from 1961 to 2016 and  their ACF and PACF (Example, 2.4 Figures 2.8-2.9 in the book)","time series, autocorrelation, returns, ACF, PACF, plot, visualisation","Huang Changquan, Alla Petukhina",pyTSA_Stationarity,1,numpy pandas matplotlib PythonTsa.RandomWalk RandomWalk_with_drift,5
Crypto_RNN_Prediction,use RNN to do cryptocurrency prediction,"crypto, RNN, prediction","Wang Wenyi, Liu Zekai, Tang Jiayun, Zhang Lin, Chen Jing",Crypto_RNN_Prediction,2,!/usr/bin/env python  coding: utf-8  In[1]: tensorflow numpy pandas matplotlib os  In[2]: tensorflow.keras Sequential tensorflow.keras.layers Dense  In[3]:  In[4]:  In[8]:  In[9]:  In[10]:  In[11]: test set reshape the train data  In[12]: sklearn.preprocessing MinMaxScaler standardlize data {x_i - u}/sigma  In[13]:  process train set: each y include 24 characteristics   In[14]: RNN the first layer  return_sequences:default is false.whcih means return to the output of the last stride. If true return to the output of the all stride in the last layer.  hidden states: 128 not work in 25% situation（avoid overfit） ## the second layer the third layer the fourth layer ## the fifth layer multi-dimension- one dimension output one dimension 100 epoch，64 samples each epoch  In[15]: predict input is the characteristic in test set using the trained model to predict  In[16]: true price plot show the relationship between the true price and the predicted price  In[17]:  RNN Correlation coefficient   In[3]: RNN for BNB  In[4]:  In[5]:  In[6]: sklearn.preprocessing MinMaxScaler standardlize data  In[7]:  process train set: each y include 24 characteristics   In[8]:  RNN the first layer  return_sequences:default is false.whcih means return to the output of the last stride. If true return to the output of the all stride in the last layer.  hidden states: 128 not work in 25% situation（avoid overfit） ## the second layer the third layer the fourth layer ## the fifith layer multi-dimension- one dimension output one dimension 50 epochs，64 samples each epoch  In[9]: Predict input is the characteristic in test set using the trained model to predict  In[10]: plot show the relationship between the true price and the predicted price  In[12]: BNB RNN Correlation coefficient   In[1]: tensorflow numpy pandas matplotlib os tensorflow.keras Sequential tensorflow.keras.layers Dense  In[3]: RNN for ETH  In[4]:  In[6]:  In[8]: sklearn.preprocessing MinMaxScaler standardlize data  In[9]:  process train set: each y include 24 characteristics   In[10]: RNN the first layer  return_sequences:default is false.whcih means return to the output of the last stride. If true return to the output of the all stride in the last layer.  hidden states: 128 ## the second layer the third layer the fourth layer ## the fifth layer multi-dimension- one dimension output one dimension  In[11]: Predict input is the characteristic in test set using the trained model to predict  In[12]: plot show the relationship between the true price and the predicted price  In[13]: ETH RNN Correlation coefficient   In[ ]:,380
1d_GAN,Application of Generative Adversarial Networks to approximate a one dimensional standard normal distribution. Both Vanilla GAN and Wasserstein GAN are tested for different starting distributions and visualized.,"GAN, WGAN, Wasserstein GAN, Vanilla GAN, one dimensional",Ramona Merkl,1d_GAN,6,!/usr/bin/env python  coding: utf-8  In[1]: os imageio skimage.io imread_collection numpy scipy matplotlib seaborn tensorflow tensorflow tensorflow.keras.models Model tensorflow.keras.optimizers Adam tensorflow.keras.layers Dense statsmodels  In[2]: # directories  In[2]: generate_noise samples  dimensions=2 return np.random.normal 0 8   samples dimensions)) return np.random.normal 8 1   samples dimensions)) generate_data samples  dimensions=2  mapping  R R)--> [-1 1]  [-1 1]) generator latent_dim  output_dim discriminator dim  build GAN given a discriminator and a generator GAN G  D  In[3]:  VISUALIZATION plots G  D  step  D_loss  G_loss  filename  plot losses  plot real and generated samples ax[0 1].set_xlim -3 3)  plot confidence of the discriminator  Q-Q Plot  In[4]:  PARAMETERS  latent dimension  subsample for gif  set to zero if no gif wanted  batch size  plotting frequency for Ggif  samples for final plot  In[5]:  produce pictures for GIF  train discriminator  train generator filename=f'images/1dim_norm 8 1)_to_stnormal/gif/pic.{k+1:03d}.jpeg')  make GIF directory = 'images/1dim_norm 8 1)_to_stnormal/gif/*.jpeg' imageio.mimsave 'images/1dim_norm 8 1)_to_stnormal/final/animation.mp4'  images  quality=9)  In[6]:  train discriminator  train generator plots G=G  D=D  step=step  D_loss=D_loss  G_loss=G_loss  filename=f'images/1dim_norm 0 8)_to_stnormal/plot_step{k+1:03d}.png') plots G=G  D=D  step=step  D_loss=D_loss  G_loss=G_loss  filename=f'images/1dim_norm 8 1)_to_stnormal/plot_step{k+1:03d}_2.png')  In[ ]:,168
1d_GAN,Application of Generative Adversarial Networks to approximate a one dimensional standard normal distribution. Both Vanilla GAN and Wasserstein GAN are tested for different starting distributions and visualized.,"GAN, WGAN, Wasserstein GAN, Vanilla GAN, one dimensional",Ramona Merkl,1d_GAN,6,!/usr/bin/env python  coding: utf-8  In[1]: os imageio skimage.io imread_collection numpy scipy matplotlib seaborn tensorflow tensorflow tensorflow.keras.models Model tensorflow.keras.optimizers RMSprop tensorflow.keras.layers Dense tensorflow.keras backend tensorflow.keras.initializers RandomNormal tensorflow.keras.constraints Constraint statsmodels  In[2]: # directories  In[3]: generate_noise samples  dimensions=2 return np.random.normal 8 1   samples dimensions)) generate_data samples  dimensions=2  Wasserstein Loss wasserstein y_true  y_pred  mapping  R R)--> [-1 1]  [-1 1]) generator latent_dim  output_dim model.add Dense output_dim)) discriminator dim  build GAN given a discriminator and a generator WGAN G  D  In[4]:  VISUALIZATION plots G  D  step  D_loss  G_loss  filename  plot losses ax[0 0].set_ylim 0.45  1.0)  plot real and generated samples ax[0 1].set_xlim -3 3)  plot confidence of the discriminator ax[1 0].set_ylim 0.0 1.0)  Q-Q Plot  In[5]:  parameters  latent dimension  batch size size = 512  In[8]:  train discriminator  train generator plots G=G  D=D  step=step  D_loss=D_loss  G_loss=G_loss  filename=f'images/1dim_norm 8 1)_to_stnormal/plot_{k+1:03d}.png')  In[ ]:,135
1d_GAN,Application of Generative Adversarial Networks to approximate a one dimensional standard normal distribution. Both Vanilla GAN and Wasserstein GAN are tested for different starting distributions and visualized.,"GAN, WGAN, Wasserstein GAN, Vanilla GAN, one dimensional",Ramona Merkl,1d_GAN,6,!/usr/bin/env python  coding: utf-8  In[1]: os imageio skimage.io imread_collection numpy matplotlib  In[2]: # directories  In[3]:  discriminator function in vanilla GAN D x  theta  sigma2  In[45]:  fix theta=1 and plot discriminator for variances varying from 0.1 to 15.0  In[46]:  make GIF  In[47]:  fix sigma^2=1 and plot discriminator for expectations varying from 0.1 to 15.0  In[48]:  make GIF,56
Web_Crawler_UBike,"A Web Crawler for UBike website, and visualizes dataset with scatter plot","crawler, crawling, ubike, scatter, web",Susan,Web_Crawler_UBike,2,!/usr/bin/env python  coding: utf-8  In[1]: requests json pandas  In[3]: os mpl_toolkits.basemap Basemap matplotlib numpy  In[ ]:,16
Bitcoin_Transaction_Analysis,"Creates a relational database, fills it with bitcoin transaction and does an analysis on them.","bitcoin, database, analysis, holding_time, trading volume",David Schulte,DEDA_class_WS21_Bitcoin_Transaction_Analysis,11,mysql login_data LOGIN_DATA __init__ self get_query_generator self  q get_query_list self  q join_tables self  tables: list get_dates self  SELECT i.wallet_id AS ip  o.wallet_id AS op  i.value AS ipv  o.value AS opv FROM  SELECT * FROM outputs_connected ORDER BY value DESC LIMIT 5) AS o INNER JOIN transactions AS t ON o.tx_from_id=t.tx_id INNER JOIN inputs_connected AS i ON t.tx_id=i.tx_to_id;,57
Bitcoin_Transaction_Analysis,"Creates a relational database, fills it with bitcoin transaction and does an analysis on them.","bitcoin, database, analysis, holding_time, trading volume",David Schulte,DEDA_class_WS21_Bitcoin_Transaction_Analysis,11,databaseReader DatabaseReader analyzer Analyzer matplotlib numpy,6
Bitcoin_Transaction_Analysis,"Creates a relational database, fills it with bitcoin transaction and does an analysis on them.","bitcoin, database, analysis, holding_time, trading volume",David Schulte,DEDA_class_WS21_Bitcoin_Transaction_Analysis,11,databaseReader DatabaseReader analyzer Analyzer matplotlib numpy,6
Bitcoin_Transaction_Analysis,"Creates a relational database, fills it with bitcoin transaction and does an analysis on them.","bitcoin, database, analysis, holding_time, trading volume",David Schulte,DEDA_class_WS21_Bitcoin_Transaction_Analysis,11,mysql transaction_objects Transaction login_data LOGIN_DATA  TABLES = ['blocks'  'transactions'  'outputs'  'inputs'] __init__ self create_tables self create_views self reset_database self drop_tables self join_tables self  tables: list add_block self  block add_transaction self  parser_tx  block_hash  SELECT i.wallet_id AS ip  o.wallet_id AS op  i.value AS ipv  o.value AS opv FROM  SELECT * FROM outputs_connected ORDER BY value DESC LIMIT 5) AS o INNER JOIN transactions AS t ON o.tx_from_id=t.tx_id INNER JOIN inputs_connected AS i ON t.tx_id=i.tx_to_id;,72
Bitcoin_Transaction_Analysis,"Creates a relational database, fills it with bitcoin transaction and does an analysis on them.","bitcoin, database, analysis, holding_time, trading volume",David Schulte,DEDA_class_WS21_Bitcoin_Transaction_Analysis,11,databaseReader DatabaseReader analyzer Analyzer matplotlib,5
Bitcoin_Transaction_Analysis,"Creates a relational database, fills it with bitcoin transaction and does an analysis on them.","bitcoin, database, analysis, holding_time, trading volume",David Schulte,DEDA_class_WS21_Bitcoin_Transaction_Analysis,11,databaseWriter DatabaseWriter,2
Bitcoin_Transaction_Analysis,"Creates a relational database, fills it with bitcoin transaction and does an analysis on them.","bitcoin, database, analysis, holding_time, trading volume",David Schulte,DEDA_class_WS21_Bitcoin_Transaction_Analysis,11,databaseReader DatabaseReader __init__ self  db: DatabaseReader get_transactions_by_day self get_volume_by_day self get_values self get_volume_per_transaction self get_tx_volume_day self get_highest_output_per_day self get_value_distribution_per_day self  month_filter=None estimate_holding_distribution self  sample_size  random_seed=42,25
Bitcoin_Transaction_Analysis,"Creates a relational database, fills it with bitcoin transaction and does an analysis on them.","bitcoin, database, analysis, holding_time, trading volume",David Schulte,DEDA_class_WS21_Bitcoin_Transaction_Analysis,11,blockchain_parser.blockchain Blockchain databaseWriter DatabaseWriter time,5
Bitcoin_Transaction_Analysis,"Creates a relational database, fills it with bitcoin transaction and does an analysis on them.","bitcoin, database, analysis, holding_time, trading volume",David Schulte,DEDA_class_WS21_Bitcoin_Transaction_Analysis,11,blockchain_parser.transaction Transaction blockchain_parser.transaction Input blockchain_parser.transaction Output __init__ self  tx: ParserTx check_if_standard self __init__ self  input_: ParserIn  tx_to_id: str __init__ self  output: ParserOut  tx_from_id: str  output_no: int,26
Bitcoin_Transaction_Analysis,"Creates a relational database, fills it with bitcoin transaction and does an analysis on them.","bitcoin, database, analysis, holding_time, trading volume",David Schulte,DEDA_class_WS21_Bitcoin_Transaction_Analysis,11,analyzer Analyzer matplotlib numpy databaseReader DatabaseReader matplotlib prepare_animation bar_container animate frame_number  simulate new data coming in,16
Bitcoin_Transaction_Analysis,"Creates a relational database, fills it with bitcoin transaction and does an analysis on them.","bitcoin, database, analysis, holding_time, trading volume",David Schulte,DEDA_class_WS21_Bitcoin_Transaction_Analysis,11,,0
Finalmodel_cnnlstm,Project_Group9 file,"NUS, FE5225, bitcoin price, CNN, LSTM",Group 9,SDA_2021_NUS_Bitcoin_Price_Prediction,2,!/usr/bin/env python  coding: utf-8  In[1]: import talib os numpy pandas pickle import quandl datetime plotly plotly plotly tensorflow keras.models Sequential keras.layers Dense keras.optimizers Adam keras.preprocessing.image ImageDataGenerator keras.callbacks ReduceLROnPlateau keras.layers LSTM  In[2]:  In[3]:  In[4]: get_train_data batch_size=1 time_step=3 train_begin=0 train_end= len final)-30) y=normalized_train_data[i:i+time_step 17 np.newaxis] get_test_data time_step=3 test_begin= len final)-30-2)  size= len normalized_test_data)+time_step-1)//time_step x=normalized_test_data[i*time_step: i+1)*time_step :17] y=normalized_test_data[i*time_step: i+1)*time_step 17] model.add Dropout 0.2)) model.add Dense 15)) model.add Permute 1 15)) model = Sequential ) model.add LSTM 30  input_shape= 1  50) return_sequences=False)) model.add Dropout 0.3)) model.add LSTM 15 return_sequences=True)) model.add LSTM 10 return_sequences=False)) model.add Dropout 0.2))  In[5]: get_train_data batch_size=1 time_step=3 train_begin=0 train_end= len final)-31) y=normalized_train_data[i:i+time_step 17 np.newaxis] get_test_data time_step=3 test_begin= len final)-31-2)  size= len normalized_test_data)+time_step-1)//time_step x=normalized_test_data[i*time_step: i+1)*time_step :17] y=normalized_test_data[i*time_step: i+1)*time_step 17] model.add Dropout 0.2)) model.add Dense 15)) model.add Permute 1 15)) model = Sequential ) model.add LSTM 30  input_shape= 1  50) return_sequences=False)) model.add Dropout 0.3)) model.add LSTM 15 return_sequences=True)) model.add LSTM 10 return_sequences=False)) model.add Dropout 0.2))  In[6]: get_train_data batch_size=1 time_step=3 train_begin=0 train_end= len final)-30) y=normalized_train_data[i:i+time_step 17 np.newaxis] get_test_data time_step=3 test_begin= len final)-30-2)  size= len normalized_test_data)+time_step-1)//time_step x=normalized_test_data[i*time_step: i+1)*time_step :17] y=normalized_test_data[i*time_step: i+1)*time_step 17] model.add Dropout 0.2)) model.add Dense 15)) model.add Permute 1 15)) model = Sequential ) model.add LSTM 30  input_shape= 1  50) return_sequences=False)) model.add Dropout 0.3)) model.add LSTM 15 return_sequences=True)) model.add LSTM 10 return_sequences=False)) model.add Dropout 0.2))  In[7]: get_train_data batch_size=1 time_step=3 train_begin=0 train_end= len final)-31) y=normalized_train_data[i:i+time_step 17 np.newaxis] get_test_data time_step=3 test_begin= len final)-31-2)  size= len normalized_test_data)+time_step-1)//time_step x=normalized_test_data[i*time_step: i+1)*time_step :17] y=normalized_test_data[i*time_step: i+1)*time_step 17] model.add Dropout 0.2)) model.add Dense 15)) model.add Permute 1 15)) model = Sequential ) model.add LSTM 30  input_shape= 1  50) return_sequences=False)) model.add Dropout 0.3)) model.add LSTM 15 return_sequences=True)) model.add LSTM 10 return_sequences=False)) model.add Dropout 0.2))  In[8]: get_train_data batch_size=1 time_step=3 train_begin=0 train_end= len final)-31) y=normalized_train_data[i:i+time_step 17 np.newaxis] get_test_data time_step=3 test_begin= len final)-31-2)  size= len normalized_test_data)+time_step-1)//time_step x=normalized_test_data[i*time_step: i+1)*time_step :17] y=normalized_test_data[i*time_step: i+1)*time_step 17] model.add Dropout 0.2)) model.add Dense 15)) model.add Permute 1 15)) model = Sequential ) model.add LSTM 30  input_shape= 1  50) return_sequences=False)) model.add Dropout 0.3)) model.add LSTM 15 return_sequences=True)) model.add LSTM 10 return_sequences=False)) model.add Dropout 0.2))  In[9]: get_train_data batch_size=1 time_step=3 train_begin=0 train_end= len final)-30) y=normalized_train_data[i:i+time_step 17 np.newaxis] get_test_data time_step=3 test_begin= len final)-30-2)  size= len normalized_test_data)+time_step-1)//time_step x=normalized_test_data[i*time_step: i+1)*time_step :17] y=normalized_test_data[i*time_step: i+1)*time_step 17] model.add Dropout 0.2)) model.add Dense 15)) model.add Permute 1 15)) model = Sequential ) model.add LSTM 30  input_shape= 1  50) return_sequences=False)) model.add Dropout 0.3)) model.add LSTM 15 return_sequences=True)) model.add LSTM 10 return_sequences=False)) model.add Dropout 0.2))  In[10]: get_train_data batch_size=1 time_step=3 train_begin=0 train_end= len final)-31) y=normalized_train_data[i:i+time_step 17 np.newaxis] get_test_data time_step=3 test_begin= len final)-31-2)  size= len normalized_test_data)+time_step-1)//time_step x=normalized_test_data[i*time_step: i+1)*time_step :17] y=normalized_test_data[i*time_step: i+1)*time_step 17] model.add Dropout 0.2)) model.add Dense 15)) model.add Permute 1 15)) model = Sequential ) model.add LSTM 30  input_shape= 1  50) return_sequences=False)) model.add Dropout 0.3)) model.add LSTM 15 return_sequences=True)) model.add LSTM 10 return_sequences=False)) model.add Dropout 0.2))  In[11]: get_train_data batch_size=1 time_step=3 train_begin=0 train_end= len final)-30) y=normalized_train_data[i:i+time_step 17 np.newaxis] get_test_data time_step=3 test_begin= len final)-30-2)  size= len normalized_test_data)+time_step-1)//time_step x=normalized_test_data[i*time_step: i+1)*time_step :17] y=normalized_test_data[i*time_step: i+1)*time_step 17] model.add Dropout 0.2)) model.add Dense 15)) model.add Permute 1 15)) model = Sequential ) model.add LSTM 30  input_shape= 1  50) return_sequences=False)) model.add Dropout 0.3)) model.add LSTM 15 return_sequences=True)) model.add LSTM 10 return_sequences=False)) model.add Dropout 0.2))  In[12]: get_train_data batch_size=1 time_step=3 train_begin=0 train_end= len final)-31) y=normalized_train_data[i:i+time_step 17 np.newaxis] get_test_data time_step=3 test_begin= len final)-31-2)  size= len normalized_test_data)+time_step-1)//time_step x=normalized_test_data[i*time_step: i+1)*time_step :17] y=normalized_test_data[i*time_step: i+1)*time_step 17] model.add Dropout 0.2)) model.add Dense 15)) model.add Permute 1 15)) model = Sequential ) model.add LSTM 30  input_shape= 1  50) return_sequences=False)) model.add Dropout 0.3)) model.add LSTM 15 return_sequences=True)) model.add LSTM 10 return_sequences=False)) model.add Dropout 0.2))  In[13]: get_train_data batch_size=1 time_step=3 train_begin=0 train_end= len final)-31) y=normalized_train_data[i:i+time_step 17 np.newaxis] get_test_data time_step=3 test_begin= len final)-31-2)  size= len normalized_test_data)+time_step-1)//time_step x=normalized_test_data[i*time_step: i+1)*time_step :17] y=normalized_test_data[i*time_step: i+1)*time_step 17] model.add Dropout 0.2)) model.add Dense 15)) model.add Permute 1 15)) model = Sequential ) model.add LSTM 30  input_shape= 1  50) return_sequences=False)) model.add Dropout 0.3)) model.add LSTM 15 return_sequences=True)) model.add LSTM 10 return_sequences=False)) model.add Dropout 0.2))  In[14]: get_train_data batch_size=1 time_step=3 train_begin=0 train_end= len final)-28) y=normalized_train_data[i:i+time_step 17 np.newaxis] get_test_data time_step=3 test_begin= len final)-28-2)  size= len normalized_test_data)+time_step-1)//time_step x=normalized_test_data[i*time_step: i+1)*time_step :17] y=normalized_test_data[i*time_step: i+1)*time_step 17] model.add Dropout 0.2)) model.add Dense 15)) model.add Permute 1 15)) model = Sequential ) model.add LSTM 30  input_shape= 1  50) return_sequences=False)) model.add Dropout 0.3)) model.add LSTM 15 return_sequences=True)) model.add LSTM 10 return_sequences=False)) model.add Dropout 0.2))  In[15]: get_train_data batch_size=1 time_step=3 train_begin=0 train_end= len final)-31) y=normalized_train_data[i:i+time_step 17 np.newaxis] get_test_data time_step=3 test_begin= len final)-31-2)  size= len normalized_test_data)+time_step-1)//time_step x=normalized_test_data[i*time_step: i+1)*time_step :17] y=normalized_test_data[i*time_step: i+1)*time_step 17] model.add Dropout 0.2)) model.add Dense 15)) model.add Permute 1 15)) model = Sequential ) model.add LSTM 30  input_shape= 1  50) return_sequences=False)) model.add Dropout 0.3)) model.add LSTM 15 return_sequences=True)) model.add LSTM 10 return_sequences=False)) model.add Dropout 0.2))  In[16]: matplotlib pyplot  In[17]:  In[18]:  In[19]:  In[20]:  In[21]:  In[22]:  In[23]:  In[24]:  In[ ]:,765
SDA_2019_St_Gallen_PD_with_ML_and_Sentiment_Analysis_Descriptive_Statistics,"Create summary statistics for financial ratios and make visualizations for macroeconomic indicators, polarity scores and overview of defaults","summary statistics, boxplot, areachart, linegraph, macroeconomic indicators, polarity scores, financial ratios",ERROR,Descriptive_Statistics,2, -*- coding: utf-8 -*- os pandas numpy seaborn matplotlib scipy math  financial ratios summary statistics  Macro indicators graphs Plot macro variables except FDI  instantiate a second axes that shares the same x-axis  otherwise the right y-label is slightly clipped Plot FDI  scale the data  Boxplots for polarity scores,48
SDA_2019_St_Gallen_PD_with_ML_and_Sentiment_Analysis_Descriptive_Statistics,"Create summary statistics for financial ratios and make visualizations for macroeconomic indicators, polarity scores and overview of defaults","summary statistics, boxplot, areachart, linegraph, macroeconomic indicators, polarity scores, financial ratios",ERROR,Descriptive_Statistics,2, -*- coding: utf-8 -*- os pandas matplotlib  Set working directory get descriptive statistics group companies by year group companies by year group companies by state group companies by state group companies by sector group companies by sector,37
pyTSA_GDPChinaDecomposition,This Quantlet performs additive and multiplicative decomposition of Chinese quarterly GDP from 1992 to 2017 and  plots results (Figures 2.21-2.23 in the book),"time series, autocorrelation, Chinese GDP, plot, visualisation","Huang Changquan, Alla Petukhina",pyTSA_GDPChinaDecomposition,1,pandas statsmodels.tsa.seasonal seasonal_decompose matplotlib pandas.plotting lag_plot PythonTsa.plot_acf_pacf acf_pacf_fig statsmodels.tsa.stattools acf statsmodels.tsa.holtwinters ExponentialSmoothing fig.set_size_inches 18.5  10.5) fig.set_size_inches 18.5  10.5),18
DEDA_NewspaperStockPrices,These program scrapes data from sueddeutsche.de and faz.net and trains with these data a decision tree and a logistic regresion.,ERROR,Christopher Ketzler,DEDA_NewspaperStockPrices,1, coding: utf-8 requests datetime bs4 BeautifulSoup nltk.tokenize sent_tokenize nltk.corpus stopwords stop_words get_stop_words feedparser selenium webdriver selenium.webdriver.common.keys Keys time csv pandas  represents an article with its important metadata  constructor for the class Article __init__ self aut  dat  titl  txt  lin  removing german stop words   remove_stop_words self  lang  webscraping object for sueddeutsche.de   loading the search load_search self searchterm time.sleep 5) # Test ####### .click )  loading of the articles  get_articles self links  webscraping object for faz.net http://www.faz.net/suche/?offset=&cid=&index=&query=VW&offset=&allboosted=&boostedresultsize=%24boostedresultsize&from=TT.MM.JJJJ&to=09.12.2017&chkBox_2=on&chkBox_3=on&chkBox_4=on&BTyp=redaktionelleInhalte&chkBoxType_2=on&author=&username=&sort=date&resultsPerPage=20  loading the search load_search self searchword  loading of the articles get_articles self links  Upload fo sock prices from a csv file getStockPrice self  loading the stock prices for a company  compute the pricedifference  Analyse create a dataFrame with two column  Date  price difference) analyseArticles self listArticles  stockpriceDifferences  price differenz ###### Main ############  Which company do we want to search?  webscrapping sueddeutsche.de  webscrapping faz.net  removing stop words #stockPriceLoader convert date format time # Analyse  returns a dataFrame with date and price difference  1 = rising stock prices  0 = failing stock prices ############# ML ############# # Dec. Tree sklearn tree sklearn preprocessing numpy  factorize in 2d list train decision tree predict # Log Regression sklearn.linear_model LogisticRegression  factorize in 2d list train Logistic Regression predict,198
DEDA_Class_2017_FacebookPartiesSentiment,These py-scripts help obtaining partie's messages from their facebook pages and analyzing their sentiment.,ERROR,Christine Lud�uweit,DEDA_FacebookPartiesSentiment,2,facebook re helps sklearn.model_selection train_test_split sklearn.linear_model LogisticRegression sklearn.metrics classification_report matplotlib  @TODO update token via https://developers.facebook.com/tools/explorer/145634995501895/  build connection to Facebook's API Graph  search for the page IDs of the seven big parties which are currently in the parlament  Bundestag)  get minimum 100 messages of the facebook page  clean messages  stem words  remove stopwords)  get the corpusses  Dictionary and Sentences)  testwords = helps.getTestDataDict )  vectorize the sentences of the corpus  split corpus into train and test set  do logistic regression  print summary of the model  contains e.g. accuracy of the model)  train classifier with whole corpus  get predictions  summarize the percentages of positive sentences  summarize the persentages of negative sentences  plot this F20000'),111
DEDA_Class_2017_FacebookPartiesSentiment,These py-scripts help obtaining partie's messages from their facebook pages and analyzing their sentiment.,ERROR,Christine Lud�uweit,DEDA_FacebookPartiesSentiment,2,"requests nltk nltk.tokenize RegexpTokenizer nltk.corpus stopwords pandas itertools nltk.stem.snowball GermanStemmer sklearn.feature_extraction.text CountVectorizer numpy pathlib Path os  get minimum 100 messages get_all_messages_of_a_page graph  page_id  get all messages out of the dict  response of API)  loop as long as minimum 100 messages are collected clean_word_list msg  find the next punctuation mark  end loop if no punctuation mark is found anymore  then i = -1)  make sure that the dot is not a part of a hyperlink or of a date  put all sentences together  filter messages  stem and remove stop words) remove_stop_words msg  remove stop words and stem words  def getTestDataDict ):     #get the Dictionary     neg = pd.read_csv path + ""SentiWS_v1.8c""+ sep + ""SentiWS_v1.8c_Negative.txt""                            sep=""\t""  header=None  names=[""wordplustype""  ""polarity""  ""sim""])  prepare Dictionary     negativedata = prepareSentimentDict neg)     pos = pd.read_csv path + ""SentiWS_v1.8c""+sep+""SentiWS_v1.8c_Positive.txt""                            sep=""\t""  header=None  names=[""wordplustype""  ""polarity""  ""sim""])     positivedata = prepareSentimentDict pos)     result = positivedata.append negativedata  ignore_index=True)     return result prepare_sentiment_dict data  seperate original word and similar words  split chain of similar words into single words  build the dataframe  repeat word tpye and polarity for every similar word)  build final dataframe get_test_data_sentences   get corpus of sentences  split sentences in phrase and polarity  + = pos  - = neg  0 = neutral  # = bipolar  both negative and positive)  remove 0 and # since only positive and negative phrases are relevant "":  concatenate the words  for regression modeling vectorize data tokenize text",226
DEDA_MachineLearning,ERROR,ERROR,Junjie Hu,DEDA_MachineLearning,4,!/usr/bin/env python  coding: utf-8  In[1]: numpy matplotlib seaborn  In[2]: 1. sigmoid  In[3]: 2. RELU rectified x  define a series of inputs  calculate outputs for our inputs  line plot of raw inputs to rectified outputs  In[4]: 3. tanh tanh x  In[5]: 4 Sigmoid,42
DEDA_MachineLearning,ERROR,ERROR,Junjie Hu,DEDA_MachineLearning,4,"keras.datasets mnist keras.utils np_utils keras.utils.vis_utils plot_model keras.models Sequential keras.layers.convolutional Convolution2D keras.layers Activation keras.layers.normalization BatchNormalization numpy time keras.layers SimpleRNN matplotlib keras.models load_model trainning_process model_history  Accuracy Increasing  Loss Decreasing  download the mnist to the path '~/.keras/datasets/' if it is the first time to be called  Parameter Specification  for reproducibility  data pre-processing  Normalize pixel data  Transform label set into binary representation  or so called ""One-hot encoding""  build RNN model  RNN model specification  Pool layer  Randomly deactivate neurons  Transform into 1 dimensional data  Full connected  Compile with defined parameters  training  Takes about two minutes to finish  The final cross validation accuracy is 93.51%  ============Load Trained Model=============",102
DEDA_MachineLearning,ERROR,ERROR,Junjie Hu,DEDA_MachineLearning,4,sklearn.datasets load_iris  More toy data sets from sklearn please refer:  http://scikit-learn.org/stable/datasets/ numpy sklearn tree graphviz sklearn.metrics accuracy_score  use this line in terminal if graphviz does not work: conda install python-graphviz   Glance the data set  name of features  data for the 4 features  name of the targets  data for the targets  randomly choose 1/3 of samples as testing data  training data  testing data  train the model  initial an decision tree classifier object with given arguments  A lot of arguments can be placed into the object  Refer docs: http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier  make prediction  visualizing the tree,92
DEDA_Class_2018WS_Berlin_Property_Analysis_Growth_Plot,"Plotting script for visualising the growth in property sale prices in Berlin, DE over the last 11 years (by district).","Time Series Data, Real Estate, Data Visualisation",Alex Truesdale,DEDA_Class_2018WS_Berlin_Property_Analysis_Growth_Plot,1, -*- coding: utf-8 -*- re os time numpy pandas matplotlib  Introduction.  Set matplotlib settings.  Read in data set.  Change directory to output dir.  Construct price growth chart for all districts  must be run in block).  Construct labels image from above plot  must be run in block).  Define of-interest districts list.  Construct price growth chart for of-interest districts  must be run in block).  Construct labels image from above plot  must be run in block).  Define of-interest districts list.  Construct price growth chart for trimmed of-interest districts  must be run in block).  Construct labels image from above plot  must be run in block).,101
SDA_2020_St_Gallen_02_Cleaning_EDA,Cleaning unstructured text data and performing an LDA analysis of quarterly Swiss National Bank Monetary Policy Assessments,"Textmining, Cleaning, Sentiment, Analysis, Latent Dirichlet Allocation, Unsupervised Machine Learning, Topic Modeling","Stefan Diener, Jan Scheidegger",SDA_2020_St_Gallen_02_Cleaning_EDA,1,"os pandas openpyxl  install this dependency time matplotlib wordcloud WordCloud re textblob TextBlob nltk WordNetLemmatizer nltk.tokenize word_tokenize nltk.corpus stopwords sklearn.feature_extraction.text CountVectorizer sklearn.decomposition LatentDirichletAllocation pyLDAvis pyLDAvis sklearn numpy matplotlib seaborn  Needs to be downloaded once!  import nltk  nltk.download 'stopwords')  nltk.download 'punkt')  nltk.download 'wordnet')  Set working directory to file location  %% Functions clean_texts text_col  define stop words  clean on tweet level  rm links  remove special characters  remove numbers  convert to lower  Clean at word level  splits text_col into tokens / words.  remove stopwords  lemmatize tokens  lemmatize tokens  remove all single get_polarity_subjectivity text_col plot_wordcloud text_col  stop_words = [""covid19""  ""coronavirus""] plot_polarity_subjectivity_dist sent_col plot_polarity_subjectivity_dev sent_col  date_col  Helper function plot_10_most_common_words count_data  count_vectorizer  Helper function print_topics model  count_vectorizer  n_top_words %d:"" % topic_idx) train_plot_LDA df  Initialise the count vectorizer with the English stop words  Fit and transform the processed titles  Visualise the 10 most common words  Tweak the two parameters below  Create and fit the LDA model  Print the topics found by the LDA model warnings  %% Main main   Cleaning  EDA  %% Run file",166
SFEAdfKpss,Computes the ADF and KPSS test statistics for 20 biggest companies (by market capitalisation) of FTSE100 and DAX. Estimated ADF and KPSS test with only a constant and a constant plus linear trend. Adds asterisk to indicate significance at 5%-confidence level,"ADF-Test, nonstationary, dax, ftse100, random-walk","Joanna Tomanek, Sophie Burgard",SFEAdfKpss,1,numpy pandas matplotlib statsmodels.tsa.stattools kpss statsmodels.tsa.stattools adfuller Import Data ADF function which inputs a dataframe  calculates test statistics and  assigns * if result is significant at 5% level  outputs marked values of ADF test statistic ADF df maxlag regression test decision KPSS function which inputs a dataframe  calculates test statistics and  assigns * if result is significant at 5% level  outputs marked values of KPSS test statistic KPSS df lags regression test decision ignore warnings from statsmodels package related to the displayed p-value warnings,84
SMScarsim_python,"computes the Jaccard, simple matching and Tanimoto proximity coefficients for binary car data","cluster-analysis, distance, proximity, Jaccard proximity coefficient, Tanimoto proximity coefficient, simple matching coefficient","Awdesch Melzer, Muhaimin",SMScarsim_python,1, -*- coding: utf-8 -*- pandas numpy sklearn metrics scipy.spatial distance renault rover toyota column mean binary matrix jaccard score simple matching score roger score,24
LDA-DTM_dirichlet_simulation,Simulated article using different Dirichlet hyperparameters to show the mechanism of LDA,"LDA, Dirichlet, Simulated Articles, Hyperparameters, Word distribution",Xinwen Ni,LDA-DTM_dirichlet_simulation,1,!/usr/bin/env python3  -*- coding: utf-8 -*- pandas numpy random matplotlib  length of the simulated document   dirictlet parameters   control document via topic  control document via topic  assume there is only one topic  generate m different documents according  to different dirictlet perameters   here fixed the number of words in the topic  plot the prob distribution ,53
Doc2VectorSpace,Represents large documents with at least partially overlapping vocabulary in a 3d vector space,"Vector Space Model, text mining, literature interpretation, matplotlib, 3d plot",Jerome Bau,Doc2VectorSpace,1,matplotlib mpl_toolkits.mplot3d Axes3D pandas  File names  Choose 3 index terms  Colors in order file_to_corpus names corpus_to_dataframe corp create_vocabulary_distinct_from_corp corp_pd corpus_to_vecs corp_pd vocab  replace nan values with 0 ex   A normalization of the vectors length can be added by dividing each column by the length of the document visualise   adapt to length of vectors for better visibility  change color,58
awcd,Implementation of the Adaptive Weights Community Detection algorithm.,"community-detection, kullback-leibler, nonparametric, adaptive-weights, gap-coefficient, graph-clustering, overlapping-communities","Larisa Adamyan, Kirill Efimov, Vladimir Spokoiny",awcd,5,numpy matplotlib src.AWCD AWCD src.awcd_help clustering_from_weights_sparse utils.graph_generator generate_sbm utils.scores NMI run_awcd_example   wa  W * A) for fast calculation  'ww'  W * W) for full calculation.  number of AWCD update steps,30
awcd,Implementation of the Adaptive Weights Community Detection algorithm.,"community-detection, kullback-leibler, nonparametric, adaptive-weights, gap-coefficient, graph-clustering, overlapping-communities","Larisa Adamyan, Kirill Efimov, Vladimir Spokoiny",awcd,5,"numpy matplotlib pylab rcParams copy scipy.sparse coo_matrix numexpr src.cython_c_wrapper.pysparsematrixdot sparse_matrix_dot draw_step_AWCD windows helper_shorter A  B  C  n  mask AWCD_step_sparce A  l  weights  KL  params={}  we remove center point i from calculation and all its edges ik  numpy S_ak = np.maximum np.dot np.dot weights  A)  weights)  1) S_ak = np.maximum np.dot np.dot weights  A)  weights)  1) A = A.tocsr ) S_ak.data = np.maximum S_ak.data  1)  numpy  numpy theta_ak = S_ak / N_ak N_ak = np.maximum np.dot np.dot weights  np.ones  n  n)))  weights)  1) theta_a_plus_ak =  S_ak + S_a) /  N_ak + N_a) theta_a_plus_k =  S_a + S_a.T) /  N_a + N_a.T) N_a = np.diagonal N_ak)[:  np.newaxis] T_A = N_a * KL[THETA_A  theta_a_plus_ak] + \       N_ak * KL[theta_ak  theta_a_plus_ak] ne.evaluate ""where T_A_data < b  b  T_A_data)"") I_1_a =  theta_ak <= THETA_A) *  THETA_A <= THETA_K) I_1_a_data =  theta_ak_data <= theta_a_row) *  theta_a_row <= theta_a_col) * 1  T = T_A_T weights[I] =  T[I] <= l) * 1 if 'remove_diag' in params: AWCD_step A  l  weights  KL  params={}  first we decide which pairs of nodes we want to consider  if dist_matrix[i  j] > 0 then we update weight[i  j]  we remove center point i from calculation and all its edges ik KL_init params init A  params AWCD A  l  K=5  sparse=False  show_step=False  show_finish=False  C_true=None  return_all_steps=False  params={}  folder_name='1'  wa  W * A) for fast calculation  'ww'  W * W) for full calculation.  number of AWCD update steps  the main cycle",235
awcd,Implementation of the Adaptive Weights Community Detection algorithm.,"community-detection, kullback-leibler, nonparametric, adaptive-weights, gap-coefficient, graph-clustering, overlapping-communities","Larisa Adamyan, Kirill Efimov, Vladimir Spokoiny",awcd,5,numpy clustering_from_weights_sparse adjmatrix,3
awcd,Implementation of the Adaptive Weights Community Detection algorithm.,"community-detection, kullback-leibler, nonparametric, adaptive-weights, gap-coefficient, graph-clustering, overlapping-communities","Larisa Adamyan, Kirill Efimov, Vladimir Spokoiny",awcd,5,numpy scipy.sparse csr_matrix generate_sbm n_list  theta,6
awcd,Implementation of the Adaptive Weights Community Detection algorithm.,"community-detection, kullback-leibler, nonparametric, adaptive-weights, gap-coefficient, graph-clustering, overlapping-communities","Larisa Adamyan, Kirill Efimov, Vladimir Spokoiny",awcd,5,sklearn metrics numpy networkx src.awcd_help clustering_from_weights_sparse NMI_sum true  answer NMI dict_1  dict_2  n  simple version  we assume the partiotions cover all nodes modularity_calc W  sparse=False  control_95=1  n=0 modularity partition  graph  weight='weight',31
SFEBarrier_Pricing_MC,"Computes Barrier option prices using Monte Carlo method for assets with/without continuous dividends. barrier option types: up-and-out, up-and-in, down-and-out, down-and-in","binomial, Monte-Carlo, asset, option, option-price, barrier-option, up-and-out, up-and-in, down-and-out, down-and-in",Franziska Wehrmann,SFEBarrier_Pricing_MC,1,"numpy scipy.stats bernoulli matplotlib matplotlib.lines Line2D calc_parameters T  N  sigma  r  div  P up movement) sample_paths S0  N  u  d  q  M  M sample paths at once  time steps valid_barrier_type paths  barrier_type  B  keep ""always smaller than barrier"" True=1  False=0  where always False  it was never > B  keep ""always bigger than barrier"" True=1  False=0  where always False  it was never > B  keep ""at least once bigger than barrier"" True=1  False=0  where at least once True  keep ""at least once smaller than barrier"" True=1  False=0  where at least once True split_paths paths  B  K  barrier_type  option  valid  not knocked out by barrier_type with B  invalid  complement of p_valid  option is exercised  subset of p_valid calc_price p_counts  K  r  T  M  option  not possible to exercise any option sample_and_plot S0  K  B  T  N  u  d  q  M  barrier_type plt.savefig 'up-and-out_call.png'  transparent=True)  plotting The following will plot the root mean square error plt.savefig 'MonteCarlo_variation.png'  transparent=True) ###### MAIN ################  current stock price  strike price  time to maturity  volatility  interest rate  dividend  steps in tree  barrier  number of paths for Monte Carlo  calculate all prices  plot sample paths  plot Monte Carlo variations    runs for 10.000  20.000  ... 500.000 paths  Value from SFEBarrier_Pricing_Tree",200
Augmento_Clustering,A clustering approach to analysis of Augmento.ai sentiments/topics (daily values). K-Means with a varying number of clusters is applied (2–15). The clustering results for 4 clusters are then visualized against BTC Price as well as the 2D depiction of Augmento.ai data using t-SNE.,"sentiments, cryptocurrency, bitcoin, market analysis, K-Means, t-SNE","Gleb Zhidkov, Anna Shchekina, Vanessa Guarino",Augmento_clustering,2,!/usr/bin/env python  coding: utf-8  In[1]:  Load required packages pandas numpy time sklearn.cluster KMeans sklearn.manifold TSNE matplotlib matplotlib seaborn  In[2]:  Load data 24h  with price  no price  In[3]:  24h -- clusters based on all sentiments  In[4]:  Quick overview over clustering results: pandas.plotting register_matplotlib_converters plt.savefig 'clusters_' + str i) + '_all_new.png'  dpi=300  transparent=True)  In[5]:  loop over plots and save them for a visual comparison  uncode to execute loop 2 4   In[6]:  24h -- 4 clusters 2 4   In[7]:  NOT used in the presentation  code originally was referencing 1h data  adjusted by G.) sklearn.decomposition TruncatedSVD matplotlib matplotlib matplotlib  In[8]:  TSNE  In[9]:  In[10]:  In[11]:,100
pyTSA_MA2,"This Quantlet simulates and plots MA(2) -  moving average process time series, its ACF and PACF","time series,  stationarity, autocorrelation, PACF, ACF, simulation, stochastic process, ARMA, moving average, autoregression",ERROR,pyTSA_SimMA2,1,numpy pandas statsmodels.tsa.arima_process arma_generate_sample matplotlib PythonTsa.plot_acf_pacf acf_pacf_fig pandas.plotting lag_plot  ar = [1] means no ar part in the model  x is not a series  now x is a series,29
BTC_ANA_address_classification,Multi Class Address Classification of unknown Bitcoin Addresses through the full transaction history of every address and the previously computed features.,"Bitcoin, Blockchain, Transactions, Machine Learning, Classification, XGB, LightGBM","David Steiner, Minh Nguyen",BTC_ANA_address_classification,3, -*- coding: utf-8 -*- numpy pandas xgboost lightgbm sklearn.linear_model LogisticRegression sklearn.model_selection train_test_split sklearn.ensemble RandomForestClassifier sklearn.metrics classification_report classification_pipeline algorithm_pipeline  =============================================================================  Load dataset  ============================================================================= get encoded labels sklearn.preprocessing LabelEncoder features  targets random oversampling imblearn.over_sampling SMOTE test  train split  =============================================================================  Logistic Regression Best Params: {'tol': 0.0001  'max_iter': 5000  'C': 1.0} Accuracy: 0.48  F1 macro: 0.42  =============================================================================  =============================================================================  Random Forest Params: {'n_estimators': 500  'max_leaf_nodes': 500  'max_depth': 24} Accuracy: 0.81 F1 macro: 81  =============================================================================  =============================================================================  XBGBoost Regression  Accuracy 0.844  =============================================================================  =============================================================================  LightGBM  Accuracy 0.85 param_grid = {'subsample_freq': [20]  'subsample': [0.7]  'reg_lambda': [1.1]  'reg_alpha': [1.2]  'num_leaves': [300]  'n_estimators': [1000]  'min_split_gain': [0.3]  'max_depth': [20]  'colsample_bytree': [0.9]}  ============================================================================= param_grid = {'subsample_freq': [20]  'subsample': [0.7]  'reg_lambda': [1.1]  'reg_alpha': [1.2]  'num_leaves': [300]  'n_estimators': [1000]  'min_split_gain': [0.3]  'max_depth': [20]  'colsample_bytree': [0.9]} Feature Importance  =============================================================================  Neural Network v1  =============================================================================  define baseline model baseline_model   create model  Compile model  =============================================================================  Neural Network   ============================================================================= keras.models Sequential keras.layers Dense keras.wrappers.scikit_learn KerasClassifier keras.utils np_utils sklearn.model_selection cross_val_score sklearn.model_selection KFold sklearn.preprocessing LabelEncoder feature and target split Standartization sklearn.preprocessing StandardScaler Label encoding sklearn.preprocessing LabelBinarizer test  train split  Initialize the constructor  input layer   hidden layer   output layer  Model details categorical_crossentropy  binary_crossentropy SGD  RMSprop  adam Evaluation  Import the modules from `sklearn.metrics` sklearn.metrics confusion_matrix  Confusion matrix  =============================================================================  Save model  ============================================================================= sklearn.externals joblib  =============================================================================  Test with validation set  GBM Accuracy: 0.883  =============================================================================   sklearn.metrics accuracy_score  =============================================================================  Predict unknown transactions  =============================================================================,215
BTC_ANA_address_classification,Multi Class Address Classification of unknown Bitcoin Addresses through the full transaction history of every address and the previously computed features.,"Bitcoin, Blockchain, Transactions, Machine Learning, Classification, XGB, LightGBM","David Steiner, Minh Nguyen",BTC_ANA_address_classification,3, -*- coding: utf-8 -*- numpy matplotlib sklearn.model_selection GridSearchCV sklearn.metrics classification_report  =============================================================================  Pipeline  ============================================================================= Plot Confusion Matrix,16
BTC_ANA_address_classification,Multi Class Address Classification of unknown Bitcoin Addresses through the full transaction history of every address and the previously computed features.,"Bitcoin, Blockchain, Transactions, Machine Learning, Classification, XGB, LightGBM","David Steiner, Minh Nguyen",BTC_ANA_address_classification,3, -*- coding: utf-8 -*- pandas sklearn.preprocessing LabelEncoder lightgbm sklearn.model_selection train_test_split classification_pipeline algorithm_pipeline  =============================================================================  Load dataset  ============================================================================= get encoded labels features  targets test  train split  =============================================================================  Model  =============================================================================  =============================================================================  Remove Mixer from categories  =============================================================================,33
LOBDeepPP_TS_properties,Joint Limit order book ask and bid price predition for multiple predicton horizons with a neural networks. This script analysis and visualizes properties of the limit order book ask and bid prices.,"Limit order book, finance, forecasting, prediction, order book size, properties, time-series",Marius Sterling,LOBDeepPP_ts_properties,1,!/usr/bin/env python3  -*- coding: utf-8 -*- os numpy scipy.stats jarque_bera scipy.stats pearsonr matplotlib sys LOB.LOB_keras_train_class LOB_keras_train_class  %%  %% Seting up model  %% Computation of h-step log returns  %% Ratio of zero h-step log returns  %% Standard deviation of h-step log returns  %% Standard deviation standardized by $h^0.5$ of h-step log returns  plt.ylim [tmp.std axis=0).min )*0  tmp.std axis=0).max ) + 0.1])  %% Mean h-step log returns  %% Skewness of h-step log-returns  %% Kurtosis of h-step log-returns  %% Bera Jarque normality statistic  %% Correlation,82
DEDA_Class_2018_Crunchbase,Predicting success of startups based on crunchbase database,"prediction, operating status, closed, startups, industry, funding, founding",Miriam Hruzova,DEDA_Class_SS2018_Sucess_of_startups-master,2,"!/usr/bin/env python  coding: utf-8  # Predicting Closed Companies Using Crunchbase Data  In[1]:  Loading the preamble: csv pandas matplotlib numpy seaborn datetime matplotlib warnings sklearn sklearn.utils resample sklearn datasets sklearn.metrics mean_squared_error sklearn decomposition sklearn.preprocessing StandardScaler sklearn.decomposition PCA sklearn.preprocessing Imputer sklearn.feature_selection f_regression sklearn.linear_model LinearRegression statsmodels scipy stats statsmodels.stats.outliers_influence variance_inflation_factor sklearn.model_selection train_test_split scipy stats sklearn model_selection sklearn.model_selection cross_val_score sklearn.linear_model LogisticRegression sklearn metrics sklearn.metrics confusion_matrix itertools sklearn.metrics roc_auc_score sklearn.metrics roc_curve sklearn.feature_selection SelectKBest matplotlib rcParams sklearn.utils resample  In[2]:  Loading datasets with information about companies and people of crunchbase.com  ## Data Manipulation and Exploratory Analysis  In[3]:  Peak at the data  In[4]:  In[5]:  In[6]:  In[7]:  In[8]:  In[9]:  In[10]:  Tranforming funding column to numerical format  In[11]:  In[12]:  In[13]:  In[14]:  Top 5 markets by company count  In[15]:  Top 10 countries by company count  In[16]:  only around 6% of companies present on crunchbase.com are registered as ""closed""  In[17]:  Check number of unique companies in dataframe  In[18]:  Check median of funding  In[19]:  Aggregate people on chrunchbase by organisation/company  In[20]:  In[21]:  In[22]:  This will create a new index  from 0 to XY)  organization will become a column   In[23]:  In[24]:  Get rid of \n  In[25]:  In[26]:  Create master dataframe from organizations AND people  In[27]:  In[28]:  Check number of observations and columns  In[29]:  Creating dummies for company status  In[30]:  Checking correlations between company status and numerical variables  In[31]:  In[32]: plt.tight_layout )  In[33]:  In[34]:  In[35]:  In[36]: plt.tight_layout )  In[37]:  In[38]: plt.tight_layout )  In[39]:  In[40]:  In[41]: plt.tight_layout )  In[42]:  In[43]:  In[44]:  For the purposes of this project  we define startups as companies founded   after the Dotcom bubble crash  i.e. the year 2000  In[45]:  In[46]:  In[47]:  In[48]:  In[49]:  In[50]: plt.tight_layout )  In[51]: plt.tight_layout )  In[52]: plt.tight_layout )  In[53]: plt.tight_layout )  In[54]:  In[55]:  In[56]:  In[57]: industry by of funding  In[58]: industry by of funding percentage  In[59]:  In[60]:  In[61]:  In[62]:  Aggregate people by organisation/company  In[63]:  In[64]:  Issues with time_between_fundings calculation  therefore export to csv  formatting will be repaired in MS Excel)  In[66]:  In[67]:  In[68]:  In[69]:  In[70]:  In[71]:  In[72]:  In[121]:  In[122]:  In[123]:  Loading additional data to analyse every founding raised per round:  In[124]:  In[120]:  In[78]:  # Making Predictions for Closed Startups  In[79]:  In[80]:  In[81]:  In[82]:  In[83]:  In[84]:  In[85]:  In[86]:  Balancing data for Machine Learning to avoid bias  Separate majority and minority classes  Downsample majority class  sample without replacement  to match minority class  reproducible results  Combine minority class with downsampled majority class  Display new class counts  In[87]:  In[88]:  In[89]:  In[90]:  In[91]:  In[92]:  In[93]:  Making predictions  In[94]:  In[95]:  Plot non-normalized confusion matrix plt.figure figsize= 8 8)) plot_confusion_matrix confusion_matrix_log  classes=class_names                        title='Confusion matrix  without normalization')  Plot normalized confusion matrix  In[96]:  In[97]:  In[98]:  In[99]:  In[100]:  In[101]:  In[102]:  In[103]:  In[104]:  In[105]:  In[106]:  In[107]:  In[108]:  In[109]: plt.legend handles=[US_median] fontsize=12)  In[ ]:",434
DEDA_Class_2018_StGallen_OLSandARMA,This Quantlet contains one of two analyses that explore the relationship between weather data and stock returns. RunOLSandARMA.py estimates parameters of different linear and autoregressive models.,ERROR,ERROR,DEDA_Class_2018_StGallen_OLSandARMA,1," -*- coding: utf-8 -*- os pandas numpy statsmodels matplotlib statsmodels.tsa.statespace.sarimax SARIMAX statsmodels.graphics.tsaplots plot_acf statsmodels.graphics.tsaplots plot_pacf  local imports from utils folder utils  making a directory to save plots if not existing:  loading the data  weather_path = ""./data/Weather_ALL.csv""  stock_path = ""./data/StockIndices.csv""  options: SSMI  SPX  NDX  IXIC  SSMI  N225  FTSE  options: Boston  Chicago  London  New York  San Francisco  Zurich main  OLS_and_ARMA index  city  Merging prices and weather  Generate weather binaries  Generate season binaries  MODEL1:  return_t = return_t-1 + binaryCOLD + binaryRAIN + interactionCOLDRAIN + binaryWINTER + binarySOMMER + binaryMONDAY  Regressions:  MODEL2:  return_t = return_t-1 + DayDifference + binaryRAIN + binaryWINTER + binarySOMMER + binaryMONDAY  Choice of exogenous variables:  Regressions:  ARMA #####################  acf & pacf  ARMA11 NO exogenous variables  Plot residual errors  ARMA11 with exogenous variables OLS_global index  Generate season binaries  Mondays are bad ^^  Winter is November  December  January  Creating weather binaries for each city  creating global weather binaries that are only true if it's in every city  convert to integers  calculating log returns  doing the regression estimate_linear df  dependent  regressors plot_and_save_stats model  name  approach improved by OP -> monospace!",177
Cryptocurrencies_Sentiment_Correlation_Analysis,Benchmarking the strength of correlation between sentiment in the market and cryptocurrency performance by conducting a sentiment analysis using Twitter data,"sentiment analysis, twitter, cryptocurrency, crosscorrelation, bitcoin","Makar Evdokimov, Selen Seltuk",Sentiment-correlation-Analysis,2,"!/usr/bin/env python  coding: utf-8  In[20]:  importing packages os re ast json time pandas numpy vaderSentiment.vaderSentiment SentimentIntensityAnalyzer datetime datetime twint nest_asyncio matplotlib  In[3]:  functions that are initially used scrapeTweets coin  date:str  limit=None get_sentiment df '  ''))  df['tweet']))  adding dictionary of words related to specific cryptos to deteermine them in tweets clean df  coin  urls  other_coins  ads format_dates df fetch_prices ticker  resolution  period aggregate_and_get_prices df_sentiment  df_prices  df_prices_minutes  resolution='hour' crosscorr x y lags  ## 1. Analysis of Bitcoin tweet sentiment and performance  ### 1.1. Scraping tweets and estimating sentiment scores  Below is the demonstration of how the data was scraped and processed. Each of the cells can be run separately if there is a need to repeat any step  tweet scraping  sentiment estimation  price fetching  aggregation). There are also already scraped and already processed data available via the link https://drive.google.com/drive/folders/1z-nmjEZJMfgC4r6-Gbf5YQrzadVJ4vm2?usp=sharing. Files shoud be added to working directory.  In[12]:  scrape tweets bitcoin'  date= '2021-02-28'  '2021-03-31'))  In[13]:  calculate the sentiment  ### 1.2. Fetching the prices  In[14]:  fetch the prices binance.client Client  ### 1.3. Visualizaing and examining raw values  In[15]:  aggregation of the data on hourly and daily level  In[54]:  In[11]:  plotting matplotlib  In[55]:  In[14]:  boxplots for each day  #### Wordcloud  In[15]: wordcloud WordCloud  stopwords.update [""elonmusk"" ""elon musk"" ""elon"" ""musk"" ""spacex""]) #adding our own stopwords if needed  In[16]:  list of ad-related words that are common in tweets  ### 1.4. Removing advertisements  URL links  and mentions of other coins  In[17]:  cleaning bitcoin data from ads  In[18]:  aggregation of the cleaned data  ### 1.5. Visualizing and examining the cleaned data  In[48]:  We can set the number of bins with the `bins` kwarg  In[56]:  In[65]:  In[61]:  In[62]:  In[21]:  In[64]:  ## 2. Estimating sentiment  fetching prices and aggregating values  In[22]: ethereum'  dogecoin'  xlm'  uniswap'  sushiswap'   In[23]: binance.client Client  In[24]: get_dataframes coin  Below the sentiment and prices are obtained for 5 different altcoins: Ethereum  Stellar Lumens  Dogecoin  Uniswap  Sushiswap.  In[25]:  In[26]:  In[27]:  In[28]:  In[29]:  ## 3. Cross-correlation Analysis  ### 3.1. Handling the missing values  Missing values are handling manually due to the lack of time - replaced with average of neighbour values.  In[90]:  In[93]:  In[95]:  In[97]:  In[99]:  In[101]:  In[30]:  ### 3.2. Forming series and checking for stationarity  In[31]: get_return df  In[32]:  returns for all cryptos  Adjusted Dickey-Fuller and KPSS test are used to check for stationarity of the resulting time series  In[33]: statsmodels.tsa.stattools adfuller adjdf series kpss_test series  In[35]:  In[36]:  While series of returns are stationary  series of sentiment scores and weighted sentiment scores proved to be non-stationary. Hence  the first-order differences were used to analyze cross-correlation.  In[37]: get_series df  In[38]:  getting series of differences  sentiment) and returns  In[ ]:  In[40]:  lags 25 hours back and 25 hours forward  In[39]: df_ccf sent_diff  wsent_diff  ret  lags  In[159]:  getting CCF values  In[182]:  cross-correlation plots  Providing x-axis name.  Plotting the Autocorreleation plot.  Displaying the plot.  In[44]:  Providing x-axis name.  Plotting the Autocorreleation plot.  Displaying the plot.  In[45]:  Providing x-axis name.  Plotting the Autocorreleation plot.  Displaying the plot.  In[46]:  Providing x-axis name.  Plotting the Autocorreleation plot.  Displaying the plot.  In[172]:  Providing x-axis name.  Plotting the Autocorreleation plot.  Displaying the plot.  In[47]:  Providing x-axis name.  Plotting the Autocorreleation plot.  Displaying the plot.  In[48]:  Providing x-axis name.  Plotting the Autocorreleation plot.  Displaying the plot.  In[163]:  In[162]:  In[164]:  In[165]:  In[166]:  In[167]:  ## Epilogue: Prices of cryptocurrencies  In[183]:  In[184]:",540
spd_trading,This package estimates the Risk Neutral Density (RND) and Historical Density (HD) of an underlying and suggests a trading strategy based on the Pricing Kernel K=RND/HD. Uses package localpoly for all fits.,"Risk Neutral Density, Historical Density, RND, HD, Pricing Kernel, Bitcoin Options, Rookley",Franziska Wehrmann,spd_trading,4,matplotlib pyplot pandas itertools groupby operator itemgetter .utils.smoothing interpolate_to_same_interval __init__ self  tau_day  date  RND  HD  similarity_threshold=0  cut_tail_percent=0.02  parameters that are created during run calc_kernel self _M_bounds_from_list self  lst  df calc_trading_intervals self __init__ self  x=0.5 kernelplot self  Kernel  blue - put  red - call  ---------------------------------------------------------------------------------------- Moneyness - Moneyness  -------------------------------------------------------------------------------------------- Kernel K = rnd/hd,52
spd_trading,This package estimates the Risk Neutral Density (RND) and Historical Density (HD) of an underlying and suggests a trading strategy based on the Pricing Kernel K=RND/HD. Uses package localpoly for all fits.,"Risk Neutral Density, Historical Density, RND, HD, Pricing Kernel, Bitcoin Options, Rookley",Franziska Wehrmann,spd_trading,4,"logging numpy scipy.stats norm statsmodels.nonparametric.bandwidths bw_silverman localpoly.base LocalPolynomialRegression .utils.smoothing bspline .utils.density pointwise_density_trafo_K2M  ----------------------------------------------------------------------------------------------------------- CALCULATOR create_bandwidth_range X  num=10 rookley_method M  S  K  o  o1  o2  r  tau  data and meta settings  parameters for LocalPolynomialRegression  parameters that are created during run curve_fit self  X  y  h=None get_rnd self  step 0: fit iv-smile to iv-over-M option values  h_m : Union[None  float]  ------------------------------------ B-SPLINE on SMILE  FIRST  SECOND  step 1: calculate spd for every option-point ""Rookley's method""  step 2: Rookley results  points in K-domain) - fit density curve  h_k : Union[None  float]  step 3: transform density POINTS from K- to M-domain  step 4: density points in M-domain - fit density curve  h_m : Union[None  float]  ----------------------------------------------------------------------------------------------------------------- PLOT matplotlib pyplot __init__ self  x=0.5 rookleyMethod self  RND  smile  derivatives  density q_k  density q_m",127
spd_trading,This package estimates the Risk Neutral Density (RND) and Historical Density (HD) of an underlying and suggests a trading strategy based on the Pricing Kernel K=RND/HD. Uses package localpoly for all fits.,"Risk Neutral Density, Historical Density, RND, HD, Pricing Kernel, Bitcoin Options, Rookley",Franziska Wehrmann,spd_trading,4,setuptools setup pathlib  The directory containing this file  The text of the README file,14
spd_trading,This package estimates the Risk Neutral Density (RND) and Historical Density (HD) of an underlying and suggests a trading strategy based on the Pricing Kernel K=RND/HD. Uses package localpoly for all fits.,"Risk Neutral Density, Historical Density, RND, HD, Pricing Kernel, Bitcoin Options, Rookley",Franziska Wehrmann,spd_trading,4, ---------------------------------------------------------------------------------------------------------------- GARCH numpy arch arch_model time copy pickle os logging .utils.density density_estimation  timeseries  here: log_returns)  parameters that are created during run _load self _save self fit self  last observed log-return  mean adjust.  previous observed log-return  ------------------------------------------- kernel density estimation _GARCH_simulate self  pars  horizon  last observed log-return mean adj. _variate_pars self  pars  bounds simulate_paths self  horizon  simulations  variate_GARCH_parameters=True  mean  std of parameters  for reproducability in _variate_pars )  set pars for first round of simulation  summed because we have log-returns  ----------------------------------------------------------------------------------------------------------- CALCULATOR pandas  parameters that are created during run _get_log_returns self _calculate_path self  simulated_log_returns  simulated_tau_mu get_hd self  variate_GARCH_parameters=True  load GARCH Model  parameters  z_dens) matplotlib pyplot __init__ self  x=0.5 density self  HD  density q_m,111
LOBDeepPP_event_count,Joint Limit order book ask and bid price predition for multiple predicton horizons with a neural networks. This script analysis and visualizes the number of events in an order book of order book size L.,"Limit order book, finance, forecasting, prediction, order book size",Marius Sterling,LOBDeepPP_event_count,1,!/usr/bin/env python3  -*- coding: utf-8 -*- pandas matplotlib matplotlib numpy  %%  %% LOB_ol_events  %% LOB_ol_events_per_day  %% LOB_ol_events_per_second   events / events.iloc[-1] * 100).plot )  %% LOB_ol_events_per_second_per_day  %% LOB_ol_events_per_ol  %% LOB_ol_events_per_second_per_ol,29
LLE_SwissRoll,Plotting the Swiss roll and reduction using LLE,ERROR,Elizaveta Zinovyeva,LLE_SwissRoll,2,!/usr/bin/env python  coding: utf-8  # Import Packages  In[12]: sklearn.decomposition PCA matplotlib mpl_toolkits.mplot3d Axes3D sklearn manifold  # Swiss Roll. Original Data  In[13]:  In[14]:  # Swiss Roll. Reduction with PCA  Principal Component Analysis)  In[15]:  PCA on 2-dimensions  Plot data with dimensionality reduction PCA  # Swiss Roll. Reduction with LLE  Locally Linear Embeddings).  In[16]:  In[17]:  LLE  Plot data with dimensionality reduction PCA,59
SDA_20200925_hw1_Pregnant_Shark_Wiggle,Use complex numbers as parameters to fit an pregnant shark that wiggling. Creates an MP4 file of the wiggling pregnant shark,ERROR,"Piotr A. Zolnierczuk, Junjie Hu, Andreas Rony Wijaya",SDA_20200925_hw1_Pregnant_Shark_Wiggle,3, -*- coding: utf-8 -*- matplotlib matplotlib animation numpy append matplotlib  Pregnant Shark parameters fourier t  C shark t  p init_plot   draw the body of the shark  move the head and tail move_head i  move head to new position  but don't move eye stored at end or array)  initial the shark body  initialize head,53
SDA_20200925_hw1_Pregnant_Shark_Wiggle,Use complex numbers as parameters to fit an pregnant shark that wiggling. Creates an MP4 file of the wiggling pregnant shark,ERROR,"Piotr A. Zolnierczuk, Junjie Hu, Andreas Rony Wijaya",SDA_20200925_hw1_Pregnant_Shark_Wiggle,3,"!/usr/bin/env python  coding: utf-8  Pregnant Shark Wiggle  vonNeuman_elephant.py      ""With four parameters I can fit an elephant          and with five I can make him wiggle his trunk.""    Original Versions:        Author[1]: Piotr A. Zolnierczuk  zolnierczukp at ornl dot gov)      Retrieved on 14 September 2011 from      http://www.johndcook.com/blog/2011/06/21/how-to-fit-an-elephant/  Modified to wiggle trunk:      2 October 2011 by David Bailey  http://www.physics.utoronto.ca/~dbailey)        Author[2]:      Advanced Physics Laboratory      https://www.physics.utoronto.ca/~phy326/python/    Modified to Pregnant Shark Wiggle:      28 October 2020 by Andreas Rony Wijaya      Based on the paper:      ""Drawing an elephant with four complex parameters""  by      Jurgen Mayer  Khaled Khairy  and Jonathon Howard       Am. J. Phys. 78  648  2010)  DOI:10.1119/1.3254017        The paper does not specify how the wiggle parameter controls the      trunk  so a guess was made.    Inspired by John von Neumann's famous quote  above) about overfitting data.      Attributed to von Neumann by Enrico Fermi  as quoted by        Freeman Dyson in ""A meeting with Enrico Fermi"" in        Nature 427  22 January 2004) p. 297          Python Version: 3.6  Modified based on author[2]'s work  Author: Junjie Hu    Overfiting problem in trading strategy stated:  Bailey  D.  Borwein  J.  Lopez de Prado  M.  & Zhu  Q.  2014).  Pseudo-mathematics and financial charlatanism: The effects of backtest overfitting on out-of-sample performance.  In[ ]: matplotlib  you might want to use the following in terminal if the graphviz does not work:  conda install -c conda-forge ffmpeg  All should be fine though if you use jupyter notebook  In[ ]: matplotlib animation numpy append matplotlib  **The Fourier Coordinate Expansion**    The coefficients come from the four input parameters    p1=70-25j  p2=-10-15j  p3=8-10j  p4=-14-60j    Then obtained the function     fx t)=70sin t)-10sin 2t)+8cos 3t)-14cos 5t)    fy t)=-60cos t)-25sin t)-15sin 2t)-10sin 3t)    In[ ]:  Pregnant Shark parameters fourier t  C shark t  p init_plot   draw the body of the shark  move the head and tail move_head i  move head to new position  but don't move eye stored at end or array)  initial the shark body  initialize head",310
FRM_8_Moving_Network,"Generate network graphics for each our as well as a gif of moving network to analyse the relationship between the coins and macroeconomic variables for ""high-lambda"" or risky period. The node size corresponds with the number of appearance of a variable in 12 regressions, and the arrow width represents the absolute coefficient size. Here, it has done for 15th-21st of Dec 2017.","FRM Crypto, coefficient analysis, network visualization, moving network, networkx","Qi Wu, Seokhee Moon",FRM_8_Moving_Network,2,"!/usr/bin/env python  coding: utf-8  In[2]: networkx pandas numpy matplotlib  In[11]:  In[4]:  set the time frame for analysis  # Moving Network  In[14]:  create the nodes  create the edges  distinguish the nodes between coins and macro variables              generate a circular network graph  20 20      plt.savefig ""graph{:04d}.png"".format t+1)  format=""PNG"")  ## Creating a gif file for the moving network  In[16]:  create respective folders 'png' and 'gif' and move the individual network graphics into the 'png' folder.  after running this code  the created gif file will be saved in the 'gif' folder os imageio",89
NN_Variables,"Inspect the derived variables from the News Network, Plots and Stats","Textual Analysis, Network Analysis",Junjie Hu,NetworkVariables_Analysis,2,sys os pandas matplotlib statsmodels Aux_funcs generate_holidays pickle numpy datetime networkx seaborn holidays_weekends_filter ts_df compute_attention_var adj_matrix  date  Create the attention variable  degrees from the past 12-month  Daily Articles'  fontsize=label_size)  Pairs'  fontsize=label_size)  Articles'  fontsize=label_size)  companies in S&P500'  fontsize=label_size)  Estimation of the degree exponent of daily aggregated network  Plot the KDE,49
NN_Variables,"Inspect the derived variables from the News Network, Plots and Stats","Textual Analysis, Network Analysis",Junjie Hu,NetworkVariables_Analysis,2,itertools datetime generate_holidays  matplotlib numpy sklearn.model_selection GridSearchCV sklearn.neighbors KernelDensity __init__ self  data_points  delete -inf  inf and nan numbers  & data_points != np.inf)& data_points != np.nan)  Default parameters  self.x_plot = np.linspace 0  1  1000)  self.file_name = file_name bandwidth_search self  method  x_grid = np.linspace min self.data_points)  max self.data_points)  int len self.data_points)/10)) pdf_calcualtion self  **kwargs  self.log_densities[f'{bandwidth}'] = log_dens  fig = plt.figure figsize= 15  7)),61
BetaBoost,Procedure of constructing BetaBoost model for credit scoring problem. Two types of base models can be selected for the model. The quality of the model is evaluated by artificial dataset. The dataset was artificially created using separate multivariate gaussian distributions for each of the classes. The parameters for the distributions were estimated using Kaggle GMSC dataset.,"beta boost, ensemble classifier, beta binomial distribution, AdaCost, AdaBoost, credit scoring",Maciej Zieba,BetaBoost,1,csv numpy time sklearn linear_model sklearn tree sklearn metrics matplotlib  Sample artificial dataset used for testing Special value for missing cases  Checking the number of rows Intialzation of training data Readnig the dataset  Number of folds to be processed Number of folds to be run  Random splitter to separate training and testing data  Structure that collects AUC values for testing data for each fold  Base model is either LR or Tree  Number of base learners cosidered in the exepriments Parameters for Beta binomial distribution  index 1 for class 1  assumed to be positive) 0 for class 0)  Structures store data for plots  for last of the executed folds) n_folds): Creating testing and training and testing data  Separate 10% of data for validation  This structure is going to store the predictions of single base learners generateSamples N_data N_out a b,139
High_Frequency_Xgboost_MLP,Using Xgboost and MLP to do high frequency prediction of stock index futures,"High frequency, CSI 300, TFT, Xgboost, MLP","Cheng Tuoyuan, Wang Duyue, Wang Wenbo, Zheng Zhongyi",High_Frequency_Xgboost_MLP,1,astropy.time.utils split sklearn.linear_model LinearRegression sklearn.linear_model LogisticRegression sklearn.datasets load_iris sklearn.model_selection train_test_split sklearn.model_selection TimeSeriesSplit sklearn.metrics r2_score sklearn.preprocessing Binarizer sklearn.metrics accuracy_score sklearn.decomposition PCA sklearn.metrics roc_auc_score sklearn.preprocessing StandardScaler matplotlib pandas numpy #### predict one day forward ##### ## train xgb model  train test split xgboost XGBRegressor ## train neural network mlp model tensorflow.keras.models Sequential tensorflow.python.keras.layers Dense tensorflow tensorflow keras.models Sequential keras.layers.core Dense keras.layers Dropout keras.layers.normalization BatchNormalization keras.callbacks EarlyStopping,64
AOBDL_TML,"Antisocial Online behaivor detection. Contains traditional machine learning methods: ridge regression, SVM, random forest, lightGBM",ERROR,Elizaveta Zinovyeva,AOBDL_TML,2,!/usr/bin/env python  coding: utf-8  In[1]:  In[2]:  In[10]: pandas numpy matplotlib os sklearn.pipeline Pipeline sklearn.model_selection GridSearchCV GradientBoostingClassifier sklearn.model_selection StratifiedKFold sklearn.linear_model LogisticRegression sklearn.naive_bayes GaussianNB random sklearn metrics collections Counter argparse sklearn.model_selection cross_validate sklearn.metrics roc_auc_score sklearn.feature_extraction.text TfidfVectorizer sklearn.linear_model SGDClassifier sklearn.model_selection ParameterGrid lightgbm  In[11]:  GOOGLE COLAB SETUP  Load the Drive helper and mount google.colab drive  This will prompt for authorization. 2. Get the file 3. Read file as panda dataframe 2. Get the file 3. Read file as panda dataframe  In[12]:  In[13]: strat_split strat=False sklearn.model_selection train_test_split sklearn.utils shuffle sklearn.model_selection train_test_split  In[14]:  Ridge  In[15]:  In[16]: auc_pr = 0.093434375824395 auc_roc = 0.02343524395  use best C  In[17]:  print performance  print performance  print performance  In[9]:  TRAIN ON WHOLE DAATA AND PREDICT ON TEST  print performance  In[ ]:  RF  In[18]:  In[19]:  print performance  print performance  print performance  In[20]:  TRAIN ON WHOLE DAATA AND PREDICT ON TEST  print performance  In[ ]:  SVM  In[24]:  best parameters  print performance  print performance  print performance  In[25]:  TRAIN ON WHOLE DAATA AND PREDICT ON TEST  print performance  In[ ]:  lightgbm  In[26]:  muner of rounds  LGB parameters  In[27]:  print performance  print performance  print performance  In[ ]:  TRAIN ON WHOLE DAATA AND PREDICT ON TEST  print performance  In[ ]:,192
Experiment_Sampling,Experiment_Sampling,"lightgbm, XGBoost, feature engineering, model encoder, AUC, credit card default, feature selection","Cao Yi, Chen Chi, Ma Haochun, Xu Xiaoxuan, Ji Xiongzhang",Experiment_Sampling,4,!/usr/bin/env python  coding: utf-8  In[10]:  pip install missingno  In[38]: pandas numpy missingno sklearn.model_selection train_test_split sklearn.linear_model LogisticRegression sklearn.metrics classification_report sklearn.metrics roc_curve sklearn.preprocessing scale sklearn.preprocessing LabelEncoder sklearn.impute SimpleImputer  In[39]:  In[40]:  replace infinity with np.nan  In[41]: missing_values_table df  总缺失值  缺失值比例  缺失值制成表格  缺失值比例列由大到小排序  打印缺失值信息  In[42]:  In[43]:  In[44]: 实例化，填充常数0，填充常数需strategy与fill_value一同使用 fit_transform一步完成调取结果 填充好的数据传回到 data['Age']列      data[col].isnull ).sum )  In[45]:  In[47]:,52
Experiment_Sampling,Experiment_Sampling,"lightgbm, XGBoost, feature engineering, model encoder, AUC, credit card default, feature selection","Cao Yi, Chen Chi, Ma Haochun, Xu Xiaoxuan, Ji Xiongzhang",Experiment_Sampling,4,"!/usr/bin/env python  coding: utf-8  # Motivation  In[182]:  为什么会有针对不平衡数据的研究? 当我们的样本数据中  正负样本的数据占比极其不均衡的时候  模型的效果就会偏向于多数类的结果.  In[183]:  pip install imblearn  In[184]:  pip install --upgrade scikit-learn  In[185]: pandas imblearn.over_sampling SMOTE imblearn.under_sampling NearMiss imblearn.combine SMOTEENN imblearn.combine SMOTETomek imblearn.under_sampling RandomUnderSampler sklearn.model_selection train_test_split sklearn.linear_model LogisticRegression sklearn.metrics classification_report sklearn.metrics roc_curve sklearn.preprocessing scale sklearn.preprocessing LabelEncoder  In[186]:  In[187]:  可以替换成其他填充好的数据，要求无缺失值和极限值  In[188]:  # Creating balanced training sets  In[189]:  # Oversampling  In[190]: sklearn.datasets make_classification collections Counter imblearn.over_sampling RandomOverSampler  In[191]:  method 1: random over sampling  In[192]:  method 2: ADASYN - 关注的是在那些基于K最近邻分类器被错误分类的原始样本附近生成新的少数类样本  In[193]:  method 3: SMOTE - 对于少数类样本a  随机选择一个最近邻的样本b  然后从a与b的连线上随机选取一个点c作为新的少数类样本;  # Undersampling  In[194]:  Method 1: Random Under Sampler  In[195]:  # Combine  In[196]:  # 在之前的SMOTE方法中  当由边界的样本与其他样本进行过采样差值时  很容易生成一些噪音数据。  因此  在过采样之后需要对样本进行清洗  TomekLink 与 EditedNearestNeighbours方法就能实现上述的要求  # Creating a balanced test set  In[197]:  In[198]:  In[199]:  In[200]:  resampled training set  train_resampled_nm1 = pd.concat [y_resampled_nm1  X_resampled_nm1]  axis=1)  resampled test set  In[201]:  # save to csv  train_resampled_ros.to_csv 'train_resampled_ros.csv')  train_resampled_adasyn.to_csv 'train_resampled_adasyn.csv')  train_resampled_smote.to_csv 'train_resampled_smote.csv')  train_resampled_rus.to_csv 'train_resampled_rus.csv')  train_resampled_nm1.to_csv 'train_resampled_nm1.csv')  train_resampled_smotetomek.to_csv 'train_resampled_smotetomek.csv')  test_resampled.to_csv 'test_resampled.csv')  In[202]:  df_test is never changed  In[203]: gen_user df  # 在平衡样本集上训练，在非平衡样本集（真实样本）上做预测  1. Oversampling  ## train_resampled_ros  In[204]: pandas sklearn.model_selection train_test_split lightgbm xgboost hyperopt fmin sklearn.preprocessing LabelEncoder gc matplotlib seaborn functools partial pprint pprint numpy hyperopt.pyll scope plotly express plotly graph_objects plotly offline sklearn.metrics make_scorer sklearn.model_selection cross_val_score sklearn.utils check_random_state  In[205]:  这里只要更换gen_user里面的参数就行了，不用read_csv，直接用上一步的计算结果  In[206]:  In[207]:  计算ROC的值 svm_threasholds为阈值  plt.savefig ""baseline_roc.png"")  In[208]:  plt.title 'LightGBM Features  avg over folds)')  plt.savefig 'lgbm_importances.png')  其他的对于train的欠采样使用方式一样  ## train_resampled_adasyn  In[209]:  In[210]:  In[211]:  计算ROC的值 svm_threasholds为阈值  plt.savefig ""baseline_roc.png"")  In[212]:  plt.title 'LightGBM Features  avg over folds)')  plt.savefig 'lgbm_importances.png')  ## train_resampled_smote  In[213]:  In[214]:  In[215]:  计算ROC的值 svm_threasholds为阈值  plt.savefig ""baseline_roc.png"")  In[216]:  plt.title 'LightGBM Features  avg over folds)')  plt.savefig 'lgbm_importances.png')  2. Undersampling  ## train_resampled_rus  In[217]:  In[218]:  In[219]:  计算ROC的值 svm_threasholds为阈值  plt.savefig ""baseline_roc.png"")  In[220]:  plt.title 'LightGBM Features  avg over folds)')  plt.savefig 'lgbm_importances.png')  ## train_resampled_smotetomek  In[221]:  In[222]:  In[223]:  计算ROC的值 svm_threasholds为阈值  plt.savefig ""baseline_roc.png"")  In[224]:  plt.title 'LightGBM Features  avg over folds)')  plt.savefig 'lgbm_importances.png')  # test resampled  In[225]:  这一步更换test set  In[226]:  ### 首先使用全特征  In[237]:  train数据集可以任意定义，这里使用全特征  In[238]:  In[239]:  test y 已经定义过了  计算ROC的值 svm_threasholds为阈值  plt.savefig ""baseline_roc.png"")  In[240]:  plt.title 'LightGBM Features  avg over folds)')  plt.savefig 'lgbm_importances.png')  In[231]:  尝试使用合成数据在平衡样本上测试  In[241]:  In[242]:  In[243]:  test y 已经定义过了  计算ROC的值 svm_threasholds为阈值  plt.savefig ""baseline_roc.png"")  In[244]:  plt.title 'LightGBM Features  avg over folds)')  plt.savefig 'lgbm_importances.png')",333
SDA_2021_St_Gallen_Prediction_of_BTC_energy_consumption,"This Quantlet uses various statistical techniques to analyze data related to BTC price, transactions, sentiment, and energy demand to forecast the future energy demand of the BTC network.","BTC, Bitcoin, energy, ARIMA, VAR, LSTM, sentiment analysis","Aleksandra Bakhareva, Mario Blauensteiner, Paul Kilian Kreutzer, Tim Ludwig Leonard Matheis",SDA_2021_St_Gallen_Prediction_of_BTC_energy_consumption,15,"!/usr/bin/env python  coding: utf-8  In[1]: pandas seaborn numpy matplotlib  # Energy consumption  In[2]:  In[3]:  In[4]:  check if data frame is complete  In[23]: matplotlib rcParams  In[30]:  plt.rcParams[""figure.figsize""] = [7.00  3.50]   plt.xticks rotation=15)  # Bitcoin prices  In[31]:  In[32]:  check if data frame is complete  In[33]:  interpolate  In[34]:  plt.xticks rotation=15)  In[36]:  plt.xticks rotation=15)  # S&P index  In[47]:  In[48]:  In[50]:  plt.xticks rotation=15)  # Fear and greed index  In[38]:  In[39]:  In[40]:  plt.xticks rotation=15)  # Electricity prices  In[43]:  In[44]:  In[45]:  plt.xticks rotation=15)  # Merge different data sources  In[86]:  In[87]:  In[88]:  In[90]:  save to data folder",89
SDA_2021_St_Gallen_Prediction_of_BTC_energy_consumption,"This Quantlet uses various statistical techniques to analyze data related to BTC price, transactions, sentiment, and energy demand to forecast the future energy demand of the BTC network.","BTC, Bitcoin, energy, ARIMA, VAR, LSTM, sentiment analysis","Aleksandra Bakhareva, Mario Blauensteiner, Paul Kilian Kreutzer, Tim Ludwig Leonard Matheis",SDA_2021_St_Gallen_Prediction_of_BTC_energy_consumption,15, Functions and tools for manually computing a sentiment index gensim pandas sent_to_words sentences flatten_list lists load_pos_neg_words directory  pos_neg_to_exclude  pos_to_include  neg_to_include get_sentiment sentence  positive_words  negative_words get_sentiment_distribution words  vocab_directory  pos_neg_to_exclude  pos_to_include  neg_to_include  formula p.7 paper Ardina 2020)  if no sentiment words are found set sentiment to neutral,45
SDA_2021_St_Gallen_Prediction_of_BTC_energy_consumption,"This Quantlet uses various statistical techniques to analyze data related to BTC price, transactions, sentiment, and energy demand to forecast the future energy demand of the BTC network.","BTC, Bitcoin, energy, ARIMA, VAR, LSTM, sentiment analysis","Aleksandra Bakhareva, Mario Blauensteiner, Paul Kilian Kreutzer, Tim Ludwig Leonard Matheis",SDA_2021_St_Gallen_Prediction_of_BTC_energy_consumption,15,"!/usr/bin/env python  coding: utf-8  # Load the previously crawled data and compute a sentiment per tweet based on the pretrained model Vader. Then aggregate the sentiment of the daily tweets to obtain a sentiment index.   In[ ]: pandas numpy re pandas.core.frame DataFrame vaderSentiment.vaderSentiment SentimentIntensityAnalyzer  ### The data set with the tweets that is read in the following cell is not on github due to its size  multiple GBs).  ### Just use the crawler to obtain a dataframe with tweets yourself.    In[ ]: create a csv reader create a dataframe and check the function of chunksize later  In[40]:  In[41]: chunksize not working clean the dataframe  drop the row where the ""text"" column is NaN  In[42]: We discovered tweet in row no.1489542 is from CryptoBot  try to drop all the tweets that are from crytobot   In[43]: drop columns=['user_name'  'user_created' 'user_verified' 'hashtags' 'source' is_retweet']   In[44]:  In[45]:  In[46]: processTweet tweet start process_tweet  process the tweets Convert to lower case Convert www.* or https?://* to URL Convert @username to AT_USER Remove additional white spaces Replace #word with word  [^\s]+)'  r'\1'  tweet)#trim end Read the tweets one by one and process it  In[47]: getCoumpoundScore Tweets Read the tweets one by one and get the sentiment score store the tweet with polarity score in a dictionary transfer into a dataframe  In[48]:  In[49]: transfer date's string type into date time in order to do time dynamic analysis Error message:'Unknown string format:'  ""['ETH'  'BTC'  'Bitcoin']""  also NaN is contained in date So clean the date column only two are detected and erased  In[50]: delete value as ""NaT"" in date  In[51]:  In[52]:  In[53]:  In[54]:  In[55]:  In[56]:  In[57]:  In[6]:  load data if necessary pandas  In[7]:  In[8]:  In[9]:  In[10]: clean_df = clean_df.set_index ""date"")  In[25]:  In[11]:  In[17]: matplotlib  In[28]:  In[13]: seaborn matplotlib rcParams  In[14]:  In[3]:  In[5]:  In[6]:  In[7]:  In[13]:",296
SDA_2021_St_Gallen_Prediction_of_BTC_energy_consumption,"This Quantlet uses various statistical techniques to analyze data related to BTC price, transactions, sentiment, and energy demand to forecast the future energy demand of the BTC network.","BTC, Bitcoin, energy, ARIMA, VAR, LSTM, sentiment analysis","Aleksandra Bakhareva, Mario Blauensteiner, Paul Kilian Kreutzer, Tim Ludwig Leonard Matheis",SDA_2021_St_Gallen_Prediction_of_BTC_energy_consumption,15,!/usr/bin/env python  coding: utf-8  In[ ]: connect colab file with drive google.colab drive  In[ ]: import modules and packages numpy pandas matplotlib datetime datetime datetime keras.callbacks EarlyStopping sklearn.preprocessing StandardScaler keras.models Sequential keras.layers Dense keras.layers LSTM keras.layers Dropout tensorflow.keras.optimizers Adam sklearn.metrics mean_squared_error  In[ ]: load data  In[ ]: check missing values  In[ ]: select features get dates  necessary for visualization)  In[ ]: preprocess date format  In[ ]: get true values  In[ ]: transform training data  In[ ]: scale features  In[ ]: create data structure with 90 timestamps and 1 output days we want to predict days we use to predict  In[ ]: initialize LSTM model add first LSTM layer add second LSTM layer add Dropout output layer compile Neural Network  In[ ]:  In[ ]: train the model  In[ ]:  In[ ]: keras.utils.vis_utils plot_model to_file='model_plot.png'  In[ ]: generate list for prediction convert dates  In[ ]: perform predictions  In[ ]: adjust data for visualisation datetime_to_timestamp x  In[ ]: seaborn pylab rcParams  In[ ]: rmse,161
SDA_2021_St_Gallen_Prediction_of_BTC_energy_consumption,"This Quantlet uses various statistical techniques to analyze data related to BTC price, transactions, sentiment, and energy demand to forecast the future energy demand of the BTC network.","BTC, Bitcoin, energy, ARIMA, VAR, LSTM, sentiment analysis","Aleksandra Bakhareva, Mario Blauensteiner, Paul Kilian Kreutzer, Tim Ludwig Leonard Matheis",SDA_2021_St_Gallen_Prediction_of_BTC_energy_consumption,15,"!/usr/bin/env python  coding: utf-8  In[1]: packages os pandas seaborn numpy matplotlib  In[2]: Change working directory  In[3]:  In[4]:  In[5]:  In[6]:  In[7]:  In[10]:  plt.rcParams[""figure.figsize""] = [7.00  3.50]   In[11]: Determining rolling statistics Rolling mean Rolling standard deviation Plot rolling statistics:  In[12]: Determining rolling statistics Rolling mean Rolling standard deviation Plot rolling statistics:  In[13]:  In[14]:  In[15]: Determining rolling statistics of differentiated data Rolling mean Rolling standard deviation Plot rolling statistics:  In[16]: Determining rolling statistics of differentiated data Rolling mean Rolling standard deviation Plot rolling statistics:  In[27]:  In[28]:  In[25]: Plotting historical BTC price and network power demand on the same chart   In[ ]:  In[ ]:  In[ ]:",102
SDA_2021_St_Gallen_Prediction_of_BTC_energy_consumption,"This Quantlet uses various statistical techniques to analyze data related to BTC price, transactions, sentiment, and energy demand to forecast the future energy demand of the BTC network.","BTC, Bitcoin, energy, ARIMA, VAR, LSTM, sentiment analysis","Aleksandra Bakhareva, Mario Blauensteiner, Paul Kilian Kreutzer, Tim Ludwig Leonard Matheis",SDA_2021_St_Gallen_Prediction_of_BTC_energy_consumption,15,"!/usr/bin/env python  coding: utf-8  In[ ]: tweepy  3.10 - THERE IS A NEW TWEEPY VERSION! --> many changes etc. textblob TextBlob csv time os  In[ ]: print ""File deleted"" print ""File was not present already"" Use csv writer Query q) ---> AND  surge OR crash OR plunge OR high OR low OR future OR amazing OR good OR bad OR record) most recent data is fetched first collecting tweets made by users with min 100k followers i+=1  Write a row to the CSV file. I use encode UTF-8 print ""------wrote a tweet-----"") print ""---------------------In sleep In sleep In sleep In sleep----------------------------"") print ""---------------------Something is wrong----------------------------"")",105
SDA_2021_St_Gallen_Prediction_of_BTC_energy_consumption,"This Quantlet uses various statistical techniques to analyze data related to BTC price, transactions, sentiment, and energy demand to forecast the future energy demand of the BTC network.","BTC, Bitcoin, energy, ARIMA, VAR, LSTM, sentiment analysis","Aleksandra Bakhareva, Mario Blauensteiner, Paul Kilian Kreutzer, Tim Ludwig Leonard Matheis",SDA_2021_St_Gallen_Prediction_of_BTC_energy_consumption,15,"!/usr/bin/env python  coding: utf-8  In[1]: os Change working directory  In[2]: packages pandas numpy seaborn matplotlib statsmodels.graphics.tsaplots plot_acf statsmodels.tsa.statespace.varmax VARMAX statsmodels.tsa.api VAR statsmodels.tsa.stattools grangercausalitytests tqdm tqdm_notebook itertools product matplotlib statsmodels sklearn.metrics mean_squared_error math statistics mean warnings  In[7]:  In[8]:  In[47]: Dickey–Fuller test. Data is stationary.  In[48]: Granger Causality test. If p-value < 0.05 the hypothesis is true.   Energy GW causes BTC Price in lags 3 and 4. BTC Price causes Energy GW.  In[49]: Granger Causality test. If p-value < 0.05 the hypothesis is true.  Energy GW causes BTC Volume. BTC Volume does not cause Energy GW.  In[50]: Granger Causality test. If p-value < 0.05 the hypothesis is true.  Energy GW causes Fear Index in lag 2. Fear Index causes Energy GW.  In[51]: Granger Causality test. If p-value < 0.05 the hypothesis is true.  Energy GW does not cause S&P. S&P causes Energy GW.  In[52]: Granger Causality test. If p-value < 0.05 the hypothesis is true.  Energy GW causes Electricity Price NY in lags 1 and 2. Electricity Price NY does not cause Energy GW.  In[193]:  In[194]:  In[195]:  In[196]: * shows where is the minimum parameter. The minimum BIC was found at lag number 14.  In[695]:  In[696]:  In[697]: print fitted_model1.summary ))  In[698]: start=""2021-10-29"" end=""2021-11-27""  In[699]:  In[700]: print predictions1)  In[701]:  In[702]:  In[703]:  In[100]:  In[101]:  In[102]:  In[103]: * shows where is the minimum parameter. The minimum BIC was found at lag number 14.  In[622]:  In[623]:  In[624]: print fitted_model2.summary ))  In[625]: start=""2021-10-29"" end=""2021-11-27"")  In[626]:  In[627]: print predictions)  In[628]:  In[629]:  In[630]:  In[114]:  In[115]:  In[116]:  In[183]: * shows where is the minimum parameter. The minimum AIC was found at lag number 17.  In[425]:  In[426]:  In[427]: print fitted_model3.summary ))  In[428]: start=""2021-10-29"" end=""2021-11-27"")  In[429]:  In[430]: print predictions)  In[431]:  In[432]:  In[434]:  In[ ]:  In[135]:  In[136]:  In[137]:  In[138]: * shows where is the minimum parameter. The minimum AIC was found at lag number 15.  In[526]:  In[527]:  In[528]: print fitted_model4.summary ))  In[529]: start=""2021-10-29"" end=""2021-11-27"")  In[530]:  In[531]: print predictions)  In[532]:  In[603]:  In[534]:  In[238]:  In[222]:  In[223]:  In[224]: * shows where is the minimum parameter. The minimum BIC was found at lag number 7.  In[567]:  In[568]:  In[552]: print fitted_model5.summary ))  In[569]: start=""2021-10-29"" end=""2021-11-27"")  In[570]:  In[571]: print predictions)  In[572]:  In[573]:  In[574]:  In[302]: start=""2021-11-28"" end=""2021-12-27"")  In[303]:  In[304]: predictions5_future.index=df_merged.index[start:end+30+1] print predictions)  In[305]:  In[306]:  In[ ]:  In[ ]:  In[ ]:  In[715]:  In[716]:  In[717]:  In[718]: * shows where is the minimum parameter. The minimum AIC was found at lag number 15.  In[719]:  In[720]:  In[721]: print fitted_model6.summary ))  In[729]: start=""2021-10-29"" end=""2021-11-27"")  In[730]:  In[731]: print predictions)  In[732]:  In[733]:  In[747]:  In[734]:  In[735]:  In[736]:  In[ ]:  In[758]:  In[759]:  In[751]:  In[752]:  In[753]:  In[754]:  In[755]:  In[ ]:  In[ ]:  In[ ]:  In[ ]:  In[309]:  In[310]:  In[ ]:  In[313]:  In[ ]:  In[ ]:  In[ ]:",437
SDA_2021_St_Gallen_Prediction_of_BTC_energy_consumption,"This Quantlet uses various statistical techniques to analyze data related to BTC price, transactions, sentiment, and energy demand to forecast the future energy demand of the BTC network.","BTC, Bitcoin, energy, ARIMA, VAR, LSTM, sentiment analysis","Aleksandra Bakhareva, Mario Blauensteiner, Paul Kilian Kreutzer, Tim Ludwig Leonard Matheis",SDA_2021_St_Gallen_Prediction_of_BTC_energy_consumption,15,!/usr/bin/env python  coding: utf-8  In[1]: pmdarima auto_arima numpy pandas statsmodels.tsa.arima_model ARIMA statsmodels.tsa.stattools adfuller sklearn.metrics mean_squared_error math sqrt arch arch_model matplotlib pyplot statsmodels.graphics.tsaplots plot_acf statsmodels.graphics.tsaplots plot_pacf  In[2]: warnings  In[4]:  In[5]:  In[54]:  In[55]:  df = df.reset_index )  In[56]: matplotlib rcParams  In[57]: matplotlib seaborn  plt.xticks rotation=15)  In[58]:  differentiated series is stationary  dickey fuller)  In[59]:  In[60]:  In[61]:  In[62]: print pred)  In[63]:  In[69]:  In[89]:  In[62]:  In[64]:  In[66]:  In[67]: print index_future_dates) print comp_pred)  In[68]:  In[121]:  create acf plot  In[13]:  plt.savefig 'partial_autocorrelation.png'  dpi=400  transparent=True)  In[108]:  In[122]:  forecast the test set  plot the actual variance  ...  In[118]:  plot forecast variance,92
Window_Size,Select the optimal window-size for the training of the LSTM,"window_size, optimal, LSTM","Georg Velev, Iliyana Pekova",Window_Size,1,numpy  Pass an numpy array and generate the subsets: generate_Xtrain_yTrain_Xtest_Ytest data train_size_percentage timesteps_in timesteps_out  We don't want to set the window size too big as this would increase the computational cost enormously  However the window size should not be set too small as the LSTM may then never reach the global minimumof the error function  test the LSTM with these four values:,62
CSC_Dapp_scraping,scraping Dapp contract hashes using API,ERROR,"Elizaveta Zinovyeva, Raphael Constantin Georg Reule",CSC_Dapp_scraping,2,!/usr/bin/env python  coding: utf-8  In[36]: pandas requests re numpy ast time  ### We need to get the list of all hashes first  In[22]:  In[23]:  In[21]:  we have the maximum of 18 pages  if the limit set to 100 apps per page  In[51]:  In[52]: tqdm tqdm  In[53]:  In[59]:  In[60]:  ### FOr each dapp  here or each slug  retrieve a list of hashes  In[62]:  In[66]:  In[129]:  In[130]:  In[134]:  In[135]:  In[138]:  In[141]:,69
RL_Experiment1Leverage,Outputs the leverage for each time period as a plot.,"reinforcement learning, neural network, machine learning, portfolio management, cryptocurrency",Ilyas Agakishiev,RL_Experiment1Leverage,1,pandas datetime datetime matplotlib matplotlib.dates DateFormatter,6
BTCtuning,Tuning of different MLP architecture for BTC trend predictions,"neural networks, MLP, deep learning, Bitcoin, cryptocurrency, prediction, time series, stock price forecast",SPILAK Bruno,BTCtuning,1,os pandas numpy  fix random seed for reproducibility time time keras.layers Dense keras backend keras.wrappers.scikit_learn KerasClassifier sklearn.preprocessing StandardScaler sklearn.metrics classification_report sklearn.model_selection GridSearchCV keras.models Sequential sklearn.model_selection TimeSeriesSplit  Input parameters look back window forecast window labeler2D x series_to_supervised_multi data  n_in=1  n_days=1  dropnan=True):  #index corresponds to  t  input sequence  t-n  ... t-1)  drop rows with NaN values create_table var  data  exogene log returns baseline_model  model.add Dense 5  kernel_initializer='uniform'  activation = 'tanh')) sklearn.model_selection cross_val_score create_model_two_layers neurons1=1  neurons2=1 create_model_three_layers neurons1=1  neurons2=1  neurons3=1 create_model_four_layers neurons1=1  neurons2=1  neurons3=1  neurons4=1 ############################################ ################# Data ##################### ############################################ Preprocessing     unscaling Baseline crossvalidation  Two layers  define the grid search parameters Define CV  summarize results Three layers  define the grid search parameters Define CV  summarize results  Four layers  define the grid search parameters Define CV  summarize results 0.555128 using {'neurons1': 50  'neurons2': 15  'neurons3': 15  'neurons4': 15}  10 layers  define the grid search parameters Define CV  summarize results,145
MLP,Use machines learning algorithms to predict daily price move in Bitcoin and perform day trading,"Bitcoin, SVM, Random Forest, Neural Network, LSTM","Hong Yuxi, Lin Jinyu, Sun Rongsheng, Zhou Xiaoqian, Zou Yutong",MLP,2,!/usr/bin/env python  coding: utf-8  In[1]: pandas matplotlib numpy matplotlib.dates DateFormatter sklearn.preprocessing MinMaxScaler datetime datetime sklearn.metrics mean_squared_error sklearn.neural_network MLPRegressor os os.path dirname  In[2]:  In[23]:  In[24]:  # Scale data  In[25]:  In[26]:  In[27]:  In[60]:  # Split the dataset into training data and testing data  In[6]: create historical data create_dataset dataset  look_back=3  In[29]:  # Train the model  In[36]:  In[35]:  In[41]:  In[46]:  In[47]:  # Trading   In[50]:  In[64]:  In[58]:  In[51]:  In[53]:  In[61]:  In[62]:  In[63]:  In[65]:  In[67]:  In[69]:  In[79]:  In[74]: plot figure  In[75]: 10Y bond yield  In[ ]:,81
pyTSA_AirTempChange,"This Quantlet simulates ARMA(2,2) - autoregressive moving average process and draws the true ACF and PACF","time series,  stationarity, autocorrelation, PACF, ACF, simulation, stochastic process, ARMA, moving average, autoregression",ERROR,pyTSA_Seasonality,1,numpy matplotlib pandas statsmodels PythonTsa.plot_acf_pacf acf_pacf_fig statsmodels.tsa.arima_process arma_generate_sample statsmodels.graphics.tsaplots quarter_plot,10
RL_WeightDistribution,Outputs a plot that displays the weight (in %) for the 5 least held coins for different target drawdowns in Experiment 1. A higher value means better diversification.,"reinforcement learning, neural network, machine learning, portfolio management, cryptocurrency",Ilyas Agakishiev,RL_WeightDistribution,1,pandas numpy matplotlib,3
SFM_Gen_Pareto,"SFM_Gen_Pareto simulates Generalized Pareto Distributions: Pareto, Exponential, Pareto II.","Pareto, Exponential, Pareto II",Daniel Traian Pele,SFM_Gen_Pareto,2,!/usr/bin/env python  coding: utf-8  In[6]: scipy.stats genpareto numpy matplotlib  In[ ]:  In[ ]:,13
SC_literature_research,"This Quantlet is dedicated to topic modeling on the existing research on Ethereum Smart Contracts. First, we encode our abstracts with Distilbert embeddings and using UMAP we reduce dimensionality reduction to perform K-Means, finally again using UMAP reduce dimensionality to 2, to be able to plot it.","Scopus, literature research, DistilBERT, topic modelling, clustering, UMAP, K-Means","Elizaveta Zinovyeva, Raphael Constantin Georg Reule",SC-literature-research,4,"!/usr/bin/env python  coding: utf-8  In[14]: numpy pandas nltk.tokenize word_tokenize nltk.corpus stopwords nltk.stem WordNetLemmatizer string punctuation collections Counter collections OrderedDict matplotlib wordcloud WordCloud  In[15]:  data loaded on 13. October 2020  correct year  conference 2019)  In[16]: remove ethereum  smart contract  smart contracts  blockchain  In[17]:  In[18]: keras.preprocessing.text Tokenizer keras.preprocessing.sequence pad_sequences  text preprocessing  max 14  In[19]:  In[20]:  Join the different processed titles together.  Create a WordCloud object  Generate a word cloud  Visualize the word cloud %matplotlib inline plt.savefig ""/gdrive/My Drive/Colab Notebooks/IRTG/Smart_contracts_paper/Literature Research/wordcloud.png"")  ### BERT EMBEDDINGS  In[20]:",82
SC_literature_research,"This Quantlet is dedicated to topic modeling on the existing research on Ethereum Smart Contracts. First, we encode our abstracts with Distilbert embeddings and using UMAP we reduce dimensionality reduction to perform K-Means, finally again using UMAP reduce dimensionality to 2, to be able to plot it.","Scopus, literature research, DistilBERT, topic modelling, clustering, UMAP, K-Means","Elizaveta Zinovyeva, Raphael Constantin Georg Reule",SC-literature-research,4,"!/usr/bin/env python  coding: utf-8  In[1]:  In[2]: numpy pandas nltk.tokenize word_tokenize nltk.corpus stopwords nltk.stem WordNetLemmatizer string punctuation collections Counter collections OrderedDict matplotlib sklearn.cluster KMeans sklearn.metrics silhouette_score sklearn.mixture GaussianMixture sentence_transformers SentenceTransformer umap matplotlib re seaborn sklearn.feature_extraction.text CountVectorizer  In[3]:  In[4]:  data loaded on 13. October 2020  correct year  conference 2019)  In[5]: remove ethereum  smart contract  smart contracts  blockchain  Code based on the blogpost https://towardsdatascience.com/topic-modeling-with-bert-779f7db187e6    In[6]:  In[7]:  In[8]:  In[9]:  In[10]: plt.title ""The Silhouette coefficient method \nfor determining number of clusters\n"" fontsize=16) plt.show )  In[11]: plt.title ""The Davies-Bouldin score \nfor determining number of clusters\n""  fontsize=16) plt.show )  In[12]:  In[13]: 3174a1'  e1812c'  3a923a'  c03d3d'  9372b2'  845b53'  d684bd'  7f7f7f'  a9aa35'  2eaab8'  465E81'  807E07'  E8BD1B'  E81B53'}  7     105  11     97  1      95  6      77  0      76  8      72  2      66  3      52  5      49  4      44  13     36  12     29  10     26  9      15  In[14]:  Prepare data  Visualize clusters  In[15]:  In[16]: os  In[17]:  In[18]: c_tf_idf documents  m  ngram_range= 1  1)  In[19]: extract_top_n_words_per_topic tf_idf  count  docs_per_topic  n=10  In[20]:  Set y-axis label  In[21]:  In[21]:",164
EtherNet_Visualization,Smart Contracts Network Parameters Visualization,"hash, algorithm, crix, cryptocurrency, bitcoin, neural network, trading, fintech, Ethereum","Raphael Constantin Georg Reule, Elizaveta Zinovyeva, Marvin Gauer",SC_Network,2,"!/usr/bin/env python  coding: utf-8  <a href=""https://colab.research.google.com/github/HektorLin/Etherscan_visualization/blob/master/Etherscan_Visualization.ipynb"" target=""_parent""><img src=""https://colab.research.google.com/assets/colab-badge.svg"" alt=""Open In Colab""/></a>  The objective of this exercise is to show various plotting techniques  using an Ethereum dataset.  In[1]: pandas numpy datetime seaborn matplotlib matplotlib gridspec matplotlib.dates AutoDateFormatter matplotlib mpl_toolkits.mplot3d Axes3D matplotlib matplotlib cm matplotlib.colors LinearSegmentedColormap matplotlib sklearn.preprocessing MinMaxScaler  # Univariate Plots    Data for all plots in this section is the Ethereum network utilization rate time-series downloaded from https://etherscan.io/chart/networkutilization.    Note that except for heatmap  where a 2D matrix is needed  all the other plots require only a single time-series  or a time-series truncated at a certain time interval.  In[2]:  load the dataset.  Replication can easily be done by replacing the dataset.  re-format the column names to non-unique  as the axis labels can always be renamed later  depending on the variable of interest  and from the date  separate year  month and week  visualize the data  ## Set global SNS tick and label size  In[6]:  ## Time-Series Line Charts  In[122]:  time series with moving average  30 days)  In[159]:  load the dataset.  Replication can easily be done by replacing the dataset.  re-format the column names to non-unique  as the axis labels can always be renamed later  depending on the variable of interest  and from the date  separate year  month and week  time series plot  every year this sets the x ticks at year level  ## Heatmap    In[124]:  create a customized colormap  using RGB values  from light to blue  In[125]:  To plot with a heatmap  the underlying data must be re-organizaed to a 2D matrix  therefore  I aggregate the data to be mapped by year to their mean values  In[126]:  heatmap with customized colormap  ## Boxplots  In[127]:  with sns.boxplot  it will show the density of the variable x  after being partitioned according to the value of y.  note the color arguement here determine the color of the space within the boxes  iterate over the boxes to color the boxes outlines in black  the default color is grey)  codes from https://stackoverflow.com/questions/43434020/black-and-white-boxplots-in-seaborn  iterate over whiskers and median lines  ## Violin plots  In[128]:  the main arguements of violin plots are analogy to that of boxplots  adjust the ""width"" can alter the fatness of the violine.  In[129]:  horizontal violin  In[130]: Draw swarms of observations on top of a violin plot:  https://seaborn.pydata.org/generated/seaborn.swarmplot.html)  ## Density Plots    In[131]:  Empirical Cumulative Density Function  ECDF)  In[132]:  overlaid density functions  plot the density for each year  ## Partial Autocorrelations  In[133]:  partial auto correlation at daily statsmodels.graphics.tsaplots plot_pacf  # Multivariate Plots    The data for this section is constructed by combining 7 time-series from https://etherscan.io/charts  which are:    - Verified Contracts  https://etherscan.io/chart/verified-contracts)  - Network Utilization  https://etherscan.io/chart/networkutilization)  - Gas Used  https://etherscan.io/chart/gasused)  - Gas Limited  https://etherscan.io/chart/gaslimit)  - Ethereum Daily Transactions  https://etherscan.io/chart/tx)  - ERC20 Daily Token Transfer  https://etherscan.io/chart/tokenerc-20txns)  - Ethereum Unique Addresses  https://etherscan.io/chart/address)    However  this dataset is essentially a collection of multiple time-series  which is only truly a multivariate dataset if assuming every daily observations are independent from each other. But as the main purpose is to demostrate plotting technique  this dataset will be used.    In[3]:  load the dataset  don't need the date as UnixTimeStamp is given  construct a main df by inner merge on date  i.e. observations with missing values on any date are dropped)  ## output as a Latex table  In[8]:  export first 10 rows as table to latex  ## multiple time-series  In[18]:  multiple time series  Because there are 7 times series  I have to manually draw the frames to achieve better alignment.  plot in every subplot a time series  In[24]:  Individual plots from above      ax.plot df[""Date""] df.iloc[: i+1])  In[7]:  load the dataset.  Replication can easily be done by replacing the dataset.  re-format the column names to non-unique  as the axis labels can always be renamed later  depending on the variable of interest  and from the date  separate year  month and week  time series plot  every day this sets the x ticks at the day level  In[8]:  load the dataset.  Replication can easily be done by replacing the dataset.  re-format the column names to non-unique  as the axis labels can always be renamed later  depending on the variable of interest  and from the date  separate year  month and week  time series plot  every year this sets the x ticks at year level  ## Bar plots  In[23]:  tilt the x axis labels  fix the cut off labels  ## Correlation Matrix and Correlation Heatmap  In[111]:  correlation matrix  maybe the correlation should be run on transformed variables  i.e. de-trended or first difference)  In[112]:  As in the step above  we already have a 2D correlation matrix. We can directly plot it in a heatmap.  create custom colormap  from light red to deep red  create a mask same dimention as the corr matrix  but with 1 on the diagonal and 0 everywhere else.   https://stackoverflow.com/questions/24475094/set-values-on-the-diagonal-of-pandas-dataframe  create the heatmap with custom color map and the mask on top  ## Radar plot  In[113]:  I aggregate the data by mean at month level  in hope of resulting a slightly more randomized dataset.  But this step does not matter for the plot in a technical perspective  As can be compared to the original dataset  the data now indexed by month instead of by day  which is the only difference.  In[114]:  In[115]:  In Radar plot  it is better to scale the data firstly for better visualization.  Here I do it with MinMax  In[116]:  In this step I define how many dimensions we want and what their names are.  the number of dimensions are transformed into an array that represent the angle between a pair of dimensions  modified from https://www.kaggle.com/typewind/draw-a-radar-chart-with-python-in-a-simple-way  labels=np.array df_grouped.columns)  In[117]:  we first draw a polar  then map the observations one by one plot all 12 months will be messy  so only four months The author had this line  but I don't see any difference  ## Density Contour Plot  In[118]: https://www.geeksforgeeks.org/kde-plot-visualization-with-pandas-and-seaborn/ https://stackoverflow.com/questions/49991657/add-labels-to-seaborn-bivariate-kde-plot changed from 2018 to 2017 Just a ratio  can be anything else I want to show the difference between shade_lowest=True and shade_lowest=False  it seems one have to add the color legend by oneself when ploting bivariate coutour.  it's only two so I did it raw without a loop  # 3D plots    In 3D plots  I use the same dataset as in multivariate plots  but condition on observations on or after 2018. The purpose is to reduce the size of the dataset to save a bit of resources.  In[51]:  I use the subset from 2018 onwards  In[52]:  In[53]:  In[48]:  surface plots ax.xaxis.set_major_locator ticker.MaxNLocator 3 integer=True)) fig.colorbar surf  shrink=0.5  aspect=5) #to include a color bar  In[49]:  multiple surface plots  In[15]:  open interactive window to enable rotation  3D scatter plots by groups and with variable sizes mpl_toolkits.mplot3d axes3d matplotlib  ax = fig.gca projection='3d')  ax = fig.add_subplot 111  projection='3d') ""s"" determines the size of the markers keep the names of the markers  rotate the axes and update  switch back to regular inline display of matplotlib plots  plt.show )",1126
LSTM,Final Project of SDA I Class: Modelling The Effect of Confirmed Number COVID-19 Ratio to The Exchange Rate of Euro to Rupiah. (You can change the data in python code to get the result of other models),ERROR,"Samingun Handoyo, Andreas Rony Wijaya",LSTM,1,"numpy matplotlib pandas math sqrt keras.regularizers l2 keras.models Sequential keras.layers Dense keras.layers LSTM sklearn.preprocessing MinMaxScaler sklearn.metrics mean_squared_error scipy.stats pearsonr define_model0 hyparam train_X You can change the data on this part to get the result of the other models colnames=[""MYR-1"" ""MYR-2"" ""MYR-3"" ""USD-1"" ""USD-2"" ""USD-3"" ""EUR-1"" ""EUR-2"" ""EUR-3"" ""EURO""] DF.columns=colnames  split into train and test sets  reshape input to be 3D [samples  timesteps  features] You can modify this value of hyperparameter lambda  plot history  make a prediction  invert scaling for forecast  invert scaling for actual  calculate RMSE model_performance A F",89
TickersIdentification,Algorithm that identify the S&P500 firm tickers from news text. The algorithm only identify those tickers in the three most commonly seen cases. Detailed description refer to the paper,"Textual Analysis, Entity Identification",Junjie Hu,Algo_TickersIdentify,1,"os pandas datetime re nltk tokenize concurrent time pickle  Symbols Extraction settings  Define the marco for regex  'CEO' in spx_tickers.index.tolist )  Remove \n and \t  correct sentence separation  Remove disclaimer  clicking  and image source till the end of the sentence  remove click here till the end of the sentence  remove_clicklink_pattern = re.compile r' Click here|Image source).*[\.?!>]+'  re.IGNORECASE)  remove copy right info till the end of the string unlist list_in bracket_symbols_extract text  text = content_clean_pattern.sub repl="" ""  string=text)  bracket_matches = bracket_symbols_pattern.findall string=text)  Find symbols in the bracket lead by lead_keys  Find the symbols in the bracket clean_bracket_matches matched_list unbracket_ticker_matching text  tickers_bracket  ticker_len_mini=None  Patter:  ^|\s) AAPL|AA|A) \s|$)  Symbols isolated by space or at the end/start of the string  print unbracket_pattern)  DO NOT try to find tickers that are not in the brackets within content  too dangerous symbol_extracter headline  content  timestamp  author  Replace newline and tab with space  separate two attached sentences  connect two sentence  Delete disclaimer  content_text = remove_clicklink_pattern.sub repl=''  string=content_text)  print content_text)  text = disclaimer_pattern.sub repl=''  string=content)  Replace newline and tab with space  Replace newline and tab with space  Replace newline and tab with space  ==Sample TEST start:  headline_test_sample = [""Apple Inc.  FB) Unleashes New MacBook Pro With Touch Bar""                           ""Tuesday Apple Rumors: MSFT Working on iMessage Android Mockup""                           ""Fitbit Inc. Had A Merry Christmas  But Why Isn't Its Stock As Jolly?""]  correct_res = ['FB'  'AAPL'  '']  headline_rest_res = [leaders_symbols textitem) for textitem in headline_test_sample]  ==TEST END  Cut the content into sentences  Split into difference sentences  clean_content_text = '. '.join content_sentences)  sentence = content_sentences[0]  Tickers Symbol matching directly  headline_FullSymbMatch = short_tickers_pattern.findall headline_text)  content_FullSymbMatch =  headline_FullSymbMatch = re.findall pattern=short_tickers_pattern  string=headline_text)  content_FullSymbMatch = re.findall pattern=long_tickers_pattern  string=content_text)  Merge found tickers from the three algos leaders_symbols headline_text  Symbols in brackets  IF can not find tickers within brackets  Mapping company name segments to tickers  IF can not fine any tickers within bracket nor company name segments  Find the tickers that are not in the brackets  now only applied on headline followers_symbols sentence  remove_bracket_pattern = re.compile r'\ .*?\)')  segments_matched_res = segments_matching text=sentence  seg_pat_lst=seg_pat_lst)  ========== Multi-process  ======= Multi-process END  STORE THE RESULT  ==== save all columns  only tickers  only text  to csv",354
Regulatory_Complexity_Distances,Computes distances between all sentences in a given version of the GBA and aggregates them to a single complexity measure.,"Regulatory_Complexity, complexity-measure, word2vec, doc2vec, gensim, euclidean-distance, word-movers-distance",Sabine Bertram,Regulatory_Complexity_Distances,2,scipy.spatial distance avAggreg inputs tfidfAggreg inputs distances inputs newDistances inputs wmdPrep paragraph  frequency  stop wmdDist inputs getVectors inputs,18
Regulatory_Complexity_Distances,Computes distances between all sentences in a given version of the GBA and aggregates them to a single complexity measure.,"Regulatory_Complexity, complexity-measure, word2vec, doc2vec, gensim, euclidean-distance, word-movers-distance",Sabine Bertram,Regulatory_Complexity_Distances,2,!/usr/bin/python  -*- coding: utf-8 -*- ###############################################################################  Computes distances of sentences generated based on several techniques  1. Average aggregation of word2vec word vectors + euclidean distance  2. TF-IDF aggregation of word2vec word vectors + euclidean distance  3. Word Movers' Dinstances of sentences using word2vec word vectors  3. doc2vec sentence vectors + euclidean distance  input: - wordModel: the gensim word2vec model         - sentenceModel: the gensim doc2vec model         - completeVersions: a dictionary with key: date  value: list of sentences  output: folder 'output' with files called METHOD_METRIC.txt for each distance          calculation method and each metric  mean  sd  iqr) ###############################################################################  imports os gensim gensim corpora logging pickle multiprocessing multiprocessing math nltk.corpus stopwords collections defaultdict itertools scipy.spatial distance numpy warnings tqdm tqdm functions ###############################################################################  functions in functions.py ###############################################################################  main  load paragraphs with dates and sentences  prepare for multithreaded processing  create directory for output measures  load neccessary data  load gensim model  load sentences for dictionary  load gensim model  load stopwords for wmd  calculate distances  dispersion measures per version  save data,163

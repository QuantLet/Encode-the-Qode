{"version": "1.0", "data": [{"input_sequence": "Author: Cheng Tuoyuan, Wang Duyue, Wang Wenbo, Zheng Zhongyi ; from astropy.time.utils import split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nfrom sklearn.preprocessing import Binarizer\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import roc_auc_score as auc\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv('201902_IF1903.csv')\ntime = data.TDATETIME\ndata.drop(['IFCD', 'TDATETIME'], axis=1, inplace=True)\nprice = data.LASTPRICE\ndata = (data - data.mean()) / data.std()\n##### predict one day forward #####\n### train xgb model\n# train test split\nlen = data.shape[0]\ntest_len = len * 9 // 10\nX = data.iloc[:test_len, :]\nwindow = 10\npred_start = 100000\npred_end = pred_start + window\nx = X.iloc[:pred_start,:]\ny = price.loc[x.index].shift(window)\ny.fillna(method='bfill', inplace=True)\nX_train, X_test, y_train, y_test = train_test_split(x, y, shuffle=False, random_state=1, test_size=0.1)\nfrom xgboost import XGBRegressor\nmodel_xgb = XGBRegressor(\nsilent=0,\nlearning_rate=0.3,\nmin_child_weight=1,\nmax_depth=10, #\ngamma=10,\nsubsample=1,\nmax_delta_step=0,\ncolsample_bytree=0.5,\nreg_lambda=0.5,\nn_estimators=50,\nseed=1000\n)\nmodel_xgb.fit(X_train, y_train)\nprint(r2_score(y_test, model_xgb.predict(X_test)))\npred_xgb = model_xgb.predict(X_test.iloc[:10,:])\n### train neural network mlp model\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.python.keras.layers import Dense, Dropout\nimport tensorflow.keras.optimizers as optimizers\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.callbacks import EarlyStopping\ntf.random.set_seed(1)\noptimizer = tf.optimizers.Adam(lr=0.005)\nmodel = Sequential()\nmodel.add(Dense(30, input_dim=X_train.shape[1], activation=\"relu\"))\nmodel.add(Dense(1))\nmodel.compile(loss='mse', optimizer='adam')\nmodel.fit(x=X_train, y=y_train, validation_data=(X_test, y_test), batch_size=64, epochs=50,verbose=1)\nprint(r2_score(y_test, model.predict(X_test)))\npred_mlp = model.predict(X_test.iloc[:10,:])\nreal = y_test.iloc[:10]\nreal_before = y_train.iloc[-25:]\nplt.plot(np.concatenate((real_before.values, real.values)))\nplt.legend(['Real', 'XGBoost', 'NN-MLP'])\n", "output_sequence": "Using Xgboost and MLP to do high frequency prediction of stock index futures"}, {"input_sequence": "Author: Lining Yu ; # clear all variables\nrm(list = ls(all = TRUE))\ngraphics.off()\n\n# install and load packages\nlibraries = c(\"qgraph\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# set the working directory setwd('C:/...')\n# read firms' data\nf = read.csv(\"100_firms_returns_and_macro_2015-04-15.csv\")\n# read the connectedness matrix based on estimated paritial derivatives on\n# 12_06_2009 as an example\ncon = as.matrix(read.csv(\"totc_JPM_t_80.csv\")[, -1])\n# extract the date\ndt = as.Date(f[, 1], format = \"%d/%m/%Y\")[49:314]\n# read the returns of 100 firms, the seven macro variables are ruled out\nff = f[, 2:(ncol(f) - 7)]\n# read the firms' names\nnames.fi = colnames(f)[2:101]\n# divide the firms into four groups: Depositories, Insurers, Broker-Dealers,\n# Others.\ngroups = list(1:25, 26:50, 51:75, 76:100)\ncol = c(rep(\"red\", 25), rep(\"blue\", 25), rep(\"green4\", 25), rep(\"mediumorchid4\",\n25))\n# plot a network based on the adjacency matrix 'con' before thresholding, so that\n# we can see the directional connection caused by spillover effects among 100\n# financial institutions.\nplot_g = qgraph(con, groups = groups, layout = \"groups\", layoutScale = c(1.2, 1.2),\nlabel.font = 2, label.cex = 2, shape = \"circle\", labels = names.fi, esize = 5,\nmaximum = max(con), color = \"white\", node.width = 0.8, label.cex = 1.8, label.color = col,\nedge.color = col, curve = 1, border.width = 1.2, border.color = col, asize = 2.5)\ntext(x = -0.9, y = 1, labels = substitute(paste(d), list(d = as.character(dt[80]))),\nxpd = NA, cex = 1.5)\nlegend(0.82, 1.1, box.lwd = 2, box.col = \"white\", bg = \"white\", c(\"Depositories\",\n\"Insurers\", \"Broker-Dealers\", \"Others\"), text.col = c(\"red\", \"blue\", \"green4\",\n\"purple3\"), cex = 1.3)\n# apply a hard thresholding to make the major connections more clearly\ncon = ifelse(abs(con) >= ((1/100) * sum(con[order(con, decreasing = T)[1:100]])),\ncon, 0)\n# plot a network based on the adjacency matrix 'con' after thresholding, so that\n\n", "output_sequence": "Plots the network based on an adjacency matrix before and after thresholding, so that we can see the directional connection caused by spillover effects among 100 financial institutions."}, {"input_sequence": "Author: Roman Lykhnenko ; # install and load packages\nlibraries = c(\"dplyr\", \"MASS\", \"gridExtra\", \"ggplot2\", \"Matrix\", \"parallel\", \"caTools\",\n\"np\", \"RColorBrewer\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# set working directory\nsetwd(\"/home/rama/Masterarbeit/pricing_kernels_and_implied_volatility\")\nload(\"epk3VolaIntervalsVDAX3m/locLinBWvdax3m.RData\")\ntimeToMaturity = 3/12\nhorizonPhysicalDensity = 300\nfor (trading_year in 2012) {\n\nyear = trading_year\n# path to Data\npathToData = paste(\"epk3VolaIntervalsVDAX3m/C_\", as.character(year), \"vdax3m\",\n\".csv\", sep = \"\")\n# read option data\nC_2012 = read.csv(pathToData, sep = \",\", header = TRUE)\nC_2012 = na.omit(C_2012)\n# load Dax and VDax data\nbothIndexesDax = read.csv(\"epk3VolaIntervalsVDAX3m/timeSeriesDaxVdax3m.csv\")\nbothIndexesDax = bothIndexesDax[, c(\"Date\", \"DAX\", \"VDAX\")]\nbothIndexesDax$Date = as.Date(as.character(bothIndexesDax$Date), \"%Y-%m-%d\")\nbothIndexesDax = bothIndexesDax[bothIndexesDax$Date >= as.Date(\"2000-01-01\"), ]\n# for selection of quantiles\nbothIndexesDaxQuant = bothIndexesDax[bothIndexesDax$Date <= as.Date(paste(as.character(year),\n\"-12-31\", sep = \"\")), ]\nbothIndexesDaxQuant = bothIndexesDaxQuant[bothIndexesDaxQuant$Date >= as.Date(paste(as.character(year -\n1), \"-12-31\", sep = \"\")), ]\nttmFraction = timeToMaturity\nspecifyQuant = c(0.05, 0.95)\nlowVolaIntUp = quantile(bothIndexesDaxQuant$VDAX, 0.35)\nvola_levels = as.character(seq(from = as.numeric(quantile(bothIndexesDaxQuant$VDAX,\nspecifyQuant))[1],\nto = as.numeric(quantile(bothIndexesDaxQuant$VDAX, specifyQuant))[2],\nlength.out = 20))\nlistRndPDpk = list()\nfor (vola_item in vola_levels) {\nprint(vola_item)\nVDAX_level = as.numeric(vola_item)\n\n# add ttm in days\nC_2012$ttmDays = (C_2012$TTM) * 365\n# moneyness(Strike/S_0)\nC_2012$moneyness = C_2012$EXERCISE_PRICE/C_2012$DAX\nmoneyness_est = seq(0.75, 1.5, length.out = 100)\n# option price\nC = C_2012[, \"settlement\"]\n# discount factor\nD = exp(-(C_2012$euriborRate) * C_2012$TTM)\n# scale by discount factor\nC = C/D\n# scale by forward price\nC = C/C_2012$DAX\n# subset option data used for estimation\noptionData = cbind(C, C_2012[, c(\"TTM\", \"moneyness\", \"VDAX\", \"Date\")])\noptionData$Date = as.Date(as.character(optionData$Date))\n# bandwidth specification as used for estimtion of the RND\nhTTM_RND = bandwidthMonKfoldCVyears[[as.character(year)]]$matur\n# Function performing local linear kernel regression\nlocLinRegression = function(moneynessValue, tauValue, data = optionData,\nh.TTM, h.moneyness, h.VDAX, output = \"C\") {\n\n# u_i, independent variables\nregressors = data[, c(\"TTM\", \"moneyness\", \"VDAX\")]\n# response(dependent) variable of regression equation\nDependentVar = data[, output]\nn = nrow(regressors)\nKernel = rep(0, n)\n# 3dim kernel function\nfor (j in 1:n) {\n\nKernel[j] = (dnorm((regressors$TTM[j] - tauValue)/h.TTM, mean = 0,\nsd = 1, log = FALSE)/h.TTM) * (dnorm((regressors$VDAX[j] - vdaxValue)/h.VDAX,\nmean = 0, sd = 1, log = FALSE)/h.VDAX) * (dnorm((regressors$moneyness[j] -\nmoneynessValue)/h.moneyness, mean = 0, sd = 1, log = FALSE)/h.moneyness)\n}\nKernel_matrix = Diagonal(n = length(Kernel), Kernel)\nregressors_minus_u = cbind(regressors$TTM - tauValue, regressors$VDAX -\nvdaxValue, regressors$moneyness - moneynessValue)\nOmega = cbind(1, regressors_minus_u)\ninvertedExpression = tryCatch(solve(t(Omega) %*% Kernel_matrix %*% Omega),\nerror = function(c) NA)\nif (is.element(NA, as.vector(invertedExpression))) {\nFirstOrderDerMon = NA\nconstTerm = NA\n} else {\nestCoef = invertedExpression %*% t(Omega) %*% Kernel_matrix %*% DependentVar\nFirstOrderDerMon = estCoef[4]\nout = list(firstOrderDerivativeMoneyness = FirstOrderDerMon, constTerm = constTerm)\nout\n}\n# 1dim RND by numerical differentiation of obtained 1st order derivative\ncoefficientsAll = mclapply(moneyness_est, locLinRegression, tauValue = ttmFraction,\nvdaxValue = VDAX_level, h.TTM = hTTM_RND, h.moneyness = hMon_RND, h.VDAX = hVDAX_RND)\nregression1DerMon = sapply(coefficientsAll, FUN = function(listItem) {\nlistItem[[\"firstOrderDerivativeMoneyness\"]]\n})\n# increment moneynes to compute derivtive\ndelta = 0.005\nmoneyness_est0 = moneyness_est + delta\n# compute regression at the changed moneyness\ncoefficientsAll0 = mclapply(moneyness_est0, locLinRegression, tauValue = ttmFraction,\nvdaxValue = VDAX_level, h.TTM = hTTM_RND, h.moneyness = hMon_RND, h.VDAX = hVDAX_RND)\nregression1DerMon0 = sapply(coefficientsAll0, FUN = function(listItem) {\nRND_given_ttm_final = (regression1DerMon0 - regression1DerMon)/delta\nRND_given_ttm = abs(RND_given_ttm_final * moneyness_est)\nexcess_return = log(moneyness_est)\n# plot(excess_return, abs(RND_given_ttm), type='p') trapz(excess_return,\n# abs(RND_given_ttm))\n# calculation of the conditional variance\noptionDataForResidualsIndex = seq(1, nrow(optionData), 280)\noptionDataForResiduals = optionData[optionDataForResidualsIndex, ]\nreplicateOptionDataForResiduals = lapply(1:nrow(optionDataForResiduals), FUN = function(index) optionDataForResiduals)\n# function to perform loc lin regression used for calculation of conditional\n# variance\nlocLinRegressionForRes = function(moneynessValue, tauValue, data = optionData,\noutput = \"C\") {\n# bandwidth specification\nh.TTM = sd(regressors$TTM) * nrow(regressors)^(-1/(4 + 3))\nestCoef = invertedExpression %*% t(Omega) %*% Kernel_matrix %*% DependentVar\n# preparation to compute residuals\nforResiduals = mcmapply(locLinRegressionForRes, optionDataForResiduals$moneyness,\noptionDataForResiduals$TTM, data = replicateOptionDataForResiduals)\nestimOptionPrice = unlist(forResiduals[\"constTerm\", ])\noptionDataForResiduals$estimOptionPrice = estimOptionPrice\n# squared residuals\noptionDataForResiduals$squaredResid = (optionDataForResiduals$C - optionDataForResiduals$estimOptionPrice)^2\n# local linear regresson with squared residual as a dependent variable\ncoefficientsResidualsReg = mclapply(moneyness_est, locLinRegressionForRes,\ntauValue = ttmFraction, vdaxValue = VDAX_level,\noutput = \"squaredResid\")\nconditionalVariance = sapply(coefficientsResidualsReg, FUN = function(listItem) {\nlistItem[[\"constTerm\"]]\nkernelConstant = 0.2115711\n# bandwidth specification for estimtion of the joint density, ROT\nhTTMjointDen = sd(optionData$TTM) * nrow(optionData)^(-1/(4 + 3)) * (4/5)^(1/(3 +\n4))\nhMonjointDen = sd(optionData$moneyness) * nrow(optionData)^(-1/(4 + 3)) * (4/5)^(1/(3 +\n4))\nhVDAXjointDen = sd(optionData$VDAX) * nrow(optionData)^(-1/(4 + 3)) * (4/5)^(1/(3 +\n4))\n# joint density of moneyness, tau, vola\njoinDensityRegressors = function(pointMon, pointTTM, pointVDAX, hMon, hTTM,\nhVDAX, data = optionData) {\njointDens = mean((1/hMon) * (1/hTTM) * (1/hVDAX) * dnorm((data$moneyness -\npointMon)/hMon,\nmean = 0, sd = 1, log = FALSE) * dnorm((data$TTM -\npointTTM)/hTTM, mean = 0, sd = 1, log = FALSE) * dnorm((data$VDAX -\npointVDAX)/hVDAX, mean = 0, sd = 1, log = FALSE))\njointDens\njointDenValues = unlist(mclapply(moneyness_est, joinDensityRegressors, pointTTM = ttmFraction,\npointVDAX = VDAX_level, hMon = hMonjointDen,\ndata = optionData))\nsigmaSquared = (moneyness_est^2) * (1/(3 * sqrt(pi))^3) * kernelConstant *\nabs(conditionalVariance) / jointDenValues\nfactorDistribution = nrow(optionData) * ((hMon_RND)^4) * (hMon_RND * hTTM_RND *\nhVDAX_RND)\nrndVariance = abs(sigmaSquared/factorDistribution)\nrndLocLinWithCI = data.frame(excess_return,\nRND_given_ttm,\nrndLocLinUp = RND_given_ttm + qnorm(0.95, 0, 1) * sqrt(rndVariance),\ncolnames(rndLocLinWithCI) = c(\"market_return\", \"RNDlocLin\", \"rndLocLinUp\",\n\"rndLocLinDown\")\n# Local const estimation of PD\n# used in contemporaneous approach for calculation of physical density\ndateHistDensity = as.Date(paste(as.character(year), \"-12-31\", sep = \"\"))\n# physical density estimation (using contemporaneous method)\n# maturity for which historical density to be calculated\ntauDays = round(ttmFraction * 365, 0)\n# how many observations from past to use for estimation of hist. density\nhorizon = horizonPhysicalDensity\n# take only those rows that are earlier than dateHistDensity\nbothIndexesDax = bothIndexesDax[bothIndexesDax$Date <= dateHistDensity, ]\n# supposed to contain returns of the index observed over each maturity\nreturnsDaxVdax = bothIndexesDax[1:horizon, c(\"DAX\", \"VDAX\")]\ncolnames(returnsDaxVdax) = c(\"DaxReturns\", \"VDAXlevel\")\nlengthBothIndexes = length(bothIndexesDax[, 1])\nfor (i in 1:horizon) {\nreturnsDaxVdax[i, 1] = log(bothIndexesDax$DAX[lengthBothIndexes - i]/(bothIndexesDax$DAX[lengthBothIndexes -\ni - tauDays]))\nreturnsDaxVdax[i, 2] = bothIndexesDax$VDAX[lengthBothIndexes - i]\n# Local const regression(NW) for esimation of conditional physical density\n# specify bandwidth\nbwC = npcdensbw(xdat = returnsDaxVdax$VDAXlevel, ydat = returnsDaxVdax$DaxReturns,\nbwmethod = \"normal-reference\")\nhDaxReturnsJoint = 1.5 * bwC$ybw\nhVDAXJoint = bwC$xbw\n# conditional kernel density estimation\nconditDensity = function(pointReturns, pointVDAX, hDaxReturnsJoint, hVDAXJoint,\nhVDAXmarginal, returnsDaxVdax) {\njointDens = mean((1/hDaxReturnsJoint) * (1/hVDAXJoint) * dnorm((returnsDaxVdax[,\n1] - pointReturns)/hDaxReturnsJoint, mean = 0, sd = 1, log = FALSE) *\ndnorm((returnsDaxVdax[, 2] - pointVDAX)/hVDAXJoint, mean = 0, sd = 1,\nlog = FALSE))\nmarginalDenVDAX = mean((1/hVDAXmarginal) * dnorm((returnsDaxVdax[, 2] -\npointVDAX)/hVDAXmarginal, mean = 0, sd = 1, log = FALSE))\noutCondDensity = jointDens/marginalDenVDAX\nnFactor = nrow(returnsDaxVdax)\nc1 = hDaxReturnsJoint\nconfInt = ((1/(4 * 3.14)) * outCondDensity/(c1 * c2 * marginalDenVDAX))/nFactor\nout = list(conditionalDensity = outCondDensity, confidenceInterval = confInt)\n# range of log returns (to calculate values of the density at these points)\neurostoxx_range = excess_return # from file with RND estimation\neurostoxx_hist_densityValues = lapply(eurostoxx_range, conditDensity,\npointVDAX = VDAX_level,\nhDaxReturnsJoint = hDaxReturnsJoint,\n# get density values, and variance separately\ndensityValues = sapply(eurostoxx_hist_densityValues, FUN = function(listItem) {\nlistItem[[1]]\nvariancePD = sapply(eurostoxx_hist_densityValues, FUN = function(listItem) {\nlistItem[[2]]\n# data frame with density values and CI\ndataPDlocConstWithCI = data.frame(returns = eurostoxx_range, PhysicalDensityLocalConstValue = densityValues,\npdLocalConstUpperBound = densityValues + qnorm(0.95, 0, 1) * sqrt(variancePD),\n# density integrates to 1 trapz(eurostoxx_range, densityValues)\n# EPK\nPK_value = (rndLocLinWithCI$RNDlocLin)/(dataPDlocConstWithCI$PhysicalDensityLocalConstValue)\nvarianceEPK = (1/(dataPDlocConstWithCI$PhysicalDensityLocalConstValue))^2 * rndVariance +\n((1/(dataPDlocConstWithCI$PhysicalDensityLocalConstValue))^4) * ((rndLocLinWithCI$RNDlocLin)^2) *\nvariancePD\nrndPDpk = data.frame(returns = eurostoxx_range,\nPricingKernel = PK_value,\npkUpperBound = PK_value + qnorm(0.95, 0, 1) * sqrt(varianceEPK),\nPhysicalDensityLocalConstValue = densityValues,\npdLocalConstUpperBound = densityValues + qnorm(0.95, 0, 1) * sqrt(variancePD),\nRND_given_ttm,\nrndLocLinUp = RND_given_ttm + qnorm(0.95, 0, 1) * sqrt(rndVariance),\nlistRndPDpk[[vola_item]] = rndPDpk\n}\n# save as RData\nsave(vola_levels,\nlistRndPDpk,\nyear,\nlowVolaIntUp,\nfile = paste0(\"epk3VolaIntervalsVDAX3m/listRndPDpkVDAX3m\", year, \".RData\"))\n# quantiles of VDAX used for CI\nquantile_20 = quantile(bothIndexesDaxQuant$VDAX, 0.2)\n#\n# definition of funtions for plotting of PKs, RNDs\nplotPK = function(yearItem){\n# load precomputed objects\nload(paste0(\"epk3VolaIntervalsVDAX3m/listRndPDpkVDAX3m\", as.character(yearItem), \".RData\"))\n# add aditional label to every plot\nremarkPlot = \"VDAX3m\"\n# prepare for plotting\nfor(itemN in (1:length(vola_levels))){\nlistRndPDpk[[vola_levels[itemN]]]$VDAXlevel = vola_levels[itemN]\n# bind all elements of the list in one data frame\ndataAllVDAXlevels = do.call(\"rbind\", listRndPDpk)\n# Plotting together with CI\n# small volatility interval\npkAllsmallVola = ggplot(data = dataAllVDAXlevels %>% filter(VDAXlevel <= as.numeric(lowVolaIntUp)),\naes(x = returns, y = PricingKernel, colour = VDAXlevel)) +\ngeom_line() +\ncoord_cartesian(xlim = c(-0.2, 0.2), ylim = c(0, 5)) +\ntheme_bw() +\ntheme(legend.position = \"none\") +\nggtitle(paste0(\"low volatility (between \",\nas.character(format(round(as.numeric(lowVolaIntDown), 2),\nnsmall = 2)),\n\" and \",\nas.character(format(round(as.numeric(lowVolaIntUp), 2), nsmall = 2)),\n\")\")) +\nxlab(\"\") +\nscale_colour_brewer(palette = \"PuOr\")\npkAllsmallVolaCI = ggplot(data = dataAllVDAXlevels %>%\nfilter(VDAXlevel ==\nas.numeric(vola_levels)[as.numeric(vola_levels)\n>= as.numeric(quantile_20)\naes(x = returns)) +\ngeom_line(aes(y = PricingKernel), size = 0.5, colour = \"black\") +\ngeom_ribbon(aes(ymin = pkLowerBound, ymax = pkUpperBound), alpha = 0.5) +\ncoord_cartesian(xlim = c(-0.2, 0.2), ylim = c(0, 5)) +\ntheme_bw() +\ntheme(legend.position = \"none\") +\nas.character(format(round(as.numeric(lowVolaIntDown),\n2), nsmall = 2)),\nxlab(\"\") +\n# medium volatility interval\npkAllmediumVola = ggplot(data = dataAllVDAXlevels %>%\nfilter(VDAXlevel > as.numeric(lowVolaIntUp),\naes(x = returns, y = PricingKernel, colour = VDAXlevel)) +\ngeom_line() +\ntheme(legend.position=\"none\") +\nggtitle(paste0(\"medium volatility (between \",\nas.character(format(round(as.numeric(lowVolaIntUp), 2), nsmall = 2)),\nylab(\"PK\") +\n# take first VDAX level that is not less than 50% quantile\npkAllmediumVolaCI = ggplot(data = dataAllVDAXlevels %>%\nfilter(VDAXlevel ==\nas.numeric(vola_levels)[as.numeric(vola_levels)\n>= as.numeric(quantile_50)\naes(x = returns))+\nas.character(format(round(as.numeric(mediumVolaIntUp), 2), nsmall = 2)),\n# high volatility\npkAllhighVola = ggplot(data = dataAllVDAXlevels %>%\nfilter(VDAXlevel > as.numeric(mediumVolaIntUp)),\naes(x = returns, y = PricingKernel, colour = VDAXlevel)) +\nggtitle(paste0(\"high volatility (between \",\nas.character(format(round(as.numeric(highVolaIntUp), 2), nsmall = 2)),\nscale_colour_brewer(palette=\"PuOr\")\npkAllhighVolaCI = ggplot(data = dataAllVDAXlevels %>%\nfilter(VDAXlevel ==\nas.numeric(vola_levels)[as.numeric(vola_levels)\n>= as.numeric(quantile_80)\naes(x = returns)) +\ncoord_cartesian(xlim = c(-0.2, 0.2), ylim = c(0, 5) ) +\n# allocate plots on the one page\nout = grid.arrange(pkAllsmallVola,\npkAllsmallVolaCI,\n# specify name of the plot to be saved\nplotName = paste(\"epk3VolaIntervalsVDAX3m/\",\n\"_\",\nas.character(year),\nggsave(plotName, out, width = 9, height = 12)\nplotPD = function(yearItem){\n# Plotting together with CI\n# small volatility interval\npdAllsmallVola = ggplot(data = dataAllVDAXlevels %>%\nfilter(VDAXlevel <= as.numeric(lowVolaIntUp)),\naes(x = returns,\ny = PhysicalDensityLocalConstValue,\ncolour = VDAXlevel)) +\ncoord_cartesian(xlim = c(-0.2, 0.2), ylim = c(0, 10)) +\nas.character(format(round(as.numeric(lowVolaIntDown), 2), nsmall = 2)),\nylab(\"PD\") +\npdAllsmallVolaCI = ggplot(data = dataAllVDAXlevels %>%\ngeom_line(aes(y = PhysicalDensityLocalConstValue), size = 0.5, colour=\"black\") +\ngeom_ribbon(aes(ymin = pdLocalConstLowerBound, ymax = pdLocalConstUpperBound), alpha=0.5) +\nylab(\"PD\")\npdAllmediumVola = ggplot(data = dataAllVDAXlevels %>%\naes(x = returns,\ny = PhysicalDensityLocalConstValue,\ncolour = VDAXlevel)) +\ncoord_cartesian(xlim = c(-0.2, 0.2), ylim=c(0, 10)) +\npdAllmediumVolaCI = ggplot(data=dataAllVDAXlevels %>%\naes(x = returns)) +\ngeom_line(aes(y = PhysicalDensityLocalConstValue), size = 0.5, colour = \"black\") +\ngeom_ribbon(aes(ymin = pdLocalConstLowerBound, ymax = pdLocalConstUpperBound), alpha = 0.5) +\npdAllhighVola = ggplot(data=dataAllVDAXlevels %>%\naes(x = returns, y = PhysicalDensityLocalConstValue, colour = VDAXlevel )) +\ncoord_cartesian(xlim = c(-0.2, 0.2), ylim = c(0, 10) ) +\nas.character(format(round(as.numeric(highVolaIntUp), 2), nsmall = 2)),\npdAllhighVolaCI = ggplot(data = dataAllVDAXlevels %>%\naes(x = returns))+\ncoord_cartesian(xlim = c(-0.2, 0.2), ylim = c(0, 8) ) +\nout = grid.arrange(pdAllsmallVola,\npdAllsmallVolaCI,\n\"_PD\",\nplotRND = function(yearItem){\naes(x = returns, y = RND_given_ttm, colour = VDAXlevel)) +\nylab(\"RND\") +\ngeom_line(aes(y = RND_given_ttm), size = 0.5, colour = \"black\") +\ngeom_ribbon(aes(ymin = rndLocLinDown, ymax = rndLocLinUp), alpha = 0.5) +\nylab(\"RND\")\naes(x = returns, y = RND_given_ttm, colour = VDAXlevel )) +\n\")\" )) +\npdAllmediumVolaCI = ggplot(data = dataAllVDAXlevels %>%\n\")\" ))+\npdAllhighVola = ggplot(data = dataAllVDAXlevels %>% filter(VDAXlevel > as.numeric(mediumVolaIntUp)),\naes(x = returns, y = RND_given_ttm, colour = VDAXlevel)) +\n\")\")) +\nylab(\"RND\") +\n\"RND\",\nplotPK(year)\n}\n", "output_sequence": "Estimates and plots (yearly) empirical pricing kernels (EPK), risk neutral densities (RND) and physical densities (PD) of DAX 30 index return conditional on time to maturity 3 months and different levels of VDAX-NEW-Subindex 3 (20 equally spaced numbers from 5% to 95% quantile of VDAX-NEW-Subindex 3 in a given year). Local linear kernel regression is used for estimation of the conditional risk neutral density and local constant kernel regression is used for estimation of conditional physical density. EPK is obtained as a ratio of RND and PD. The panels on the right-hand side of figures depict EPK, RND, PD conditional on 20%, 50% and 80% quantiles of VDAX-NEW-Subindex 3 with 95% confidence intervals. Colors from red to blue correspond to increasing values of volatility within each interval. All results are shown on a continuously compounded 3-months period returns scale."}, {"input_sequence": "Author: Mona Timmermann ; # Using this script, we load the raw and messy datasets, which will then be used in the models quantlet\n# We refrain from putting this in the Models script just to keep it tidier.\n# dataset 1: messy data\n# merge train and macro data sets by timestamp\n# and delete the transaction ID variable\ntraincsv = read.csv('train.csv', sep=',')\ndataset1 = merge(traincsv, macrocsv, by='timestamp')\ndataset1$timestamp = as.Date(dataset1$timestamp)\ndataset1$id = NULL\n\n# dataset 2: clean data\n# read previously cleaned data\n# delete meterprice variable, which had been used to construct area_meterprice\n# cut off extraordinary high values of the target variable (it improved accuracy a lot), i.e. > 75%-Quantile + 1.5*IQR (heuristic)\n# and again, delete the transaction ID variable\ndataset2 = readRDS('Data_Clean', refhook=NULL)\ndataset2$meterprice = NULL\noptions(scipen=10)\n#boxplot(dataset2$price_doc)\ni25 = as.numeric(summary(dataset2$price_doc)[2])\nmaximale = i75 + 1.5 * (i75 - i25)\ndataset2$price_doc[dataset2$price_doc>maximale] = maximale\n# dataset 3: clean data and standardised predictors\n# adapt dataset 2 by standardising numeric predictors\ndataset3 = dataset2\nstandardise = function(var) {\nmu = mean(var)\nresult = (var - mu) / sd\nreturn(result)\n}\nidx.num = sapply(dataset3, is.numeric)\ndataset3[,idx.num & !colnames(dataset3) %in% c('price_doc')] = lapply(dataset3[,idx.num & !colnames(dataset3) %in% c('price_doc')], standardise)\n# dataset 4:\n# has been adapted directly in the model script by\n# changing target in train() function to sqrt(price_doc) and adapting prediction by using ^2,\n# e.g. xgb.pred = predict(xgb, newdata = val)^2 to then be able to compare the RMSE value to the other models\n", "output_sequence": "Applying eXtreme Gradient Boosting to three different sets of variables, comparing performances on the validation set, and using Partial Least Squares Regression as a benchmark"}, {"input_sequence": "Author: Mona Timmermann ; # Name of Quantlet: SPL_Models\n# Published in: Statistical Programming Languages\n# Description: 'Applying eXtreme Gradient Boosting to three different sets of variables,\n# comparing performances on the validation set, and\n# using Partial Least Squares Regression as a benchmark'\n# Keywords: prediction, boosting, machine, regression, trees, partial least squares, RMSE\n# Author: Mona Timmermann Submitted: Fr, Aug 18 2017\n# We chose to apply eXreme Gradient Boosting because it is supposed to be more robust than other techiques and\n# has often not only shown high predictive accuracy but also computational efficiency\n# set random number generator\nset.seed(194)\n# Install and load packages for XGB\nif (!require(\"caret\")) install.packages(\"caret\")\nlibrary(\"caret\")\nif (!require(\"xgboost\")) install.packages(\"xgboost\")\nlibrary(\"xgboost\")\nif (!require(\"plyr\")) install.packages(\"plyr\")\nlibrary(\"plyr\")\n# we apply the following script on different data sets load all datasets and then define what dataset should be\n# used in the following\nsource(\"Helperscript_LoadDatasets.R\")\n# dataset1 = messy data, dataset2 = clean data, dataset3 = clean data and standardised numeric variables\n# dataset4: use dataset3 and set target variable as sqrt(price_doc) in caret's train function & then always\n# adapt the predictions by using ^2, e.g. xgb.pred = predict(xgb, newdata = val)^2\nset1 = dataset2 # change depending on which dataset you want to use\n# fyi: we have trained eXtreme gradient boosting regression trees on three sets of variables: 1) all variables,\n# including many highly correlated variables 2) variables selected using xgb variable importance 3) few\n# hand-selected variables, loosely based on theoretical underpinnings\n# we use RMSE as a metric because it is commonly used for regressions and because larger errors have a\n# disproportionately large effect on RMSE, thus we are trying to reduce especially large errors\n## --------------------------CONTROL-------------------------- ##\n# Specify how the models will be compared (cv\n# for cross validation, 5 for 5 folds):\ncontrol = trainControl(method = \"cv\", number = 5, returnData = TRUE)\n## --------------------------TUNING PARAMETERS-------------------------- ##\n# we tune the models based on the following parameters: note that this also gives\n# the algorithm the opportunity to behave differently for different sets of variables\n# define tuning parameters:\nn = 700 # number of boosting steps\nm = c(4, 8) # depth of boosted trees\ne = c(0.01, 0.05) # learning rate, common is < default of 0.3 to avoid overfitting\ng = 0 # minimum loss reduction required to make a further partition on a leaf node\nb = 0.8 # subsample ratio of columns when constructing a tree\nw = 1 # minimum sum of instance weight: minimum number/weight of observations required to mke a further partition on a leaf node\ns = 0.8 # random subsample of observations used to build the individual trees\nxgb.parms = expand.grid(nrounds = n, max_depth = m, eta = e, gamma = g, colsample_bytree = b, min_child_weight = w,\nsubsample = s)\n## --------------------------SET 1 XGB-------------------------- ##\n## SET 1: All variables\n# Use split sampling, so we can later assess the model's performance on the validation set:\nidx1 = sample(1:nrow(set1), size = 0.8 * nrow(set1))\ntrain1 = set1[idx1, ]\n# Train the model\nxgb1 = train(price_doc ~ ., data = train1, method = \"xgbTree\", tuneGrid = xgb.parms, trControl = control, metric = \"RMSE\",\nna.action = na.pass)\n# Store the results:\nxgb1_results = xgb1$results\n# Best tuning parameters:\nxgb1_tuning_RMSE = subset(xgb1_results, xgb1_results$RMSE == min(xgb1_results$RMSE))\nxgb1_tuning_RMSE\n# Prediction for validation set\nxgb1.pred = predict(xgb1, newdata = val1, na.action = na.pass)\n# Evaluate performance on validation set using RMSE (and R-Squared):\nxgb1_performance = postResample(xgb1.pred, val1$price_doc)\n# How wrong were we? In what direction?\nxgb1_diff = xgb1.pred - val1$price_doc\nsummary(xgb1_diff)\n## --------------------------SET 2 XGB-------------------------- ##\n# based on variable importance of previously built xgb model, we select the following variables, where variable\n# importance >= 0.5 (heuristic)\nvarImp = data.frame(varImp(xgb1)$importance) # save importance measures\nvarImp$Vars = row.names(varImp) # include variable names in new column\nvarImp_reduced = t(subset(varImp, varImp$Overall >= 0.5)) # transpose to have variables in columns\n# set 2 reduces set 1 to important variables and the target variable:\nset2 = set1[, names(set1) %in% varImp_reduced | names(set1) %in% c(\"price_doc\")]\nidx2 = sample(1:nrow(set2), size = 0.8 * nrow(set2))\ntrain2 = set2[idx2, ]\n''\nxgb2 = train(price_doc ~ ., data = train2, method = \"xgbTree\", tuneGrid = xgb.parms, trControl = control, metric = \"RMSE\",\nxgb2_results = xgb2$results\nxgb2_tuning_RMSE = subset(xgb2_results, xgb2_results$RMSE == min(xgb2_results$RMSE))\nxgb2_tuning_RMSE\nxgb2.pred = predict(xgb2, newdata = val2, na.action = na.pass)\nxgb2_performance = postResample(xgb2.pred, val2$price_doc)\nxgb2_diff = xgb2.pred - val2$price_doc\nsummary(xgb2_diff)\n## --------------------------SET 3 XGB-------------------------- ##\n# in the following we train a model on just a few of the variables we consider important to find out how much\n# worse our predictions get\n## SET 3:\nset3_vars = c(\"full_sq\", \"area_meterprice\", \"kitch_sq\", \"material\", \"num_room\", \"rentincrease\", \"life_sq\", \"big_road_1km\",\n\"bulvar_ring_km\", \"ID_railroad_station_avto\", \"month\", \"buildage\", \"month\", \"mortgage_rate\", \"deposits_growth\",\n\"salary_growth\", \"gdp_deflator\", \"usdrub\", \"salary_growth\", \"pop_migration\", \"price_doc\", \"meterprice\")\nset3 = set1[, colnames(set1) %in% set3_vars]\nidx3 = sample(1:nrow(set3), size = 0.8 * nrow(set3))\ntrain3 = set3[idx3, ]\nxgb3 = train(price_doc ~ ., data = train3, method = \"xgbTree\", tuneGrid = xgb.parms, trControl = control, metric = \"RMSE\",\nxgb3_results = xgb3$results\nxgb3_tuning_RMSE = subset(xgb3_results, xgb3_results$RMSE == min(xgb3_results$RMSE))\nxgb3_tuning_RMSE\nxgb3.pred = predict(xgb3, newdata = val3, na.action = na.pass)\nxgb3_performance = postResample(xgb3.pred, val3$price_doc)\nxgb3_diff = xgb3.pred - val3$price_doc\nsummary(xgb3_diff)\n## --------------------------PLS-------------------------- ##\n# run PLS on dataset3 (clean; standardized predictors) only:\nset1 <- dataset3\n# load required package\nif (!require(\"pls\")) install.packages(\"pls\")\nlibrary(\"pls\")\n### PLS on all vars:\n# PLS can easily be implemented using the Caret Package:\npls1 = train(train1[, !colnames(train1) %in% c(\"price_doc\")], train1$price_doc, method = \"pls\", trControl = control,\ntuneLength = 25)\nyhat.pls1 = predict(pls1, newdata = val1)\n# Evaluate model out-of-sample performance using RMSE (and R-Squared):\npls1_performance = postResample(yhat.pls1, val1$price_doc)\n### PLS on hand-selected vars: PLS can easily be implemented using the Caret Package:\npls3 = train(train3[, !colnames(train3) %in% c(\"price_doc\")], train3$price_doc, method = \"pls\", trControl = control,\nyhat.pls3 = predict(pls3, newdata = val3)\npls3_performance = postResample(yhat.pls3, val3$price_doc)\n", "output_sequence": "Applying eXtreme Gradient Boosting to three different sets of variables, comparing performances on the validation set, and using Partial Least Squares Regression as a benchmark"}, {"input_sequence": "Author: Luisa Krawczyk ; # This file generates a hierarchical clustering as well as a dendrogram and a\n# plot indicating cluster sizes\n\n# Importing the data set\nquantlet <- read.table(\"C:/Users/Luisa/Desktop/Clustering/quantlet_data.txt\", header=T, sep=\",\")\nX <- quantlet[1:2064,2:1287] # removing the first column which contains the file names\nmatrix <- as.matrix(X)\n# dist generates a distance matrix, by default complete linkage is used\nclusters <- hclust(dist(matrix))\nplot(clusters) # plots the dendrogram\n# You can plot a truncated dendrogram like this:\ndend <- as.dendrogram(clusters)\nplot(cut(dend, h=6)$upper) # h is the distance you want to trucate at\n# print the whole dendrogram as PDF\npdf(\"whole_dendrogram.pdf\", width = 40, height = 15)\nplot(dend)\ndev.off()\n# print only truncated dendrogram :\npdf(\"dendrogram_truncated\", width = 40, height = 15)\nplot(cut(dend, h = 5)$upper)\n# creating 10 clusters\nclusterCut <- cutree(clusters, k=40)\ntable(clusterCut)\n# Export a plot indicating cluster sizes as PDF\npdf(\"40_clusters_hierarchical_obs.pdf\", width = 10, height = 4)\nbarplot(table(clusterCut), main=\"Hierarchical Clustering 40 clusters \", ylab=\"Number of observations per cluster\", xlab=\"Cluster\")\n# Computing average linkage clustering\nclusters_average <- hclust(dist(matrix), method = 'average')\ndend_average <- as.dendrogram(clusters_average)\n# plot(cut(dend,h=6)$upper)\nclusterCut_average <- cutree(clusters_average, 40)\ntable(clusterCut_average)\n", "output_sequence": "Generating a hierarchical clustering as well as a dendrogram and a plot indicating cluster sizes"}, {"input_sequence": "Author: Luisa Krawczyk ; # -*- coding: utf-8 -*-\n\"\"\"\nCreated on Sat Apr 20 15:02:46 2019\n\n@author: Luisa\n''' Determining the optimal k for k-means clustering\nThis code computes the sum of squares for each value of k\nInput: matrix is supposed to be a numpy term-document matrix\nwith documents as rows and terms as columns\n'''\n# sum of squares as a function of k, decide for k=15 or k=18 or k=9, 22 or 26\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nSum_of_squared_distances = []\nK = range(1,30)\nfor k in K:\nkm = KMeans(n_clusters=k)\nkm = km.fit(matrix)\nSum_of_squared_distances.append(km.inertia_)\nplt.plot(K, Sum_of_squared_distances, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Sum_of_squared_distances')\nplt.title('Elbow Method For Optimal k')\nplt.show()\n", "output_sequence": "Plotting the sum of squared distances for each k of a k-means clustering"}, {"input_sequence": "Author: Georg Velev, Iliyana Pekova ; from keras.callbacks import EarlyStopping\ndef train_predict_evaluate(params,data,train_test_split):\nrmse=[]\nfor i in range(0,len(params)):\nprint(\"This is iteration \",(i+1),\" from \",len(params), \" iterations.\")\nparameters=params[i]\nX_train, X_test, y_test=generate_Xtrain_yTrain_Xtest_Ytest(data,train_test_split,24,24)\nmodel_LSTM=multi_step_LSTM(n_steps_in=24, n_features=1,n_steps_out=24,nr_neurons=parameters['number_neurons'],epochs=parameters[\"epochs\"],lr=parameters[\"learning_rate\"])\nearly_stop=EarlyStopping(monitor='loss', mode='min', verbose=0, patience=10, restore_best_weights=True)\nmodel_LSTM.fit(X_train, y_train, batch_size=parameters[\"batch_size\"],epochs=parameters[\"epochs\"],verbose=0,callbacks=[early_stop])\nrmse_result=model_LSTM.evaluate(X_test, y_test, verbose=0)\nprint(\"Total RMSE from iteration \", (i+1), \" is equal to \",rmse_result,\".\",\"\\n\")\nrmse.append(rmse_result)\n\nnew_data=pd.DataFrame({\"RMSE\": rmse,\"Parameters\": params}, columns=[\"RMSE\",\"Parameters\"])\nnew_data=new_data.sort_values(by='RMSE', ascending=True)\nreturn new_data\nfrom sklearn.model_selection import ParameterGrid\nparam_grid_LSTM = {\n'batch_size': [16,32],\n'epochs': [30,60],\n'number_neurons': [100,150],\n'learning_rate': [0.05,0.01]\n}\nparams=list(ParameterGrid(param_grid_LSTM))\nresults_ETH=train_predict_evaluate(params,ETH,0.8)\nprint(\"Minimal RMSE achieved: \",round(results_ETH.iloc[0,0],5))\nprint(\"Corresponding parameters: \",results_ETH.iloc[0,1])\n", "output_sequence": "Define a parameter grid for the LSTM and choose the optimal set of parameters that leads to a minimal root-mean-squared error."}, {"input_sequence": "Author: Junjie Hu and Isabell Fetzer and Lucas Umann ; \"\"\"\nThis is a multi-line comment block. It is set in-between six quotation marks.\nPython provides a straight way for numerical operations.\nTry the basics:\n+, -, *, /, %, **, //\n\"\"\"\n# This is a one-line comment in Python. It extends to the end of the physical line.\na = 5 # initiates integer variable a which the value 5 is assigned to\nprint(a)\nb = 3 # another integer variable b which the value 3 is assigned to\nprint(b)\nb = 5 # the old value of b is overwritten by 5\na, b = 7, 3 # initialising two variables at once: a takes the value 7; b takes 3\nprint(a,b)\na += 1 # short for a = a+1 so again a new value is assigned to a\n\"\"\"\nPython provides versatile set of functions for variables.\nprint(a) # the fct. print() out the value of a; the output is 8\nc = round(a / b) # the fct. round() 8/3 and assigns the output to c\nprint(c)\n# Function can also take arguments, which allow us further specifications.\nc = round(a / b, ndigits=4) # ndigits specifies the number of decimal places\nprint(c) # output is 2.6667\nc = round(a / b, 3) # arguments do not need to be spelled out, if placed correctly\n\"\"\"\nString is a basic type in python, it's commonly used and very powerful.\nw = 'Hello' # initiates string variables by single quotation marks\nx = \"World\" # \u2026 or by double quotation marks\nprint(w + x) # HelloWorld\ny = '20' # numbers can also be assigned as strings too\ne = '10'\nprint(y + e) # concatenates strings so output is \u20182010\u2019\nf = int(y) + int(e) # the fct. int() converts strings to integers, f is 30 now\nprint(f)\n# formatting strings\n# using f '\u2026.' string, you can write variable names inside the brackets {}, directly.\ngreeting = f'{w} Python-{x}.' # Hello Python-World.\nprint(greeting)\ngreeting = f'{w} {x.upper()}.' # Hello WORLD.\nPython provides a straight way for numerical operations. Try comparison operations ==,<,<=,>,>=,!=\na = 5\nprint(a == 5) # checks if a equals 5; output will be True\nprint(a > 5) # False\na = 'A' # old integer value of a (which is 7) is overwritten by string value \u2018A\u2019\nprint(a == b) # False\nList is a versatile Python data type to group values.\nIn Python a list is created by placing all the items inside square brackets [] , separated by commas.\nmy_list = [] # initiates an empty list, which we choose to call \u201cmy_list\u201d\nmy_list = [2, 3, 5, 7, 11] # adds elements to the list\nprint(my_list) # output: [2, 3, 5, 7, 11]\n# indexing\nprint(my_list[0]) # the first element of my_list is called, which is 2\n# slicing\nprint(my_list[:2]) # calls the first two elements of my_list [2, 3]\n# appending\nmy_list.append(13) # [2, 3, 5, 7, 11, 13]\nprint(my_list)\nmy_list.extend([17, 19]) # [2, 3, 5, 7, 11, 19]\nLists can contain different types, e.g. strings, numbers,\nfunctions, lists, \u2026\nmy_list = ['Welcome', 'to', 'Python'] # adds three string elements to the list\nprint(my_list[0]) # Welcome\nprint(my_list) # [\u2018Welcome', 'to', \u2018Python']\n# using for loop to iterate all elements in the list:\nfor word in my_list: # word we choose randomly, any other name works too\nFunctions can help when working with lists.\n# to print it out in one line we need to apply the fct join() to our list:\nlist_as_sentence = '*'.join(my_list) # sets * as separators\nprint(list_as_sentence) # Welcome*to*Python\nlist_as_sentence = ' '.join(my_list) # sets white spaces as separators\nprint(list_as_sentence) # Welcome to Python\n# Try to execute this code and see what happens.\nlist_sliced = list_as_sentence[0:12] # slices string by indices\nprint(f'{list_sliced}.') # Welcome to P.\n# Try changing the indices to negative.\nsentence_big = list_as_sentence.upper()\nChecking for possible operations:\n# by using dir() function, or help() function; you may see all ops on the str object\n# or a str instance\n# likewise,\nDictionaries are used to store data values in key:value pairs. A dictionary is a collection which is unordered, changeable and does not allow duplicates.\ncourse = dict(name='DEDA', unit=0) # {'name': 'DEDA', 'unit': 0}\nprint(course)\ncourse = {'name': 'DEDA', 'unit': 0} # alternative\n# accessing\nprint(course['unit']) # 0\n# assigning\ncourse['unit'] = 1\nprint(course['unit'])\n# get keys\nprint(course.keys()) # ['name', 'unit']\n# adding values\ncourse.update({'lecturers': ['Chen', 'H\u00e4rdle']})\n# {'name': 'DEDA', 'unit': 1, 'lecturers': ['Chen', 'H\u00e4rdle']}\nControl Flow Tools: if/elif/else.\nComparison statements can be used.\nx = 10 # initialising a numeric variable x\nif x < 0: # checking for the value of x\nprint('Negative Value') # note the four additional level of indentation (4 spaces)\nelif x == 0:\nprint('Zero')\nelse:\nControl Flow Tools: if/elif/else\n# Conditions can be combined or altered with: and, or, not, is, not\np = [2, 3, 5, 7, 11] # initialising a list p\nprint(p)\nprint(3 in p) # True\nprint(3 in p and 4 in p) # False\n# Empty/Missing Values can be initialised by the term \u201cNone\u201d\ny = None # initialising an empty variable x\nif y is not None: # alternatively: if not y is None: \u2026.\nprint('Value is not None')\nFor Loop can iterate over all elements. The i is a self-chosen variable to use to represent the current increment in a loop.\nl = list([1, 2, 3, 4, 5])\nfor i in l:\nprint(i * 2, end=' ') # output: 2 4 6 8 10\nfor i in range(6): # iterates from 0 to 5\nif i == 3:\ncontinue # skip 3*2 in output\nprint(i * 2, end=' ') # output: 0 2 4 8 10\nfor i in 'DEDA':\nprint(i, end=' ') # output: D E D A\nd = dict(a=1, b=2)\nfor k, v in d.items():\nprint('\\n{} has value {}'.format(k, v))\n# a has value 1\nWith the while loop we can execute a set of statements as long as a condition is true. ATTENTION: Make sure not to have infinite loop with tautology in condition).\n# Fibonacci series: sum of two preceding numbers defines next number\na, b = 0, 1\nwhile b < 100:\nprint(b, end=' ')\na, b = b, b + a\nThe < break > statement in Python terminates the current loop and resumes execution at the next statement.\nfib = [0, 1] # a list called fib\nwhile True:\n# appends the sum of the last 2 elements of our list to our new list\nfib.append(sum(fib[-2:]))\nif fib[-1] > 100:\nbreak\nDefine your own function using def, followed by a name, parameters in parentheses (), a colon, and a block of code.\ndef my_first_fct():\nprint('This is my first function.') # remember the four indentation\nmy_first_fct() # call the function\n# another function with two parameter m and n\ndef my_second_fct(m, n):\nmn = m % n\nreturn print(f'The remainder of the devision of {m} by {n} is {mn}.')\nmy_second_fct(4, 3) # call the function and pass the variables m = 4 and n = 3\nAnother example for functions\ndef square_numeric(x): # Squares numeric x\nreturn x ** 2\ndef square_iterable(x): # Squares numerics in iterable x\nlist = []\nfor i in x:\nlist.append(square_numeric(i))\nreturn list\ndef square_iterabel_short(x): # Squares numerics in iterable x\nreturn [square_numeric(i) for i in x]\nx = [1, 2, 3, 4, 5]\nsquare_iterable(x) # [1, 4, 9, 16, 25]\nWiggling Elephant Trunk:\nvonNeuman_elephant.py\n\"With four parameters I can fit an elephant,\nand with five I can make him wiggle his trunk.\"\nOriginal Versions:\nAuthor[1]: Piotr A. Zolnierczuk at ornl dot gov)\nRetrieved on 14 September 2011 from\nhttp://www.johndcook.com/blog/2011/06/21/how-to-fit-an-elephant/\nModified to wiggle trunk:\n2 October 2011 by David Bailey (http://www.physics.utoronto.ca/~dbailey)\nAuthor[2]:\nAdvanced Physics Laboratory\nhttps://www.physics.utoronto.ca/~phy326/python/\nBased on the paper:\n\"Drawing an elephant with four complex parameters\", by\nJurgen Mayer, Khaled Khairy, and Jonathon Howard,\nAm. J. Phys. 78, 648 (2010), DOI:10.1119/1.3254017\nThe paper does not specify how the wiggle parameter controls the\ntrunk, so a guess was made.\nInspired by John von Neumann's famous quote (above) about overfitting data.\nAttributed to von Neumann by Enrico Fermi, as quoted by\nFreeman Dyson in \"A meeting with Enrico Fermi\" in\nNature 427 (22 January 2004) p. 297\nPython Version: 3.6\nModified based on author[2]'s work\nAuthor: Junjie Hu\nOverfiting problem in trading strategy\nBailey, D., Borwein, J., Lopez de Prado, M., & Zhu, Q. (2014).\nPseudo-mathematics and financial charlatanism: The effects of backtest overfitting on out-of-sample performance.\n# import matplotlib\n# matplotlib.use('TKAgg')\nfrom matplotlib import animation\nfrom numpy import append, cos, linspace, pi, sin, zeros\nimport matplotlib.pyplot as plt\nfrom IPython.display import HTML\n# PLEASE NOTE IN SPYDER YOU SHOULD DISABLE THE ACTIVE SUPPORT in PREFs\n# elephant parameters\nparameters = [50 - 30j, 18 + 8j, 12 - 10j, -14 - 60j, 20 + 20j]\n# patrick's happy spermwhale\n# parameters = [30 - 10j, 20 + 20j, 40 + 10j, 20 - 50j, -40 + 10j]\n# philipp's flying swan\n# parameters = [1 - 2j, 9 + 9j, 1 - 2j, 9 + 9j, 0 + 0j]\n# kathrin's hungry animal\n# parameters = [50 - 50j, 30 + 10j, 5 - 2j, -5 - 6j, 20 + 20j]\n# anna\u2019s happy hippo\n# parameters = [50 - 15j, 5 + 2j, -10 - 10j, -14 - 60j, 5 + 30j]\n# fabio\u2019s bird with right wing paralysis\n# parameters = [50 - 15j, 5 + 2j, -1 - 5j, -14 - 60j, 18 - 40j]\n# for pea shooter see code below\ndef fourier(t, C):\nf = zeros(t.shape)\nfor k in range(len(C)):\nf += C.real[k] * cos(k * t) + C.imag[k] * sin(k * t)\nreturn f\ndef elephant(t, p):\nnpar = 6\nCx = zeros((npar,), dtype='complex')\nCx[1] = p[0].real * 1j\nCy[1] = p[3].imag + p[0].imag * 1j\nCx[2] = p[1].real * 1j\nx = append(fourier(t, Cy), [p[4].real])\nreturn x, y\ndef init_plot():\n# draw the body of the elephant & create trunk\nx, y = elephant(linspace(2.9 * pi, 0.4 + 3.3 * pi, 1000), parameters)\nfor ii in range(len(y) - 1):\ny[ii] -= sin(((x[ii] - x[0]) * pi / len(y))) * sin(float(0)) * parameters[4].real\ntrunk.set_data(x, y)\nreturn trunk,\ndef move_trunk(i):\n# move trunk to new position (but don't move eye stored at end or array)\ny[ii] -= sin(((x[ii] - x[0]) * pi / len(y))) * sin(float(i)) * parameters[4].real\nfig, ax = plt.subplots()\n# initial the elephant body\nx, y = elephant(t=linspace(0.4 + 1.3 * pi, 2.9 * pi, 1000), p=parameters)\nplt.plot(x, y, 'b.')\nplt.xlim([-75, 90])\nplt.axis('off')\ntrunk, = ax.plot([], [], 'b.') # initialize trunk\nani = animation.FuncAnimation(fig=fig,\nfunc=move_trunk,\nani\nHTML(ani.to_html5_video())\n# Uncomment if you would like to save video externally\nWriter = animation.writers['ffmpeg']\nmetadata = dict(title='Elephant Trunk Wiggling', artist='Jenny Beth\u00e4user')\nwriter = Writer(fps=30, metadata=metadata, bitrate=1800)\nani.save(filename='bulldog_trunk_wiggle.mp4', writer=writer)\nplt.show()\nWentian\u2019s pea shooter:\nand with five I can make him wiggle his trunk.\"\nThe paper does not specify how the wiggle parameter controls the\ntrunk, so a guess was made.\n\n# for peashooter:\nyou might want to use the following in terminal if the graphviz does not work:\nconda install -c conda-forge ffmpeg\nAll should be fine though if you use jupyter notebook\nparameters = [50 - 50j, 18 + 80j, 12 - 10j, -14 - 60j, 20 + 20j]\ndef peashooter(t, p):\nx, y = peashooter(linspace(2 * pi + 0.9 * pi, 0.4 + 3.3 * pi, 1000), parameters)\nplt.xlim([-175, 190])\ntrunk, = ax.plot([], [], 'b.')\ninterval=500,\n# Video will be externally saved\nmetadata = dict(title='Wentians pea shooter')\nani.save(filename='peashooter.mp4', writer=writer)\n", "output_sequence": "UNIT 1 of DEDA. Introduce basic syntax, such as numeric and string, and basic data structure, like list, tuple, set and dict in Python"}, {"input_sequence": "Author: Natalie Habib ; #!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n'''\nLast Modified on Mon July 07 2018\n@author:\nNatalie\n@description:\nDigital Economy and Decision Analytics\nProject IRTG 1792 Guest Matching\n\nThe first step in the project 'IRTG 1792 Guest Matching' is to retrieve the data about former and current research guests in the IRTG 1792 program from\nthe two websites\nhttps://www.wiwi.hu-berlin.de/de/forschung/irtg/guests/former-guests\nFirstly, the function download(url, num_retries) download the HTML-content from the one website and parse the HTML-content to a dataframe respectively\nfor former and current researchers.\nThe function concatCurrentFormerDf() concatenates the dataframes of current and former researchers to one dataframe, which is the input of the function\nwriteCSV(df, name). The result is one csv-File with all researchers at 'HOME/IRTG/data/researchers_all_[today-data].csv' .\n@copyright:\n# import custom webscraping class 'scrape' from WebScraping.scrape\nfrom WebScraping.scrape import scrape\nprint('--- Start of Web-Scrpaing --- \\n')\n# url: target website\nirtg_former_url = 'https://www.wiwi.hu-berlin.de/de/forschung/irtg/guests/former-guests'\n# scrape guest reserachers from wiwi.hu-berlin.de\nprint('Start scraping .. \\n')\nscrp = scrape(current_url=irtg_current_url, former_url=irtg_former_url)\nscrp.download(url=scrp.former_url, num_retries=2)\n# save scraped data to csv-file\nscrp.concatCurrentFormerDf()\nscrp.writeCSV(df=scrp.all_researcher_df, name='all')\nprint('--- End of Web-Scraping. ---')\n# MainWebScraping.py\n", "output_sequence": "Scarping from IRTG webpage to obtain researchers information for further analysis"}, {"input_sequence": "Author: Sophie Burgard ; #open dataset, set working directoy if requires\n#setwd(....)\navg_emo = read.csv2('ECB_avg_emo.csv')\n#select relevant columns. here: ignore 'neutral'\nlevels = avg_emo[, c(2:6,8,9)]\n#transpose and redefine as matrix\nlevels = t(as.matrix(levels))\n#set color scheme for each emotion\ncols = c('steelblue', #anger\n'red', #contempt\n'khaki1', #disgust\n'springgreen3', #fear\n'tomato1', #happiness\n'darkblue') #surprise\n#generate graphic\ndev.off() # delete old graphical parameters\n#general setting\nm = barplot(levels,\nmain = 'Average level of emotion per ECB press conference',\ncol = cols,\nxaxt = 'n') #deactivate automatic axes labeling\n#legend specifications\nlegend('bottom',\nfill = cols,\nc('anger', 'contempt', 'disgust', 'fear', 'happiness', 'surprise'),\nhoriz = TRUE,\ninset = c(0, -0.4),\nbty = 'n')\n#add box around plot\nbox(which = 'plot', lty = 'solid')\n#preparation for x-axis labeling\nevent.plot = avg_emo$event\nevent.plot = as.character(event.plot)\nevent.plot = as.data.frame(paste0('20', event.plot))\nnames(event.plot) = 'event'\nevent.plot$event = as.character(event.plot$event)\nevent.plot$event = paste0(substr(event.plot$event, 1, 4), '-', substr(event.plot$event, 5, 6))\nevent.plot = append(append(event.plot[1:61,], '2016-09'), event.plot[62:69,])\n# add to x-axis tick and labeling\naxis(side = 1,\nat = m[c(1, 10, 70)],\n", "output_sequence": "Plots scores for seven emotions (anger, contempt, disgust, fear, happiness, sadness, surprise) generated by Microsoft cognitive API for 70 press conferences held by the European Central Bank (Jan. 2011 - Sep 2017)."}, {"input_sequence": "Author: Sophie Burgard ; path = '...'\nimport os\nos.chdir(path)\nos.getcwd()\nimport numpy as np\nimport pandas as pd\nimport keras\nfrom scipy.misc import imread\nfrom sklearn.metrics import accuracy_score\nfrom keras.layers import Input, Convolution2D, MaxPooling2D, Flatten, Dense, InputLayer\nfrom keras.models import model_from_json, Sequential\n# To stop potential randomness\nseed = 128\nrng = np.random.RandomState(seed)\n#load data, requires function load_dataset() from Q-let: FVCload_dataset\n#and original data from Kaggle's Facial Expression Recognition Competition 2013\nfrom FVCload_dataset import load_dataset\n(x_train, (x_test, (x_valid, = load_dataset()\n#reshape data;\n#x_train, x_test have inverted names from now on\nx_train = x_train.reshape(-1, 48, 1)\n#normalize\nx_train /= 255.0\n#one-hot-encoding\ny_train = keras.utils.np_utils.to_categorical(y_train)\n# define variables\ninput_shape = (2304,)\ninput_reshape = (48, 1)\n#filter sizes\nconv_num_filters = 5\n#pooling size\npool_size = (2, 2)\n#number of units\nhidden_num_units = 1024\noutput_num_units = 7\n#iterations\nepochs = 10\n#size of each mini batch\nbatch_size = 125\n#model definition\nmodel = Sequential()\nmodel.add(InputLayer(input_reshape))\nmodel.add(MaxPooling2D(pool_size = (2, 2), strides = None))\nmodel.add(Convolution2D(64, (3, 3), activation='relu'))\n#strides: Integer, or None (def). Factor by which to downscale. E.g. 2 will halve the input.\n#If None, it will default to pool_size.\nmodel.add(MaxPooling2D(pool_size = (2,2), strides = None))\nmodel.add(Convolution2D(128, (3, 3), activation='relu'))\nmodel.add(Flatten())\n#output_dim = units in keras 2 API\nmodel.add(Dense(units = hidden_num_units, activation='relu'))\nmodel.add(Dense(units = output_num_units, input_dim = hidden_num_units, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\ntrained_model_conv = model.fit(x_train, y_train, nb_epoch=epochs, batch_size=batch_size, validation_data=(x_test, y_test))\n# serialize model to JSON\nmodel_json = model.to_json()\nwith open(\"model_CNN.json\", \"w\") as json_file:\njson_file.write(model_json)\n# serialize weights to HDF5\nmodel.save_weights(\"weights_CNN.h5\")\nprint(\"Saved model to disk\")\n\n# for reloading model:\n# load json and create model\njson_file = open('model_CNN.json', 'r')\nloaded_model_json = json_file.read()\njson_file.close()\nloaded_model = model_from_json(loaded_model_json)\n# load weights into new model\nloaded_model.load_weights(\"weights_CNN.h5\")\nprint(\"Loaded model from disk\")\n# evaluate loaded model on test data\nloaded_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nscore = loaded_model.evaluate(x_train, y_train, verbose=0)\nprint(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\n", "output_sequence": "Fits an Convolutional Neural Network that classifies facial expressions into seven basic emotions (anger, disgust, fear, happiness, neutral, sadness, surprise) using the Keras library for python. The training and testing dataset can be obtained from kaggle's 'Facial Expression Recognition Challenge'. The subfolder 'model weights' contains model weights and model architecture in .json-Format. Allows to access the estimation results without rerunning the calculations."}, {"input_sequence": "Author: Elizaveta Zinovyeva, Raphael Constantin Georg Reule ; import numpy as np\nimport pandas as pd\nfrom datetime import date\nfrom bq_helper import BigQueryHelper\n# Helper object for BigQuery Ethereum dataset\neth = BigQueryHelper(active_project=\"bigquery-public-data\",\nquery = \"\"\"\nWITH contracts_in_block as (\nSELECT\nblock_timestamp as timestamp,\ncount(*) as count\nFROM\n`bigquery-public-data.crypto_ethereum.transactions`\nWHERE\nto_address is null -- contract creation indicator\nGROUP BY\n1\n)\nSELECT\n*\nFROM\ncontracts_in_block\nORDER BY\n1 asc\n# Estimate total data scanned for query (in GB)\n# Store the results into a Pandas DataFrame\n# Convert strings into datetime objects and resample the data to monthly totals\ndf['date'] = pd.to_datetime(df.timestamp)\nagg.plot(y='count', legend=False, figsize=(16, 9))# title='New Ethereum Smart Contracts (monthly totals)',\n", "output_sequence": "This Quantlet is dedicated to visualization of time-series of the amount of Smart contracts created since the invention of Ethereum"}, {"input_sequence": "Author: Elizaveta Zinovyeva, Raphael Constantin Georg Reule ; import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndf = pd.read_csv(\"./data/contracts_over_time_2020-12-09.csv\")\ndf['date'] = pd.to_datetime(df.timestamp)\nagg = df.resample('M', on='date').sum()\ncums = agg['count'].cumsum()\ncontracts = contracts.loc[4:]\nfig, ax1 = plt.subplots(figsize=(20,10))\ncolor = '#ef476f'\n#ax1.set_xlabel('date')\nax1.set_ylabel('New contracts', color=color,fontsize=20)\nax1.plot(agg['count'], color=color)\nax1.tick_params(axis='y', labelcolor=color)\nplt.xticks(fontsize=20)\nax2 = ax1.twinx() # instantiate a second axes that shares the same x-axis\ncolor = '#19751C'\nax2.set_ylabel('Total contracts', color=color,fontsize=20) # we already handled the x-label with ax1\nax2.plot(cums, color=color)\nax2.tick_params(axis='y', labelcolor=color)\ncolor = '#14213d'\nax1.plot(agg['dapps'], color=color)\nfig.tight_layout()\ndf_t = pd.read_csv(\"./data/transactions_over_time_2020-12-09.csv\")\ndf_t['date'] = pd.to_datetime(df_t.timestamp)\nagg_t = df_t.resample('M', on='date').sum()\ncolor = '#118ab2'\nax1.plot(agg_t['count'], color=color)\nax2 = ax1.twinx()\ncolor = '#073b4c'\nax2.plot(cums_t, color=color)\n", "output_sequence": "This Quantlet is dedicated to visualization of time-series of the amount of Smart contracts created since the invention of Ethereum"}, {"input_sequence": "Author: Elizaveta Zinovyeva, Raphael Constantin Georg Reule ; import re\nimport pandas as pd\nimport numpy as np\nimport pickle\nimport os\n#https://stackoverflow.com/questions/2319019/using-regex-to-remove-comments-from-source-files\n#https://github.com/HektorLin/HU-IRTG/blob/master/Partition%20Codes%20and%20Comments.ipynb\ndef remove_comments(string):\npattern = r\"(\\\".*?\\\"|\\'.*?\\')|(/\\*.*?\\*/|//[^\\r\\n]*$)\"\n# first group captures quoted strings (double or single)\n# second group captures comments (//single-line or /* multi-line */)\nregex = re.compile(pattern, re.MULTILINE|re.DOTALL)\ndef _replacer(match):\n# if the 2nd group (capturing comments) is not None,\n# it means we have captured a non-quoted (real) comment string.\nif match.group(2) is not None:\nreturn \"\" # so we will return empty to remove the comment\nelse: # otherwise, we will return the 1st group\nreturn match.group(1) # captured quoted-string\nreturn regex.sub(_replacer, string)\n#following the example above, distinguish group(1): \"\" or'' and group(2,true comments): \\*.*\\ or //\n#return when a true comment is located\ndef leave_only_comments(string):\ncomments = [x[1] for x in regex.findall(string)]\ncomments = ' '.join(comments)\nfiles = os.listdir('data/sol_source_open_source')\nfor file in tqdm(files):\ncontract_hash = file.split('_')[0]\nname = file.split('_')[1].split('.')[0]\nwith open(f'data/sol_source_open_source/{file}', 'r', encoding=\"utf8\") as readf:\ncontract = readf.read()\ndf_temp = pd.DataFrame(data={'hash':contract_hash,\n'name_from_SC': name,\n'SC_no_comments': remove_comments(contract),\ndf = pd.concat([df, df_temp])\ndf_additional_info = pd.read_csv('data/dapps_open_source.csv')\ndf = df.merge(df_additional_info, how='left', left_on='hash',\n", "output_sequence": "This Quantlet is dedicated to gathering data on the open source source codes of Solidity Smart Contracts. First, the Ethereum Dapps names are obtained using the API https://www.stateofthedapps.com/. For each DApp the list of smart contract hashes were obtained if available (Also through the API of state of the Dapps). And followingly using the API of https://etherscan.io/, the source codes (if verified) are obtained for the open source Dapps. Finally, the codes are parsed and preprocessed to extract comments and merged with the category from the State of the Dapps and stored as a csv file."}, {"input_sequence": "Author: Elizaveta Zinovyeva, Raphael Constantin Georg Reule ; import requests\nimport pandas as pd\nimport numpy as np\nimport sys\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\ncontract = contract[contract.license!='Proprietary']\ncontract = contract[contract.license!='Proprietary (May change later as we publish)']\ncontract = contract[contract.license!='Company Registration']\ncontract = contract[-contract.license.str.contains('agpl', case=False)]\nos_contracts = pd.concat([mit_contract, open_contract, gnu_contract])\nos_contracts = os_contracts[-os_contracts.license.str.contains('proprietary', case=False)]\nos_contracts = os_contracts[-os_contracts.license.str.contains('Towards OpenSource/MIT, but work in progress', case=False)]\n# contants\nwith open(\"token.txt\") as file:\nsol_source = 'data/sol_source_open_source'\nif not os.path.exists(sol_source):\nos.mkdir(sol_source)\n\nif not os.path.exists(not_verified):\nfiles_in_sol = os.listdir(sol_source)\nfiles_in_sol = [i.split('_')[0] for i in files_in_sol]\nfiles_in_not_ver = os.listdir(not_verified)\nfiles_in_not_ver = [i.split('_')[0] for i in files_in_not_ver]\nfiles_in_sol.extend(files_in_not_ver)\ndef scrape_ether_contract_and_write(address_array, API_Token):\nc = 0\nfor i, address in tqdm(enumerate(address_array)):\n# time.sleep(0.01) # we can do 5 GET/POST requests per sec\nurl = f'https://api.etherscan.io/api?module=contract&action=getsourcecode&address={address}&apikey={API_Token}'\nresp = requests.get(url=url)\ndata = resp.json()\ntry:\ncontract_name = data['result'][0]['ContractName']\n# save solidity source code\nwith open(f\"{sol_source}/{address}_{contract_name}.sol\", \"w\", encoding=\"utf-8\") as f:\nprint(data['result'][0]['SourceCode'], file=f)\nexcept:\nc += 1\ntime.sleep(0.25)\n\n", "output_sequence": "This Quantlet is dedicated to gathering data on the open source source codes of Solidity Smart Contracts. First, the Ethereum Dapps names are obtained using the API https://www.stateofthedapps.com/. For each DApp the list of smart contract hashes were obtained if available (Also through the API of state of the Dapps). And followingly using the API of https://etherscan.io/, the source codes (if verified) are obtained for the open source Dapps. Finally, the codes are parsed and preprocessed to extract comments and merged with the category from the State of the Dapps and stored as a csv file."}, {"input_sequence": "Author: Elizaveta Zinovyeva, Raphael Constantin Georg Reule ; import pandas as pd\nimport requests\nimport numpy as np\nimport ast\nfrom datetime import date\nwith open('address.txt', 'r') as file:\naddress = file.read()\n\naddress_to_get_all = address + '?platform=ethereum&limit=100&page='\ndata = pd.DataFrame(columns=['category',\n'categories',\nfor page in range(1, 21):\ntest = requests.get(f'{address_to_get_all}{page}')\nfor i, item in enumerate(test.json()['items']):\ncategory = test.json()['items'][i]['categories']\nif len(category)>1:\ncategories = ' '.join(category)\nelse:\ncategories = 0\ncreated = test.json()['items'][i]['created']\ndf_temp = pd.DataFrame(data={'category': category,\n'categories':\ndata = pd.concat([data, df_temp])\naddress_for_dapp = f'{address}/'\ndata.reset_index(drop=True, inplace=True)\nfor i, slug in enumerate(data.slug):\nslug = slug\ntest = requests.get(f'{address_for_dapp}{data.slug[i]}')\nc = test.json()['item']['contractsMainnet']\nif len(c)==0:\ndescription = test.json()['item']['description']\ncontract_hash = 0\nlicense = test.json()['item']['license']\ndf_temp = pd.DataFrame(data={'slug': slug,\n'hash': contract_hash,\n'license': index=[0])\nelif len(c)==1:\ncontract_hash = c[0]\n'license': index=[0])\nelif len(c)> 1:\ndf_temp = pd.DataFrame(columns=['slug', 'hash', 'description', 'license'])\nfor contract in c:\ndescription = test.json()['item']['description']\ncontract_hash = contract\nlicense = test.json()['item']['license']\ndf_t = pd.DataFrame(data={'slug': slug,\n'hash': contract_hash,\ndf_temp = pd.concat([df_temp, df_t])\nhashes = pd.concat([hashes, df_temp])\nhashes.reset_index(drop=True, inplace=True)\n#hashes.to_csv(f'data/hashes_with_licenses_{today}.csv', index=False)\ndapps_df = hashes.merge(data, how='left', left_on='slug',\n", "output_sequence": "This Quantlet is dedicated to gathering data on the open source source codes of Solidity Smart Contracts. First, the Ethereum Dapps names are obtained using the API https://www.stateofthedapps.com/. For each DApp the list of smart contract hashes were obtained if available (Also through the API of state of the Dapps). And followingly using the API of https://etherscan.io/, the source codes (if verified) are obtained for the open source Dapps. Finally, the codes are parsed and preprocessed to extract comments and merged with the category from the State of the Dapps and stored as a csv file."}, {"input_sequence": "Author: Elizaveta Zinovyeva, Raphael Constantin Georg Reule ; import pandas as pd\nimport numpy as np\nnames = pd.read_csv('data/dapps_names_2020-12-09.csv')\ndf.created = pd.to_datetime(df.created)\ncontracts_per_timepoint = pd.DataFrame(df.resample('M', on='created').count()['slug'])\ndf_grouped = df.groupby('slug', as_index=False)['hash'].count()\ndf_grouped = df_grouped.merge(df[['slug', 'created']], on='slug')\ndf_grouped.drop_duplicates(inplace=True)\nmonthly_mean = df_grouped.set_index('created').resample('1M').mean()\nwithout_sc = names[names.slug.isin(set(names.slug) - set(df.slug))]\nwithout_sc.created = pd.to_datetime(without_sc.created)\nwithout_sc_grouped_year = without_sc.resample('M', on='created').count().drop(['created', 'name', 'rank', 'slug', 'teaser'], axis=1)\nwithout_sc_grouped_year = without_sc_grouped_year.join(monthly_mean, how='left')\nsc_from_dapps = pd.DataFrame(contracts_per_timepoint['slug'] + without_sc_grouped_year['hash'])\n", "output_sequence": "This Quantlet is dedicated to gathering data on the open source source codes of Solidity Smart Contracts. First, the Ethereum Dapps names are obtained using the API https://www.stateofthedapps.com/. For each DApp the list of smart contract hashes were obtained if available (Also through the API of state of the Dapps). And followingly using the API of https://etherscan.io/, the source codes (if verified) are obtained for the open source Dapps. Finally, the codes are parsed and preprocessed to extract comments and merged with the category from the State of the Dapps and stored as a csv file."}, {"input_sequence": "Author: Yoshiro Yamamoto ; # clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# install and load packages\nlibraries = c(\"twitteR\", \"ROAuth\", \"base64enc\", \"dplyr\", \"ggplot2\",\n\"RMeCab\", \"wordcloud\", \"RColorBrewer\", \"igraph\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# pre install RMeCab for Win\n# install.packages(\"RMeCab\", repos = \"http://rmecab.jp/R\")\n# Twitter REST API\n# GET search/tweets\n# https://syncer.jp/twitter-api-matome/get/search/tweets\n#\n# Please change following keys into your own twitter Apps key\nAPIKey = \"6uSqLddicwPWF1C5qiLYgFxii\"\nAPISecret = \"o8mKGWMOXJpGqwR2F5rH12iWXf1g6WsZlP7lROB8fSD5t8IIeD\"\naccessToken = \"74756179-Qbtef8SRTpEKhoYif7tygbTOeeSp1ApaOibPPsvqm\"\naccessSecret = \"prbXalyRt82kYIzYaxZQboyNSCXTvv2iL2x6tQQDSr0wr\"\nsetup_twitter_oauth(APIKey, APISecret, accessToken, accessSecret)\n# select 1:\n# change the key word(s) you want to search\nsearchword = \"????????? ???\"\nsearchquery = iconv(paste0(searchword,\" AND -filter:links AND -RT\"), to=\"UTF-8\")\ntw.df = twListToDF(searchTwitter(searchquery,\nsince=as.character(Sys.Date()-8),\nnames(tw.df)\n# totalize of tweets\n# total number of tweets for each day\ntw.daily = tw.df %>%\nmutate(twdate=as.Date(created)) %>%\ngroup_by(twdate) %>% summarize(cnt = n())\ntw.daily\nggplot(tw.daily, aes(twdate,cnt))+ geom_bar(stat=\"identity\")\n# PDF output\n#cairo_pdf(\"tw-daily.pdf\", width=8, height=8, family=\"MixMix 1P\")\n#ggplot(tw.daily, aes(twdate,cnt))+ geom_bar(stat=\"identity\")+ #theme_bw(base_size=18)\n#dev.off()\n# total number of tweets for each day and hour\ntw.hourly = tw.df %>%\nmutate(twhour=as.POSIXct(format(created, \"%Y-%m-%d %H:00:00\"))) %>%\ngroup_by(twhour) %>% summarize(cnt = n())\ntw.hourly\nggplot(tw.hourly, aes(twhour, cnt))+ geom_bar(stat=\"identity\")\n#cairo_pdf(\"tw-hourly.pdf\", width=8, height=8, family=\"MixMix 1P\")\n#ggplot(tw.hourly, aes(twhour, cnt))+ geom_bar(stat=\"identity\", #fill=I(\"#666666\")) +\n# theme_bw(base_size=18)\n# Preprocessing (\"RMeCab\" package)\ntw.txt = unique(tw.df$text)\ntw.txt = gsub(\"[[:print:]]\", \"\", tw.txt, perl=TRUE)\ntw.txt = iconv(tw.txt, from=\"UTF-8\", to=\"CP932\", \"\") # for Windos (SHIFT-JIS)\ntw.txt = tw.txt[-grep(\"^RT\", tw.txt)]\ntw.dmat = docMatrixDF(tw.txt, pos = c(\"??????\"))\ndim(tw.dmat)\ntw.wcnt = as.data.frame(apply(tw.dmat, 1, sum))\ntw.wcnt = tw.wcnt[\n!(row.names(tw.wcnt) %in% unlist(strsplit(searchword, \" \"))),\n1, drop=FALSE]\ntw.wcnt2 = data.frame(word=as.character(row.names(tw.wcnt)),\nfreq=tw.wcnt[,1])\ntw.wcnt2 = subset(tw.wcnt2, rank(-freq)<25)\nggplot(tw.wcnt2, aes(x=reorder(word,freq), y=freq)) +\ngeom_bar(stat=\"identity\", fill=\"grey\", color=\"black\") +\ntheme_bw(base_size=20) + coord_flip() + xlab(\"word\")\n#cairo_pdf(\"tw-wordcount.pdf\", family=\"MigMix 1P\", width=12, height=8)\n#ggplot(tw.wcnt2, aes(x=reorder(word,freq), y=freq)) +\n# geom_bar(stat=\"identity\", fill=\"grey\", color=\"black\") +\n# theme_bw(base_size=20) + coord_flip() + xlab(\"word\")\n# word cloud (\"wordcloud\" package)\ntw.wcnt = subset(tw.wcnt, tw.wcnt[, 1] >= 30)\npal = brewer.pal(8,\"Dark2\")\nwordcloud(row.names(tw.wcnt), tw.wcnt[, 1], scale = c(4, .2),\nrandom.order = T, rot.per = .15, colors = pal)\n#cairo_pdf(\"tw-wordcloud.pdf\", family=\"Meiryo\", width=8, height=8)\n#wordcloud(row.names(tw.wcnt), tw.wcnt[, 1], scale = c(4, .2),\n# random.order = T, rot.per = .15, colors = pal)\n# Network analysis (\"igraph\" package)\ntw.file = tempfile()\nwrite(gsub(\"\\n\", \"\", tw.txt), file=tw.file)\ntw.bigram = NgramDF(tw.file, type = 1, N = 2,\nc(\"??????\",\nsortlist = order(tw.bigram[,3],decreasing = TRUE)\ntw.bigram = tw.bigram[sortlist,]\ntw.bigram = subset(tw.bigram, Freq>20)\nhead(tw.bigram)\ntw.graph = graph.data.frame(tw.bigram)\n# selecting comunity\neb = edge.betweenness.community(tw.graph)\n# Output network diagram\nplot(tw.graph, vertex.label=V(tw.graph)$name,\nvertex.label.family=\"Meiryo\",\nvertex.size=3*log(degree(tw.graph)),\nvertex.color=cut_at(eb, 10), edge.arrow.size=0.1,\nvertex.label.cex=1, edge.arrow.width=1)\n#cairo_pdf(\"tw-network.pdf\", family=\"Meiryo\", width=8, height=8)\n#plot(tw.graph, vertex.label=V(tw.graph)$name,\n# vertex.label.family=\"Meiryo\",\n# vertex.size=3*log(degree(tw.graph)),\n# vertex.color=cut_at(eb, 10), edge.arrow.size=0.1,\n# vertex.label.cex=1, edge.arrow.width=1)\n", "output_sequence": "Textmining and visualization for tweets in Japanese"}, {"input_sequence": "Author: Marlene Mueller ; # clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# install and load packages\nlibraries = c(\"KernSmooth\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# load data\nx = read.csv(\"fes76.txt\", sep = \";\", dec = \".\")\nx = x[, c(\"NIC\", \"FOO\")]\nx = x[order(x$NIC), ]\n# plot\nplot(x, col = \"lightblue\", pch = 20, main = \"Engel Curve\") # Plot data points\nmh = locpoly(x$NIC, x$FOO, degree = 0, kernel = \"normal\", bandwidth = 20) # Nadaraya-Watson regression\nlines(mh, lwd = 2)\nmh1 = locpoly(x$NIC, x$FOO, degree = 1, kernel = \"normal\", bandwidth = 20) # Local linear regression\nlines(mh1, col = \"blue\", lwd = 2)\nlegend(\"topright\", c(\"NadWat\", \"LocLin\"), col = c(\"black\", \"blue\"), lwd = c(1, 2))\n", "output_sequence": "Computes different regressions of food on net-income or the UK 1976 expenditure data with Nadaraya Watson and with local linear regression."}, {"input_sequence": "Author: Marlene M\u00fcller ; # clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# install and load packages\nlibraries = c(\"gam\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# load data\nx = read.csv2(\"bostonh.txt\", dec = \".\")\ny = x$VALUEOFHOME\nx1 = log(x$CRIMERATE)\nx3 = log(x$NOCONCENTRATION)\nx4 = log(x$NOOFROOMS)\nx5 = log(x$AGE)\nx6 = log(x$DISTANCE)\nx8 = log(x$PUPILTEACHER)\nx9 = log(x$BLACKPEOPLE)\nx10 = log(x$LOWERSTATUS)\nx11 = x$RIVER # add without s()\nam = gam(y ~ s(x1) + s(x2) + s(x3) + s(x4) + s(x5) + s(x6) + s(x7) + s(x8) + s(x9) +\ns(x10) + x11, family = gaussian)\nprint(summary(am))\nprint(coef(am)) # coefficients of linear parts\nplot(am, ask = TRUE)\n", "output_sequence": "Computes an additive fit for the Boston housing prices."}, {"input_sequence": "Author: Marlene Mueller ; # clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# Product Epanechnikov\nproduct.Epa = function(u) {\nk = 0.75^2 * (1 - u[, 1]^2) * (1 - u[, 2]^2)\nk = k * (abs(u[, 1]) <= 1) * (abs(u[, 2]) <= 1)\nreturn(k)\n}\n# Spherical Epanechnikov\nradial.Epa = function(u) {\nu = as.matrix(u)\nd = ncol(u)\np = 2\nr = 1 - sqrt(rowSums(u * u))^p\nc = p * gamma(d/2) * gamma(1 + q + d/p)/(2 * pi^(d/2) * gamma(1 + q) * gamma(d/p))\nk = c * r^q * (r >= 0)\nH1 = diag(2) # identity matrix\nH2 = diag(c(1, 0.5)) # diagonal matrix\nH3 = 0.5 * diag(2) + 0.5 # general symmetric(!) matrix\nH4 = matrix(c(0.5, 0.25, 1), 2, 2)\nH = H2 # ... change here! ....\n# Plot kernels\nu1 = u2 = seq(-1, 1, length = 30)\ninvH = chol2inv(chol(H))\ngrid = t(invH %*% t(expand.grid(u1, u2)))\nP.Kernel = product.Epa(grid)/det(H)\nP.Kernel = matrix(P.Kernel, length(u1),\nR.Kernel = radial.Epa(grid)/det(H)\nR.Kernel = matrix(R.Kernel, length(u1),\nlevels = seq(0.1, 1, length = 10)\ncontour(u1, u2, P.Kernel, levels = levels, col = \"blue\", main = \"Product Kernel\")\ndev.new()\ncontour(u1, u2, R.Kernel, levels = levels, col = \"blue\", main = \"Radial-symmetric Kernel\")\n", "output_sequence": "Illustrates the difference between a bivariate product and a bivariate radial-symmetric Epanechnikov kernel with equal bandwidths."}, {"input_sequence": "Author: Marlene Mueller ; # clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# load data\nx = read.csv(\"stockres.txt\")\nx = unlist(x)\nx0 = 0 # Origin\nh = 0.02 # Binwidth\n# Select bins\nbreaks = function(x, x0, h) {\nb = floor((min(x) - x0)/h):ceiling((max(x) - x0)/h)\nb = b * h + x0\nreturn(b)\n}\n# Plot histogram\nhist(x, freq = FALSE, breaks = breaks(x, x0, h), main = \"Histogram of Stock Returns\",\n", "output_sequence": "Computes a histogram for stock returns data with binwidth h = 0.02 and origin x0 = 0."}, {"input_sequence": "Author: Awdesch Melzer ; # clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# load data\nx = read.table(\"stockres.dat\")\nx = as.matrix(x)\nn = length(x) # Number of observations.\nstep = 8 # Define the number of shifts.\nt0 = NULL\nh = 0.04 # binwidth\nt0 = h * (floor(min(x)/h) + 0.5) # Min\nm = step\ndelta = h/m\nnbin = floor((max(x) - min(x))/delta)\nbinedge = seq(min(x), max(x) + 0.01, delta) # Define the bins of the histogram\nvk = hist(x, binedge, plot = FALSE)$counts # Count the number of elements in each bin\nfhat = c(rep(0, m - 1), vk, rep(0, m - 1))\nkern = function(s) 1 - abs(s)\nind = (1 - m):(m - 1)\nden = sum(kern(ind/m))\nwm = m * (kern(ind/m))/den\nfhatk = matrix(0, 0, n + 1)\nfor (k in 1:nbin) {\nind = k:(2 * m + k - 2)\nfhatk[k] = sum(wm * fhat[ind])\n}\nfhatk = fhatk/(n * h)\nbinedge = c(rep(min(binedge), 1), binedge)\nfhatk = c(rep(0, 1), fhatk, rep(0, 3))\n# Plot ash\nplot(binedge, fhatk, type = \"s\", main = \"Average Shifted Histogram\", xlab = \"Stock Returns\",\nylab = \"ASH\")\n", "output_sequence": "Illustrates an averaged shifted histogram for stock returns."}, {"input_sequence": "Author: Awdesch Melzer ; # clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# install and load packages\nlibraries = c(\"waveslim\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# set pseudo random numbers\nset.seed(117117)\n# parameter settings\nn = 256 # number of observations\nx = seq(0, by = 2 * pi/(n - 1), length = n) # grid\nm = (x > pi) + sin(x^2) * (x <= pi) # true funtion\ny = m + rnorm(n)/4 # add noise\ny.dwt = dwt(y, wf = \"d8\", 4) # discrete wavelet transform\nmhat = idwt(manual.thresh(y.dwt, 5, 1.9, hard = F))[1:n] # inverse dwt, with manual threshold 1.9 for smoothness\n# plot\nplot(x, y, type = \"n\", ylab = \"Y\", xlab = \"X\")\ntitle(\"Wavelet Regression\")\npoints(x, y, pch = 19, col = \"blue3\", cex = 0.5)\nlines(x, mhat, col = \"black\", lwd = 2.3)\n", "output_sequence": "Computes the wavelet regression using Daubechies basis for a simulated data set."}, {"input_sequence": "Author: Marlene Mueller ; # clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# install and load packages\nlibraries = c(\"splines\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# load\nx = read.csv2(\"fes76.txt\", dec = \".\")\nx = x[, c(\"NIC\", \"FOO\")]\nx = x[order(x$NIC), ]\n# plot\nplot(x, col = \"lightblue\", pch = 20, main = \"Engel Curve: Spline regression\")\nmh = smooth.spline(x$NIC, x$FOO, df = 10)\nlines(x$NIC, fitted(mh))\n", "output_sequence": "Computes the regression of food on net-income for the UK 1976 expenditure data."}, {"input_sequence": "Author: Marlene Mueller ; # clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# install and load packages\nlibraries = c(\"KernSmooth\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# load data\nx = read.csv(\"fes76.txt\", sep = \";\", dec = \".\")\nx = x[, c(\"NIC\", \"FOO\")]\nx = x[order(x$NIC), ]\nmh1 = locpoly(x$NIC, x$FOO, degree = 1, kernel = \"normal\", bandwidth = 20) # Estimate regression function using local polynomials.\nmh2.1 = locpoly(x$NIC, x$FOO, drv = 1, degree = 2, kernel = \"normal\", bandwidth = 20) # Determine first order derivative.\n# plot\nfac = 20 # Multiplication factor for first order derivative - for representation purposes.\nylim = range(c(mh1$y, fac * mh2.1$y))\nplot(mh1, col = \"blue\", type = \"l\", ylim = ylim, main = \"Derivative Estimation\", xlab = \"Net-Income\",\nylab = \"Food\")\nlines(mh2.1$x, fac * mh2.1$y, col = \"blue\", lwd = 2, lty = 2)\nabline(a = 0, b = 0, col = \"red\")\nlegend(\"topleft\", c(\"LocLin\", paste(\"1st Deriv *\", fac)), col = \"blue\", lwd = c(1,\n2), lty = c(1, 2))\n", "output_sequence": "Computes the derivative of the food on net-income regression for the UK 1976 expenditure data using local polynomials."}, {"input_sequence": "Author: Ratmir Miftachov ; setwd(\"/Users/ratmir/MISE/application\")\nsource(\"functions.R\")\ncov=0\nsigma_sim=matrix(c(1, cov,\ncov, 1), nrow=2, ncol=2)\nsource(\"integral_estimation_2d.R\")\nsource(\"shapley_int.R\")\nlibrary(np)\nlibrary(pracma)\nlibrary(cubature)\nlibrary(MASS)\n#load data\nf <- file.choose()\ndata=read.csv(file=f, header = TRUE, sep = \",\", dec = \".\")\ndata_imp = data[, c(\"price.baseMSRP\",\"BLUETOOTH\",\"HEATED.SEATS\", \"NAVIGATION\",\n\"features.Measurements.Curb.weight.lbs\", \"features.Engine.Horsepower.hp\", \"year\",\n\"features.Measurements.Width.in.\",\n\"features.Measurements.Height.in.\", \"Total.Seating\", \"features.Tires.and.Wheels.Tire.Aspect.Ratio\",\ndata_imp[,1] = data_imp[,1]/1000\n#binary\ndata_imp$BLUETOOTH = ifelse(data_imp$BLUETOOTH == 'Yes', 1, 0)\ndata_imp$HEATED.SEATS = as.factor(data_imp$HEATED.SEATS)\ndata_imp$NAVIGATION = ifelse(data_imp$NAVIGATION == 'Yes', 1, 0)\nnames(data_imp) = c(\"price\", \"BLUETOOTH\",\"heated\",\"navi\",\"weight\", \"hp\", \"year\",\n\"width\", \"length\", \"height\", \"seats\", \"ratio\", \"wheels\", \"diameter\")\n#stratify\ndat = data_imp[,c(\"price\", \"hp\", \"year\", \"weight\", \"length\")]\nsummary(dat)\ndat_sort = dat[order(dat[,\"year\"]),]\ndat_first = dat_sort[1:12230, ]\ndat_second = dat_sort[12231:24185, ]\n#All possible subsets, specify time interval\nX = within(dat_first, rm(\"year\", \"price\"))\nd = ncol(X)\nnames = names(X)\nnames(X) = c(\"X1\", \"X2\", \"X3\")\nY = dat_first$price\n#summary statistics\npar(mar = c(1.3, 2.5, 3, 0.4))\nboxplot(X[!(X[,1]>400 | X[,1]<120),1], cex.main=3,\ncex.axis=2, font.main = 2, main=\"Horsepower\")\nboxplot(X[!(X[,2]<2000 | X[,2]>6000),2], cex.main=3,\ncex.axis=2, font.main = 2, main=\"Weight\")\nboxplot(X[!(X[,3]<170 | X[,3]>260),3], cex.main=3,\ncex.axis=2, font.main = 2, main=\"Length\")\nsubs <<- subsets(X)\n#Get model fits and sort them in a list\nsub_bw = subs\nc=3\nsub_bw[[1]][,1] = 1*c\nmodel_list <<- model_list_fct(subs=subs, alt=FALSE, sub_bw)\ngrid_a = seq(120, 400, length.out=100)\n\neval_a = rbind(grid_a, rep(3500, 100), rep(190, 100))\ncol_a = shapley_vec(j=1, x_eval=eval_a , alt=FALSE, model_list = model_list)\neval_b = rbind(rep(250, 100), grid_b, rep(190, 100))\ncol_b = shapley_vec(j=2, x_eval=eval_b , alt=FALSE, model_list = model_list)\neval_c = rbind(rep(250, 100), rep(3500, 100), grid_c)\ncol_c = shapley_vec(j=3, x_eval=eval_c, alt=FALSE, model_list = model_list)\n#Sequential plots over x1:\ncol_b = shapley_vec(j=2, x_eval=eval_a , alt=FALSE, model_list = model_list)\n#Sequential plots over x2:\neval_b = rbind(rep(250, 100), grid_b, rep(190, 100))\ncol_a = shapley_vec(j=1, x_eval=eval_b , alt=FALSE, model_list = model_list)\n#Sequential plots over x3:\neval_c = rbind(rep(250, 100), rep(3500, 100), grid_c)\ncol_a = shapley_vec(j=1, x_eval=eval_c , alt=FALSE, model_list = model_list)\n#Get prediction\npred_fct = function(eval){\nx_eval = t(eval)\nx_eval_vec <- data.frame(X1 = c(x_eval[,1]), X2 = c(x_eval[,2]), X3 = c(x_eval[,3]))\nnames_vars = model_list[[7]]$xnames\npred = predict(model_list[[7]], newdata = x_eval_vec[, c(names_vars), drop = FALSE ])\nreturn(pred)\n}\npred = pred_fct(eval = eval_c)\n#Slice plots with CI\npar(mar = c(4.2, 2.2, 0.7, 0.4)) # for overleaf 620x498\nplot(y=col_a, x=grid_a, type=\"l\", xlab=\"Horsepower [hp], 2014 - 2020\", ylab=\"\", ylim=c(-20,70), main=\"\", cex.main=1.5,\ncex.axis=1.5, font.main = 1.5, cex.lab=1.5)\nx1_ci = seq(120, 400, length.out=48)\npoints(x=x1_ci, y=points[1,] ,cex=0.3, col=\"red\")\nabline(a=0,b=0, lty=2)\nplot(density(X[,1]), xlab=\"Horsepower [hp]\", ylab=\"\",main=\"\", cex.main=1.5,\npoints(x=x1_ci, y=points[7,] ,cex=0.3, col=\"blue\")\nplot(y=col_b, x=grid_b, type=\"l\", xlab=\"Weight [lbs], 2014 - 2020\", ylab=\"\", ylim=c(-35,30), main=\"\", cex.main=1.5,\nx2_ci = seq(2000, 6000, length.out=48)\npoints(x=x2_ci, y=points[3,] ,cex=0.3, col=\"red\")\nplot(y=col_c, x=grid_c, type=\"l\", xlab=\"Length [in], 2014 - 2020\", ylab=\"\", ylim=c(-35,14), main=\"\", cex.main=1.5,\nx3_ci = seq(170, 260, length.out=48)\npoints(x=x3_ci, y=points[5,] ,cex=0.3, col=\"red\")\n#3D plots\nx1_grid = seq(150, 350, length.out=100)\ngrid = t(expand.grid(x1_grid, x2_grid))\ngrid = rbind(grid , rep(190,ncol(grid)) )\nshap_eval_est1=shapley_vec(j=1, grid, alt=FALSE, model_list = model_list)\n#On data points\n#shap_eval_est1=shapley_vec(j=1, t(X), alt=TRUE, model_list = model_list)\n#plot(y=shap_eval_est1, x=X[,1])\n#exclude outliers\noutl_one = shap_eval_est1<=100 & shap_eval_est1>=-100\nshap_eval_est1[!outl_one] = median(shap_eval_est1)\n#On X3:\nx2_grid = seq(2000, 6000, length.out=100)\n#grid = t(expand.grid(x1_grid, x3_grid))\n#grid = rbind(rep(250,ncol(grid)),grid)\ngrid = t(expand.grid(x2_grid, x3_grid))\ngrid = rbind(rep(250,ncol(grid)), grid[1,],\nshap_eval_est3=shapley_vec(j=3, grid, alt=FALSE, model_list = model_list)\nsurface3_est=t(pracma::Reshape(shap_eval_est3, length(x2_grid),\nsurface1_est=t(pracma::Reshape(shap_eval_est1, length(x1_grid), #strong guess:columns are varied over x1\nlibrary(plotly)\npar(mar = c(0, 0, 0))\nfig1_est = plot_ly(x=sort(x1_grid), z=surface1_est, type=\"surface\") %>% hide_colorbar()\nfig1_est <- fig1_est %>% layout(scene=list(xaxis=list(title='x1'),yaxis=list(title='x2'),zaxis=list(title='shap1')))\nfig2_est = plot_ly(x=sort(x1_grid), z=surface2_est, type=\"surface\") %>% hide_colorbar()\nfig2_est <- fig2_est %>% layout(scene=list(xaxis=list(title='x1'),yaxis=list(title='x2'),zaxis=list(title='shap2')))\nfig3_est = plot_ly(x=sort(x2_grid), z=surface3_est, type=\"surface\") %>% hide_colorbar()\nfig3_est <- fig1_est %>% layout(scene=list(xaxis=list(title='x1'),yaxis=list(title='x3'),zaxis=list(title='shap3')))\nmrg <- list(l = 0, r = 0,\nb = 0, t = 0,\n#different colors\nplot_ly(showscale = FALSE) %>%\nadd_surface(x=~sort(x1_grid), y=~sort(x2_grid),z = ~surface1_est, opacity = 0.8) %>%\nlayout(scene=list(xaxis=list(title='Horsepower [hp]'), yaxis=list(title='Weight [lbs]'), zaxis=list(title='', range = c(-25,70)),\ncamera = list(eye = list(x = 1.25, y = 1.25, z = 1.75)) ), margin=mrg)\nadd_surface(x=~sort(x1_grid), y=~sort(x2_grid),z = ~surface2_est, opacity = 0.8) %>%\nlayout(scene=list(xaxis=list(title='Horsepower [hp]'), yaxis=list(title='Weight [lbs]'), zaxis=list(title='', range = c(-15,40)),\nadd_surface(x=~sort(x2_grid), y=~sort(x3_grid),z = ~surface3_est, opacity = 0.8) %>%\nlayout(scene=list(xaxis=list(title='Horsepower [hp]'), yaxis=list(title='Length [in]'), zaxis=list(title='', range = c(-20,8)),\n", "output_sequence": "Application plots: Boxplots, slide plots with CI, density plots, 3d plots"}, {"input_sequence": "Author: Ratmir Miftachov ; dat <- data.frame(x0=c(1:100), x1 = col_a + mean(pred), x2 = rep(mean(pred), 100) )\ndata <- dat\nintersects <- function(x1, x2) {\nseg1 <- which(!!diff(x1 > x2)) # location of first point in crossing segments\nabove <- x2[seg1] > x1[seg1] # which curve is above prior to crossing\nslope1 <- x1[seg1+1] - x1[seg1]\nx <- seg1 + ((x2[seg1] - x1[seg1]) / (slope1 - slope2))\ny <- x1[seg1] + slope1*(x - seg1)\ndata.frame(x=x, y=y, pindex=seg1, pabove=(1:2)[above+1L])\n# pabove is greater curve prior to crossing\n}\nfillColor <- function(data, addLines=TRUE) {\n## Find points of intersections\nints <- intersects(data[,2], data[,3]) # because the first column is for Dates\nintervals <- findInterval(1:nrow(data), c(0, ints$x))\n\n## 1) Make plot for horsepower\npar(mar = c(4.2, 2.2, 0.7, 0.4)) # for overleaf 620x498\nmatplot(data, type=\"n\", col=2:3, lty=1, lwd=4, xaxt='n', ylim=c(0,80), xlab=\"Horsepower [hp]\", cex.axis=1.5, font.main = 1.5, cex.lab=1.5)\n#for horsepower plot:\nfaktor = (grid_a[100] - grid_a[1])/100\naxis(1, at=c(100-250/faktor , 100-200/faktor , 100-150/faktor , 100-100/faktor, 100), labels = c(150, 200, 350, 400),\ncex.axis=1.5, font.main = 1.5, cex.lab=1.5)\n\n## 2) Make plot for weight\npar(mar = c(4.2, 2.2, 0.7, 0.4)) # f\u00fcr overleaf 620x498\nmatplot(data, type=\"n\", col=2:3, lty=1, lwd=4, xaxt='n', ylim=c(0,80), xlab=\"Weight [lbs]\", cex.axis=1.5, font.main = 1.5, cex.lab=1.5)\n\nfaktor = (grid_b[100] - grid_b[1])/100\naxis(1, at=c(1 , 100-3000/faktor , 100-2000/faktor, 100), labels = c(2000, 3000, 6000),\n## 3) Make plot for length\nmatplot(data, type=\"n\", col=2:3, lty=1, lwd=4, xaxt='n', ylim=c(0,80), xlab=\"Length [in]\", cex.axis=1.5, font.main = 1.5, cex.lab=1.5)\nfaktor = (grid_c[100] - grid_c[1])/100\naxis(1, at=c(100-80/faktor, , 100-40/faktor, 100), labels = c(180, 200, 260),\n## Draw the polygons\nfor (i in (seq_along(table(intervals))-1) ) {\nxstart <- ifelse(i == 1, ints$x[i-1])\nxend <- ints$x[i]\nx <- seq(nrow(data))[intervals == i]\npolygon(c(xstart, x, xend, rev(x)), c(ystart, data[x,2], yend, rev(data[x,3])),\ncol=ints$pabove[i]%%2+2)\n}\n# add end of plot\nxstart <- ints[dim(ints)[1],1]\nxend <- nrow(data)\nyend <- data[dim(data)[1],2]\nx <- seq(nrow(data))[intervals == max(intervals)]\npolygon(c(xstart, x, xend, rev(x)), c(ystart, data[x,2], yend, rev(data[x,3])),\ncol=ints[dim(ints)[1]-1,4]%%2+2)\n## Add lines for curves\n#if (addLines)\n# invisible(lapply(1:2, function(x) lines(seq(nrow(data)), data[,x], col=x%%2+2, lwd=2)))\nlines(y = data[,2], x = c(1:100), col=\"blue\", lwd=3)\n## Plot the data\nfillColor(dat,FALSE)\n", "output_sequence": "Sequential Shapley curves as described in the paper."}, {"input_sequence": "Author: Gonzalo Garcia ; library(\"shiny\")\nlibrary(\"tidyverse\")\nlibrary(\"zoo\")\nlibrary(\"reshape2\")\nlibrary(\"htmltools\")\nsource(\"CRIXdataloader/CRIXdataloader.R\")\nsource(\"r/analysis.R\")\n\nETS <-\nreadRDS(file = \"models/ets.rds\")\ncoin <- get_crypto()\ncoin$date <- ymd(coin$date)\nlog_returns <- diff.xts(coin[, 2], lag = 1, log = T)\nlog_returns <- xts(log_returns, order.by = coin$date)\nlog_returns <- na.omit(log_returns)\nui <- fluidPage(\ntags$head(tags$style(\".rightAlign{float:right;}\")), # Align right the plots from Analysis page\nuseShinyjs(),\ntheme = 'bootstrap.min.css',\n\n# Navigation Bar\nnavbarPage(\n\"Sei\u00f0r\",\n\n# Forecast Tab\ntabPanel(\n\"Forecast\",\nsidebarPanel(\ntextInput(\n\"coin\",\n),\nnumericInput(\"periods_to_forecast\",\n\"Select Number of Days to Forecast:\",\nvalue = 1),\nuiOutput('no_value'), # No error caused by empty periods_to_forecast\nuiOutput('ptf_text'), # Prompt user to enter a value if periods_to_forecast is empty\nselectInput(\n\"model\",\n\"Select Model:\",\nc(\n\"LSTM\" = 'lstm',\n\"Naive\" = 'naive_lr',\n)\nuiOutput('lstm_loading_time_text'), # Note about LSTM loading times\n# Show ETS parameters. Hidden if model != ETS\nconditionalPanel(\ncondition = \"input.model == 'ets_lr'\",\ntextInput(\"spec_ets\",\n\"Method\",\nvalue = ETS$method),\ntextInput(\"error\",\nvalue = ETS$components[1]),\ntextInput(\"trend\",\nvalue = ETS$components[2]),\ntextInput(\"seasonality\",\nvalue = ETS$components[3]),\nnumericInput(\"alpha\",\n\"Alpha\",\nsignif(ETS[[\"par\"]][[\"alpha\"]], 3))\nactionButton(\"update_plot\", \"Plot\") # Button to update plot.\n),\nmainPanel(dygraphOutput(\"series\")) # App Output\n),\n# Analysis Tab\n\"coin_analysis\",\n\"Autocorrelation\" = \"acf\",\n\"Monthly Decomposition\" = \"mdecomp\"\nactionButton(\"update_analysis\", \"Plot\")\n# Determines which analysis is shown based on user input.\nmainPanel(\ncondition = \"input.analysis == 'mdecomp'\",\ndygraphOutput(\"seasonal\",height='230px')\ncondition = \"input.analysis == 'mdecomp'\",\ndygraphOutput(\"trend\",height='230px')\ncondition = \"input.analysis == 'acf' | input.analysis == 'pacf'\",\nplotOutput(\"acfpacf\")\n)\n)),\ntabPanel(\"References\"),\n# Dark background for the NavBar\ninverse = TRUE\n)\n)\nserver <- function(input, output) {\n# Backend of \"Plot\" button\nobserveEvent(input$update_plot, {\nif (is.na(input$periods_to_forecast)) {\n} else {\noutput$series <- renderDygraph({\ninteractive_graph(\nperiods_to_forecast = isolate(floor(input$periods_to_forecast)),\nfor_model = isolate(input$model)\n)\n})\n# Backend of \"Prompt user to enter a value if periods_to_forecast is empty\" text\noutput$ptf_text <-\nrenderUI({\nif (is.na(input$periods_to_forecast)){\nbr()\n} else if(input$model == 'lstm' & input$periods_to_forecast > 1){\nreturn(p(\"Warning: The LSTM model was designed to forecast exactly one step into the future. The chosen forecasting horizon is greater than one.\", style = \"color:red\"))\n}\n# Backend of LSTM loading times text\noutput$lstm_loading_time_text <-\nif(input$model == 'lstm'){\nreturn(p(\"Note: Loading the necessary libraries for prediction with the LSTM model might take a few seconds.\", style = \"color:gray\"))\n# Backend of empty periods_to_forecast\nresult<-reactive({\nvalidate(\nneed(input$periods_to_forecast, \"Please input a value.\")\n)\noutput$no_value <- renderPrint({\nresult()\n# Disable coin Input\ndisable('coin')\ndisable('coin_analysis')\n# Disable user Input on ETS parameters\ndisable('spec_ets')\n# Analysis Backend\nobserveEvent(input$update_analysis, {\nif (isolate(input$analysis) == \"mdecomp\") {\noutput$seasonal <- renderDygraph({\nanalysis(for_analysis = isolate(input$analysis), part=\"seasonal\")\n})\noutput$trend <- renderDygraph({\nanalysis(for_analysis = isolate(input$analysis), part=\"trend\")\noutput$remainder <- renderDygraph({\nanalysis(for_analysis = isolate(input$analysis), part=\"remainder\")\n})\n} else if (isolate(input$analysis) == \"acf\") {\noutput$acfpacf <- renderPlot({\nanalysis(for_analysis = isolate(input$analysis))\n} else if (isolate(input$analysis) == \"pacf\") {\n}\nshinyApp(ui = ui, server = server)\n", "output_sequence": "R-Shiny app which does a one day forecast of the CRIX (thecrix.de) using an LSTM Model"}, {"input_sequence": "Author: ['Zijin Wang'] ; # Install and load packages\n#install.packages(\"IntroCompFinR\", repos=\"http://R-Forge.R-project.org\")\nlibraries = c(\"matrixcalc\", \"MASS\", \"PortfolioAnalytics\",\n\"PerformanceAnalytics\", \"zoo\",\"xts\",\"timeSeries\",\"readxl\",\n\"plotly\", \"RiskPortfolios\", \"devtools\", \"PMwR\",\"Jmisc\",\n\"igraph\",\"readxl\",\"quadprog\",\n\"viridis\",\"hrbrthemes\",\n\"FRAPO\", \"R.utils\", \"ade4\", \"grDevices\", \"foreach\",\n\"doParallel\",\"StepwiseTest\", \"stringr\", \"DescTools\",\"DT\",\n\"ggpubr\",\"knitr\", \"tidyr\", \"plotly\", \"rmarkdown\", \"gridExtra\", \"reticulate\",\n\"fPortfolio\",\n\"xtable\", \"DEoptim\", \"ROI\", \"ROI.plugin.quadprog\",\n\"nlshrink\", \"psych\",\n\"quantmod\",\n\"tcltk2\",\"limSolve\",\n\"MTS\",\"Matrix\",\"IntroCompFinR\",\"pracma\",\"glasso\",\"MST\",\n\"RColorBrewer\", \"broom\", \"egg\", \"nloptr\", \"fAssets\",\n\"magick\")\n\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n", "output_sequence": "Boxplot of distance between MSFT and AMZN of 167 rolling wondows. Each window size is 500 days."}, {"input_sequence": "Author: ['Zijin Wang'] ; # Box plot of distance\n# In this file, we draw the box plot of distance between MSFT and AMZN in 167 time window\nrm(list = ls())\nsetwd(\"~/Documents/Code/Network structure based portfolio/Box plot of MSFT and AMZN\")\n# Load Functions and other Files\nsource('./PackagesNetworkPortfolio.R')\n# load data\nprices<-read_excel(\"SP500 securities.xlsx\")\nZOO <- zoo(prices[,-1], order.by=as.Date(as.character(prices$Dates), format='%Y-%m-%d'))\n#return\nreturn<- Return.calculate(ZOO, method=\"log\")\nreturn<- ]\nreturnstd<-xts(return)\np=dim(return)[2]\n# set label\nnode.label=colnames(returnstd)\nnode.label<-gsub(\"Equity\",\"\",node.label)\nnames(returnstd) = node.label\n# rolling window\nW<-list()\nfor(t in 0: 166){\nW[[(t+1)]]=returnstd[(1+t*7):(500+t*7),]\n}\nW_in<-list()\nW_in[[(t+1)]]=W[[t+1]][c(1:493),]\nT.windows<-length(W)\nd=c()\nfor (t in 1:T.windows) {\nd[t]=sqrt(2-2*cor(W_in[[(t)]])[c(\"MSFT \"),c(\"AMZN \")])\npngname<-paste0(getwd(),\"/boxplot_MSFT_AMZN\",\".png\")\npng(file = pngname, width=500, height=400, bg = \"transparent\")\nboxplot(d)\ndev.off()\n", "output_sequence": "Boxplot of distance between MSFT and AMZN of 167 rolling wondows. Each window size is 500 days."}, {"input_sequence": "Author: ['Zijin Wang'] ; p_turnover = function(weights){\nout<-vector()\nfor (i in 2:dim(weights)[1]) {\n# print(i)\nout[i-1]=sum(abs(weights[i,]-weights[i-1,]))/sum((abs(weights[i,])+abs(weights[i-1,]))>0)\n}\n# print(1)\nout = mean(out)\nreturn(out)\n}\npturnoverDN = function(weights, rets, freq){\nresults = Return.portfolio(R = rets,\nweights = weights,\nrebalance_on = freq, verbose = T)\nbop = results$BOP.Weight #beginning of period weights\nbop\neop = results$EOP.Weight #end of period weights\neop\nf = abs(bop - eop)\nout = sum(f)*(1/(length(ep) - 1)) #\nmad_ew = function(x){\na = abs(x - 1/ncol(ret))\nreturn(a)\ntrans_cost = function(weights, rets, freq, c){\n\nout = c*row_sums(abs(eop - bop))\n}\ncalculatePerformanceMeasures = function(start,end){\ncollNumbers = vector()\ncollres = xts()\nweightsNumbers = vector()\nfor (stratloop in 1:length(strats)){\nRb = as.xts(rescoll[, which(strats %in% c(\"EqualWeight\",\"EW\"))],\norder.by = as.Date(rownames(rescoll)))\nportfolioret_net = na.omit(rescoll[,stratloop])\nstrat_weights = weightscoll[[stratloop]]\nstrat_weights[is.nan(strat_weights)] = 0.0\nportfolioret_net_xts = as.xts(as.matrix(na.omit(rescoll[,stratloop])),\norder.by = as.Date(na.omit(rownames(rescoll))))\nportfolioEquity_net = 1 + cumsum(portfolioret_net)\ncumWealth = tail(portfolioEquity_net, 1)\nfirstsignal = start\nrettt = portfolioret_net[firstsignal:end]\nret_data = rets_log\nstock_rets = ret_data[firstsignal:end]\ntc = trans_cost(strat_weights[firstsignal:end,], stock_rets, freq, c = transi)\nportfolioret_net_tc = portfolioret_net[firstsignal:(end - 1)] - tc\nportfolioEquity_net_tc = 1 + cumsum(portfolioret_net_tc)\ncumWealth_tc = tail(portfolioEquity_net_tc, 1)\nT = (commonDate[end] - commonDate[firstsignal])/365\nReturn.ann = (portfolioEquity_net[end]/portfolioEquity_net[firstsignal - 1])^(1/T) - 1\nVola.ann = sd(rettt)*sqrt(252);\nVola.ann_tc = sd(portfolioret_net_tc)*sqrt(252);\nSharpe.ann = Return.ann/Vola.ann\ntarget_turnover = vector();\nfor (i in 2:dim(strat_weights)[1]) {\ntarget_turnover[i] = sum(abs(matrix(strat_weights[i, ]) - matrix(strat_weights[i - 1,])))/dim(strat_weights)[2]\n}\nTurnover = mean(na.omit(target_turnover))\nvalue = portfolioEquity_net\nma = unlist(lapply(c(2:length(value)),function(x) max(value[1:x])))\ndddisc = value[-1]/ma - 1\ndatums = commonDateR[firstsignal:end]\nnum = as.numeric(tail(datums,1)-datums[1])\nPR = as.vector(PainRatio(rettt_xts))\nTurnoverDM = pturnoverDN(strat_weights[firstsignal:end,], stock_rets, freq)\nReturn_annual = as.vector(Return.annualized(rettt_xts, geometric = F))\nAverageDrawdown = as.numeric(AverageDrawdown(rettt_xts))\nSharpe = as.numeric(SharpeRatio(rettt_xts))[1]\nStdDev.annualized = as.numeric(StdDev.annualized(rettt_xts))\ncollNumbers = cbind(collNumbers, as.vector(c(cumWealth, 100*Sharpe.ann,\nTurnover, TurnoverDM)))#,\ncollNumbers_tc = cbind(collNumbers_tc, as.vector(c(cumWealth_tc,\n100*Sharpe.ann_tc, Turnover, TurnoverDM)))\nweightcoll = as.data.frame(strat_weights[first_signal:end])\nweightsNumbers = cbind(weightsNumbers, as.vector(c((mean(apply(weightcoll, 1, min))), mean(apply(weightcoll, 1, max)),\nmean(apply(weightcoll, 1, sd)), mean(apply(weightcoll, 1, mad_ew)), mean(diff(apply(weightcoll, 1, range))))))\ncollNumbers = round(collNumbers,4)\nres = as.xts(portfolioret_net[first_signal:end],order.by=index(ret)[first_signal:end])\ncollres = cbind(collres,res)\nfirst(index(res))\n\nreturn(list(collNumbers,collres,weightsNumbers, collNumbers_tc))\n#Functions tangency.portfolio, globalMin.portfolio and efficient.frontier are copied\n#from the package \"IntroCompFinR: Introduction to Computational Finance in R\" @author Eric Zivot\ntangency.portfolio =\nfunction(er,cov.mat,risk.free, shorts=TRUE)\n{\ncall = match.call()\n#\n# check for valid inputs\nasset.names = names(er)\nif(risk.free < 0)\nstop(\"Risk-free rate must be positive\")\ner = as.vector(er)\ncov.mat = as.matrix(cov.mat)\nN = length(er)\nif(N != nrow(cov.mat))\nstop(\"invalid inputs\")\nif(any(diag(chol(cov.mat)) <= 0))\nstop(\"Covariance matrix not positive definite\")\n# remark: could use generalized inverse if cov.mat is positive semi-definite\n# compute global minimum variance portfolio\ngmin.port = globalMin.portfolio(er, cov.mat, shorts=shorts)\nif(gmin.port$er < risk.free)\nstop(\"Risk-free rate greater than avg return on global minimum variance portfolio\")\n#\n# compute tangency portfolio\nif(shorts==TRUE){\ncov.mat.inv = solve(cov.mat)\nw.t = cov.mat.inv %*% (er - risk.free) # tangency portfolio\nw.t = as.vector(w.t/sum(w.t)) # normalize weights\n} else if(shorts==FALSE){\nDmat = 2*cov.mat\ndvec = rep.int(0, N)\ner.excess = er - risk.free\nAmat = cbind(er.excess, diag(1,N))\nbvec = c(1, rep(0,N))\nresult = quadprog::solve.QP(Dmat=Dmat,dvec=dvec,Amat=Amat,bvec=bvec,meq=1)\nw.t = round(result$solution/sum(result$solution), 6)\n} else {\nstop(\"Shorts needs to be logical. For no-shorts, shorts=FALSE.\")\nnames(w.t) = asset.names\ner.t = crossprod(w.t,er)\nsd.t = sqrt(t(w.t) %*% cov.mat %*% w.t)\ntan.port = list(\"call\" = call,\n\"er\" = as.vector(er.t),\n\"weights\" = w.t)\nclass(tan.port) = \"portfolio\"\nreturn(tan.port)\nefficient.portfolio =\nfunction(er, cov.mat, target.return, shorts=TRUE)\ner = as.vector(er) # assign names if none exist\n# remark: could use generalized inverse if cov.mat is positive semidefinite\n# compute efficient portfolio\nones = rep(1, N)\ntop = cbind(2*cov.mat, er, ones)\nbot = cbind(rbind(er, ones), matrix(0,2,2))\nA = rbind(top, bot)\nb.target = as.matrix(c(rep(0, N), target.return, 1))\nx = solve(A, b.target)\nw = x[1:N]\nAmat = cbind(rep(1,N), er, diag(1,N))\nbvec = c(1, target.return, rep(0,N))\nresult = quadprog::solve.QP(Dmat=Dmat,dvec=dvec,Amat=Amat,bvec=bvec,meq=2)\nw = round(result$solution, 6)\nstop(\"shorts needs to be logical. For no-shorts, shorts=FALSE.\")\n# compute portfolio expected returns and variance\nnames(w) = asset.names\ner.port = crossprod(er,w)\nsd.port = sqrt(w %*% cov.mat %*% w)\nans = list(\"call\" = call,\n\"er\" = as.vector(er.port),\n\"weights\" = w)\nclass(ans) = \"portfolio\"\nreturn(ans)\nefficient.frontier =\nfunction(er, cov.mat, nport=20, alpha.min=-0.5, shorts=TRUE)\n# create portfolio names\nport.names = rep(\"port\",nport)\nns = seq(1,nport)\nport.names = paste(port.names,ns)\ncov.mat.inv = solve(cov.mat)\none.vec = rep(1, N)\nport.gmin = globalMin.portfolio(er, cov.mat, shorts)\nw.gmin = port.gmin$weights\n# compute efficient frontier as convex combinations of two efficient portfolios\n# 1st efficient port: global min var portfolio\n# 2nd efficient port: min var port with ER = max of ER for all assets\ner.max = max(er)\nport.max = efficient.portfolio(er,cov.mat,er.max)\nw.max = port.max$weights\na = seq(from=alpha.min,to=alpha.max,length=nport) # convex combinations\nwe.mat = a %o% w.gmin + (1-a) %o% w.max # rows are efficient portfolios\ner.e = we.mat %*% er # expected returns of efficient portfolios\ner.e = as.vector(er.e)\nwe.mat = matrix(0, nrow=nport, ncol=N)\nwe.mat[1,] = w.gmin\nwe.mat[nport, which.max(er)] = 1\ner.e = as.vector(seq(from=port.gmin$er, to=max(er), length=nport))\nfor(i in 2:(nport-1))\nwe.mat[i,] = efficient.portfolio(er, cov.mat, er.e[i], shorts)$weights\nnames(er.e) = port.names\ncov.e = we.mat %*% cov.mat %*% t(we.mat) # cov mat of efficient portfolios\nsd.e = sqrt(diag(cov.e)) # std devs of efficient portfolios\nsd.e = as.vector(sd.e)\nnames(sd.e) = port.names\ndimnames(we.mat) = list(port.names,asset.names)\n# summarize results\n\"er\" = er.e,\n\"weights\" = we.mat)\nclass(ans) = \"Markowitz\"\nans\n# function to calculate recurrence vector\nRP.P =\nfunction(x,epsilon)\nRecurrence_Matrix = matrix(0, length(x),\nfor (i in 1:length(x)) {\nif(epsilon-abs(x[i]-x[j])>=0) Recurrence_Matrix[i,j]=1\nelse Recurrence_Matrix[i,j]=0\n}\nP=matrix(0,length(x)-1,1)\nfor (tau in 1:(length(x)-1)) {\nP[tau,1]=P[tau,1]+Recurrence_Matrix[j,j+tau]\n}\nP[tau,1]=P[tau,1]/(length(x)-tau)\nP=P-mean(P)\nreturn(P)\n# function to calculate recurrence matrix\nRP.Mtx =\nreturn(Recurrence_Matrix)\n# Construct network of portfolio based on correlation matrix\nnetwork.portfolio =\nfunction(returnstd)\n# correlation matrix\nCormat<-cor(returnstd) # correlation matrix\ncolnames(Cormat)<-colnames(returnstd)\n# distance matrix\n# Dist_mat<-sqrt(2-2*Covmat) # distance matrix\nDist_mat<-as.matrix(Dist_mat)\ncolnames(Dist_mat)<-colnames(returnstd)\n# construct network\nnetwork=graph_from_adjacency_matrix(Dist_mat,weighted=T,\nmode=\"undirected\", diag=F)\nEdgelist_network<-get.edgelist(network) # edges of network\nA<-cbind(Edgelist_network,weight_network)\nA<-as.matrix(A)\nlinks2_network<-as.data.frame(A) # links of network\ncolnames(links2_network)<-c(\"from\",\"to\",\"weight\")\nnet_port<- graph_from_data_frame(d=links2_network, directed=F) # net of whole data\nreturn(net_port)\n# the difference beween network.portfolio is that this directly use correlation matrix\nnetwork.correlation =\n# Dist_mat<-sqrt(2-2*Cormat) # distance matrix\nDist_mat[is.nan(Dist_mat)]<-0\nmode=\"undirected\", diag=F)\n# network=graph_from_adjacency_matrix(Cormat,weighted=T,\n# mode=\"undirected\", diag=F)\n# Edgelist_network<-get.edgelist(network) # edges of network\n# return(network)\nnetwork.abs =\nCorMat<-cor(returnstd) # correlation matrix\ncolnames(CorMat)<-colnames(returnstd)\nnetwork=graph_from_adjacency_matrix(abs(CorMat)-diag(1,ncol=ncol(CorMat),nrow = nrow(CorMat)),\nweighted=T,\n#\n# network.decomposition =\n# function(returnstd)\n# {\n# correlation matrix\n# CorMat<-cor(returnstd) # correlation matrix\n# colnames(CorMat)<-colnames(returnstd)\n# decomposition into positive and negative matrix\n# Lambda<-CorMat-diag(1,ncol=ncol(CorMat),nrow = nrow(CorMat))\n# for (i in seq(1,nrow(CorMat))) {\n# if (CorMat[i,j]>0){Lambda1[i,j]<-CorMat[i,j]}\n# }\n# construct network\n# network.positive=graph_from_adjacency_matrix(Lambda1,\n# weighted=T,\n# mode=\"undirected\", diag=F)\n# network.negative=graph_from_adjacency_matrix(Lambda2,\n# weighted=T,\n# Edgelist_network<-get.edgelist(network) # edges of network\n# A<-cbind(Edgelist_network,weight_network)\n# A<-as.matrix(A)\n# links2_network<-as.data.frame(A) # links of network\n# colnames(links2_network)<-c(\"from\",\"to\",\"weight\")\n# net_port<- graph_from_data_frame(d=links2_network, directed=F) # net of whole data\n# return(network.positive)\n# }\n# Construct network of portfolio based on correlation matrix\n# decompose correlation matrix into an indicator matrix and a matrix Lambda\n# Lambda decomposed into a positive matrix and negative matrix\nnetwork.posi.naga =\n# decompose coefficient matrix\nLambda<- CorMat-diag(1,nrow=nrow(CorMat),ncol = ncol(CorMat))\nLambda1<-\nLambda1[Lambda1<0]<- 0\nLambda2<- -Lambda\nLambda2[Lambda2<0]<- 0\n# construct positive coefficient network\nnetwork_positive=graph_from_adjacency_matrix(Lambda1,weighted=T,\nEdgelist_network<-get.edgelist(network_positive) # edges of network\nnetwork_positive<- graph_from_data_frame(d=links2_network, directed=F) # net of positive coefficient matrix\n# construct negative coefficient network\nnetwork_negative=graph_from_adjacency_matrix(Lambda2,weighted=T,\nmode=\"undirected\", diag=F)\nnetwork_negative<- graph_from_data_frame(d=links2_network, directed=F) # net of negative coefficient matrix\n# output\nnetwork_coefficient<-list(\"positive\"=network_positive,\"negative\"=network_negative)\nreturn(network_coefficient)\n# Construct Minimum spanning tree of portfolio based on covariance\nMST.portfolio =\nCovmat<-cor(returnstd) # correlation matrix\ncolnames(Covmat)<-colnames(returnstd)\nDist_mat<-sqrt(2-2*Covmat) # distance matrix\nmode=\"undirected\", diag=F)\nmst_port<- minimum.spanning.tree(net_port) # minimum spanning tree\nreturn(mst_port)\nnetwork.globalMin.portfolio =\nfunction(nc, cov.mat, shorts=TRUE)\nasset.names = names(nc)\nnc = as.vector(nc) # assign names if none exist\nN = length(nc)\n# compute global minimum portfolio\none.vec = rep(1,N)\nw.gmin = rowSums(cov.mat.inv) / sum(cov.mat.inv)\nw.gmin = as.vector(w.gmin)\nAmat = cbind(rep(1,N), diag(1,N))\nw.gmin = round(result$solution, 6)\n# if(shorts==TRUE){\n# cov.mat.inv = solve(cov.mat)\n# one.vec = rep(1,N)\n# w.gmin = rowSums(cov.mat.inv) / sum(cov.mat.inv)\n# w.gmin = as.vector(w.gmin)\n# } else if(shorts==FALSE){\n# Dmat = 2*cov.mat\n# dvec = rep.int(0, N)\n# Amat = cbind(rep(1,N), diag(1,N))\n# bvec = c(1, rep(0,N))\n# result = quadprog::solve.QP(Dmat=Dmat,dvec=dvec,Amat=Amat,bvec=bvec,meq=1)\n# w.gmin = round(result$solution, 6)\n# } else {\n# stop(\"shorts needs to be logical. For no-shorts, shorts=FALSE.\")\n# }\nnames(w.gmin) = asset.names\nnc.gmin = crossprod(w.gmin,nc)\nsd.gmin = sqrt(t(w.gmin) %*% cov.mat %*% w.gmin)\ngmin.port = list(\"call\" = call,\n\"nc\" = as.vector(nc.gmin),\nclass(gmin.port) = \"portfolio\"\ngmin.port\nnetwork.efficient.portfolio =\nfunction(nc, cov.mat, target.nc, shorts=TRUE)\n# ones = rep(1, N)\n# top = cbind(2*cov.mat, nc, ones)\n# bot = cbind(rbind(nc, ones), matrix(0,2,2))\n# A = rbind(top, bot)\n# b.target = as.matrix(c(rep(0, N), target.nc, 1))\n# x = solve(A, b.target)\n# w = x[1:N]\nAmat = cbind(rep(1,N), -nc)\nbvec = cbind(1, -target.nc)\nAmat = cbind(rep(1,N), -nc, diag(1,N))\nbvec = c(1, -target.nc, rep(0,N))\nnc.port = crossprod(nc,w)\n\"nc\" = as.vector(nc.port),\n\"weights\" = w)\nnetwork.2constraint.portfolio =\nfunction(nc,er, cov.mat, target.nc,target.er, shorts=TRUE)\nAmat = cbind(rep(1,N), -nc, er)\nbvec = cbind(1, -target.nc,\nAmat = cbind(rep(1,N), -nc, diag(1,N), er)\nbvec = c(1, -target.nc, rep(0,N), target.er)\n\"er\" = as.vector(er.port),\nnetwork.efficient.frontier =\nfunction(nc, cov.mat, nport=20, alpha.min=-0.5, shorts=TRUE)\ner = as.vector(nc)\nport.gmin = network.globalMin.portfolio(nc, cov.mat, shorts)\nnc.min = min(nc)\nport.min = network.efficient.portfolio(nc,cov.mat,nc.min, shorts)\nw.min = port.min$weights\nwe.mat = a %o% w.gmin + (1-a) %o% w.min # rows are efficient portfolios\nnc.e = we.mat %*% nc # expected returns of efficient portfolios\nnc.e = as.vector(nc.e)\nwe.mat[nport, which.min(nc)] = 1\nnc.e = as.vector(seq(from=port.gmin$nc, to=min(nc), length=nport))\nwe.mat[i,] = efficient.portfolio(nc, cov.mat, nc.e[i], shorts)$weights\nnames(nc.e) = port.names\n\"nc\" = nc.e,\n\"weights\" = we.mat)\n##### portfolio using Dantzig type selector with 1 constraints #####\nlinfun1=function(Sn,b,lambda)\n{\n#equivalent to solving min 1'u\n#such that u-x>=0\n# u+x>=0\n# -hatS x>=-(lambda 1+b)\n# x^T1=1\na=rep(0,2*p)\na[c(1,1+p)]=1\nA0=toeplitz(a)\nA0[upper.tri(A0)]=-A0[upper.tri(A0)]\nA1=cbind(matrix(0,p,p),-Sn)\nA2=-A1\nA=rbind(A0,A1,A2)\nrhs=c(rep(0,2*p),c(-b,b)-lambda*rep(1,2*p))\nC=rep(c(1,0),c(p,p))\n# FF=1\nsolvetheta=linp(#E=EE,F=FF,\nG=A,H=rhs,Cost=C,ispos=FALSE)$X[(p+1):(2*p)]\n# return(solvetheta)\n##### portfolio using Dantzig type selector with 2 constraints #####\nlinfun2=function(Sn,b1,b2,lambda)\n#equivalent to solving min 1'u1 + 1'u1\n#such that u1-x1>=0\n# u1+x1>=0\n# -hatS x1>=-(lambda 1+b1)\n# x^T1=1 (I do not think so)\na=rep(0,4*p)\na[c(1,1+2*p)]=1\nA1=cbind(matrix(0,p,2*p),-Sn,matrix(0,p,p))\nA4=-A1\nA=rbind(A0,A1,A2,A3,A4)\nrhs=c(rep(0,4*p),c(-b1,b1,-b2,b2)-lambda*rep(1,4*p))\nC=rep(c(1,0),c(2*p,2*p))\ntheta=linp(G=A,H=rhs,Cost=C,ispos=FALSE)\nsolvetheta=list(\"theta1\"=theta$X[(2*p+1):(3*p)],\n", "output_sequence": "Boxplot of distance between MSFT and AMZN of 167 rolling wondows. Each window size is 500 days."}, {"input_sequence": "Author: Andreas Rony Wijaya ; import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Ellipse\n### Load bank dataset\ndata = pd.read_csv(\"bank2.txt\", sep = \"\\s+\", header=None)\nxx = data.sample(n = 20, random_state = 11, replace=False)\nmean = np.mean(xx)\nm = np.array([mean]*len(xx))\nx = xx - m\neva = np.linalg.eig(np.cov(x.T))[0]\ny = x @ eve\n### PCA for 20 randomly chosen bank notes\nfig, ax = plt.subplots(figsize = (10, 10))\nax.scatter(ym[0], ym[1], c = \"w\")\nfor i in range(0, len(ym)):\nax.text(ym.iloc[i,0]-0.1, ym.iloc[i,1]-0.07, xx.index[i], fontsize = 20)\nax.set_xlabel(\"first PC\")\nax.set_ylim(-5, 4)\nax.set_title(\"20 Swiss bank notes\", fontsize = 20)\n### The dendrogram for the 20 bank notes after applying the Ward algorithm\nd = np.zeros([len(xx),len(xx)])\nfor i in range(0, len(xx)):\nd[i, j] = np.linalg.norm(xx.iloc[i, :] - xx.iloc[j, :])\ndd = (d**2)\nddd = dd[1:, :-1][:, 0]\nfor i in range(1, len(xx)-1):\nddd = np.concatenate((ddd, dd[1:, :-1][i:, i]))\n\nw = cluster.hierarchy.linkage(ddd, 'ward')\nh = cluster.hierarchy.dendrogram(w, labels=xx.index)\nplt.title(\"Dendrogram for 20 Swiss bank notes\", fontsize = 16)\nplt.ylabel(\"Squared Euclidean Distance\")\nplt.xlabel(\"Ward algorithm\")\n### PCA with clusters\ngroups = cluster.hierarchy.cut_tree(w, height=20)\nmerg = np.concatenate((ym, groups), axis = 1)\nmerg = pd.DataFrame(merg).sort_values(by = 2)\nmerg1 = merg.iloc[:len(merg[merg.iloc[:, 2] == 0]), :2]\ncovm = np.cov(merg1.iloc[:, 0], merg1.iloc[:, 1])\neigva = np.sqrt(np.linalg.eig(covm)[0])\nax.add_patch(Ellipse(xy = (np.mean(merg1.iloc[:, 0]), np.mean(merg1.iloc[:, 1])),\nwidth = eigva[0]*3*2, height = eigva[1]*3*2,\nangle = -np.rad2deg(np.arccos(eigve[0, 0])), facecolor = \"w\", edgecolor = \"r\", zorder = 0))\nax.add_patch(Ellipse(xy = (np.mean(merg2.iloc[:, 0]), np.mean(merg2.iloc[:, 1])),\nwidth = eigva1[0]*3*2, height = eigva1[1]*3*2,\nangle = -np.rad2deg(np.arccos(eigve1[0, 0])), facecolor = \"w\", edgecolor = \"b\", zorder = 0))\nax.scatter(merg1.iloc[:, 0], merg1.iloc[:, 1], c = \"r\")\nax.set_title(\"20 Swiss bank notes, cut height 20\", fontsize = 20)\n", "output_sequence": "Performs a PCA and a cluster analysis for 20 randomly chosen bank notes from the swiss bank notes dataset."}, {"input_sequence": "Author: Andreas Rony Wijaya ; #Generate 2D Normal Distribution: Box-Muller\n#size\nn = 100000\n#Uniform\nu = runif(n)\nx=rep(0,n)\nfor (i in 1:n){\nx[i] = sqrt(-2*log(u[i]))*cos(2*pi*v[i])\n}\n#Two dimensional Normal Distribution\ndf <- data.frame(x,y)\nprint(df)\ntwonorm <- c(x,y)\nhist(twonorm, main=\"Histogram of 2D Normal\", xlab=\"value\")\n#Histogram\nh <- hist(twonorm, breaks=10, col=\"grey\",\nmain=\"Histogram of 2D Normal\", xlab=\"value\", ylim=c(0,80000))\nxfit <- seq(min(twonorm), length = 40)\nyfit <- dnorm(xfit, mean = mean(twonorm), sd = sd(twonorm))\nyfit <- yfit * diff(h$mids[1:2]) * length(twonorm)\nlines(xfit, yfit, col = \"black\", lwd = 2)\n", "output_sequence": "Generating 2D Normal Distribution with Polar Coordinates"}, {"input_sequence": "Author: Awdesch Melzer, Muhaimin ; # -*- coding: utf-8 -*-\n\"\"\"SMScarsim\nAutomatically generated by Colaboratory.\nOriginal file is located at\nhttps://colab.research.google.com/drive/16XnfiDhvyAUlc1UmZibdNZb2G1HfWdO6\n\"\"\"\nimport pandas as pd\nimport numpy as np\nfrom sklearn import metrics\nfrom scipy.spatial import distance\ndf = pd.read_csv('sample_data/carmean2.txt', header=None, sep='|', usecols=[1,2,3,4,5,6,7,8])\ndf1719 = df.iloc[16:19,] #renault rover toyota\ndf_mu = df1719.mean() #column mean\ndf_mu = np.array(df_mu)\ny = np.zeros((3,8))\nfor i in range(0,3):\nif x1719[i,j] > df_mu[j]:\ny[i,j]=1\nelse:\ny[i,j]=0\ny #binary matrix\n#jaccard score\njaccard_sim = np.zeros((3,3))\nfor j in range(0,3):\nif i==j:\njaccard_sim[i,j]=1\njaccard_sim[i,j]= jaccard_score(y[i,:], y[j,:])\njaccard_sim\n#simple matching score\nsimatch_sim = np.zeros((3,3))\nsimatch_sim[i,j] = 1\nsimatch_sim[i,j] = sum(y[i,:] == y[j,:])/8\nsimatch_sim\n#roger score\nrogerstan_sim = np.zeros((3,3))\nrogerstan_sim[i,j] = 1\nrogerstan_sim[i,j] = 1-distance.rogerstanimoto(y[i,:], y[j,:])\n", "output_sequence": "computes the Jaccard, simple matching and Tanimoto proximity coefficients for binary car data"}, {"input_sequence": "Author: Andrija Mihoci, Xiu Xu ; clear all; clc;\n% Input\nK = 11;\nCARE_RB_Th1_005 = load('CARE_RB_Th1_005');\nT_k_Th1_005 = load('T_k_Th1_005');\ny_t_005_Th1 = load('y_t_005_Th1');\nV = size(y_t_005_Th1, 2);\nCARE_risk_bound_Th1_005 = load('CARE_risk_bound_Th1_005.mat');\nCARE_LR_c_Th1_005 = CARE_risk_bound_Th1_005.CARE_LR_c_Th1_005;\n% Programme Code\nz_k_c_Th1_005 = zeros(K - 1, K - 1);\nfor s = 1 : 1 : (K - 1)\n\nz_k_c_Th1_005(s, s_step) = LCARE_Propagation_Condition(s, s_step, ...\nV, T_k_Th1_005, CARE_LR_c_Th1_005, 1)...\nz_k_d_Th1_005(s, s_step) = LCARE_Propagation_Condition(s, s_step, ...\nV, T_k_Th1_005, CARE_LR_d_Th1_005, 2)...\n\nend\nz_k_c_Th1_005 = diag(z_k_c_Th1_005);\nz_k_Th1_005 = [z_k_c_Th1_005,\nfor i = 1 : 1 : 2\nfor s = 2 : 1 : length(z_k_Th1_005)\nif z_k_Th1_005(s, i) > z_k_Th1_005(s - 1, i)\nelse\nz_k_Th1_005(s, i) = z_k_Th1_005(s, i);\nend\n% Output\nOut.z_k_Th1_005 = z_k_Th1_005;\nsave('z_k_Th1_005', '-ascii');\n", "output_sequence": "Evaluates the critical values based on the likelihood ration and risk bound at parameter constellation theta1 and expectile leve tau = 0.05"}, {"input_sequence": "Author: Andrija Mihoci, Xiu Xu ; function Out = LCARE_Propagation_Condition(z, s_step, V, Tv, LR, r_ind)\n% Input\nK = 11;\n% Programme Code\nB_Tv = ones(K - 1, V, K - 1);\nfor s = 1 : 1 : (K - 1)\nB_Tv(:, :, s) = Tv > z;\nend\nE_LR_Bs = mean(LR .* B_Tv, 2);\nPC = zeros(K - 1, 1);\nfor s = 1 : 1 : (K - 1)\nPC(s) = max(E_LR_Bs(:, :, s) - (rho * s) / (K - 1) * r_ind);\n% Output\nOut = PC(s_step);\n", "output_sequence": "Evaluates the critical values based on the likelihood ration and risk bound at parameter constellation theta1 and expectile leve tau = 0.05"}, {"input_sequence": "Author: Andrija Mihoci, Xiu Xu ; % tau = 0.05\nclear all; clc;\nload('LCARE_005.mat');\nllen_DAX_005_c = lcare_DAX_005_c(11, :)';\nlen_005 = [llen_DAX_005_c ...\nllen_FTSE_005_d\n% smooth line\nlcare_length_005_d_week = Smooth_mean(len_005, 20);\nfigure\nsubplot(2, 3, 1);\nplot(lcare_DAX_005_c(11, :), '-b', 'LineWidth', 1.5);\ntitle('DAX', 'FontName', 'Times New Roman', 'FontSize', 16);\nhold on;\nplot(lcare_length_005_d_week(:, 1), '-r', 'LineWidth', 1.5);\nylabel('Length');\nylabel('Length'); ylim([30, 240]); xlim([1, 2870]);\nYear = {'2008', '2016'}; hold on; set(gca, 'xtick', [522 1566 2610]);\nLength = {'60', '120', '180'};\nset(gca, 'ytick', [60 120 180]);\nset(gca, 'xticklabel', Year);\nset(gca, 'xgrid', 'on')\nsubplot(2, 3, 2);\nplot(lcare_FTSE_005_c(11, :), '-b', 'LineWidth', 1.5);\ntitle('FTSE 100', 'FontName', 'Times New Roman', 'FontSize', 16);\nplot(lcare_length_005_d_week(:, 2), '-r', 'LineWidth', 1.5);\nsubplot(2, 3, 3);\nplot(lcare_SP_005_c(11, :), '-b', 'LineWidth', 1.5);\ntitle('S&P 500');\ntitle('S&P 500', 'FontName', 'Times New Roman', 'FontSize', 16);\nplot(lcare_length_005_d_week(:, 3), '-r', 'LineWidth', 1.5);\nsubplot(2, 3, 4);\nplot(lcare_DAX_005_d(11, :), '-b', 'LineWidth', 1.5);\nsubplot(2, 3, 5);\nplot(lcare_FTSE_005_d(11, :), '-b', 'LineWidth', 1.5);\nsubplot(2, 3, 6);\nplot(lcare_SP_005_d(11, :), '-b', 'LineWidth', 1.5);\n", "output_sequence": "Estimates length of the interval of homogeneity in trading days across the selected three stock markets from 3 January 2006 to 31 December 2014 for the modest (upper panel, r = 0.5) and the conservative (lower panel, r = 1) risk cases, at expectile level 0.05."}, {"input_sequence": "Author: Andrija Mihoci, Xiu Xu ; function Sdata = Smooth_mean(data, frequency)\n[N, M] = size(data);\nSdata = data;\nfor i = 1 : frequency - 1\nSdata(i, :) = mean(data(1 : i + frequency - 1, :));\nend\nSdata(frequency : N - frequency) = 0;\nSdata(i, :) = mean(data(i - frequency + 1 : i + frequency - 1, :));\nfor i = N - frequency + 1 : N\nSdata(i, :) = mean(data(i - frequency + 1: i, :));\n", "output_sequence": "Estimates length of the interval of homogeneity in trading days across the selected three stock markets from 3 January 2006 to 31 December 2014 for the modest (upper panel, r = 0.5) and the conservative (lower panel, r = 1) risk cases, at expectile level 0.05."}, {"input_sequence": "Author: Andrija Mihoci, Xiu Xu ; clear all; clc;\n% Input\nn_0 = 20;\nTh3_001 = [0.00000, 0.12637, 0.00008, 0.03325, 0.00040]';\nTheta = Th3_001;\nc_r = 0.50;\nd_r = 1.00;\nrho = 0.25;\ntau = 0.01;\nCARE_yv = load('y_t_001_Th3');\nCARE_Intervals = load('CARE_Intervals');\nV = size(CARE_yv, 2);\n% Programme Code\nLk_Thk = zeros(K - 1, V, K - 1);\nfor v = 1 : 1 : V\nfor j = 1 : 1 : K - 1\nLk_Thk(k, v, j) = LCARE_Estimation_Loglik(CARE_yv(CARE_Intervals(k + 1, 7) : end, v), tau);\nend\nend\nRB = [max(mean(abs(Lk_Thk(:, :, 1) - Lk_Th(:, :, 1)) .^ c_r, 2));\nCARE_RB_Th3_001 = rho * RB ./ [K - 1, K - 1]';\nCARE_LR_c_Th3_001 = abs(Lk_Thk - Lk_Thl) .^ c_r;\n% Output\nOut.Lk_Thk = Lk_Thk;\nsave('CARE_RB_Th3_001', '-ascii');\nsave('CARE_risk_bound_Th3_001.mat');\n", "output_sequence": "Simulates the risk bound of a CARE model according to theta3 parameter constellation at expectile level tau = 0.01"}, {"input_sequence": "Author: Andrija Mihoci, Xiu Xu ; function Out = LCARE_Estimation_Loglik(y, tau)\nObjective = @(th) -LCARE_Loglik(y, tau, th);\nOptions = optimset('Display', 'off', 'Algorithm', 'interior-point', 'TolX', 1e-3, 'TolFun', 1e-3, 'MaxIter', 1000);\nth0 = [0.10, 1.00, 0.01]';\nA = [0 0 -1];\nb = 0;\n\n[~, lik_value] = fmincon(Objective, th0, A, b, [], Options);\nOut = -lik_value;\nend\n", "output_sequence": "Simulates the risk bound of a CARE model according to theta3 parameter constellation at expectile level tau = 0.01"}, {"input_sequence": "Author: Andrija Mihoci, Xiu Xu ; function Out = LCARE_Estimation_Theta(y, tau)\nObjective = @(th) - LCARE_Loglik(y, tau, th);\nOptions = optimset('Display', 'off', 'Algorithm', 'interior-point', 'TolX', 1e-3, 'TolFun', 1e-3, 'MaxIter', 1000);\nth0 = [0.00, 0.01]';\nA = [0 0 -1];\nb = 0;\nOut = fmincon(Objective, th0, A, b, [], Options);\n\nend\n", "output_sequence": "Simulates the risk bound of a CARE model according to theta3 parameter constellation at expectile level tau = 0.01"}, {"input_sequence": "Author: Andrija Mihoci, Xiu Xu ; function Out = LCARE_Loglik(y, tau, th)\nn = length(y);\ny_lag = y(1 : 1 : n - 1, 1);\ny_lag_plus = (y_lag .* (y_lag > 0)) .^ 2;\ne = y(2 : n, 1) - th(1) - th(2) * y_lag - th(3) * y_lag_plus - th(4) * y_lag_neg;\nrho_tau = abs(tau - 1 * (e <= 0)) .* e.^2;\nf_y = 2 / sqrt(th(5)) * (sqrt(pi / abs(tau - 1)) + sqrt(pi / tau))^(-1) * exp (- rho_tau / th(5));\nf_y(f_y == 0) = 1;\nOut = sum(log(f_y));\nend\n", "output_sequence": "Simulates the risk bound of a CARE model according to theta3 parameter constellation at expectile level tau = 0.01"}, {"input_sequence": "Author: Andrija Mihoci, Xiu Xu ; clear all; clc;\ny_t_001_Th2 = load('y_t_001_Th2');\ntau = 0.01;\nA_t_001_Th2 = (1 : 1 : n_t_001_Th2)';\nLI_t_001_Th2 = arrayfun(@(x) LCARE_Test_Statistics_LR(y_t_001_Th2(:, x), ...\ntau), A_t_001_Th2, 'UniformOutput', false);\nLR_t_001_Th2 = reshape(cell2mat(LI_t_001_Th2), [], n_t_001_Th2);\nL_A_Th2_001 = LR_t_001_Th2(1 : 166, :);\nCARE_Intervals = load('CARE_Intervals');\nT_n_k = CARE_Intervals(2 : 1 : 11, 2) - CARE_Intervals(1, 2);\nT_k_Th2_001 = zeros(10, 1000);\nT_k_Th2_001(1, :) = max(L_LR_Th2_001(1 : T_n_k(1), :));\nfor j = 2 : 1 : length(T_n_k)\nT_k_Th2_001(j, :) = max(L_LR_Th2_001((T_n_k(j - 1) + 1) : 1 : T_n_k(j), :));\nend\nsave('T_k_Th2_001', '-ascii');\n", "output_sequence": "Evaluates the likelihood ratio test statistics for parameter constellation theta2 and expectile leve 0.01"}, {"input_sequence": "Author: Andrija Mihoci, Xiu Xu ; function Out = LCARE_Estimation_Loglik(y, tau)\nObjective = @(th) - LCARE_Loglik(y, tau, th);\nOptions = optimset('Display', 'off', 'Algorithm', 'interior-point', ...\n'TolX', 1e-3, 'TolFun', 1e-3, 'MaxIter', 1000);\nth0 = [0.10, 1.00, 0.01]';\nA = [0 0 -1];\nb = 0;\n\n[~, lik_value] = fmincon(Objective, th0, A, b, [], Options);\nOut = -lik_value;\nend\n", "output_sequence": "Evaluates the likelihood ratio test statistics for parameter constellation theta2 and expectile leve 0.01"}, {"input_sequence": "Author: Andrija Mihoci, Xiu Xu ; function Out = LCARE_Test_Statistics_LR(y, tau)\nCARE_Intervals = load('CARE_Intervals');\nB = (CARE_Intervals(2, 4) : -1 : CARE_Intervals(11, 3))' + 1;\nL_B = arrayfun(@(x) LCARE_Estimation_Loglik(y(x : 1 : ...\nCARE_Intervals(1, 10), tau), B);\nk = 1;\nJ1 = (CARE_Intervals(k + 1, 4) : - 1 : CARE_Intervals(k + 1, 3))';\nL_A_k1 = arrayfun(@(x) LCARE_Estimation_Loglik(y(CARE_Intervals(k + 1, 5)...\n: 1 : x, 1), tau), J1);\nL_I_2 = LCARE_Estimation_Loglik(y(CARE_Intervals(k + 1, 9) : 1 : ...\nCARE_Intervals(k + 1, 10), tau) * ones(length(J1), 1);\nk = 2;\nJ2 = (CARE_Intervals(k + 1, 4) : - 1 : CARE_Intervals(k + 1, 3))';\nL_A_k2 = arrayfun(@(x) LCARE_Estimation_Loglik(y(CARE_Intervals(k + 1, 5)...\n: 1 : x, 1), tau), J2);\nL_I_3 = LCARE_Estimation_Loglik(y(CARE_Intervals(k + 1, 9) : 1 : ...\nCARE_Intervals(k + 1, 10), tau) *ones(length(J2), 1);\nk = 3;\nJ3 = (CARE_Intervals(k + 1, 4) : - 1 : CARE_Intervals(k + 1, 3))';\nL_A_k3 = arrayfun(@(x) LCARE_Estimation_Loglik(y(CARE_Intervals(k + 1, 5)...\n: 1 : x, 1), tau), J3);\nL_I_4 = LCARE_Estimation_Loglik(y(CARE_Intervals(k + 1, 9) : 1 : ...\nCARE_Intervals(k + 1, 10), tau) * ones(length(J3), 1);\nk = 4;\nJ4 = (CARE_Intervals(k + 1, 4) : - 1 : CARE_Intervals(k + 1, 3))';\nL_A_k4 = arrayfun(@(x) LCARE_Estimation_Loglik(y(CARE_Intervals(k + 1, 5)...\n: 1 : x, 1), tau), J4);\nL_I_5 = LCARE_Estimation_Loglik(y(CARE_Intervals(k + 1, 9) : 1 : ...\nCARE_Intervals(k + 1, 10), tau) * ones(length(J4), 1);\nk = 5;\nJ5 = (CARE_Intervals(k + 1, 4) : - 1 : CARE_Intervals(k + 1, 3))';\nL_A_k5 = arrayfun(@(x) LCARE_Estimation_Loglik(y(CARE_Intervals(k + 1, 5)...\n: 1 : x, 1), tau), J5);\nL_I_6 = LCARE_Estimation_Loglik(y(CARE_Intervals(k + 1, 9) : 1 : ...\nCARE_Intervals(k + 1, 10), tau) * ones(length(J5), 1);\nk = 6;\nJ6 = (CARE_Intervals(k + 1, 4) : - 1 : CARE_Intervals(k + 1, 3))';\nL_A_k6 = arrayfun(@(x) LCARE_Estimation_Loglik(y(CARE_Intervals(k + 1, 5)...\n: 1 : x, 1), tau), J6);\nL_I_7 = LCARE_Estimation_Loglik(y(CARE_Intervals(k + 1, 9) : 1 : ...\nCARE_Intervals(k + 1, 10), tau) * ones(length(J6), 1);\nk = 7;\nJ7 = (CARE_Intervals(k + 1, 4) : - 1 : CARE_Intervals(k + 1, 3))';\nL_A_k7 = arrayfun(@(x) LCARE_Estimation_Loglik(y(CARE_Intervals(k + 1, 5)...\n: 1 : x, 1), tau), J7);\nL_I_8 = LCARE_Estimation_Loglik(y(CARE_Intervals(k + 1, 9) : 1 : ...\nCARE_Intervals(k + 1, 10), tau) * ones(length(J7), 1);\nk = 8;\nJ8 = (CARE_Intervals(k + 1, 4) : - 1 : CARE_Intervals(k + 1, 3))';\nL_A_k8 = arrayfun(@(x) LCARE_Estimation_Loglik(y(CARE_Intervals(k + 1, 5)...\n: 1 : x, 1), tau), J8);\nL_I_9 = LCARE_Estimation_Loglik(y(CARE_Intervals(k + 1, 9) : 1 : ...\nCARE_Intervals(k + 1, 10), tau) * ones(length(J8), 1);\nk = 9;\nJ9 = (CARE_Intervals(k + 1, 4) : - 1 : CARE_Intervals(k + 1, 3))';\nL_A_k9 = arrayfun(@(x) LCARE_Estimation_Loglik(y(CARE_Intervals(k + 1, 5)...\n: 1 : x, 1), tau), J9);\nL_I_10 = LCARE_Estimation_Loglik(y(CARE_Intervals(k + 1, 9) : 1 : ...\nCARE_Intervals(k + 1, 10), tau) * ones(length(J9), 1);\nk = 10;\nJ10 = (CARE_Intervals(k + 1, 4) : - 1 : CARE_Intervals(k + 1, 3))';\nL_A_k10 = arrayfun(@(x) LCARE_Estimation_Loglik(y(CARE_Intervals(k + 1, 5)...\n: 1 : x, 1), tau), J10);\nL_I_11 = LCARE_Estimation_Loglik(y(CARE_Intervals(k + 1, 9) : 1 : ...\nCARE_Intervals(k + 1, 10), tau) * ones(length(J10), 1);\nL_A = [L_A_k1; ...\nL_A_k8; L_A_k10];\nL_I = [L_I_2; ...\nL_I_10;\nL_LR = L_A + L_B - L_I;\nOut = [L_A; L_B; L_LR];\nend\n", "output_sequence": "Evaluates the likelihood ratio test statistics for parameter constellation theta2 and expectile leve 0.01"}, {"input_sequence": "Author: Andrija Mihoci, Xiu Xu ; function Out = LCARE_Loglik(y, tau, th)\nn = length(y);\ny_lag = y(1 : 1 : n - 1, 1);\ny_lag_plus = (y_lag .* (y_lag > 0)) .^ 2;\ne = y(2 : n, 1) - th(1) - th(2) * y_lag - th(3) * y_lag_plus ...\n- th(4) * y_lag_neg;\nrho_tau = abs(tau - 1 * (e <= 0)) .* e .^ 2;\nf_y = 2 / sqrt(th(5)) * (sqrt(pi / abs(tau - 1)) + ...\nsqrt(pi / tau))^(-1) * exp (- rho_tau / th(5));\nf_y(f_y == 0) = 1;\nOut = sum(log(f_y));\n", "output_sequence": "Evaluates the likelihood ratio test statistics for parameter constellation theta2 and expectile leve 0.01"}, {"input_sequence": "Author: Lenka Zbo\u0148\u00e1kov\u00e1 ; # Clear all variables\nrm(list = ls(all = TRUE))\ngraphics.off()\n# Set directory\nsetwd(\"\")\n# Install and load packages\nlibraries = c(\"MASS\", \"bbmle\", \"glmnet\", \"doParallel\", \"LaplacesDemon\", \"optimx\", \"lars\",\n\"scales\", \"tilting\", \"VGAM\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)} )\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# Call functions computing PAM\nsource(\"PAMfunc.r\")\n# Load data for PAM and Cochrane and Piazzesi (2005)\ntmpdata = read.csv(\"BRP_data.csv\",sep=\",\") # Bond Risk Premium data\ndata = sapply(subset(tmpdata, select = c(2:(ncol(tmpdata)))), as.numeric)\ndates = as.Date(as.character(as.POSIXct(as.character(tmpdata[, 1]),\nformat = \"%Y%m%d\")))\n# Load macro data from Ludvigson and Ng (2009)\ntmpdataLN = read.csv(\"LN_macrodata_transformed.csv\", sep=\",\")\ndataLN = sapply(subset(tmpdataLN, select = c(2:ncol(tmpdataLN))), as.numeric)\ndatesLN = as.Date(as.character(as.POSIXct(as.character(tmpdataLN[, 1]),\n# Model settings\n# Define time span of the data\nstart.date = grep(\"1964\", dates)[1]\n# Data selection\ncovariatesCP = data[start.date:end.date, 5:9]\nn.obs = nrow(covariatesCP)\n# ----------------------------------------------------------------------------------------\n# Cochrane and Piazzesi (2005): model with forward rates only\nobject.brp2 = lm(BRP2 ~ covariatesCP)\nfit.brp2 = predict(object.brp2, as.data.frame(covariatesCP))\nRMSE.brp2 = sqrt(1/n.obs * sum((BRP2 - fit.brp2)^2))\nRsq.brp2 = sum((fit.brp2 - mean(BRP2))^2)/sum((BRP2 - mean(BRP2))^2)\nRadj.brp2 = 1 - ((1 - Rsq.brp2) * (n.obs - 1))/(n.obs - 5 - 1)\nobject.brp3 = lm(BRP3 ~ covariatesCP)\nfit.brp3 = predict(object.brp3, as.data.frame(covariatesCP))\nRMSE.brp3 = sqrt(1/n.obs * sum((BRP3 - fit.brp3)^2))\nRsq.brp3 = sum((fit.brp3 - mean(BRP3))^2)/sum((BRP3 - mean(BRP3))^2)\nRadj.brp3 = 1 - ((1 - Rsq.brp3) * (n.obs - 1))/(n.obs - 5 - 1)\nobject.brp4 = lm(BRP4 ~ covariatesCP)\nfit.brp4 = predict(object.brp4, as.data.frame(covariatesCP))\nRMSE.brp4 = sqrt(1/n.obs * sum((BRP4 - fit.brp4)^2))\nRsq.brp4 = sum((fit.brp4 - mean(BRP4))^2)/sum((BRP4 - mean(BRP4))^2)\nRadj.brp4 = 1 - ((1 - Rsq.brp4) * (n.obs - 1))/(n.obs - 5 - 1)\nobject.brp5 = lm(BRP5 ~ covariatesCP)\nfit.brp5 = predict(object.brp5, as.data.frame(covariatesCP))\nRMSE.brp5 = sqrt(1/n.obs * sum((BRP5 - fit.brp5)^2))\nRsq.brp5 = sum((fit.brp5 - mean(BRP5))^2)/sum((BRP5 - mean(BRP5))^2)\nRadj.brp5 = 1 - ((1 - Rsq.brp5) * (n.obs - 1))/(n.obs - 5 - 1)\n# Regression of the average excess return on all forward rates\navg.brp = (BRP2 + BRP3 + BRP4 + BRP5)/4\nobject.avgbrp = lm(avg.brp ~ covariatesCP)\nfit.avgbrp = predict(object.avgbrp, as.data.frame(covariatesCP))\n# Finding out b_n coefficients\nobject.2 = lm(BRP2 ~ fit.avgbrp)\nb2 = object.2$coefficients[2]\nobject.3 = lm(BRP3 ~ fit.avgbrp)\nb3 = object.3$coefficients[2]\nobject.4 = lm(BRP4 ~ fit.avgbrp)\nb4 = object.4$coefficients[2]\nobject.5 = lm(BRP5 ~ fit.avgbrp)\nb5 = object.5$coefficients[2]\n# Fitting rx_(t+1)^n, n = 2, 3, 4, 5\nfit.brp2cp = b2 * fit.avgbrp\nRMSE.brp2cp = sqrt(1/n.obs * sum((BRP2 - fit.brp2cp)^2))\nRsq.brp2cp = sum((fit.brp2cp - mean(BRP2))^2)/sum((BRP2 - mean(BRP2))^2)\nRadj.brp2cp = 1 - ((1 - Rsq.brp2cp) * (n.obs - 1))/(n.obs - 1 - 1)\nfit.brp3cp = b3 * fit.avgbrp\nRMSE.brp3cp = sqrt(1/n.obs * sum((BRP3 - fit.brp3cp)^2))\nRsq.brp3cp = sum((fit.brp3cp - mean(BRP3))^2)/sum((BRP3 - mean(BRP3))^2)\nRadj.brp3cp = 1 - ((1 - Rsq.brp3cp) * (n.obs - 1))/(n.obs - 1 - 1)\nfit.brp4cp = b4 * fit.avgbrp\nRMSE.brp4cp = sqrt(1/n.obs * sum((BRP4 - fit.brp4cp)^2))\nRsq.brp4cp = sum((fit.brp4cp - mean(BRP4))^2)/sum((BRP4 - mean(BRP4))^2)\nRadj.brp4cp = 1 - ((1 - Rsq.brp4cp) * (n.obs - 1))/(n.obs - 1 - 1)\nfit.brp5cp = b5 * fit.avgbrp\nRMSE.brp5cp = sqrt(1/n.obs * sum((BRP5 - fit.brp5cp)^2))\nRsq.brp5cp = sum((fit.brp5cp - mean(BRP5))^2)/sum((BRP5 - mean(BRP5))^2)\nRadj.brp5cp = 1 - ((1 - Rsq.brp5cp) * (n.obs - 1))/(n.obs - 1 - 1)\n# Ludvigson and Ng (2009): model with forward rates and macro factors\n# Find first 8 factors as described in Ludvigson and Ng (2009)\nobject.PCA = princomp(scale(covariatesLN), cor = FALSE)\nfactorsLN = object.PCA$scores[, 1:8]\n# Five factor + forward factor\nF5 = cbind(factorsLN[, 1], factorsLN[, 1]^3, factorsLN[, c(3, 4, 8)],\nfit.avgbrp)\nobject.brp2F5 = lm(BRP2 ~ F5)\nfit.brp2F5 = predict(object.brp2F5, as.data.frame(F5))\nRMSE.brp2F5 = sqrt(1/n.obs * sum((BRP2 - fit.brp2F5)^2))\nRsq.brp2F5 = sum((fit.brp2F5 - mean(BRP2))^2)/sum((BRP2 - mean(BRP2))^2)\nRadj.brp2F5 = 1 - ((1 - Rsq.brp2F5) * (n.obs - 1))/(n.obs - 6 - 1)\nobject.brp3F5 = lm(BRP3 ~ F5)\nfit.brp3F5 = predict(object.brp3F5, as.data.frame(F5))\nRMSE.brp3F5 = sqrt(1/n.obs * sum((BRP3 - fit.brp3F5)^2))\nRsq.brp3F5 = sum((fit.brp3F5 - mean(BRP3))^2)/sum((BRP3 - mean(BRP3))^2)\nRadj.brp3F5 = 1 - ((1 - Rsq.brp3F5) * (n.obs - 1))/(n.obs - 6 - 1)\nobject.brp4F5 = lm(BRP4 ~ F5)\nfit.brp4F5 = predict(object.brp4F5, as.data.frame(F5))\nRMSE.brp4F5 = sqrt(1/n.obs * sum((BRP4 - fit.brp4F5)^2))\nRsq.brp4F5 = sum((fit.brp4F5 - mean(BRP4))^2)/sum((BRP4 - mean(BRP4))^2)\nRadj.brp4F5 = 1 - ((1 - Rsq.brp4F5) * (n.obs - 1))/(n.obs - 6 - 1)\nobject.brp5F5 = lm(BRP5 ~ F5)\nfit.brp5F5 = predict(object.brp5F5, as.data.frame(F5))\nRMSE.brp5F5 = sqrt(1/n.obs * sum((BRP5 - fit.brp5F5)^2))\nRsq.brp5F5 = sum((fit.brp5F5 - mean(BRP5))^2)/sum((BRP5 - mean(BRP5))^2)\nRadj.brp5F5 = 1 - ((1 - Rsq.brp5F5) * (n.obs - 1))/(n.obs - 6 - 1)\n# Six factor model\nF6 = cbind(factorsLN[, 1], factorsLN[, 1]^3, factorsLN[, c(2, 3, 4, 8)])\nobject.brp2F6 = lm(BRP2 ~ F6)\nfit.brp2F6 = predict(object.brp2F6, as.data.frame(F6))\nRMSE.brp2F6 = sqrt(1/n.obs * sum((BRP2 - fit.brp2F6)^2))\nRsq.brp2F6 = sum((fit.brp2F6 - mean(BRP2))^2)/sum((BRP2 - mean(BRP2))^2)\nRadj.brp2F6 = 1 - ((1 - Rsq.brp2F6) * (n.obs - 1))/(n.obs - 6 - 1)\nobject.brp3F6 = lm(BRP3 ~ F6)\nfit.brp3F6 = predict(object.brp3F6, as.data.frame(F6))\nRMSE.brp3F6 = sqrt(1/n.obs * sum((BRP3 - fit.brp3F6)^2))\nRsq.brp3F6 = sum((fit.brp3F6 - mean(BRP3))^2)/sum((BRP3 - mean(BRP3))^2)\nRadj.brp3F6 = 1 - ((1 - Rsq.brp3F6) * (n.obs - 1))/(n.obs - 6 - 1)\nobject.brp4F6 = lm(BRP4 ~ F6)\nfit.brp4F6 = predict(object.brp4F6, as.data.frame(F6))\nRMSE.brp4F6 = sqrt(1/n.obs * sum((BRP4 - fit.brp4F6)^2))\nRsq.brp4F6 = sum((fit.brp4F6 - mean(BRP4))^2)/sum((BRP4 - mean(BRP4))^2)\nRadj.brp4F6 = 1 - ((1 - Rsq.brp4F6) * (n.obs - 1))/(n.obs - 6 - 1)\nobject.brp5F6 = lm(BRP5 ~ F6)\nfit.brp5F6 = predict(object.brp5F6, as.data.frame(F6))\nRMSE.brp5F6 = sqrt(1/n.obs * sum((BRP5 - fit.brp5F6)^2))\nRsq.brp5F6 = sum((fit.brp5F6 - mean(BRP5))^2)/sum((BRP5 - mean(BRP5))^2)\nRadj.brp5F6 = 1 - ((1 - Rsq.brp5F6) * (n.obs - 1))/(n.obs - 6 - 1)\n# PAM with forward rates and macro factors\n# Define response variable and design matrix\nY.real = BRP2\nY.set = rev(Y.real)\nX.set = apply(cbind(covariatesCP, covariatesLN[, c(1, 3, 6, 19, 23, 49, 70,\n80, 84:100, 110, 113)]), 2, rev)\n# Define pre-specified parameters\nn.years = 4 # Number of years as increment between successive subintervals\nn.boot = 1000 # Number of bootstrapped multipliers\nmb.type = \"Pois\" # Bound, Exp or Pois\n# Function for computing PAM on a given dataset\nPAM.fit = function(X, Y, n.years){\n\n# Initial settings\nn.par <<- ncol(X) # Number of parameters in the model\nsd.eps <<- 1 # Assumed standard deviation\na <<- 3.7 # Second parameter of SCAD method\nn.obsy = length(Y)/n.obs * 12 # Number of observations per year\nb.pts = numeric(0) # Vector of change points\nm = 1\nbeta = list()\n# Definition of a sequence K and M (equal increments between successive subintervals)\nn.tmp = n.obsy * n.years\nif ((nrow(X)%%n.tmp) == 0){\nK = rep(n.tmp, (nrow(X)%/%n.tmp))\n} else {\nK = c(rep(n.tmp, (nrow(X)%/%n.tmp - 1)),\n(nrow(X) - n.tmp * nrow(X)%/%n.tmp + n.tmp))\n}\nM = length(K)\nK.seq = 0\nfor (i in 1:length(K)){\nK.seq = c(K.seq, sum(K[1:i]))\n# 1.step: Fit the model for assumed homogeneous interval I_t^(1)\nX.tmp.L = X[(K.seq[k1] + 1):K.seq[k2], ]\nn.obs.tmp.L = length(Y.tmp.L)\nobject.tmp.L = Onestep.SCAD(X.tmp.L, Y.tmp.L, a, n.obs.tmp.L)\nbeta[[m]] = object.tmp.L$beta\nwhile (m < M){\n# Fit the model over the next interval I_t^(k + 1) - I_t^(k)\nX.tmp.R = X[(K.seq[k2] + 1):K.seq[(k2 + 1)], ]\nn.obs.tmp.R = length(Y.tmp.R)\nobject.tmp.R = Onestep.SCAD(X.tmp.R, Y.tmp.R, a, n.obs.tmp.R)\nbeta.tmp.R = object.tmp.R$beta\n\n# Fit the model over the whole interval I_t^(k + 1)\nX.tmp.T = X[(K.seq[k1] + 1):K.seq[(k2 + 1)], ]\nn.obs.tmp.T = length(Y.tmp.T)\nobject.tmp.T = Onestep.SCAD(X.tmp.T, Y.tmp.T, a, n.obs.tmp.T)\nbeta.tmp.T = object.tmp.T$beta\nlik.T = loglik.pen(a.tmp.T, beta0.tmp.T, lambda.tmp.T,\nX.tmp.T,\n# Simulation of multipliers u_i (i = 1, ..., n.obs.tmp.T)\nset.seed = 20170424 * m * 10\nif (mb.type == \"Bound\"){\nmultipliers = matrix(runif.mod(n.boot * n.obs.tmp.T),\nncol = n.obs.tmp.T, nrow = n.boot) # Bounded distribution\n}\nif (mb.type == \"Exp\"){\nmultipliers = matrix(rexp(n.boot * n.obs.tmp.T, rate = 1),\nncol = n.obs.tmp.T, nrow = n.boot) # Exp(1) distribution\nif (mb.type == \"Pois\"){\nmultipliers = matrix(rpois(n.boot * n.obs.tmp.T, lambda = 1),\nncol = n.obs.tmp.T, nrow = n.boot) # Pois(1) distribution\nMB.ratios = numeric(0)\nsel = 0 # Sequence of points where to divide new subinterval\nfor (ind in 1:length(sel)){\nlik.ratio = numeric(0)\n\n# 1.step: Fit the model for assumed homogeneous interval I_t^(k,s)\nX.sel.L = X[(K.seq[k1] + 1):(K.seq[k2] + sel[ind]), ]\nn.obs.sel.L = length(Y.sel.L)\nobject.sel.L = Onestep.SCAD(X.sel.L, Y.sel.L, a, n.obs.sel.L)\nbeta.sel.L = object.sel.L$beta\n# 2. a) step: Fit the model over the next interval I_t^(k + 1) - I_t^(k,s)\nX.sel.R = X[(K.seq[k2] + 1 + sel[ind]):K.seq[(k2 + 1)], ]\nn.obs.sel.R = length(Y.sel.R)\nobject.sel.R = Onestep.SCAD(X.sel.R, Y.sel.R, a, n.obs.sel.R)\nbeta.sel.R = object.sel.R$beta\n# 2. b) step: Fit the model over the whole interval I_t^(k + 1)\nX.sel.T = X.tmp.T\n# 3.step: Evaluate the test statistic\n# Real likelihood ratio\nlik.sel.L = loglik.pen(a.sel.L, beta0.sel.L, lambda.sel.L,\nX.sel.L,\nlik.sel.R = loglik.pen(a.sel.R, beta0.sel.R, lambda.sel.R,\nX.sel.R,\nlik.sel.T = lik.T\nlik.sel = c(lik.sel, (((n.obs.sel.L/n.obs.sel.T) * lik.sel.L)\n+ ((n.obs.sel.R/n.obs.sel.T) * lik.sel.R) - lik.sel.T))\nfor (l in 1:(n.boot)){\n# Multiplier bootstrap for left-hand interval I_t^(k,s)\nmultipl.L = multipliers[l, 1:n.obs.sel.L]\n\nboot.object.L = Onestep.SCAD.MB(X.sel.L, Y.sel.L, a, n.obs.sel.L,\nas.numeric(lambda.sel.L), multipl.L)\nboot.beta.L = boot.object.L$beta\nlik.boot.L = loglik.pen.MB(boot.a.L, boot.beta0.L,\nas.numeric(lambda.sel.L), X.sel.L,\nmultipl.L)\n# Multiplier bootstrap for right-hand interval I_t^(k + 1) - I_t^(k,s)\nmultipl.R = multipliers[l, (n.obs.sel.L + 1):n.obs.sel.T]\nboot.object.R = Onestep.SCAD.MB(X.sel.R, Y.sel.R, a, n.obs.sel.R,\nas.numeric(lambda.sel.R), multipl.R)\nboot.beta.R = boot.object.R$beta\nlik.boot.R = loglik.pen.MB(boot.a.R, boot.beta0.R,\nas.numeric(lambda.sel.R), X.sel.R,\nmultipl.R)\n# Multiplier bootstrap for the whole interval I_t^(k + 1) (with shift)\nmultipl = multipliers[l, ]\nboot.object.T = Onestep.SCAD.MB.shift(X.sel.L, Y.sel.L, a,\nlambda.sel.L, multipl.L,\nboot.beta.T = boot.object.T$beta\nY.boot.T = c(Y.sel.L, - rep((a.sel.R - a.sel.L), n.obs.sel.R)\n- X.sel.R %*% (beta.sel.R - beta.sel.L)))\nlik.boot.T = loglik.pen.MB(boot.a.T, boot.beta0.T, boot.lambda.T,\nX.sel.T, Y.boot.T, multipl)\nlik.ratio = c(lik.ratio, ((n.obs.sel.L/n.obs.sel.T) * lik.boot.L\n- lik.boot.T))\n}\nMB.ratios = cbind(MB.ratios, lik.ratio)\n# Find maximum over the real and bootstrapped likelihood ratios\nt.stat = max(lik.sel)\nMB.ratios.max = apply(MB.ratios, 1, max)\nq90 = quantile(MB.ratios.max, probs = 0.9)\nk2 = k2 + 1\n# 5.step: Test for homogeneity and evaluate current beta estimator\nif (t.stat <= q95){\nfor (k3 in k1:m){\nbeta[[k3]] = beta.tmp.T\n} else {\nb.pts = c(b.pts, (K.seq[m] + 1))\nk1 = k2 - 1\nbeta[[m]] = beta.tmp.R\n}\n# Find fitted values of Y\nY.fitted = numeric(0)\nb.pts.seq = c(1, b.pts, (K.seq[length(K.seq)] + 1))\nlb.pts.seq = length(b.pts.seq)\nfor (k in 1:(lb.pts.seq - 1)){\nk1 = b.pts.seq[k]\nk3 = which(K.seq == k2) - 1\nY.tmp = Y[k1:k2]\nno.tmp = length(Y.tmp)\nX.tmp = X[k1:k2,]\nY.fitted = c(Y.fitted, (X.tmp %*% beta[[k3]] + rep(a0[[k3]], no.tmp)))\n# Compute average number of nonzero coefficients\nact.set = numeric(0)\nfor (m in 1:M){\nact.set = c(act.set, sum(beta[[m]] != 0))\nvalues = list(Y.fitted, beta, a0, b.pts, mean(act.set))\nnames(values) = c(\"Y.fitted\", \"beta\", \"a0\", \"b.pts\", \"as.avg\")\nreturn(values)\n}\n# Fit the model to selected data\nobject.pam = PAM.fit(X.set, Y.set, n.years)\nY.fittmp = object.pam$Y.fitted\nY.fitted = rev(Y.fittmp)\nRMSE.pam = sqrt(1/n.obs * sum((Y.real - Y.fitted)^2))\nRsq.pam = sum((Y.fitted - mean(Y.real))^2)/sum((Y.real - mean(Y.real))^2)\nRadj.pam = 1 - ((1 - Rsq.pam) * (n.obs - 1))/(n.obs - object.pam$as.avg - 1)\n# Plot all of the models vs. observations\npar(mfrow = c(1 ,1))\nat.tmp = c(grep(\"1970\", dates)[1], grep(\"1980\", dates)[1], grep(\"1990\", dates)[1],\ngrep(\"2000\", dates)[1]) - (start.date) + 1\nplot(Y.real, type = \"l\", col = \"darkgray\",\nylim = c((min(Y.real)-0.007),\naxes = FALSE, xlab = \"Year\", frame = TRUE,\nylab = expression(paste(\"rx \" [t+1]) ^ {(2)}), cex.lab = 1.1, lwd = 1.3)\naxis(1, cex.axis = 1, labels = seq(1970, 2000, 10), at = at.tmp)\naxis(2, cex.axis = 1)\nlines(fit.brp2cp, col = \"red\", lty = 2, lwd = 1.3)\nplot(Y.real, type = \"l\", col = \"darkgray\",\nlines(Y.fitted, col = \"darkgreen\", lty = 2, lwd = 1.3)\n", "output_sequence": "\u2018Performs the Penalized Adaptive Method (PAM), a combination of propagation-separation approach and a SCAD penalty, to fit a model with possible structural changes on a given dataset. Compares the fit to models of Cochrane and Piazzesi (2005) and Ludvigson and Ng (2009). The input data are monthly observations of k-year excess bond risk premia, k = 2, 3, 4, 5, forward rates and other pre-defined macro variables. Computes RMSE, MAE, R^2 and adjusted R^2 for the fitted models. Plots the time series of the fitted vs. observed values of bond risk excess premia.'"}, {"input_sequence": "Author: Lenka Zbo\u0148\u00e1kov\u00e1 ; # -------------------------------------------------------------------------------\n# One-step SCAD with mutl\u00edplier bootstrap\n# -------------------------------------------------------------------------------\n# Define function for computing derivative of the SCAD penalty\npen.prime = function(beta, lambda, a){\nindicator = ifelse(abs(beta) <= lambda, 1, 0)\ntmp = numeric(0)\nfor (jpp in 1:length(beta)){\ntmp[jpp] = max(a * lambda - abs(beta[jpp]), 0)\n}\npen.value = (lambda * indicator) + ((tmp/(a - 1)) * (1 - indicator))\npen.value\n}\n# Define function for computing the SCAD penalty\npen.scad = function(beta, lambda, a){\npen.value = numeric(0)\nfor (jps in 1:length(beta)){\nif (abs(beta[jps]) <= lambda){\npen.value[jps] = lambda * abs(beta[jps])\n} else if (lambda < abs(beta[jps]) && abs(beta[jps]) <= a * lambda){\npen.value[jps] = - ((abs(beta[jps])^2 - 2 * a * lambda * abs(beta[jps]) + lambda^2)/\n(2 * (a - 1)))\n} else {\npen.value[jps] = ((a + 1) * lambda^2)/2\n}\n# Define function for submatrix X_U\nXU.fct = function(X, beta, lambda, a){\npen = pen.prime(beta, lambda, a) # Evaluate penalty function\nif (min(pen) == 0){\nU.index = which(pen == 0)\nX.U = as.matrix(X[, U.index])\n} else {\nX.U = matrix(c(-Inf, -Inf, nrow = 2, ncol = 2)\nX.U\n# Define function for submatrix X_V\nXV.fct = function(X, beta, lambda, a){\npen = pen.prime(beta, lambda, a) # Evaluate penalty function\nif (max(pen) == 0){\nX.V = matrix(c(-Inf, -Inf, nrow = 2, ncol = 2)\nbeta.new = numeric(0)\nV.index = which(pen != 0)\npen.new = pen[V.index]\nX.V = as.matrix(X[, V.index])\nfor (jxv in 1:length(V.index)){\nX.V[, jxv] = X.V[, jxv] * lambda/ (pen.new[jxv])\nvalues = list(X.V, beta.new)\nnames(values) = c(\"Xmatrix\", \"betapar\")\nreturn(values)\n# Define function for columns' order in X_U and X_V\norder.fct = function(X, beta, lambda, a){\npen = pen.prime(beta, lambda, a) # Evaluate penalty function\norder = seq(1, dim(X)[2], 1)\nU.index = which(pen == 0)\nU.order = order[U.index]\norder.new = c(U.order, V.order)\n\norder.new\n# Define function for finding real X.V coefficients\nV.coeff.fct = function(coeff, beta, lambda, a){\npen = pen.prime(beta, lambda, a) # Evaluate penalty function\npen.new = pen[V.index]\nV.coeff = numeric(0)\nfor (jvc in 1:length(V.index)){\nV.coeff[jvc] = coeff[jvc] * lambda / (pen.new[jvc]) # Transform coeff.'s back\nV.coeff\n# Define function to select a grid of lambda values\ngrid.fct = function(n.obs, Y, X){\none = rep(1, n.obs)\nX.mean = drop(one %*% X)/n.obs\nX.cent = scale(X, X.mean, FALSE)\nX.norm = sqrt(drop(one %*% (X.cent^2)/n.obs))\nX.maxl = scale(X.cent, FALSE, X.norm)\nY.mean = drop(one %*% Y)/n.obs\nY.maxl = scale(Y, Y.mean, FALSE)\nmax.lambda.tmp = numeric(0)\nfor (j in 1:dim(X.maxl)[2]){\nmax.lambda.tmp = c(max.lambda.tmp, t(as.vector(Y.maxl)) %*% X.maxl[, j])\nmax.lambda = max.lambda.tmp[which.max(max.lambda.tmp)]\nlambda.grid = c(max.lambda/(n.obs^seq(0.49, 0.01, -0.03)*1000)\nlambda.grid\n# Define function to select a grid of lambda values for the prolonged interval\n# under homogeneity\ngrid.fct.T = function(n.obs, Y, X){\nn.obs.L = length(Y)\none = rep(1, n.obs.L)\nX.mean = drop(one %*% X)/n.obs.L\nX.norm = sqrt(drop(one %*% (X.cent^2)/n.obs.L))\nY.mean = drop(one %*% Y)/n.obs.L\n, max.lambda/(n.obs^seq(0.49, 0.01, -0.03)/100))\n# Define function to compute One-step SCAD algorithm, Li & Zou (2008)\nOnestep.SCAD = function(X, Y, a, n.obs){\nbeta.tmp = list()\nlambda.grid = grid.fct(n.obs, Y, X)\n# Normalize design X (mean = 0, var = 1) and response Y (mean = 0)\nX.orig = X\nX.norm0 = sqrt(drop(one %*% (X.cent^2)/n.obs))\nX.norm = scale(X.cent, FALSE, X.norm0)\nY.norm = scale(Y, Y.mean, FALSE)\nX = X.norm\n# 0. STEP: Find initial values of coefficients - unpenalized fit\nobject.ols = glmnet(X.norm2, Y.orig, family = \"gaussian\", alpha = 0, lambda = 0,\nstandardize = FALSE, intercept = TRUE)\n# Beta with normalisation - to be used in the next step\nbeta.fit.ols = as.vector(object.ols$beta)\n# Original unpenalized coefficients\nbeta.0 = as.vector(object.ols$beta)/X.norm0\na.0 = object.ols$a0\nmu0.hat = X.norm %*% beta.fit.ols\n# 1. STEP: Create working data\n# 1.a)\nY.star = mu0.hat # Create response according to the inital fit (not centralised)\nX.star = as.matrix(X.norm) # Scaled but nor centralised matrix X\n# Compute the best couple (lambda, beta(lambda)) for the algorithm\nfor (llam in 1:length(lambda.grid)){\n\nlambda1 = lambda.grid[llam]\n# 1.b)\nX.U = XU.fct(X.star, beta.fit.ols, lambda1, a)\nX.V = XV.tmp$Xmatrix\nV.beta = XV.tmp$betapar\nord = order.fct(X.star, beta.fit.ols, lambda1, a)\n# 1.c)\nif (X.V[1, 1] != -Inf){\nY.2star = Y.star - X.U %*% ginv(t(X.U) %*% X.U) %*% t(X.U) %*% Y.star\n} else {\nY.2star = Y.star\n}\n# 2. STEP: Lasso estimation using CV\nif (dim(X.V)[2] == 1){\nV.coeff.tmp = ifelse(abs(V.beta) <= 2 * lambda1,\nsign(V.beta) * max((abs(V.beta) - lambda1), 0),\n((a - 1) * V.beta - sign(V.beta) * a * lambda1)/(a - 2))\n\nV.coeff = V.coeff.tmp\nobject = glmnet(X.V2star, Y.2star, family = \"gaussian\", alpha = 1,\nlambda = lambda1, standardize = FALSE, intercept = FALSE)\nV.coeff.tmp = as.vector(object$beta) # Extract coefficients from the fit\nV.coeff = V.coeff.fct(V.coeff.tmp, beta.fit.ols, lambda1, a)\n# 3. STEP: Coefficients associated with X_U and X_V\nU.coeff = as.vector(ginv(t(X.U) %*% X.U) %*% t(X.U)\n%*% (Y.star - X.V %*% V.coeff.tmp))\nU.coeff = numeric(0)\ncoeff.tmp2 = rbind(ord, c(U.coeff, V.coeff)) # Fitted coefficients\ncoeff = coeff.tmp2[2, order(coeff.tmp2[1, ])]\nact.set.tmp = sum(coeff != 0) # No. of nonzero coefficients\nbeta.fit = coeff/X.norm0\n} else if (X.V[1, 1] == -Inf){\n# 2. STEP: Skip - no V coefficients\nU.coeff = as.vector(ginv(t(X.U) %*% X.U) %*% t(X.U) %*% Y.star)\nV.coeff = numeric(0)\nbeta.fit = coeff/X.norm0\nact.set[[llam]] = act.set.tmp\nbeta.tmp[[llam]] = beta.fit\na0.tmp[[llam]] = Y.mean - (X.mean %*% beta.tmp[[llam]])\nbic = numeric(0)\nfor (lbic in 1:length(lambda.grid)){\nbic[lbic] = (log(n.obs) * act.set[[lbic]]/n.obs * max(log(log(n.par)),\nsqrt(n.obs)/n.par)\n+ log((t(Y.orig - rep(a0.tmp[[lbic]], n.obs) -\nX.orig %*% beta.tmp[[lbic]]) %*%\nindex = which.min(bic)\nvalues = list(a.0, a0.tmp[[index]], beta.0, beta.tmp[[index]],\nlambda.grid[index], act.set[[index]], index, bic[index])\nnames(values) = c(\"a.0\", \"a\", \"beta.0\", \"lambda\", \"act.set\", \"index\", \"bic\")\n# Define function to compute One-step SCAD algorithm combined with multiplier bootstrap\nOnestep.SCAD.MB = function(X, Y, a, n.obs, lambda.value, multipl){\nbeta.tmp = list()\nX.mb.mean.vector = t((t(X.orig) %*% multipl))/sum(multipl)\nX.mb.mean.matrix = matrix(rep(X.mb.mean.vector, n.obs),\nncol= length(X.mb.mean.vector), byrow = TRUE)\nX.mb = (X.orig - X.mb.mean.matrix) * sqrt(multipl)\nY.mb = (Y.orig - ((t(Y.orig) %*% multipl)/sum(multipl))) * sqrt(multipl)\nX.mbnorm = scale(X.mb, FALSE, X.norm0)\nobject.ols = glmnet(X.mbnorm, Y.mb, family = \"gaussian\", alpha = 0, lambda = 0,\nstandardize = FALSE, intercept = FALSE)\n# Original OLS coefficients\na.0 = (((t(Y.orig) %*% multipl)/sum(multipl))\n- ((t((t(X.orig) %*% multipl)) %*% beta.0)/sum(multipl)))\nmu0.hat = X.mbnorm %*% beta.fit.ols # Initial fit\nY.star = mu0.hat # Create response according to the inital fit\nX.star = X.mbnorm\nlambda = lambda.value\n# 1.b)\nX.U = XU.fct(X.star, beta.fit.ols, lambda, a)\nX.V = XV.tmp$Xmatrix\nV.beta = XV.tmp$betapar\nord = order.fct(X.star, beta.fit.ols, lambda, a)\n# 1.c)\nif (X.V[1, 1] != -Inf){\nY.2star = Y.star - X.U %*% ginv(t(X.U) %*% X.U) %*% t(X.U) %*% Y.star\nY.2star = Y.star\n# 2. STEP: Lasso estimation using CV\nif (dim(X.V)[2] == 1){\nV.coeff.tmp = ifelse(abs(V.beta) <= 2 * lambda,\nsign(V.beta) * max((abs(V.beta) - lambda), 0),\n((a - 1) * V.beta - sign(V.beta) * a * lambda)/(a - 2))\nV.coeff = V.coeff.tmp\nobject = glmnet(X.V2star, Y.2star, family = \"gaussian\", alpha = 1,\nlambda = lambda.value, standardize = FALSE,\nintercept = FALSE)\nV.coeff.tmp = as.vector(object$beta) # Extract coefficients from the fit\nV.coeff = V.coeff.fct(V.coeff.tmp, beta.fit.ols, lambda, a)\n# 3. STEP: Coefficients associated with X_U and X_V\nU.coeff = as.vector(ginv(t(X.U) %*% X.U) %*% t(X.U)\n%*% (Y.star - X.V %*% V.coeff.tmp))\nU.coeff = numeric(0)\ncoeff.tmp2 = rbind(ord, c(U.coeff, V.coeff)) # Fitted coefficients\ncoeff = coeff.tmp2[2, order(coeff.tmp2[1, ])]\nact.set = sum(coeff != 0) # No. of nonzero coefficients\nbeta.fit = coeff/X.norm0\n} else if (X.V[1, 1] == -Inf){\n# 2. STEP: Skip - no V coefficients\nU.coeff = as.vector(ginv(t(X.U) %*% X.U) %*% t(X.U) %*% Y.star)\nV.coeff = numeric(0)\nbeta.fit = coeff/X.norm0\nlambda.fit = lambda.value\na0.tmp = (((t(Y.orig) %*% multipl)/sum(multipl))\n- ((t((t(X.orig) %*% multipl)) %*% beta.fit)/sum(multipl)))\nbic = (log(n.obs) * act.set/n.obs * max(log(log(n.par)), sqrt(n.obs)/n.par)\n+ log((t(Y.mb - X.mb %*% beta.fit) %*% (Y.mb - X.mb %*% beta.fit))/n.obs))\nvalues = list(a.0, a0.tmp, beta.0, beta.fit, lambda.fit, act.set, Y.mean, bic)\nnames(values) = c(\"a.0\", \"a\", \"beta.0\", \"lambda\", \"act.set\", \"intercept\", \"bic\")\n# Define function to compute One-step SCAD algorithm for the prolonged interval\nOnestep.SCAD.MB.shift = function(X.tmp.L, Y.tmp.L, a, lambda.L,\nlambda.R, multipl.L, beta.L,\na.L, a.R){\nbeta.tmp = list()\nn.obs.L = length(Y.tmp.L)\n# Normalize design X.T (mean = 0, var = 1) and response Y.T (mean = 0)\nY.orig.T = c(Y.tmp.L, Y.tmp.R)\nmultipl.T = c(multipl.L,\nn.obs.T = n.obs.L + n.obs.R\none.T = rep(1, n.obs.T)\nX.mean.T = drop(one.T %*% X.orig.T)/n.obs.T\nX.cent.T = scale(X.orig.T, X.mean.T, FALSE)\nX.norm0.T = sqrt(drop(one.T %*% (X.cent.T^2)/n.obs.T))\nX.norm.T = scale(X.cent.T, FALSE, X.norm0.T)\nX.mb.mean.vector = t((t(X.orig.T) %*% multipl.T))/sum(multipl.T)\nX.mb.mean.matrix = matrix(rep(X.mb.mean.vector, n.obs.T),\nX.mb.T = (X.orig.T - X.mb.mean.matrix) * sqrt(multipl.T)\nX.mbnorm.T = scale(X.mb.T, FALSE, X.norm0.T)\nY.T = c(Y.tmp.L, - (rep((a.R - a.L), n.obs.R)\n+ X.tmp.R %*% (beta.R - beta.L))))\nY.mb.T = (Y.T - ((t(Y.T) %*% multipl.T)/sum(multipl.T))) * sqrt(multipl.T)\n# 0. STEP: Find initial values of coefficients - penalized fit\nobject.ols = glmnet(X.mbnorm.T, Y.mb.T, family = \"gaussian\", alpha = 0, lambda = 0,\nstandardize = FALSE, intercept = FALSE)\nbeta.0 = as.vector(object.ols$beta)/X.norm0.T\na.0 = (((t(Y.T) %*% multipl.T)/sum(multipl.T))\n- ((t((t(X.orig.T) %*% multipl.T)) %*% beta.0)/sum(multipl.T)))\nmu0.hat = X.mbnorm.T %*% beta.fit.ols # Initial fit\nY.star = mu0.hat # Create response according to the inital fit\nX.star = X.mbnorm.T\nindex = which(grid.fct(length(Y.tmp.L), Y.tmp.L, == lambda.L)\nlambda1 = grid.fct.T(length(Y.star), Y.tmp.L, X.tmp.L)[index]\nX.U = XU.fct(X.star, beta.fit.ols, lambda1, a)\nX.V = XV.tmp$Xmatrix\nV.beta = XV.tmp$betapar\nord = order.fct(X.star, beta.fit.ols, lambda1, a)\nV.coeff.tmp = ifelse(abs(V.beta) <= 2 * lambda1,\nsign(V.beta) * max((abs(V.beta) - lambda1), 0),\n((a - 1) * V.beta - sign(V.beta) * a * lambda1)/(a - 2))\nlambda = lambda1, standardize = FALSE, intercept = FALSE)\nV.coeff = V.coeff.fct(V.coeff.tmp, beta.fit.ols, lambda1, a)\nU.coeff = as.vector(ginv(t(X.U) %*% X.U) %*% t(X.U)\nbeta.fit = coeff/X.norm0.T\ncoeff.tmp2 = rbind(ord, c(U.coeff, V.coeff)) # Fitted coefficients\ncoeff = coeff.tmp2[2, order(coeff.tmp2[1, ])]\nbeta.fit = coeff/X.norm0.T\nlambda.fit = lambda1\na0.tmp = (((t(Y.T) %*% multipl.T)/sum(multipl.T))\n- ((t((t(X.orig.T) %*% multipl.T)) %*% beta.fit)/sum(multipl.T)))\nact.set = sum(beta.fit != 0)\nvalues = list(a.0, a0.tmp, beta.0, beta.fit, lambda.fit, act.set)\nnames(values) = c(\"a.0\", \"a\", \"beta.0\", \"lambda\", \"act.set\")\n# Define function to compute bootstrapped penalized log-likelihood function\nloglik.pen.MB = function(a0, beta.0, lambda, X, Y, multipl){\nn.obs = length(Y)\none = rep(1, n.obs)\nX.mean = drop(one %*% X)/n.obs\nX.cent = scale(X, X.mean, FALSE)\nX.norm0 = sqrt(drop(one %*% (X.cent^2)/n.obs))\nX = X * sqrt(multipl)\nvar.eps = 1\nloglik1 = (- (1/(n.obs * 2 * var.eps) * t(Y - X %*% beta) %*% (Y - X %*% beta))\n- (t(pen.prime(beta.0 * X.norm0, lambda, a)) %*% abs(beta * X.norm0)))\nloglik1\n# Define function to compute bootstrapped unpenalized log-likelihood function\nloglik.MB = function(a0, beta, X, Y, multipl){\nn.obs = length(Y)\nX = X * sqrt(multipl)\nloglik1 = (- (1/(n.obs * 2) * t(Y - X %*% beta) %*% (Y - X %*% beta)))\n# Define function to compute penalized log-likelihood function\nloglik.pen = function(a0, beta.0, lambda, X, Y){\nloglik1 = (- (1/(n.obs * 2) *\n(t(Y - rep(a0, n.obs) - X %*% beta)\n# Define function to compute unpenalized log-likelihood function\nloglik = function(a0, beta, X, Y){\nloglik1 = (- (1/(n.obs * 2 ) * t(Y - rep(a0, n.obs)\n- (X %*% beta)) %*% (Y - rep(a0, n.obs)\n- (X %*% beta))))\n", "output_sequence": "\u2018Performs the Penalized Adaptive Method (PAM), a combination of propagation-separation approach and a SCAD penalty, to fit a model with possible structural changes on a given dataset. Compares the fit to models of Cochrane and Piazzesi (2005) and Ludvigson and Ng (2009). The input data are monthly observations of k-year excess bond risk premia, k = 2, 3, 4, 5, forward rates and other pre-defined macro variables. Computes RMSE, MAE, R^2 and adjusted R^2 for the fitted models. Plots the time series of the fitted vs. observed values of bond risk excess premia.'"}, {"input_sequence": "Author: Maria Grith ; % clear variables and close windows\nclear all;\nclc;\n%%% Start READ DATA\nload Results.mat % the workspace results are generated by the code FPCAreal_data.m\nload('xData.txt','-ascii');\n%%% End READ DATA\n%%% Start PLOT FIGURE\n% define axis\naxis_x=linspace(min(mx),max(mx),50); % maturity\n[Xa,Ya] = meshgrid(axis_x,axis_y);\n% interpolate data for the functional components\nFFun1 = TriScatteredInterp(mx,my,V2b(:,1), 'linear');\nZa1b=FFun1(Xa,Ya);\nfigure(2)\nhFig = figure(2);\nset(hFig, 'Position', 0.8*[500 500 1000 900])\nsubplot(3,2,1)\nsurfl(Xa,Ya,-Za1b);\nset(gca,'XTick',[0.50 0.75 1 1.25 1.50])\nxh=xlabel('Moneyness')\nyh=ylabel('Maturity')\nzh=zlabel('$$\\hat{\\gamma}^{(d)}_{1,T}$$')\nset(zh,'Interpreter','latex')\nset([xh,yh,zh],'fontsize',10)\nsubplot(3,2,2)\nplot(xData,-loadsb(1,Is),'blue')\nxlim([min(xData) max(xData)])\nzh=ylabel('$$\\hat{\\delta}_{1,T}$$')\nset(gca,'XTick',tickdatayear([1,3,5,7,9,11]))\nset(gca,'XTickLabel',[2002 2004 2010 2012])\nset(gca, 'LooseInset', get(gca, 'TightInset'));\nset([xh,yh],'fontsize',10)\nsubplot(3,2,5)\nsurfl(Xa,Ya,-Za7b);\nzh=zlabel('$$\\hat{\\gamma}^{(d)}_{7,T}$$')\nsubplot(3,2,6)\nplot(xData,-loadsb(7,Is),'blue')\nzh=ylabel('$$\\hat{\\delta}_{7,T}$$')\nsubplot(3,2,3)\nsurfl(Xa,Ya,-Za3b);\nzh=zlabel('$$\\hat{\\gamma}^{(d)}_{3,T}$$')\nsubplot(3,2,4)\nplot(xData,-loadsb(3,Is),'blue')\nzh=ylabel('$$\\hat{\\delta}_{3,T}$$')\n%%% End PLOT FIGURE\n", "output_sequence": "It interpolates the estimated functional components at a common grid and plots them, together with the estimates loadings time series."}, {"input_sequence": "Author: Maria Grith ; % clear variables and close windows\nclear all;\nclc;\n\n%%add your path here; genpath(folderName) returns a path string that includes folderName and multiple levels of subfolders below folderName.\n%addpath(genpath('your path'));\n%%% Start READ DATA\n% read expiration dates\nfid = fopen('ExpirationDates.txt','rt');\nindata = textscan(fid, '%s %s', 'HeaderLines',1);\nfclose(fid);\nexpdata = indata{1,1};\n[ye, me, de] = datevec(expdata,'dd-mmm-yy');\n% read DAX index\n[a1,a2]=textread('dax_index.txt','%s %f');\ndax=horzcat(a2);\ndate_dax=datenum(a1,'dd.mm.yyyy');\n% read VDAX index\n[a1v,a2v]=textread('vdax.txt','%s %f');\nvdax=horzcat(a2v);\ndate_vdax=datenum(a1v,'dd.mm.yyyy');\n% read interest rate\n[b1,b2,b3,b4,b5,b6,b7,b8,b9,b10,b11,b12,b13,b14,b15,b16]=textread('IRs.dat','%s %f %f');\ndate_ir=datenum(b1,'dd.mm.yyyy');\nIR=horzcat(b2,b3,b4,b5,b6,b7,b8,b9,b10,b11,b12,b13,b14,b15,b16);\n% read call data\n[c11,c12,c13,c14,c15,c16,c17]=textread('C_2002.txt', '%s %f %f');\nc1g=[c11;c21;c31;c41;c51;c61;c71;c81;c91;c101];\n% define maturies for the interest rates\nT=[7/365 14/365 1/12 1];\ndays=unique(c2g);\nday=datenum(c2g, 'dd/mm/yyyy');\nN=length(days);\n%initiate container to store the results\nvdaxc=[];\nR=[];\ndaysa=[];\ncgridx=[];\nx1minc=[];\nfor d=2:(N+1)\ntday=datenum(days((d-1)), 'dd/mm/yyyy');\nselect= find( day==tday ) ;\nc1=c1g(select);\nrefdmy_c=datenum(c2, 'dd/mm/yyyy');\ncd=[refdmy_c, c4, c7];\nnumdate=unique(refdmy_c);\n% compute maturities\nclear tauc\nfor i=1:length(cd)\naa(i)=find(ye==c5(i) & me==c4(i));\ntauc(i)=(datenum(expdata(aa(i)),'dd-mmm-yy')-refdmy_c(i,1))/365;\nend\n% find stock price\ndday=find(date_dax==numdate);\ns=dax(dday);\nodax=[odax,s];\nif(s>0)\ndaysa=[daysa, dday ];\n% find volatility index\nvdday=find(date_vdax==numdate);\nsv=vdax(vdday);\nvdaxc=[vdaxc,sv];\n% find interest rate\nrday=find(date_ir==numdate);\nR=IR(rday,:);\n% interpolate interest rate\nrc=interp1(T,R,tauc,'linear','extrap');\n% date rescaled_strike call_price interest_rate maturity\ncdd=[cd(:,1) cd(:,4)./(s*exp(rc'/100.*tauc')) rc'/100 tauc'];\n% remove observations with maturity larger than one year\ncdd(find(tauc>1),:)=[];\n% sort maturities\n[cdd(:,5), Im] = sort( cdd(:,5) );\ncdd(:,2)=cdd(Im,2);\n% sorting moneyness\nmatu=unique(cdd(:,5));\nfor l=1:length(matu)\nIk=find(cdd(:,5)==matu(l) );\n[cdd(Ik,2),Il] =sort(cdd(Ik,2));\ncdd(Ik,3)=cdd((min(Ik)-1)+Il,3);\nend\n% interpolate data for FPCA\nF = TriScatteredInterp(cdd(:,5),cdd(:,2),cdd(:,3), 'linear');\n% store interpolated curves\nFc{d-1}=F;\n% find smallest common grid\nx1minc=[ min(cdd(:,5))];\ncgridx=union(cgridx,cdd(:,5));\nc5unil{d-1}= cdd(:,5); % maturity\nend\n%%% End READ DATA\n%%% Start ESTIMATION\nx1min =median( x1minc );\nx2max =1.4; % or use median( x2maxc );\n% construct common grid\nmy =x1min + ( x1max-x1min ).*rand( 512 , 1 ); % maturity\n% estimate the variances\nsigma=zeros(N,1);\nparfor i=1:N\nsigma(i)=FPCAvariance(cell2mat( c5unil(i) ),cell2mat( c2unil(i) ),cell2mat( c3unil(i) ) );\nL=3; % low dimensional space parameter\nmethod=1; % decomposition of the dual covariance matrix of the observed curves\n% estimation using FPCAgpu\n[hX2b, V2b, loadsb,Meansmo2bb,Db]=FPCAgpu(L,Fc,x1minc,x2minc,x1maxc,x2maxc,cgridx,cgridy,c5unil,c2unil,c3unil,c3unil,N,mx,my,method,'cpu',sigma,1);\n%%% End ESTIMATION\n", "output_sequence": "Estimates state price densities (SPD) from a sample of discretely observed and noisy call price curves at different time to maturity and strike prices. Use FPCA decomposition of the dual covariance matrix and estimate functional components of the SPD and their corresponding loadings."}, {"input_sequence": "Author: Heiko Wagner ; function [sigma] = FPCAvariance( X1, Y)\nx1 =X1;\nkernel ='';\nN1 = length( X1 );\nd =2;\nh0 =T^(-2/(4+d));\nWges =zeros(N1,T);\nfor (j=1:T)\nXmx1 = X1 - x1(j);\nif(strcmp(kernel,'Gauss')==1)\nW = normpdf(Xmx1/h0) ;\n%'using gaussian kernel'\nelse\nW = FPCAepan(Xmx1/h0) ;\nend\nWges(:,j) =W/sum(W);\nend\nv = T - 2 * sum(diag(Wges)) + sum(sum( Wges.^2)) ;\nsigma = 1/v* sum( (Y-Wges'*Y).^2 );\n", "output_sequence": "Calculates two-dimensional variance estimator, based on Hall and Marron (1990)."}, {"input_sequence": "Author: Heiko Wagner ; function [Fc,x1minc,x2minc,x1maxc,x2maxc,cgridx,cgridy,c5unil,c2unil,c3unil,c3unilr,realD,mx,my]=FPCAsimulate_input(N,k,T)\nx1minc =[];\nQs =[];\ntaugrid =[];\n%simulate data\nrandn('state',k+6);\nino =randn(1,N+2)';\nr =0.02; %interest rate\nsigma =0.1; %observation error\nTout =256; %number of observations true model\n%simulate stock price\nst(1)=100;\nfor i=2:(N+1)\nst(i)=100;\nend\n%parameters of the mixtures\nm =[0.4 0.7 0.1]; %mean\ns =[0.5 0.3 0.3]; %standard deviation\n%weights of the mixture\nrand('state',k+1);\np =abs( randn(3,N+1 ) );\np =p./repmat(sum(p),3,1);\nXj =[];\nfor j=1:T\ntau = 0.2+rand( 1 , 1 )*0.5 ; %maturity\nfor l=1:3\nqu(l,:) =1./(x*sqrt(2*pi*s(l)^2*tau)).*exp(-1/2*((log(x)- (m(l)-s(l)^2/2)*tau)/(s(l)*sqrt(tau))).^2); %density components of the mixture\nCa(l,:) =exp(m(l)*tau)*normcdf((log(1/x)+(m(l)+s(l)^2/2)*tau)/(s(l)*sqrt(tau)))-x.*normcdf((log(1/x)+(m(l)-s(l)^2/2)*tau)/(s(l)*sqrt(tau))); %components of the call price\nend\nCa(isnan(Ca)) = 0 ;\nX(j,:,i) =x; %observation Grid K\nQ(j,:,i) =p(:,i)'*qu; %value of risk neutral density\nY(j,:,i) =C(j,:,i)+sigma*randn(1,1 ); %obervation with error\nXj =[Xj ; X(j,:,i)']; %strike\ntauj =[tauj ; tau*ones(size(C,2),1)]; %moneyness\nqj =[qj ; Q(j,:,i)'];\nend\ncdd=[i*ones(length(Xj),1) Xj Yj r*ones(length(Xj),1) tauj ]; % Day strike call_price interest_date maturity\n%%simulated completed\n%store interpolated curves\nF =TriScatteredInterp(cdd(:,2),cdd(:,5),cdd(:,3), 'nearest');\nFc{i-1} =F;\n%find smallest common grid and scale prices\nx1minc min(cdd(:,5))]; %maturity\nx2maxc max(cdd(:,2))];\ncgridx =union(cgridx,cdd(:,5));\nc5unil{i-1} =cdd(:,5); %maturity\nc3unilr{i-1} =Cj/st(i).*exp(r*tauj);\n%store true curves\nx1min =min( x1minc );\nmy =0.2+rand( Tout , 1 )*0.5;\nX =mx;\nfor l=1:3\nQQ(l,:) =1./(X.*sqrt(2*pi*s(l)^2.*T)).*exp(-1/2*((log(X)- (m(l)-s(l)^2/2)*T)./(s(l)*sqrt(T))).^2); % equivalent of qu for X and T\nrealD=QQ'*p(:,2:end);\n", "output_sequence": "Simulates real and noisy call prices using a mixture of log-normal densities."}, {"input_sequence": "Author: Heiko Wagner ; setwd(\"/Users/yifuwang/Documents/crypto_network\")\nlibrary(PerformanceAnalytics)##Return.calculate\nlibrary(highfrequency)\n## install.packages(\"lpSolve\",destdir = \"D:/r_packages/download\")\n# think the difference of cryptocurrency and traditional assets\nlibrary(clime)\nlibrary(tidyverse)\nlibrary(ggcorrplot)\n# library(network)\nlibrary(igraph)\n# library(gridExtra) #grid.arrange\nlibrary(corrplot)\n## install.packages(\"stargazer\",destdir = \"D:/r_packages/download\")\n## library(stargazer)\n## define necessary functions\n### function for extracting the date information\nextract_and_select_date<-function(df){\ndate_ymd<-str_sub(df[,2],1,10)\ndf<-cbind(df,date_ymd)\ndf1<-filter(df,df$date_ymd>=\"2020-08-03\"&df$date_ymd<='2022-03-31')\nnew_df<-arrange(df1,df1[,1])\n}\nreserve_null<-function(df){\ndf1<-filter(df,df$tradecount == \"NULL\")\nLTM<-function(volumedata){\n## whether the first row contains 0s\nLTM0<-sum(volumedata[1,])/100\nLTM_all<-rowSums(volumedata)/LTM0\n{\n## BTC\nGemini_BTCUSD_1h <- read.csv(file = \"Gemini_BTCUSD_1h.csv\", header = TRUE, skip = 1)\n\nBTC_1h_list <- list(Gemini_BTCUSD_1h,Bitstamp_BTCUSD_1h,Binance_BTCUSDT_1h,Kucoin_BTCUSDT_1h, FTX_BTCUSD_1h, Bitfinex_BTCUSD_1h, Exmo_BTCUSD_1h,Cex_BTCUSD_1h)\nn_BTC <- length(BTC_1h_list)\nBTC_1h_1<-lapply(BTC_1h_list, extract_and_select_date)\nnames_exchanges <- c(\"Gemini\", \"Bitstamp\", \"Binance\", \"Kucoin\", \"FTX\", \"Bitfinex\", \"Exmo\", \"Cex\")\nnames(BTC_1h_1) <- names_exchanges\n## loop to full join(merge) the data\nfor (i in 1:length(BTC_1h_1)) {\nif (i==1){\nclose_join <- cbind(BTC_1h_1[[i]][2],\ncolnames(close_join)[2] <- names_exchanges[i]\nvolume_join <- cbind(BTC_1h_1[[i]][2],\ncolnames(volume_join)[2] <- names_exchanges[i]\n}\nelse{\nclose_to_join <- cbind(BTC_1h_1[[i]][2],\nclose_join <- full_join(close_to_join, close_join, by = \"date\")\n}\ncolnames(close_join)[2] <- \"Gemini\"\nclose_join_BTC <- close_join %>% arrange(date) %>% mutate(date_daily = as.Date(substr(date,1,10)))\nclose_BTC_df <- close_join_BTC %>% dplyr::select(-c(date,date_daily))\nreturn_BTC_mt <- rbind(rep(0,ncol(close_BTC_df)),diff(log(as.matrix(close_BTC_df))))\nreturn_BTC_df <- data.frame(close_join_BTC$date_daily, close_join_BTC$date, return_BTC_df)\nnames(return_BTC_df) <- c(\"date\", \"date_time\", colnames(return_BTC_mt))\npng(\"close_tsplot_Bitcoin_new.png\", width = 1200, height = 600, bg = \"transparent\")\npar(mfrow=c(2,4))\nplot(return_BTC_df$Gemini~return_BTC_df$date,type = \"l\", main = \"Gemini\",xlab=\"\", ylab=\"\",ylim=c(-0.15,0.15), cex.axis = 2, cex.main = 2)\ndev.off()\nvolume_join_BTC <- volume_join %>% arrange(date) %>% mutate(date_daily = as.Date(substr(date,1,10)))\npng(\"Qtsplot_Bitcoin_new.png\", width = 900, height = 600, bg = \"transparent\")\nplot(volume_join_BTC$Gemini~volume_join_BTC$date_daily,type = \"l\", main = \"Gemini\",xlab=\"\", ylab=\"\",ylim=c(0,490000000), cex.axis = 2, cex.main = 2)\n### network centrality\nn_loop <- length(unique(return_BTC_df$date))-30\nevcent_BTC <- matrix(nrow = n_loop, ncol = n_BTC)\ndate_start_fixed = \"2020-08-03\"\ndate_end_fixed = as.character(as.Date(\"2020-08-03\")+n_loop+29)\nfig = image_graph(width = 1200, height = 600, res = 256, bg = \"transparent\")\n# for (t in 1:2) {\nfor (t in 1:n_loop) {\n# for (t in 1:(length(date_unique)-29)){\n## extract the date for 1,7 and 30 days average\n# date_rolling_all <- seq.Date(from = (as.Date(\"2020-08-03\")+t-1),to = (as.Date(\"2020-08-03\")+t+29), by = \"day\")\ndate_rolling_estimate <- as.Date(\"2020-08-03\")+t+29\ndate_rolling_7days <- seq.Date(from = (as.Date(\"2020-08-03\")+t+22),to = (as.Date(\"2020-08-03\")+t+28), by = \"day\")\n## select the data in this loop\nreturn_BTC_rolling_estimate <- return_BTC_df %>% dplyr::filter(date %in% date_rolling_estimate) %>% dplyr::select(-c(date,date_time))\n## rolling window MHAR and plot\ntryCatch ({\nRcov_BTC_1h_estimate_vech <- vech(t(chol(rCov(return_BTC_rolling_estimate))))\n\nX_HAR <- cbind(rep(1,length(Rcov_BTC_1h_estimate_vech)),Rcov_BTC_1h_1day_vech,Rcov_BTC_1h_7days_vech,Rcov_BTC_1h_30days_vech)\nbeta_HAR <- solve(t(X_HAR) %*% X_HAR) %*% t(X_HAR) %*% Rcov_BTC_1h_estimate_vech\nY_HAR <- X_HAR %*% beta_HAR\n# back to cov matrix\nRC_HAR <- (vech2full(Y_HAR) * lower.tri(matrix(rep(1,n_BTC^2),nrow = n_BTC),diag = T))%*%(vech2full(Y_HAR)*upper.tri(matrix(rep(1, n_BTC^2),nrow = n_BTC),diag = T))\nprecision_HAR = solve(RC_HAR)\nsdmatrix <- sqrt(diag(diag(precision_HAR)))\npcor_rolling <- - solve(sdmatrix) %*% precision_HAR %*% solve(sdmatrix)\ndiag(pcor_rolling) <- rep(1,ncol(pcor_rolling))\npcor_rolling_0.1 <- pcor_rolling*(pcor_rolling>0.1)\n# pcor_sum\n### note the choice of lambda\n# clime_rolling_0.2 = clime(RC_HAR,sigma = TRUE,linsolver = \"simplex\",lambda = 0.2)\n# inv_rolling_0.2<-clime_rolling_0.2[[\"Omegalist\"]][[1]]\n# sdmatrix <- sqrt(diag(diag(inv_rolling_0.2)))\n# pcor_rolling_0.2 <- solve(sdmatrix) %*% inv_rolling_0.2 %*% solve(sdmatrix)\n# clime_rolling_0.3 = clime(rc_rolling,sigma = TRUE,linsolver = \"simplex\",lambda = 0.3)\n# inv_rolling_0.3<-clime_rolling_0.3[[\"Omegalist\"]][[1]]\n# sdmatrix <- sqrt(diag(diag(inv_rolling_0.3)))\n# pcor_rolling_0.3 <- solve(sdmatrix) %*% inv_rolling_0.3 %*% solve(sdmatrix)\n#\n# clime_rolling_0.4 = clime(rc_rolling,sigma = TRUE,linsolver = \"simplex\",lambda = 0.4)\n# inv_rolling_0.4<-clime_rolling_0.4[[\"Omegalist\"]][[1]]\n# sdmatrix <- sqrt(diag(diag(inv_rolling_0.4)))\n# pcor_rolling_0.4 <- solve(sdmatrix) %*% inv_rolling_0.4 %*% solve(sdmatrix)\n# clime_rolling_0.5 = clime(rc_rolling,sigma = TRUE,linsolver = \"simplex\",lambda = 0.5)\n# inv_rolling_0.5<-clime_rolling_0.5[[\"Omegalist\"]][[1]]\n# sdmatrix <- sqrt(diag(diag(inv_rolling_0.5)))\n# pcor_rolling_0.5 <- solve(sdmatrix) %*% inv_rolling_0.5 %*% solve(sdmatrix)\n# clime_rolling_0.6 = clime(rc_rolling,sigma = TRUE,linsolver = \"simplex\",lambda = 0.6)\n# inv_rolling_0.6<-clime_rolling_0.6[[\"Omegalist\"]][[1]]\n# sdmatrix <- sqrt(diag(diag(inv_rolling_0.6)))\n# pcor_rolling_0.6 <- solve(sdmatrix) %*% inv_rolling_0.6 %*% solve(sdmatrix)\n# sdmatrix1 <- sqrt(diag(diag(rc_rolling)))\n# rcor_rolling <- solve(sdmatrix1) %*% rc_rolling %*% solve(sdmatrix1)\n# rcor_rolling[abs(rcor_rolling)<0.4] <- 0\n# p1<-ggcorrplot(pcor_rolling_0.2[,46:1], title = \"\u03bb=0.2\")\n# p2<-ggcorrplot(pcor_rolling_0.3[,46:1], method = \"square\", type = \"full\", title = \"\u03bb=0.3\")\n# p5<-ggpubr::ggarrange(p1,p2,p3,p4,nrow = 2,ncol = 2,labels = c(\"A\",\"B\",\"C\",\"D\"))\n# ggsave(filename = paste(\"heatplot_assets_\", t, \".png\"), plot = p5)\nrownames(pcor_rolling) <- names_exchanges\n# layout(matrix(c(1,1,2,2,2), nrow = 1, ncol = 5, byrow = TRUE))\n# p_heatmap <-\n# ggcorrplot(pcor_rolling_0.1, method = \"square\", type = \"full\", title = as.character(as.Date(\"2020-08-03\")+t+29), lab = TRUE)\n# ggsave(filename = paste(\"heatplot_all_BTC_CCs_\",t,\".png\",sep = \"\"), plot = p_heatmap)\n#plot cor matrix\n# corrplot(pcor_rolling, order=\"original\", method=\"circle\", tl.pos=\"lt\", type=\"upper\",\n# tl.col=\"black\", tl.cex=0.8, tl.srt=36,\n# cl.cex=0.6,\n# addCoef.col=\"black\", addCoefasPercent = TRUE,\n# p.mat = 1-abs(pcor_rolling), sig.level=0.90, insig = \"blank\")\nnetwork_BTC_all_t <- graph.adjacency(pcor_rolling_0.1, mode = \"undirected\", weighted = \"TRUE\", diag = FALSE)\nevcent_BTC_rcor<-eigen_centrality(network_BTC_all_t)\nevcent_BTC_rcor_v<-evcent_BTC_rcor$vector\ndegree_BTC_pcor <- degree(network_BTC_all_t)\ndegree_cent_BTC[t,] <- degree_BTC_pcor\nV(network_BTC_all_t)$name <- names_exchanges\n# png(paste(\"Networks_BTC_\",t,\".png\",sep = \"\"), width = 600, height = 600, bg = \"transparent\")\n# plot(network_BTC_all_t, main = as.character(as.Date(\"2020-08-03\")+t+29),\n# layout = layout_in_circle,\n# vertex.size = 40, # Moderate nodes\n# vertex.label = V(network_BTC_all_t)$name, # Set the labels\n# vertex.label.cex = 0.9, # Slightly smaller font\n# vertex.label.dist = 1, # Offset the labels\n# vertex.label.color = \"black\"\n# )\n# grid.arrange(p_heatmap, network_plot, ncol=2, nrow = 1)\n# dev.off()\n},\nerror = function(e){\nmessage(paste(\"An error occurred for item\", t,\":\\n\"), e)\n})\nanimation <- image_animate(fig, fps = 5)\nimage_write(animation, paste0(getwd(), \"/Network_\", date_start_fixed, \"_\",\ndate_end_fixed,\".gif\"))\n## centrality overview\ndegree_cent_BTC_mean <- apply(na.omit(degree_cent_BTC), 2, mean)\n# CRIX return ACF\nBinance_BTC_price <- Binance_BTCUSDT_1h$close\nBinance_BTC_return <- diff(log(Binance_BTC_price))\npng(\"Binance_BTC_return_abs_acf.png\", width = 900, height = 900, bg = \"transparent\")\nacf(Binance_BTC_return_abs, cex.axis = 2, cex.lab = 2, lag.max = 24*7, xlab = \"Lag\", ylab = \"Sample ACF\", main = \"ACF of Binance BTC absolute return\")\n## read all the files in the directory\n# filenames <- list.files(pattern=\"*.csv\", full.names=TRUE)\n# lapply(filenames, read.csv, header = TRUE, skip = 1)\n# res <- lapply(ldf, summary)\n# names(res) <- substr(filenames, 6, 30)\n## US&UK\n### GEMINI not OK\ngemini_BTCUSD_1h<-read.csv(file = \"Gemini_BTCUSD_1h.csv\", header = TRUE, skip = 1)\ngemini_1h_list<-list(gemini_BTCUSD_1h, gemini_ZECUSD_1h)\ngemini_1h_1<-lapply(gemini_1h_list, extract_and_select_date)\nn_row<-length(gemini_1h_1[[1]]$close)\ngemini_close_1h_p<-matrix(nrow = n_row,ncol = length(gemini_1h_1))\nfor (i in 1:length(gemini_1h_1)) {\ngemini_close_1h_p[,i]<-gemini_1h_1[[i]]$close\ncolnames(gemini_close_1h_p)<-c(\"Gemini_BTCUSD_1h\",\"Gemini_ETHUSD_1h\",\"Gemini_LTCUSD_1h\",\"Gemini_ZECUSD_1h\")\n### Bitstamp OK\nBitstamp_BCHUSD_1h<-read.csv(file = \"Bitstamp_BCHUSD_1h.csv\", header = TRUE, skip = 1)\nBitstamp_1h_list<-list(Bitstamp_BCHUSD_1h,Bitstamp_BTCUSD_1h,Bitstamp_ETHUSD_1h,Bitstamp_LTCUSD_1h,Bitstamp_XRPUSD_1h)\nBitstamp_1h_1<-lapply(Bitstamp_1h_list, extract_and_select_date)\nn_row<-length(Bitstamp_1h_1[[1]]$close)\nBitstamp_close_1h_p<-matrix(nrow = n_row,ncol = length(Bitstamp_1h_1))\nfor (i in 1:length(Bitstamp_1h_1)) {\nBitstamp_close_1h_p[,i]<-Bitstamp_1h_1[[i]]$close\ncolnames(Bitstamp_close_1h_p)<-c(\"Bitstamp_BCHUSD_1h\",\"Bitstamp_BTCUSD_1h\", \"Bitstamp_ETHUSD_1h\",\n### Bitfinex not OK\nBitfinex_BATUSD_1h<-read.csv(file = \"Bitfinex_BATUSD_1h.csv\", header = TRUE, skip = 1)\nBitfinex_1h_list<-list(Bitfinex_BATUSD_1h,Bitfinex_BTCUSD_1h,Bitfinex_DAIUSD_1h,Bitfinex_DASHUSD_1h,\nBitfinex_TRXUSD_1h,Bitfinex_XLMUSD_1h,Bitfinex_XMRUSD_1h,Bitfinex_XRPUSD_1h,Bitfinex_XVGUSD_1h)\nBitfinex_1h_1<-lapply(Bitfinex_1h_list, extract_and_select_date)\nn_row<-length(Bitfinex_1h_1[[1]]$close)\nBitfinex_close_1h_p<-matrix(nrow = n_row,ncol = length(Bitfinex_1h_1))\nfor (i in 1:length(Bitfinex_1h_1)) {\nBitfinex_close_1h_p[,i]<-Bitfinex_1h_1[[i]]$close\ncolnames(Bitfinex_close_1h_p)<-c(\"Bitfinex_BATUSD_1h\",\"Bitfinex_BTCUSD_1h\", \"Bitfinex_DAIUSD_1h\",\n\"Bitfinex_TRXUSD_1h\", \"Bitfinex_XMRUSD_1h\",\"Bitfinex_XRPUSD_1h\",\"Bitfinex_XVGUSD_1h\")\n### Cexio\nCexio_BTCUSD_1h<-read.csv(file = \"Cexio_BTCUSD_1h.csv\", header = TRUE, skip = 1)\nCexio_1h_list<-list(Cexio_BTCUSD_1h,Cexio_ETHUSD_1h,Cexio_ETHUSD_1h,Cexio_XLMUSD_1h,Cexio_XRPUSD_1h)\nCexio_1h_1<-lapply(Cexio_1h_list, extract_and_select_date)\nn_row<-length(Cexio_1h_1[[1]]$Close)\nCexio_close_1h_p<-matrix(nrow = n_row,ncol = length(Cexio_1h_1))\nfor (i in 1:length(Cexio_1h_1)) {\nCexio_close_1h_p[,i]<-Cexio_1h_1[[i]]$Close\ncolnames(Cexio_close_1h_p)<-c(\"Cexio_BTCUSD_1h\", \"Cexio_ETHBTC_1h\")\n### Poloniex\nPoloniex_ETCETH_1h<-read.csv(file = \"Poloniex_ETCETH_1h.csv\", header = TRUE, skip = 1)\nPoloniex_1h_list<-list(Poloniex_ETCETH_1h,Poloniex_OMGETH_1h, Poloniex_ZECETH_1h,\nPoloniex_1h_1<-lapply(Poloniex_1h_list, extract_and_select_date)\nn_row<-length(Poloniex_1h_1[[1]]$Close)\nPoloniex_close_1h_p<-matrix(nrow = n_row,ncol = length(Poloniex_1h_1))\nfor (i in 1:length(Poloniex_1h_1)) {\nPoloniex_close_1h_p[,i]<-Poloniex_1h_1[[i]]$Close\ncolnames(Poloniex_close_1h_p)<-c(\"Poloniex_ETCETH_1h\", \"Poloniex_OMGETH_1h\",\n### Bittrex\nBittrex_BTCUSD_1h<-read.csv(file = \"Bittrex_BTCUSD_1h.csv\", header = TRUE, skip = 1)\nBittrex_1h_list<-list(Bittrex_BTCUSD_1h, Bittrex_ETHUSD_1h,\nBittrex_1h_1<-lapply(Bittrex_1h_list, extract_and_select_date)\nn_row<-length(Bittrex_1h_1[[1]]$Close)\nBittrex_close_1h_p<-matrix(nrow = n_row,ncol = length(Bittrex_1h_1))\nfor (i in 1:length(Bittrex_1h_1)) {\nBittrex_close_1h_p[,i]<-Bittrex_1h_1[[i]]$Close\ncolnames(Bittrex_close_1h_p)<-c(\"Bittrex_BTCUSD_1h\", \"Bittrex_ETHUSD_1h\",\n## EU&Russia\n### EXMO\n# Exmo_BTCUSD_1h<-read.csv(file = \"Exmo_BTCUSD_1h.csv\", header = TRUE, skip = 1)\n#\n# Exmo_1h_list<-list(Exmo_BTCUSD_1h, Exmo_ETHUSD_1h,\n# Exmo_BTCEUR_1h, Exmo_ETHBTC_1h)\n# Exmo_1h_1<-lapply(Exmo_1h_list, extract_and_select_date)\n# n_row<-length(Exmo_1h_1[[1]]$close)\n# Exmo_close_1h_p<-matrix(nrow = n_row,ncol = length(Exmo_1h_1))\n# for (i in 1:length(Exmo_1h_1)) {\n# Exmo_close_1h_p[,i]<-Exmo_1h_1[[i]]$close\n# }\n# colnames(Exmo_close_1h_p)<-c(\"Exmo_BTCUSD_1h.csv\", \"Exmo_ETHUSD_1h.csv\",\n### Bitbay\n# Bitbay_BTCUSD_1h<-read.csv(file = \"Bitbay_BTCUSD_1h.csv\", header = TRUE, skip = 1)\n# Bitbay_1h_list<-list(Bitbay_BTCUSD_1h,Bitbay_LTCUSD_1h,Bitbay_ETHUSD_1h,Bitbay_BCHUSD_1h,\n# Bitbay_LSKUSD_1h,Bitbay_BTCEUR_1h,Bitbay_LTCEUR_1h,Bitbay_ETHEUR_1h,\n# Bitbay_BCHEUR_1h,Bitbay_LSKEUR_1h)\n# Bitbay_1h_1<-lapply(Bitbay_1h_list, extract_and_select_date)\n# n_row<-length(Bitbay_1h_1[[1]]$Close)\n# Bitbay_close_1h_p<-matrix(nrow = n_row,ncol = length(Bitbay_1h_1))\n# for (i in 1:length(Bitbay_1h_1)) {\n# Bitbay_close_1h_p[,i]<-Bitbay_1h_1[[i]]$Close\n# colnames(Bitbay_close_1h_p)<-c(\"Bitbay_BTCUSD_1h\", \"Bitbay_LTCUSD_1h\",\n# \"Bitbay_LSKUSD_1h\",\n### Abucoins\n# Abucoins_BTCUSD_1h<-read.csv(file = \"Abucoins_BTCUSD_1h.csv\", header = TRUE, skip = 1)\n# Abucoins_1h_list<-list(Abucoins_BTCUSD_1h,Abucoins_BTCPLN_1h,Abucoins_ETHPLN_1h,Abucoins_BCHPLN_1h,\n# Abucoins_SCBTC_1h,Abucoins_DASHBTC_1h,Abucoins_ZECBTC_1h,Abucoins_XMRBTC_1h,\n# Abucoins_ETCBTC_1h)\n# Abucoins_1h_1<-lapply(Abucoins_1h_list, extract_and_select_date)\n# n_row<-length(Abucoins_1h_1[[1]]$Close)\n# Abucoins_close_1h_p<-matrix(nrow = n_row,ncol = length(Abucoins_1h_1))\n# for (i in 1:length(Abucoins_1h_1)) {\n# Abucoins_close_1h_p[,i]<-Abucoins_1h_1[[i]]$Close\n# colnames(Abucoins_close_1h_p)<-c(\"Abucoins_BTCUSD_1h\",\"Abucoins_BTCPLN_1h\",\"Abucoins_ETHPLN_1h\",\"Abucoins_BCHPLN_1h\",\n# \"Abucoins_XRPBTC_1h\",\"Abucoins_LTCBTC_1h\",\"Abucoins_ETHBTC_1h\",\"Abucoins_BCHBTC_1h\",\n# \"Abucoins_ETCBTC_1h\")\n## Asian-Pacific\n### Zaif\n# Zaif_BCHBTC_1h<-read.csv(file = \"Zaif_BCHBTC_1h.csv\", header = TRUE, skip = 1)\n# Zaif_1h_list<-list(Zaif_BCHBTC_1h,Zaif_ETHBTC_1h,Zaif_MONABTC_1h,Zaif_XEMBTC_1h,\n# Zaif_BTCJPY_1h,Zaif_XEMJPY_1h,Zaif_BCHJPY_1h,Zaif_ETHJPY_1h,Zaif_MONAJPY_1h,Zaif_ZAIFJPY_1h)\n# Zaif_1h_1<-lapply(Zaif_1h_list, extract_and_select_date)\n# n_row<-length(Zaif_1h_1[[1]]$Close)\n# Zaif_close_1h_p<-matrix(nrow = n_row,ncol = length(Zaif_1h_1))\n# for (i in 1:length(Zaif_1h_1)) {\n# Zaif_close_1h_p[,i]<-Zaif_1h_1[[i]]$Close\n# colnames(Zaif_close_1h_p)<-c(\"Zaif_BCHBTC_1h\",\"Zaif_ETHBTC_1h\",\"Zaif_MONABTC_1h\",\"Zaif_XEMBTC_1h\",\"Zaif_BTCJPY_1h\",\n# \"Zaif_XEMJPY_1h\",\"Zaif_BCHJPY_1h\",\"Zaif_ETHJPY_1h\",\"Zaif_MONAJPY_1h\",\"Zaif_ZAIFJPY_1h\")\n### Okcoin\nOkcoin_LTCUSD_1h<-read.csv(file = \"Okcoin_LTCUSD_1h.csv\", header = TRUE, skip = 1)\nOkcoin_1h_list<-list(Okcoin_LTCUSD_1h,Okcoin_ETHUSD_1h,Okcoin_BTCUSD_1h)\nOkcoin_1h_1<-lapply(Okcoin_1h_list, extract_and_select_date)\nn_row<-length(Okcoin_1h_1[[1]]$Close)\nOkcoin_close_1h_p<-matrix(nrow = n_row,ncol = length(Okcoin_1h_1))\nOkcoin_volume_1h0<-rep(NA,length(Okcoin_1h_list))\nOkcoin_volume_1h<-matrix(nrow = n_row,ncol = length(Okcoin_1h_1))\nfor (i in 1:length(Okcoin_1h_1)) {\nOkcoin_close_1h_p[,i]<-Okcoin_1h_1[[i]]$Close\n### zeros on \"2018-05-17\", so we calaulate the LTM from the beginning,\nfor (i in 1:length(Okcoin_1h_list)) {\nOkcoin_volume_1h0[i]<-Okcoin_1h_list[[i]]$Volume.USD[1]\ncolnames(Okcoin_close_1h_p)<-c(\"Okcoin_LTCUSD_1h\",\"Okcoin_ETHUSD_1h\",\"Okcoin_BTCUSD_1h\")\n### Okex\nOkex_EOSBTC_1h<-read.csv(file = \"Okex_EOSBTC_1h.csv\", header = TRUE, skip = 1)\nOkex_1h_list<-list(Okex_EOSBTC_1h,Okex_OMGBTC_1h,Okex_NEOBTC_1h,\nOkex_QTUMBTC_1h,Okex_ZECBTC_1h,Okex_BCHBTC_1h,Okex_DASHBTC_1h,\nOkex_1h_1<-lapply(Okex_1h_list, extract_and_select_date)\nn_row<-length(Okex_1h_1[[1]]$Close)\nOkex_close_1h_p<-matrix(nrow = n_row,ncol = length(Okex_1h_1))\nfor (i in 1:length(Okex_1h_1)) {\nOkex_close_1h_p[,i]<-Okex_1h_1[[i]]$Close\n}\ncolnames(Okex_close_1h_p)<-c(\"Okex_EOSBTC_1h\",\"Okex_OMGBTC_1h\",\"Okex_NEOBTC_1h\",\n\"Okex_QTUMBTC_1h\",\"Okex_ZECBTC_1h\",\"Okex_BCHBTC_1h\",\"Okex_DASHBTC_1h\",\ncolnames(Okex_volume_1h)<-c(\"Okex_EOSBTC_1h\",\"Okex_OMGBTC_1h\",\"Okex_NEOBTC_1h\",\n### Bithumb\nBithumb_BTGKRW_1h<-read.csv(file = \"Bithumb_BTGKRW_1h.csv\", header = TRUE, skip = 1)\nBithumb_1h_list<-list(Bithumb_BTGKRW_1h,Bithumb_LTCKRW_1h,Bithumb_ETHKRW_1h,Bithumb_ZECKRW_1h,\nBithumb_1h_1<-lapply(Bithumb_1h_list, extract_and_select_date)\nn_row<-length(Bithumb_1h_1[[1]]$Close)\nBithumb_close_1h_p<-matrix(nrow = n_row,ncol = length(Bithumb_1h_1))\nfor (i in 1:length(Bithumb_1h_1)) {\nBithumb_close_1h_p[,i]<-Bithumb_1h_1[[i]]$Close\ncolnames(Bithumb_close_1h_p)<-c(\"Bithumb_BTGKRW_1h\",\"Bithumb_LTCKRW_1h\",\"Bithumb_ETHKRW_1h\",\"Bithumb_ZECKRW_1h\",\n### Binance\nBinance_ADABTC_1h<-read.csv(file = \"Binance_ADABTC_1h.csv\", header = TRUE, skip = 1)\nBinance_1h_list_1<-list(Binance_ADABTC_1h,Binance_BTCUSDT_1h,Binance_XLMBTC_1h,Binance_EOSBTC_1h,\nBinance_ETCBTC_1h,Binance_ETHBTC_1h,Binance_LTCBTC_1h,\n### remove repeated dates(not null in tradeout)\nBinance_1h_list_1<-lapply(Binance_1h_list_1, reserve_null)\nBinance_1h_1<-lapply(Binance_1h_list, extract_and_select_date)\nn_row<-length(Binance_1h_1[[1]][,7])\nBinance_close_1h_p<-matrix(nrow = n_row,ncol = length(Binance_1h_1))\nfor (i in 1:length(Binance_1h_1)) {\nBinance_close_1h_p[,i]<-Binance_1h_1[[i]][,7]\ncolnames(Binance_close_1h_p)<-c(\"Binance_ADABTC_1h\",\"Binance_BTCUSDT_1h\",\"Binance_XLMBTC_1h\", \"Binance_EOSBTC_1h\",\n\"Binance_ETCBTC_1h\",\ncolnames(Binance_volume_1h)<-c(\"Binance_ADABTC_1h\",\"Binance_BTCUSDT_1h\",\"Binance_XLMBTC_1h\", \"Binance_EOSBTC_1h\",\n\"Binance_ETCBTC_1h\",\n## other international\n### Bitso\nBitso_BTCMXN_1h<-read.csv(file = \"Bitso_BTCMXN_1h.csv\", header = TRUE, skip = 1)\nBitso_1h_list<-list(Bitso_BTCMXN_1h, Bitso_ETHMXN_1h)\nBitso_1h_1<-lapply(Bitso_1h_list, extract_and_select_date)\nn_row<-length(Bitso_1h_1[[1]]$Close)\nBitso_close_1h_p<-matrix(nrow = n_row,ncol = length(Bitso_1h_1))\nfor (i in 1:length(Bitso_1h_1)) {\nBitso_close_1h_p[,i]<-Bitso_1h_1[[i]]$Close\ncolnames(Bitso_close_1h_p)<-c(\"Bitso_BTCMXN_1h\", \"Bitso_ETHMXN_1h\")\n### Unocoin\n# Unocoin_BTCINR_1h<-read.csv(file = \"Unocoin_BTCINR_1h.csv\", header = TRUE, skip = 1)\n# Unocoin_1h_list<-list(Unocoin_BTCINR_1h)\n# Unocoin_1h_1<-lapply(Unocoin_1h_list, extract_and_select_date)\n# n_row<-length(Unocoin_1h_1[[1]]$Close)\n# Unocoin_close_1h_p<-matrix(nrow = n_row,ncol = length(Unocoin_1h_1))\n# for (i in 1:length(Unocoin_1h_1)) {\n# Unocoin_close_1h_p[,i]<-Unocoin_1h_1[[i]]$Close\n# colnames(Unocoin_close_1h_p)<-c(\"Unocoin_BTCINR_1h\")\n### Bit2c\n# Bit2c_BTCILS_1h<-read.csv(file = \"Bit2c_BTCILS_1h.csv\", header = TRUE, skip = 1)\n# Bit2c_1h_list<-list(Bit2c_BTCILS_1h, Bit2c_LTCILS_1h)\n# Bit2c_1h_1<-lapply(Bit2c_1h_list, extract_and_select_date)\n# n_row<-length(Bit2c_1h_1[[1]]$Close)\n# Bit2c_close_1h_p<-matrix(nrow = n_row,ncol = length(Bit2c_1h_1))\n# for (i in 1:length(Bit2c_1h_1)) {\n# Bit2c_close_1h_p[,i]<-Bit2c_1h_1[[i]]$Close\n# colnames(Bit2c_close_1h_p)<-c(\"Bit2c_BTCILS_1h\", \"Bit2c_LTCILS_1h\")\nnum_p_1day=24\ndate_all<-Binance_1h_1[[1]]$date_ymd\ndate_unique<-unique(date_all)\n# xts_time<-strptime(xts_time, \"%Y-%m-%d %I-%P\")\n# Binance_close_1h_p0<-cbind(Binance_close_1h_p,xts_date)\n# Binance_close_1h_r<-Return.calculate(Binance_close_1h_p, method = \"log\")\n###all assets whole period\nclose_1h_p <- cbind(Cexio_close_1h_p, Okcoin_close_1h_p,\nOkex_close_1h_p, Bithumb_close_1h_p,\nnassets <- c(ncol(Cexio_close_1h_p),\nindex_assets <- cumsum(nassets)\nclose_1h_r0<-apply(log(close_1h_p), 2, diff)\n### add a line of 0s to obtain a balanced sheet\nclose_1h_r<-rbind(rep(0,ncol(close_1h_p)),close_1h_r0)\n###\nvolume_1h<-cbind(Cexio_volume_1h, Poloniex_volume_1h,\nOkex_volume_1h, Bithumb_volume_1h,\n## write data for realized principle component\n# write.table(close_1h_r, file = \"close_1h_r.txt\", row.names = FALSE, col.names = FALSE)\n## load data for network analysis\n### for loops according to the date (location) change to length(date_unique)-30\n# setwd(\"E:\\\\workspace\\\\R\\\\realized_comoment\\\\heatplot_assets\")\nfor (t in 1:2){\n# for (t in 1:(length(date_unique)-29)){\n## extract the date\ndate_rolling <- date_unique[t:(t+30)]\nn_Binance <- index_assets[length(index_assets)-1]-index_assets[length(index_assets)-2]+0\nclose_1h_r_rolling <- close_1h_r[((t-1)*num_p_1day+1):((t+30)*num_p_1day),(index_assets[length(index_assets)-2]+1):index_assets[length(index_assets)-1]]\nclose_1h_r_estimate = close_1h_r_rolling[(nrow(close_1h_r_rolling)-num_p_1day+1):nrow(close_1h_r_rolling),]\nRcov_close_1h_r_estimate_vech <- vech(t(chol(rCov(close_1h_r_estimate))))\nX_HAR <- cbind(rep(1,length(Rcov_close_1h_r_estimate_vech)),Rcov_close_1h_r_1day_vech,Rcov_close_1h_r_7days_vech,Rcov_close_1h_r_30days_vech)\nbeta_HAR <- solve(t(X_HAR) %*% X_HAR) %*% t(X_HAR) %*% Rcov_close_1h_r_estimate_vech\nY_HAR <- X_HAR %*% beta_HAR\n# back to cov matrix\nRC_HAR <- (vech2full(Y_HAR) * lower.tri(matrix(rep(1,n_Binance^2),nrow = n_Binance),diag = T))%*%(vech2full(Y_HAR)*upper.tri(matrix(rep(1, n_Binance^2),nrow = n_Binance),diag = T))\nprecision_HAR = solve(RC_HAR)\nsdmatrix <- sqrt(diag(diag(precision_HAR)))\npcor_rolling <- - solve(sdmatrix) %*% precision_HAR %*% solve(sdmatrix)\n### note the choice of lambda\n# clime_rolling_0.2 = clime(RC_HAR,sigma = TRUE,linsolver = \"simplex\",lambda = 0.2)\n# inv_rolling_0.2<-clime_rolling_0.2[[\"Omegalist\"]][[1]]\n# sdmatrix <- sqrt(diag(diag(inv_rolling_0.2)))\n# pcor_rolling_0.2 <- solve(sdmatrix) %*% inv_rolling_0.2 %*% solve(sdmatrix)\n# clime_rolling_0.3 = clime(rc_rolling,sigma = TRUE,linsolver = \"simplex\",lambda = 0.3)\n# inv_rolling_0.3<-clime_rolling_0.3[[\"Omegalist\"]][[1]]\n# sdmatrix <- sqrt(diag(diag(inv_rolling_0.3)))\n# pcor_rolling_0.3 <- solve(sdmatrix) %*% inv_rolling_0.3 %*% solve(sdmatrix)\n# clime_rolling_0.4 = clime(rc_rolling,sigma = TRUE,linsolver = \"simplex\",lambda = 0.4)\n# inv_rolling_0.4<-clime_rolling_0.4[[\"Omegalist\"]][[1]]\n# sdmatrix <- sqrt(diag(diag(inv_rolling_0.4)))\n# pcor_rolling_0.4 <- solve(sdmatrix) %*% inv_rolling_0.4 %*% solve(sdmatrix)\n# clime_rolling_0.5 = clime(rc_rolling,sigma = TRUE,linsolver = \"simplex\",lambda = 0.5)\n# inv_rolling_0.5<-clime_rolling_0.5[[\"Omegalist\"]][[1]]\n# sdmatrix <- sqrt(diag(diag(inv_rolling_0.5)))\n# pcor_rolling_0.5 <- solve(sdmatrix) %*% inv_rolling_0.5 %*% solve(sdmatrix)\n# clime_rolling_0.6 = clime(rc_rolling,sigma = TRUE,linsolver = \"simplex\",lambda = 0.6)\n# inv_rolling_0.6<-clime_rolling_0.6[[\"Omegalist\"]][[1]]\n# sdmatrix <- sqrt(diag(diag(inv_rolling_0.6)))\n# pcor_rolling_0.6 <- solve(sdmatrix) %*% inv_rolling_0.6 %*% solve(sdmatrix)\n# sdmatrix1 <- sqrt(diag(diag(rc_rolling)))\n# rcor_rolling <- solve(sdmatrix1) %*% rc_rolling %*% solve(sdmatrix1)\n# rcor_rolling[abs(rcor_rolling)<0.4] <- 0\n# p1<-ggcorrplot(pcor_rolling_0.2[,46:1], title = \"\u03bb=0.2\")\n# p2<-ggcorrplot(pcor_rolling_0.3[,46:1], method = \"square\", type = \"full\", title = \"\u03bb=0.3\")\n# p5<-ggpubr::ggarrange(p1,p2,p3,p4,nrow = 2,ncol = 2,labels = c(\"A\",\"B\",\"C\",\"D\"))\n# ggsave(filename = paste(\"heatplot_assets_\", t, \".png\"), plot = p5)\nrownames(pcor_rolling) <- str_sub(colnames(Binance_close_1h_p), 1, str_length(colnames(Binance_close_1h_p))-6)\np_heatmap <-ggcorrplot(pcor_rolling, method = \"square\", type = \"full\", title = \"Binance\", lab = TRUE)\n# +\n# ggplot2::labs(x = 'X label', y = 'Y label') +\n# ggplot2::theme(\n# axis.title.x = element_text(angle = 0, color = 'grey20'),\n# )\nggsave(filename = paste(\"heatplot_all_Binance_CCs_\",t,\".png\",sep = \"\"), plot = p_heatmap)\nnetwork_rcor_all_t <- graph.adjacency(pcor_rolling, mode = \"undirected\", weighted = \"TRUE\", diag = FALSE)\nV(network_rcor_all_t)$name <- str_sub(colnames(Binance_close_1h_p), 1, str_length(colnames(Binance_close_1h_p))-6)\npng(paste(\"Networks_all_Binance_CCs_\",t,\".png\",sep = \"\"), width = 600, height = 600, bg = \"transparent\")\nplot(network_rcor_all_t, main = \"Networks of CCs\",\nvertex.size = 4, # Smaller nodes\nvertex.label = V(network_rcor_all_t)$name, # Set the labels\nvertex.label.cex = 0.8, # Slightly smaller font\nvertex.label.dist = 0.4, # Offset the labels\nvertex.label.color = \"black\")\ndev.off()\n# gif\nlist.files(path=\"/Users/yifuwang/Documents/crypto_network/gif1\", pattern = '*.png', full.names = TRUE) %>%\nimage_read() %>% # reads each path file\nimage_join() %>% # joins image\nimage_animate(fps=4) %>% # animates, can opt for number of loops\nimage_write(\"FileName.gif\") # write to current dir\n## LTM index\nLTM_Cexio<-LTM(Cexio_volume_1h)\nLTM_Poloniex<-LTM(Poloniex_volume_1h)\nLTM_Bithumb<-LTM(Bithumb_volume_1h)\n### zeros on \"2018-05-17\", so we calaulate the LTM from the beginning,\nLTM_Okcoin0<-sum(Okcoin_volume_1h0)/100\nLTM_Okcoin<-rowSums(Okcoin_volume_1h)/LTM_Okcoin0\nLTM_ex_all<-cbind(LTM_Cexio,LTM_Poloniex,LTM_Bittrex,LTM_Okcoin,\nLTM_Okex,LTM_Bithumb,LTM_Binance,LTM_Bitso)\n# summary statistics\npc_mean<-apply(pc_ex_all,2,mean)\npc_kurtosis<-apply(pc_ex_all,2, kurtosis)\n# realized network(correlation)\nrc_ex_all<-rCov(pc_ex_all)\nsdmatrix_pc <- sqrt(diag(diag(rc_ex_all)))\nrcor_pc_all <- solve(sdmatrix_pc) %*% rc_ex_all %*% solve(sdmatrix_pc)\n#rcor_all[abs(rcor_all)<0.4] <- 0\nnet_pc_rcor_all <- graph.adjacency(abs(rcor_pc_all),weighted=TRUE,diag=FALSE)\n# realized network(partial correlation)\ninv_pc_all<-solve(rc_ex_all)\nsdmatrix_inv_pc <- sqrt(diag(diag(inv_pc_all)))\npcor_pc_all <- solve(sdmatrix_inv_pc) %*% inv_pc_all %*% solve(sdmatrix_inv_pc)\nnet_pc_pcor_all <- graph.adjacency(abs(pcor_pc_all),weighted=TRUE,diag=FALSE)\n# analyze the network(table)\ndegree_pc_rcor<-degree(net_pc_rcor_all)\ncloseness_pc_pcor<-closeness(net_pc_pcor_all)\nevcent_pc_rcor_v<-evcent_pc_rcor$vector\n###\nnames(degree_pc_rcor)=c(\"Cexio\",\"Poloniex\",\"Bittrex\",\"Okcoin\",\"Okex\",\"Bithumb\",\"Binance\",\"Bitso\")\n# plot the networks\npar(mfrow=c(1,2))\n# set.seed(1234)\nl01<-layout.lgl(net_pc_rcor_all)\nplot(net_pc_rcor_all,layout=l01, vertex.label.color = \"black\",edge.arrow.size=0.1, main = \"A: realized correlation\",\nvertex.label=c(\"Cexio\",\"Poloniex\",\"Bittrex\",\"Okcoin\",\"Okex\",\"Bithumb\",\"Binance\",\"Bitso\"))\nplot(net_pc_pcor_all,layout=l02, vertex.label.color = \"black\",edge.arrow.size=0.1, main = \"B: realized partial correlation\",\ncolnames(rcor_pc_all)=c(\"Cexio\",\"Poloniex\",\"Bittrex\",\"Okcoin\",\"Okex\",\"Bithumb\",\"Binance\",\"Bitso\")\np1_1<-ggcorrplot(rcor_pc_all[,8:1], title = \"realized correlation\")\np1_2<-ggcorrplot(pcor_pc_all[,8:1], method = \"square\", type = \"full\", title = \"realized partial correlation\")\np1_3<-ggpubr::ggarrange(p1_1,p1_2,nrow = 1,ncol = 2,labels = c(\"A\",\"B\"))\nc_mean<-apply(close_1h_r,2,mean)\nc_kurtosis<-apply(close_1h_r,2, kurtosis)\nc_summary<-cbind(c_mean,c_median,c_max,c_min,c_skewness,c_kurtosis)\n# write.csv(c_summary,file = \"summary_statistics.csv\")\n### names\n# names<-c(rep(\"Okex\",15),rep(\"Bithumb\",8),rep(\"Binance\",21))\nrc_all<-rCov(close_1h_r)\n### note the choice of lambda\nclime_all_0.2 = clime(rc_all,sigma = TRUE,linsolver = \"simplex\",lambda = 0.1)\ninv_all_0.2<-clime_all_0.2[[\"Omegalist\"]][[1]]\nsdmatrix <- sqrt(diag(diag(inv_all_0.2)))\npcor_all_0.2 <- solve(sdmatrix) %*% inv_all_0.2 %*% solve(sdmatrix)\n# colnames(pcor_all_0.2)<-names\nclime_all_0.3 = clime(rc_all,sigma = TRUE,linsolver = \"simplex\",lambda = 0.3)\ninv_all_0.3<-clime_all_0.3[[\"Omegalist\"]][[1]]\nsdmatrix <- sqrt(diag(diag(inv_all_0.3)))\npcor_all_0.3 <- solve(sdmatrix) %*% inv_all_0.3 %*% solve(sdmatrix)\nclime_all_0.4 = clime(rc_all,sigma = TRUE,linsolver = \"simplex\",lambda = 0.4)\ninv_all_0.4<-clime_all_0.4[[\"Omegalist\"]][[1]]\nsdmatrix <- sqrt(diag(diag(inv_all_0.4)))\npcor_all_0.4 <- solve(sdmatrix) %*% inv_all_0.4 %*% solve(sdmatrix)\nclime_all_0.5 = clime(rc_all,sigma = TRUE,linsolver = \"simplex\",lambda = 0.5)\ninv_all_0.5<-clime_all_0.5[[\"Omegalist\"]][[1]]\nsdmatrix <- sqrt(diag(diag(inv_all_0.5)))\npcor_all_0.5 <- solve(sdmatrix) %*% inv_all_0.5 %*% solve(sdmatrix)\nclime_all_0.6 = clime(rc_all,sigma = TRUE,linsolver = \"simplex\",lambda = 0.6)\ninv_all_0.6<-clime_all_0.6[[\"Omegalist\"]][[1]]\nsdmatrix <- sqrt(diag(diag(inv_all_0.6)))\npcor_all_0.6 <- solve(sdmatrix) %*% inv_all_0.6 %*% solve(sdmatrix)\nsdmatrix1 <- sqrt(diag(diag(rc_all)))\nrcor_all <- solve(sdmatrix1) %*% rc_all %*% solve(sdmatrix1)\nrcor_all[abs(rcor_all)<0.4] <- 0\n### cor heatplot\np1<-ggcorrplot(pcor_all_0.2[,64:1], title = \"\u03bb=0.2\")\np2<-ggcorrplot(pcor_all_0.3[,64:1], method = \"square\", type = \"full\", title = \"\u03bb=0.3\")\np5<-ggpubr::ggarrange(p1,p2,p3,p4,nrow = 2,ncol = 2,labels = c(\"A\",\"B\",\"C\",\"D\"))\n### threshold low\n# dfcor[dfcor<0.5] <- 0\n### generate network with adjacent matrix\npar(mfrow=c(2,2))\nnet_pcor_0.2 <- graph.adjacency(abs(pcor_all_0.2),weighted=TRUE,diag=FALSE)\nset.seed(123)\nl1 <- layout_with_lgl(net_pcor_0.2)\nplot(net_pcor_0.2, layout=l1, vertex.size=5,vertex.label.color = \"black\",edge.arrow.size=0.01, vertex.label=colnames(close_1h_p))\nnet_pcor_0.3 <- graph.adjacency(abs(pcor_all_0.3),weighted=TRUE,diag=FALSE)\nl2 <- layout_with_lgl(net_pcor_0.3)\nplot(net_pcor_0.3, layout=l2, vertex.size=5,vertex.label.color = \"black\",edge.arrow.size=0.01, vertex.label=colnames(close_1h_p))\nnet_pcor_0.4 <- graph.adjacency(abs(pcor_all_0.4),weighted=TRUE,diag=FALSE)\nl3 <- layout_with_kk(net_pcor_0.4)\nplot(net_pcor_0.4, layout=l3, vertex.size=5,vertex.label.color = \"black\",edge.arrow.size=0.01, vertex.label=colnames(close_1h_p))\n# net_pcor_0.5 <- graph.adjacency(abs(pcor_all_0.5),weighted=TRUE,diag=FALSE)\n# set.seed(123)\n# l4 <- layout_with_kk(net_pcor_0.5)\n# plot(net_pcor_0.5, layout=l4, vertex.size=5,vertex.label.color = \"black\",edge.arrow.size=0.01, vertex.label=colnames(close_1h_p))\n# net_pcor_0.6 <- graph.adjacency(pcor_all_0.6,weighted=TRUE,diag=FALSE)\n# net_pcor <- as.network(pcor_Binance,matrix.type = 'adjacency')\n# plot(net_pcor)\nnet_rcor <- graph.adjacency(abs(rcor_all),weighted=TRUE,diag=FALSE)\nl5 <- layout_with_lgl(net_rcor)\nplot(net_rcor, layout=l5, vertex.size=5,vertex.label.color = \"black\",edge.arrow.size=0.01, vertex.label=colnames(close_1h_p))\n#degree_pc_rcor<-degree(net_pcor_0.3)\ncloseness_pcor_0.3<-closeness(net_pcor_0.3)\n#betweenness_pc_rcor<-betweenness(net_pcor_0.3)\nnames(betweenness_pcor_0.3)=colnames(close_1h_p)\n#evcent_pc_rcor<-evcent(net_pcor_0.3)\nevcent__pcor_0.3_v<-evcent_pcor_0.3$vector\nnames(evcent__pcor_0.3_v)=colnames(close_1h_p)\n## adjust the layout referring to ?igraph::layout\n### vname as lable\n### vname<-c(colnames(close_1h_p))\npar(mfrow=c(2,3))\nset.seed(1234)\nplot(net_pcor_0.2,layout=layout_nicely, vertex.label.color = \"black\",edge.arrow.size=0.1, main = \"A: \u03bb=0.2\")\nset.seed(229)\nplot(net_pcor_0.3,layout=layout.fruchterman.reingold, vertex.label.color = \"black\",edge.arrow.size=0.1, main = \"B: \u03bb=0.3\")\n# set.seed(12345)\n# plot(net_pcor_0.5,layout=layout.fruchterman.reingold, vertex.label.color = \"black\",edge.arrow.size=0.1)\nplot(net_rcor,layout=layout.circle, vertex.label.color = \"black\",edge.arrow.size=0.1, main = \"F: Realized Correlation\")\n# plot(net_rcor,layout=layout.fruchterman.reingold, vertex.label.color = \"black\",edge.arrow.size=0.1)\n## subsample\nclose_1h_rt<-cbind(date_all,close_1h_r)\nclose_1h_rt_down<-close_1h_rt[close_1h_rt[,1]>=\"2019-08-01\"&close_1h_rt[,1]<='2019-09-30',]\nclose_1h_r_down<-close_1h_rt_down[,-1]\nclose_1h_rt_up<-close_1h_rt[close_1h_rt[,1]>=\"2019-02-01\"&close_1h_rt[,1]<='2019-05-31',]\nclose_1h_r_up<-close_1h_rt_up[,-1]\nplot(close_1h_pt_down[,\"Bitso_BTCMXN_1h\"]~as.Date(close_1h_pt_down[,1],\"%Y-%m-%d\"),type = \"l\",xlab=\"Time\", ylab=\"Hourly Price\")\nrc_down<-rCov(close_1h_r_down)\nclime_down_0.3 = clime(rc_down,sigma = TRUE,linsolver = \"simplex\",lambda = 0.3)\ninv_down_0.3<-clime_down_0.3[[\"Omegalist\"]][[1]]\nsdmatrix <- sqrt(diag(diag(inv_down_0.3)))\npcor_down_0.3 <- solve(sdmatrix) %*% inv_down_0.3 %*% solve(sdmatrix)\nrc_up<-rCov(close_1h_r_up)\nclime_up_0.3 = clime(rc_up,sigma = TRUE,linsolver = \"simplex\",lambda = 0.3)\ninv_up_0.3<-clime_up_0.3[[\"Omegalist\"]][[1]]\nsdmatrix <- sqrt(diag(diag(inv_up_0.3)))\npcor_up_0.3 <- solve(sdmatrix) %*% inv_up_0.3 %*% solve(sdmatrix)\nl11 <- layout_with_lgl(net_pcor_0.3)\n# rolling estimate\ndate_0<-unique(date_all)\ndate_1<-date_0[-(1:7)]\nnum_days=nrow(close_1h_r)/24\nnum_edges_0.2=rep(0,num_days-7)\n## lambda = 0.2\nfor(i in 1:(num_days-7)){\nif(i == 1 | i == 2 | i == 5){\ncat(i,\"loops complete\\n\")\nclose_1h_r0 = close_1h_r[i:(24*7+i),]\nrc_0<-rCov(close_1h_r0)\nclime_0_0.2 = clime(rc_0,sigma = TRUE,linsolver = \"simplex\",lambda = 0.2)\ninv_0_0.2<-clime_0_0.2[[\"Omegalist\"]][[1]]\nsdmatrix0 <- sqrt(diag(diag(inv_0_0.2)))\npcor_0_0.2 <- solve(sdmatrix0) %*% inv_0_0.2 %*% solve(sdmatrix0)\nnet_pcor_0.2_0 <- graph.adjacency(pcor_0_0.2,weighted=TRUE,diag=FALSE)\nnum_edges_0.2[i]=ecount(net_pcor_0.2_0)\n## lambda = 0.3\nclime_0_0.3 = clime(rc_0,sigma = TRUE,linsolver = \"simplex\",lambda = 0.3)\ninv_0_0.3<-clime_0_0.3[[\"Omegalist\"]][[1]]\nsdmatrix0 <- sqrt(diag(diag(inv_0_0.3)))\npcor_0_0.3 <- solve(sdmatrix0) %*% inv_0_0.3 %*% solve(sdmatrix0)\nnet_pcor_0.3_0 <- graph.adjacency(pcor_0_0.3,weighted=TRUE,diag=FALSE)\nnum_edges_0.3[i]=ecount(net_pcor_0.3_0)\n## lambda = 0.4\nclime_0_0.4 = clime(rc_0,sigma = TRUE,linsolver = \"simplex\",lambda = 0.4)\ninv_0_0.4<-clime_0_0.4[[\"Omegalist\"]][[1]]\nsdmatrix0 <- sqrt(diag(diag(inv_0_0.4)))\npcor_0_0.4 <- solve(sdmatrix0) %*% inv_0_0.4 %*% solve(sdmatrix0)\nnet_pcor_0.4_0 <- graph.adjacency(pcor_0_0.4,weighted=TRUE,diag=FALSE)\nnum_edges_0.4[i]=ecount(net_pcor_0.4_0)\n## lambda = 0.5\nclime_0_0.5 = clime(rc_0,sigma = TRUE,linsolver = \"simplex\",lambda = 0.5)\ninv_0_0.5<-clime_0_0.5[[\"Omegalist\"]][[1]]\nsdmatrix0 <- sqrt(diag(diag(inv_0_0.5)))\npcor_0_0.5 <- solve(sdmatrix0) %*% inv_0_0.5 %*% solve(sdmatrix0)\nnet_pcor_0.5_0 <- graph.adjacency(pcor_0_0.5,weighted=TRUE,diag=FALSE)\nnum_edges_0.5[i]=ecount(net_pcor_0.5_0)\n## lambda = 0.6\n## correlation\nsdmatrix1 <- sqrt(diag(diag(rc_0)))\nrcor_0 <- solve(sdmatrix1) %*% rc_0 %*% solve(sdmatrix1)\nrcor_0[abs(rcor_0)<0.4] <- 0\nnet_rcor_0 <- graph.adjacency(rcor_0,weighted=TRUE,diag=FALSE)\nnum_edges_rcor[i]=ecount(net_rcor_0)\n## empirical results\n### fig 1:1time series of prices and returns\n# par(mfrow=c(2,2))\n### Cexio\nplot(close_1h_p[,\"Cexio_BTCUSD_1h\"]~as.Date(date_all,\"%Y-%m-%d\"),type = \"l\",main = \"Cexio\",xlab=\"Time\", ylab=\"Hourly Price\",ylim=c(min(Cexio_close_1h_p),max(Cexio_close_1h_p)))\nlines(close_1h_p[,\"Cexio_ETHBTC_1h\"]~as.Date(date_all,\"%Y-%m-%d\"),col=\"royalblue\",lty=1)\npar(mai=c(2,0.5,0.5,0.5))\nxy=par(\"usr\") ## the location of x axis and y axis\n# par(mfrow=c(1,1))\n### Poloniex\nlegend(x=xy[2]-xinch(0.1),y=xy[3]-yinch(0.2),\nlegend=c(\"ETCETH\",\"OMGETH\",\"ZECETH\",\"GNOETH\",\"REPETH\"),\ncol=c(\"steelblue\",\"deepskyblue\",\"darkolivegreen\",\"lightsalmon\",\"coral\"),\nlty=1,lwd=1.5,xpd=TRUE,ncol=3,bty= \"n\")\nplot(close_1h_p[,\"Poloniex_ETCETH_1h\"]~as.Date(date_all,\"%Y-%m-%d\"),type = \"l\",col = \"steelblue\", main = \"Poloniex\",xlab=\"Time\", ylab=\"Hourly Price\",ylim=c(min(Poloniex_close_1h_p),max(Poloniex_close_1h_p)))\nlines(close_1h_p[,\"Poloniex_OMGETH_1h\"]~as.Date(date_all,\"%Y-%m-%d\"),col=\"deepskyblue\",lty=1)\n### Bittrex\n# plot(close_1h_p[,\"Bittrex_BTCUSD_1h\"]~as.Date(xts_time,\"%Y-%m-%d\"),type = \"l\",main = \"Bittrex\",xlab=\"Time\", ylab=\"Hourly Price\",ylim=c(min(Bittrex_close_1h_p),max(Bittrex_close_1h_p)))\n#lines(close_1h_p[,\"Bittrex_ETHUSD_1h\"]~as.Date(xts_time,\"%Y-%m-%d\"),col=\"Blue\",lty=1)\n### Okcoin\nplot(close_1h_p[,\"Okcoin_LTCUSD_1h\"]~as.Date(date_all,\"%Y-%m-%d\"),type = \"l\", col = \"orangered\", main = \"Okcoin\",xlab=\"Time\", ylab=\"Hourly Price\",ylim=c(min(Okcoin_close_1h_p[,-3]),max(Okcoin_close_1h_p[,-3])))\nlines(close_1h_p[,\"Okcoin_ETHUSD_1h\"]~as.Date(date_all,\"%Y-%m-%d\"),col=\"royalblue\",lty=1)\n### Okex\nplot(close_1h_p[,\"Okex_ETHBTC_1h\"]~as.Date(date_all,\"%Y-%m-%d\"),type = \"l\",main = \"Okex\",col = \"royalblue\", xlab=\"Time\", ylab=\"Hourly Price\",ylim=c(min(Okex_close_1h_p),max(Okex_close_1h_p)))\n#lty means line type\nlines(close_1h_p[,\"Okex_EOSBTC_1h\"]~as.Date(date_all,\"%Y-%m-%d\"),col=\"chocolate\",lty=1)\n### Bithumb\nplot(close_1h_p[,\"Bithumb_BTGKRW_1h\"]~as.Date(date_all,\"%Y-%m-%d\"),type = \"l\",col = \"olivedrab\", main = \"Bithumb\",xlab=\"Time\", ylab=\"Hourly Price\",ylim=c(min(Bithumb_close_1h_p[,-7]),max(Bithumb_close_1h_p[,-7])))\nlines(close_1h_p[,\"Bithumb_LTCKRW_1h\"]~as.Date(date_all,\"%Y-%m-%d\"),col=\"orangered\",lty=1)\n### Binance\n# parse time stamp\ndate_Binance_char <- Binance_1h_1[[1]]$datedate_Binance\n# which(is.na(date_Binance))\nBinance_close_1h_r0<-apply(log(Binance_close_1h_p), 2, diff)\nBinance_close_1h_r<-rbind(rep(0,ncol(Binance_close_1h_p)),Binance_close_1h_r0)\nxts_1h_p <- xts(scale(Binance_close_1h_p), date_Binance)\ncol_palette <- c(\"deepskyblue4\", \"chocolate\", \"mediumpurple4\", \"slateblue\", \"purple\",\n\"turquoise4\", \"skyblue4\", \"steelblue\", \"blue2\", \"navyblue\",\n\"orangered3\", \"lightgoldenrod4\", \"coral3\", \"palevioletred4\", \"red3\",\n\"springgreen4\", \"darkolivegreen\", \"tan4\", \"dodgerblue4\", \"brown\", \"grey30\")\nplot.xts(xts_1h_p, col = col_palette, main =\"Cryptocurrencies in Binance\", format.labels=\"%b-%Y\")\naddLegend(\"topright\", on = 1, lty = rep(1,21), legend.names = colnames(Binance_close_1h_p), col = col_palette, ncol = 3)\n### Bitso\n# plot(close_1h_p[,\"Bitso_BTCMXN_1h\"]~as.Date(xts_time,\"%Y-%m-%d\"),type = \"l\",main = \"Bitso\",xlab=\"Time\", ylab=\"Hourly Price\",ylim=c(min(Bitso_close_1h_p),max(Bitso_close_1h_p)))\n# lines(close_1h_p[,\"Bitso_ETHMXN_1h\"]~as.Date(xts_time,\"%Y-%m-%d\"),col=\"royalblue\",lty=1)\n### plot BTC alone\nplot(close_1h_p[,\"Bitso_BTCMXN_1h\"]~as.Date(date_all,\"%Y-%m-%d\"),type = \"l\",col=\"royalblue\",main = \"Bitso_BTC\",xlab=\"Time\", ylab=\"Hourly Price\")\n### plot the difference instead (exchange rate or standardized??) by Prof. Haerdle, or just return (by Ren)\n# diff_BTC_Binance_Bithumb <- scale(close_1h_p[,\"Bitso_BTCMXN_1h\"])-scale(close_1h_p[,\"Bithumb_BTCKRW_1h\"])\nBinance_BTCUSDT_1h<-read.csv(file = \"Binance_BTCUSDT_1h.csv\", header = TRUE, skip = 1)\nBinance_BTCUSDT_1h_list <- Binance_BTCUSDT_1h %>% reserve_null %>% extract_and_select_date\nBinance_BTCUSDT_1h_r <- c(0, diff(log(Binance_BTCUSDT_1h_list$close)))\npng(\"Rtsplot_Bitcoin.png\", width = 900, height = 600, bg = \"transparent\")\nplot(Binance_BTCUSDT_1h_r~as.Date(date_all,\"%Y-%m-%d\"),type = \"l\",main = \"Binance\",xlab=\"\", ylab=\"\", cex.axis = 2, cex.main = 2)\npng(\"Qtsplot_Bitcoin.png\", width = 900, height = 600, bg = \"transparent\")\nplot(Binance_BTCUSDT_1h_list$Volume.USDT~as.Date(date_all,\"%Y-%m-%d\"),type = \"l\",main = \"Binance\",xlab=\"\", ylab=\"\", cex.axis = 2, cex.main = 2)\n## acf plot of returns\npng(\"Binance_BTCUSDT_r_1h_acf.png\", width = 900, height = 600, bg = \"transparent\")\nplot(acf(Binance_BTCUSDT_1h_r), type = \"h\")\npng(\"Bittrex_BTCUSD_r_1h_acf.png\", width = 900, height = 600, bg = \"transparent\")\nplot(acf(close_1h_r[,\"Bittrex_BTCUSD_1h\"]), type = \"h\")\nplot(close_1h_r[,\"Okex_ETHBTC_1h\"]~as.Date(date_all,\"%Y-%m-%d\"),type = \"l\", main = \"A: Okex_ETHBTC\", xlab=\"Time\", ylab=\"Hourly Return\")\nplot(num_edges_0.2~as.Date(date_1,\"%Y-%m-%d\"),type = \"l\",main = \"A: \u03bb=0.2\",xlab=\"Date\", ylab=\"Number of Edges\")\n", "output_sequence": "Simulates real and noisy call prices using a mixture of log-normal densities."}, {"input_sequence": "Author: Eren Allak ; # Import packages.\nimport numpy as np\nclass Task:\n#\n# Attributes.\ntask_name = None\nduration_execution = None\nn_max_task = None\n# Methods.\ndef fitness(self, schedule, empty_team_id, n_time_slots_cycle):\nfitness_value = np.zeros(2)\n# Remove emtpy schedule entries.\nnon_empty_slots = schedule[1] != empty_team_id\nnon_empty_schedule = [schedule[0][non_empty_slots],\n# Ideal distribution of execution times.\nif non_empty_schedule[0].shape[0] <= 1:\nfitness_value[0] = 0\nelse:\n# Time distance between schedule elements.\n# The time distance of duration_next is ideal and gives f_1 = 1 (100%).\ndiff = non_empty_schedule[0][1:] - non_empty_schedule[0][0:-1]\nf_1 = (self.duration_next - diff) / float(self.duration_next)\nf_1 = 1.0 - np.abs(f_1)\n# f_1 = f_1.clip(0) # Restrict to positive values.\n# Value can be negative to penalize bad distribution.\n# Time distance to the start and of planning cycle.\n# The schedule starting at 0 is ideal and gives f_cycle = 1 (100%).\nf_cycle = non_empty_schedule[0][0] / float(self.duration_next)\nf_cycle = 1.0 - np.abs(f_cycle)\nf_1 = np.append(f_1, f_cycle)\n# The schedule ending at (n_time_slots_cycle - duration_next) is ideal\n# and gives f_cycle = 1 (100%).\ndiff_end = n_time_slots_cycle - non_empty_schedule[0][-1]\nf_cycle = (self.duration_next - diff_end) / float(self.duration_next)\nf_1 = f_1.mean()\nfitness_value[0] = f_1\n# Ideal number of executions in cycle.\nN_non_empty = non_empty_schedule[0].shape[0]\nf_2 = 1.0 - np.abs((self.n_max_task - N_non_empty) / float(self.n_max_task))\nfitness_value[1] = f_2\nreturn fitness_value\ndef __init__(self, name, d1, n1):\nself.task_name = name\nself.duration_execution = d1\n", "output_sequence": "Scheduling algorithm for UAV teams based on a genetic algorithm."}, {"input_sequence": "Author: Eren Allak ; #!/usr/bin/env python2\n\"\"\"\n@author: Eren Allak\n#\n# Packages.\n# Import packages.\nfrom genetic_algorithm import GeneticAlgorithm\nfrom task import Task\n# Parameters and constants.\nclass GeneticAlgorithmParameters:\ndef __init__(self):\n# General Parameters.\nself.RAND_SEED = 1\n# Planning parameters.\nself.N_TIME_SLOTS_CYCLE = 12 # The scheduling cycle is split into equal time slots.\nself.N_TIME_SLOTS_TASK = self.N_TIME_SLOTS_CYCLE / 2 # Maximal number of task time slots.\nself.N_TASKS = 5 # Number of tasks.\nself.N_TEAMS = 10 # Number of teams to carry out tasks.\n# Parameters for genetic algorithm.\n# Genetic representation of scheduling by a double array.\n# Arrays correspond to time slots for the start of the task and corresponding team id.\nself.N_CHROMOSOME_LENGTH = self.N_TIME_SLOTS_TASK * self.N_TASKS\nself.N_CHROMOSOMES = 100\n# Constants.\nself.EMTPY_TEAM_ID = -1\n# Genetic algorithm parameters.\nself.N_ITERATIONS = 200\nself.N_CROSSOVER = int(round(self.N_CHROMOSOMES / 4))\n# Randomized Simulation.\nself.N_RANDOMIZED_SIM = 10\n# Main function.\ndef main():\n# Define Tasks. Input:\n# - execution duration for a team in time slots\n# - ideal number of time slots until next execution\n# - ideal number of executions in whole planning cycle\ntask_a1 = Task(\"A1 Standard Border Check Side 1\", 3, 2, 6)\ntask_c1 = Task(\"C1 Long Term Cartography\", 3, 6, 2)\ntasks = [task_a1,\nparam = GeneticAlgorithmParameters()\nga.start()\nga.plot_results()\n# Show fitness values and best schedule.\n# print(\"Best schedules:\")\n# for i in range(1, 6):\n# print(\"Fitness: {0}\".format(ga.fitness_values[-i]))\n# ga.print_schedule([ga.chromosomes[0][-i], ga.chromosomes[1][-i]])\n# print (\"\")\n# print(\"Worst schedules:\")\n# for i in range(0, 5):\n# print(\"Fitness: {0}\".format(ga.fitness_values[i]))\n# ga.print_schedule([ga.chromosomes[0][i], ga.chromosomes[1][i]])\nprint(\"Best solution:\")\nga.print_team_time_slots(-1)\nif __name__ == \"__main__\":\nmain()\n", "output_sequence": "Scheduling algorithm for UAV teams based on a genetic algorithm."}, {"input_sequence": "Author: Eren Allak ; # Import packages.\nimport numpy as np\nimport matplotlib.pyplot as plt\n#\n# Static functions.\ndef check_occupied(occupancy_mat, team_id, idx, duration):\ndim = occupancy_mat.shape\nend_idx = idx + duration if idx + duration < dim[1] else dim[1]\noccupied_sum = occupancy_mat[team_id, idx:end_idx].sum()\nreturn occupied_sum > 0\ndef set_occupied(occupancy_mat, team_id, idx, duration):\noccupancy_mat[team_id, idx:end_idx] = np.ones((1, end_idx - idx))\nclass GeneticAlgorithm:\n#\n# Attributes.\ntasks = None\nn_time_slots_cycle = None\nn_task = None\nn_chromo_len = None\nn_iterations = None\nchromosomes = None\nfitness_values = None\nchromo_size = None\nempty_team_id = None\nresults_fitness = None\n# Methods.\ndef __init__(self, param, tasks_in):\n# Set parameters.\nself.random_seed = param.RAND_SEED\nself.n_time_slots_cycle = param.N_TIME_SLOTS_CYCLE\nself.n_task = param.N_TASKS\nself.n_chromo_len = param.N_CHROMOSOME_LENGTH\nself.chromo_size = (self.n_chromosomes,\nself.n_teams = param.N_TEAMS\nself.n_crossover = param.N_CROSSOVER\nself.empty_team_id = param.EMTPY_TEAM_ID\nself.n_randomized_sim = param.N_RANDOMIZED_SIM\n# Save tasks.\nself.tasks = tasks_in\n# Initialize empty chromosomes as a double int array.\nself.chromosomes = [self.empty_team_id * np.ones(self.chromo_size, dtype=np.int),\n# Initialize fitness values.\nself.fitness_values = np.zeros(self.n_chromosomes)\n# Initialize parents.\nself.parents_1 = np.zeros(self.n_chromosomes, dtype=np.int)\n# Initialize result variables.\nself.results_fitness = np.zeros((self.n_chromosomes, self.n_iterations))\ndef initialize_chromosomes(self):\n\"\"\"\nInitialization of the chromosomes by random values, then ensure feasibility.\n# Generate random chromosomes.\nfor i in range(self.n_chromosomes):\nrandom_chromosome = self.generate_random_chromosome()\nself.repair_chromosomes()\ndef select_chromosomes(self):\n# Generate cumulative probability for selection process.\npositive_fitness_values = self.fitness_values.clip(0)\ncum_fitness = np.cumsum(positive_fitness_values, axis=0)\n# Pick parent indices with proportional probability to their fitness.\nrand1 = np.random.uniform(0, 1) * cum_fitness[-1]\nparent_1_idx = np.argmax(cum_fitness >= rand1)\nself.parents_1[i] = parent_1_idx\ndef crossover(self):\nnew_chromo_size = (self.n_crossover, self.n_chromo_len)\nnew_chromosomes = [self.empty_team_id * np.zeros(new_chromo_size, dtype=np.int),\nfor i in range(self.n_crossover):\n# Parent indices.\np_idx_1 = self.parents_1[i]\n# Crossover time slot.\nt1 = self.chromosomes[0][p_idx_1, j]\nd_t = np.abs(t1 - t2)\nt_rand = np.random.uniform(t1 - d_t / 2.0, t2 + d_t / 2.0)\nt_rand = min(self.n_time_slots_cycle - 1, max(round(t_rand), 0))\nnew_chromosomes[0][i, j] = t_rand\n# Crossover team id.\nrand_team_idx = p_idx_1 if np.random.uniform(0, 1) < 0.5 else p_idx_2\nteam_id = self.chromosomes[1][rand_team_idx, j]\nnew_chromosomes[1][i, j] = team_id\n# Replace least fit chromosomes with new chromosomes.\nself.chromosomes[0][0:self.n_crossover, :] = new_chromosomes[0]\ndef mutate(self):\n# Range of chromosomes allowed to mutate.\nmin_index = int(round(0 * self.n_chromosomes))\n# Mutate chromosomes.\nfor i in range(self.n_mutate):\nrand_chromosome_idx = np.random.random_integers(min_index, max_index)\ntime_slot = np.random.random_integers(0, self.n_time_slots_cycle - 1)\nself.chromosomes[0][rand_chromosome_idx, rand_gene_idx] = time_slot\ndef mutate2(self):\nold_time_slot = self.chromosomes[0][rand_chromosome_idx, rand_gene_idx]\ntime_slot = old_time_slot + 1 if np.random.uniform(0, 1) < 0.5 else old_time_slot - 1\ntime_slot = min(self.n_time_slots_cycle - 1, max(int(round(time_slot)), 0))\ndef repair_chromosomes(self):\n# Correct for feasible schedules and compute fitness.\n# Occupancy table to identify infeasible schedules.\nteam_occupancy = np.zeros((self.n_teams, self.n_time_slots_cycle), dtype=np.int)\nself.ensure_feasibility([self.chromosomes[0][i, :], self.chromosomes[1][i, :]], team_occupancy)\nself.fitness_values[i] = self.compute_fitness_value(i, team_occupancy)\nself.sort_chromosomes_by_fitness()\ndef start(self):\nfor i_sim in range(0, self.n_randomized_sim):\nprint (\"\\n\\nSimulation %d\" % i_sim)\n# Seed random value.\nnp.random.seed(self.random_seed + i_sim)\n# Initialize Chromosomes.\nfor i in range(self.n_iterations):\n# Select and save parents.\nself.select_chromosomes()\n# Genetic operator: crossover.\nself.crossover()\n# Genetic operator: mutate.\nself.mutate2()\n# Repair schedules to be feasible.\nself.repair_chromosomes()\n# Save results.\nself.results_fitness[:, i] = self.fitness_values\nprint (\"Iteration %d\" % i)\n# Compute results.\nself.sim_results_mean_fitness[i_sim, :] = self.results_fitness.mean(axis=0)\n# print(\"--------\")\n# print(self.fitness_values)\n# print\ndef plot_results(self):\n# Plot results.\nplt.ioff()\nfig, ax = plt.subplots() # Create a figure containing a single axes.\n# for i in range(0,self.n_randomized_sim):\n# ax.plot(np.arange(1, self.n_iterations+1), self.sim_results_best_fitness[i,:])\n# Plot confidence interval.\nmu_best_fitness = self.sim_results_best_fitness.mean(axis=0)\nax.fill_between(np.arange(1, self.n_iterations + 1),\n(mu_best_fitness - sigma_best_fitness),\n(mu_best_fitness + sigma_best_fitness), alpha=0.2, label=\"Confidence Max Fitness (1 std)\")\n(mu_mean_fitness - sigma_mean_fitness),\n(mu_mean_fitness + sigma_mean_fitness), alpha=0.2, label=\"Confidence Mean Fitness (1 std)\")\n# Plot last simulation.\nax.plot(np.arange(1, self.n_iterations + 1), self.results_fitness[-1, :], label=\"Fitness Max\")\n# Configure plot appearance.\nax.set_xlabel(\"Iteration\")\nax.set_ylabel(\"Average and Max Fitness\")\nax.grid(True)\nax.legend(loc='center left', bbox_to_anchor=(1, 0.9))\nfig.subplots_adjust(right=0.625) # Try to fit the legend box located outside.\nfig.set_figwidth(8) # In inches.\nax.set_title(\"Fitness Values\")\nplt.show()\ndef generate_random_chromosome(self):\n# Initialize time slots.\nN_slots_cycle = self.n_time_slots_cycle\nchromosome = [self.empty_team_id * np.ones(self.n_chromo_len, dtype=np.int),\n# Random time slots, sorted in ascending order.\nfor j in range(self.n_task):\nrand_row = np.random.random_integers(0, N_slots_cycle - 1, (1, N_slots_task))\nrand_row = np.sort(rand_row)\nchromosome[0][j * N_slots_task:(j + 1) * N_slots_task] = rand_row\n# Random team IDs.\nrand_row = np.random.random_integers(0, self.n_teams - 1, self.n_chromo_len)\nchromosome[1] = rand_row\nreturn chromosome\ndef ensure_feasibility(self, schedule, team_occupancy):\n# Iterate through the schedule and remove double missions on the same time slot.\nfor i in range(1, self.n_time_slots_task):\nidx_gene = i + j * self.n_time_slots_task\nif schedule[0][idx_gene] == schedule[0][idx_gene - 1] and \\\nschedule[1][idx_gene - 1] != self.empty_team_id:\n# Iterate through time slots in tasks and remove resource conflicts.\nfor i in range(self.n_time_slots_task):\nidx_time_slot = schedule[0][idx_gene]\n# Check if any team is assigned for the time slot.\n# If not occupied, set occupied. Else, cancel the task.\nis_occupied = check_occupied(team_occupancy, team_id, idx_time_slot, self.tasks[j].duration_execution)\nif team_id != self.empty_team_id and not is_occupied:\nset_occupied(team_occupancy, team_id, idx_time_slot, self.tasks[j].duration_execution)\nelif team_id != self.empty_team_id and is_occupied:\ndef compute_fitness_value(self, idx_chromosome, team_occupancy):\nfitness_value = 0\n# Compute fitness value from task constraints.\nfor i in range(self.n_task):\nN_slots = self.n_time_slots_task\nf_i = self.tasks[i].fitness([self.chromosomes[0][idx_chromosome, i * N_slots:(i + 1) * N_slots],\nself.empty_team_id, self.n_time_slots_cycle)\nfitness_value = fitness_value + f_i.mean()\n# Compute fitness value from resource usage.\nsum_occupancy = np.sum(team_occupancy, axis=1)\nf_team = 1.0 - np.abs((sum_occupancy - mean_sum_occupancy) / mean_sum_occupancy)\nfitness_value = fitness_value + f_team.mean()\nreturn fitness_value\ndef compute_fitness_vector(self, schedule, team_occupancy):\nfitness_value = []\nf_i = self.tasks[i].fitness([schedule[0][i * N_slots:(i + 1) * N_slots],\nfitness_value.append(f_i)\ndef sort_chromosomes_by_fitness(self):\nsorted_idx_vec = np.argsort(self.fitness_values)\nself.fitness_values = self.fitness_values[sorted_idx_vec]\nself.chromosomes[0] = self.chromosomes[0][sorted_idx_vec, :]\ndef print_schedule(self, schedule):\n# Generate schedule in array.\ntime_table = self.empty_team_id * np.ones((self.n_task, self.n_time_slots_cycle), dtype=np.int)\nif time_table[j, idx_time_slot] == self.empty_team_id:\n# header_time_table = np.zeros((self.n_task + 1, self.n_time_slots_cycle), dtype=np.int)\n# header_time_table[0, :] = np.arange(self.n_time_slots_cycle)\n# header_time_table[1:self.n_task + 1, :] = time_table\n#\n# Remove empty elements.\n# str_time_table = str(header_time_table)\n# str_time_table = str_time_table.replace(str(self.empty_team_id), \"--\")\n# print str_time_table\nprint_str = \" + str(np.arange(self.n_time_slots_cycle)) + \"\\n\"\nfor i in range(0, time_table.shape[0]):\ntask_str = \"Task %2d: \" % i\nnext_str = np.array2string(time_table[i, :], formatter={'int_kind': lambda x: \"%2d\" % x})\nnext_str = next_str.replace(\"-1\", \"--\")\nprint_str += task_str + next_str + \"\\n\"\nprint(print_str)\ndef print_team_time_slots(self, idx):\nprint(\"\\nFitness: {0}\".format(self.fitness_values[idx]))\nprint(\"Task schedule:\")\nself.print_schedule([self.chromosomes[0][idx], self.chromosomes[1][idx]])\nprint (\"\\nTeam time slots of best schedule:\")\nteam_occupancy = np.zeros((self.n_teams, self.n_time_slots_cycle), dtype=np.int)\nself.ensure_feasibility([self.chromosomes[0][idx, :], self.chromosomes[1][idx, :]], team_occupancy)\nfor i in range(0, team_occupancy.shape[0]):\nteam_str = \"Team %2d: \" % i\nnext_str = np.array2string(team_occupancy[i, :], formatter={'int_kind': lambda x: \"%2d\" % x})\nnext_str = next_str.replace(\" 0\", \"--\")\nprint_str += team_str + next_str + \"\\n\"\n", "output_sequence": "Scheduling algorithm for UAV teams based on a genetic algorithm."}, {"input_sequence": "Author: Gunawan ; # creating values a as a sequence from -3 to 3\na = seq(from = -3, to = 3, by = 0.01)\n# calculating the densities of these values given the default mean=0 sd=1 normal distribution\nb = dnorm(a)\n# margins (bottom,left,top,right) , font style, axis label size\npar(mai = c(b = 0.5, l = 0.5, t = 0.5, r = 0.5), font = 4, cex.axis = 1.5)\n# axis numeration switched off, line width=2\nplot(x = a, y = b, type = \"l\", ylab = \"\", xlab = \"\", axes = F, lwd = \"2\")\n# 2 horizontal lines\nabline(h = 0)\nabline(v = -1.5, lty = 2)\n# create and label x-axis\naxis(1, at = -1.5, labels = expression(z[alpha]))\n# Filling LEFT\npolygon(x = c(-1.5, a[1:151]), y = c(0, 0, b[2:151]), col = \"gray94\")\n# write alpha in the area of interest\n", "output_sequence": "Graphic of a normal distribution N(0,1) with a right-tailed (1-alpha)-confidence interval"}, {"input_sequence": "Author: Johannes Haupt ; # setting up the axis label size and margin (bottom, left, top, right)\npar(cex.lab = 1.2, cex.main = 1.1, mar = c(4, 4.5, 3, 2) + 0.1, mgp = c(2.9, 1, 0))\n# set the values\nn = 20\nlambda = 10\n# Create a poisson distribution with x and lambda and plot it\nprob = dpois(x, lambda)\nplot(x, prob, type = \"h\", main = \"Pois(10) vs. B(100, 0.1)\", ylab = \"Probability\", ylim = c(0, 0.15))\n# Create a binomial distribution and it as dots.\ns = 0:100\nf = dbinom(x = s, 100, 0.1)\npoints(0:100, f, pch = 16, cex = 1.5, col = \"black\")\n", "output_sequence": "Graphic of the probability mass function of the poission distribution vs. binomial distribution. The binomial is given by bars, while the binomial distr. is plotted as dots. It can be seen that the poisson as well as the binomial distribution closely approache the binomial distribution for the specified values."}, {"input_sequence": "Author: Polina Marchenko ; # make sure the package is installed install.packages('lattice')\nlibrary(\"lattice\")\ngraphics.off()\n# Time series plot\nplot1 = xyplot(Nile)\n# Time series cut-and-stack plots for further documentation see ?xyplot()\nplot2 = xyplot(Nile, aspect = \"xy\", cut = list(number = 3, overlap = 0.1), strip = strip.custom(bg = \"yellow\", fg = \"lightblue\"))\n# plots both graphs\nprint(plot1, position = c(0, 0, 1, 1), split = c(1, 1, 2, 1), more = TRUE)\n", "output_sequence": "Time series datasets are easily plotted with lattice. Here the dataset Nile is used to illustrate the usage of the package for time series data. The plots show the annual flow of the river Nile from 1871 to 1970. The first plot depicts the whole series and the second plot splits the series into three intervals, where each interval is one panel."}, {"input_sequence": "Author: Johannes Haupt ; data(UScereal, package = \"MASS\") # load the dataset\nfit = lm(calories ~ protein + fat + carbo + sugars, data = UScereal)\n# fit the regression model\ndev.new()\nlayout(matrix(c(1, 2, 3, 4), 2.1, 2)) # display 4 graphics in one plot\n", "output_sequence": "Perform multiple linear regression (MLR) on the dataset \"UScereal\" from package \"MASS\" and construct four diagnostic plots. These show first the residual errors plotted against their fitted values to check for a distinct trend. The second plot shows spread-location to detect skewness. The third plot is a QQ plot to check the deviation from the theoretical distribution of the errors. The fourth plot shows each points leverage."}, {"input_sequence": "Author: Polina Marchenko ; # Load tkrplot, tcltk and rpanel packages (have to be installed)\nrequire(tkrplot)\n# Load the trees data (included in R)\nattach(trees)\n# Define r to be range of variable Height\nr = diff(range(Height))\nif (interactive()) {\n# use the following code only in an interactive R session specify a function to be called by rp.tkrplot\ndraw = function(panel) {\nplot(density(panel$y, panel$sp), col = \"red\", main = \"\") # define color and title of plot\npanel\n}\nredraw = function(panel) {\n# specify a function to be called if double button widget is used\nrp.tkrreplot(panel, density) # replot the plot called 'density' in the window 'panel'\nrpplot = rp.control(title = \"Demonstration of rp.tkrplot\", y = Height, sp = r/8) # variables controlled by control panel\n\nrp.tkrplot(rpplot, density, draw) # specify the function to be called to draw the plot\nrp.doublebutton(rpplot, sp, 1.03, log = T, range = c(r/50, NA), title = \"Bandwidth\", action = redraw) # call the function redraw if widget is used\n}\n", "output_sequence": "The rpanel package employs different graphical user interface (GUI) controls to enable the immediate communication with the graphical output and provides dynamic graphics. The rp.tkrplot function uses the tkrplot package and enables to place a plot and its control panel in a single window. In this example a density plot with a control for altering the bandwidth is drawn."}, {"input_sequence": "Author: Christoph Schult ; graphics.off()\nrequire(datasets)\n# compute log returns for DAX index\nr.dax = diff(log(EuStockMarkets[, 1]))\n# axes scales\nxscale = c(-0.06, 0.06)\n# name of the graph and label of x\nnameChart = \"BCS_HistBinSizes.pdf\"\nxlabels = \"log-returns\"\n# plot the histograms for different bin sizes\npar(mar = par()$mar + c(0, 1.5, -1, 0.1), mfrow = c(3, 1), cex.lab = 2, cex.axis = 2, no.readonly = T)\nhist(r.dax, breaks = 5, main = NULL, xlab = NULL)\n", "output_sequence": "This Quantlet produces plots to show the effect of the bin size on the smoothness of an histogram."}, {"input_sequence": "Author: Christoph Schult ; # The R function kernel.wfunc implements all different kernel weighting functions in R\nkernel.wfunc = function(x, xzero, kernel) {\nn = length(x) # number of points to evaluate\ny = NA # weight vector\ndist = x - xzero # distance vector\nfor (i in 1:n) {\nif (dist[i] <= 1 & dist[i] >= -1) {\n# weights for each point x not to far from xzero uniform weights\nif (kernel == \"Uniform\") {\ny[i] = 0.5\n}\nif (kernel == \"Triangular\") {\n# triangular weights\ny[i] = 1 - abs(dist[i])\nif (kernel == \"Epanechnikov\") {\n# Epanechnikov weights\ny[i] = 0.75 * (1 - dist[i]^2)\nif (kernel == \"Quartic\") {\n# Quartic weights\ny[i] = 15/16 * (1 - dist[i]^2)^2\n} else {\ny[i] = 0\n} # weights for points too far from xzero\n}\nreturn(y)\n}\nkernel_names = c(\"Uniform\", \"Triangular\", \"Epanechnikov\", \"Quartic\")\nx = seq(-2, 2, by = 0.1) # sequence for points to evaluate\npar = par(mfrow = c(2, 2), cex.main = 1.5, lwd = 2, cex.axis = 1.5, lty = \"solid\")\nfor (i in 1:length(kernel_names)) {\n# loop creating the four plots\ny = kernel.wfunc(x, xzero = 0, kernel = kernel_names[i])\nplot(x, y, type = \"l\", xlim = c(-2, 2), ylim = c(0, 1), col = \"red\", xlab = \"\", ylab = \"\", main = paste(kernel_names[i],\n\"kernel\"))\n", "output_sequence": "A function is implemented to use all four different kernel weighting functions to compute the respective weight of a point x w.r.t. its distance to the point of evaluation x_0. Four plots are created to illustrate each weighting functions and their properties."}, {"input_sequence": "Author: Ostap Okhrin ; # make sure the package is installed\nlibrary(fMultivar)\n# defines how the plots will appear\nlayout(matrix(c(1, 2), 1, 2))\n# vector of random variable\nx = (-45:45)/20\n# produces X with columns x and y\nX = grid2d(x)\n# bivariate normal distribution for x and y\nz = dnorm2d(X$x, X$y, rho = 0.7)\n# produces matrix Z with three columns (x,y,z) z is the pdf of y and x\nZ = list(x = x, y = x, z = matrix(z, ncol = length(x)))\n# 3D scatterplot of bivariate normal pdf\npersp(Z, theta = -15, phi = 25, col = \"steelblue\")\n# contour plot of Z\n", "output_sequence": "The code creates two graphics. One graphic is a scatterplot of the bivariate normal distribution. Contours are plotted to illustrate for which combinations of the two variables the density function is the same. You can call them also isolines. The other plot is a 3 dimensional scatterplot, which shows the specific value of the joint pdf. It looks like a mountain. The highest point is the expectation."}, {"input_sequence": "Author: Martin Schelisch ; lfg = function(j, k, l, n) {\n# generate the seed\nseed = runif(j, 0, 2^l)\nfor (i in 1:n) {\nseed[j + i] = seed[i] + seed[j + i - k]%%2^l\n}\n# standardize to [0:1]\nX = seed[(j + 1):length(seed)]/max(seed)\nprint(X)\n}\n# generate 4 new random numbers\nplot(lfg(17, 5, 31, 100), main = \"100 Numbers Generated by LFG(17,5,31)\", xlab = \"Number of Observations\", ylab = \"Generated Number\",\n", "output_sequence": "Implementation of the Lagged Fibonacci Generator (LFG) for random numbers. The seed is a sequence of j integers, of which one integer should be odd. The value of the modulus is 2^l and j and k denote the number of lags. Resulting from the use of the Fibonacci Sequence, the generated sequences do not have satisfactory randomness properties."}, {"input_sequence": "Author: Martin Schelisch ; # set a seed here or inside the RANDU function seed = 1\n# Implementation of the RANDU generator (with seed=1)\nRANDU = function(n, seed = 1) {\n# predefine constants\nU = NULL\na = 2^16 + 3\nm = 2^31\nfor (i in 1:n) {\nseed = (a * seed)%%m\nU[i] = seed/m # normalize the values to [0,1]\n}\nprint(U)\n}\n# Run the generator four times for 4 (insufficiently) random numbers\nRANDU(4)\n", "output_sequence": "Implementation of the RANDU random number generator developed by IBM in the 1960s. It is a Linear Congruential Generator procedure. This generator has some cearly non-random characteristics, due to badly chosen starting values."}, {"input_sequence": "Author: Polina Marchenko ; # Normal density fit\nrequire(rpanel) # load required package\ndata(trees) # load required data\ny = Height # define y as height\n", "output_sequence": "Histogram for the height of trees from the trees package. It is possible to display density estimates for the height based on a assumed normal distribuion and estimated density."}, {"input_sequence": "Author: Franziska Wehrmann ; import os\nimport pickle\nfrom matplotlib import pyplot as plt\ncwd = os.path.join(os.getcwd(), 'DEDA_2020SS_Crypto_Options_RND_HD',\n'CrypOpt_RiskNeutralDensity')\ndata_path = os.path.join(cwd, 'data') + '/'\nday = '2020-03-11'\nres = pickle.load(open(data_path + 'results_{}.pkl'.format(day), 'rb'))\n# ---------------------------------------------------------------------- SMILES\nfig1, axes = plt.subplots(2,4, figsize=(10,7))\nfor key, ax in zip(sorted(res), axes.flatten()):\nprint(key, ax)\nax.plot(res[key]['df'].M, res[key]['df'].iv, '.')\nax.text(0.99, 0.99, r'$\\tau$ = ' + str(key),\nhorizontalalignment='right',\ntransform=ax.transAxes)\naxes.flatten()[0].set_ylabel('implied volatility')\nplt.tight_layout()\nfig1.savefig(os.path.join(cwd, '{}_smiles.png'.format(day)), transparent=True)\n# ------------------------------------------------------------------------ RNDs\nfig2, axes = plt.subplots(2,4, figsize=(10,7))\nax.plot(res[key]['K'][::-1], res[key]['q'])\nax.set_yticks([])\naxes.flatten()[0].set_ylabel('risk neutral density')\nfig2.savefig(os.path.join(cwd, '{}_RND.png'.format(day)), transparent=True)\n# ----------------------------------------------------------------- DERIVATIVES\nfig3, axes = plt.subplots(2,4, figsize=(10,7))\nax.plot(res[key]['M'], res[key]['first'])\nax.text(0.99, 0.01, r'$\\tau$ = ' + str(key),\nverticalalignment='bottom',\nfig3.savefig(os.path.join(cwd, '{}_derivatives.png'.format(day)), transparent=True)\n# ----------------------------------------------------------------- TAU PROCESS\nfor key in res:\ns = res[key]\nfig4, axes = plt.subplots(1,3, figsize=(10,4))\nax = axes[0]\nax.plot(s['df'].M, s['df'].iv, '.', c='r')\nax.plot(s['M'], s['smile'])\nax.set_xlabel('moneyness')\nax.set_ylabel('implied volatility')\nax = axes[1]\nax.plot(s['M'], s['first'])\nax = axes[2]\nax.plot(s['S'], s['q'])\nax.set_xlabel('spot price')\nax.set_ylabel(r'risk neutral density')\nplt.tight_layout()\nfig4.savefig(os.path.join(cwd, '{}_T{}.png'.format(day, key)), transparent=True)\n", "output_sequence": "Uses Rookley Method to estimate risk neutral density. Based on data of crypto options traded on Deribit."}, {"input_sequence": "Author: Franziska Wehrmann ; import numpy as np\nfrom scipy.stats import norm\ndef rnd_appfinance(M, S, K, o, o1, r, tau):\n\"\"\" from Applied Quant. Finance - Chapter 8\n\"\"\"\nst = np.sqrt(tau)\nrt = r*tau\nert = np.exp(rt)\nd1 = (np.log(M) + (r + 1/2 * o**2)*tau)/(o*st)\nd2 = d1 - o*st\ndel_d1_M = 1/(M*o*st)\ndel_d2_M = del_d1_M\ndel_d1_o = -(np.log(M) + rt)/(o**2 *st) + st/2\nd_d1_M = del_d1_M + del_d1_o * o1\ndd_d1_M = (-(1/(M*o*st))*(1/M + o1/o)\n+ o2*(st/2 - (np.log(M) + rt)/(o**2*st))\n+ o1 * (2*o1 * (np.log(M)+rt)/(o**3*st) - 1/(M*o**2*st))\n)\ndd_d2_M = (-(1/(M*o*st))*(1/M + o1/o)\n- o2*(st/2 + (np.log(M) + rt)/(o**2*st))\nd_c_M = (norm.pdf(d1) * d_d1_M\n- 1/ert * norm.pdf(d2)/M * d_d2_M\ndd_c_M = (norm.pdf(d1) * (dd_d1_M - d1 * (d_d1_M)**2)\n- norm.pdf(d2)/(ert*M) * (dd_d2_M - 2/M * d_d2_M - d2 * (d_d2_M)**2)\n- 2*norm.cdf(d2)/(ert * M**3))\ndd_c_K = dd_c_M * (M/K)**2 + 2 * d_c_M * (M/K**2)\nq = ert * S * dd_c_K\nreturn q\n", "output_sequence": "Uses Rookley Method to estimate risk neutral density. Based on data of crypto options traded on Deribit."}, {"input_sequence": "Author: Franziska Wehrmann ; import os\nimport pandas as pd\nimport pickle\nimport numpy as np\nfrom smoothing import locpoly_smoothing\nfrom risk_neutral_density import rnd_appfinance\ncwd = os.path.join(os.getcwd(), 'DEDA_2020SS_Crypto_Options_RND_HD',\n'CrypOpt_RiskNeutralDensity')\ndata_path = os.path.join(cwd, 'data') + '/'\n# ------------------------------------------------------------------------ MAIN\nd = pd.read_csv(data_path + 'trades_clean.csv')\nprint('exclude values with too big or too smal Moneyness : ',\nsum(d.M > 1.3) + sum(d.M <= 0.7))\nd = d[d.M <= 1.2] # filter out Moneyness bigger than 1.3\nprint(d.date.value_counts())\nday = '2020-03-11'\ndf = d[(d.date == day)]\nprint(df.tau_day.value_counts())\nres = dict()\nnum = 140\nfor tau_day in df.tau_day.value_counts().index:\nprint(tau_day)\ndf_tau = d[(d.tau_day == tau_day) & (d.date == day)]\nh = df_tau.shape[0] ** (-1 / 9)\ntau = df_tau.tau.iloc[0]\ndf_tau['M_std'] = (df_tau.M - np.mean(df_tau.M)) / np.std(df_tau.M)\n# --------------------------------------------------------------- SMOOTHING\nsmoothing_method = locpoly_smoothing\nsmile, first, second, M, S, K = smoothing_method(df_tau, tau, h,\nh_t=0.1, gridsize=num, kernel='epak')\n# ----------------------------------------------------------- CALCULATE SPD\nr = df.r.iloc[0]\nspd = rnd_appfinance\nresult = spd(M, S, K, smile, first, second, r, tau)\nres.update({tau_day : {'df': df_tau[['M', 'iv', 'S', 'K']],\n'M': M,\n'smile':\n}})\nwith open(data_path + 'results_{}.pkl'.format(day), 'wb') as f:\npickle.dump(res, f)\n# plotting in separate script\n", "output_sequence": "Uses Rookley Method to estimate risk neutral density. Based on data of crypto options traded on Deribit."}, {"input_sequence": "Author: Franziska Wehrmann ; import numpy as np\nfrom scipy.stats import norm\nimport scipy.interpolate as interpolate # B-Spline\ndef _gaussian_kernel(M, m, h_m, T, t, h_t):\nu_m = (M-m)/h_m\nreturn norm.cdf(u_m) * norm.cdf(u_t)\ndef _epanechnikov(M, m, h_m, T, t, h_t):\nreturn 3/4 * (1-u_m)**2 * 3/4 * (1-u_t)**2\ndef _local_polynomial(df, m, t, h_m, kernel=_gaussian_kernel):\nM = np.array(df.M)\nn = df.shape[0]\nX1 = np.ones(n)\nX2 = M - m\nX3 = (M-m)**2\nX4 = T-t\nX5 = (T-t)**2\nX6 = X2*X4\nX = np.array([X1, X2, X6]).T\nker = kernel(M, m, h_m, T, t, h_t)\nW = np.diag(ker)\nXTW = np.dot(X.T, W)\nbeta = np.linalg.pinv(np.dot(XTW, X)).dot(XTW).dot(y)\nreturn beta[0], 2*beta[2]\ndef locpoly_smoothing(df, tau, h_m, h_t=0.05, gridsize=50, kernel='epak'):\nif kernel=='epak':\nkernel = _epanechnikov\nelif kernel=='gauss':\nkernel = _gaussian_kernel\nelse:\nprint('kernel not know, use epanechnikov')\nnum = gridsize\nM_min, M_max = min(df.M),\nM = np.linspace(M_min, M_max, gridsize)\nsig = np.zeros((num, 3))\nfor i, m in enumerate(M):\nsig[i] = _local_polynomial(df, m, tau, h_m, kernel)\nsmile = sig[:, 0]\nS_min, S_max = min(df.S),\nS = np.linspace(S_min, S_max, gridsize)\nreturn smile, first, second, M, S, K\n", "output_sequence": "Uses Rookley Method to estimate risk neutral density. Based on data of crypto options traded on Deribit."}, {"input_sequence": "Author: Yaojun Liu, Yinan Wu ; % ---------------------------------------------------------------------\n% Quantlet: SFEBitree_steps\n% ---------------------------------------------------------------------\n% Description: Computes the exercise price of\n% American put option using a binomial tree for assets\n% without dividends. In order to see whether the pricing is\n% reliable or not.\n% Inputs: s0 - Stock Price\n% k - Exercise Price\n% sig - Volatility\n% t - Time to Expiration\n% n - Number of Intervals\n% Output: Option's price with different steps under the same time\n% to maturity.\n% Author: Yaojun Liu, Yinan Wu 20151228\nclear,clc;clear all;\n%% User inputs parameters\ndisp('Please input Price of Underlying Asset s0, Exercise Price k, Domestic Interest Rate per Year i');\ndisp('Volatility per Year sig, Time to Expiration (Years) t, Number of steps n');\ndisp('as: [230, 210, 0.04545, 0.25, 200]');\ndisp(' ') ;\npara=input('[s0, k, i, sig, t, n]=');\nwhile length(para) < 6\ndisp('Not enough input arguments. Please input in 1*6 vector form like [230, 210, 0.04545, 0.25, 50]');\ndisp(' ') ;\npara=input('[s0, k, i, sig, t, n]=');\nend\ns0=para(1); % Stock price\nsig=para(4); % Volatility\nt=para(5); % Time to expiration\n%Check conditions\nif s0<=0\ndisp('SFEBiTree: Price of Underlying Asset should be positive! Please input again')\ns0=input('s0=');\nif k<0\ndisp('SFEBiTree: Exercise price couldnot be negative! Please input again')\nk=input('k=');\nif sig<0\ndisp('SFEBiTree: Volatility should be positive! Please input again')\nsig=input('sig=');\nif t<=0\ndisp('SFEBiTree: Time to expiration should be positive! Please input again')\nt=input('t=');\nif n<1\ndisp('SFEBiTree: Number of steps should be at least equal to 1! Please input again')\nn=input('n=');\n%% Main computation\nprice_nn=zeros(1,n);\nfor nn=1:n % Different steps\nu=exp(sig*sqrt(dt)); % Up movement parameter u\nb=i; % Costs of carry\np=0.5+0.5*(b-sig^2/2)*sqrt(dt)/sig; % Probability of up movement\n% Pricing by using different steps\ns=ones(nn+1,nn+1)*s0;\nun=zeros(nn+1,1);\ndm=un';\nj=1;\nwhile j<nn+1\nd1=[zeros(1,nn-j) (ones(1,j+1)*d).^((1:j+1)-1)];\ndm=[dm; d1]; % Down movement dynamics\nu1=[ones(1,nn-j)-1 (ones(1,j+1)*u).^((j:-1:0))];\num=[um; u1]; % Up movement dynamics\nj=j+1;\nend\num=[un';um]';\ndm=dm';\ns=s(1,1).*um.*dm; % Stock price development\nStock_Price=s;\ns=flipud(s); % Rearangement\nopt = zeros(size(s));\n% Option is an american put\nopt(:,nn+1) = max(k-s(:,nn+1),0); % Determine option values from prices\nfor j = nn:-1:1\nl = 1:j;\n% Probable option values discounted back one time step\ndiscopt = ((1-p)*opt(l,j+1)+p*opt(l+1,j+1))*exp(-b*dt);\n% Option value is max of X - current price or discopt\nopt(:,j) = [max(k-s(1:j,j),discopt);zeros(nn+1-j,1)];\nAmerican_Put_Price = flipud(opt);\nprice_nn(nn)=American_Put_Price(end,1); % The price of the option under different steps\n%% The figure of option price with different steps(the same time to maturity)\nplot(1:n,price_nn,'b-');\ntitle('American put option price with different steps')\nxlabel('Steps')\nylabel('Option rice')\n", "output_sequence": "Computes the exercise price of American put option using a binomial tree for assets without dividends. In order to see whether the pricing is reliable or not."}, {"input_sequence": "Author: Yaojun Liu, Yinan Wu ; function [S,t,V] = AmericanOption(K,T,r,delta,sigma,type,m,n)\n\n% Input:\n%\n% K = strike price\n% delta = rate of dividend payment\n% sigma = volatility\n% type = type of option ('call' or 'put')\n% Output:\n% S = range of stock prices\n% t = range of time points from 0 to T\n% V = corresponding option prices, i.e.\n% V(i,j) is an approximation of V(S(i),t(j))\n%% Verification\nswitch type\ncase 'call'\n% For a call, r must be larger than delta\nif r <= delta\nerror('r must be larger than delta')\nend\ncase 'put'\notherwise\nerror('type must be call or put')\nend\n\n%% Define parameters\n% dimensionless parameters\nq = 2*r/sigma^2;\nq_delta = 2*(r-delta)/sigma^2;\n% asset space\nx_min = -5;\ndx = (x_max-x_min)/m;\n% time space\ntau_max = .5*sigma^2*T;\ndtau = tau_max/n;\n% numerical parameters\neps = 1e-6;\ntheta = .5;\nomega_R = 1;\nlambda = dtau/dx^2;\nalpha = lambda*theta;\n% verify stability condition\nif lambda > .5\ns = sprintf('lambda = %4.2f',lambda);\ndisp(s)\nerror(strcat('The algorithm is unstable. Stability can be obtained ',...\n' by increasing the value of n or by decreasing the value of m'))\n%% Initialization\n% state and time space\nx = (x_min:dx:x_max)';\ntau = 0:dtau:tau_max;\n% For performance reasons we compute one matrix with all the g values\nX = repmat(x,1,n+1);\nG = g(X,Y);\n% dimensionless option value\nw = zeros(m+1,n+1);\n% boundary conditions\nw(:,1) = G(:,1);\nw(m+1,:) = G(end,:);\n% righthandside is needed in core algorithm\nb = zeros(m-1,1);\n% SOR iteration vector needs to be pre-allocated only once\nvnew = zeros(m-1,1);\n%% Core algorithm\nfor j = 2:n+1\n% create righthandside b\nfor k = 1:m-1\nswitch k\ncase 1\nb(k) = w(2,j-1)+lambda*(1-theta)*(w(1,j-1)-2*w(2,j-1)+w(3,j-1))+alpha*w(1,j);\ncase m-1\nb(k) = w(m,j-1)+lambda*(1-theta)*(w(m-1,j-1)-2*w(m,j-1)+w(m+1,j-1))+alpha*w(m+1,j);\notherwise\nb(k) = w(k+1,j-1)+lambda*(1-theta)*(w(k,j-1)-2*w(k+1,j-1)+w(k+2,j-1));\nend\n% initialize vector v\nv = max(w(2:m,j-1),G(2:m,j));\n% the variable iter is introduced to manage the SOR iteration\niter = 1;\n% SOR iteration\nwhile iter == 1\nfor k = 1:m-1\nswitch k\nrho = (b(k)+alpha*v(k+1))/(1+2*alpha);\ncase m-1\nrho = (b(k)+alpha*vnew(k-1))/(1+2*alpha);\notherwise\nrho = (b(k)+alpha*(vnew(k-1)+v(k+1)))/(1+2*alpha);\nend\nvnew(k) = max(G(k+1,j),v(k)+omega_R*(rho-v(k)));\nif norm(v-vnew) <= eps\niter = 0;\nelse\nv = vnew;\nw(2:m,j) = vnew;\n%% Transformation to original dimensions\nS = K*exp(x);\nt = T-2*tau/sigma^2;\nV = K*exp(-.5*(q_delta-1)*x)*exp(-(.25*(q_delta-1)^2+q)*tau).*w;\n% re-arrange t and V in increasing time order\nt = fliplr(t);\n%% Define function g as a nested function\nfunction boundary = g(x,tau)\n% This function is the lower bound for the dimensionless solution\n% of the linear complementarity problem. It is also used for\n% the boundary and initial conditions.\nabb1 = exp(((q_delta-1)^2+4*q)*tau/4);\nswitch type\ncase 'put'\nboundary = abb1.*max(abb2-abb3,0);\ncase 'call'\nboundary = abb1.*max(abb3-abb2,0);\n", "output_sequence": "Computes the exercise price of American put option using a binomial tree for assets without dividends. In order to see whether the pricing is reliable or not."}, {"input_sequence": "Author: Yaojun Liu, Yinan Wu ; function Sf = FreeBoundary(S,t,V,K,type)\n\nSf = zeros(1,length(t));\neps_star = K*1e-5;\nswitch type\ncase 'put'\nfor j = 1:length(t)\nSf(j) = S(find(abs(V(:,j)-K+S)< eps_star, 1, 'last'));\nend\ncase 'call'\nSf(j) = S(find(abs(V(:,j)+K-S)< eps_star, 1, 'first'));\nend\n", "output_sequence": "Computes the exercise price of American put option using a binomial tree for assets without dividends. In order to see whether the pricing is reliable or not."}, {"input_sequence": "Author: Michael Lebacher, Johannes Stoiber ; # clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# Paramters for the general Ito process\ndt = 0.0001 # delta t, determines the length of the step size\nc = 1 # constant c\nstart_val = 1 # defines the starting value\nset.seed(0) # regulates that random numbers do not change with repeated executions of the code\nmu = function(S,t) {\nmu = 1*S+t # define the functional form of mu\nreturn(mu)\n}\nsigma = function(S,t) {\nsigma = S*t # define the functional form of sigma\nreturn(sigma)\n# calculation of related basic parameters\nn = floor(1/dt)\nt = seq(0, n, by = dt)\n# calculation of the Wiener process\nw = matrix(runif(n), n, 1) # defines a vector w which contains values randomly choosen greater or smaller than zero\nw = 2 * (w > 0.5) - 1 # rescales the vector w to -1 or 1\ndx = c * sqrt(dt) # defines the scaling factor dx\ndw = w * dx # gives the increments of a Wiener process\n\n# calculation of the general Ito process\nS = matrix(0, n, 1) # defines an vector s of length n containing zeros\nS[1] = start_val # defines the staring value\nfor (i in 2:dim(dw)[1]) {\nS[i] = mu(S[i-1],t[i-1])*dt + sigma(S[i-1],t[i-1])*dw[i] + S[i-1]\n# plotting to a file\npng(\"SFEItoProcess.png\", width = 675, height = 600)\npar(mar=c(5.1, 4.1, 10.1), xpd=TRUE)\ncols = c(\"white\",\nax = c(seq(0, 1, by = 0.25))\nmatplot(S, lwd = 2, type = \"l\", lty = 1, ylim = c(min(S), max(S)), col = \"blue\", main = \"General Ito Process\",\nxlab = \"Time t\", ylab = \"Values of the general Ito process\", xaxt=\"n\")\naxis(side=1, at = seq(0,n, by = n/4), labels = ax)\nlegend(\"topright\", inset = c(-0.3,0), legend=c(paste(\"dt = \", dt, sep=\"\"),paste(\"c = \", c, sep=\"\"),\npaste(\"s_0 = \",start_val , sep=\"\")), col = cols,lty=0, ncol=1,cex=1.3,bty=\"o\",lwd=0)\ndev.off()\n", "output_sequence": "Generates and plots the path of a general Ito process."}, {"input_sequence": "Author: Michael Lebacher, Johannes Stoiber ; # clear all variables\nrm(list = ls(all = TRUE))\ngraphics.off()\n# install and load packages\nlibraries = c(\"KernSmooth\", \"matlab\", \"kedd\", \"scatterplot3d\", \"scales\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# ---------------------------------------------------------------------------#\n# Data preparation #\n# Read data\nodata = read.table(\"odata.txt\", header = T, sep = \"\\t\", dec = \".\") # option data\nidata$Date = as.Date(idata$Date, format = \"%Y-%m-%d\")\ndax = data.frame(idata$Date, idata$Adj.Close)\nnames(dax) = c(\"Date\", \"DAX\")\n# Select data for the chosen time to maturity\nodax = subset(odata, odata$maturity < 0.75)\n# Function for estimation of state price densities (risk neutral densities) #\n# Result of Breeden and Litzenberger (1978) is needed spdbl uses the\n# Breeden and Litzenberger (1978) method and a semiparametric\n# specification of the Black-Scholes option pricing function to\n# calculate the empirical State Price Density. The analytic formula\n# uses an estimate of the volatility smile and its first and second\n# derivative to calculate the State-price density. This method can only\n# be applied to European options (due to the assumptions).\nspdbl = function(m, sigma, s, r, tau) {\nrm = length(m)\nst = sqrt(tau)\nert = exp(r * tau)\n\n# Modified Black-Scholes scaled by S-div instead of F\nd1 = (log(m) + tau * (r + 0.5 * (sigma^2)))/(sigma * st)\nd2 = d1 - sigma * st\nf = pnorm(d1, mean = 0, sd = 1) - pnorm(d2, mean = 0, sd = 1)/(ert * m)\n# First derivative of d1 term\nd11 = (1/(m * sigma * st)) - (1/(st * (sigma^2))) * ((log(m) + tau *\nr) * sigma1) + 0.5 * st * sigma1\n# First derivative of d2 term\nd21 = d11 - st * sigma1\n# Second derivative of d1 term\nd12 = -(1/(st * (m^2) * sigma)) - sigma1/(st * m * (sigma^2)) + sigma2 *\n(0.5 * st - (log(m) + rt)/(st * (sigma^2))) + sigma1 * (2 * sigma1 *\n# Second derivative of d2 term\nd22 = d12 - st * sigma2\n# Please refer to either Rookley (1997) or the XploRe Finance Guide for\n# derivations\nf1 = dnorm(d1, mean = 0, sd = 1) * d11 + (1/ert) * ((-dnorm(d2, mean = 0,\nsd = 1) * d21)/m + pnorm(d2, mean = 0, sd = 1)/(m^2))\nf2 = dnorm(d1, mean = 0, sd = 1) * d12 - d1 * dnorm(d1, mean = 0, sd = 1) *\n(d11^2) - (1/(ert * m) * dnorm(d2, mean = 0, sd = 1) * d22) + ((dnorm(d2,\nmean = 0, sd = 1) * d21)/(ert * m^2)) + (1/(ert * m) * d2 * dnorm(d2,\nmean = 0, sd = 1) * (d21^2)) - (2 * pnorm(d2, mean = 0, sd = 1)/(ert *\n(m^3))) + (1/(ert * (m^2)) * dnorm(d2, mean = 0, sd = 1) * d21)\n# Recover strike price\nx = s/m\nc1 = -(m^2) * f1\nc2 = s * ((1/x^2) * ((m^2) * f2 + 2 * m * f1))\n# Calculate the quantities of interest\ncdf = ert * c1 + 1\nfstar = (ert * c2)\nreturn(fstar)\n}\n# Estimation and plotting of Risk Neutral Densities #\ndtau = round(odax$maturity * 365) # maturity in days\nodax = cbind(odax, dtau)\nmat = odax[, 4]\nForwardPrice = odax[, 1] * exp(IR * mat)\nRawData = cbind(ForwardPrice, odax[, 2], IR, mat, odax[, 5], odax[, 6],\nodax[, 7], odax[, 9])\n# Based on Rookley (1997) the following function estimates the smile\n# with an automatic selection of the local bandwidths. Then, the same\n# bandwith is reused to estimate the first and second derivative of the\n# smile. In a last step, these estimations are used to compute the spd\n# based on the result of Breeden and Litzenberger\nSPDrookley = function(RawData, tau_day) {\nData = subset(RawData, RawData[, 8] == tau_day)\n# Selecting interval of the strike price\nif (tau_day <= 150) {\nxGrid = seq(min(Data[, 2]) - 100, max(Data[, 2]) + 100, length.out = length(Data[, 1]) * 3)\n} else {\nif (tau_day > 150) {\nqS_d = as.numeric(quantile(Data[, 2], probs = 0.05, type = 6))\nxGrid = seq(qS_d - 500, qS_u + 500, length.out = length(Data[, 1]) * 2)\n}\n# Automatic bandwidth selection (see H\u00e4rdle et al. 2004)\nlocband = length(Data[, 1])^(-1/9)\nScorrected = as.matrix(Data[, 1] * exp(-Data[, 3] * Data[, 4]))\nMoneyness = Scorrected/Data[, 2]\nMaturity = Data[, 4]\n# Computing moneyness for the selected interval of the strike price\nxGridTemp = as.matrix(Scorrected[1]/xGrid)\n# Standardizing Moneyness and xGridTemp\nMoneynessSTD = (Moneyness - matrix(mean(Moneyness), nrow(Moneyness),\nncol(Moneyness)))/matrix(sqrt(var(Moneyness)), nrow(Moneyness),\nncol(Moneyness))\nxGridTempSTD = (xGridTemp[, 1] - mean(Moneyness))/sqrt(var(Moneyness))\ndataf = data.frame(tmp1 = MoneynessSTD, tmp2 = ivola)\n# Estimation of the volatility smile and its first and second\n# derivatives by local polynomials (Nadaraya-Watson)\nsmile = locpoly(x = MoneynessSTD, y = ivola, gridsize = length(xGridTempSTD),\nkernel = EpaK, range.x = c(min(xGridTempSTD),\nbandwidth = locband)$y\nFirstDerSmile = locpoly(x = MoneynessSTD, y = ivola, gridsize = length(xGridTempSTD),\nbandwidth = locband, drv = 1)$y/c(sqrt(var(Moneyness)))\nSecondDerSmile = locpoly(x = MoneynessSTD, y = ivola, gridsize = length(xGridTempSTD),\nbandwidth = locband, drv = 2)$y/c(sqrt(var(Moneyness)))\n# Using spdbl to estimate SPD\nresult = spdbl(flipud(xGridTemp), smile, FirstDerSmile, SecondDerSmile,\nScorrected[1, 1], Data[1, 3], Data[1, 4])\nreturn(list(xGrid = flipud(xGrid), result = result))\n####################### Plot SPD for different maturities #############################\ntau_day = round(sort(unique(odax$maturity), decreasing = F) * 365)\nSPD = list()\ncol = c(\"red\", \"#A7A7A7\", \"dodgerblue\", \"forestgreen\", \"gold\")\n# Separately\nxlim_rnd = matrix(c(6000, 7000, 4500, 6000, 12000,\n13000, nrow = 5)\nylim_rnd = matrix(c(rep(0, length = 5), 8e-04,\nnrow = 5)\nfor (i in 1:length(tau_day)) {\nSPD[[i]] = SPDrookley(RawData, tau_day[i])\nplot(SPD[[i]]$xGrid, SPD[[i]]$result, xlim = xlim_rnd[i, ], ylim = ylim_rnd[i,\n], xlab = \"Spot price\", ylab = \"Density\", type = \"l\",\nmain = paste(\"Risk neutral density,\",\ntau_day[i], \"days to maturity\"), lwd = 3, col = col[i])\n# Together\nlegend_labels = paste(as.character(tau_day), \"Days\")\nSPD[[1]] = SPDrookley(RawData, tau_day[1])\nplot(SPD[[1]]$xGrid, SPD[[1]]$result, xlim = c(6500, 12500), ylim = c(0,\n8e-04), lwd = 3, xlab = \"Spot Price\", ylab = \"Density\", type = \"l\",\nmain = \"Risk neutral density for different maturities\", col = col[1])\nfor (i in 2:length(tau_day)) {\nlines(SPD[[i]]$xGrid, SPD[[i]]$result, col = col[i], lwd = 3)\nlegend(\"topleft\", legend_labels, lty = 1, lwd = 2, col = col, bty = \"n\", cex = 0.8)\n# 3D plot of risk neutral densities for all maturities\ncol_rnd = alpha(\"blue3\", alpha = 0.2)\ns_rnd = c(SPD[[1]]$xGrid,\nSPD[[5]]$xGrid)\ndens_rnd = c(SPD[[1]]$result,\nSPD[[5]]$result)\ntau_rnd = c(rep(10, length(SPD[[1]]$xGrid)), rep(20, length(SPD[[2]]$xGrid)),\nlength(SPD[[5]]$xGrid)))\ndf_rnd = cbind(tau_rnd, s_rnd, dens_rnd)\ns3d_rnd = scatterplot3d(df_rnd, type = \"h\", color = col_rnd,\nmain = \"Risk Neutral Densities for different maturities\",\ntick.marks = TRUE, font.main = 2, xlab = \"Days to maturity\", ylab = \"Stock Price\",\nzlab = \"Density\", x.ticklabs = c(\"25\", \"60\", \"88\", \"179\", \"270\"), z.ticklabs = c(\"0\",\n\"2e-04\", zlim = c(0, 8e-04), ylim = c(4000, 13500))\n# Estimation of historical densities #\n# Set time interval for the observations\nInterval = 3\nObsDate = as.character(unique(odax$date))\nStartYear = as.numeric(substr(ObsDate, 1, 4)) - Interval\nStartDate = ifelse(Interval == 0, min(idata$Date), paste(StartYear, substr(ObsDate,\n6, 7), substr(ObsDate, 9, 10), sep = \"-\"))\n# Select data for chosen time interval\nidax = subset(dax, (dax$Date <= ObsDate & dax$Date >= StartDate))\n# Plot Stock Price for the given time interval\nhdax = subset(dax, dax$Date >= \"2010-12-22\")\nusr = par(\"usr\")\nplot(hdax$Date, hdax$DAX, type = \"l\", col = \"blue3\", lwd = 3, font.main = 2,\npanel.first = grid(0, 2), main = \"Historical Stock Prices of DAX Index\",\nxlab = \"\", ylab = \"DAX Stock Price\", xaxt = \"n\")\nabline(v = as.Date(\"2014-12-22\"), col = \"red3\", lwd = 3)\nabline(v = axis.Date(1, at = seq(as.Date(\"2011-12-22\"),\n\"years\"), format = \"%Y/%m/%d\"), col = \"lightgray\", lty = \"dotted\", lwd = par(\"lwd\"))\nrect(usr[1], usr[2], as.Date(\"2011-12-22\"), usr[3], col = alpha(\"gray\",\nalpha = 0.2))\nrect(as.Date(\"2014-12-22\"), usr[3], col = alpha(\"gray\",\n# Calculate returns for each maturity\ndaxT = matrix(NA, length(idax$DAX) - tau_day - 1, length(tau_day))\nS = idax$DAX\ndaxT[1:(length(S) - tau_day[i] - 1), i] = S[1] * (S[2:(length(S) -\ntau_day[i])]/S[(2 + tau_day[i]):length(S)])\n# Calculate the bandwith for Kernel Estimation via cross-validation\n# method\nhdens = list()\nband = vector(length = length(tau_day))\nband_ccv = function() {\nfor (i in 1:length(tau_day)) {\nband[i] = h.ccv(daxT[, i], kernel = \"biweight\")$h\nreturn(band)\n################# Plot historical density for different maturities#################\nxlim_hd = matrix(c(7500, 7000, 6500, 12500,\n15000), nrow = 5)\nylim_hd = matrix(c(rep(0, length = 5), 7e-04,\nnrow = 5)\nh = band_ccv()\nhdens[[i]] = density(daxT[, i], bw = h[[i]], na.rm = T, kernel = \"biweight\")\nplot(hdens[[i]], xlim = xlim_hd[i, ], ylim = ylim_hd[i, ], xlab = \"Spot price\",\nylab = \"Density\", type = \"l\", main = paste(\"Historical density,\",\ntau_day[i], \"days to maturity\"), lwd = 3, col = \"red3\")\n# Plot historical density for the first maturity\ncol = c(\"red\", \"#A7A7A7\", \"dodgerblue\", \"forestgreen\", \"gold\")\nplot(hdens[[1]], xlim = c(7000, 15000), ylim = c(0, 7e-04), col = col[1],\nlwd = 3, xlab = \"Spot Price\", main = \"Historical density for different maturities\")\n# Add plots for the next maturities\nlines(hdens[[i]], col = col[i], lwd = 3)\nlegend(\"topleft\", legend_labels, lty = 1, lwd = 2, col = col, bty = \"n\",\ncex = 0.8)\n# 3D plot of historical densities for all maturities\ncol_hd = alpha(\"red3\", alpha = 0.2)\ns_hd = c(hdens[[1]]$x,\ntau_hd = c(rep(10, length(hdens[[1]]$x)), rep(20, length(hdens[[2]]$x)),\nlength(hdens[[5]]$x)))\ndf_hd = cbind(tau_hd, s_hd, dens_hd)\ns3d_hd = scatterplot3d(df_hd, type = \"h\", color = alpha(\"red3\", alpha = 0.1),\nmain = \"Historical density for different maturities\", tick.marks = TRUE,\nfont.main = 2, xlab = \"Days to maturity\", ylab = \"Stock Price\", zlab = \"Density\",\nx.ticklabs = c(\"25\", \"60\", \"88\", \"179\", \"270\"), z.ticklabs = c(\"0\",\n\"1e-04\", \"\", \"3e-04\", \"\", \"5e-04\", \"\", \"7e-04\"))\n# Comparing risk neutral and historical density for different maturities #\n# RND and HD for different maturities\nxlim = matrix(c(7500, 7000, 5500, 6000, 12500,\n15000), nrow = 5)\nylim = matrix(c(rep(0, length = 5), 8e-04,\nnrow = 5)\nplot(SPD[[i]]$xGrid, SPD[[i]]$result, xlim = xlim[i, ], ylim = ylim[i,\n], col = \"blue3\", type = \"l\", lwd = 3, xlab = \"Stock price\", ylab = \"Density\",\nmain = paste(\"RND (blue) and HD (red) for\", tau_day[i], \"days to maturity\"))\nlines(hdens[[i]], col = \"red3\", lwd = 3)\n# RND and HD for the first maturity - Trading Strategy\nplot(SPD[[1]]$xGrid, SPD[[1]]$result, xlim = xlim[1, ], ylim = ylim[1,], col = \"blue3\", type = \"l\", lwd = 3,\nxlab = \"Stock price\", ylab = \"Density\",\nmain = paste(\"RND and Historical Density for\", tau_day[1], \"days to maturity\"))\nlines(hdens[[1]], col = \"red3\", lwd = 3)\ntext(8200, 0.000125, \"Sell Puts\", cex = 0.8)\n", "output_sequence": "Generates and plots the path of a general Ito process."}, {"input_sequence": "Author: Yaojun Liu, Yinan Wu ; function [S,t,V] = AmericanOption(K,T,r,delta,sigma,type,m,n)\n\n% Input:\n%\n% K = strike price\n% delta = rate of dividend payment\n% sigma = volatility\n% type = type of option ('call' or 'put')\n% Output:\n% S = range of stock prices\n% t = range of time points from 0 to T\n% V = corresponding option prices, i.e.\n% V(i,j) is an approximation of V(S(i),t(j))\n%% Verification\nswitch type\ncase 'call'\n% For a call, r must be larger than delta\nif r <= delta\nerror('r must be larger than delta')\nend\ncase 'put'\notherwise\nerror('type must be call or put')\nend\n\n%% Define parameters\n% dimensionless parameters\nq = 2*r/sigma^2;\nq_delta = 2*(r-delta)/sigma^2;\n% asset space\nx_min = -5;\ndx = (x_max-x_min)/m;\n% time space\ntau_max = .5*sigma^2*T;\ndtau = tau_max/n;\n% numerical parameters\neps = 1e-6;\ntheta = .5;\nomega_R = 1;\nlambda = dtau/dx^2;\nalpha = lambda*theta;\n% verify stability condition\nif lambda > .5\ns = sprintf('lambda = %4.2f',lambda);\ndisp(s)\nerror(strcat('The algorithm is unstable. Stability can be obtained ',...\n' by increasing the value of n or by decreasing the value of m'))\n%% Initialization\n% state and time space\nx = (x_min:dx:x_max)';\ntau = 0:dtau:tau_max;\n% For performance reasons we compute one matrix with all the g values\nX = repmat(x,1,n+1);\nG = g(X,Y);\n% dimensionless option value\nw = zeros(m+1,n+1);\n% boundary conditions\nw(:,1) = G(:,1);\nw(m+1,:) = G(end,:);\n% righthandside is needed in core algorithm\nb = zeros(m-1,1);\n% SOR iteration vector needs to be pre-allocated only once\nvnew = zeros(m-1,1);\n%% Core algorithm\nfor j = 2:n+1\n% create righthandside b\nfor k = 1:m-1\nswitch k\ncase 1\nb(k) = w(2,j-1)+lambda*(1-theta)*(w(1,j-1)-2*w(2,j-1)+w(3,j-1))+alpha*w(1,j);\ncase m-1\nb(k) = w(m,j-1)+lambda*(1-theta)*(w(m-1,j-1)-2*w(m,j-1)+w(m+1,j-1))+alpha*w(m+1,j);\notherwise\nb(k) = w(k+1,j-1)+lambda*(1-theta)*(w(k,j-1)-2*w(k+1,j-1)+w(k+2,j-1));\nend\n% initialize vector v\nv = max(w(2:m,j-1),G(2:m,j));\n% the variable iter is introduced to manage the SOR iteration\niter = 1;\n% SOR iteration\nwhile iter == 1\nfor k = 1:m-1\nswitch k\nrho = (b(k)+alpha*v(k+1))/(1+2*alpha);\ncase m-1\nrho = (b(k)+alpha*vnew(k-1))/(1+2*alpha);\notherwise\nrho = (b(k)+alpha*(vnew(k-1)+v(k+1)))/(1+2*alpha);\nend\nvnew(k) = max(G(k+1,j),v(k)+omega_R*(rho-v(k)));\nif norm(v-vnew) <= eps\niter = 0;\nelse\nv = vnew;\nw(2:m,j) = vnew;\n%% Transformation to original dimensions\nS = K*exp(x);\nt = T-2*tau/sigma^2;\nV = K*exp(-.5*(q_delta-1)*x)*exp(-(.25*(q_delta-1)^2+q)*tau).*w;\n% re-arrange t and V in increasing time order\nt = fliplr(t);\n%% Define function g as a nested function\nfunction boundary = g(x,tau)\n% This function is the lower bound for the dimensionless solution\n% of the linear complementarity problem. It is also used for\n% the boundary and initial conditions.\nabb1 = exp(((q_delta-1)^2+4*q)*tau/4);\nswitch type\ncase 'put'\nboundary = abb1.*max(abb2-abb3,0);\ncase 'call'\nboundary = abb1.*max(abb3-abb2,0);\n", "output_sequence": "Computes the optimal exercise boundary problem of American put option using a binomial tree under different volatilities"}, {"input_sequence": "Author: Yaojun Liu, Yinan Wu ; % ---------------------------------------------------------------------\n% Quantlet: SFEBoundary_V\n% ---------------------------------------------------------------------\n% Description: SFEBoundary_V computes the optimal exercise boundary problem\n% of American put option using a binomial tree under\n% different volatility.\n% Inputs: s0 - Stock Price\n% k - Exercise Price\n% sig - Volatility\n% t - Time to Expiration\n% n - Number of Intervals\n% Output: binomial trees and boundary figure\n% Author: Yaojun Liu, Yinan Wu 20151228\nclear,clc;clear all;\n%% User inputs parameters\ndisp('Please input Price of Underlying Asset s0, Exercise Price k, Domestic Interest Rate per Year i');\ndisp('Time to Expiration (Years) t, Number of steps n');\ndisp('as: [230, 210, 0.04545,0.5, 50]');\ndisp(' ') ;\npara=input('[s0, k, i, t, n]=');\nwhile length(para) < 5\ndisp('Not enough input arguments. Please input in 1*5 vector form like [230, 210, 0.04545, 0.25, 50]');\ndisp(' ') ;\npara=input('[s0, k, i, t, n]=');\nend\ns0=para(1); % Stock price\nt=para(4); % Time to expiration\nn=para(5); % Number of intervals\nsig=0.01:0.01:1; % Volatility sigma\n%Check conditions\nif s0<=0\ndisp('SFEBiTree: Price of Underlying Asset should be positive! Please input again')\ns0=input('s0=');\nif k<0\ndisp('SFEBiTree: Exercise price couldnot be negative! Please input again')\nk=input('k=');\nif t<=0\ndisp('SFEBiTree: Time to expiration should be positive! Please input again')\nt=input('t=');\nif n<1\ndisp('SFEBiTree: Number of steps should be at least equal to 1! Please input again')\nn=input('n=');\n%% Main computation\nboundary=zeros(length(sig),1);\nfor o=1:length(sig); % Different volatility\ndt=t/n; % Interval of step\nu(o)=exp(sig(o)*sqrt(dt)); % Up movement parameter u\nb=i; % Costs of carry\np(o)=0.5+0.5*(b-sig(o)^2/2)*sqrt(dt)/sig(o); % Probability of up movement\n% Pricing the option under different volatility\ns=ones(n+1,n+1)*s0;\nun=zeros(n+1,1);\ndm=un';\nj=1;\nwhile j<n+1\nd1=[zeros(1,n-j) (ones(1,j+1)*d(o)).^((1:j+1)-1)];\ndm=[dm; d1]; % Down movement dynamics\nu1=[ones(1,n-j)-1 (ones(1,j+1)*u(o)).^((j:-1:0))];\num=[um; u1]; % Up movement dynamics\nj=j+1;\nend\num=[un';um]';\ndm=dm';\ns=s(1,1).*um.*dm; % Stock price development\nStock_Price=s;\ns=flipud(s); % Rearangement\nopt = zeros(size(s));\n%% Option is an american put\nopt(:,n+1) = max(k-s(:,n+1),0); % Determine option values from prices\nfor j = n:-1:1\nl = 1:j;\n% Probable option values discounted back one time step\ndiscopt = ((1-p(o))*opt(l,j+1)+p(o)*opt(l+1,j+1))*exp(-b*dt);\n% Option value is max of X - current price or discopt\nopt(:,j) = [max(k-s(1:j,j),discopt);zeros(n+1-j,1)];\nAmerican_Put_Price = flipud(opt);\nboundary(o)=k-American_Put_Price(end,1); % Boundary price with different volatility\n%% Figures of boundary price under different volatility\nplot(sig,boundary,'k-')\ntitle('Exercise boundary')\nxlabel('Volatility(sigma)')\nylabel('Stock price')\n", "output_sequence": "Computes the optimal exercise boundary problem of American put option using a binomial tree under different volatilities"}, {"input_sequence": "Author: Yaojun Liu, Yinan Wu ; function Sf = FreeBoundary(S,t,V,K,type)\n\nSf = zeros(1,length(t));\neps_star = K*1e-5;\nswitch type\ncase 'put'\nfor j = 1:length(t)\nSf(j) = S(find(abs(V(:,j)-K+S)< eps_star, 1, 'last'));\nend\ncase 'call'\nSf(j) = S(find(abs(V(:,j)+K-S)< eps_star, 1, 'first'));\nend\n", "output_sequence": "Computes the optimal exercise boundary problem of American put option using a binomial tree under different volatilities"}, {"input_sequence": "Author: Johannes Stoiber ; ###############################################################################\n##\n## Preperation and helperfunctions for VWAP_Forecast ##\n## This file contains some data preperation and helperfunctions ##\n## to make sure that the quantlets run ##\n###############################################################################\n### preperations\nrm(list = ls())\ngraphics.off()\nlibraries = c(\"stargazer\", \"plyr\", \"moments\", \"zoo\", \"forecast\", \"urca\", \"expectreg\", \"fda.usc\", \"vars\",\n\"lattice\", \"tseries\", \"abind\", \"sm\", \"quantreg\")\nlapply(libraries, function(x)if(!(x %in% installed.packages())){install.packages(x)})\nlapply(libraries, require, quietly = TRUE, character.only = TRUE)\n## Data preperation ##\n## Remark: Data are artificial and only for ##\n## demonstration that the code works ##\n# specifiy paht here:\nsetwd(\"C:/Users/Johannes/Documents/GitHub/VWAP_Forecast\")\ndf.data = read.csv(\"vwap_data.csv\")\nrel_act = matrix(df.data[,3], ncol = 24, byrow = TRUE, dimnames = list(unique(df.data[,1]),\n## Holiday Effects ##\n## create holiday dummy\n# fill in this vector public holidays or other days with special effects.\nholiday_dummy = as.numeric(rownames(vwap) %in% c(\"2015-01-01\", \"2015-02-07\"))\n## Helperfunctions ##\n# function to compute fourier series\nfourier.series = function(t, terms, period){\nn = length(t)\nX = matrix(NA, nrow = n, ncol = 2*terms)\nfor(i in 1:terms){\nX[, 2*i-1] = sin(2*pi*i*t/period)\n}\ncolnames(X) = paste(c(\"S\", \"C\"), rep(1:terms, rep(2, terms)), sep = \"\")\nreturn(X)\n}\n# function to compute saisonalizde, data and produce forecast\ndsc = function(Load_list, WD, PH, n, k, hours, p, model = \"no\"){\n\n# Create arrays with NA, length of Load_list, to save components\nDT_Load = array(NA, dim = length(Load_list))\n\n# matrix with daily interval to store day ahead forecast\nseason_forecast = matrix(ncol = 1, nrow = hours)\n# parameters for the nonparametric estiamtes\nnonpar = c(\"smooth.spline\", \"sm.regression\", \"loess\")\ndays = 365\nyear = 1:days\nfor(i in 1:hours){\n# index that will indicate i-th interval of day\nindex = as.numeric(seq(from = i, to = (n + k) * hours, by = hours))\n# get load demand from every days i-th qaurter/15-min interval of day\ntemp = Load_list[index]\n\n# vector from 1 to length of training period\nt = 1:length(temp)\n# check if nonparametric estimation should be conducted\nif (model %in% nonpar){\n\nmat = matrix(temp)\nmod_year = days - length(temp)%%days\nif(mod_year == max(year)){\nmod_year = 0\nmat = mat\n} else {mat = rbind(mat, matrix(NaN, nrow = mod_year, ncol = 1) )}\nYN1 = matrix(data = mat, nrow = days, byrow = FALSE)\nY1 = rowMeans(YN1, na.rm = TRUE) #mean over years\nif(model == \"smooth.spline\"){\nfit.mod = smooth.spline(x = year, y = Y1)\nresid.mod = (mat - rep(fit.mod$y, dim(mat)[1]/days))[1:(length(mat) - mod_year)]\nfit.val = fit.mod$y\n}\nif(model == \"sm.regression\"){\nhs = h.select(x = year, y = Y1, method = \"cv\")\nfit.mod = sm.regression(x = year, y = Y1, h = hs, eval.points = year, display = 'none')\nresid.mod = (mat - rep(fit.mod$estimate, dim(mat)[1]/days))[1:(length(mat) - mod_year)]\nfit.val = fit.mod$estimate\nif(model == \"loess\"){\nfit.mod = loess(Y1 ~ year)\nresid.mod = (mat - rep(fit.mod$fitted, dim(mat)[1]/days))[1:(length(mat) - mod_year)]\nfit.val = fit.mod$fitted\n# extract residuals, to get the desaesonalized series, and also the seasonal part\nDT_Load[index] = resid.mod\nseason[index] = rep(fit.val, length = (n + k))\n# forecast\nif (i == hours){\nidx.np = ifelse((n + k + p)%%days == 0, days, (n + k + p)%%days)\nseason_forecast = matrix(season, nrow = hours)[, idx.np]\n}\nif(model == \"fourier\"){\n# regress load of i-th interval of day on day, dummies for weekdays and dummies for public holiday and fourier series\nltsc = lm(temp ~ t + WD[1:(n + k), ] + PH[1:(n + k), ] + fourier.series(t, 4, 365.25))\n# n is training period. p is forecast horizon.\nnew = as.data.frame(t(c(1, (max(t) + p), WD[(n + k + p), ], PH[(n + k + p), ],\nfourier.series((max(t) + p), 4, 365.25))))\n# extract residuals, to get the desaesonalized series, and also extract the seasonal part\nDT_Load[index] = ltsc$residuals\nseason[index] = ltsc$fitted.values\n# extract coefficients from regression\ncoeff = ltsc$coefficients\nnames(new) = names(coeff)\nseason_forecast[i, 1] = sum(coeff * new)\nif(model == \"no\"){\n# regress load of i-th interval of day on day, dummies for weekdays and dummies for public holiday\nltsc = lm(temp ~ t + WD[1:(n + k), ] + PH[1:(n + k), ])\nnew = as.data.frame(t(c(1, (max(t) + p), WD[(n + k + p), ], PH[(n + k + p), ])))\nseason_forecast[i,1]=sum(coeff * new)\n\n# return deseasonalized series and 1 step ahead forecast\nreturn(list(\"detrended\" = DT_Load, \"fitted\" = season, \"forecast\" = season_forecast))\n# function to extract the pc scores\nfpca = function(num, fit, x, nh = 4){\n# num= 1 # 1-7\n# fit=\n# x = x # equispaced intevalls defined above\ndataResid = Data2fd(x, t(fit[, , num]))\nPCA_Resid = pca.fd(dataResid, nharm = nh, centerfns = FALSE)\nreturn(PCA_Resid)\n# function to plot the eigenfunctions of the fpca\nplot.PC = function(ind, num, result){\nplot(result[1, ][[num]][ind], ylab = paste(\"FPC\" , ind), xlab = \"Hour\", cex.lab = 2, cex.axis = 2,lwd = 3,\nylim = c(-2, 2), xaxt = 'n')\nabline(h = 0, lty = 2)\naxis(1, c(0, 0.25, 1), c(\"00:00\", \"12:00\", cex.axis = 1.5)\n# function to plot the scores of the fpca\nplot.score = function(ind, num, result){\nplot(result[3, ][[num]][, ind], ylab = bquote(alpha[.(ind)]),\nxlab = \"Day\",\nylim = c(range(result[3,][[num]])),\ncex.lab = 2, cex.axis = 2,lwd = 3, type = \"l\")\n# function to test for stationarity. ADF, KPSS\nstationarity.test = function(x){\nadf.result = apply(x, 2, adf.test)\np.adf = sapply(adf.result, \"[[\", \"p.value\")\nkpss.result = apply(x, 2, kpss.test)\np.kpss = sapply(kpss.result, \"[[\", \"p.value\")\ndd = length(adf.result)\nresult = array(NA,\ndim = c(2, dd),\ndimnames = list(c(\"ADF\", \"KPSS\"), paste(\"PC\", seq(1:dd), sep = \"\")))\nresult[1:2, ] = rbind(p.adf, p.kpss)\nreturn(result)\n}\n# function to extract PC scores due to certain threshold\npca.man = function(X, tau = .9){\npca = prcomp(X, center = TRUE, scale = TRUE)\nvar = pca$sdev^2\nidx = which(cumsum(var)/sum(var) > tau)[1]\nexpl = var/sum(var)\ncum.prop = cumsum(expl)\nscores = pca$x[, 1:idx]\nreturn(list(\"scores\" = scores,\"expl\" = expl, \"cum.prop\" = cum.prop ))\n# function to apply auto.arima on each score series\nfit.auto.arima = function(end, exo, expec = tau[2], tau){\nexpec = which(tau %in% expec)\n# for the number of different models\nle = length(exo)\n# pc to include:\npc = dim(end[[expec]])[2]\nfor (i in 1:le){\nif (i ==1){result.arr = array(NA, dim = c(n, pc, le))}\nif (i < le){\nfor (j in 1:pc){\nresult.arr[, j, i] = fitted(auto.arima(end[[expec]][, j], xreg = exo[[i]], ic = \"aic\", seasonal = FALSE))\nif (i == le){\nresult.arr[, j, i] = fitted(auto.arima(end[[expec]][, j], ic = \"aic\", seasonal = FALSE))\n# reorganize into a list\nfit.scores = alply(result.arr, 3, .dims = TRUE)\nreturn(fit.scores)\n# function to apply auto.arima on each score series and produce a forecast for t+1\nforc.auto.arima = function(end, exo, expec = tau[2], tau){\ntau = tau\nexpec = which(tau %in% expec)\nle = length(exo)\n# pc to include and dimension\nif (i ==1){result.arr = array(NA, dim = c(pc, le))}\nresult.arr[j, i] = forecast(auto.arima(end[[expec]][, j], xreg = exo[[i]][c(1:n), ], ic = \"aic\", seasonal = FALSE),\nxreg = matrix(exo[[i]][(n+1), ], nrow = 1), h = 1)$mean\nresult.arr[j, i] = forecast(auto.arima(end[[expec]][, j], ic = \"aic\", seasonal = FALSE), h = 1)$mean\nreturn(result.arr)\n# fit the model\nfit.curve = function(num, fd, model, mean, season, xsim, startperiod = 10){\nfd = fd[, num]\nfit.model = apply(model[[num]], 1, FUN = function(x){eval.fd(evalarg = xsim, fdobj = (fd$harmonics[1]*x[1]\n+ fd$harmonics[2]*x[2]\nfit.model = fit.model + mean[, num]\n# compute difference of columns to take lag order into account\nl = abs(dim(fit.model)[2] - dim(season)[2])\nif(l != 0){\nseason = season[, -c(1:l)]\nfit = season + fit.model\nfit = fit[, -c(1:(startperiod - l))]\nif (l == 0){\nreturn(fit)\nfit.eval = function(num, model, origin){\nXX = c(model[[num]] - origin)\nMAE = mean(abs(XX))\n# MSE = mean(XX^2)\nreturn(c(\"MAE\" = MAE, \"RMSE\" = RMSE))\n# reconstruct VARX curves with Karhunen - Loeve\nKarLoe = function(num, mean, model, fd, xsim, season){\nfd = fd[,num]\nx = model[[num]]\ncurve = eval.fd(evalarg = xsim, fdobj = (fd$harmonics[1]*x[1]\n+ fd$harmonics[2]*x[2]\nf.curve = curve + season + mean[, num]\ncolnames(f.curve) = \"fcst\"\nreturn(f.curve)\n# reconstruct arima curve with Karhunen - Loeve\nKarLoe.ar = function(num, model, mean, fd, xsim, season){\nx = model[[num]]\ncurve = eval.fd(evalarg =xsim, fdobj = (fd$harmonics[1]*x[1]\n+ fd$harmonics[2]*x[2]\nf.curve = curve + season + mean\n# function to compute rolling window RMSE\nrollingRMSE = function(r.length, origin, forc){\nerr2 = (origin - forc)^2\n# for the framework\nfor(i in 1:r.length){\nif (i == 1){\nnum = array(NA, dim=c((dim(err2)[2] + 1 - r.length),\nnum[, i] = seq(1:(dim(err2)[2] + 1 - r.length))\n}else{\nnum[, i] = num[, i - 1] + 1\nperiodic.RMSE = array(NA, dim = c(dim(num)[1]))\n# rmse for desired window\nfor(i in 1:c(dim(num)[1])){\n#i = 1\nperiodic.RMSE[i] = sqrt(mean(err2[, num[i, ]]))\nreturn(periodic.RMSE)\n################### functions for FASTEC\n# from: https://github.com/QuantLet/FASTEC-with-Expectiles\n# mqr, G.qr, simtune\n##################### Function: SFISTA algorithm ########################################\nmqr = function(Y, X, tau, lambda, kappa = 1e-04, epsilon = 10^(-6), itt = 2000) {\n### Initialize ################\nm = ncol(Y)\nX2norm = base::norm(X, type = \"2\")\n#kappa = 1e-04 # instead of epsilon/(2*m*n) by theory, like Chen,Lin,Kim,Carbonell and Xing we use fixed kappa, the value we choose here is equivalent to epsilon = 50.\nL = X2norm^2/(kappa * m^2 * n^2)\nOmega = matrix(0, nrow = p, ncol = m)\ndelta = 1 # step size\nerror = 1e+07\nit = 1\n### Output ###################\nA = matrix(0, nrow = p, ncol = m)\n### Main iteration #########\nwhile (it < itt & error > epsilon) {\nS = svd(Omega - L^(-1) * G.qr(Omega, Y, X, tau, kappa, m = m, n = n), nu = p, nv = m)\ntemp.sv = S$d - (lambda/L)\ntemp.sv[temp.sv < 0] = 0\nA = S$u %*% diag(temp.sv, nrow = p, ncol = m) %*% t(S$v)\ndelta.for = (1 + sqrt(1 + 4 * delta^2))/2\nOmega = A + (delta - 1)/(delta.for) * (A - A_)\nerror = L.error - (sum((tau - matrix(as.numeric(Y - X %*% A < 0), n, m)) *\n(Y - X %*% A)) + lambda * sum(temp.sv)) # Before: Put abs around to ensure that it is positive (just in case)\nL.error = sum((tau - matrix(as.numeric(Y - X %*% A < 0), n, m)) * (Y - X %*%\nA)) + lambda * sum(temp.sv)\nA_ = A\ndelta = delta.for\nit = it + 1\nprint(c(error, delta, sum((tau - matrix(as.numeric(Y - X %*% A < 0), n, m)) *\n(Y - X %*% A)), sum(temp.sv)))\n# if(it < 10){error=1000000}\nlist(Gamma = A,\nd = S$d,\nerror = error,\nloss = sum((tau - matrix(as.numeric(Y - X %*% A < 0), n, m)) * (Y - X %*% A)),\nnorm = sum(temp.sv), lambda = lambda, iteration = it)\n##################### Function: Computing the gradient of the loss function ##############\nG.qr = function(A, Y, X, tau, kappa, m, n) {\nW = (m * n * kappa)^(-1) * (Y - X %*% A)\nindex_p = which(W > tau, arr.ind = TRUE)\nW[index_p] = tau\nindex_n = which(W < tau - 1, arr.ind = TRUE)\nW[index_n] = tau - 1\ntemp = (-t(X) %*% W)/(m * n)\ntemp\n##################### Function: Estimation of penalizing parameter 'lambda' #############\nsimtune = function(m, tau, XX, alpha = 0.1, B = 500, const = 2) {\nset.seed(1001)\nsim.lambda = numeric(0)\nn = nrow(XX)\nfor (i in 1:B) {\nW.temp = matrix(as.numeric(runif(n * m) < tau) - tau, nrow = n, ncol = m)\ntemp.score = norm(t(XX) %*% W.temp/n, type = \"F\")\nsim.lambda[i] = temp.score/m\nlam = const*quantile(sim.lambda, p = (1 - alpha))\nlist(sim.lambda = sim.lambda, lambda = lam)\n# add seasonal and estmiated stochastic component\ncurve.mqr = function(X, U, D, fitted.model, season, startid){\ncurve.mat = X %*% U %*% D %*% t(fitted.model)\ndiffs = startid -1 - (dim(season)[2] - dim(curve.mat)[2])\n# take into account that diffs can be 0\nif(diffs == 0){\ntrim.curve = curve.mat\n}else{\ntrim.curve = curve.mat[, -c(1:diffs)]\ntrim.season = season[, -c(1:(startid-1))]\nresult = trim.curve + trim.season\n# Add forecasted of seasonal and stochastic component\nforecast.mqr = function(X, U, D, fitted.model, season){\ncurve.mat = X %*% U %*% D %*% sapply(fitted.model[[1]], \"[[\",1)\nresult = curve.mat + season\n# Count Percentage of vwap between predicted tau range\nhit.range = function(band.array, origin, lower, upper){\nsum(c(band.array[, , lower]) < c(origin) & c(band.array[, , upper]) > c(origin)) / length(c(band.array[, , upper]))\nsave.image(file=\"VWAP.RData\")\n", "output_sequence": "Quantlets for the codes used in my master thesis. The data published here are artificial and only for the purpose to illustrate how the codes work. This thesis uses the concept of generalized quantiles to compute probabilistic forecasts of volume weighted average prices (VWAP) stemming from the German intraday market for electricity contracts. These prices exhibit extreme values in both directions and correlate inter- and intradaily. Dimensions are reduced by functional principal component and factorisable sparse tail event curve techniques. The dependency structure of the factor scores for generalized quantiles is analyzed with a VAR model that allows for incorporation of exogenous information such as renewable energy production forecasts. Price forecasts from both models are evaluated with root mean squared error and mean absolute error. Interval forecasts are evaluated, to which share the interval captures observed prices. Supplementary material for this thesis is available online."}, {"input_sequence": "Author: Johannes Stoiber ; ###############################################################################\n##\n## VWAP FASTEC Forecast. R-Script to apply FASTEC to desaisonalized ##\n## the vwap weries, and forecast loadings with VAR. ##\n###############################################################################\n# load the R Environment with prepared data and packages\nload(\"VWAP.RData\")\nlibraries = c(\"stargazer\", \"plyr\",\"moments\",\"zoo\",\"forecast\",\"urca\",\"expectreg\",\"fda.usc\", \"vars\",\"lattice\",\"tseries\", \"abind\",\"sm\",\"quantreg\",\"matrixStats\")\nlapply(libraries,function(x)if(!(x %in% installed.packages())){install.packages(x)})\nlapply(libraries,require,quietly=TRUE,character.only=TRUE)\n# set some prarameter\nn = 65 # Train Data length\nhour = 24 # number of avialable daily observations\nTAU = c(0.01, 0.1, 0.25, 0.9, 0.99) # quantiles of interest\nmodel = \"fourier\" # estimation of seasonal component\n# \"fourier\" includes also dummies for weekday and holidays\n# \"smooth.spline\", \"sm.regression\", \"loess\" do not include holiday and weekday effects\ntrsh = .95 # threshold how mouch should be explained by exogenous variables; apply PCA\nlag.l = 3 # (lag.l-1) determines allowed maximum lag length for VAR models.chose fiteen to allow for past two weeks as impact; 30 days windwo: 3; 60 days window 5\np = 1 # parameter demanded by for seasonality forecast\nk = 0 # k is used to set increasing steps, only needed to vary in testperiod\nlt = length(TAU) # number of expectils\nholiday_dummy = holiday_dummy # dummy variable indicating public holidays\nwindow = 65 # shorter window\nwindow = ifelse(exists(\"window\"),\nwindow, n)\ncrit = c(\"SC\", \"AIC\") # information criteria to select lag order\npf = ceiling(hour^0.4) # number of factors to choose\nqt.5 = which(TAU %in% 0.5) # index of 0.5 expectile\nqt.l = which(TAU %in% min(TAU)) # lowest quantile in TAU vector\nh = dim(vwap)[1] - n # forecast length.\n################################################################################\nstart = Sys.time()\n# Variables and parameters for MQR\nxx = seq(1/hour, 1, length = hour)\nX.fac = bs(xx, df = pf, intercept = TRUE)\nX.val = X.fac[,]\nlamb = lapply(X = c(1:lt), FUN = function(X){simtune(m = n, tau = TAU[X], XX = X.val, alpha = 0.1, B = 1000, const = 2)$lambda})\nnames(lamb) = paste(TAU)\nsapply(lamb, \"[[\", 1)\nitt = 2000\nepsilon = 1e-06\n# Start loop for 1 day-ahead forecasts\nfor(k in 0:(h-2)){\n# assume we know all the exogen variables one day in advance.\nInput_list = c(t(vwap[c(1:(n + k)), ]))\n\n## DSC\n## DSC: construct dummies for deseasonalization. wd and ph are only zeros\n## because sund and wind do not care for weekend or public holidays\ntempseq = as.numeric(rep(rep(c(1:7), each = 1), length.out = n + k + 2))\nWD = sapply(1:6, FUN = function(num){as.numeric(tempseq == num)})\nwd = matrix(rep(0, prod(dim(WD))) , ncol = dim(WD)[2])\nholiday_dummy = as.matrix(holiday_dummy, ncol = 1)\nph = matrix(rep(0, (prod(dim(holiday_dummy)))))\n# The seasonal component\nif(k == 0){DSC_Forc.mat = array(NA, dim = c((h - 1), hour))}\nDSC_Forc.mat[(k + p), ] = dsc(Input_list, WD, PH = holiday_dummy, n = n, k = k, hours = hour, p = p, model = model)$forecast\n# Compute deseasonalized series\nDSC_Input = dsc(Input_list, WD = WD, PH = holiday_dummy, n = n, k = k, hours = hour, p = p, model = model)[[1]]\n# create matrix of residuals. rows are 1-h intervalls. columns are the sepcific calender days\nresid_data_yearly = matrix(DSC_Input,nrow = hour)\nex2_data_yearly = matrix(DSC_Input_ex2 - DSC_Input_ex3 - DSC_Input_ex4, nrow = hour)\nex4_data_yearly = ex2_data_yearly - ex3_data_yearly\n# compute Principal Components of the explanatory variables\nPCA_ex = list( pca.man(t(ex1_data_yearly), tau = trsh),\n# get the scores\nscores.ex = lapply(lapply(PCA_ex, \"[[\", \"scores\" ), \"[\",c((n + k + p - window):(n + k + p)),TRUE)\n# MQR\nY = data.matrix(resid_data_yearly[, c((n + k + p - window):(n + k ))])\nfit.vwap = lapply(X = c(1:lt), FUN = function(X){mqr(Y = Y, X = X.val, tau = TAU[X], kappa = kappa, epsilon = epsilon, lambda = lamb[[X]], itt =itt)})\n# eonomy SVD, and assign column names for factor loadings\neconU = lapply(fit.vwap, \"[[\", \"U\")\necond = lapply(lapply(fit.vwap, \"[[\", \"d\"), diag)\neconV = lapply(econV, \"colnames<-\", paste0(\"load\",seq(1,pf)))\n# for both criteria\nfor (IC in crit){\n# run VAR model\nmodel.spot = lapply(X = c(1:lt), FUN = function(X){VAR(y = econV[[X]], exogen = scores.ex[[1]][-(window + p), ], lag.max = (lag.l - 1), type = \"const\", ic = IC)})\n\n# forecast the factor loadings\npred.spot = lapply(X =c(1:lt), FUN = function(X){predict(model.spot[[X]], n.ahead = p, dumvar = matrix(scores.ex[[1]][(window + p), ], nrow = p))})\npred.endo = lapply(X =c(1:lt), FUN = function(X){predict(model.endo[[X]], n.ahead = p)})\n# reconstruct forecasted stochastic curve and seasonal forecast\nforc.spot = lapply(X = c(1:lt), FUN = function(X){forecast.mqr(X = X.val, U = econU[[X]], D = econd[[X]], fitted.model = pred.spot[[X]], season = DSC_Forc.mat[(k + p), ])})\n# depending on selected IC, save forecasted values, container to store results is created in first loop (k=0)\nif (IC == \"AIC\"){\nif ( k == 0 ){\n# 3d-array to store results\nforc.endo.AIC = array(NA, dim = c(hour, (h - 1), lt) )\n}\nforc.endo.AIC[, k + p, ] = abind(forc.endo, along = 2)\n}\nif (IC == \"SC\"){\nforc.endo.SC = array(NA, dim = c(hour, (h - 1), lt) )\n}\nforc.endo.SC[, k + p, ] = abind(forc.endo, along = 2)\n} # end \"SC\" result savings\n} # end loop over IC\n} # end k loop\nSys.time() - start\n## Plot and Evaluation ##\n## Plot: Daily Curves with forecast from best model ##\n# Table:\n# compute errors at median\norigin = t(vwap[c((n+1):(n+h-1)), ])\n# save results in list\nmedian.Fit = list(\"DA spot AIC\" = forc.spot.AIC[, , qt.5],\n\"RL actuals AIC\" = forc.RLac.AIC[, , qt.5],\n\"DA spot\" = t(spot[c((n+1):(n+h-1)), ]),\n\"Trend\" = t(DSC_Forc.mat)\n)\n# compute errors\nerror.insample = t(sapply(X=c(1:length(median.Fit)), fit.eval, origin = origin, model = median.Fit))\nbind.models = list(forc.spot.AIC,\nforc.RLac.AIC,\n# add information how often VWAP is within predicted tau range\nerror.insample = cbind(error.insample,\n\"within_tau.01\" = c(sapply(X = c(1:length(bind.models)), FUN = function(X){(hit.range(bind.models[[X]], origin, qt.l, qt.u))}),\nrep(NA, 2)),\n\"within_tau.05\" = c(sapply(X = c(1:length(bind.models)), FUN = function(X){(hit.range(bind.models[[X]], origin, 2, 6))}),\nrep(NA, 2)))\n# assign model specifications\nerror.insample = data.frame(\"TS model\" = c(rep(\"VAR\", 10), \"DA Spot\", \"Trend\"),\n\"Selection\" = c(rep(\"AIC\",5), rep(\"BIC\", 5), rep(\" - \", 2)),\n\"Exogenous Variable\" = c(rep(c(\"DA Spot\", \"RL actual\", \"RL forecast\", \"RL difference\", \" - \"), rep(\" - \", 2)),\nerror.insample)\n# order models by rmse\nerror.order = error.insample[order(unlist(error.insample[, 5]), -unlist(error.insample[, 6]) ), ]\n# Table in TeX output\nstargazer(error.order, summary = FALSE, rownames = FALSE, type = \"text\")\n# Plot\n# plot of forecast curves with best forecast from VAR SC with DA spot\npar(mfrow = c(1,2),cex.lab = 1.2, cex.axis= 1.2, cex.main = 1.2 )\nfor(a in 2:3){\nfa = n + a\n# organzize the data\nplot.data = array(NA, dim = c(hour, (3 + lt)))\nplot.data[, c(1:3)] = cbind( c(t(vwap[fa, ])), c(t(spot[fa, ])), DSC_Forc.mat[a, ])\n# here can change the forecast model.\nfor (j in c(1:lt) ) {\nplot.data[, (j + 3)] = forc.spot.SC[, a, j]\n}\n# plot the data.\nplot(plot.data[, 1], type = \"l\", col = \"red\",\nlty = 6,\nylim = range(plot.data),\nmain = rownames(vwap)[fa],\nxaxt = \"n\",\ncex.axis = 1.5,\nlwd = 2)\nlines(plot.data[, 2], lwd = 2, col = \"darkgreen\")\nfor (j in 1:lt){\ncol.exp = ifelse(j == qt.5, \"black\", \"grey\")\nlines(plot.data[, (j + 3)], lwd = 2, col = col.exp)\naxis(1, c(1, 7, 13, 24), c(\"00:00\", \"12:00\", cex.axis = 1.2)\n}\ndev.off()\n", "output_sequence": "Forecast of daily VWAP curves with FASTEC VAR model. Generate table with error measures and plots of daily curves."}, {"input_sequence": "Author: Ramona Merkl ; import numpy as np\n'''\nfunctions to calculate parameters after liquidation\n# coins\ndef Ni(L):\nN = N0*(1-(LTVH/(2*LB)))**L\n\nif LTVH > 2*LB:\nN = 0\nreturn N\n# strike price\ndef Ki(L):\nN = Ni(L)\nK = (N0*K0)/((2**L)*N)\nreturn K\n# payoff\ndef pi(L, x):\nK = Ki(L)\nreturn N*(x - K)\n# price threshold\ndef aH(L):\n# smoot-fit function\ndef g(L, b):\na = aH(L-1)\npi_a = pi(L,a)\nexemplary parameters\ngamma = 0.1 # contract interest\nr = 0.05 # risk-free rate\ndelta = -0.0 # dividend; default: coins do not pay dividends\n#r_tilde = r - gamma\nsigma = 1.03 # volatility\n# paramteres for smooth-fit function\nmu = - sigma/2 - (gamma - r + delta)/sigma\nlambda1 = (- mu + np.sqrt(mu**2 - 2*(gamma-r)))/sigma\n#lambda1 = ((- r_tilde + sigma**2/2) + np.abs(r_tilde + sigma**2/2))/sigma**2\ncontract parameters\nLTVH = 0.85 # threshold LTV\nLB = 0.95 # liquidation bonus\nN0 = 3 # initial coins\na = q/(LTVH*N0) # liquidation threshold\nLTV0 = 0.74 # initial LTV (chosen by borrower)\nS0 = q/(N0*LTV0) # starting coin price S(0)\nK0 = LTV0*S0 # initial strike price\nprint(a, lambda1, mu)\n#b = np.linspace(a+1, 11000, 20000)\nL = 1 # number of liquidations\nb = np.linspace(aH(L-1)+1, 11000, 20000)\nprint(Ni(L-1), Ni(L))\nplt.plot(b, g(L, b))\nplt.axhline(y=0, color='r', linestyle='-')\nplt.xlabel('b')\n#plt.ylim(0,100)\nplot of smooth-fit function with zero dividend and no drift\nb = np.linspace(aH(L-1)+1, 20000,\nplt.plot(b, g(L, b)/nom)\n#plt.ylabel(r\"$f'(b)-N_{i-1}$\")\nplt.ylim(-0.5,1.5)\nplt.xlim(a-500,20000)\nplt.axvline(x=a, color='r', linestyle=':')\n#plt.savefig('g_fct_0.png', transparent=True)\nplt.tight_layout()\nalternative (but equivalent) way of defining smooth-fit function\ndef C1_C2(L, a, b):\npi_a = pi(L, a)\nz = np.sqrt(mu**2 - 2*(gamma - r))/sigma\nC = (b/a)**z - (a/b)**z\nA = b**(-z)/(C*a**(-mu/sigma))\nC1 = 1/C * pi_b*b**(mu/sigma)*a**(-z) - pi_a/C * (b**(-z))/(a**(-mu/sigma))\nreturn C1, C2\ndef f(L, a, b):\nC1, C2 = C1_C2(L, a, b)\nreturn C1*b**lambda1 + C2*b**lambda2\ndef f_prime(L, a, b):\n#L = 1\nplt.plot(b, f_prime(L, a, b)-Ni(L-1))\n#plt.axhline(y=0, color='r', linestyle='-')\nplt.ylabel(r\"$f'(b)-N_{i-1}$\")\nplt.ylim(-0.5,4)\nplt.axvline(x = a, color='r', linestyle=':')\nplt.axhline(y = 0, color='r')\nC1,C2 = C1_C2(L, a, 3500)\nprint(r'C1 =', C1)\n# root of f_prime\ngb_arr = f_prime(L,a,b)-Ni(L-1)\ntar = 0\nloss = (gb_arr - tar)**2\ntry for different possible drifts of the underlying process\ndrift = np.linspace(0,-0.1,6)\nfor d in drift:\nmu = - sigma/2 - (gamma - r + d)/sigma\nlambda1 = (- mu + np.sqrt(mu**2 - 2*(gamma-r)))/sigma\nplt.plot(b, g(L, b)/nom, label=-np.round(d,2))\nplt.axhline(y=0, color='r', linestyle='-')\nplt.xlabel('b')\n#plt.ylabel(r\"$f'(b)-N_{i-1}$\")\nplt.ylim(-0.2,0.5)\nplt.axvline(x=a, color='r', linestyle=':')\nplt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n#plt.savefig('g_fct_drift.png', transparent=True)\nb = np.linspace(aH(L-1)+1, 15000, 20000)\nfor d in drift:\nplt.plot(b, g(L, b),label=-np.round(d,2))\nplt.ylabel('g(b)')\n#plt.ylim(-0.1,0.2)\n", "output_sequence": "Solution to the attempt of finding an optimal exercise strategy for a CP2P contract based on Aave\u00b4s lending pool mechanism. Plotting the function shows, that there is no root and therefore no optimal exercise strategy, if continuous price paths are considered."}, {"input_sequence": "Author: Ramona Merkl ; import pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib.transforms import Bbox\nimport numpy as np\nimport squarify\nimport seaborn as sns\nborrow = pd.read_csv('AAVE_V1/Borrow.csv')\nborrow._timestamp = pd.to_datetime(borrow._timestamp)\nborrow = borrow.drop_duplicates()\ndeposit = pd.read_csv('AAVE_V1/Deposit.csv')\ndeposit._timestamp = pd.to_datetime(deposit._timestamp)\ndeposit = deposit.drop_duplicates()\nliquidation = pd.read_csv('AAVE_V1/LiquidationCall.csv')\nliquidation._timestamp = pd.to_datetime(liquidation._timestamp)\nliquidation = liquidation.drop_duplicates()\nredeem = pd.read_csv('AAVE_V1/RedeemUnderlying.csv')\nredeem._timestamp = pd.to_datetime(redeem._timestamp)\nredeem = redeem.drop_duplicates()\nrepay = pd.read_csv('AAVE_V1/Repay.csv')\nrepay._timestamp = pd.to_datetime(repay._timestamp)\nrepay = repay.drop_duplicates()\nprice = pd.read_csv('AAVE_V1/V1_price.csv')\nprice = price.drop_duplicates('blockNumber')\nrepay_liqu = repay[(repay._user != repay._repayer)]\nprint('There were ', len(repay_liqu), ' repayments executed by 3rd parties.\\n')\nfrom web3 import Web3\nendPoint = \"wss://speedy-nodes-nyc.moralis.io/d61daff0632b8f2bb3fcc91e/eth/mainnet/archive/ws\"\nw3 = Web3(Web3.WebsocketProvider(endPoint))\nbN = []\nbN += list(borrow.blockNumber)\nbN = list(set(bN))\nbN1 = int(np.min(bN))\nstart = pd.to_datetime(w3.eth.get_block(bN1).timestamp, unit='s').strftime('%Y-%m-%d')\nprint('We analyse the transaction data of AAVE v1 from block %s to %s.\\nThis is equivalent to time from %s to %s.'\n%(bN1,bN2, start, end),'\\n')\nprint('Number of borrowers: ', len(borrow._user.value_counts()))\nprint('Number of liquidators: ', len(liquidation._liquidator.value_counts()))\n'''\nstable interest rate: 1\ninterest = borrow._borrowRateMode.value_counts()\ninterest_share = interest[:]/interest[:].sum()\n# number of coins borrwed per user\npv_borrow = borrow.pivot_table(index = '_user', columns = 'reserveName', values = 'amountCleaned', aggfunc = 'count')\npv_borrow = pv_borrow/pv_borrow\npv_borrow.fillna(0)\nN_borrow = pv_borrow.sum(axis=1).sort_values(ascending=False)\n# number of coins deposited per user\npv_deposit = deposit.pivot_table(index = '_user', columns = 'reserveName', values = 'amountCleaned', aggfunc = 'count')\npv_deposit = pv_deposit/pv_deposit\npv_deposit.fillna(0)\nN_deposit = pv_deposit.sum(axis=1).sort_values(ascending=False)\nfig = plt.subplots(figsize=(15,5))\nplt.subplot(1, 2, 1)\nplt.title('Number of Borrowed Coins per User')\nsns.histplot(data = N_borrow, x = N_borrow[1:len(N_borrow)], stat='percent', discrete=True, color='grey')\nplt.subplot(1, 2, 2)\nplt.title('Number of Deposited Coins per User')\nsns.histplot(data = N_deposit, x = N_deposit[1:len(N_deposit)], stat='percent', discrete=True, color='grey')\nax = plt.gca()\nax.xaxis.get_major_locator().set_params(integer=True)\nplt.savefig('per_user_tx.png', transparent=True)\nsingle_borrow = N_borrow[N_borrow == 1]\nprint(len(single_borrow)/len(N_borrow)*100, '% of all users borrow only a single coin.')\nsingle_deposit = N_deposit[N_deposit == 1]\nprint('Average number of borrowed coins per user:')\nprint(np.mean(pv_borrow.sum(axis=1)))\nprint('Average number of deposited coins per user:')\npopular coins for borrowing\nn = 6\n# colormap for plots\ncmap = plt.get_cmap('viridis')\ncol = [cmap(i) for i in np.linspace(0, 1, n+1)]\n# coins by NUMBER\nloans_num = borrow.reserveName.value_counts()\nshares_num = loans_num[:]/loans_num[:].sum()\nloans_all = pd.concat([loans_num,shares_num], axis=1, keys=['Total', 'Percent'])\n# summarize small shares by 'other'\nloans_summary = loans_num[:n].copy()\nother_num = pd.Series(loans_num[n:].sum(), index=['Other'])\nloans_summary = pd.concat([loans_summary, other_num])\nshares_other = loans_summary[:]/loans_summary[:].sum()\nloans_summary = pd.concat([loans_summary,shares_other], axis=1, keys=['Total', 'Percent'])\n#coins by VALUE\nloans_val = borrow.groupby(['reserveName'])['amountUSDT'].sum().sort_values(ascending=False)\nshares_val = loans_val[:]/loans_val[:].sum()\nloans_all2 = pd.concat([loans_val,shares_val], axis=1, keys=['Total', 'Percent'])\nloans_summary2 = loans_val[:n].copy()\nother_val = pd.Series(loans_val[n:].sum(), index=['Other'])\nloans_summary2 = pd.concat([loans_summary2, other_val])\nshares_other = loans_summary2[:]/loans_summary2[:].sum()\nloans_summary2 = pd.concat([loans_summary2,shares_other], axis=1, keys=['Total', 'Percent'])\nprint('Loans by Number and Value in USDT:')\nprint(loans_summary)\n#plt.title('Borrowings by Count')\n#squarify.plot(loans_summary.Total, label=loans_summary.index, color=col, alpha=0.75)\n#plt.savefig('plots/popular_borrowings.png', transparent=True)\n#plt.close()\n#plt.title('Borrowings by Value in USDT')\n#squarify.plot(loans_summary2.Total, label=loans_summary2.index, color=col, alpha=0.75)\n#plt.savefig('plots/popular_borrowings_vol.png', transparent=True)\n#plot both\nplt.title('Borrowings by Count')\nsquarify.plot(loans_summary.Total, label=loans_summary.index, color=col, alpha=0.75)\nplt.title('Borrowings by Value in USDT')\nsquarify.plot(loans_summary2.Total, label=loans_summary2.index, color=col, alpha=0.75)\nfrequently liquidated\nliqu_num = liquidation.collateralName.value_counts()\nshares_num = liqu_num[:]/liqu_num[:].sum()\nliqu_all = pd.concat([liqu_num,shares_num], axis=1, keys=['Total', 'Percent'])\nliqu_summary = liqu_num[:n].copy()\nother_num = pd.Series(liqu_num[n:].sum(), index=['Other'])\nliqu_summary = pd.concat([liqu_summary, other_num])\nshares_other = liqu_summary[:]/liqu_summary[:].sum()\nliqu_summary = pd.concat([liqu_summary,shares_other], axis=1, keys=['Total', 'Percent'])\nliqu_val = liquidation.groupby(['collateralName'])['liquidatedCollateralAmountUSDT'].sum().sort_values(ascending=False)\nshares_val = liqu_val[:]/liqu_val[:].sum()\nliqu_all2 = pd.concat([liqu_val,shares_val], axis=1, keys=['Total', 'Percent'])\nliqu_summary2 = liqu_val[:n].copy()\nother_val = pd.Series(liqu_val[n:].sum(), index=['Other'])\nliqu_summary2 = pd.concat([liqu_summary2, other_val])\nshares_other = liqu_summary2[:]/liqu_summary2[:].sum()\nliqu_summary2 = pd.concat([liqu_summary2,shares_other], axis=1, keys=['Total', 'Percent'])\nprint('Liquidations by Number and Value in USDT:')\nprint(liqu_summary)\n#plt.title('Liquidations by Count')\n#squarify.plot(liqu_summary.Total, label=liqu_summary.index, color=col, alpha=0.75)\n#plt.savefig('plots/freq_liqu.png', transparent=True)\n#plt.title('Liquidations by Value in USDT')\n#squarify.plot(liqu_summary2.Total, label=liqu_summary2.index, color=col, alpha=0.75)\n#plt.savefig('plots/freq_liqu_vol.png', transparent=True)\nplt.title('Liquidations by Count')\nsquarify.plot(liqu_summary.Total, label=liqu_summary.index, color=col, alpha=0.75)\nplt.title('Liquidations by Value in USDT')\nsquarify.plot(liqu_summary2.Total, label=liqu_summary2.index, color=col, alpha=0.75)\n#plt.savefig('freq_liquidations.png', transparent=True)\nCoins most common as deposits in general\ndep_num = deposit.reserveName.value_counts()\nshares_num = dep_num[:]/dep_num[:].sum()\ndep_all = pd.concat([dep_num,shares_num], axis=1, keys=['Total', 'Percent'])\ndep_summary = dep_num[:n].copy()\nother_num = pd.Series(dep_num[n:].sum(), index=['Other'])\ndep_summary = pd.concat([dep_summary, other_num])\nshares_other = dep_summary[:]/dep_summary[:].sum()\ndep_summary = pd.concat([dep_summary,shares_other], axis=1, keys=['Total', 'Percent'])\ndep_val = deposit.groupby(['reserveName'])['amountUSDT'].sum().sort_values(ascending=False)\nshares_val = dep_val[:]/dep_val[:].sum()\ndep_all2 = pd.concat([dep_val,shares_val], axis=1, keys=['Total', 'Percent'])\ndep_summary2 = dep_val[:n].copy()\nother_val = pd.Series(dep_val[n:].sum(), index=['Other'])\ndep_summary2 = pd.concat([dep_summary2, other_val])\nshares_other = dep_summary2[:]/dep_summary2[:].sum()\ndep_summary2 = pd.concat([dep_summary2,shares_other], axis=1, keys=['Total', 'Percent'])\nprint('Deposits by Number and Value in USDT:')\nprint(dep_summary)\n#plt.title('Deposits by Value in USDT')\n#squarify.plot(dep_summary2.Total, label=dep_summary2.index, color=col, alpha=0.75)\n#plt.savefig('plots/dep_vol.png', transparent=True)\nplt.title('Deposits by Count')\nsquarify.plot(dep_summary.Total, label=dep_summary.index, color=col, alpha=0.75)\nplt.title('Deposits by Value in USDT')\nsquarify.plot(dep_summary2.Total, label=dep_summary2.index, color=col, alpha=0.75)\n#plt.savefig('popular_deposits.png', transparent=True)\nplot liquidations by value in USDT\nPT1 = pd.pivot_table(liquidation,\nindex='collateralName', columns= 'reserveName',\nvalues='liquidatedCollateralAmountUSDT',\naggfunc='sum').fillna(0)\ncollateralNames = list(PT1.index)\nreserveNames = list(PT1.columns)\nfig = plt.figure(figsize=(8,8))\nax = fig.add_subplot(111)\n#cax = ax.matshow(PT1.T, interpolation='nearest', cmap='seismic')\nfig.colorbar(cax)\nax.xaxis.set_major_locator(ticker.MultipleLocator(1))\nax.set_xticklabels(['']+collateralNames, rotation=90)\nax.yaxis.set_major_locator(ticker.MultipleLocator(1))\nax.set_yticklabels(['']+reserveNames)\nax.xaxis.set_label_position('top')\nax.set_xlabel('Collateral Type')\nax.set_ylabel('Debt Coins')\n#ax.set_title('Total Value in USDT')\nplot liquidations by total number\nPT2 = pd.pivot_table(liquidation,\nvalues='status',\ncollateralNames = list(PT2.index)\nreserveNames = list(PT2.columns)\n#cax = ax.matshow(PT2.T, interpolation='nearest', cmap='seismic')\n#ax.set_title('Total Number of Liquidations')\n#plt.savefig('plots/freq_coin_combinations.png', transparent=True)\npd.options.mode.chained_assignment = None\neth_liqu = liquidation[(liquidation.collateralName == 'ETH')]\nprice_eth = price[['ETH', 'USDC', 'USDT']]\nprice_eth['ETH_USDT'] = price_eth.ETH/price_eth.USDT\neth_liqu = price_eth.merge(eth_liqu, on=['blockNumber'], how='outer')\nfig, ax1 = plt.subplots(figsize=(16,9))\ncolor = 'darkblue'\nax1.set_xlabel('Block Number')\nax1.set_ylabel('ETH Value in USDT', color=color)\nax1.plot(eth_liqu.blockNumber, eth_liqu.ETH_USDT, color=color)\nax1.tick_params(axis='y', labelcolor=color)\nax2 = ax1.twinx()\ncolor = 'tab:red'\nax2.set_ylabel('Liquidation Size in USDT', color=color)\nax2.stem(eth_liqu.blockNumber, eth_liqu.liquidatedCollateralAmountUSDT, 'darkred', markerfmt = ' ', basefmt = ' ', use_line_collection=True)\nax2.axhline(y=0, color='black')\nax2.tick_params(axis='y', labelcolor=color)\n#plt.title('')\n#fig.tight_layout()\n#plt.savefig('.png', transparent=True)\n# Added block time\ntimeLabels = []\nfor item in ax1.get_xticklabels():\nbN = int(item.get_unitless_position()[0])\ntry:\nblock = w3.eth.get_block(bN)\ntime = pd.to_datetime(block.timestamp, unit='s')\ntimeLabels.append(time.strftime('%Y-%m-%d'))\nexcept:\n#fig, ax1 = plt.subplots(figsize=(12,8))\ncolor = 'navy'\nax1.set_xlabel('Date')\nax1.tick_params(axis = 'y', labelcolor = color)\nax2.stem(eth_liqu.blockNumber, eth_liqu.liquidatedCollateralAmountUSDT, 'tab:red', markerfmt = ' ', basefmt = ' ', use_line_collection=True)\nax2.axhline(y = 0, color = 'black')\nax2.tick_params(axis = 'y', labelcolor = color)\nax1.set_xticklabels(timeLabels)\n#plt.savefig('plots/liqu_ETH_date_large.png', transparent=True)\neth_dep = deposit[(deposit.reserveName == 'ETH')]\n#price_eth = price[['ETH', 'USDC', 'USDT']]\n#price_eth['ETH_USDT'] = price_eth.ETH/price_eth.USDT\neth_dep = price_eth.merge(eth_dep, on=['blockNumber'], how='outer')\n#fig, ax1 = plt.subplots(figsize=(16,9))\nax1.plot(eth_dep.blockNumber, eth_dep.ETH_USDT, color=color)\ncolor = 'darkgreen'\nax2.set_ylabel('ETH Deposits in USDT', color=color)\nax2.stem(eth_dep.blockNumber, eth_dep.amountUSDT, 'darkgreen', markerfmt = ' ', basefmt = ' ', use_line_collection=True)\n#plt.savefig('plots/dep_ETH_date_large.png', transparent=True)\nfig, ax1 = plt.subplots(figsize=(12,9))\nax1.stem(eth_dep.blockNumber, eth_dep.amountUSDT, 'darkgreen', markerfmt = ' ', basefmt = ' ', use_line_collection=True)\n", "output_sequence": "Empirical analysis of on-chain data from Aave v1."}, {"input_sequence": "Author: Lucas Valentin Umann ; import numpy as np\nimport statsmodels.tsa.arima_process as stats\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport umap\n%matplotlib inline\nimport random\nplt.rcParams['figure.figsize'] = (10.0,\ndef ARIMA(phi = np.array([0]), theta = np.array([0]), d = 0, t = 0, mu = 0, sigma = 1, n = 20, burn = 10):\n\"\"\" Simulate data from ARMA model (eq. 1.2.4):\nz_t = phi_1*z_{t-1} + ... + phi_p*z_{t-p} + a_t + theta_1*a_{t-1} + ... + theta_q*a_{t-q}\nwith d unit roots for ARIMA model.\nArguments:\nphi -- array of shape (p,) or (p, 1) containing phi_1, ... for AR model\nd -- number of unit roots for non-stationary time series\nt -- value deterministic linear trend\nmu -- mean value for normal distribution error term\nn -- length time series\nburn -- number of discarded values because series beginns without lagged terms\nReturn:\nx -- simulated ARMA process of shape (n, 1)\nReference:\nTime Series Analysis by Box et al.\n\"\"\"\n# add \"theta_0\" = 1 to theta\ntheta = np.append(1, theta)\n\n# set max lag length AR model\np = phi.shape[0]\n# set max lag length MA model\nq = theta.shape[0]\n# simulate n + q error terms\na = np.random.normal(mu, sigma, (n + max(p, q) + burn, 1))\n# create array for returned values\nx = np.zeros((n + max(p, q) + burn, 1))\n# initialize first time series value\nx[0] = a[0]\nfor i in range(1, x.shape[0]):\nAR = np.dot(phi[0 : min(i, p)], np.flip(x[i - min(i, p) : i], 0))\nx[i] = AR + MA + t\n# add unit roots\nif d != 0:\nARMA = x[-n: ]\nm = ARMA.shape[0]\nz = np.zeros((m + 1, 1)) # create temp array\nfor i in range(d):\nz[j + 1] = ARMA[j] + z[j]\nARMA = z[1: ]\n\n#Window width (Number of columns)\nw = 30\n#Stride\ns = 1\n#Start time\nt_0 = 0\n#Desired rows\nr = 700\n#Length of matrix\nphi = np.array([0.9]) # AR part using 1 lag\nn = 1000 # number of simulated values\n# to get comparable results\n\nseason = np.zeros(shape = [n,1])\nfor i in np.arange(n):\nseason[i] = math.sin(i/20)*0.15\nfor i in range(r):\nstarttime = t_0 + i * s\nendtime = starttime + w\nZ[i,:] = np.ravel(x[starttime:endtime])\nreducer = umap.UMAP(random_state=42)\nplt.scatter(\nembedding[:, 0],\nplt.plot(x)\nmean = 0\nstd = 1\nnum_samples = 1000\ndata_white = pd.DataFrame(data = Z)\nembedding = reducer.fit_transform(data_white)\n", "output_sequence": "Simulates time series data (AR1, sine curve, white noise) and applies the UMAP VizTech to reflect local and global structure of the data. "}, {"input_sequence": "Author: Thomas Haslwanter ; '''Logistic Regression\nA logistic regression is an example of a \"Generalized Linear Model (GLM)\".\n\nThe input values are the recorded O-ring data from the space shuttle launches before 1986,\nand the fit indicates the likelihood of failure for an O-ring.\nTaken from http://www.brightstat.com/index.php?option=com_content&task=view&id=41&Itemid=1&limit=1&limitstart=2\n'''\n# Copyright(c) 2020, Thomas Haslwanter. All rights reserved, under the CC BY-SA 4.0 International License\n# Import standard packages\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport pandas as pd\nimport seaborn as sns\n# additional packages\nimport sys\nsys.path.append(os.path.join('..', '..', 'Utilities'))\ntry:\n# Import formatting commands if directory \"Utilities\" is available\nfrom ISP_mystyle import setFonts, showData\n\nexcept ImportError:\n# Ensure correct performance otherwise\ndef setFonts(*options):\nreturn\ndef showData(*options):\nplt.show()\nfrom statsmodels.formula.api import glm\nfrom statsmodels.genmod.families import Binomial\nsns.set_context('poster')\ndef getData():\n'''Get the data '''\ninFile = 'challenger_data.csv'\ndata = np.genfromtxt(inFile, skip_header=1, usecols=[1, 2],\nmissing_values='NA', delimiter=',')\n# Eliminate NaNs\ndata = data[~np.isnan(data[:, 1])]\nreturn data\ndef prepareForFit(inData):\n''' Make the temperature-values unique, and count the number of failures and successes.\nReturns a DataFrame'''\n# Create a dataframe, with suitable columns for the fit\ndf = pd.DataFrame()\ndf['temp'] = np.unique(inData[:,0])\ndf['failed'] = 0\ndf.index = df.temp.values\n# Count the number of starts and failures\nfor ii in range(inData.shape[0]):\ncurTemp = inData[ii,0]\ndf.loc[curTemp,'total'] += 1\nif curVal == 1:\ndf.loc[curTemp, 'failed'] += 1\nelse:\ndf.loc[curTemp, 'ok'] += 1\nreturn df\ndef logistic(x, beta, alpha=0):\n''' Logistic Function '''\nreturn 1.0 / (1.0 + np.exp(np.dot(beta, x) + alpha))\ndef showResults(challenger_data, model):\n''' Show the original data, and the resulting logit-fit'''\ntemperature = challenger_data[:,0]\n# First plot the original data\nplt.figure()\nsetFonts()\nsns.set_style('darkgrid')\nnp.set_printoptions(precision=3, suppress=True)\nplt.scatter(temperature, failures, s=200, color=\"k\", alpha=0.5)\nplt.yticks([0, 1])\nplt.ylabel(\"Damage Incident?\")\nplt.xlabel(\"Outside Temperature [F]\")\nplt.title(\"Defects of the Space Shuttle O-Rings vs temperature\")\nplt.tight_layout\n# Plot the fit\nx = np.arange(50, 85)\nalpha = model.params[0]\ny = logistic(x, beta, alpha)\nplt.plot(x,y,'r')\nplt.xlim([50, 85])\noutFile = 'ChallengerPlain.png'\nshowData(outFile)\nif __name__ == '__main__':\ninData = getData()\ndfFit = prepareForFit(inData)\n# fit the model\n# --- >>> START stats <<< ---\nmodel = glm('ok + failed ~ temp', data=dfFit, family=Binomial()).fit()\n# --- >>> STOP stats <<< ---\nprint(model.summary())\nshowResults(inData, model)\n", "output_sequence": "Logistic Regression A logistic regression is an example of a <Generalized Linear Model (GLM)>.\nThe input values are the recorded O-ring data from the space shuttle launches before 1986, and the fit indicates the likelihood of failure for an O-ring.\nTaken from http://www.brightstat.com/index.php?option=com_content&task=view&id=41&Itemid=1&limit=1&limitstart=2"}, {"input_sequence": "Author: Thomas Haslwanter ; ''' Practical demonstration of the central limit theorem, based on the uniform distribution '''\n\n# Copyright(c) 2015, Thomas Haslwanter. All rights reserved, under the CC BY-SA 4.0 International License\n# Import standard packages\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\n# additional packages\nimport sys\nsys.path.append(os.path.join('..', '..', 'Utilities'))\ntry:\n# Import formatting commands if directory \"Utilities\" is available\nfrom ISP_mystyle import setFonts, showData\n\nexcept ImportError:\n# Ensure correct performance otherwise\ndef setFonts(*options):\nreturn\ndef showData(*options):\nplt.show()\n# Formatting options\nsns.set(context='poster', style='ticks', palette='muted')\n# Input data\nndata = 100000\nnbins = 50\ndef showAsHistogram(axis, data, title):\n'''Subroutine showing a histogram and formatting it'''\naxis.hist( data, bins=nbins)\naxis.set_xticks([0, 0.5, 1])\naxis.set_title(title)\ndef main():\n'''Demonstrate central limit theorem.'''\nsetFonts(24)\n# Generate data\ndata = np.random.random(ndata)\n# Show three histograms, side-by-side\nfig, axs = plt.subplots(1,3)\nshowAsHistogram(axs[0], data, 'Random data')\nshowAsHistogram(axs[1], np.mean(data.reshape((ndata//2, 2 )), axis=1), 'Average over 2')\n# Format them and show them\naxs[0].set_ylabel('Counts')\nplt.tight_layout()\nshowData('CentralLimitTheorem.png')\nif __name__ == '__main__':\nmain()\n", "output_sequence": "Practical demonstration of the central limit theorem, based on the uniform distribution"}, {"input_sequence": "Author: Thomas Haslwanter ; '''Simple linear models.\n- \"model_formulas\" is based on examples in Kaplan's book \"Statistical Modeling\".\n- \"polynomial_regression\" shows how to work with simple design matrices, like MATLAB's \"regress\" command.\n'''\n\n# Copyright(c) 2015, Thomas Haslwanter. All rights reserved, under the CC BY-SA 4.0 International License\n# Import standard packages\nimport numpy as np\nimport pandas as pd\n# additional packages\nfrom statsmodels.formula.api import ols\nimport statsmodels.regression.linear_model as sm\nfrom statsmodels.stats.anova import anova_lm\ndef model_formulas():\n''' Define models through formulas '''\n\n# Get the data:\n# Development of world record times for the 100m Freestyle, for men and women.\ndata = pd.read_csv('swim100m.csv')\n# Different models\nmodel1 = ols(\"time ~ sex\", data).fit() # one factor\nmodel3 = ols(\"time ~ sex * year\", data).fit() # two factors with interaction\n# Model information\nprint((model1.summary()))\n# ANOVAs\nprint('----------------- Results ANOVAs: Model 1 -----------------------')\nprint((anova_lm(model1)))\nprint('--------------------- Model 2 -----------------------------------')\nprint((anova_lm(model2)))\nprint('--------------------- Model 3 -----------------------------------')\nmodel3Results = anova_lm(model3)\nprint(model3Results)\n# Just to check the correct run\nreturn model3Results['F'][0] # should be 156.1407931415788\ndef polynomial_regression():\n''' Define the model directly through the design matrix.\nSimilar to MATLAB's \"regress\" command.\n'''\n# Generate the data: a noisy second order polynomial\n# To get reproducable values, I provide a seed value\nnp.random.seed(987654321)\nt = np.arange(0,10,0.1)\ny = 4 + 3*t + 2*t**2 + 5*np.random.randn(len(t))\n# --- >>> START stats <<< ---\n# Make the fit. Note that this is another \"OLS\" than the one in \"model_formulas\",\n# as it works directly with the design matrix!\nM = np.column_stack((np.ones(len(t)), t, t**2))\nres = sm.OLS(y, M).fit()\n# --- >>> STOP stats <<< ---\n\n# Display the results\nprint('Summary:')\nprint(('The fit parameters are: {0}'.format(str(res.params))))\nprint('The confidence intervals are:')\nprint((res.conf_int()))\nreturn res.params # should be [ 4.74244177, 2.60675788, 2.03793634]\nif __name__ == '__main__':\nmodel_formulas()\npolynomial_regression()\n", "output_sequence": "Simple linear models. - <model_formulas> is based on examples in Kaplan's book <Statistical Modeling>. - <polynomial_regression> shows how to work with simple design matrices, like MATLAB's <regress> command."}, {"input_sequence": "Author: Jerome Bau ; import logging\nimport os\nfrom gensim import corpora, utils\nfrom gensim.models.wrappers.dtmmodel import DtmModel\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.legend_handler import HandlerLine2D\nfrom gensimLDA import * # dependency on gensimLDA.py\n\"\"\"\nProcesses saved news articles into a Dynamic topic model\nPlots chosen index terms and their probability distribution in a given topic\nThe input needs to be as follows:\n*.txt files, encoded in the standard ACII encoding format that have been cleaned from any punctuation marks and\nany other non text elements.\nI did this using a simple bash script, included in this repository\nsource_folder = home/example # folder that contains all the news articles\ndtm_compiled_path = home/example/dtm # direct path to the C-Code compiled form David Blei's git\n# (https://github.com/blei-lab/dtm)\ncustom_stop_words = ['zeit', 'is', 'a', 'the'] # should be adapted to the source of articles and content\ng = gensimLDA()\ng.change_operating_language('de')\ng.generate_stop_words(custom=custom_stop_words)\n# I named articles per {$month}+{article number}, such as 304 for the 5th selected article in March\narticles = [str(100+i)+'c' for i in range(13)] + [str(200+i)+'c' for i in range(11)]\\\n+ [str(500+i)+'c' for i in range(15)]\ng.define_filenames_and_foldername_source(articles, source_folder)\ng.load_raw_corpus()\ntime_slices = [13, 11, 15] # number of documents for each month\nclass DTMcorpus(corpora.textcorpus.TextCorpus):\ndef get_texts(self):\nreturn self.input\ndef __len__(self):\nreturn len(self.input)\ncorpus = DTMcorpus(g.corpus_raw)\nmodel = DtmModel(dtm_compiled_path, corpus, time_slices, num_topics=2, id2word=corpus.dictionary, initialize_lda=True)\n# collect probabilities for chosen keyterms\nwords_of_interest = ['Tuerkei', 'Fluechtlinge', 'Oesterreich']\ntopic_choice = 0\nresults = {}\nfor w in words_of_interest:\nresults[w] = []\nfor i in range(5):\nfor (p,w) in model.show_topic(topic_choice, i):\nif w in int_words:\nresults[w].append(p)\n# plot\nlabels = [\"January '16\", \"February '16\", \"March '16\", \"April '16'\", \"Mai '16\"]\nline1, = plt.plot(results['Fluechtlinge'], label='\"Refugee\"', marker='o')\nplt.xticks([0, 1, 2, 3, 4], labels, rotation=70)\nplt.ylabel='Word probability'\nplt.xlabel='Publishing month of article'\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)}, bbox_to_anchor=(1.3, 1))\nplt.show()\n", "output_sequence": "Contains applications based on Python gensim to train and use Latent Dirichlet Allocation (LDA) and Dynamic Topic Models (DTM)"}, {"input_sequence": "Author: Jerome Bau ; from gensimLDA import *\n\"\"\"\nMatch each article with their closest (topic-distribution-wise) article\nArticles need to be individual *.txt files, in standard ASCII encoding standard, cleaned of any punctuation marks and non-textual elements\nI cleaned articles with a simple bash script, included in this repository\nAll articles are searched for by default in a folder called query/articles\nReturns a dictionary with the corresponding match\nl = gensimLDA()\nl.lda = g.lda\nl.corpus_dict = g.corpus_dict\nl.generate_stop_words()\n# saved all query articles in a folder called query\nall_query_articles = {0: 'article1-1', 1: 'article1-2', 2: 'article2-1', 3: 'article2-2',\n12: 'article7-1', 13: 'article7-2'}\n# filter meta_topics and stop word-like topics\nfiltered_topics = [13]\nfor article in all_query_articles.values():\nl.define_filename_and_foldername_query(foldername='query/articles', filename=article)\nl.create_new_query_from_raw()\nl.transform_query_to_dict()\nl.search_query()\nl.query_top_topics(5)\ndef dist(a,b):\n\"\"\" distance function - here: cosine similarity\"\"\"\nreturn spatial.distance.cosine(non_sparse[a], non_sparse[b])\ndef transform_to_non_sparse_vector():\n\"\"\"\"create non sparse vectors, same dimensions as the topic space\"\"\"\nnon_sparse = {}\nfor art in l.query_results:\nvec = [0 for i in range(100)]\nfor (k,v) in l.query_results[art]:\nif k not in filtered_topics:\nvec[k] = v\nnon_sparse[art] = vec\nreturn non_sparse\nnon_sparse = transform_to_non_sparse_vector()\ndef create_matches():\n\"\"\" Creates a matrix that matches documents based on min distance\"\"\"\nmatrix = np.zeros((len(non_sparse),len(non_sparse)))\nfor i in range(len(non_sparse)):\nif i==j:\nmatrix[i,j] = 1.0\nelse:\nmatrix[i,j] = dist(all_query_articles[i],\nmatrix[matrix==0]=1\nmatch = {}\nfor i in all_query_articles:\nmatch[all_query_articles[i]] = all_query_articles[matrix[i].argmin()]\nreturn match\nmatch = create_matches()\n", "output_sequence": "Contains applications based on Python gensim to train and use Latent Dirichlet Allocation (LDA) and Dynamic Topic Models (DTM)"}, {"input_sequence": "Author: Jerome Bau ; from stop_words import get_stop_words\nfrom gensim import corpora, models, similarities\nimport pandas as pd\n\"\"\"\nExpected input (as clearified in the function descriptions):\n- txt files cleaned of any punctuation marks (for example using the bash script, located in this repository)\n- any saved mm corpus and dictionary produced by other gensim scripts\n- including the output produced by the wiki make script from gensim (https://radimrehurek.com/gensim/wiki.html)\nclass gensimLDA(object):\n\"\"\" Collection of functions that work on the Python library gensim to work queries against a LDA model\nbased on a document corpus\"\"\"\ndef __init__(self):\n# Preprocessing\nself.operating_language = 'en'\nself.source_filenames = None\n# Corpora\nself.corpus_raw= None\n# Model\nself.lda = None\n# Query\nself.query_list = {}\nself.query_results_topics_probs = {}\n# Paths\ndef define_filenames_and_foldername_source(self, filenames, foldername):\n\"\"\"\nDefine names of the documents that are meant to be included in the corpus\nTRAINING DATA\nself.source_filenames = filenames\ndef define_filenames_and_foldername_save(self, prefix, foldername):\nDefine where documents shall be saved and using which prefix\nSAVED DATA\nself.save_prefix = prefix\nself.save_foldername = foldername\ndef define_filename_and_foldername_query(self, filename, foldername):\nself.query_filename = filename\n# Preprocessing\ndef change_operating_language(self, lang):\nChange the operating language (for stopwords list)\nDefault: English ('en')\nself.operating_language = lang\n# Corpus Creation\ndef generate_stop_words(self, numbers='include', custom=[]):\nGenerate a list of stop words\nPer default: Include single numbers and no custom words\n:param: custom - include custom words (lower case)\nself.stoplist = set(get_stop_words(self.operating_language))\nif numbers == 'include':\nself.stoplist.update(range(100)+ [10**i for i in range(5)])\nself.stoplist.update(custom)\ndef load_raw_corpus(self):\nLoad from txt file and convert into list\nVerify: Document must be cleaned of any punctuation marks (,.!'\";: , etc)\nif len(self.stoplist) == 0:\nprint('Empty stop word list')\ncorpus = []\nfor filename in self.source_filenames:\ntemp = []\nwith open(self.source_foldername+'/'+filename + '.txt', 'r') as f:\nfor line in f:\nfor word in line.split():\nif word.lower() not in self.stoplist:\ntemp.append(word.lower())\ncorpus.append(temp)\nself.corpus_raw = corpus\nprint(str(len(self.corpus_raw)) + ' loaded into corpus')\ndef create_frequency_count(self):\n\"\"\"Create a dictionary that contains all word counts\"\"\"\ndict = {}\nfor document in self.corpus_raw:\nfor token in document:\ntry:\ndict[token] += 1\nexcept KeyError:\ndict[token] = 1\nself.token_count = dict\ndef remove_once(self):\n\"\"\"Remove all words that occur only once over the whole corpus\"\"\"\nself.corpus = [[token for token in document if self.token_count[token] > 1] for document in self.corpus_raw]\ndef save_dict(self):\n\"\"\" Save the corpus as a gensim dictionary for future use\"\"\"\ndictionary = corpora.Dictionary(self.corpus)\ndictionary.save(self.save_foldername+'/'+self.save_prefix+'.dict')\nself.corpus_dict = dictionary\ndef save_mm(self):\n\"\"\" Saves the coprus in gensim's Mm Corpus format\"\"\"\ncorpus = [self.corpus_dict.doc2bow(document) for document in self.corpus]\ncorpora.MmCorpus.serialize(self.save_foldername+'/'+self.save_prefix+'.mm', corpus)\nself.corpus_mm = corpus\ndef load_corpus_mm(self):\n\"\"\" Load a corpus from memory - Format: *.mm\"\"\"\ncorpus = corpora.MmCorpus(self.save_foldername + '/' + self.save_prefix + '_bow.mm')\ndef load_corpus_dict(self):\n\"\"\" Load a corpus from memory - Format: *.dict\"\"\"\ncorpus = corpora.Dictionary.load(self.save_foldername + '/' + self.save_prefix + '.dict')\nself.corpus_dict = corpus\ndef load_corpus_dict_from_text(self):\n\"\"\" Load coprus from a gensim created dictionary .txt file\"\"\"\nid2word = corpora.Dictionary.load_from_text(self.save_foldername + '/' +\nself.save_prefix + '_wordids.txt')\nself.corpus_dict = id2word\ndef train_lda_model(self, topics=100, chunksize=1000, passes = 3):\n\"\"\" Runs the LDA model with k topics\"\"\"\nself.lda = models.LdaModel(self.corpus_mm, id2word=self.corpus_dict, num_topics=topics, update_every=1,\nchunksize=chunksize, passes=passes)\nprint('LDA model trained')\ndef save_lda_model(self):\n\"\"\"Save the LDA model\"\"\"\nself.lda.save(self.save_foldername+'/'+self.save_prefix)\ndef create_single_line_query(self, query):\n\"\"\" Add a new query - one string\"\"\"\ntemp = []\nfor word in query.split():\nif word.lower() not in self.stoplist:\ntemp.append(word.lower())\nself.query_list = temp\ndef create_new_query_from_raw(self):\n\"\"\" Add a new query from a txt file - without any punctuation marks\"\"\"\nif type(self.stoplist)!= set:\nprint('Stop word list not yet generated')\nwith open(self.query_foldername + '/' + self.query_filename + '.txt', 'r') as f:\nfor line in f:\nfor word in line.split():\nif word.lower() not in self.stoplist:\ntemp.append(word.lower())\nself.query_list[self.query_filename] = temp\ndef transform_query_to_dict(self):\n\"\"\"Take saved query in list form and transforms it to a bow vector\"\"\"\ntry:\nself.query_dict[self.query_filename] = self.corpus_dict.doc2bow(self.query_list[self.query_filename])\nexcept AttributeError:\nprint('No corpus dict loaded!')\ndef search_query(self):\n\"\"\"Search the query against the LDA model\"\"\"\nself.query_results[self.query_filename] = self.lda[self.query_dict[self.query_filename]]\nreturn self.query_results\ndef query_top_topics(self, topics):\n\"\"\"Returns the k most matching topics\"\"\"\nfor i in self.query_results[self.query_filename]:\ndict[i[0]] = i[1]\nself.query_results_topics_probs[self.query_filename] = pd.Series(dict).sort_values(ascending=False).iloc[:topics].to_dict()\nself.query_results_topics[self.query_filename] = self.query_results_topics_probs[self.query_filename].keys()\n", "output_sequence": "Contains applications based on Python gensim to train and use Latent Dirichlet Allocation (LDA) and Dynamic Topic Models (DTM)"}, {"input_sequence": "Author: Georg Keilbar ; # clear all variables\nrm(list = ls(all = TRUE))\ngraphics.off()\n#setwd(\"\")\n# read data\ndata = read.csv(file = \"data.csv\")\ndate = as.Date(data[,1])\nx0 = data[,-1]\nVaR = as.matrix(read.csv(file = \"VaR.csv\"))\n# plot returns, VaR and CoVaR for each bank\npar(mfrow = c(3, 3), mar = c(2.5, 2, 3.5, 1))\nfor (j in 1 : ncol(x0)) {\nplot(x0[, j] ~ date, type = \"p\",\nylim = c(-0.6,0.35) , #c(min(x0[, j], CoVaR[, j]) * 1.1, max(x0[, j]) * 1.1)\nylab = \"\", pch=\".\", cex = 3, main = colnames(x0)[j], cex.main = 2)\nlines(VaR[, j] ~ date, col = \"blue\", lwd = 2)\n}\n", "output_sequence": "Plots the returns, the VaR and the CoVaR of eight global systemically important banks from the US."}, {"input_sequence": "Author: Yafei Xu ; ############################# computation of VaR using HAC-Gumbel copula\nrm(list = ls(all = TRUE))\ngraphics.off()\n# please set working directory setwd('C:/...') #\n# linux/mac os setwd('/Users/...') # windows\nlibraries = c(\"HAC\", \"CDVine\", \"fGarch\", \"foreach\", \"doParallel\", \"copula\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n############################# set up\neps1 = read.csv(\"eps1.csv\")\nsp.126 = read.csv(\"sp126.csv\", header = T)[, -c(1, 2)]\np1 = sp.126[-1, ]\np2 = sp.126[-length(sp.126[, 1]), ]\nl.r = 100 * log(p1/p2)\nr = l.r\nr1 = r[, -c(1)]\ndims = 5\nattach(r2)\nr2 = data.frame(cbind(AVB, EQR, TXN, ADI, LLY))\nM = 1000\nVaR = matrix(NA, backtestNr, 4)\nslidingWindowLength = 300\neps = matrix(NA, backtestNr, dim(r2)[2])\ncolnames(eps) = colnames(r2)\neps1 = data.frame()\ndat = eps1\nparaMat = list()\npara = matrix(NA, dims, 4)\nsigma = matrix(NA, backtestNr, dims)\n############################# estimation GARCH parameter\ncl = makeCluster(37)\nregisterDoParallel(cl)\ngetDoParWorkers()\ndat = r2\ndatMat = list()\nfor (i in 1:backtestNr) {\ndatMat[[i]] = dat[c(i:(i + (slidingWindowLength - 1))), 1:dims]\n}\nlengthPara = 4\nlengthEps1 = slidingWindowLength\nobjFun = function(dMat) {\nparaComb = list()\nlibrary(fGarch)\nfor (i in 1:dims) {\nfit = garchFit(~garch(1, 1), data = dMat[[i]], trace = F)\neps1.loop = fit@residuals/fit@sigma.t\npara.loop = fit@fit$coef\nlengthPara = length(para.loop)\nparaComb[[i]] = c(para.loop, sigma.loop, eps1.loop)\n}\nreturn(paraComb)\nresultD = foreach(dMat = datMat) %dopar% objFun(dMat)\nstopCluster(cl)\n############################# estimation copula parameter\ntotalLength = lengthPara + lengthEps1 + lengthSigma\nepsComb = list()\nk = pobs(as.data.frame(resultD[[i]]))\nepsComb[[i]] = k[-(1:(lengthPara + lengthEps1)), ]\n\ndatMatEps = epsComb\nlibrary(HAC)\ncolnames(dMat) = c(\"AVB\", \"EQR\", \"TXN\", \"ADI\", \"LLY\")\nhead(dMat)\ntree = list(list(list(\"TXN\", \"ADI\", 2.35), list(\"AVB\", \"EQR\", 3.25),\n1.45), \"LLY\", 1.44)\nmodel = hac(type = 1, tree = tree)\nresult = estimate.copula(dMat, hac = model, margins = \"norm\")\nresult\nplot(result, circles = 0.3, index = TRUE, l = 1.7)\nparaHAC = get.params(result)\nparaHAC\nreturn(paraHAC)\nptm = proc.time()\ncl = makeCluster(37)\nregisterDoParallel(cl)\nresultDcopPara = foreach(dMat = datMatEps, .combine = \"rbind\") %dopar%\nobjFun(dMat)\n############################# VaR computation\nspread.ra.0 = sp.126\nattach(spread.ra.0)\nspread.real = data.frame(cbind(AVB, EQR, TXN, ADI, LLY))\nS = rowSums(spread.real)\nS1 = S[-1]\nS2 = S[-length(S)]\nS3 = S2 - S1\nL.real = S3[301:(300 + backtestNr)]\nlengthPara = 4\nlengthEps1 = slidingWindowLength\npara.vec = resultDcopPara\nVaR = matrix(NA, backtestNr, 4)\ndatMatIndex = c(1:backtestNr)\nobjFunVaR = function(dMat) {\ni = dMat\npara.EmployedHAC = para.vec[i, ]\ntree = list(list(list(\"AVB\", \"EQR\", para.EmployedHAC[4]), list(\"TXN\",\n\"ADI\", para.EmployedHAC[3]), \"LLY\", para.EmployedHAC[1])\nmodel = hac(type = 1, tree = tree)\nu = rHAC(M, model)\nhead(u)\nk = as.data.frame(resultD[[i]])\nk1 = k[c(1:lengthPara, lengthPara + lengthEps1, totalLength), ]\nh = sqrt(k1[2, ] + k1[3, ] * k1[lengthPara + 1, ]^2 * k1[lengthPara +\nk2 = matrix(NA, M, dims)\nk3[1, ] = as.matrix(k1[lengthPara + 1, ])\nsig.t = k3[rep(1, M), ]\nR = mu.t + sig.t * u\nspread.real.loop = read.csv(\"sp126.csv\")[, -c(1:2)]\nattach(spread.real.loop)\nspread = data.frame(cbind(AVB, EQR, TXN, ADI, LLY))\ns1 = spread[i + (slidingWindowLength), ]\nst = data.frame(matrix(NA, M, dims))\nst[1, ] = s1\nst = st[rep(1, M), ]\nL.sim = matrix(NA, M, 1)\nL.sim = rowSums(st * (exp(0.01 * R) - 1))\nVaRt1005 = quantile(L.sim, 0.05)\nVaRt = c(VaRt1005, VaRt10001)\nhead(VaRt)\nreturn(VaRt)\ncl = makeCluster(37)\nregisterDoParallel(cl)\nresultVaR = foreach(dMat = datMatIndex, .combine = \"rbind\") %dopar% objFunVaR(dMat)\nhead(resultVaR)\nVaR = resultVaR\nresultComb = data.frame(L.real, VaR)\n############################# exceeding ratio\nExceeding_Ratio = numeric(4)\nfor (alpha in 2:5) {\nnullVector = rep(0, backtestNr)\nfor (i in 1:length(resultComb[, alpha])) {\nif (resultComb[, 1][i] < resultComb[, alpha][i]) {\nnullVector[i] = 1\n} else {\nnullVector[i] = 0\n}\nExceeding_Ratio[alpha] = sum(nullVector)/backtestNr\nExceeding_Ratio\n############################# plot of VaR and quantile = 0.001\nalpha = 5\nptForPlot = c(min(resultComb[, 1]), min(resultComb[, 2]), min(resultComb[,\n3]), min(resultComb[, 4]), min(resultComb[, 5]))\nlowPt = min(ptForPlot)\nupPt = max(resultComb[, 1])\nPortfolio_Value = seq(lowPt, upPt, length.out = length(resultComb[, 1]))\n1])))\nplot(Time_Index,\nPortfolio_Value,\ncol = \"white\",\npch = 19,\nxlab = \"Time Index\",\nylab = \"Profit and Loss of Portfolio\")\nlines(resultComb[, alpha], col = \"gray\", lwd = 6)\nfor (i in 1:length(resultComb[, alpha])) {\nif (resultComb[, 1][i] < resultComb[, alpha][i]) {\npoints(i, lowPt + 1, col = \"black\", pch = 17, cex = 1.5)\npoints(i, resultComb[, 1][i],\ncol = \"black\",\n} else {\npch = 19,\n", "output_sequence": "Shows VaRs of HAC-Gumbel copula model introduced in the paper. The alphas are chosen eqaul to .001."}, {"input_sequence": "Author: Lenka Zbo\u0148\u00e1kov\u00e1 ; # Clear all variables\nrm(list = ls(all = TRUE))\ngraphics.off()\n# Set working directory\n# setwd(\"\")\n# Install and load packages\nlibraries = c(\"quantmod\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)} )\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# Read the file with time series of average lambda from FRM\ntmpdata = read.csv(\"lambda_mean_206vars_2016-07-15.csv\", sep=\",\")\n# Create vector of (normalized) lambda values and dates\nlambda = tmpdata[, 2]\nlambda.norm = (lambda - min(lambda))/(max(lambda) - min(lambda))\ndates = as.Date(tmpdata[, 1], \"%Y-%m-%d\")\n# Download daily VIX values from Yahoo Finance and normalize\nstart = dates[1]\nend = dates[length(lambda)]\nvix = as.vector(getSymbols(\"^VIX\", src = \"yahoo\", from = start, to = end,\nauto.assign = FALSE)[, 6])\nvix.norm = (vix - min(vix))/(max(vix) - min(vix))\n# Plot normalized time series of FRM lambda and VIX\npar(mfrow = c(1,1))\npar(mar = c(5, 6, 1, 1))\nplot(vix.norm, type = \"l\", col = \"darkblue\", axes = FALSE,\nxlab = \"Year\", frame = TRUE, cex.main = 1.5,\nylab = expression(paste(lambda, \" vs. VIX\")), cex.lab = 2)\nat.tmp = c(grep(\"2008\", dates)[1], grep(\"2009\", dates)[1], grep(\"2010\", dates)[1],\naxis(1, cex.axis = 1.5, labels = c(2008:2016), at = at.tmp)\naxis(2, cex.axis = 1.5)\nlines(lambda.norm, col = \"red3\")\n", "output_sequence": "Plots time series of average lambda taken from FinancialRiskMeter together with the implied volatility index reported by the Chicago Board Options Exchange. Daily observations are collected from 6 July 2007 to 14 July 2016 and normalized to interval (0,1)."}, {"input_sequence": "Author: Marco Linton ; options(stringsAsFactors = FALSE)\nsetwd(\"\")\ndata.df = read.csv(\"term_evolution.csv\")\ndata.df$date = sapply(strsplit(data.df$date, \" \"), \"[\", 1)\ndata.df$date = as.Date(data.df$date, \"%Y-%m-%d\")\ndev.new(width = 10, height = 6)\npar(mar = c(3, 4.1, 3, 2.1), mfrow = c(2, 2))\n# CPU\nplot(data.df$cpu,\nmain = \"CPU\",\nlwd = 2,\ncol = \"black\",\nbty = 'l',\nxaxt = \"n\",\nxlim = c(min(data.df$date),\nylim = c(0, max(data.df$cpu)))\nlines(data.df$date, data.df$cpu, lwd = 1.5)\nbox(lwd = 1.5, bty = 'l', col = \"gray22\")\nmtext(\"p(w = cpu | k = 50)\",\nside = 2,\ncol = \"gray22\",\nlas = 3)\naxis(side = 2,\nround(data.df$cpu, 1),\ntick = FALSE,\ncex.lab = 1,\nline = -0.8,\ncol.axis = \"gray22\",\nlas = 1)\naxis(side = 1,\ndata.df$date, format(data.df$date, \" %m/%Y \"),\ncex.lab = 0.1,\n# GPU\nplot(data.df$gpu,\nmain = \"GPU\",\nylim = c(0, max(data.df$gpu)))\nlines(data.df$date, data.df$gpu, lwd = 1.5)\nmtext(\"p(w = gpu | k = 50)\",\nround(data.df$gpu, 2),\ndata.df$date, format(data.df$date, \" %m/%Y \"),\n# asics\nplot(data.df$asics,\nmain = \"ASICS\",\nylim = c(0, max(data.df$asics)))\nlines(data.df$date, data.df$asics, lwd = 1.5)\nmtext(\"p(w = asics | k = 50)\",\nround(data.df$asics, 2),\n# adminer\nplot(data.df$aminer,\nmain = \"Antminer\",\nylim = c(0, max(data.df$aminer)))\nlines(data.df$date, data.df$aminer, lwd = 1.5)\nmtext(\"p(w = antminer | k = 50)\",\nround(data.df$aminer, 3),\n", "output_sequence": "Plots word evolution for different cryptocurrency mining technologies."}, {"input_sequence": "Author: Yafei Xu ; ############################# computation of VaR using Student-t copula\nrm(list = ls(all = TRUE))\ngraphics.off()\n# please set working directory setwd('C:/...') #\n# linux/mac os setwd('/Users/...') # windows\nlibraries = c(\"fGarch\", \"foreach\", \"doParallel\", \"copula\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n############################# set up\neps1 = read.csv(\"eps1.csv\")\nsp.126 = read.csv(\"sp126.csv\", header = T)[, -c(1, 2)]\np1 = sp.126[-1, ]\np2 = sp.126[-length(sp.126[, 1]), ]\nl.r = 100 * log(p1/p2)\nr = l.r\nr1 = r[, -c(1)]\ndims = 5\nattach(r2)\nr2 = data.frame(cbind(AVB, EQR, TXN, ADI, LLY))\nM = 1000\nVaR = matrix(NA, backtestNr, 4)\nslidingWindowLength = 300\neps = matrix(NA, backtestNr, dim(r2)[2])\ncolnames(eps) = colnames(r2)\neps1 = data.frame()\ndat = eps1\nparaMat = list()\npara = matrix(NA, dims, 4)\nsigma = matrix(NA, backtestNr, dims)\n############################# estimation GARCH parameter\ncl = makeCluster(37)\nregisterDoParallel(cl)\ngetDoParWorkers()\ndat = r2\ndatMat = list()\nfor (i in 1:backtestNr) {\ndatMat[[i]] = dat[c(i:(i + (slidingWindowLength - 1))), 1:dims]\n}\nlengthPara = 4\nlengthEps1 = slidingWindowLength\nobjFun = function(dMat) {\nparaComb = list()\nlibrary(fGarch)\nfor (i in 1:dims) {\nfit = garchFit(~garch(1, 1), data = dMat[[i]], trace = F)\neps1.loop = fit@residuals/fit@sigma.t\npara.loop = fit@fit$coef\nlengthPara = length(para.loop)\nparaComb[[i]] = c(para.loop, sigma.loop, eps1.loop)\n}\nreturn(paraComb)\nresultD = foreach(dMat = datMat) %dopar% objFun(dMat)\nstopCluster(cl)\n############################# estimation copula parameter\ncl = makeCluster(37)\nregisterDoParallel(cl)\nlengthPara = 4\nlengthEps1 = slidingWindowLength\ntotalLength = lengthPara + lengthEps1 + lengthSigma\nepsComb = list()\nk = pobs(as.data.frame(resultD[[i]]))\nepsComb[[i]] = k[-(1:(lengthPara + lengthEps1)), ]\ndatMatEps = epsComb\nobjFun = function(dMat) {\nlibrary(copula)\nt.cop = tCopula(0.3, dim = dims, dispstr = \"ex\", df = 2)\nfit.mln = fitCopula(t.cop, dMat, method = \"mpl\")\npara.normal = summary(fit.mln)$coefficient[1:2]\nreturn(para.normal)\nresultDcopPara = foreach(dMat = datMatEps, .combine = \"rbind\") %dopar%\nobjFun(dMat)\n############################# VaR computation\nspread.real.0 = sp.126\nattach(spread.real.0)\nspread.real = data.frame(cbind(AVB, EQR, TXN, ADI, LLY))\nS = rowSums(spread.real)\nS1 = S[-1]\nS2 = S[-length(S)]\nS3 = S2 - S1\nL.real = S3[301:(300 + backtestNr)]\ncl = makeCluster(37)\npara.vec = resultDcopPara\nVaR = matrix(NA, backtestNr, 4)\ndatMatIndex = c(1:backtestNr)\nVaRt_store = matrix(777, backtestNr, 4)\nobjFunVaR = function(dMat) {\ni = dMat\npara.normal = as.numeric(para.vec[i, ])\nt.cop.2 = tCopula(para.normal[1], dim = dims, dispstr = \"ex\", df = para.normal[2])\nu = rCopula(M, t.cop.2)\nk = as.data.frame(resultD[[i]])\nk1 = k[c(1:lengthPara, lengthPara + lengthEps1, totalLength), ]\nh = sqrt(k1[2, ] + k1[3, ] * k1[lengthPara + 1, ]^2 * k1[lengthPara +\nk2 = matrix(NA, M, dims)\nk3[1, ] = as.matrix(k1[lengthPara + 1, ])\nsig.t = k3[rep(1, M), ]\nR = mu.t + sig.t * u\nspread.real.loop = read.csv(\"sp126.csv\")[, -c(1:2)]\nattach(spread.real.loop)\nspread = data.frame(cbind(AVB, EQR, TXN, ADI, LLY))\ns1 = spread[i + (slidingWindowLength), ]\nst = data.frame(matrix(NA, M, dims))\nst[1, ] = s1\nst = st[rep(1, M), ]\nL.sim = rowSums(st * (exp(0.01 * R) - 1))\nVaRt1005 = quantile(L.sim, 0.05)\nVaRt = c(VaRt1005, VaRt10001)\nreturn(VaRt)\nresultVaR = foreach(dMat = datMatIndex, .combine = \"rbind\") %dopar% objFunVaR(dMat)\nVaR = resultVaR\nresultComb = data.frame(L.real, VaR)\n############################# exceeding ratio\nExceeding_Ratio = numeric(4)\nfor (alpha in 2:5) {\nnullVector = rep(0, backtestNr)\nfor (i in 1:length(resultComb[, alpha])) {\nif (resultComb[, 1][i] < resultComb[, alpha][i]) {\nnullVector[i] = 1\n} else {\nnullVector[i] = 0\n}\nExceeding_Ratio[alpha] = sum(nullVector)/backtestNr\nExceeding_Ratio\n############################# plot of VaR and quantile = 0.001\nalpha = 5\nptForPlot = c(min(resultComb[, 1]), min(resultComb[, 2]), min(resultComb[,\n3]), min(resultComb[, 4]), min(resultComb[, 5]))\nlowPt = min(ptForPlot)\nupPt = max(resultComb[, 1])\nPortfolio_Value = seq(lowPt, upPt, length.out = length(resultComb[, 1]))\n1])))\nplot(Time_Index,\nPortfolio_Value,\ncol = \"white\",\npch = 19,\nxlab = \"Time Index\",\nylab = \"Profit and Loss of Portfolio\")\nlines(resultComb[, alpha], col = \"gray\", lwd = 6)\nfor (i in 1:length(resultComb[, alpha])) {\nif (resultComb[, 1][i] < resultComb[, alpha][i]) {\npoints(i, lowPt + 1, col = \"black\", pch = 17, cex = 1.5)\npoints(i, resultComb[, 1][i],\ncol = \"black\",\n} else {\npch = 19,\n", "output_sequence": "Shows VaRs of Student-t copula model introduced in the paper. The alphas are chosen eqaul to .001."}, {"input_sequence": "Author: Guo Li ; # please download the following package\nlibraries = c(\"lubridate\", \"poweRlaw\",\"igraph\",\"tables\",\"texreg\")\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# Part1: for figure 2,5,7 & Table 1 specify the path to the desired folder,\n# please set your own directory, e.g (C:/Users/Desktop/...)\n#setwd(\"~/Desktop/Data\")\n# pre-allocating for caculation of Power Law parameters\nXmin = rep(NA, 60)\n# Wealth distribution calculation loop (Fitted by Power Law model)\nfor (i in 1:length(Xmin)) {\nprint(i)\nData.PL = read.csv(paste(i, \".csv\", sep = \"\"),\nheader = T)[, 1]\nfit = power.law.fit(Data.PL, xmin = 1,\nstart = 2,\nP[i] = fit$KS.p\n}\n", "output_sequence": "Estimates the Power Law parameter Alpha as well as the goodness of fit of wealth distribution of Bitcoin with Xmin = 1 by default."}, {"input_sequence": "Author: Wei Li, Wolfgang Karl H \u0308ardl ; \"\"\"\nCreated on Thu Oct 29 20:17:56 2020\n@author: Wei Li\n\"\"\"\n#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import RobustScaler\nimport math\nimport random\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_score, f1_score, roc_auc_score, accuracy_score, classification_report,precision_recall_fscore_support\nfrom sklearn.model_selection import KFold, StratifiedKFold\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom timeit import default_timer as timer\nfrom collections import Counter\nfrom numba import jit\nimport pyswarms as ps\nimport logging\ncuda_logger = logging.getLogger('numba.cuda.cudadrv.driver')\ncuda_logger.setLevel(logging.ERROR) # only show error\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import MinMaxScaler\nfrom measure_others import measure_others\nfrom get_importance import get_importance\ndef dist_computation(w, p, p1, m, r,q):\nf = m.shape[0]\nsim = 0.0\nfor i in range(f):\nif q[i] >= r[i]:\nsim += ((((maxmin[i] - abs(q[i] - r[i]))/maxmin[i])**p[i])**2)*w[i]\nelse:\nsim += ((((maxmin[i] - abs(q[i] - r[i]))/maxmin[i])**p1[i])**2)*w[i]\nsim = math.sqrt(sim)\nreturn sim\ndef get_result(k, dist, y_train, y_val):\nn = y_val.shape[0]\npredict = np.zeros((n,))\nfor i in range(n):\na = y_train[dist[i,:].argsort()[-k:]].tolist()\nif sum(a) == len(a)/2:\npredict[i] = random.randint(0,1)\nelse:\npredict[i] = max(set(a), key=lambda x: a.count(x))\nreturn accuracy_score(y_val,predict)# from sklearn.linear_model import Lasso\n@cuda.jit\ndef compute_sim(w, p, p1, m, ref, query, k, dist):\nheight = query.shape[0]\nstartX, = cuda.grid(2)\ngridX = cuda.gridDim.x * cuda.blockDim.x;\n\nfor x in range(startX, width, gridX):\ndist[y,x] = dist_gpu(w, p, p1, m, ref[x,:], query[y,:])\ndef test(w,p,p1,m,q,r):\nsim += ((((maxmin[i] - abs(q[i] - r[i]))/maxmin[i])**p[i])**2)*w[i]\ndef get_result1(k, dist, y_train, y_test):\nn = y_test.shape[0]\na = y_train[dist[i,:].argsort()[-k:]].tolist()\nreturn predict\n###############################################################################################\n#Importing the data\ndata = pd.read_csv('german.csv')\nprint(data.isnull().values.any())\nseed = 42\ndata_X_train = []\nfor jjj in range(10):\nprint(\"experiment: \",jjj)\nrandom_t = jjj\ndata = data.sample(frac=1, random_state=random_t)\n# amount of fraud classes\ngood_data = data.loc[data['credit_risk'] == 1][:Counter(data['credit_risk'])[0]]\nbad_data = data.loc[data['credit_risk'] == 0]\nnormal_distributed_data = pd.concat([good_data, bad_data])\n# Shuffle dataframe rows\nnew_data = normal_distributed_data.sample(frac=1, random_state=42)\nX = new_data.drop('credit_risk', axis=1)\nprint(Counter(y))\n#############################################################################################\nmin_max_scaler = preprocessing.MinMaxScaler()\nX = min_max_scaler.fit_transform(X)\nscaler = MinMaxScaler()\ny = np.array(y, dtype='float')\n###############################################################################################\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train = X_train\ny_test = y_test\ndata_X_train.append(X_train.tolist())\nk, y_pred_log_reg, y_pred_knear, measure_others(X_train, y_train,X_test,y_test)\ny_pred_all = [y_test.tolist(),y_pred_log_reg.tolist(), y_pred_knear.tolist(),\n##############################################################################################\nnf = X.shape[1]\nmaxmin = np.array(np.max(X, 0))\nk0=k\ndist_gpu = cuda.jit(device=True)(dist_computation)\nX_train0 = X_train\nall_importances = []\ncost_all = []\nname = ['gini','entropy','mutual_info_classif','chi2','f_classif','ReliefF']\nk_n = len(name)\nfor iii in range(k_n):\nimportances = get_importance(X_train,y_train,name[iii])\nprint('current iii: ', name[iii])\nkf = KFold(n_splits = 10,random_state=None, shuffle=False)\ndef scale_imp(importances):\nb = 1\nimp = (b -a) * (importances - importances.min())/(importances.max() - importances.min()) + a\nreturn imp\nimportances0 = importances\n###################################################################################################\nimportances = scale_imp(importances0)\n#####################################################################################################\nstart = timer()\ndef get_f(params):\ncbr_predict = []\ngimportance = importances\nweights = gimportance/sum(gimportance)\npolynomial = params[:nf]\nfor train_index, val_index in kf.split(X_train0):\nX_train, X_val = X_train0[train_index],\ndist = np.zeros((len(X_val), len(X_train)), dtype = np.float)\nblockdim = (32, 8)\ngrid_x = math.ceil(X_val.shape[0]/32)+10\ngriddim = (grid_x,grid_y)\nd_train = cuda.to_device(X_train)\ncompute_sim[griddim, blockdim](weights, polynomial,polynomial1, maxmin, d_train, d_val, k, d_dist)\nd_dist.to_host()\ncbr_predict.append(get_result(k,dist,y_train,y_val))\ncbr_mean = np.mean(np.array(cbr_predict))\nreturn(-cbr_mean)\n#pso\nswarm_size = nf\ndim = nf # Dimension of X\nepsilon = 1.0\noptions = {'c1': 1.5, 'c2':1.5, 'w':0.8}\n\nswarm_size = nf*2\ndim = nf*2 # Dimension of X\nconstraints_p = (np.ones((nf*2,))/10,\ndef opt_func(X):\nn_particles = X.shape[0] # number of particles\ndist = [get_f(X[i]) for i in range(n_particles)]\nreturn np.array(dist)\noptimizer = ps.single.GlobalBestPSO(n_particles=swarm_size,\ndimensions=dim,\nbounds=constraints_p,init_pos = np.array([np.ones((2*nf,)),]*dim))\ncost, joint_vars = optimizer.optimize(opt_func, iters=500)\ndt = timer() - start\nprint(\"Data compute time %f s\" % dt)\ncost_all.append(cost)\npoly_all.append(joint_vars)\npolynomial = joint_vars\npolynomials = polynomial[:nf]\n################################################################################################\ndist = np.zeros((len(X_test), len(X_train)), dtype = np.float)\nblockdim = (32, 8)\ngrid_x = math.ceil(X_test.shape[0]/32)+10\ngriddim = (grid_x,grid_y)\nd_train = cuda.to_device(X_train)\ncompute_sim[griddim, blockdim](importances, polynomials, maxmin, d_train, d_val, k, d_dist)\nd_dist.to_host()\ny_pred_cbr_e = get_result1(k,dist,y_train,y_test)\nprint('E-CBR Classifier:')\nprint(classification_report(y_test, y_pred_cbr_e))\n################################################################################################\nm = min(cost_all)\nmin_i = [i for i, j in enumerate(cost_all) if j == m]\nimportances = all_importances[min_i[0]]\npolynomial = poly_all[min_i[0]]\n################################################################################################\ndist = np.zeros((len(X_test), len(X_train)), dtype = np.float)\nblockdim = (32, 8)\ngrid_x = math.ceil(X_test.shape[0]/32)+10\ngriddim = (grid_x,grid_y)\nd_train = cuda.to_device(X_train)\ncompute_sim[griddim, blockdim](importances, polynomials, maxmin, d_train, d_val, k, d_dist)\nd_dist.to_host()\ny_pred_cbr_e = get_result1(k,dist,y_train,y_test)\nprint('E-CBR Classifier:')\nprint(classification_report(y_test, y_pred_cbr_e))\nprint('E-CBR Classifier:',precision_recall_fscore_support(y_test, y_pred_cbr_e, average=None,labels=[1]))\ny_pred_all.append(y_pred_cbr_e.tolist())\ndist = np.zeros((len(X_test), len(X_train0)), dtype = np.float)\ngrid_y = math.ceil(X_train0.shape[0]/8)+10\nd_train = cuda.to_device(X_train0)\ncompute_sim[griddim, blockdim](np.ones((nf,)), np.ones((nf,)), maxmin, d_train, d_val, k, d_dist)\nd_dist.to_host()\ny_pred_cbr_eq = get_result1(k,dist,y_train0,y_test)\nprint('Equal CBR Classifier:')\nprint(classification_report(y_test, y_pred_cbr_eq))\ny_pred_all.append(y_pred_cbr_eq.tolist())\n##################################################################################################\nprint('log_reg: ',precision_recall_fscore_support(y_test, y_pred_log_reg, average=None))\n###################################################################################################\nprint('log_reg: ',roc_auc_score(y_test, y_pred_log_reg))\n", "output_sequence": "This Quantlet is an example of training case-based reasoning and compared with other machine learning methods. The dataset is the German credit data publicly available in UCI. The credit reform data used in paper is confidential and can be collected based on requests from the Blockchain Research Center (BRC, https://blockchain-research-center.de/)"}, {"input_sequence": "Author: Wei Li, Wolfgang Karl H \u0308ardl ; #!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Tue Oct 27 15:44:18 2020\n@author: waynelee\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import chi2, f_classif, mutual_info_classif\nfrom skrebate import ReliefF\ndef get_importance(X_train, y_train, name):\nif name == \"gini\":\nforest = ExtraTreesClassifier(n_estimators=250, criterion=\"gini\",random_state=42)\nforest.fit(X_train, y_train)\nimportances = forest.feature_importances_\nelif name == \"entropy\":\nforest = ExtraTreesClassifier(n_estimators=250, criterion=\"entropy\",random_state=42)\nelif name == \"mutual_info_classif\":\nimportances = mutual_info_classif(X_train, y_train)\nelif name == \"chi2\":\nimportances = chi2(X_train, y_train)[0]\nelif name == \"f_classif\":\nimportances = f_classif(X_train, y_train)[0]\nelif name == 'ReliefF':\nrr = ReliefF()\nrr.fit(X_train, y_train)\nimportances = rr.feature_importances_\nreturn importances\n\n", "output_sequence": "This Quantlet is an example of training case-based reasoning and compared with other machine learning methods. The dataset is the German credit data publicly available in UCI. The credit reform data used in paper is confidential and can be collected based on requests from the Blockchain Research Center (BRC, https://blockchain-research-center.de/)"}, {"input_sequence": "Author: Wei Li, Wolfgang Karl H \u0308ardl ; #!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Sun Oct 25 13:50:29 2020\n@author: wayne\n# Classifier Libraries\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import confusion_matrix\nfrom scipy.stats import uniform, randint\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, KFold, RandomizedSearchCV, train_test_split\nfrom sklearn.metrics import precision_score, f1_score, roc_auc_score, accuracy_score, classification_report,precision_recall_fscore_support\n# Use GridSearchCV to find the best parameters.\nfrom sklearn.model_selection import GridSearchCV\ndef measure_others(X_train, y_train,X_test,y_test):\n# Logistic Regression\nlog_reg_params = {\"penalty\": ['none', 'l2'], 'C': [0.001, 0.1, 1, 10, 1000]}\n\ngrid_log_reg = GridSearchCV(LogisticRegression(), log_reg_params)\ngrid_log_reg.fit(X_train, y_train)\n# We automatically get the logistic regression with the best parameters.\nlog_reg = grid_log_reg.best_estimator_\nknears_params = {\"n_neighbors\": list(range(1,10,1)), 'algorithm': ['auto', 'ball_tree', 'brute']}\ngrid_knears = GridSearchCV(KNeighborsClassifier(), knears_params)\ngrid_knears.fit(X_train, y_train)\n# KNears best estimator\nknears_neighbors = grid_knears.best_estimator_\nk = grid_knears.best_estimator_.n_neighbors # k nearest\n# Support Vector Classifier\nsvc_params = {'C': [0.5, 0.7, 1], 'kernel': ['rbf', 'poly', 'sigmoid', 'linear']}\ngrid_svc = GridSearchCV(SVC(), svc_params)\ngrid_svc.fit(X_train, y_train)\n# SVC best estimator\nsvc = grid_svc.best_estimator_\n# DecisionTree Classifier\ntree_params = {\"criterion\": [\"gini\", \"entropy\"], \"max_depth\": list(range(2,4,1)),\n\"min_samples_leaf\": list(range(5,7,1))}\ngrid_tree = GridSearchCV(DecisionTreeClassifier(), tree_params)\ngrid_tree.fit(X_train, y_train)\n# tree best estimator\ntree_clf = grid_tree.best_estimator_\n########################################################################################\ny_pred_log_reg = log_reg.predict(X_test)\ny_pred_knear = knears_neighbors.predict(X_test)\n###############################################################################################\ngnb = GaussianNB()\ny_pred_gnb = gnb.fit(X_train, y_train).predict(X_test)\n########################################################################################\n", "output_sequence": "This Quantlet is an example of training case-based reasoning and compared with other machine learning methods. The dataset is the German credit data publicly available in UCI. The credit reform data used in paper is confidential and can be collected based on requests from the Blockchain Research Center (BRC, https://blockchain-research-center.de/)"}, {"input_sequence": "Author: Song Song ; #rm(list=ls(all=TRUE))\nlibrary(quantreg)\nlibrary(KernSmooth)\n#setwd(\"C:/Richard/Study/Quantile Regression/Jeo Hae Son Quantile Causality Test/Program\")\n\n\"lprq2\" <- function(x, y, h, tau, x0) # modified from lprq, s.t. we can specify where to estimate quantiles\n{ xx <- x0\nfor(i in 1:length(xx)) {\nz <- x - xx[i]\nr <- rq(y~z, weights=wx, tau=tau, ci=FALSE)\nfv[i] <- r$coef[1.]\n}\nlist(xx = xx, fv = fv, dv = dv)}\n\n# generate AR1 process, similar for ARMA, n is the #, mu is the constant, phi is the AR1 coefficient, sigma2 is the variance, burn is just internal used, some \"staring\" point\ngenerateAR1<-function(n,mu,phi, sigma2, burn)\n{arptemp<-rep(0,n+burn)\nerror<-rnorm(n+burn, mean=0, sd=sqrt(sigma2))\nfor ( i in 2:(n+burn)) {\narptemp[i]<-mu+phi*arptemp[i-1]+error[i]\nreturn(arptemp[1:(burn+n)]) # this indeed returns 1:5500 overall 5501 items\n}\n# generate y process, alpha means the significance of the causality and caus is the causality-bringing variable\ngeneratey<-function(n,mu,phi, sigma2, burn, alpha, caus){\narptemp<-rep(0,n+burn)\nerror<-rnorm(n+burn, mean=0, sd=sqrt(sigma2))\nfor ( i in 2:(n+burn)) {\narptemp[i]<-mu+phi*arptemp[i-1]+ alpha*caus[i-1]+error[i]\nreturn(arptemp[(burn):(burn+n)]) # this indeed returns 5000:5500 overall 501 items\n############# Parameter Set\nqvec <-seq(0.01, 0.99, by = 0.01)\ntstatvec <- vector(length= 99, mode=\"numeric\") # initilize the tstat vector\n############ Simulate Data, could be replaced by real data\n#test <- function(tn, alpha){ # alpha = how strong the causality is tn # T\nalpha = 0.05 # how strong the causality is\ntn = 5000 # T\nrepeatation =500 # to repeat the test to get the ppower\npowertemp = 0\nq = qvec[90] # quantile\nppower=0\nfor (repn in 1:repeatation) {\nw<-generateAR1(tn, 1, 1/2, 1, 5000)\ny<-generatey(tn, -qnorm(q), 1/2, 1, 5000, alpha, w^2)\nw = w[5000:(5000+tn-1)]\n#gold<-read.table(\"gold.txt\") #\n#w <- data.matrix(gold) # here is the log returns\n#tn = length(y)-1\n#plot(data.matrix(gold)/5,lty = 1, type = \"l\", lwd=4, col = \"red\", xlab=\"Days (19970220 - 20090717)\", ylab=\"Oil - Gold - GBP/USD\", cex.lab=2, cex.axis = 2, ylim = c(min(min(data.matrix(oil)), min(data.matrix(gbp))*100), max(max(data.matrix(oil)), max(data.matrix(gbp))*100)))\n#lines(data.matrix(oil), col = \"blue\", lty = 2, lwd=3)\n############ Main computation part\nyuv <-y[1: tn]\nh <- dpill(yuv, yur, gridsize = tn) # calculate the optimal bandwith qrh which is based on the optimal bandwidth from mean regression, as in yu and jones 1998\n#for ( jj in 1:99) {\nqrh <- h*((q*(1-q)/(dnorm(qnorm(p=q))^2))^(1/5))\nfit <- lprq2(yuv,yur,h= qrh,tau=q, x0=yuv)\niftemp <- (yur <=fit$fv) - q\nifvector <- data.matrix(iftemp)\nkk <- matrix(data = 0, nrow = tn, ncol = tn)\nymatrix = kronecker(y[1: tn], t(vector(length= tn, mode=\"numeric\")+1))-t(kronecker(y[1: tn], t(vector(length= tn, mode=\"numeric\")+1)))\nkk=dnorm(ymatrix/qrh)*dnorm(wmatrix/(qrh/sd(y)*sd(w)))\ntstat <- t(ifvector)%*%kk%*%ifvector*sqrt(tn/2/q/(1-q)/(tn-1)/sum(kk^2))\n#tstatvec=tstat\npowertemp = (tstat>1.96) + powertemp\nppower = powertemp/repn\nprint(repn)\nprint(ppower)\nsave.image(file = \"90005.RData\")\n#return(ppower)\n#}\n#test(500, 0.0)\n#plot(seq(0.01, 0.99, by = 0.01), tstatvec, lty = 1, type = \"l\", lwd=3, col = \"blue\", xlab=\"Different Quantiles\", ylab=\"Test Statistic (Gold - GBP/USD)\", cex.lab=1.6, cex.axis = 1.6, ylim=c(0, max(max(tstatvec), 1.96)))\n", "output_sequence": "Simulations are carried out to illustrate the behavior of the test under the null and also the power of the test under plausible alternatives. An economic application considers the causal relations between the crude oil price, the USD/GBP exchange rate, and the gold price in the gold market."}, {"input_sequence": "Author: Song Song ; rm(list=ls(all=TRUE))\nlibrary(quantreg)\nlibrary(KernSmooth)\n\"lprq2\" <- function(x, y, h, tau, x0) # modified from lprq, s.t. we can specify where to estimate quantiles\n{ xx <- x0\nfor(i in 1:length(xx)) {\nz <- x - xx[i]\nr <- rq(y~z, weights=wx, tau=tau, ci=FALSE)\nfv[i] <- r$coef[1.]\n}\nlist(xx = xx, fv = fv, dv = dv)}\n# generate AR1 process, similar for ARMA, n is the #, mu is the constant, phi is the AR1 coefficient, sigma2 is the variance, burn is just internal used, some \"staring\" point\ngenerateAR1<-function(n,mu,phi, sigma2, burn)\n{arptemp<-rep(0,n+burn)\nerror<-rnorm(n+burn, mean=0, sd=sqrt(sigma2))\nfor ( i in 2:(n+burn)) {\narptemp[i]<-mu+phi*arptemp[i-1]+error[i]\nreturn(arptemp[1:(burn+n)]) # this indeed returns 1:5500 overall 5501 items\n}\n# generate y process, alpha means the significance of the causality and caus is the causality-bringing variable\ngeneratey<-function(n,mu,phi, sigma2, burn, alpha, caus){\narptemp<-rep(0,n+burn)\nerror<-rnorm(n+burn, mean=0, sd=sqrt(sigma2))\nfor ( i in 2:(n+burn)) {\narptemp[i]<-mu+phi*arptemp[i-1]+ alpha*caus[i-1]+error[i]\nreturn(arptemp[(burn):(burn+n)]) # this indeed returns 5000:5500 overall 501 items\n############# Parameter Set\nqvec <-seq(0.01, 0.99, by = 0.01)\ntstatvec <- vector(length= 99, mode=\"numeric\") # initilize the tstat vector\n############ Simulate Data, could be replaced by real data\n#test <- function(tn, alpha){ # alpha = how strong the causality is tn # T\n#alpha = 0.05 # how strong the causality is\n#tn = 5000 # T\n#repeatation =1 # to repeat the test to get the ppower\n#powertemp = 0\n#ppower=0\n#for (repn in 1:repeatation) {\n#w<-generateAR1(tn, 1, 1/2, 1, 5000)\n#y<-generatey(tn, -qnorm(q), 1/2, 1, 5000, alpha, w^2)\n#w = w[5000:(5000+tn-1)]\n\ngold<-read.table(\"gold.txt\") #\ngold<-diff(as.matrix(log(gold))) # here is the log returns\noil<-read.table(\"oil.txt\") #\noil<-diff(as.matrix(log(oil))) # here is the log returns\nrate<-read.table(\"rate.txt\")\nrate<-diff(as.matrix(rate)) # here is the log returns\nw <- data.matrix(rate)\ntn = length(y)-1\n#plot(data.matrix(gold)/5,lty = 1, type = \"l\", lwd=4, col = \"red\", xlab=\"Days (19970220 - 20090717)\", ylab=\"Oil - Gold - GBP/USD\", cex.lab=2, cex.axis = 2, ylim = c(min(min(data.matrix(oil)), min(data.matrix(rate))*100), max(max(data.matrix(oil)), max(data.matrix(rate))*100)))\n#lines(data.matrix(oil), col = \"blue\", lty = 2, lwd=3)\n############ Main computation part\nyuv <-y[1: tn]\nh <- dpill(yuv, yur, gridsize = tn) # calculate the optimal bandwith qrh which is based on the optimal bandwidth from mean regression, as in yu and jones 1998\nfor ( jj in 41:99) {\nq = qvec[jj] # quantile\nqrh <- h*((q*(1-q)/(dnorm(qnorm(p=q))^2))^(1/5))\nfit <- lprq2(yuv,yur,h= qrh,tau=q, x0=yuv)\niftemp <- (yur <=fit$fv) - q\nifvector <- data.matrix(iftemp)\nkk <- matrix(data = 0, nrow = tn, ncol = tn)\nymatrix = kronecker(y[1: tn], t(vector(length= tn, mode=\"numeric\")+1))-t(kronecker(y[1: tn], t(vector(length= tn, mode=\"numeric\")+1)))\nkk=dnorm(ymatrix/qrh)*dnorm(wmatrix/(qrh/sd(y)*sd(w)))\ntstat <- t(ifvector)%*%kk%*%ifvector*sqrt(tn/2/q/(1-q)/(tn-1)/sum(kk^2))\ntstatvec[jj]=tstat\nprint(jj)\nsave.image(file = \"RateGold.RData\")\n#powertemp = (tstat>1.96) + powertemp\n#ppower = powertemp/repn\n#print(repn)\n#print(ppower)\n#}\nplot(seq(0.01, 0.99, by = 0.01), tstatvec, lty = 1, type = \"l\", lwd=3, col = \"blue\", xlab=\"Different Quantiles\", ylab=\"RateGold\", cex.lab=1.6, cex.axis = 1.6, ylim=c(0, max(max(tstatvec), 1.96)))\nlines(seq(0.01, 0.99, by = 0.01), vector(length= 99, mode=\"numeric\")+ 1.96, col = \"red\", lty = 2, lwd=2)\n", "output_sequence": "Simulations are carried out to illustrate the behavior of the test under the null and also the power of the test under plausible alternatives. An economic application considers the causal relations between the crude oil price, the USD/GBP exchange rate, and the gold price in the gold market."}, {"input_sequence": "Author: Song Song ; rm(list=ls(all=TRUE))\nlibrary(quantreg)\nlibrary(KernSmooth)\n\"lprq2\" <- function(x, y, h, tau, x0) # modified from lprq, s.t. we can specify where to estimate quantiles\n{ xx <- x0\nfor(i in 1:length(xx)) {\nz <- x - xx[i]\nr <- rq(y~z, weights=wx, tau=tau, ci=FALSE)\nfv[i] <- r$coef[1.]\n}\nlist(xx = xx, fv = fv, dv = dv)}\n# generate AR1 process, similar for ARMA, n is the #, mu is the constant, phi is the AR1 coefficient, sigma2 is the variance, burn is just internal used, some \"staring\" point\ngenerateAR1<-function(n,mu,phi, sigma2, burn)\n{arptemp<-rep(0,n+burn)\nerror<-rnorm(n+burn, mean=0, sd=sqrt(sigma2))\nfor ( i in 2:(n+burn)) {\narptemp[i]<-mu+phi*arptemp[i-1]+error[i]\nreturn(arptemp[1:(burn+n)]) # this indeed returns 1:5500 overall 5501 items\n}\n# generate y process, alpha means the significance of the causality and caus is the causality-bringing variable\ngeneratey<-function(n,mu,phi, sigma2, burn, alpha, caus){\narptemp<-rep(0,n+burn)\nerror<-rnorm(n+burn, mean=0, sd=sqrt(sigma2))\nfor ( i in 2:(n+burn)) {\narptemp[i]<-mu+phi*arptemp[i-1]+ alpha*caus[i-1]+error[i]\nreturn(arptemp[(burn):(burn+n)]) # this indeed returns 5000:5500 overall 501 items\n############# Parameter Set\nqvec <-seq(0.01, 0.99, by = 0.01)\ntstatvec <- vector(length= 99, mode=\"numeric\") # initilize the tstat vector\n############ Simulate Data, could be replaced by real data\n#test <- function(tn, alpha){ # alpha = how strong the causality is tn # T\n#alpha = 0.05 # how strong the causality is\n#tn = 5000 # T\n#repeatation =1 # to repeat the test to get the ppower\n#powertemp = 0\n#ppower=0\n#for (repn in 1:repeatation) {\n#w<-generateAR1(tn, 1, 1/2, 1, 5000)\n#y<-generatey(tn, -qnorm(q), 1/2, 1, 5000, alpha, w^2)\n#w = w[5000:(5000+tn-1)]\n\ngold<-read.table(\"gold.txt\") #\ngold<-diff(as.matrix(log(gold))) # here is the log returns\noil<-read.table(\"oil.txt\") #\noil<-diff(as.matrix(log(oil))) # here is the log returns\nrate<-read.table(\"rate.txt\")\nrate<-diff(as.matrix(rate)) # here is the log returns\nw <- data.matrix(oil)\ntn = length(y)-1\n#plot(data.matrix(gold)/5,lty = 1, type = \"l\", lwd=4, col = \"red\", xlab=\"Days (19970220 - 20090717)\", ylab=\"Oil - Gold - GBP/USD\", cex.lab=2, cex.axis = 2, ylim = c(min(min(data.matrix(oil)), min(data.matrix(rate))*100), max(max(data.matrix(oil)), max(data.matrix(rate))*100)))\n#lines(data.matrix(oil), col = \"blue\", lty = 2, lwd=3)\n############ Main computation part\nyuv <-y[1: tn]\nh <- dpill(yuv, yur, gridsize = tn) # calculate the optimal bandwith qrh which is based on the optimal bandwidth from mean regression, as in yu and jones 1998\nfor ( jj in 41:99) {\nq = qvec[jj] # quantile\nqrh <- h*((q*(1-q)/(dnorm(qnorm(p=q))^2))^(1/5))\nfit <- lprq2(yuv,yur,h= qrh,tau=q, x0=yuv)\niftemp <- (yur <=fit$fv) - q\nifvector <- data.matrix(iftemp)\nkk <- matrix(data = 0, nrow = tn, ncol = tn)\nymatrix = kronecker(y[1: tn], t(vector(length= tn, mode=\"numeric\")+1))-t(kronecker(y[1: tn], t(vector(length= tn, mode=\"numeric\")+1)))\nkk=dnorm(ymatrix/qrh)*dnorm(wmatrix/(qrh/sd(y)*sd(w)))\ntstat <- t(ifvector)%*%kk%*%ifvector*sqrt(tn/2/q/(1-q)/(tn-1)/sum(kk^2))\ntstatvec[jj]=tstat\nprint(jj)\nsave.image(file = \"OilGold.RData\")\n#powertemp = (tstat>1.96) + powertemp\n#ppower = powertemp/repn\n#print(repn)\n#print(ppower)\n#}\nplot(seq(0.01, 0.99, by = 0.01), tstatvec, lty = 1, type = \"l\", lwd=3, col = \"blue\", xlab=\"Different Quantiles\", ylab=\"OilGold\", cex.lab=1.6, cex.axis = 1.6, ylim=c(0, max(max(tstatvec), 1.96)))\nlines(seq(0.01, 0.99, by = 0.01), vector(length= 99, mode=\"numeric\")+ 1.96, col = \"red\", lty = 2, lwd=2)\n", "output_sequence": "Simulations are carried out to illustrate the behavior of the test under the null and also the power of the test under plausible alternatives. An economic application considers the causal relations between the crude oil price, the USD/GBP exchange rate, and the gold price in the gold market."}, {"input_sequence": "Author: ['Zijin Wang'] ; %% SP500 by Hierarchical Clustering\n% to produce dendrogram\n% treat every date as a node\nclc,clear\nload(\"sp500.mat\");\nlogret=sample(:,4);\ntimeline=datenum(sample(:,1:3));\nDM=pdist2(logret,logret);\nfor i=1:length(DM)-2\nfor j=i+2:length(DM)\nDM(i,j)=inf;\nend\nend\nfor j=1:length(DM)-2\nfor i=j+2:length(DM)\n%% generate hierarchical clustering\nZ=linkage(squareform(DM));% default is singleton\n%% plot hierarchical clustering K=20\nK=20;\nfigure;\n[H,T,outperm]=dendrogram(Z,0,\"ColorThreshold\",Z(end-K+2,3),\"Reorder\",1:length(Z)+1);\nset(gca,'xtick',[],'xticklabel',[]);\ndateaxis('x',12)\n%% save hierarchical clustering K=20\nsaveas(gcf,'Hierarchical_sp500','png')\n", "output_sequence": "Give an example of using hierarchical clustering to illustrate how to choose the number of clusters by minimum spanning tree. The data set is S&P 500 index."}, {"input_sequence": "Author: ['Zijin Wang'] ; %% MST clustering for figure\nclc,clear\nax=gca;\nax.DataAspectRatio=[1 1 1];\nax.XLim=[0 1];\nax.XColor='none';\nhold(ax,'on')\n% =========================================================================\nrng default\nr=1/(2+2*cos(pi/12));\np=1-r-2*r*sin(pi/12);\nP11 = rand([6,2]).*[r*1/4 1/20]*[1 2.8;0 1]+[1/3*r p-0.4*r];\nP1=[P11;P12;P13;P14];\nP21 = rand([10,2]).*[r*4/3 1/20]+[p-2/3*r 4/3*r];\nP22 = rand([10,2]).*[1/20 r*4/3]+[p r/6];\nP2=[P21;P22];\nP31 = rand([10,2]).*[1/30 5/4*pi]+[r*1/3 pi*1/4];P31 = [cos(P31(:,2)) sin(P31(:,2))].*P31(:,1)+[1-r 1-r+1/3*r];\nP3=[P31;P32];\nscatter(P1(:,1),P1(:,2),100,'k','LineWidth',2,'MarkerFaceColor','auto');\nP=[P1;P2;P3];\nDM = pdist2(P,P);\nG=graph(DM);\nT=minspantree(G,'Method','sparse');\n% figure\nF=getframe(ax);\n[imind,cm]=rgb2ind(F.cdata,256);\nT1.Edges=sortrows(T.Edges,2);\nj=0.98;\nfor i = 1:floor(length(T1.Edges.Weight)*j)\nplot([P(T1.Edges.EndNodes(i,1),1),P(T1.Edges.EndNodes(i,2),1)]...\nend\n[X,Y]=getEllipse([r,p],[1,0;0,0.7],r^2,200);\nplot(X,Y,'Color',[250,250,0]./255,'LineWidth',5);\n[X,Y]=getEllipse([p,r],[0.8,0;0,0.7],r^2,200);\nplot(X,Y,'Color',[0,250,0]./255,'LineWidth',5);\n[X,Y]=getEllipse([1-r,1-r],[0.7,0;0,0.8],r^2,200);\nplot(X,Y,'Color',[0,0,250]./255,'LineWidth',5);\n%% save the final figure as a single\nsaveas(gcf,'MST_Clustering','png')\n%% \u692d\u5706\u6570\u636e\u8ba1\u7b97\u51fd\u6570\uff0c\u8f93\u5165\u534f\u65b9\u5dee\u77e9\u9635\u3001\u4e2d\u5fc3\u70b9\u3001\u534a\u5f84\u751f\u6210\u692d\u5706\u6570\u636e\nfunction [X,Y]=getEllipse(Mu,Sigma,S,pntNum)\n% (X-Mu)*inv(Sigma)*(X-Mu)=S\ninvSig=inv(Sigma);\n[V,D]=eig(invSig);\naa=sqrt(S/D(1));\nt=linspace(0,2*pi,pntNum);\nXY=V*[aa*cos(t);bb*sin(t)];\nX=(XY(1,:)+Mu(1))';\n", "output_sequence": "Give an example of using minimum spanning tree for clustering. Divide vertices into three groups."}, {"input_sequence": "Author: ['Zijin Wang'] ; %% MST clustering for gif by Kruskal\nclc,clear\nax=gca;\nax.DataAspectRatio=[1 1 1];\nax.XLim=[0 1];\nax.XColor='none';\nhold(ax,'on')\nDelayTime=.2;\n% =========================================================================\nrng default\nr=1/(2+2*cos(pi/12));\np=1-r-2*r*sin(pi/12);\nP11 = rand([6,2]).*[r*1/4 1/20]*[1 2.8;0 1]+[1/3*r p-0.4*r];\nP1=[P11;P12;P13;P14];\nP21 = rand([10,2]).*[r*4/3 1/20]+[p-2/3*r 4/3*r];\nP22 = rand([10,2]).*[1/20 r*4/3]+[p r/6];\nP2=[P21;P22];\nP31 = rand([10,2]).*[1/30 5/4*pi]+[r*1/3 pi*1/4];P31 = [cos(P31(:,2)) sin(P31(:,2))].*P31(:,1)+[1-r 1-r+1/3*r];\nP3=[P31;P32];\nscatter(P1(:,1),P1(:,2),100,'k','LineWidth',2,'MarkerFaceColor','auto');\npause(.5)\nF=getframe(ax);\n[imind,cm]=rgb2ind(F.cdata,256);\nimwrite(imind,cm,'Kruskal_Clustering.gif','gif','Loopcount',inf,'DelayTime',DelayTime);\nP=[P1;P2;P3];\nDM = pdist2(P,P);\nG=graph(DM);\nT=minspantree(G,'Method','sparse');\n% figure\nT1.Edges=sortrows(T.Edges,2);\nj=0.98;\nfor i = 1:floor(length(T1.Edges.Weight)*j)\nplot([P(T1.Edges.EndNodes(i,1),1),P(T1.Edges.EndNodes(i,2),1)]...\npause(.2);saveFrame(ax,DelayTime)\nend\npause(2);saveFrame(ax,2)\n[X,Y]=getEllipse([r,p],[1,0;0,0.7],r^2,200);\nplot(X,Y,'Color',[250,250,0]./255,'LineWidth',5);\n[X,Y]=getEllipse([p,r],[0.8,0;0,0.7],r^2,200);\nplot(X,Y,'Color',[0,250,0]./255,'LineWidth',5);\n[X,Y]=getEllipse([1-r,1-r],[0.7,0;0,0.8],r^2,200);\nplot(X,Y,'Color',[0,0,250]./255,'LineWidth',5);\n%% save the final figure as a single\n% saveas(gcf,'MST_Clustering','png')\n%% \u692d\u5706\u6570\u636e\u8ba1\u7b97\u51fd\u6570\uff0c\u8f93\u5165\u534f\u65b9\u5dee\u77e9\u9635\u3001\u4e2d\u5fc3\u70b9\u3001\u534a\u5f84\u751f\u6210\u692d\u5706\u6570\u636e\nfunction [X,Y]=getEllipse(Mu,Sigma,S,pntNum)\n% (X-Mu)*inv(Sigma)*(X-Mu)=S\ninvSig=inv(Sigma);\n[V,D]=eig(invSig);\naa=sqrt(S/D(1));\nt=linspace(0,2*pi,pntNum);\nXY=V*[aa*cos(t);bb*sin(t)];\nX=(XY(1,:)+Mu(1))';\n%% save gif\nfunction saveFrame(ax,DelayTime)\nimwrite(imind,cm,'kruskal_Clustering.gif','gif','WriteMode','append','DelayTime',DelayTime);\n", "output_sequence": "Give an example of using minimum spanning tree for clustering. Divide vertices into three groups."}, {"input_sequence": "Author: ['Zijin Wang'] ; %% MST clustering for gif by Prim\nclc,clear\nax=gca;\nax.DataAspectRatio=[1 1 1];\nax.XLim=[0 1];\nax.XColor='none';\nhold(ax,'on')\nDelayTime=.2;\n% =========================================================================\nrng default\nr=1/(2+2*cos(pi/12));\np=1-r-2*r*sin(pi/12);\nP11 = rand([6,2]).*[r*1/4 1/20]*[1 2.8;0 1]+[1/3*r p-0.4*r];\nP1=[P11;P12;P13;P14];\nP21 = rand([10,2]).*[r*4/3 1/20]+[p-2/3*r 4/3*r];\nP22 = rand([10,2]).*[1/20 r*4/3]+[p r/6];\nP2=[P21;P22];\nP31 = rand([10,2]).*[1/30 5/4*pi]+[r*1/3 pi*1/4];P31 = [cos(P31(:,2)) sin(P31(:,2))].*P31(:,1)+[1-r 1-r+1/3*r];\nP3=[P31;P32];\nscatter(P1(:,1),P1(:,2),100,'k','LineWidth',2,'MarkerFaceColor','auto');\nP=[P1;P2;P3];\nDM = pdist2(P,P);\nG=graph(DM);\nresult=nan(length(DM)-1,3);pp=1;tb=2:length(DM);\nfor i=1:length(DM)-1\ntemp=DM(pp,tb);temp=temp(:);\nd=min(temp);\n[Frow,Fcolume]=find(DM(pp,tb)==d);\nj=pp(Frow(1));k=tb(Fcolume(1));\nresult(i,1)=pp(Frow(1));\nresult(i,3)=d;\npp=[pp,k];tb(tb==k)=[];\nend\nT.Edges=table(result(:,1:2),result(:,3),'VariableNames',[\"EndNodes\",\"Weight\"]);\nT1.Edges=sortrows(T.Edges,2);\n% figure\npause(.5)\nF=getframe(ax);\n[imind,cm]=rgb2ind(F.cdata,256);\nimwrite(imind,cm,'Prim_Clustering.gif','gif','Loopcount',inf,'DelayTime',DelayTime);\n% j=0.98;\n% for i = 1:floor(length(T1.Edges.Weight)*j)\n% plot([P(T1.Edges.EndNodes(i,1),1),P(T1.Edges.EndNodes(i,2),1)]...\n% end\nfor i = 1:length(T.Edges.Weight)\nplot([P(T.Edges.EndNodes(i,1),1),P(T.Edges.EndNodes(i,2),1)]...\npause(.2);saveFrame(ax,DelayTime)\n%\n% [X,Y]=getEllipse([r,p],[1,0;0,0.7],r^2,200);\n% plot(X,Y,'Color',[250,250,0]./255,'LineWidth',5);\n% [X,Y]=getEllipse([p,r],[0.8,0;0,0.7],r^2,200);\n% plot(X,Y,'Color',[0,250,0]./255,'LineWidth',5);\n% [X,Y]=getEllipse([1-r,1-r],[0.7,0;0,0.8],r^2,200);\n% plot(X,Y,'Color',[0,0,250]./255,'LineWidth',5);\npause(2);saveFrame(ax,2)\nfor i = ceil(length(T1.Edges.Weight)*0.98):floor(length(T1.Edges.Weight)*1)\nplot([P(T1.Edges.EndNodes(i,1),1),P(T1.Edges.EndNodes(i,2),1)]...\nfor i = 1:floor(length(T1.Edges.Weight)*0.98)\n,[P(T1.Edges.EndNodes(i,1),2),P(T1.Edges.EndNodes(i,2),2)],'r',\"LineWidth\",3);\n[X,Y]=getEllipse([r,p],[1,0;0,0.7],r^2,200);\nplot(X,Y,'Color',[250,250,0]./255,'LineWidth',5);\n[X,Y]=getEllipse([p,r],[0.8,0;0,0.7],r^2,200);\nplot(X,Y,'Color',[0,250,0]./255,'LineWidth',5);\n[X,Y]=getEllipse([1-r,1-r],[0.7,0;0,0.8],r^2,200);\nplot(X,Y,'Color',[0,0,250]./255,'LineWidth',5);\n%% \u692d\u5706\u6570\u636e\u8ba1\u7b97\u51fd\u6570\uff0c\u8f93\u5165\u534f\u65b9\u5dee\u77e9\u9635\u3001\u4e2d\u5fc3\u70b9\u3001\u534a\u5f84\u751f\u6210\u692d\u5706\u6570\u636e\nfunction [X,Y]=getEllipse(Mu,Sigma,S,pntNum)\n% (X-Mu)*inv(Sigma)*(X-Mu)=S\ninvSig=inv(Sigma);\n[V,D]=eig(invSig);\naa=sqrt(S/D(1));\nt=linspace(0,2*pi,pntNum);\nXY=V*[aa*cos(t);bb*sin(t)];\nX=(XY(1,:)+Mu(1))';\n%% save gif\nfunction saveFrame(ax,DelayTime)\nimwrite(imind,cm,'Prim_Clustering.gif','gif','WriteMode','append','DelayTime',DelayTime);\n", "output_sequence": "Give an example of using minimum spanning tree for clustering. Divide vertices into three groups."}, {"input_sequence": "Author: Lam Tuyen Nguyen, Wee Song Chua, Simon Trimborn ; ##### VaR and Expected Shortfall ####\nlibrary(\"evd\")\nVAR.ES.EVT2 = function(x, alpha) {\nL = -x\nzq = quantile(L, 1 - alpha)\nif( thr==max(L) ){\nevtES = -zq\n}\nelse {\nfitty = fpot(L, thr, model = \"gpd\", std.err = F)\nscale = as.numeric(fitty$scale)\nevtES = -(zq/(1 - shape) + (scale - shape * thr)/(1 - shape))\ntemp = c(-zq, evtES)\nnames(temp) = c(\"VaR\", \"ES\")\nreturn(temp)\n}\n#### GARCH fit ####\n", "output_sequence": "Generate SCRIX"}, {"input_sequence": "Author: Lam Tuyen Nguyen, Wee Song Chua, Simon Trimborn ; ## set up basics\n# clear workspace\nrm(list=ls(all=TRUE))\n\n# set working directory\nsetwd(\"C:/Users/user/Desktop/Q Kolleg/smart beta/SmartIndexData/\")\n# load library\nlibrary(\"zoo\")\nlibrary(\"fGarch\")\nlibrary(\"PerformanceAnalytics\")\n# load data\nload(\"market.RData\")\nload(\"crix_numb_const.RData\")\nload(\"tmi.RData\")\n# import function to calculate VaR and ES\nsource(\"VaR_ES2.r\")\n# identify the column position of the cryptos in the matrix of data\ncryptos_pos = list()\nfor (i in 1:length( crix_numb_const ) ){\nfor (j in 1:3){\ncryptos_pos[[3*(i-1) + j]] = which(\ncolnames( market ) %in%\ncryptos[[ 3*(i - 1) + j ]][ 1:crix_numb_const[i] ] )\n}\n}\n# set rolling window of 100 days\nroll = 100\n# starting time point for first date of the crix data\ndate = rownames( market )\ntstart = which( date == names(crix[1]) )\ntend = length( date )\nnt = tend - tstart + 1\n# clean and prepare data\n# trim data to have same data period as CRIX\nmarketTrim = market[ (tstart - roll):tend, ]\n# use last observation carried forward to deal with NA\n# do not remove leading NA\nmarketTrim = na.locf( marketTrim, F )\ndateTrim = rownames( volumeTrim )\n# use next observation carried backward for leading NA\nmarketTrim = na.locf( marketTrim, T, T )\n# calculate log returns\nreturnTrim = diff( log(returnTrim) )\n# index for date where constituents changes\n# 01 of every month, every 3 months the number of constituents changes\ndateShort = substr(dateTrim, nchar(dateTrim)-1,\nmonth1Index = which( dateShort == \"01\" )\n# vector to store scrix\nttend = length(crix)\nscrix = numeric( ttend )\n# calculate quantities to assign weights\n# create a list to store\ntemplist = list()\n# num of index prior to starting point of crix\ntempInd = max( which( month1Index <= roll ))\n# for loop to calculate quantities used to assign weights\nfor (i in 1:( length(month1Index) - tempInd ) ){\ntempN = length( cryptos_pos[[i]] )\ntemplist[[i]] = numeric( tempN )\nk = i + tempInd\n#################### returns for sharpe ratio #####################\ntempreturn = numeric( tempN )\ntempreturn = returnTrim[(month1Index[k] - 1),cryptos_pos[[i]]]\n###################################################################\nfor (j in 1:tempN){\n# using returns for VaR and ES #############################\ntempEst = VAR.ES.EVT2(returnTrim[\n(month1Index[k] - roll + 1):(month1Index[k] - 1),\ncryptos_pos[[i]][j] ],0.05 )\n\n# 2 for ES; 1 for VaR\ntemplist[[i]][j] = tempEst[2]\n############################################################\n# sharpe ratio, tempEst 1 for VaR; 2 for ES ################\n# tempEst = VAR.ES.EVT2(priceTrim[\n# (month1Index[k] - roll + 1):(month1Index[k] - 1),\n# cryptos_pos[[i]][j] ],0.05 )\n# templist[[i]][j] = tempreturn[j] / (tempEst[2]+ 0.000001)\n# ############################################################\n# templist[[i]][j] = 1/tempN\n# ###########################################################\n# using returns for VaR\n# tempEst = VaR(returnTrim[ (month1Index[k] - roll + 1):(month1Index[k] - 1),\n# cryptos_pos[[i]][j] ],0.95, method=\"modified\" )\n# using returns for ES\n# tempEst = ES(returnTrim[ (month1Index[k] - roll + 1):(month1Index[k] - 1),\n# cryptos_pos[[i]][j] ],0.95, method=\"modified\" )\n# if(is.na(tempEst)){tempEst=quantile(returnTrim[\n# (month1Index[k] - roll + 1):(month1Index[k] - 1),cryptos_pos[[i]][j] ],0.05)}\n# templist[[i]][j] = tempEst\n# ###############################################################################\n# using VaR in SR\n# #tempEst = SharpeRatio.modified(returnTrim[\n# (month1Index[k] - roll + 1):(month1Index[k] - 1),cryptos_pos[[i]][j] ],\n# Rf = 0, p = 0.95, FUN = \"VaR\")\n# using ES in SR\n# tempEst = SharpeRatio.modified(returnTrim[\n# (month1Index[k] - roll + 1):(month1Index[k] - 1),cryptos_pos[[i]][j] ],\n# Rf = 0, p = 0.95, FUN = \"ES\")\n# if(is.na(tempEst)){\n# tempDen = abs(quantile(returnTrim[\n# (month1Index[k] - roll + 1):(month1Index[k] - 1),cryptos_pos[[i]][j] ],\n# 0.05))\n# if(tempDen==0){\n# tempEst=mean(returnTrim[\n# (month1Index[k] - roll + 1):(month1Index[k] - 1),cryptos_pos[[i]][j] ])\n# }else{\n# (month1Index[k] - roll + 1):(month1Index[k] - 1),\n# cryptos_pos[[i]][j] ])/tempDen\n# }}\n# ##############################################################################\n# weights of the cryptos using Markowitz portfolio (min variance portfolio)\n# weight = list()\n# for (i in 1:( length(month1Index) - tempInd ) ){\n# k = i + tempInd\n# min_func = function(x){\n# last_index_period_time = priceTrim[\n# cryptos_pos[[i]] ]\n# a = apply(sweep(last_index_period_time,MARGIN=2,x,`*`), 1, sum)\n# b = volatility(garchFit(~garch(1,1), data = diff(log(a)), trace = F))\n# sum(b)\n# }\n# min_equal = function(x){\n# sum(x)\n# start_weight = rep(1/(length(cryptos_pos[[i]])),\n# tempweight = solnp(pars = start_weight, fun = min_func, eqfun = min_equal,\n# eqB = 1, control=list(delta=1e-5),\n# LB = rep(0, length(cryptos_pos[[i]])),\n# weight[[i]] = tempweight$pars\n# }\n# weights of the cryptos using equal weight\n# for (i in 1:length(templist)){\n# weight[[i]] = templist[[i]]\n# weights of the cryptos using VaR and ES using price\n# use inverse, then take proportion\n# tempW = 1/(templist[[i]]+0.000001)\n# test = tempW / sum( tempW )\n# weight[[i]] = test\n# weights of the cryptos using VaR and ES using returns\nweight = list()\nfor (i in 1:length(templist)){\ntempW = templist[[i]] + 2*abs(min(templist[[i]])) + 0.0000001\ntest = tempW / sum( tempW )\nweight[[i]] = test\n# weights of the cryptos using SR with VaR or ES as volatility\n# measured by price\n# tempW = templist[[i]]\n# tempW = tempW + 2*abs(min(tempW)) +0.000001\n# weight[[i]] = test\n# starting index for scrix\nscrixtstart = month1Index[tempInd + 1]\n# calculate old amount for CRIX\n# index_old_amount = list()\n# for (i in 1:( length(month1Index) - tempInd )){\n# index_old_amount[[i]] = marketTrim[\n# (month1Index[tempInd + i]-1), cryptos_pos[[i]]] /\n# calculate old amount for SCRIX\nindex_old_amount = list()\nfor (i in 1:( length(month1Index) - tempInd )){\nindex_old_amount[[i]] = sum(marketTrim[\n(month1Index[tempInd + i]-1), cryptos_pos[[i]]])* weight[[i]] /\n# calculate divisor\nDiv = numeric( length(month1Index) - tempInd )\nDiv[1] = sum( marketTrim[\n(month1Index[tempInd + 1]-1), cryptos_pos[[1]] ] )/ 1000\nfor (i in 2:length(Div) ){\nDiv[i] = sum( marketTrim[\n(month1Index[tempInd + i ]-1), cryptos_pos[[i]] ] ) /\n(sum( priceTrim[\n(month1Index[tempInd + i ] - 1), cryptos_pos[[i - 1]] ] *\nindex_old_amount[[i-1]] ) / Div[i-1] )\n# repeat the divisors, cryptos_pos, and amount for days in the month\nDivrep = numeric( ttend )\ncryptos_posrep = list( ttend )\ntempIndex = month1Index[ (tempInd + 1):length(month1Index) ] - roll\ntempIndex = c(tempIndex, ttend + 1)\nfor (i in 1:(length(tempIndex) - 1)){\nfor (j in 1:(tempIndex[i+1]-tempIndex[i]) ){\nDivrep[tempIndex[i] - 1 + j] = Div[i]\ncryptos_posrep[[tempIndex[i] - 1 + j]] = cryptos_pos[[i]]\n# calculate SCRIX\nfor (i in 1:ttend){\nscrix[i] = sum( priceTrim[scrixtstart + i - 1, cryptos_posrep[[i]] ]*\nold_amountrep[[i]] ) / Divrep[i]\n# name the scrix with dates\nnames(scrix) = names(crix)\n## save scrix\nscrix_es_return=scrix\nsave(scrix_es_return, file = \"scrix_es_return.RData\")\n", "output_sequence": "Generate SCRIX"}, {"input_sequence": "Author: Elizaveta Zinovyeva, Raphael Constantin Georg Reule ; from google.cloud import bigquery\nimport pandas as pd\nclient = bigquery.Client()\nquery = \"\"\"\nSELECT\ncontracts.address, COUNT(1) AS tx_count\nFROM\n`bigquery-public-data.crypto_ethereum.contracts` AS contracts\nJOIN\n`bigquery-public-data.crypto_ethereum.transactions` AS transactions\nON (transactions.to_address = contracts.address)\nGROUP BY contracts.address\nORDER BY tx_count DESC\n# Create a \"Client\" object\n# Construct a reference to the \"crypto_ethereum\" dataset\ndataset_ref = client.dataset(\"crypto_ethereum\", project=\"bigquery-public-data\")\n# API request - fetch the dataset\ndataset = client.get_dataset(dataset_ref)\n# List all the tables in the \"crypto_ethereum\" dataset\ntables = list(client.list_tables(dataset))\n# Print names of all tables in the dataset (there's only one!)\nfor table in tables:\nquery_job = client.query(query)\niterator = query_job.result(timeout=300)\nrows = list(iterator)\n# Transform the rows into a nice pandas dataframe\ndf = pd.DataFrame(data=[list(x.values()) for x in rows], columns=list(rows[0].keys()))\n# Look at the first 10\n", "output_sequence": "BigQuery on Kaggle platform to obtain dataset with ethereum hashes, contracts"}, {"input_sequence": "Author: Elizaveta Zinovyeva, Raphael Constantin Georg Reule ; from google.cloud import bigquery\nimport pandas as pd\nclient = bigquery.Client()\nquery = \"\"\"\nSELECT\n`address`, `is_erc20`, `block_timestamp`\nFROM\n`bigquery-public-data.crypto_ethereum.contracts` AS contacts\n# Create a \"Client\" object\n# Construct a reference to the \"crypto_ethereum\" dataset\ndataset_ref = client.dataset(\"crypto_ethereum\", project=\"bigquery-public-data\")\n# API request - fetch the dataset\ndataset = client.get_dataset(dataset_ref)\n# List all the tables in the \"crypto_ethereum\" dataset\ntables = list(client.list_tables(dataset))\n# Print names of all tables in the dataset (there's only one!)\nfor table in tables:\nquery_job = client.query(query)\niterator = query_job.result(timeout=30)\nrows = list(iterator)\n# Transform the rows into a nice pandas dataframe\ndf = pd.DataFrame(data=[list(x.values()) for x in rows], columns=list(rows[0].keys()))\n# Look at the first 10\n", "output_sequence": "BigQuery on Kaggle platform to obtain dataset with ethereum hashes, contracts"}, {"input_sequence": "Author: Lasse Groth ; # clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# parameter settings\nS0 = 100 # Underlying price at time t_0\nstrike = seq(85, 115, 1) # range of strike prices\ntau = 0.25 # time to maturity\n# Calculation of option price by BS model\nblsprice = function(S0, strike, r, tau, sigma) {\nd1 = (log(S0/strike) + (r + sigma^2/2) * tau)/(sigma * sqrt(tau))\nd2 = d1 - sigma * sqrt(tau)\nblsprice = S0 * pnorm(d1) - strike * exp(-r * tau) * pnorm(d2)\n}\n# calculate\nivsmileK = 0.000167 * strike^2 - 0.03645 * strike + 2.08\ncallinit = blsprice(S0, strike, r, tau, ivsmileK)\nivsmilemon = 0.000167 * (strike * S0/S1)^2 - 0.03645 * strike * S0/S1 + 2.08\ncallstickymon = blsprice(S1, strike, r, tau, ivsmilemon)\n# plot\nplot(strike, callinit, type = \"l\", col = \"blue\", xlab = \"Strike\", ylab = \"Call prices\",\nylim = c(0, 25))\nlines(strike, callstickystrike, col = \"dark green\")\ntitle(\"Call prices as a function of strikes\")\ndev.new()\nplot(strike, (callstickystrike - callstickymon)/callstickystrike, type = \"l\", col = \"blue\",\nxlab = \"Strike\", ylab = \"\")\ntitle(\"Relative diff. between call prices for diff. stickyness assumptions\")\nplot(strike, ivsmileK, type = \"l\", col = \"blue\", xlab = \"Strike\", ylab = \"sigma_imp\")\nlines(strike, ivsmilemon, col = \"dark green\")\n", "output_sequence": "Plots call option prices as a function of strikes for 85<K<115. The implied volatility function may be fixed to the strike prices (sticky strike) or moneyness K/S (sticky moneyness). Compares the relative difference of both approaches."}, {"input_sequence": "Author: Zografia Anastasiadou ; # clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# Brownian Bridge main calculation\ndt = 0.004\nn = 250\nl = 1\nt = seq(0, n * dt,\nset.seed(0)\nz = runif(n, min = 0, max = 1)\nz = 2 * (z > 0.5) - 1\nz = z * sqrt(dt) # to get finite and non-zero varinace\nz2 = dt * sum(z)\nz = z - z2\nx = c(0, cumsum(z))\nlistik = cbind(t, x)\n# Output\nplot(listik[, 1], listik[, 2], type = \"l\", col = \"blue\", xlab = \"Time\", ylab = \"Values of process X_t\")\n", "output_sequence": "Plots a sample path of Brownian Bridge U_t=W_t-tW_1 in interval [0, 1]."}, {"input_sequence": "Author: Joanna Janczura, Awdesch Melzer ; # clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# gamma densities\nstep = 10\nx = (1:(8 * step))/step\ny1 = dgamma(x, shape = 1, scale = 2)\n# linear plot\nplot(x, y1, col = \"black\", type = \"l\", lwd = 2, xlab = \"x\", ylab = \"PDF(x)\", ylim = c(-0.01, 0.6))\ntitle(\"Gamma densities\")\nlines(x, y2, col = \"red3\", lty = 3, lwd = 2)\n# semi-logarithmic plot\ndev.new()\nplot(x, y1, log = \"y\", col = \"black\", type = \"l\", lwd = 2, xlab = \"x\", ylab = \"PDF(x)\", ylim = c(0.001, 1))\npar(new = T)\nplot(x, y2, type = \"l\", log = \"y\", axes = F, frame = F, ylab = \"\", xlab = \"\", col = \"red3\", lty = 3, lwd = 2, ylim = c(0.001,\n1))\nplot(x, y3, log = \"y\", type = \"l\", axes = F, frame = F, ylab = \"\", xlab = \"\", col = \"blue3\", lty = 2, lwd = 2, ylim = c(0.001,\n# log-normal densities\nx = (1:(25 * step))/step\ny1 = dlnorm(x, 2, 1)\ntitle(\"Log-normal densities\")\n", "output_sequence": "Plots three sample gamma pdfs, Gamma(alpha, beta), on linear and Semi-logarithmic scales. Note, that the first one (black solid line) is an exponential law, while the last one (dashed blue line) is a chi-square distribution with nu=6 degrees of freedom. It also plots three sample log-normal pdfs, LogN(mu,sigma), on linear and semi-logarithmic plots. For small sigma the log-normal distribution resembles the Gaussian."}, {"input_sequence": "Author: Joanna Janczura, Awdesch Melzer ; % clear variables and close windows\nclear all\nclc\nstep=10;\nx=(1:8*step)/step;\ny1=gampdf(x,1,2);\nfigure(1)\nplot(x,y1,'k','LineWidth',2);\nhold on\nplot(x,y2,':r','LineWidth',2);\nxlabel('x','FontSize',16,'FontWeight','Bold');\nset(gca,'Ytick',[0.0:0.1:0.6],'YTickLabel',[0.0:0.1:0.6],'FontSize',16,'FontWeight','Bold')\ntitle('Gamma densities','FontSize',16,'FontWeight','Bold');\nylim([-0.01 0.6]);\nset(gca,'LineWidth',1.6,'FontSize',16,'FontWeight','Bold');\nbox on\n% to save the plot in pdf or png please uncomment next 2 lines:\n%print -painters -dpdf -r600 STFloss04_01.pdf\nfigure(2)\nsemilogy(x,y1,'k','LineWidth',2);\nylim([10e-4, 10e-1]);\n%print -painters -dpdf -r600 STFloss04_02.pdf\nfigure(3)\nx=(1:25*step)/step;\ny1=lognpdf(x,2,1);\nplot(x,y1,'k' ,'LineWidth',2);\nxlim([0 25]);\nylim([-0.01 0.6])\ntitle('Log-normal densities','FontSize',16,'FontWeight','Bold');\nset(gca,'Xtick',[0:5:25],'XTickLabel',[0:5:25],'FontSize',16,'FontWeight','Bold')\n%print -painters -dpdf -r600 STFloss04_03.pdf\nfigure(4)\nsemilogy(x,y1,'k' ,'LineWidth',2);\nylim([10e-4 10e-1])\n%print -painters -dpdf -r600 STFloss04_04.pdf\n", "output_sequence": "Plots three sample gamma pdfs, Gamma(alpha, beta), on linear and Semi-logarithmic scales. Note, that the first one (black solid line) is an exponential law, while the last one (dashed blue line) is a chi-square distribution with nu=6 degrees of freedom. It also plots three sample log-normal pdfs, LogN(mu,sigma), on linear and semi-logarithmic plots. For small sigma the log-normal distribution resembles the Gaussian."}, {"input_sequence": "Author: Joanna Janczura, Awdesch Melzer ; # clear all variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\nmixexppdf = function(x, alpha, beta1, {\n# MIXEXPPDF Mixed exponential probability density function (pdf). Y = MIEXEXPPDF(X,ALPHA,BETA1,BETA2) returns the pdf of\n# the mixed exponential distribution with mixing probability A and distributions parameters BETA1, evaluated at the\n# values in X. For CONTROL=0 the error message is displayed, if the parmeters are negative or a>1. The default values for\n# A, BETA1, are 0.5, 1, 2.\nif (missing(alpha) == T)\nalpha = 0.5\nif (missing(beta1) == T)\nbeta1 = 1\nif (missing(beta2) == T)\nbeta2 = 2\nif (beta1 <= 0) {\nstop(\"Non-positive beta1! Please insert a positive beta1!\")\n}\nif (beta2 <= 0) {\nstop(\"Non-positive beta2! Please insert a positive beta2!\")\nif (alpha <= 0) {\nstop(\"Alpha lesser or equal 0! Please insert alpha between 0 and 1!\")\nif (alpha >= 1) {\nstop(\"Alpha greater or equal 1! Please insert alpha between 0 and 1!\")\n\ny = matrix(0, dim(x)[1],\npos = x > 0\ny[pos] = alpha * beta1 * exp(-beta1 * x[pos]) + (1 - alpha) * beta2 * exp(-beta2 * x[pos])\nreturn(cbind(y))\n}\nstep = 10\nx = cbind((1:(8 * step))/step)\ny1 = dexp(x, 1/3)\ny3 = mixexppdf(x, 0.5, 1)\nplot(x, y1, col = \"black\", type = \"l\", lwd = 2, xlab = \"x\", ylab = \"PDF(x)\", ylim = c(-0.01, 0.8))\ntitle(\"Mixture of two exponential densities\")\nlines(x, y2, col = \"red3\", lty = 3, lwd = 2)\ndev.new()\nplot(x, y1, log = \"y\", col = \"black\", type = \"l\", lwd = 2, xlab = \"x\", ylab = \"PDF(x)\", frame = T, ylim = c(0.01, 1))\npar(new = T)\nplot(x, y2, log = \"y\", axes = F, frame = F, col = \"red3\", type = \"l\", lty = 3, lwd = 2, ylab = \"\", xlab = \"\", ylim = c(0.01,\n1))\nplot(x, y3, log = \"y\", axes = F, frame = F, col = \"blue3\", type = \"l\", lty = 2, lwd = 2, ylab = \"\", xlab = \"\", ylim = c(0.01,\ntitle(\"Mixture of two exponential semi-log densities\")\n", "output_sequence": "Plots a probability density function (pdf) of a mixture of two exponential distributions with mixing parameter a = 0.5 superimposed on the pdfs of the component distributions as well as semi-logarithmic plots. In the semi-logarithmic plot, the exponential pdfs are straight lines with slopes -beta. Note, the curvature of the pdf of the mixture if two exponentials. Requires mixexppdf.m to run the program (see quantnet)."}, {"input_sequence": "Author: Joanna Janczura, Awdesch Melzer ; function y=mixexppdf(x,alpha,beta1,beta2,control)\n%MIXEXPPDF Mixed exponential probability density function (pdf).\n% Y = MIEXEXPPDF(X,ALPHA,BETA1,BETA2) returns the pdf of the mixed\n% exponential distribution with mixing probability A and distributions\n% parameters BETA1, evaluated at the values in X.\n% For CONTROL=0 the error message is displayed, if the parmeters are\n% negative or a>1.\n%\n% The default values for A, BETA1, and CONTROL are 0.5, 1, 2, 0\n% respectively.\nif nargin<5\ncontrol=0;\nend\nif nargin<2\na=.5;\nif nargin<3\nb1=1;\nif nargin<4\nb2=2;\nif(control==0)\nif beta1<=0\nerror('Non-positive beta1!');\nend\nif beta2<=0\nerror('Non-positive beta2!');\nend\nif alpha<=0\nerror('Alpha lesser or equal 0!');\nif alpha>=1\nerror('Alpha greater or equal 1!');\n\ny=zeros(size(x));\npos=find(x>0);\ny(pos)=alpha*beta1*exp(-beta1*x(pos))+(1-alpha)*beta2*exp(-beta2*x(pos));\n", "output_sequence": "Plots a probability density function (pdf) of a mixture of two exponential distributions with mixing parameter a = 0.5 superimposed on the pdfs of the component distributions as well as semi-logarithmic plots. In the semi-logarithmic plot, the exponential pdfs are straight lines with slopes -beta. Note, the curvature of the pdf of the mixture if two exponentials. Requires mixexppdf.m to run the program (see quantnet)."}, {"input_sequence": "Author: Joanna Janczura, Awdesch Melzer ; % clear all variables and close windows\nclear all\nclc\nstep = 10;\nx=(1:8*step)/step;\ny1 = exppdf(x,3);\ny3 = mixexppdf(x,0.5,0.3,1);\nfigure(1)\nplot(x,y1,'k','LineWidth',2);\nhold on\nplot(x,y2,':r','LineWidth',2);\nset(gca,'LineWidth',1.6,'FontSize',16,'FontWeight','Bold');\nbox on\nxlabel('x','FontSize',16,'FontWeight','Bold');\ntitle('Mixture of two exponential densities','FontSize',16,'FontWeight','Bold');\nset(gca,'Ytick',[0.0:0.2:1],'YTickLabel',{0.0,0.2,0.4,0.6,0.8,1.0},'FontSize',16,'FontWeight','Bold')\nylim([-0.01 0.8]);\n% to save the plot in pdf or png please uncomment next 2 lines:\n% print -painters -dpdf -r600 STFloss03_01.pdf\nfigure(2)\nsemilogy(x,y1,'k','LineWidth',2);\ntitle('Mixture of two exponential semi-log densities','FontSize',16,'FontWeight','Bold');\nylim([10e-3 10e-1]);\n% print -painters -dpdf -r600 STFloss03_02.pdf\n", "output_sequence": "Plots a probability density function (pdf) of a mixture of two exponential distributions with mixing parameter a = 0.5 superimposed on the pdfs of the component distributions as well as semi-logarithmic plots. In the semi-logarithmic plot, the exponential pdfs are straight lines with slopes -beta. Note, the curvature of the pdf of the mixture if two exponentials. Requires mixexppdf.m to run the program (see quantnet)."}, {"input_sequence": "Author: Zografia Anastasiadou, Awdesch Melzer ; function [y] = quantilelines(data,step,perc)\nif (size(step,1)~=1||step<=0)\nerror('quantiles: step must be a positive scalar.');\nend\nif (size(perc,2)~=1||size(perc,1)==1)\nerror('quantiles: perc must be n x 1 vector.');\nif(exist('perc')==0)\nperc =(1:9)/10;\nend\nN = size(data,2);\nbegin = data(1,1,1);\nnumofpoints = (theend-begin)/step+1;\nlast = begin+(numofpoints-1)*step;\nvecstep = (begin:step:last)';\ny = zeros(1,size(perc,1));\ni = 1;\nwhile(i<=numofpoints)\nj = 1;\nvecval = 0;\nwhile(j<=N)\naux1 = data(:,j,1);\npos = sum(aux1<=vecstep(i));\nif(pos<R)\nvecval=[vecval;aux2(pos)+(vecstep(i)-aux1(pos))*(aux2(pos+1)-aux2(pos))/(aux1(pos+1)-aux1(pos))];\nelse\nvecval=[vecval;aux2(pos)];\nend\nj = j + 1;\nend\ny = [y;(quantile(vecval(2:N+1),perc))'];\ni = i + 1;\ny = [vecstep,y(2:numofpoints+1,:)];\n", "output_sequence": "Produces the plot of an aggregate loss process together with a real-world trajectory, two sample quantile lines and mean of the process."}, {"input_sequence": "Author: Zografia Anastasiadou, Awdesch Melzer ; function [y] = simNHPPALP(lambda,parlambda,distrib,params,T,N)\n\nif(lambda ~= 0 && lambda ~= 1 && lambda~=2)\nerror('simNHPPALP: Lambda must be either 0,1 or 2.');\nend\nif(T <= 0 || (length(T))~=1)\nerror('simNHPPALP: T must be a positive scalar.');\nif(N <= 0 || (length(N))~=1)\nerror('simNHPPALP: N must be a positive scalar.');\nif(length(parlambda)~=3 && (lambda)~=1)\nerror('simNHPPALP: for lambda 0 or 2, parlambda must be a 3 x 1 vector.');\nif(length(parlambda)~=2 && (lambda)==1)\nerror('simNHPPALP: for lambda 1, parlambda must be a 2 x 1 vector.');\nif((strcmp(distrib,'Burr') || strcmp(distrib,'mixofexps')) && (length(params)~=3))\nerror('simNHPPALP: for Burr and mixofexps distributions, params must be a 3 x 1 vector.');\nif((strcmp(distrib,'gamma') || strcmp(distrib,'lognormal')|| || strcmp(distrib,'Weibull')) && (length(params))~=2)\nerror('simNHPPALP: for gamma, lognormal, Pareto and Weibull distributions, params must be a 2 x 1 vector.');\nif(strcmp(distrib,'exponential') && (length(params))~=1)\nerror('simNHPPALP: for exponential distribution, params must be a scalar.');\nif(strcmp(distrib, 'exponential')==0 && strcmp(distrib, 'gamma')==0 && strcmp(distrib, 'mixofexps')==0 && strcmp(distrib,'Weibull')==0 && strcmp(distrib, 'lognormal')==0 && strcmp(distrib,'Pareto')==0 && strcmp(distrib,'Burr')==0)\nerror('simNHPPALP: distribs should be: exponential, gamma, mixofexps, Weibull, lognormal, Pareto or Burr');\npoisproc = simNHPP(lambda,parlambda,T,N);\nrpp = size(poisproc,1);\nlosses = zeros(rpp,cpp);\nswitch distrib\ncase 'Burr'\ni = 1;\nwhile(i<=N)\naux = min(find(poisproc(:,i,1)==T,i,'first'));\nif(aux>2)\nlaux = cumsum(Burrrnd(params(1),params(2),params(3),aux/2-1,1));\nlosses(3:aux,i) = laux(ceil((1:(aux-2))/2));\nif(aux<rpp)\nlosses((aux+1):rpp,i) = laux(length(laux))*ones(rpp-aux,1);\nend\nelse\nlosses(:,i)=zeros(rpp,1);\nend\ni = i + 1;\nend\ncase'exponential'\naux = min(find(poisproc(:,i,1)==T,i,'first'));\nlaux = cumsum(exprnd(1/params(1),aux/2-1,1));\nlosses(3:aux,i) = laux(ceil((1:aux-2)/2));\ncase 'gamma'\nlaux = cumsum(gamrnd(params(1),(1/params(2)),aux/2-1,1));\nlosses(:,i) = zeros(rpp,1);\ncase 'lognormal'\nlaux = cumsum(lognrnd(params(1),params(2),aux/2-1,1));\ncase 'mixofexps'\nlaux = cumsum(mixexprnd(params(1),params(2),params(3),aux/2-1,1));\ncase 'Pareto'\nlaux = cumsum(Paretornd(params(1),params(2),aux/2-1,1));\ncase 'Weibull'\nlaux=cumsum(wblrnd(params(1)^(-1/params(2)),params(2),aux/2-1,1));\ny = zeros(size(poisproc));\ny(:,:,1) = poisproc(:,:,1);\ny(:,:,2) = losses;\nend\n", "output_sequence": "Produces the plot of an aggregate loss process together with a real-world trajectory, two sample quantile lines and mean of the process."}, {"input_sequence": "Author: Zografia Anastasiadou, Awdesch Melzer ; function y = simHPP(lambda,T,N)\n% SIMHPP Homogeneous Poisson process.\n% Y = SIMHPP(lambda,T,N) generates N trajectories of the\n% homogeneous Poisson process with intensity LAMBDA. T is the time\n% horizon.\nif lambda <= 0\nerror('simHPP: Lambda must be a positive real number');\nend\nif T <= 0\nerror('simHPP: T must be a positive real number');\nEN = poissrnd(lambda*T,N,1);\nmax(EN) ;\ny=T*ones(max(EN)+1,N);\ni=1;\nwhile(i<=N)\ny(1,i)=0;\nif EN(i)>0\ny(2:EN(i)+1,i)=sort(T*rand(EN(i),1));\nend\ni=i+1;\n", "output_sequence": "Produces the plot of an aggregate loss process together with a real-world trajectory, two sample quantile lines and mean of the process."}, {"input_sequence": "Author: Zografia Anastasiadou, Awdesch Melzer ; clear all\nclose all\nclc\nRandStream.setGlobalStream(RandStream('mt19937ar','seed',99));\nlambda = 0; % default Poisson process NHPP1\nparlambda = [35.32,2.32*2*pi,-0.2]; % NHPP1\ndistr = 'lognormal'; % default distribution\nparams = [18.3806,1.1052];\nT = 10; % max time (years)\nN = 100; % number of trajectories in NHPPALP, default 5000\nqu = [0.05,0.95]; % quantiles\nstep = 0.05; % step in quantile plot\n%=========== aggregate process - single realization\ny = simNHPPALP(lambda,parlambda,distr,params,T,1);\ny(:,:,2) = y(:,:,2)/1e+9;\n%=========== real PCS trajectory\nc = load('ncl.dat');\nt2 = c(ceil((1:2*size(c(:,2),1))/2),2);\nt2 = [0;t2;T];\nPCS = cumsum(c(:,3))/1e+9;\nPCS2 = PCS(ceil((1:2*size(PCS,1))/2));\nPCS2 = [0;0;PCS2];\nz = [t2,PCS2];\n%=========== mean of aggregate loss process (only for NHPP1 and lognormal loss size distribution)\nt = [(0:100*T)/100]';\nRP = exp(params(1)+params(2).^2/2).*(parlambda(1)*t-parlambda(2)/2/pi*(cos(2*pi*(t+parlambda(3))) - cos(2*pi*parlambda(3))));\nme = [t,RP/1e+9];\n%=========== quantiles\nvqu = quantilelines(simNHPPALP(lambda,parlambda,distr,params,T,N),step,qu');\n%=========== plot\nplot(y(:,:,1),y(:,:,2),'b-')\nhold on\nplot(me(:,1),me(:,2),'r-.')\nplot(z(:,1),z(:,2),'g-','LineWidth',1.5)\nplot(vqu(:,1),vqu(:,2)/1e+9, 'm--')\nxlim([0,10])\nxlabel('Years','FontSize',16,'FontWeight','bold')\nylabel('Aggregate loss process (USD billion)','FontSize',16,'FontWeight','bold')\nhold off\nbox on\nset(gca,'FontSize',16,'LineWidth',2,'FontWeight','bold');\n% print -painters -dpdf -r600 STFcat02.pdf\n", "output_sequence": "Produces the plot of an aggregate loss process together with a real-world trajectory, two sample quantile lines and mean of the process."}, {"input_sequence": "Author: Zografia Anastasiadou, Awdesch Melzer ; rm(list = ls(all = TRUE))\n# setwd('C:/...')\ninstall.packages(\"abind\")\nlibrary(abind)\n# generates a homogeneous Poisson process with intensity lambda\nsimHPP <- function(lambda, T, N) {\n# lambda: scalar, intensity of the Poisson process T: scalar, time horizon N: scalar, number of trajectories\nEN <- rpois(N, lambda * T)\ny <- matrix(T, nrow = 2 * max(EN) + 2, ncol = N) * matrix(1, nrow = 2 * max(EN) + 2, ncol = N)\nyy <- abind(y, matrix(1, nrow = 2 * max(EN) + 2, ncol = N) * EN, along = 3)\ni = 1\nwhile (i <= N) {\nif (EN[i] > 0) {\nyy[1:(2 * EN[i] + 1), i, 1] <- c(0, rep(sort(T * runif(EN[i])), each = 2))\n} else {\nyy[1, i, 1] = 0\n}\nyy[1:(2 * EN[i] + 2), i, 2] <- c(0, floor((1:(2 * EN[i]))/2), EN[i])\ni = i + 1\n}\nreturn(yy)\n}\n# generates a non-homogeneous Poisson process with intensity lambda\nsimNHPP <- function(lambda, parlambda, T, N) {\n# lambda: scalar, intensity function, sine function (lambda=0), linear function (lambda=1) or sine square function\n# (lambda=2) parlambda: n x 1 vector, parameters of the intensity function lambda (n=2 for lambda=1, n=3 otherwise) T:\n# scalar, time horizon N: scalar, number of trajectories\na <- parlambda[1]\nif (lambda == 0) {\nc <- parlambda[3]\nJM <- simHPP(a + b, T, N)\n} else {\nif (lambda == 1) {\nJM <- simHPP(a + b * T, N)\nif (lambda == 3) {\nJM <- simHPP(a + b * T, N)\n}\nrjm <- nrow(JM)\nyy <- abind(matrix(T, nrow = rjm, ncol = N), matrix(0, nrow = rjm, ncol = N), along = 3)\nmaxEN = 0\npom <- JM[, i, 1][JM[, i, 1] < T]\npom <- pom[2 * (1:(length(pom)/2))]\nR <- runif(NROW(pom))\nif (lambda == 0) {\nlambdat <- (a + b * sin(2 * pi * (pom + c)))/(a + b)\nif (lambda == 1) {\nlambdat <- (a + b * pom)/(a + b * T)\n} else {\nif (lambda == 3) {\nlambdat <- (a + b * sin(2 * pi * (pom + c))^2)/(a + b)\n}\npom <- pom[R < lambdat]\nEN <- NROW(pom)\nmaxEN <- max(maxEN, EN)\nyy[1:(2 * EN + 1), i, 1] <- c(0, rep(pom, each = 2))\nyy[2:(2 * EN), i, 2] <- c(floor((1:(2 * EN - 1))/2))\nyy[(2 * EN + 1):rjm, i, 2] <- matrix(EN, nrow = rjm - 2 * EN, ncol = 1)\nyy <- yy[1:(2 * maxEN + 2), , ]\n# generates aggregate loss process driven by the non-homogeneous Poisson process for lognormal distribution\nsimNHPPALP <- function(lambda, parlambda, params, T, N) {\n# (lambda=2) parlambda: n x 1 vector, parameters of the intensity function lambda (n=2 for lambda=1, n=3 otherwise) params:\n# n x 1 vector, parameters of the lognormal distribution # T: scalar, time horizon N: scalar, number of trajectories\nif (N == 1) {\npoisproc <- simNHPP(lambda, parlambda, T, N)\npoisproc <- abind(matrix(poisproc[, 1]), matrix(poisproc[, 2]), along = 3)\nrpp <- nrow(poisproc)\nlosses <- matrix(0, nrow = rpp, ncol = cpp)\naux <- min(which(poisproc[, i, 1] == T))\nif (aux > 2) {\nlaux <- cumsum(rlnorm(aux/2 - 1, params[1],\nlosses[3:aux, i] <- rep(laux, each = 2)\nif (aux < rpp) {\nlosses[(aux + 1):rpp, i] <- laux[NROW(laux)] * matrix(1, nrow = rpp - aux)\nlosses[, i] <- matrix(0, nrow = rpp)\ny <- abind(poisproc[, , 1], losses)\nreturn(y)\nset.seed(2)\nlambda1 <- 0 # intensity\nparlambda1 <- c(35.32, 2.32 * 2 * pi, -0.2) # parameters of intensity function\nparams1 <- c(18.3806, 1.1052) # parameters of the lognormal distribution\nT1 <- 10 # time\nN1 <- 1 # trajectories\nN2 <- 100\ny1 <- simNHPPALP(lambda1, parlambda1, T1, N1) # aggregate loss process for lognormal distribution\ny1[, 2] <- y1[, 2]/1e+09\nx <- read.table(\"ncl.dat\")\nt <- rep(x[, 2], each = 2)\nt1 <- c(0, t, T1)\nPCS <- cumsum(x[, 3])/1e+09\nPCS1 <- rep(PCS, each = 2)\nPCS1 <- c(0, 0, PCS1)\nz <- cbind(t1, PCS1)\n# mean of aggregate loss process\nt2 <- (0:(100 * T1))/100\nRP <- exp(params1[1] + params1[2]^2/2) * (parlambda1[1] * t2 - parlambda1[2]/2/pi * (cos(2 * pi * (t2 + parlambda1[3])) - cos(2 *\npi * parlambda1[3])))\nme <- cbind(t2, RP/1e+09)\n# computes quantiles of trajectories\nquantilelines <- function(data, step, perc) {\n# data: n x m x 2 array, data, where n is the length of trajectories and m the number of trajectories step: scalar, time\n# interval between points at which the quantiles are computed perc: s x 1 vector, orders of quantiles\nN <- ncol(data)\nbegin <- data[1, 1, 1]\nnumofpoints <- (theend - begin)/step + 1\nvecstep <- seq(begin, theend, step)\ny <- matrix(0, nrow = 1, ncol = NROW(perc))\nwhile (i <= numofpoints) {\nj = 1\nvecval = 0\nwhile (j <= N) {\naux1 = data[, j, 1]\npos = sum(aux1 <= vecstep[i])\nif (pos < R) {\nvecval <- c(vecval, aux2[pos] + (vecstep[i] - aux1[pos]) * (aux2[pos + 1] - aux2[pos])/(aux1[pos + 1] - aux1[pos]))\nvecval <- c(vecval, aux2[pos])\nj = j + 1\ny <- rbind(y, quantile(vecval[2:(N + 1)], perc))\ny = cbind(vecstep, y[2:(numofpoints + 1), ])\nstep1 <- 0.05 #time\nperc1 <- c(0.05, 0.95) #quantiles\n# quantiles of trajectories\nqq1 <- quantilelines(simNHPPALP(lambda1, parlambda1, T1, N2), step1, perc1[1]) # 0.05 quantile of trajectories\nqq1 <- cbind(qq1[, 1], qq1[, 2]/1e+09)\nplot(y1, type = \"l\", col = \"blue\", ylim = c(0, 120), xlab = \"Years\", ylab = \"Aggregate loss process (USD billion)\", cex.lab = 1.4,\ncex.axis = 1.4, lwd = 2)\nlines(me, col = \"red\", lty = 2, lwd = 2)\nlines(qq1[1:200, ], col = \"brown\", lty = 3, lwd = 2)\nabline(h = 60, lwd = 3)\n", "output_sequence": "Produces the plot of an aggregate loss process together with a real-world trajectory, two sample quantile lines and mean of the process."}, {"input_sequence": "Author: Zografia Anastasiadou, Awdesch Melzer ; function [y] = simNHPP(lambda,parlambda,T,N)\n\n% SIMNHPP Non-homogeneous Poisson process.\n% Y = SIMNHPP(lambda,parlambda,T,N) generates N trajectories of the\n% non-homogeneous Poisson process with intensity specified by LAMBDA\n% (0 - sine function, 1 - linear function, 2 - sine square function)\n% with paramters in PARLAMBDA. T is the time horizon. The function\n% usues thining method.\na = parlambda(1);\nif (a<=0)\nerror('simNHPP: a must be a positive real number');\nend\nif (a+b<= 0)\nerror('simNHPP: b does not fulfill special condition');\nif (T <= 0)\nerror('simNHPP: T must be a positive real number');\nif (lambda == 0)\nc = parlambda(3);\nJM = simHPP(a+b,T,N);\nelseif( lambda==1)\nJM = simHPP(a+b*T,T,N);\nelseif(lambda==2)\n\nrjm = size(JM,1);\ny = ones(rjm,N,2);\ny(:,:,1) = T*y(:,:,1);\nmaxEN = 0;\nwhile(i<=N)\nJMind = find(JM(:,i,1)<T);\npom = JM(JMind,i,1);\npoml = length(pom);\npom = pom(2*(1:(poml-1)/2));\nU = unifrnd(0,1,length(pom),1);\n\nif(lambda == 0)\nlambdat = (a+b*sin(2*pi*(pom+c)))/(a+b);\nelseif(lambda == 1)\nlambdat = (a+b*pom)/(a+b*T);\nelseif(lambda == 2)\nlambdat = (a+b*sin(2*pi*(pom+c)).^2)/(a+b);\nend\npomind = find(U<lambdat);\nEN = length(pom);\nmaxEN = max([maxEN,EN]);\ny(1:(2*EN+1),i,1) = [0;pom(ceil((1:(2*EN))/2))];\ny((2*EN+1):rjm,i,2) = EN*ones((rjm-2*EN),1);\ni = i+1;\ny = y(1:(2*maxEN+2),:,:);\n", "output_sequence": "Produces the plot of an aggregate loss process together with a real-world trajectory, two sample quantile lines and mean of the process."}, {"input_sequence": "Author: Janusz Miskiewicz, Awdesch Melzer ; # clear history\nrm(list = ls(all = TRUE))\ngraphics.off()\n########################################### Subroutine ultra(x) ############\nultra = function(x) {\n# Ultrametric distance between time series. x - time series matrix\nh = nrow(x)\nretval = sqrt(abs(0.5 * (matrix(1, k, k) - cor(x))))\nreturn(retval)\n}\n########################################### Subroutine umlp(x,y) ##########\numlp = function(x, y) {\n# unidirextional minimum length path algorithm x - the distance matrix y - root of the chain (the number of column) The\n# rsult is presentes as a set of links between nodes\n\nn = nrow(x)\nnet = matrix(0, n - 1, 3)\nlicz = 0\nonnet[y] = 1\nmaxx = 10 * max(x)\nsmax = maxx * diag(nrow = nrow(x), ncol = ncol(x))\nx = x + smax\nwhile (licz < n - 1) {\nminx = min(x[y, ])\nit = which(x[y, ] == minx, arr.ind = T)\n\nif (length(it) > 1) {\ntmp = 1\nwhile ((onnet[it[tmp]] == 1) && (tmp < length(it))) {\ntmp = tmp + 1\n}\nif (onnet[it[tmp]] == 0) {\nii = it(tmp)\n} else {\nii = it[1]\n}\nif (onnet[it] == 0) {\nlicz = licz + 1\nnet[licz, 1] = y\nx[y, it] = maxx\nif ((onnet[it] == 1) && (onnet[y] == 1)) {\n}\nretval = net\n########################################### Main calculation #############\ndata = read.table(\"gwp.csv\", header = T) # load data\ndata = diff(log(data)) # log return\ndl_szer = nrow(data)\npodmioty = ncol(data)\nczes = matrix(0, podmioty,\nwindow = 100 #time window size\n# moving time window\nwynik = matrix(0, dl_szer - window - 1, 5)\nfor (t in 1:(dl_szer - window - 1)) {\nwindow_data = data[t:(t + window - 1), ]\nwind_dist = ultra(window_data)\nwind_umlp = umlp(wind_dist, 5)\nfor (i in 1L:(podmioty - 1)) {\nif (wind_umlp[i, 1] < wind_umlp[i, 2]) {\nczes[wind_umlp[i, 1], wind_umlp[i, 2]] = czes[wind_umlp[i, 1], wind_umlp[i, 2]] + 1\n} else {\nczes[wind_umlp[i, 2], wind_umlp[i, 1]] = czes[wind_umlp[i, 2], wind_umlp[i, 1]] + 1\nwind_umlp = numeric()\ncompanies = c(\"WIG20 \", \"ACP \", \"BIO \", \"BRE \", \"BZW \", \"CEZ \", \"CPS \", \"GTN \", \"GTC \", \"KGH \", \"LTS \", \"PBG \", \"PEO \", \"PKN \",\n\"PKO \", \"PXM \", \"TPS \", \"TVN \")\nfreq = round(1e+05 * czes/sum(sum(czes)))/1000\nfreqTEMP = freq\nrownames(freqTEMP) = companies\nfreqTEMP\n\n", "output_sequence": "Presents the analysis of the network evolution. The procedure generate networks in moving time window and calculate the frequency of connections between elements of the chosen WIG 20 companies (gwp.csv). The unidirectional minimum length path is constructed. Requires umlp.m, ultra.m and {Statistical Toolbox} from Matlab to run the quantlet."}, {"input_sequence": "Author: Janusz Miskiewicz, Awdesch Melzer ; function retval=ultra(x)\n% Ultrametric distance between time series.\n% x - time series matrix\n%\n% Author: Janusz Mi\u015bkiewicz, email: jamis@ift.uni.wroc.pl\n[h,k]=size(x);\nretval= sqrt(abs(0.5*(ones(k)-corr(x))));\n", "output_sequence": "Presents the analysis of the network evolution. The procedure generate networks in moving time window and calculate the frequency of connections between elements of the chosen WIG 20 companies (gwp.csv). The unidirectional minimum length path is constructed. Requires umlp.m, ultra.m and {Statistical Toolbox} from Matlab to run the quantlet."}, {"input_sequence": "Author: Janusz Miskiewicz, Awdesch Melzer ; % clear history\nclear all\nclc\ndata = load('gwp.csv'); % load data\ndata = diff(log(data)); % log return\n[dl_szer,podmioty] = size(data);\nczes = zeros(podmioty);\nwindow = 100; %time window size\n% moving time window\nwynik = zeros(dl_szer - window-1,5);\nfor t=1:(dl_szer - window-1)\nwindow_data = data(t:(t+window-1),:);\nwind_dist = ultra(window_data);\nfor i=1:podmioty-1\nif (wind_umlp(i,1)<wind_umlp(i,2))\nczes(wind_umlp(i,1),wind_umlp(i,2))=czes(wind_umlp(i,1),wind_umlp(i,2))+1;\nelse\nczes(wind_umlp(i,2),wind_umlp(i,1))=czes(wind_umlp(i,2),wind_umlp(i,1))+1;\nend;\nend;\nwind_umlp = [];\nend;\ncompanies = [' ','WIG20 ','ACP ','BIO ','CEZ ','GTN ','KGH ','LTS ','PBG ','TPS '];\nfreq = round(100000*czes/sum(sum(czes)))/1000;\nfreqTEMP = [companies1(1:18,:),num2str(freq(:,:))];\ndisp(' WIG20 ACP BIO BRE BZW CEZ CPS GTN KGH LTS PBG PEO PKN PXM TPS TVN ')\ndisp(freqTEMP)\n", "output_sequence": "Presents the analysis of the network evolution. The procedure generate networks in moving time window and calculate the frequency of connections between elements of the chosen WIG 20 companies (gwp.csv). The unidirectional minimum length path is constructed. Requires umlp.m, ultra.m and {Statistical Toolbox} from Matlab to run the quantlet."}, {"input_sequence": "Author: Janusz Miskiewicz, Awdesch Melzer ; function retval = umlp(x,y)\n% unidirextional minimum length path algorithm\n% x - the distance matrix\n% y - root of the chain (the number of column)\n%\n% The rsult is presentes as a set of links between nodes\n% Author: Janusz Mi\u015bkiewicz, email: jamis@ift.uni.wroc.pl\n[n,m]=size(x);\nnet=zeros(n-1,3);\nlicz=0;\nonnet(y)=1;\nmaxx=10*max(max(x));\nsmax=maxx*eye(size(x));\nx=x+smax;\nwhile (licz<n-1)\nminx=min(x(y,:));\nit=find(x(y,:)==minx);\nif (length(it) > 1)\ntmp=1;\nwhile ((onnet(it(tmp))==1) && (tmp < length(it)))\ntmp=tmp+1;\nend;\nif (onnet(it(tmp))==0)\nii=it(tmp);\nit=[];\nelse\nii=it(1);\nend;\nif (onnet(it)==0 )\nlicz=licz+1;\nnet(licz,1)=y;\nx(it,y)=maxx;\ny=it;\n\nif ((onnet(it)==1) && (onnet(y)==1))\nend;\nretval=net;\n", "output_sequence": "Presents the analysis of the network evolution. The procedure generate networks in moving time window and calculate the frequency of connections between elements of the chosen WIG 20 companies (gwp.csv). The unidirectional minimum length path is constructed. Requires umlp.m, ultra.m and {Statistical Toolbox} from Matlab to run the quantlet."}, {"input_sequence": "Author: Awdesch Melzer ; rm(list = ls(all = TRUE))\ngraphics.off()\n########################## SUBROUTINES ##########################\nBondOnlyCoupon = function(C, D, T, r, lambda, parlambda, distr, params, Tmax, N) {\n# computes price of the CAT bond paying only coupons for the given claim amount distribution and the non-homogeneous Poisson\n# process governing the flow of losses ---------------------------------------------------------------------------- y =\n# BondOnlyCoupon(C,D,T,r,lambda,parlambda,distr,params,Tmax,N)\n# ---------------------------------------------------------------------------- Input: Parameter: C Definition: scalar,\n# coupon payments (cease at the threshold time or Tmax) Parameter: D Definition: n1 x 1 vector, threshold level Parameter: T\n# Definition: n2 x 1 vector, time to expiry Parameter: r Definition: scalar, continuously-compounded discount rate\n# Parameter: lambda Definition: scalar, intensity function, if lambda=0, a sine function if lambda=1, a linear function if\n# lambda=2, a sine square function Parameter: parlambda Definition: n x 1 vector, parameters of the intensity function\n# lambda (n=2 for lambda=1, n=3 otherwise) Parameter: distrib Definition: string, claim size distribution Parameter: params\n# Definition: n x 1 vector, parameters of the claim size distribution n = 1 (exponential) n = 2 (gamma, lognormal, Pareto,\n# Weibull) n = 3 (Burr, mixofexps) Parameter: Tmax Definition: scalar, time horizon Parameter: N Definition: scalar, number\n# of trajectories ---------------------------------------------------------------------------- Output: Parameter: y\n# Definition: m x 3 matrix, the first column are times to bond's expiration, the second threshold levels and the third\n# corresponding prices of the bond ----------------------------------------------------------------------------\n\nif (lambda != 0 && lambda != 1 && lambda != 2) {\nstop(\"BondOnlyCoupon: Lambda must be either 0,1 or 2.\")\n}\nif (length(C) != 1) {\nstop(\"BondOnlyCoupon: coupon payments C needs to be a scalar\")\nif (length(r) != 1) {\nstop(\"BondOnlyCoupon: discount rate needs to be a scalar\")\nif (length(D) == 1) {\nstop(\"BondOnlyCoupon: threshold level D needs to be a vector \")\nif (length(T) == 1) {\nstop(\"BondOnlyCoupon: time to expiry T needs to be a vector \")\nx = simNHPPALP(lambda, parlambda, distr, params, Tmax, N)\nTl = length(T)\ny = matrix(0, Tl * Dl, 3)\ni = 1 #loop (times to maturity)\nwyn = 0\nwhile (i <= Tl) {\nwhile (k <= N) {\ntraj = cbind(x[, k, 1], x[, k, 2])\nif (traj[length(traj[which(traj[, 1] <= T[i]), 1]), 2] <= D[j]) {\nwyn = wyn + (1 - exp(-r * T[i]))/r\n} else {\nwyn = wyn + (1 - exp(-r * traj[length(traj[which(traj[, 2] <= D[j])]), 1]))/r\n}\nk = k + 1\n}\ny[(i - 1) * Dl + j, 1] = T[i]\nwyn = 0\n}\nj = 1\nreturn(y)\n}\nBurrrnd = function(alpha, lambda, tau, n, m) {\n# BURRRND Random arrays from Burr distribution. --------------------------------------------------------------------- R =\n# BURRRND(ALPHA,LAMBDA,TAU,N,M) returns an M-by-N array of random numbers chosen from the Burr distribution with parameters\n# ALPHA, LAMBDA, TAU. --------------------------------------------------------------------- The default values for the\n# parameters ALPHA, LAMBDA, TAU, M, N are 1, 2, 1, respectively. BURRRND uses the inversion method.\n# ---------------------------------------------------------------------\nif (missing(m)) {\nm = 1\nif (missing(n)) {\nn = 1\nif (missing(tau)) {\ntau = 2\nif (missing(lambda)) {\nlambda = 1\nif (missing(alpha)) {\nalpha = 1\nu = matrix(0, n, m)\nfor (i in 1:m) {\nu[, i] = (lambda * (runif(n, 0, 1)^(-1/alpha) - 1))^(1/tau)\ny = u\nmixexprnd = function(p, beta1, n, m) {\n# MIXEXPRND Random arrays from the mixed exponential distribution.\n# --------------------------------------------------------------------- Y = MIXEXPRND(P,BETA1,BETA2,N,M) returns an M-by-N\n# array of random numbers chosen from the mixed exponential distribution with parameters P, BETA1,\n# --------------------------------------------------------------------- The default values for A, BETA1, N, M are\n# 0.5, 1, 2, 1, respectively. MIXEXPRND uses the exponential number generator.\nif (missing(p)) {\np = 0.5\nif (missing(beta1)) {\nbeta1 = 1\nif (missing(beta2)) {\nbeta2 = 2\ny = rexp(n * m, rate = (1/beta2))\naux = which(runif(n * m, 0, 1) <= p)\nif (!missing(aux)) {\ny[aux] = rexp(length(aux), 1/beta1)\ny = matrix(y, n, m)\nsimHPP = function(lambda, T, N) {\n# SIMHPP Homogeneous Poisson process. --------------------------------------------------------------------- Y =\n# SIMHPP(lambda,T,N) generates N trajectories of the homogeneous Poisson process with intensity LAMBDA. T is the time\n# horizon. ---------------------------------------------------------------------\nif (lambda <= 0 || length(lambda) != 1) {\nstop(\"simHPP: Lambda must be a positive scalar.\")\nif (T <= 0 || length(T) != 1) {\nstop(\"simHPP: T must be a positive scalar.\")\nif (N <= 0 || length(N) != 1) {\nstop(\"simHPP: N must be a positive scalar.\")\nEN = rpois(N, lambda * T)\nym = matrix(T, 2 * max(EN) + 2, N)\ny = tmp\ny[, , 1] = ym\ny[, , 2] = matrix(1, 2 * max(EN) + 2, 1) %*% t(EN)\ni = 1\nwhile (i <= N) {\nif (EN[i] > 0) {\nttmp = c(sort(T * runif(EN[i])))\ny[1:(2 * EN[i] + 1), i, 1] = c(0, ttmp[ceiling((1:(2 * EN[i]))/2)])\n} else {\ny[1, i, 1] = 0\ny[1:(2 * EN[i] + 2), i, 2] = c(0, floor((1:(2 * EN[i]))/2), EN[i])\nsimNHPP = function(lambda, parlambda, T, N) {\n# SIMNHPP Non-homogeneous Poisson process. --------------------------------------------------------------------- Y =\n# SIMNHPP(lambda,parlambda,T,N) generates N trajectories of the non-homogeneous Poisson process with intensity specified by\n# LAMBDA (0 - sine function, 1 - linear function, 2 - sine square function) with paramters in PARLAMBDA. T is the time\n# horizon. The function usues thining method. ---------------------------------------------------------------------\n# lambda: scalar, intensity function, sine function (lambda=0), linear function (lambda=1) or sine square function\n# (lambda=2) parlambda: n x 1 vector, parameters of the intensity function lambda (n=2 for lambda=1, n=3 otherwise) T:\n# scalar, time horizon N: scalar, number of trajectories\na = parlambda[1]\nif (lambda == 0) {\nd = parlambda[3]\nJM = simHPP(a + b, T, N)\n} else if (lambda == 1) {\nJM = simHPP(a + b * T, N)\n} else if (lambda == 2) {\nrjm = nrow(JM)\nyy = array(0, c(rjm, N, 2))\nyy[, , 1] = matrix(T, nrow = rjm, ncol = N)\nmaxEN = 0\npom = JM[, i, 1][JM[, i, 1] < T]\npom = pom[2 * (1:(length(pom)/2))]\nR = runif(NROW(pom))\nif (lambda == 0) {\nlambdat = (a + b * sin(2 * pi * (pom + d)))/(a + b)\nif (lambda == 1) {\nlambdat = (a + b * pom)/(a + b * T)\n} else {\nif (lambda == 2) {\nlambdat = (a + b * sin(2 * pi * (pom + d))^2)/(a + b)\npom = pom[R < lambdat]\nEN = NROW(pom)\nmaxEN = max(maxEN, EN)\nyy[1:(2 * EN + 1), i, 1] = c(0, rep(pom, each = 2))\nyy[2:(2 * EN), i, 2] = c(floor((1:(2 * EN - 1))/2))\nyy[(2 * EN + 1):rjm, i, 2] = matrix(EN, nrow = rjm - 2 * EN, ncol = 1)\nyy = yy[1:(2 * maxEN + 2), , ]\nreturn(yy)\nParetornd = function(alpha, lambda, n, m) {\n# PARETORND Random arrays from Pareto distribution. --------------------------------------------------------------------- Y\n# = PARETORND(ALPHA,LAMBDA,N,M) returns an M-by-N array of random numbers chosen from the Pareto distribution with\n# parameters ALPHA, LAMBDA. The default values for ALPHA, LAMBDA, N, M 1, respectively. PARETORND uses the\n# inversion method. ---------------------------------------------------------------------\nu[, i] = lambda * (runif(n, 0, 1)^(-1/alpha) - 1)\nsimNHPPALP = function(lambda, parlambda, distrib, params, T, N) {\n# generates aggregate loss process driven by the non-homogeneous Poisson process.\n# --------------------------------------------------------------------- y = simNHPPALP(lambda,parlambda,distrib,params,T,N)\n# array, generated process - max is the maximum number of jumps for all generated trajectories\n# --------------------------------------------------------------------- Input: Parameter: lambda Definition: scalar,\n# intensity function, sine function (lambda=0), linear function (lambda=1) or sine square function (lambda=2) Parameter:\n# parlambda Definition: n x 1 vector, parameters of the intensity function lambda (n=2 for lambda=1, n=3 otherwise)\n# Parameter: distrib Definition: string, claim size distribution Parameter: params Definition: n x 1 vector, parameters of\n# the claim size distribution n = 1 (exponential) n = 2 (gamma, lognormal, Pareto, Weibull) n = 3 (Burr, mixofexps)\n# Parameter: T Definition: scalar, time horizon Parameter: N Definition: scalar, number of trajectories\nstop(\"simNHPPALP: Lambda must be either 0,1 or 2.\")\nif (T <= 0 || (length(T)) != 1) {\nstop(\"simNHPPALP: T must be a positive scalar.\")\nif (N <= 0 || (length(N)) != 1) {\nstop(\"simNHPPALP: N must be a positive scalar.\")\nif (length(parlambda) != 3 && (lambda) != 1) {\nstop(\"simNHPPALP: for lambda 0 or 2, parlambda must be a 3 x 1 vector.\")\nif (length(parlambda) != 2 && (lambda) == 1) {\nstop(\"simNHPPALP: for lambda 1, parlambda must be a 2 x 1 vector.\")\nif ((distrib == \"Burr\" || distrib == \"mixofexps\") && (length(params)) != 3) {\nstop(\"simNHPPALP: for Burr and mixofexps distributions, params must be a 3 x 1 vector.\")\nif ((distrib == \"gamma\" || distrib == \"lognormal\" || distrib == \"Pareto\" || distrib == \"Weibull\") && (length(params)) !=\n2) {\nstop(\"simNHPPALP: for gamma, lognormal, Pareto and Weibull distributions, params must be a 2 x 1 vector.\")\nif (distrib == \"exponential\" && (length(params)) != 1) {\nstop(\"simNHPPALP: for exponential distribution, params must be a scalar.\")\nif (distrib != \"exponential\" && distrib != \"gamma\" && distrib != \"mixofexps\" && distrib != \"Weibull\" && distrib != \"lognormal\" &&\ndistrib != \"Pareto\" && distrib != \"Burr\") {\nstop(\"simNHPPALP: distribs should be: exponential, gamma, mixofexps, Weibull, lognormal, Pareto or Burr\")\npoisproc = simNHPP(lambda, parlambda, T, N)\nrpp = dim(poisproc)[1]\nlosses = matrix(0, rpp, cpp)\nif (distrib == \"Burr\") {\ni = 1\nwhile (i <= N) {\naux = min(as.matrix(which(poisproc[, 1] == T))) #[1:i,])\nif (aux > 2) {\nlaux = cumsum(Burrrnd(params[1], params[2], aux/2 - 1, 1))\nlosses[3:aux, i] = laux[ceiling((1:(aux - 2))/2)]\nif (aux < rpp) {\nlosses[(aux + 1):rpp, i] = matrix(laux[length(laux)], rpp - aux, 1)\nlosses[, i] = rep(0, rpp)\ni = i + 1\n} else if (distrib == \"exponential\") {\nlaux = cumsum(rexp(aux/2 - 1, rate = 1/params[1]))\nlosses[3:aux, i] = laux[ceiling((1:aux - 2)/2)]\n} else if (distrib == \"gamma\") {\nlaux = cumsum(rgamma(aux/2 - 1, shape = params[1], rate = params[2], scale = (1/params[2])))\n} else if (distrib == \"lognormal\") {\nlaux = cumsum(rlnorm(aux/2 - 1, meanlog = params[1], sdlog = params[2]))\n} else if (distrib == \"mixofexps\") {\nlaux = cumsum(mixexprnd(params[3], params[1], aux/2 - 1, 1))\n} else if (distrib == \"Pareto\") {\nlaux = cumsum(Paretornd(params[1], params[2], aux/2 - 1, 1))\n} else if (distrib == \"Weibull\") {\nlaux = cumsum(rweibull(aux/2 - 1, scale = params[1]^(-1/params[2]), shape = params[2]))\nif (N == 1) {\ny = array(0, dim(poisproc))\ny[, 1] = poisproc[, 1]\n} else {\ny[, , 1] = poisproc[, , 1]\ny[, , 2] = losses\n########################## MAIN PROGRAM ##########################\n# parlambda = 34.2 # HPP\nparlambda = c(35.32, 2.32 * 2 * pi, -0.2) # NHPP1\ndistr = \"Burr\" # 'lognormal'\n# params = c(18.3806,1.1052) # lognormal\nparams = c(0.4801, 3.9495 * 1e+16, 2.1524) # Burr\ndata = read.table(\"ncl.dat\")\nA = mean(data[, 3]) * (34.2/4)\nC = 0.06\nna = 41 # default 41\nD = seq(A, length = na, by = (12 * A - A)/(na - 1))\nB = 0.25\nnb = 41 # default 41\nT = seq(B, length = nb, by = (8 * B - B)/(nb - 1))\nTmax = max(T)\nlambda = 0\nN = 100 # default 1000\nr = log(1.025)\nd1 = BondOnlyCoupon(C, D, T, r, lambda, parlambda, distr, params, Tmax, N)\ny = d1[, 1]\nx = d1[, 2]/1e+09\nz = d1[, 3]\ndata = data.frame(cbind(x, y, z))\nrequire(lattice)\npar.set = list(axis.line = list(col = \"transparent\"), clip = list(panel = \"off\"))\nwireframe(z ~ x + y, data = data, screen = list(z = 30, x = -60), drape = TRUE, colorkey = F, ticktype = \"detailed\", scales = list(arrows = FALSE,\ncol = \"black\", distance = 1, tick.number = 8, cex = 0.7, x = list(labels = round(seq(min(x), max(x), length = 11),\ny = list(labels = round(seq(min(y), max(y), length = 11), z = list(labels = round(seq(min(z), max(z), length = 11),\n2))), xlab = list(\"\", rot = 30, cex = 1.2), ylab = list(\"\", rot = -40, cex = 1.2), zlab = list(\"\", rot = 95, cex = 1.1),\npar.settings = par.set)\n", "output_sequence": "Plots the CAT bond price, for the bond paying only coupons, for the Burr claim amounts and a non-homogeneous Poisson process governing the flow of losses."}, {"input_sequence": "Author: Awdesch Melzer ; function [y] = simNHPP(lambda,parlambda,T,N)\n\n% SIMNHPP Non-homogeneous Poisson process.\n% Y = SIMNHPP(lambda,parlambda,T,N) generates N trajectories of the\n% non-homogeneous Poisson process with intensity specified by LAMBDA\n% (0 - sine function, 1 - linear function, 2 - sine square function)\n% with paramters in PARLAMBDA. T is the time horizon. The function\n% usues thining method.\na = parlambda(1);\nif (a<=0)\nerror('simNHPP: a must be a positive real number');\nend\nif (a+b<= 0)\nerror('simNHPP: b does not fulfill special condition');\nif (T <= 0)\nerror('simNHPP: T must be a positive real number');\nif (lambda == 0)\nc = parlambda(3);\nJM = simHPP(a+b,T,N);\nelseif( lambda==1)\nJM = simHPP(a+b*T,T,N);\nelseif(lambda==2)\n\nrjm = size(JM,1);\ny = ones(rjm,N,2);\ny(:,:,1) = T*y(:,:,1);\nmaxEN = 0;\nwhile(i<=N)\nJMind = find(JM(:,i,1)<T);\npom = JM(JMind,i,1);\npoml = length(pom);\npom = pom(2*(1:(poml-1)/2));\nU = unifrnd(0,1,length(pom),1);\n\nif(lambda == 0)\nlambdat = (a+b*sin(2*pi*(pom+c)))/(a+b);\nelseif(lambda == 1)\nlambdat = (a+b*pom)/(a+b*T);\nelseif(lambda == 2)\nlambdat = (a+b*sin(2*pi*(pom+c)).^2)/(a+b);\nend\npomind = find(U<lambdat);\nEN = length(pom);\nmaxEN = max([maxEN,EN]);\ny(1:(2*EN+1),i,1) = [0;pom(ceil((1:(2*EN))/2))];\ny((2*EN+1):rjm,i,2) = EN*ones((rjm-2*EN),1);\ni = i+1;\ny = y(1:(2*maxEN+2),:,:);\n", "output_sequence": "Plots the CAT bond price, for the bond paying only coupons, for the Burr claim amounts and a non-homogeneous Poisson process governing the flow of losses."}, {"input_sequence": "Author: Awdesch Melzer ; close all\nclear all\nclc\n%parlambda = 34.2 % HPP\nparlambda = [35.32,2.32*2*pi,-0.2]; % NHPP1\ndistr = 'Burr'; % \"lognormal\"\n%params = c(18.3806,1.1052) % lognormal\nparams = [0.4801,3.9495*1e16,2.1524]; % Burr\ndata = load('ncl.dat');\nA = mean(data(:,3))*(34.2/4);\nC = 0.06;\nna = 41; % default 41\nAE = 12*A;\nD = A:(12*A-A)/(na-1):AE;\nB = 0.25;\nnb = 41; % default 41\nBE = 8*B;\nT = B:(8*B-B)/(nb-1):BE;\nTmax = max(T);\nlambda = 0;\nN = 100; % default 1000\nr = log(1.025);\nd1 = BondOnlyCoupon(C,D,T,r,lambda,parlambda,distr,params,Tmax,N);\ny = d1(:,1);\nyy = reshape(y,na,nb);\n% Plot\nmesh(xx,yy,zz)\nxlim([min(x),max(x)])\n\nset(gca,'FontSize',16,'LineWidth',2,'FontWeight','bold');\n% print -painters -dpdf -r600 STFcat05.pdf\n", "output_sequence": "Plots the CAT bond price, for the bond paying only coupons, for the Burr claim amounts and a non-homogeneous Poisson process governing the flow of losses."}, {"input_sequence": "Author: Awdesch Melzer ; function y = simHPP(lambda,T,N)\n% SIMHPP Homogeneous Poisson process.\n% Y = SIMHPP(lambda,T,N) generates N trajectories of the\n% homogeneous Poisson process with intensity LAMBDA. T is the time\n% horizon.\nif lambda <= 0\nerror('simHPP: Lambda must be a positive real number');\nend\nif T <= 0\nerror('simHPP: T must be a positive real number');\nEN = poissrnd(lambda*T,N,1);\nmax(EN) ;\ny=T*ones(max(EN)+1,N);\ni=1;\nwhile(i<=N)\ny(1,i)=0;\nif EN(i)>0\ny(2:EN(i)+1,i)=sort(T*rand(EN(i),1));\nend\ni=i+1;\n", "output_sequence": "Plots the CAT bond price, for the bond paying only coupons, for the Burr claim amounts and a non-homogeneous Poisson process governing the flow of losses."}, {"input_sequence": "Author: Awdesch Melzer ; function [y] = simNHPPALP(lambda,parlambda,distrib,params,T,N)\n\nif(lambda ~= 0 && lambda ~= 1 && lambda~=2)\nerror('simNHPPALP: Lambda must be either 0,1 or 2.');\nend\nif(T <= 0 || (length(T))~=1)\nerror('simNHPPALP: T must be a positive scalar.');\nif(N <= 0 || (length(N))~=1)\nerror('simNHPPALP: N must be a positive scalar.');\nif(length(parlambda)~=3 && (lambda)~=1)\nerror('simNHPPALP: for lambda 0 or 2, parlambda must be a 3 x 1 vector.');\nif(length(parlambda)~=2 && (lambda)==1)\nerror('simNHPPALP: for lambda 1, parlambda must be a 2 x 1 vector.');\nif((strcmp(distrib,'Burr') || strcmp(distrib,'mixofexps')) && (length(params)~=3))\nerror('simNHPPALP: for Burr and mixofexps distributions, params must be a 3 x 1 vector.');\nif((strcmp(distrib,'gamma') || strcmp(distrib,'lognormal')|| || strcmp(distrib,'Weibull')) && (length(params))~=2)\nerror('simNHPPALP: for gamma, lognormal, Pareto and Weibull distributions, params must be a 2 x 1 vector.');\nif(strcmp(distrib,'exponential') && (length(params))~=1)\nerror('simNHPPALP: for exponential distribution, params must be a scalar.');\nif(strcmp(distrib, 'exponential')==0 && strcmp(distrib, 'gamma')==0 && strcmp(distrib, 'mixofexps')==0 && strcmp(distrib,'Weibull')==0 && strcmp(distrib, 'lognormal')==0 && strcmp(distrib,'Pareto')==0 && strcmp(distrib,'Burr')==0)\nerror('simNHPPALP: distribs should be: exponential, gamma, mixofexps, Weibull, lognormal, Pareto or Burr');\npoisproc = simNHPP(lambda,parlambda,T,N);\nrpp = size(poisproc,1);\nlosses = zeros(rpp,cpp);\nswitch distrib\ncase 'Burr'\ni = 1;\nwhile(i<=N)\naux = min(find(poisproc(:,i,1)==T,i,'first'));\nif(aux>2)\nlaux = cumsum(Burrrnd(params(1),params(2),params(3),aux/2-1,1));\nlosses(3:aux,i) = laux(ceil((1:(aux-2))/2));\nif(aux<rpp)\nlosses((aux+1):rpp,i) = laux(length(laux))*ones(rpp-aux,1);\nend\nelse\nlosses(:,i)=zeros(rpp,1);\nend\ni = i + 1;\nend\ncase'exponential'\naux = min(find(poisproc(:,i,1)==T,i,'first'));\nlaux = cumsum(exprnd(1/params(1),aux/2-1,1));\nlosses(3:aux,i) = laux(ceil((1:aux-2)/2));\ncase 'gamma'\nlaux = cumsum(gamrnd(params(1),(1/params(2)),aux/2-1,1));\nlosses(:,i) = zeros(rpp,1);\ncase 'lognormal'\nlaux = cumsum(lognrnd(params(1),params(2),aux/2-1,1));\ncase 'mixofexps'\nlaux = cumsum(mixexprnd(params(1),params(2),params(3),aux/2-1,1));\ncase 'Pareto'\nlaux = cumsum(Paretornd(params(1),params(2),aux/2-1,1));\ncase 'Weibull'\nlaux=cumsum(wblrnd(params(1)^(-1/params(2)),params(2),aux/2-1,1));\ny = zeros(size(poisproc));\ny(:,:,1) = poisproc(:,:,1);\ny(:,:,2) = losses;\nend\n", "output_sequence": "Plots the CAT bond price, for the bond paying only coupons, for the Burr claim amounts and a non-homogeneous Poisson process governing the flow of losses."}, {"input_sequence": "Author: Awdesch Melzer ; function y=Burrrnd(alpha,lambda,tau,n,m)\n%BURRRND Random arrays from Burr distribution.\n% R = BURRRND(ALPHA,LAMBDA,TAU,N,M) returns an M-by-N array of random numbers\n% chosen from the Burr distribution with parameters ALPHA, LAMBDA, TAU.\n%\n% The default values for the parameters ALPHA, LAMBDA, TAU, M, N are\n% 1, 2, 1, respectively.\n% BURRRND uses the inversion method.\nif nargin<5\nm=1;\nend\nif nargin<4\nn=1;\nif nargin<3\ntau=2;\nif nargin<2\nlambda=1;\nif nargin<1\nalpha=1;\n\ny=(lambda*(rand(n,m).^(-1/alpha)-1)).^(1/tau);\n", "output_sequence": "Plots the CAT bond price, for the bond paying only coupons, for the Burr claim amounts and a non-homogeneous Poisson process governing the flow of losses."}, {"input_sequence": "Author: Awdesch Melzer ; function [y] = BondOnlyCoupon(C,D,T,r,lambda,parlambda,distr,params,Tmax,N)\nif(lambda ~= 0 && lambda ~= 1 && lambda~=2)\nerror('BondOnlyCoupon: Lambda must be either 0,1 or 2.');\nend\nif(length(C) ~=1)\nerror('BondOnlyCoupon: coupon payments C needs to be a scalar');\nif(length(r) ~=1)\nerror('BondOnlyCoupon: discount rate needs to be a scalar');\nif(length(D)==1)\nerror('BondOnlyCoupon: threshold level D needs to be a vector ');\nif(length(T)==1)\nerror('BondOnlyCoupon: time to expiry T needs to be a vector ');\nx = simNHPPALP(lambda,parlambda,distr,params,Tmax,N);\nTl = length(T);\ny = zeros(Tl*Dl,3);\ni = 1; %loop (times to maturity)\nwyn = 0;\nwhile(i<=Tl)\nwhile(k<=N)\ntraj = [x(:,k,1),x(:,k,2)];\nif (traj(length(traj(find(traj(:,1)<=T(i)),1)),2)<=D(j))\nwyn = wyn + (1-exp(-r*T(i)))/r;\nelse\nwyn = wyn + (1-exp(-r*traj(length(traj(find(traj(:,2)<=D(j)))),1)))/r;\nend\nk = k + 1;\nend\ny((i-1)*Dl+j,1) = T(i);\nwyn = 0;\nj = j + 1;\nend\nj = 1;\ni = i + 1;\nend\n", "output_sequence": "Plots the CAT bond price, for the bond paying only coupons, for the Burr claim amounts and a non-homogeneous Poisson process governing the flow of losses."}, {"input_sequence": "Author: Pavel Cizek, Wolfgang K. Haerdle, Rafal Weron ; # Clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# Please change working directory setwd('C:/...')\ndata <- read.delim2(\"SP1997-2005s.txt\")\ntime <- (1:length(data[, 1]))\ndat0 <- data[, 1] - c(mean(data[, 1]))\ndat0 <- dat0/sd(dat0)\ntimet <- (time - 1078)/250 + 2001\nplot(timet[time >= 1075], dat0[time >= 1075], xaxp = c(2001, 2005, 4), xlab = \"Time\", ylab = \"Log-returns\", type = \"l\")\n", "output_sequence": "Plots the daily log-returns of S&P 500 in the years  2001 to 2005."}, {"input_sequence": "Author: Zografia Anastasiadou, Rafal Weron ; function [x,y]=stabpdf_fft(alpha,sigma,beta,mu,xmax,n,S0,mult)\n%STABPDF_FFT Stable probability density function (pdf) via FFT.\n% [X,Y] = STABPDF_FFT(ALPHA,SIGMA,BETA,MU,XMAX,N) returns the stable pdf\n% ((X,Y) pairs) with characteristic exponent, ALPHA, scale, SIGMA,\n% skewness, BETA, and location MU at 2^N evenly spaced values in\n% [-XMAX,XMAX] in the S0 parametrization. Default values for XMAX and N\n% are 20 and 12 respectively.\n% [X,Y] = STABPDF_FFT(ALPHA,SIGMA,BETA,MU,XMAX,N,0) returns the stable\n% pdf in the S parametrization.\n%\n% For .999 < ALPHA < 1.001 the stable pdf is calculated using the formula\n% for ALPHA = 1, otherwise numerical errors creep in.\n% Due to the nature of FFT, values away from the center may be underestimated.\n% For this reason STABPDF_FFT calculates the stable pdf on the interval\n% [-XMAX,XMAX]*2^MULT and then truncates it to the original interval.\n% The default value of MULT is 4, however, for better accuracy use MULT>4.\n% The full syntax is [X,Y] = STABPDF_FFT(ALPHA,SIGMA,BETA,XMAX,N,PARAM,MULT).\n% Reference(s):\n% [1] R.Weron (2004) \"Computationally intensive Value at Risk\n% calculations\", in \"Handbook of Computational Statistics: Concepts and\n% Methods\", eds. J.E. Gentle, W. Haerdle, Y. Mori, Springer, Berlin,\n% 911-950.\n% Copyright (c) 1996-2009 by The Hugo Steinhaus Center\n% 1996.07.01 Rafal Weron\n% 2009.03.27 Rafal Weron\nif nargin < 8,\nmult = 4;\nend\nif nargin < 7,\nS0 = 1;\nif nargin < 6,\nn = 12;\nif nargin < 5;\nxmax = 20;\n% calculate pdf on a twice larger interval\nn = n+mult;\nxmax = xmax*(2^mult);\nM = 2^n;\nR = pi/xmax;\ndt = 1/(R*M);\n\nxx = (-2^(n-1)+.5:(2^(n-1)-.5))/(2^n*dt);\n% stable characteristic function\nif abs(alpha-1) < .001,\nif S0,\nmu = mu - beta*sigma*2/pi*log(sigma);\nend\nyy = exp(-sigma*(abs(xx)).*(1+i*beta.*sign(xx).*(2/pi).*log((abs(xx)))) + i*mu*xx);\nelse\nif S0,\nmu = mu - beta*sigma*tan(0.5*pi*alpha);\nyy = exp(-sigma^alpha.*(abs(xx)).^alpha.*(1-i*beta.*sign(xx).*tan((pi.*alpha)/2)) + i*mu*xx);\nend;\n% FFT\nyy1 = [yy((2^(n-1)+1):2^n), yy(1:2^(n-1))];\nz = real( fft(yy1) )/(2*pi)*R;\n% stable density\nx = (2*pi)*((0:1:(M-1))/(M*R)-1/(2*R));\ny = [z((2^(n-1)+1):2^n), z(1:2^(n-1))];\n% shrink to the original interval\nT = find((x<=xmax/(2^mult)) & (x>=-xmax/(2^mult)));\nx = x(T);\n", "output_sequence": "STFstab02.R creates a) a plot of stable probability density function for different betas b) a plot of probability density functions of Gaussian, Cauchy, Levy distributions. STFstab02.m shows a semi-logarithmic plot of symmetric (beta=mu=0) stable densities for four values of alpha and a plot of stable densities for alpha = 1.2 and four values of beta. Requires the \"stabpdf_fft.m\" function."}, {"input_sequence": "Author: Zografia Anastasiadou, Rafal Weron ; % clear variables and close windows\nclear all\nclc\ncmd = [1 2];\nif ismember(2,cmd),\nf = figure(2);\nsubplot(1,2,1)\n[x1,y1] = stabpdf_fft(2,1,0,0,6);\nsemilogy(x1,y1,'b-','linewidth',2)\nhold on\nsemilogy(x2,y2,'-.','color',[0 .5 0],'linewidth',1)\nhold off\nxlabel('x')\nlegend('alpha=2','alpha=1.9','alpha=1.5','alpha=0.5','Location','South')\nset(gca,'ylim',[1e-4 .8]);\nsubplot(1,2,2)\n[x1,y1] = stabpdf_fft(1.2,2,0,0,5);\nplot(x1,y1,'b-','linewidth',2)\nplot(x2,y2,'-.','color',[0 .5 0],'linewidth',1)\nplot(x3,y3,'r--','linewidth',1)\nlegend('beta=0','beta=-1','beta=0.5','beta=1','Location','South')\nset(gca,'xtick',-5:2.5:5,'ylim',[0 .16])\n\nprint(f,'-dpsc2','STF2stab02.ps')\n", "output_sequence": "STFstab02.R creates a) a plot of stable probability density function for different betas b) a plot of probability density functions of Gaussian, Cauchy, Levy distributions. STFstab02.m shows a semi-logarithmic plot of symmetric (beta=mu=0) stable densities for four values of alpha and a plot of stable densities for alpha = 1.2 and four values of beta. Requires the \"stabpdf_fft.m\" function."}, {"input_sequence": "Author: Zografia Anastasiadou, Rafal Weron ; rm(list = ls(all = TRUE))\ngraphics.off()\n# setwd('C:/...')\ninstall.packages(\"fBasics\")\nlibrary(stabledist)\nlibrary(fBasics)\nx <- c(-50:50)/10\nalpha <- 1.2\nbeta <- c(0, 0.5, 1)\n# stable pdfs\nw1 <- dstable(x, alpha, beta = beta[1], pm = 1)\nplot(x, w1, type = \"l\", main = \"Dependence on beta\", xlab = \"x\", ylab = \"PDF(x)\", cex.axis = 2, cex.lab = 1.4, cex.main = 2,\nlwd = 3)\nlines(x, w2, col = \"red\", lwd = 3, lty = 3)\n###################################\n# Gaussian, Cauchy, Levy pdfs\nw5 <- dnorm(x)\nw6 <- dcauchy(x, location = 0, scale = 1)\nw7 <- matrix(0, length(x))\nfor (i in 1:length(x)) {\nif (x[i] > 0) {\nw7[i] <- dstable(x[i], alpha = 0.5, beta = 1, pm = 1)\n}\n}\ndev.new()\nplot(x, w7, type = \"l\", main = \"Gaussian, Cauchy and Levy distributions\", xlab = \"x\", ylab = \"PDF(x)\", cex.axis = 2, cex.lab = 1.4,\ncex.main = 2, col = \"blue\", lwd = 3, lty = 2)\nlines(x, w5, lwd = 3)\nlines(x, w6, col = \"red\", lwd = 3, lty = 3)\n", "output_sequence": "STFstab02.R creates a) a plot of stable probability density function for different betas b) a plot of probability density functions of Gaussian, Cauchy, Levy distributions. STFstab02.m shows a semi-logarithmic plot of symmetric (beta=mu=0) stable densities for four values of alpha and a plot of stable densities for alpha = 1.2 and four values of beta. Requires the \"stabpdf_fft.m\" function."}, {"input_sequence": "Author: Joanna Janczura, Awdesch Melzer ; % ---------------------------------------------------------------------\n% clear variables and close windows\nclear all\nclc\nstep=20;\n% Burr densities\nx=(1:144*step)/step;\ny1=Burrpdf(x,0.5,2,1.5);\nfigure(1)\nplot(x,y1,'k','LineWidth',2);\nhold on\nplot(x,y2,':r','LineWidth',2);\nxlim([0,8])\nxlabel('x','FontSize',16,'FontWeight','Bold');\ntitle('Burr densities','FontSize',16,'FontWeight','Bold');\nylim([0 1.2]);\nset(gca,'LineWidth',1.6,'FontSize',16,'FontWeight','Bold');\nbox on\n% to save the plot in pdf or png please uncomment next 2 lines:\nprint -painters -dpdf -r600 STFloss06_01.pdf\n% Weibull densities\ny1=wblpdf(x,1.^(-1./0.5),0.5);\nfigure(2)\nxlim([0,5])\ntitle('Weibull densities','FontSize',16,'FontWeight','Bold');\nylim([-0.01 1.2]);\nprint -painters -dpdf -r600 STFloss06_02.pdf\n% Burr double-logarithmic densities\nfigure(3)\nloglog(x,y1,'k','LineWidth',2);\nylim([10e-5, 10e-1]);\n%print -painters -dpdf -r600 STFloss06_03.pdf\n% Weibull double-logarithmic densities\nfigure(4)\nsemilogy(x,y1,'k','LineWidth',2);\nylim([10e-4 10e-1]);\n%print -painters -dpdf -r600 STFloss06_04.pdf\n", "output_sequence": "Plots three sample Burr pdfs, Burr(alpha, lambda, tau), on linear and double-logarithmic scales and three sample Weibull pdfs, Weib(beta, tau), on a linear and semi-logarithmic scales. We can see that for tau<1 the tails are much heavier and they look like power-law. Requires Burrpdf.m to run the program (see quantnet)."}, {"input_sequence": "Author: Joanna Janczura, Awdesch Melzer ; # clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\nBurrpdf = function(x, alpha, lambda, tau) {\n# BURRPDF Burr probability density function (pdf). Y = BURRPDF(X,ALPHA,LAMBDA,TAU) returns the pdf of the Burr distribution\n# with parameters ALPHA,LAMBDA,TAU, evaluated at the values in X. For CONTROL=0 the error message is displayed, if the\n# parmeters are negative. The default values for the parameters ALPHA, LAMBDA, TAU, CONTROL are 1, 2, 0, respectively.\nif (missing(tau) == TRUE) {\ntau = 2\n}\nif (missing(lambda) == TRUE) {\nlambda = 1\nif (missing(alpha) == TRUE) {\nalpha = 1\nif (missing(x) == TRUE) {\nstop(\"stats:normpdf:TooFewInputs! Input argument X is undefined.\")\nif (lambda <= 0) {\nstop(\"Non-positive lambda!\")\nif (alpha <= 0) {\nstop(\"Non-positive alpha!\")\n\nx = cbind(x)\ny = matrix(0, dim(x)[1],\npos = x > 0\ny[pos] = tau * alpha * lambda^alpha * x[pos]^(tau - 1) * (lambda + x[pos]^tau)^(-alpha - 1)\n}\nstep = 20\n# Burr densities\nx = (1:(144 * step))/step\ny1 = Burrpdf(x, 0.5, 2, 1.5)\n# Burr linear plot\nplot(x, y1, col = \"black\", type = \"l\", lwd = 2, xlab = \"x\", ylab = \"PDF(x)\", ylim = c(0, 1.2), xlim = c(0, 8))\ntitle(\"Burr densities\")\nlines(x, y2, col = \"red3\", lty = 3, lwd = 2)\n# Burr double-logarithmic densities\ndev.new()\nplot(x, y1, log = \"xy\", col = \"black\", type = \"l\", lwd = 2, xlab = \"x\", ylab = \"PDF(x)\", ylim = c(1e-04, 1), xlim = c((0.1),\n(100)))\npar(new = T)\nplot(x, y2, type = \"l\", log = \"xy\", axes = F, frame = F, ylab = \"\", xlab = \"\", col = \"red3\", lty = 3, lwd = 2, ylim = c(1e-04,\n1), xlim = c((0.1), (100)))\nplot(x, y3, type = \"l\", log = \"xy\", axes = F, frame = F, ylab = \"\", xlab = \"\", col = \"blue3\", lty = 2, , lwd = 2, ylim = c(1e-04,\n# Weibull densities\ny1 = dweibull(x, shape = 0.5, scale = (1^(-1/0.5)))\n# Weibull linear plot\nplot(x, y1, col = \"black\", type = \"l\", lwd = 2, xlab = \"x\", ylab = \"PDF(x)\", ylim = c(0, 1.2), xlim = c(0, 5))\ntitle(\"Weibull densities\")\n# Weibull semi-logarithmic plot\nplot(x, y1, log = \"y\", col = \"black\", type = \"l\", lwd = 2, xlab = \"x\", ylab = \"PDF(x)\", ylim = c(0.001, 1), xlim = c(0, 5))\nplot(x, y2, type = \"l\", log = \"y\", axes = F, frame = F, ylab = \"\", xlab = \"\", col = \"red3\", lty = 3, lwd = 2, ylim = c(0.001,\n1), xlim = c(0, 5))\nplot(x, y3, type = \"l\", log = \"y\", axes = F, frame = F, ylab = \"\", xlab = \"\", col = \"blue3\", lty = 2, , lwd = 2, ylim = c(0.001,\ntitle(\"Weibull densities\")\n", "output_sequence": "Plots three sample Burr pdfs, Burr(alpha, lambda, tau), on linear and double-logarithmic scales and three sample Weibull pdfs, Weib(beta, tau), on a linear and semi-logarithmic scales. We can see that for tau<1 the tails are much heavier and they look like power-law. Requires Burrpdf.m to run the program (see quantnet)."}, {"input_sequence": "Author: Joanna Janczura, Awdesch Melzer ; function y=Burrpdf(x,alpha,lambda,tau)\n%BURRPDF Burr probability density function (pdf).\n% Y = BURRPDF(X,ALPHA,LAMBDA,TAU) returns the pdf of the Burr\n% distribution with parameters ALPHA,LAMBDA,TAU, evaluated at the values in X.\n% For CONTROL=0 the error message is displayed, if the parmeters are\n% negative.\n%\n% The default values for the parameters ALPHA, LAMBDA, TAU, CONTROL are\n% 1, 2, 0, respectively.\nif nargin<4\ntau=2;\nend\nif nargin<3\nlambda=1;\nend\nif nargin<2\nalpha=1;\nif nargin<1\nerror('stats:normpdf:TooFewInputs','Input argument X is undefined.');\nif tau<=0\nerror('Non-positive tau!');\nif lambda<=0\nerror('Non-positive sigma!');\nif alpha<=0\nerror('Non-positive alpha!');\ny=zeros(size(x));\npos=find(x>0);\ny(pos)=tau*alpha*lambda^alpha*x(pos).^(tau-1).*(lambda+x(pos).^tau).^(-alpha-1);\n", "output_sequence": "Plots three sample Burr pdfs, Burr(alpha, lambda, tau), on linear and double-logarithmic scales and three sample Weibull pdfs, Weib(beta, tau), on a linear and semi-logarithmic scales. We can see that for tau<1 the tails are much heavier and they look like power-law. Requires Burrpdf.m to run the program (see quantnet)."}, {"input_sequence": "Author: Rafal Weron, Szymon Borak ; function [alpha,sigma,beta,mu]=stabcull(x)\n%STABCULL Quantile parameter estimates of a stable distribution.\n% [ALPHA,SIGMA,BETA,MU]=STABCULL(X) returns the estimated (McCulloch's\n% method) parameters ALPHA, SIGMA, BETA, MU of a stable distribution\n% vector X.\n%\n% Reference(s):\n% [1] J.H.McCulloch (1986) \"Simple Consistent Estimators of Stable\n% Distribution Parameters\", Commun. Statist. - Simul. 15(4) 1109-1136\n% [2] R.Weron (2004) \"Computationally intensive Value at Risk\n% calculations\", in \"Handbook of Computational Statistics: Concepts and\n% Methods\", eds. J.E. Gentle, W. Haerdle, Y. Mori, Springer, Berlin,\n% 911-950.\n% Written by Szymon Borak and Rafal Weron (2000.12.15, rev. 2002.12.04)\n% Copyright (c) 2000-2006 by Rafal Weron\n% Compute quantiles\nx = sort(x);\nx05 = prctile(x,5);\n% Compute quantile statistics\nva = (x95-x05)./(x75-x25);\nvs = x75-x25;\n% Define interpolation matrices (see [1])\ntva = [2.439 2.5 3.0 10.0 25.0];\ntvb = [0.0, 0.1, 1.0];\nta = [2.0 1.9 0.8 0.5];\ntb = [0.0, 0.25, 1.0];\npsi1 = [2.000,\n1.916, 1.924;\n0.896, 0.769;\npsi2 = [0.000, 2.160, 1.000,\n0.000, 0.136, 0.271, 0.404, 0.689, 1.230, 2.195;\npsi3 = [1.908,\n1.933, 2.043;\n2.000, 2.311, 2.624, 2.973;\n2.337, 3.542, 4.808, 6.247;\n2.588, 3.073, 4.534, 6.636, 9.144];\n\npsi4 = [0.0, 0.0;\n0.0, -0.017, -0.064;\n% Compute estimates by interpolationg through the tables\n[xrow,xcol] = size(x);\nif (xrow == 1), xcol = 1; end;\nfor n = 1:xcol,\ntvai1 = max([1 find(tva <= va(n))]);\ndista = (tva(tvai2)-tva(tvai1));\nif dista ~= 0,\ndista = (va(n)-tva(tvai1))/dista;\nend;\ndistb = (tvb(tvbi2)-tvb(tvbi1));\nif distb ~= 0,\ndistb = (abs(vb(n))-tvb(tvbi1))/distb;\npsi1b1 = dista*psi1(tvai2,tvbi1)+(1-dista)*psi1(tvai1,tvbi1);\nalpha(n) = distb*psi1b2+(1-distb)*psi1b1;\npsi2b1 = dista*psi2(tvai2,tvbi1)+(1-dista)*psi2(tvai1,tvbi1);\nbeta(n) = sign(vb(n))*(distb*psi2b2+(1-distb)*psi2b1);\ntai1 = max([1 find(ta >= alpha(n))]);\ndista = (ta(tai2)-ta(tai1));\nif dista ~= 0,\ndista = (alpha(n)-ta(tai1))/dista;\ndistb = (tb(tbi2)-tb(tbi1));\ndistb = (abs(beta(n))-tb(tbi1))/distb;\npsi3b1 = dista*psi3(tai2,tbi1)+(1-dista)*psi3(tai1,tbi1);\nsigma(n) = vs(n)/(distb*psi3b2+(1-distb)*psi3b1);\npsi4b1 = dista*psi4(tai2,tbi1)+(1-dista)*psi4(tai1,tbi1);\nzeta = sign(beta(n))*sigma(n)*(distb*psi4b2+(1-distb)*psi4b1) + x50 ;\nif (abs(alpha(n)-1) < 0.05 )\nmu(n) = zeta;\nelse\nmu(n) = zeta - beta(n)* sigma(n) * tan(0.5 * pi *alpha(n));\nend;\n% Correct estimates for out of range values\nalpha(alpha <= 0) = 10^(-10)+0*alpha(alpha <= 0);\nbeta(beta < -1) = -1+0*beta(beta < -1);\n", "output_sequence": "Gives the quantile parameter estimates of a stable distribution. Method is based on J.H.McCulloch (1986) 'Simple Consistent Estimators of Stable Distribution Parameters'"}, {"input_sequence": "Author: Awdesch Melzer ; Paretornd = function(alpha,lambda,n,m){\n#PARETORND Random arrays from Pareto distribution.\n# Y = PARETORND(ALPHA,LAMBDA,N,M) returns an M-by-N array of random numbers\n# chosen from the Pareto distribution with parameters ALPHA, LAMBDA.\n#\n# The default values for ALPHA, LAMBDA, N, M 1, respectively.\n# PARETORND uses the inversion method.\nif (missing(m)){\nm = 1\n}\nif (missing(n)){\nn = 1\nif (missing(lambda)){\nlambda = 1\nif (missing(alpha)){\nalpha = 1\nu = matrix(0,n,m)\nfor (i in 1:m){\nu[,i] = lambda*(runif(n,0,1)^(-1/alpha)-1)\ny = u\nreturn(y)\n", "output_sequence": "Generates a vector of pseudo random variables coming from a Pareto distribution."}, {"input_sequence": "Author: Rafal Weron ; function x = simGBM(n,x0,mu,sigma,delta,no,method)\n%\n% X = SIMGBM(N,X0,MU,SIGMA,DELTA,NO,METHOD) returns a vector of a sample\n% trajectory of Geometric Brownian Motion on the time interval [0,N]:\n% dX(t) = MU*X(t)*dt + SIGMA*X(t)*dW(t),\n%\n% time step size DELTA, array of normally distributed pseudorandom\n% numbers NO (array NO is simulated if not provided as an input variable)\n% and method:\n% METHOD = 0 (default) - direct integration\n% METHOD = 1 - Euler scheme\n% METHOD = 3 - 2nd order Milstein scheme\n%\n% Sample use:\n% >> simGBM(1,.84,.02,sqrt(.1),1/100,normrnd(0,1,100,1),1)\n% Reference(s):\n% [1] P.E.Kloeden, E.Platen (1995) Numerical Solution\n% of Stochastic Differential Equations, Springer.\n% [2] R.Korn, G.Kroisandt (2010) Monte Carlo Methods and\n% Models in Finance and Insurance, CRC Press.\n% Written by Rafal Weron (2004.05.21)\n% Revised and added functionality by Rafal Weron (2010.12.27)\n% Set default value of 'method'\nif (nargin<7)\nmethod = 0; %direct integration\nend\n% Generate normal random numbers if not provided in 'no'\nif (nargin<6)\nno = normrnd(0,1,ceil(n/delta),1);\n% Check whether length of 'no' is appropriate\nelse\nno = no(:);\nif (length(no) ~= ceil(n/delta))\nerror ('Error: length(no) <> n/delta');\nend\n\nswitch method\ncase 1 % Euler scheme\nx = x0*cumprod(1 + mu*delta + sigma.*delta^0.5.*no);\ncase 2 % Milstein scheme\nx = x0*cumprod(1 + mu*delta + sigma*delta^0.5.*no ...\n+ 0.5*sigma^2*delta*(no.^2-1));\ncase 3 % 2nd order Milstein scheme\n+ 0.5*sigma^2*delta*(no.^2-1) ...\n+ mu*sigma.*no.*(delta^1.5) + 0.5*(mu^2).*(delta^2));\notherwise % Direct integration\nx = x0*exp(cumsum((mu - 0.5*sigma^2)*delta + sigma.*delta^0.5.*no));\n% Add starting value\n", "output_sequence": "SIMGBM Simulates Geometric Brownian Motion (GBM). Parameters for starting value, drift, volatility and step size can be chosen freely."}, {"input_sequence": "Author: Rafal Weron, Agnieszka Janek ; function P = HestonVanillaLipton(phi,S,K,T,r,rf,kappa,theta,sigma,rho,v0)\n%HESTONVANILLALIPTON European FX option price in the Heston model obtained\n%using the Lewis-Lipton formula.\n% P = HESTONVANILLALIPTON(PHI,S,K,T,R,RF,KAPPA,THETA,SIGMA,RHO,V0)\n% returns a price of European call (PHI =1) or put (PHI = -1) option\n% given spot price S, exercise price K, initial volatility VO, volatility\n% of volatility SIGMA, domestic interest rate R, foreign interest rate\n% RF, time to maturity (in years) T, rate of mean reversion KAPPA,\n% average level of volatility THETA and the correlation between two\n% Wiener processes RHO.\n%\n% Sample use:\n% >> HestonVanillaLipton(1,100,100,1,.1, .1,.1,.9,.3,0,.05)\n% Reference(s):\n% [1] A.Janek, T.Kluge, R.Weron, U.Wystup (2010) FX smile in the\n% Heston model, see http://ideas.repec.org/p/pra/mprapa/25491.html\n% {Chapter prepared for the 2nd edition of \"Statistical Tools for\n% Springer-Verlag, forthcoming in 2011.}\n% [2] A.Lipton (2001) Mathematical methods for foreign exchange, World\n% Scientific, 375-387\n% [4] M.Schmelzle (2010) Option Pricing Formulae using Fourier Transform:\n% Theory and Application, Working Paper\n\n% Written by Agnieszka Janek and Rafal Weron (2010.10.20)\n% Revised by Rafal Weron (2010.12.27)\n% Calculate call option price using adaptive Gauss-Kronrod quadrature\nC = exp(-rf.*T).*S - exp(-r.*T).*K./(pi).*quadgk(@(v) HestonVanillaLiptonInt(S,K,T,r,rf,kappa,theta,sigma,rho,v0,v),0,inf,'RelTol',1e-8);\nif (phi==1) %call option\nP = C;\nelse % put option\n% Calculate put option price via put-call parity\nP = C - S.*exp(-rf.*T) + K.*exp(-r.*T);\nend\n%%%%%%%%%%%%% INTERNALLY USED ROUTINE %%%%%%%%%%%%%\nfunction payoff = HestonVanillaLiptonInt(S,K,T,r,rf,kappa,theta,sigma,rho,v0,v)\n%HESTONVANILLALIPTONINT Auxiliary function used by HESTONVANILLALIPTON.\n% PAYOFF=HESTONFFTVANILLAINT(S,K,T,R,RF,KAPPA,THETA,SIGMA,RHO,V0,V)\n% returns the values of the auxiliary function evaluated at points V,\n% given spot price S, strike K, time to maturity (in years) T,\n% domestic interest rate R, foreign interest rate RF, level of mean\n% reversion KAPPA, long-run variance THETA, vol of vol SIGMA, correlation\n% RHO and initial volatility VO.\n\n% See [3], formulas (6)-(7)\nX = log(S/K) + (r - rf).*T;\nkappa_hat = kappa - rho*sigma/2;\nzeta = sqrt( v.^2.*sigma.^2.*(1-rho.^2) + 2.*1i.*v.*sigma.*rho.*kappa_hat + kappa_hat.^2 + sigma.^2/4 );\npsi_plus = - ( 1i*kappa*rho*sigma + kappa_hat ) + zeta;\nalpha = - kappa.*theta/(sigma.^2).*( psi_plus.*T + 2.*log( ( psi_minus + psi_plus.*exp(-zeta.*T) )./(2.*zeta) ) );\nbeta = ( 1 - exp(-zeta.*T) )./( psi_minus + psi_plus.*exp(-zeta.*T) ); % corrected typo (\"-\" -> \"+\" in [3])\npayoff = real( exp( ( -1i.*v + 0.5 ).*X + alpha - (v.^2 + 0.25).*beta.*v0 ) )./(v.^2 + 0.25);\n", "output_sequence": "Obtains the European FX option price in the Heston model using the Lewis-Lipton formula. Required by STFhes03.m function."}, {"input_sequence": "Author: Rafal Weron ; function [ay,by,dy,my]=hypest(x)\nm=mean(x);\nb=1;\na=2;\nd=var(x);\ny=fminsearch('hypmml',[a,b,d,m],[],x);\nay=y(1);\n", "output_sequence": "Estimates hyperbolic distribution. Auxiliary Function, Required by STFstab05.m and STFstab06.m function."}, {"input_sequence": "Author: Joanna Janczura ; function y=Paretornd(alpha,lambda,n,m)\n%PARETORND Random arrays from Pareto distribution.\n% Y = PARETORND(ALPHA,LAMBDA,N,M) returns an M-by-N array of random numbers\n% chosen from the Pareto distribution with parameters ALPHA, LAMBDA.\n%\n% The default values for ALPHA, LAMBDA, N, M 1, respectively.\n% PARETORND uses the inversion method.\nif nargin<4\nm=1;\nend\nif nargin<3\nn=1;\nif nargin<2\nlambda=1;\nif nargin<1\nalpha=1;\n\ny=lambda*(rand(n,m).^(-1/alpha)-1);\n", "output_sequence": "Generates a vector of pseudo random variables coming from a Pareto distribution."}, {"input_sequence": "Author: Joanna Janczura ; Paretornd = function(alpha,lambda,n,m){\nif (missing(m)){\nm = 1\n}\nif (missing(n)){\nn = 1\nif (missing(lambda)){\nlambda = 1\nif (missing(alpha)){\nalpha = 1\nu = matrix(0,n,m)\nfor (i in 1:m){\nu[,i] = lambda*(runif(n,0,1)^(-1/alpha)-1)\ny = u\nreturn(y)\n", "output_sequence": "Generates a vector of pseudo random variables coming from a Pareto distribution."}, {"input_sequence": "Author: Joanna Janczura ; function y=Burrpdf(x,alpha,lambda,tau)\n%BURRPDF Burr probability density function (pdf).\n% Y = BURRPDF(X,ALPHA,LAMBDA,TAU) returns the pdf of the Burr\n% distribution with parameters ALPHA,LAMBDA,TAU, evaluated at the values in X.\n% For CONTROL=0 the error message is displayed, if the parmeters are\n% negative.\n%\n% The default values for the parameters ALPHA, LAMBDA, TAU, CONTROL are\n% 1, 2, 0, respectively.\nif nargin<4\ntau=2;\nend\nif nargin<3\nlambda=1;\nend\nif nargin<2\nalpha=1;\nif nargin<1\nerror('stats:normpdf:TooFewInputs','Input argument X is undefined.');\nif tau<=0\nerror('Non-positive tau!');\nif lambda<=0\nerror('Non-positive sigma!');\nif alpha<=0\nerror('Non-positive alpha!');\ny=zeros(size(x));\npos=find(x>0);\ny(pos)=tau*alpha*lambda^alpha*x(pos).^(tau-1).*(lambda+x(pos).^tau).^(-alpha-1);\n", "output_sequence": "Returns the cumulative density distribution of the Burr distribution. The default parameters are: ALPHA  = 1, LAMBDA = 1, TAU = 2.  Required by STFloss06."}, {"input_sequence": "Author: Rafal Weron ; function y=hyppdf(x,a,b,d,m)\nif a^2-b^2>0 & a>0\ng=sqrt(a^2-b^2);\ny=g/(2*a*d*besselk(1,d*g))*exp(-a*sqrt(d^2+(x-m).^2)+b*(x-m));\nelse\ny=NaN(size(x));\n", "output_sequence": "Contains the hyperbolic probability distribution function (pdf). Function is required by STFstab04.m."}, {"input_sequence": "Author: Jovanka Lili Matic ; ## Clearing Variables and Close Windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n\n## Loading Libraries\nlibraries = c(\"ggplot2\", \"reshape2\", \"stats\", \"zoo\", \"tidyr\", \"lattice\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# fix values\nset.seed(1)\n# Poisson generator\nPPgen = function(lambda){\nX = 0\nwhile (flag==0){\nE = -log(runif(1))\nSum = Sum + E\nif(Sum < lambda){\nX = X+1\n}\nelse {\nflag = 1\n}\nreturn(X)\n}\n# Merton model\nMerton = function(mu,sigma,lambda,mu_y,sigma_y, N, T) {\nt = seq(0,T)/N\nX[1] = 0\nF = rep(0, N+1)\n\nfor(i in 1:N){\nI[i] = PPgen(h*lambda)\nif(I[i ]== 0){\nF[i] = 0\nelse {\nF[i] = mu_y*I[i]+ sqrt(sigma_y)*sqrt(I[i])*rnorm(1)\nX[i+1] = X[i] + mu*h+sigma*sqrt(h)*rnorm(1)+F[i]\n}\n## Generate 10 Poisson Process Paths and 10 Compound Poisson Process Paths\nlambda = 10\ncol_name = c('S1', 'S2','S3', 'S10', 'time')\n## Initialization\nS_Merton = matrix(0, stim_up, N+1)\n## Generating\nfor(i in 1:stim_up){\nS_Merton[i,] = Merton(mu,sigma,lambda,mu_y,sigma_y, N, T)\nS_Merton = data.frame(t(S_Merton))\nS_Merton$time = c(0:1000)\ncolnames(S_Merton) = col_name\n# Plot price paths\npic_1 = xyplot(S1 + S2 + S3 + S4 + S5 + S6 + S7 + S8 + S9 + S10 ~ time, data = S_Merton,\ntype = \"l\",\nxlab = list(label = \"Time\", cex = 1),\nylab = list(label = \"Simulated asset price\", cex =1),\nscales = list(tck=c(1,0), x=list(cex=1),\nauto.key = FALSE)\nprint(pic_1)\n", "output_sequence": "Simulation of price paths with Poisson Generating Function and the Merton Model."}, {"input_sequence": "Author: Huang Changquan, Alla Petukhina ; import numpy as np\nimport matplotlib.pyplot as plt\nfrom PythonTsa.RandomWalk import RandomWalk_with_drift\nfrom PythonTsa.plot_acf_pacf import acf_pacf_fig\nnp.random.seed(1373)\nrw0 = RandomWalk_with_drift(drift = 0.0, nsample = 250, burnin = 10)\n# burnin: the number of observation at the beginning of the sample to drop.\n# Used to reduce dependence on initial values.\nrw0.plot();\nplt.savefig('pyTSA_Trend_fig9-1.png', dpi = 1200, bbox_inches ='tight',\ntransparent = True, legend = None); plt.show()\nt = np.arange(1,len(rw0) + 1)\nMydata = 0.3 + 0.2*t + rw0\nMydata.plot()\nplt.savefig('pyTSA_Trend_fig9-2.png', dpi = 1200, bbox_inches ='tight',\ndrw = rw0.diff(1).dropna()\ndrw.plot()\nplt.savefig('pyTSA_Trend_fig9-3.png', dpi = 1200, bbox_inches ='tight',\nacf_pacf_fig(drw, both = False, lag = 15)\nplt.savefig('pyTSA_Trend_fig9-4.png', dpi = 1200, bbox_inches ='tight',\n", "output_sequence": "This Quantlet simulates ARMA(2,2) - autoregressive moving average process and draws the true ACF and PACF"}, {"input_sequence": "Author: Huang Changquan, Alla Petukhina ; import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.arima_process import arma_generate_sample\nfrom PythonTsa.plot_acf_pacf import acf_pacf_fig\nar = np.array([1, -0.8, 0.6])\nnp.random.seed(123457)\nx = arma_generate_sample(ar = ar, ma = ma, nsample = 500)\nx = pd.Series(x)\nx.plot(); plt.savefig('pyTSA_ARMA_fig3-19.png', dpi = 1200,\nbbox_inches ='tight', transparent = True); plt.show()\nacf_pacf_fig(x, both = True, lag = 50);\nplt.savefig('pyTSA_ARMA_fig3-20.png', dpi = 1200,\n", "output_sequence": "This Quantlet simulates and plots ARMA(2,2) - autoregressive moving average process time series, its ACF and PACF"}, {"input_sequence": "Author: Huang Changquan, Alla Petukhina ; import pandas as pd\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nfrom PythonTsa.plot_acf_pacf import acf_pacf_fig\nfrom PythonTsa.Selecting_arma import choose_arma\nfrom scipy import stats\nfrom PythonTsa.ModResidDiag import plot_ResidDiag\ntep = pd.read_csv('Global mean surface air temp changes 1880-1985.csv',\nheader = None)\ndates = pd.date_range('1880-12',periods = len(tep),freq = 'A-DEC')\ntep.index = dates\ntepts = pd.Series(tep[0])\n# equivalent to 'tepts = tep[0]'\nplt.plot(tepts, color = 'b')#; plt.show()\nplt.savefig('pyTSA_AirTempChange_fig4-11.png', dpi = 1200,\nbbox_inches ='tight', transparent = True, legend = None);\ndtepts = tepts.diff(1)\nplt.plot(dtepts, color = 'b')#; plt.show()\nplt.savefig('pyTSA_AirTempChange_fig4-12.png', dpi = 1200,\nsm.tsa.kpss(dtepts, regression = 'c', lags = 'auto')\nacf_pacf_fig(dtepts, both = True, lag = 20)\nplt.savefig('pyTSA_AirTempChange_fig4-13.png', dpi = 1200,\nchoose_arma(dtepts, max_p = 7, max_q = 7, ctrl = 1.03)\ninf = sm.tsa.arma_order_select_ic(dtepts, max_ar = 7,\nmax_ma = 7, ic = ['aic', 'hqic'], trend = 'c')\ninf.aic_min_order\narma11 = sm.tsa.ARMA(dtepts, order = (1,1)).fit(trend = 'c')\nprint(arma11.summary())\nresid11 = arma11.resid\nstats.normaltest(resid11)\nplot_ResidDiag(resid11,noestimatedcoef = 2,nolags = 20,lag = 25)\nplt.savefig('pyTSA_AirTempChange_fig4-15.png', dpi = 1200,\n# noestimatedcoef = number of estimated coefficients\n# nolags = max number of added terms in LB statistic.\n# lag = number of lags for ACF\narma11.plot_predict(start = '1960-12', end = '1990-12')\nplt.savefig('pyTSA_AirTempChange_fig4-14.png', dpi = 1200,\n", "output_sequence": "This Quantlet simulates ARMA(2,2) - autoregressive moving average process and draws the true ACF and PACF"}, {"input_sequence": "Author: Huang Changquan, Alla Petukhina ; import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.api import VAR\nfrom PythonTsa.plot_multi_ACF import multi_ACFfig\nfrom PythonTsa.plot_multi_Q_pvalue import MultiQpvalue_plot\ngEco = pd.read_csv('EconGermany.dat', header = 0, sep = '\\s+')\n# Extension name of the data is dat, not csv or txt.\n# argument sep = '\\s+' needed.\ndates = pd.date_range('1960-03', periods = len(gEco), freq = 'Q')\ngEco.index = dates\ngEco = gEco[['inv.', 'inc.', 'cons.']]\ngEco.plot()\nplt.savefig('pyTSA_MacroDE_fig7-10.png', dpi = 1200, bbox_inches ='tight',\ntransparent = True, legend = None); plt.show()\ndlge = np.log(gEco).diff(1).dropna()\ndlge.plot()\nplt.savefig('pyTSA_MacroDE_fig7-11.png', dpi = 1200, bbox_inches ='tight',\ndlge.tail(4)\ndlgem = dlge['1960-06-30':'1981-12-31']\n# leave the last four data for forecasting comparison\ndlgem.tail(4)\nmulti_ACFfig(dlgem, nlags = 10)\nplt.savefig('pyTSA_MacroDE_fig7-12.png', dpi = 1200, bbox_inches ='tight',\ndlgemMod = VAR(dlgem)\nprint(dlgemMod.select_order(maxlags = 4))\ndlgemRes = dlgemMod.fit(maxlags = 2, ic = None, trend = 'c')\n# ic = None since the order has been selected.\nprint(dlgemRes.summary())\ndlgemRes.is_stable()\nresid = dlgemRes.resid\nmulti_ACFfig(resid, nlags = 10)\nplt.savefig('pyTSA_MacroDE_fig7-13.png', dpi = 1200, bbox_inches ='tight',\nq, p = MultiQpvalue_plot(resid, p = 2, q = 0, noestimatedcoef = 18, nolags = 24)\nplt.savefig('pyTSA_MacroDE_fig7-14.png', dpi = 1200, bbox_inches ='tight',\ncoefMat = dlgemRes.coefs\ncoefMat\nsigma_u = dlgemRes.sigma_u\nsigma_u\ndlgem = dlgem.values\n# transferred to 'numpy.ndarray' class\ntype(dlgem)\n# for forecast, the data needs to belong to the class 'ndarray'\nfore_interval = dlgemRes.forecast_interval(dlgem, steps = 4)\npoint\nlower\nupper\ndlgemRes.plot_forecast(steps = 4)\nplt.savefig('pyTSA_MacroDE_fig7-15.png', dpi = 1200, bbox_inches ='tight',\ntransparent = True, legend = None); plt.show() ; plt.show()\ng1 = dlgemRes.test_causality(caused = 'cons.', causing = 'inc.',\nkind = 'f', signif = 0.05)\nprint(g1)\ng2 = dlgemRes.test_causality(caused = 'inc.', causing = 'cons.',\nprint(g2)\nirf = dlgemRes.irf(periods = 10)\nirf.plot(); plt.savefig('pyTSA_MacroDE_fig7-16.png', dpi = 1200, bbox_inches ='tight',\ntransparent = True, legend = None); plt.show()\nirf.plot_cum_effects()\nplt.savefig('pyTSA_MacroDE_fig7-17.png', dpi = 1200, bbox_inches ='tight',\n", "output_sequence": "This Quantlet plots monthly time series of returns of Procter and Gamble from 1961 to 2016 and  their ACF and PACF (Example, 2.4 Figures 2.8-2.9 in the book)"}, {"input_sequence": "Author: Huang Changquan, Alla Petukhina ; import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport statsmodels.api as sm\nfrom PythonTsa.plot_acf_pacf import acf_pacf_fig\nfrom statsmodels.tsa.arima_process import arma_generate_sample\nfrom statsmodels.graphics.tsaplots import quarter_plot\nsar1 = np.array([1, 0, -0.2])\nnp.random.seed(137)\nx1 = arma_generate_sample(ar = sar1, ma = [1], nsample = 200)\nx1 = pd.Series(x1); x2 = pd.Series(x2)\nfig = plt.figure()\nx1.plot(marker = '.', ax = fig.add_subplot(221))\nplt.title('$X_t = 0.2X_{t-4}+\\epsilon_t$')\nx2.plot(marker = '.', ax = fig.add_subplot(222))\nplt.title('$X_t = 0.6X_{t-4}+\\epsilon_t$')\nx3.plot(marker = '.', ax = fig.add_subplot(223))\nplt.title('$X_t = 0.8X_{t-4}+\\epsilon_t$')\nx4.plot(marker = '.', ax = fig.add_subplot(224))\nplt.title('$X_t = 1.0X_{t-4}+\\epsilon_t$')\nplt.savefig('pyTSA_Seasonality_fig9-9.png', dpi = 1200, bbox_inches ='tight',\ntransparent = True, legend = None); plt.show()\nax = fig.add_subplot(221)\nacf_pacf_fig(x1, both = False, lag = 16)\nax = fig.add_subplot(222)\nacf_pacf_fig(x2, both = False, lag = 16)\nax = fig.add_subplot(223)\nacf_pacf_fig(x3, both = False, lag = 16)\nax = fig.add_subplot(224)\nacf_pacf_fig(x4, both = False, lag = 16)\nplt.savefig('pyTSA_Seasonality_fig9-10.png', dpi = 1200, bbox_inches ='tight',\nsperiod = pd.date_range('2001-01', periods = len(x1), freq = 'Q')\nx1.index = speriod\nquarter_plot(x1, ax = fig.add_subplot(221))\nplt.title('Seasonal plot for $X_t = 0.2X_{t-4}+\\epsilon_t$')\nquarter_plot(x2, ax = fig.add_subplot(222))\nplt.title('Seasonal plot for $X_t = 0.6X_{t-4}+\\epsilon_t$')\nquarter_plot(x3, ax = fig.add_subplot(223))\nplt.title('Seasonal plot for $X_t = 0.8X_{t-4}+\\epsilon_t$')\nquarter_plot(x4, ax = fig.add_subplot(224))\nplt.title('Seasonal plot for $X_t = 1.0X_{t-4}+\\epsilon_t$')\nplt.savefig('pyTSA_Seasonality_fig9-11.png', dpi = 1200, bbox_inches ='tight',\ny = pd.DataFrame(index = range(0, int(len(x1)/4)),\ncolumns = ['0', '1', '3'])\nfor i in range(0, 4):\nfor j in range(i, len(x1), 4):\ny.iat[int(j/4), i] = x1[j]\nz = pd.concat([y['0'], y['1'], ignore_index = True)\nax = fig.add_subplot(211)\nz.plot()\nplt.title('The concatenate series of the seasonal subseries for $X_t = 0.2X_{t-4}+\\epsilon_t$')\nax = fig.add_subplot(212)\nacf_pacf_fig(z, both = False, lag = 16)\nplt.savefig('pyTSA_Seasonality_fig9-12.png', dpi = 1200, bbox_inches ='tight',\nsrw = pd.DataFrame(index = range(0, int(len(x1)/4)),\nfor j in range(i, len(x4), 4):\nsrw.iat[int(j/4), i] = x4[j]\ncsrw = pd.concat([srw['0'], srw['1'], ignore_index = True)\ncsrw.plot()\nplt.title('The concatenate series of the seasonal subseries for $X_t = 1.0X_{t-4}+\\epsilon_t$')\nacf_pacf_fig(csrw, both = False, lag = 16)\nplt.savefig('pyTSA_Seasonality_fig9-13.png', dpi = 1200, bbox_inches ='tight',\n", "output_sequence": "This Quantlet simulates ARMA(2,2) - autoregressive moving average process and draws the true ACF and PACF"}, {"input_sequence": "Author: Huang Changquan, Alla Petukhina ; import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.api import VAR\nimport statsmodels.api as sm\nfrom PythonTsa.plot_multi_ACF import multi_ACFfig\nfrom PythonTsa.plot_multi_Q_pvalue import MultiQpvalue_plot\nmdata = sm.datasets.macrodata.load_pandas().data\nmdata = mdata[['realgdp', 'realcons', 'realinv']]\ndates = pd.date_range('1959-01', periods = len(mdata), freq = 'Q')\nmdata.index = dates\nmdata.plot()\nplt.savefig('pyTSA_MacroUS_fig7-18.png', dpi = 1200, bbox_inches ='tight',\ntransparent = True, legend = None); plt.show()\ndLdata = np.log(mdata).diff(1).dropna()\n# log, difference and then drop NAN\nfig = plt.figure()\ndLdata['realgdp'].plot(ax = fig.add_subplot(311))\nplt.title('Differenced log of real GDP')\ndLdata['realcons'].plot(ax = fig.add_subplot(312))\nplt.title('Differenced log of real consume')\ndLdata['realinv'].plot(ax = fig.add_subplot(313))\nplt.title('Differenced log of real invest')\nplt.savefig('pyTSA_MacroUS_fig7-19.png', dpi = 1200, bbox_inches ='tight',\ndLdata.tail(3)\nmyd = dLdata['1959-06-30' : '2008-12-31']\n# leave the last three data for forecasting comparison\nmyd.tail(4)\nmulti_ACFfig(myd, nlags = 10)\nplt.savefig('pyTSA_MacroUS_fig7-20.png', dpi = 1200, bbox_inches ='tight',\nmydmod1 = VAR(myd)\nprint(mydmod1.select_order(maxlags = 10))\n# here select p = 3 for VAR(p).\nmydmod = VARMAX(myd, order = (3, 0), enforce_stationarity = True)\nmodfit = mydmod.fit()\nprint(modfit.summary())\nresid = modfit.resid\nmulti_ACFfig(resid, nlags = 10)\nplt.savefig('pyTSA_MacroUS_fig7-21.png', dpi = 1200, bbox_inches ='tight',\nqs, pv = MultiQpvalue_plot(resid, p = 3, q = 0, noestimatedcoef = 27, nolags = 24)\nplt.savefig('pyTSA_MacroUS_fig7-22.png', dpi = 1200, bbox_inches ='tight',\nparam = mydmod.param_names\nmydmodf = VARMAX(myd, order = (3, 0), enforce_stationarity = False)\n# Cannot fix individual autoregressive parameters when\n# \u2018enforce_stationarity = True\u2018. In this case, must either\n# fix all autoregressive parameters or none.\nwith mydmodf.fix_params({param[0]: 0.0, param[5]: 0.0,\nparam[6]: 0.0, param[8]: 0.0,\nparam[29]: 0.0}): modff = mydmodf.fit(method = 'bfgs')\nprint(modff.summary())\nresidf = modff.resid\nmulti_ACFfig(residf, nlags = 10)\nplt.savefig('pyTSA_MacroUS_fig7-23.png', dpi = 1200, bbox_inches ='tight',\nqs, pv = MultiQpvalue_plot(residf, p = 3, q = 0, noestimatedcoef = 13, nolags = 24)\nplt.savefig('pyTSA_MacroUS_fig7-24.png', dpi = 1200, bbox_inches ='tight',\nfore = modff.predict(end = '2009-09-30')\nrealgdpFitgdp = pd.DataFrame({'realgdp':dLdata['realgdp'], 'fittedgdp':fore['realgdp']})\nrealgdpFitgdp.plot(style = ['-', '--'], ax = fig.add_subplot(311))\nplt.savefig('pyTSA_MacroUS_fig7-25.png', dpi = 1200, bbox_inches ='tight',\n", "output_sequence": "This Quantlet plots monthly time series of returns of Procter and Gamble from 1961 to 2016 and  their ACF and PACF (Example, 2.4 Figures 2.8-2.9 in the book)"}, {"input_sequence": "Author: Ranqing Song ; # clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# install and load packages\nlibraries = c(\"stats\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# parameter settings\nn1 = 100\nbeta = 0.8\n# simulation of MA(1)-processes\nset.seed(123)\nx1 = arima.sim(n = n1, list(ma = beta), innov = rnorm(n1))\n# Plot\npar(mfrow = c(2, 1))\nplot.ts(x1, col = \"blue\", ylab = \"y(t)\")\ntitle(paste(\"MA(1) Process, n =\", n1))\npar(mfg = c(2, 1))\nplot.ts(x2, col = \"red\", ylab = \"y(t)\")\ntitle(paste(\"MA(1) Process, n =\", n2))\n", "output_sequence": "Plots two realizations of an MA(1) (moving average) process with MA coefficient beta,  simulate from an arima model with innovations n1 and n2."}, {"input_sequence": "Author: Ranqing Song ; # clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# parameter settings\nn = 1000\nbeta = 0.5\n#generate time-series data set\nset.seed(123)\nx = arima.sim(n = n, list(ma = beta), innov = rnorm(n))\nx = matrix(x)\n#generate candidates for estimated beta, -1<beta<1\nbetahat = seq(from = -0.99, to = 0.99, by = 0.02)\nk= 100\nli = c(1:k)\ne[1] = x[1]\n# likelihood function, assuming standard normal distributed errors\nfor (i in 1:k){\nb = betahat[i]\ngamma0 = diag(1+b^2, n, n)\ngamma1 = cbind(0, gamma1)\ngamma = gamma0 + gamma1 + tgamma1\nbetacoef = (-b) ^ (1:(n-1))\n#unconditional maximal likelihood function\nli[i] = -n/2 * log(2*pi) - 1/2 * log(det(gamma)) - 1/2 * t(x) %*% solve(gamma) %*% x\n#error terms\nfor (j in 2:n){\ne[j] = x[j] + sum(betacoef[1:(j-1)] * x[(j-1):1, 1])\n}\n#conditional maximal likelihood function\ncli[i] = -n/2 * log(2*pi) - 1/2 * log(det(gamma)) - 1/2 * sum(e^2)\n}\n#plot the likelihood functions\noutput = cbind(betahat, li, cli)\nplot(output[,c(1,2)], col = 4, xlab = \"Beta\", ylab = \"log-Likelihood\",type = \"l\", lwd = 2,\nmain = paste(\"likelihood function of a MA(1) Process with n=\", n, sep = \"\"))\nabline(v = output[which.max(li),1], col = \"blue\")\npoints(output[, c(1,3)], type = \"l\", col = 2, lty = 2, lwd = 2)\nabline(v = output[which.max(cli),1], col = \"red\")\n", "output_sequence": "Plots two realizations of an MA(1) (moving average) process with MA coefficient beta,  simulate from an arima model with innovations n1 and n2."}, {"input_sequence": "Author: Ranqing Song ; # clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# parameter settings\nn = 100\nbeta = 0.5\n#generate time-series data set\nset.seed(123)\nx = arima.sim(n = n, list(ma = beta), innov = rnorm(n))\nx = matrix(x)\n#generate candidates for estimated beta, -1<beta<1\nbetahat = seq(from = -0.99, to = 0.99, by = 0.02)\nk= 100\nli = c(1:k)\ne[1] = x[1]\n# likelihood function, assuming standard normal distributed errors\nfor (i in 1:k){\nb = betahat[i]\ngamma0 = diag(1+b^2, n, n)\ngamma1 = cbind(0, gamma1)\ngamma = gamma0 + gamma1 + tgamma1\nbetacoef = (-b) ^ (1:(n-1))\n#unconditional maximal likelihood function\nli[i] = -n/2 * log(2*pi) - 1/2 * log(det(gamma)) - 1/2 * t(x) %*% solve(gamma) %*% x\n#error terms\nfor (j in 2:n){\ne[j] = x[j] + sum(betacoef[1:(j-1)] * x[(j-1):1, 1])\n}\n#conditional maximal likelihood function\ncli[i] = -n/2 * log(2*pi) - 1/2 * log(det(gamma)) - 1/2 * sum(e^2)\n}\n#plot the likelihood functions\noutput = cbind(betahat, li, cli)\nplot(output[,c(1,2)], col = 4, xlab = \"Beta\", ylab = \"log-Likelihood\",type = \"l\", lwd = 2,\nmain = paste(\"likelihood function of a MA(1) Process with n=\", n, sep = \"\"))\nabline(v = output[which.max(li),1], col = \"blue\")\npoints(output[, c(1,3)], type = \"l\", col = 2, lty = 2, lwd = 2)\nabline(v = output[which.max(cli),1], col = \"red\")\n", "output_sequence": "Plots two realizations of an MA(1) (moving average) process with MA coefficient beta,  simulate from an arima model with innovations n1 and n2."}, {"input_sequence": "Author: Pablo Spitzley Rodriguez, Andrey Lyan ; # clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n\n# Creating Portfolio-Allocation-Function\nportfolio_allocation <- function(probability, gamma, stock_zero, budget){\n\n# Initial Value of the Stock\nS_0 <- stock_zero\n# Value of Stock in up-state\nS_u <- stock_up\n# Value of Stock in down-state\nS_d <- stock_down\n# The given Budget\nw_0 <- budget\n# Probablity of stock going up, (1-p) is the probability of the stock going down\np <- probability\n# Gamma, determines wether investor is risk-averse (Gamma positive), or risk-loving (Gamma negative)\ng <- gamma\n# Making sure it is S_d < S_0 < S_u\nif(S_d > S_0 | S_u < S_0){\nstop(\"It must be: Stock_down < Stock_zero < Stock_up.\")\n}\n# Making sure p is in [0,1]\nif(p < 0 | p > 1){\nstop(\"The parameter probability has to be in the interval from 0 to 1.\")\n# Calculating q (risk neutral probality)\nq <- solve(S_u-S_d,S_0-S_d)\n# Calculating Pricing Kernels\nK_u <- q/p\nK_d <- (1-q)/(1-p)\n# Calculating parameter lambda\nl <- solve((q*K_u^(-1/g)+(1-q)*K_d^(-1/g))^(-g),w_0^(-g))\n# Calculating X_u and X_d\nX_u <- (l*K_u)^(-1/g)\n# Calculating x and y, x = # of shares, y = # of bonds\nA <- matrix(data = c(S_u,1,S_d,1), nrow = 2, ncol = 2, byrow = T)\nnum <- solve(A,b)\nx<- num[1,1]\n# Creating Output-Data-Matrix\noutput <- round(matrix(data = c(p,q,g,K_u,K_d,X_u,X_d,x,y),ncol = 9, nrow = 1, byrow = T),3)\ncolnames(output) <- c(\"p\",\"q\",\"gamma\",\"K_u\",\"K_d\",\"X_u\",\"X_d\",\"x\",\"y\")\nrownames(output) <- c(\"Values\")\nreturn(output)\n}\n# Creating Data-Frame with varying parameter gamma\n# Table: gamma from (-1 to 1, by 0.2)\nlist_1 <- lapply(X = seq(-1,1,0.2),function(x){\nportfolio_allocation(probability = 0.2, gamma = x,\nstock_zero = 270, stock_up = 300, stock_down = 250, budget = 1200)\n)\ndf_1 <- as.data.frame(do.call(rbind, list_1))\nrownames(df_1) <- NULL\ndf_1\n# Creating Data-Frame with varying parameter p and fix gamma at 1 (risk averse)\n# Table: p from (0.1 to 0.9, by 0.1)\nlist_2 <- lapply(X = seq(0.1,0.9,0.1),function(x){\nportfolio_allocation(probability = x, gamma = 1,\ndf_2 <- as.data.frame(do.call(rbind, list_2))\nrownames(df_2) <- NULL\ndf_2\n# Creating Data-Frame with varying parameter p and fix gamma at -1 (risk loving)\nlist_3 <- lapply(X = seq(0.1,0.9,0.1),function(x){\nportfolio_allocation(probability = x, gamma = -1,\ndf_3 <- as.data.frame(do.call(rbind, list_3))\nrownames(df_3) <- NULL\ndf_3\n# Plotting number of bonds and number of stocks against a changing gamma\n# First use Portfolio-function to calculate the values\nlist_4 <- lapply(X = seq(-10,10,0.2),function(x){\ndf_4 <- as.data.frame(do.call(rbind, list_4))\nrownames(df_4) <- NULL\n# Plot 1: number of stock against gamma and Plot 2: number of bonds against gamma\npar(mfrow=c(1,2))\nplot(df_4$gamma , df_4$y,type = \"l\", col = \"red\",\nlwd = 2, ylab = \"number of bonds\", xlab = \"gamma\")\nplot(df_4$gamma , df_4$x,type = \"l\", col = \"blue\",\nlwd = 2, ylab = \"number of stocks\", xlab = \"gamma\")\n# Plot of Utility function, to illustrate what different values of gamma imply\n# Creating Utility function\nutility <- function(x,a){\nif(a==1){\ny <- log(x)\n}else\n{\ny <- (x^(1-a))/(1-a)\nreturn(y)\nx <- seq(0.01,10,0.01)\n# Setting a equal to zero -> risk-neutrality\ny_a_zero <- utility(x,0)\n# Setting a bigger than zero (here a = 1) -> risk-aversion\ny_a_pos <- utility(x,1)\n# Setting a smaller than zero (here a = -1) -> risk-proclivity\ny_a_neg <- utility(x,-1)\npar(mfrow=c(1,1))\npar(pty=\"s\")\nplot(x,y_a_zero,type = \"l\",col=\"blue\",lwd=2,xlab = \"Portfolio Value\", ylab = \"Utility\")\nlines(x,y_a_pos,type = \"l\",col=\"red\", lwd=2)\n", "output_sequence": "Returns the Portfolio Allocation in a one-period Binomial-Model, for different Pricing Kernels and utility-functions"}, {"input_sequence": "Author: Pablo Spitzley Rodriguez, Andrey Lyan ; #tail dependency plot for simulated: bivariate normal copula with normal margins;\n#bivariate t copula with t margins;\nrm(list = ls(all = TRUE))\n#setwd(\"C:/...\")\n#install.packages(\"copula\")\nlibrary(copula)\nset.seed(100)\npng(\"taildep.png\", width = 800, height = 600)\npar(bg=NA)\nlayout(matrix(c(1,2,3,4), 2, byrow = TRUE))\npar(mar = c(2, 2, 2))\ncop_norm_norm = mvdc(normalCopula(0.6), c(\"norm\",\nlist(mean = 0, sd =1))\nsim1 = rMvdc(5000, cop_norm_norm)\nplot(sim1[,1],sim1[,2],col='black',main = \"Normal Copula with Normal Marg.\",\nxlim=c(-6,6), pch = 1)\ngrid (NULL,NULL, lty = 4, col = \"darkgrey\")\nlinear1 = lm(sim1[,2]~sim1[,1])\nabline(linear1, col = 'black')\ncop_t_t = mvdc(tCopula(0.6,dim=2,df=3), c(\"t\", \"t\"),\nlist(df=7, df=7))\nsim3 = rMvdc(5000, cop_t_t)\nplot(sim3[,1], sim3[,2], col='black',main = \"t-Copula (dof = 3) with t-Marg. \",\nxlim=c(-6,6),\nlinear3 = lm(sim3[,2]~sim3[,1])\nabline(linear3, col = 'black')\ncop_t_norm = mvdc(tCopula(0.6,dim=2,df=3), c(\"norm\",\nlist(mean = 0, sd =1))\nsim2 = rMvdc(5000, cop_t_norm)\nplot(sim2[,1], sim2[,2], col='black',main = \"t-Copula (dof = 3) with Normal Marg.\",\nlinear2 = lm(sim2[,2]~sim2[,1])\nabline(linear2, col = 'black')\ncop_ind_norm = mvdc(normalCopula(0), c(\"norm\",\nlist(mean = 0, sd =1))\nsim4 = rMvdc(5000, cop_ind_norm)\nplot(sim4[,1], sim4[,2], col='black',main = \"Independence Copula with Normal Marg.\",\ngrid (NULL,NULL, lty = 4, col = \"darkgrey\")\ndev.copy(png,'taildep.png')\n", "output_sequence": "Returns the Portfolio Allocation in a one-period Binomial-Model, for different Pricing Kernels and utility-functions"}, {"input_sequence": "Author: Alla Petukhina ; %run('CCPConstruction.m')\n%% Performance measures\nsharpeIND = mean(IND_RET(end-size(CC_IND_RET_EW,1)+1:end,4))/std(IND_RET(end-size(CC_IND_RET_EW,1)+1:end,4)); %5 - Euro stoxx 50 4 S&PP100\nadjsharpeIND = sharpeIND*(1+skewness(IND_RET(end-size(CC_IND_RET_EW,1)+1:end,4))/6*sharpeIND-(kurtosis(IND_RET(end-size(CC_IND_RET_EW,1)+1:end,4))-3)/24*sharpeIND^2);\nsharpeIND_EW = mean(IND_RET_EW)/std(IND_RET_EW);\nbetaIND_EW = cov(IND_RET_EW,IND_RET(end-size(CC_IND_RET_EW,1)+1:end,4))/var(IND_RET(end-size(CC_IND_RET_EW,1)+1:end,4)); %MArket - SP100\nbetaIND_EW = betaIND_EW(2,1);\ntreynorIND_EW = mean(IND_RET(end-size(CC_IND_RET_EW,1)+1:end,4))/betaIND_EW;\nmsquaredIND_EW = mean(CC_IND_RET_EW)*(std(IND_RET(end-size(CC_IND_RET_EW,1)+1:end,5))/std(CC_IND_RET_EW)) - mean(IND_RET(end-size(CC_IND_RET_EW,1)+1:end,5));\njalphaIND_EW = mean(CC_IND_RET_EW) - betaIND_EW*mean(IND_RET(end-size(CC_IND_RET_EW,1)+1:end,:));\nceqIND_EW = mean(CC_IND_RET_EW) - 0.5*var(CC_IND_RET_EW);\nadjsharpeIND_EW = sharpeIND_EW*(1+skewness(IND_RET_EW)/6*sharpeIND_EW-(kurtosis(IND_RET_EW)-3)/24*sharpeIND_EW^2);\nsharpeIND_MAXRET = nanmean(IND_RET_MAXRET)/nanstd(IND_RET_MAXRET);\nbetaIND_MAXRET = cov(IND_RET_MAXRET,IND_RET(end-size(CC_IND_RET_EW,1)+1:end,4))/var(IND_RET(end-size(CC_IND_RET_EW,1)+1:end,5)); %MArket - SP100\nbetaIND_MAXRET = betaIND_MAXRET(2,1);\ntreynorMAXRET = mean(IND_RET)/betaIND_MAXRET;\nmsquareIND_MAXRET = mean(IND_RET_MAXRET)*(std(IND_RET(end-size(CC_IND_RET_EW,1)+1:end,4))/std(IND_RET_MAXRET)) - mean(IND_RET(end-size(CC_IND_RET_EW,1)+1:end,4)); % (!!!) not only the rank, but also whether beats the market (beats if positINV_VOLAe)\njalphaIND_MAXRET = mean(IND_RET_MAXRET) - betaIND_MAXRET*mean(IND_RET(end-size(CC_IND_RET_EW,1)+1:end,4)); % if the Jensen's alpha is positINV_VOLAe, then portfolio has outperformed the market\nceqIND_MAXRET = nanmean(IND_RET_MAXRET) - 0.5*nanvar(IND_RET_MAXRET);\nadjsharpeIND_MAXRET = sharpeIND_MAXRET*(1+skewness(IND_RET_MAXRET)/6*sharpeIND_MAXRET-(kurtosis(IND_RET_MAXRET)-3)/24*sharpeIND_MAXRET^2);\nsharpeIND_MAXSHARPE = nanmean(IND_RET_MAXSHARPE)/nanstd(IND_RET_MAXSHARPE);\nbetaIND_MAXSHARPE = cov(IND_RET_MAXSHARPE,IND_RET(end-size(CC_IND_RET_EW,1)+1:end,4))/var(IND_RET(end-size(CC_IND_RET_EW,1)+1:end,5)); %MArket - SP100\nbetaIND_MAXSHARPE = betaIND_MAXSHARPE(2,1);\ntreynorMAXSHARPE = mean(IND_RET)/betaIND_MAXSHARPE;\nmsquareIND_MAXSHARPE = mean(IND_RET_MAXSHARPE)*(std(IND_RET(end-size(CC_IND_RET_EW,1)+1:end,4))/std(IND_RET_MAXSHARPE)) - mean(IND_RET(end-size(CC_IND_RET_EW,1)+1:end,4)); % (!!!) not only the rank, but also whether beats the market (beats if positINV_VOLAe)\njalphaIND_MAXSHARPE = mean(IND_RET_MAXSHARPE) - betaIND_MAXSHARPE*mean(IND_RET(end-size(CC_IND_RET_EW,1)+1:end,4)); % if the Jensen's alpha is positINV_VOLAe, then portfolio has outperformed the market\nceqIND_MAXSHARPE = nanmean(IND_RET_MAXSHARPE) - 0.5*nanvar(IND_RET_MAXSHARPE);\nadjsharpeIND_MAXSHARPE = sharpeIND_MAXSHARPE*(1+skewness(IND_RET_MAXSHARPE)/6*sharpeIND_MAXSHARPE-(kurtosis(IND_RET_MAXSHARPE)-3)/24*sharpeIND_MAXSHARPE^2);\nsharpeEW = mean(CC_IND_RET_EW)/std(CC_IND_RET_EW);\nbetaEW = cov(CC_IND_RET_EW,IND_RET(end-size(CC_IND_RET_EW,1)+1:end,4))/var(IND_RET(end-size(CC_IND_RET_EW,1)+1:end,4)); %MArket - SP100\nbetaEW = betaEW(2,1);\ntreynorEW = mean(IND_RET)/betaEW;\nmsquaredEW = mean(CC_IND_RET_EW)*(std(IND_RET(end-size(CC_IND_RET_EW,1)+1:end,4))/std(CC_IND_RET_EW)) - mean(IND_RET(end-size(CC_IND_RET_EW,1)+1:end,4));\njalphaEW = mean(CC_IND_RET_EW) - betaEW*mean(IND_RET(end-outsample_width+1:end,:));\nceqEW = mean(CC_IND_RET_EW) - 0.5*var(CC_IND_RET_EW);\nadjsharpeEW = sharpeEW*(1+skewness(CC_IND_RET_EW)/6*sharpeEW-(kurtosis(CC_IND_RET_EW)-3)/24*sharpeEW^2);\nsharpeMAXRET = mean(CC_IND_RET_MAXRET)/std(CC_IND_RET_MAXRET);\nbetaMAXRET = cov(CC_IND_RET_MAXRET,IND_RET(end-size(CC_IND_RET_EW,1)+1:end,4))/var(IND_RET(end-size(CC_IND_RET_EW,1)+1:end,4)); %MArket - SP100\nbetaMAXRET = betaMAXRET(2,1);\ntreynorMAXRET = mean(IND_RET)/betaMAXRET;\nmsquaredMAXRET = mean(CC_IND_RET_MAXRET)*(std(IND_RET(end-size(CC_IND_RET_EW,1)+1:end,4))/std(CC_IND_RET_MAXRET)) - mean(IND_RET(end-size(CC_IND_RET_EW,1)+1:end,4)); % (!!!) not only the rank, but also whether beats the market (beats if positINV_VOLAe)\njalphaMAXRET = mean(CC_IND_RET_MAXRET) - betaMAXRET*mean(IND_RET(end-size(CC_IND_RET_EW,1)+1:end,4)); % if the Jensen's alpha is positINV_VOLAe, then portfolio has outperformed the market\nceqMAXRET = mean(CC_IND_RET_MAXRET) - 0.5*var(CC_IND_RET_MAXRET);\nadjsharpeMAXRET = sharpeMAXRET*(1+skewness(CC_IND_RET_MAXRET)/6*sharpeMAXRET-(kurtosis(CC_IND_RET_MAXRET)-3)/24*sharpeMAXRET^2);\nsharpeMINVAR = mean(CC_IND_RET_MINVAR)/std(CC_IND_RET_MINVAR);\nbetaMINVAR = cov(CC_IND_RET_MINVAR, IND_RET(end-size(CC_IND_RET_EW,1)+1:end,4))/var(IND_RET(end-size(CC_IND_RET_EW,1)+1:end,4)); %MArket - SP100\nbetaMINVAR = betaMINVAR(2,1);\ntreynorMINVAR = mean(IND_RET)/betaMINVAR;\nmsquaredMINVAR = mean(CC_IND_RET_MINVAR)*(std(IND_RET(end-size(CC_IND_RET_EW,1)+1:end,4))/std(CC_IND_RET_MINVAR)) - mean(IND_RET(end-size(CC_IND_RET_EW,1)+1:end,4)); % (!!!) not only the rank, but also whether beats the market (beats if positINV_VOLAe)\njalphaMINVAR = mean(CC_IND_RET_MINVAR) - betaMINVAR*mean(IND_RET(end-size(CC_IND_RET_EW,1)+1:end,4)); % if the Jensen's alpha is positINV_VOLAe, then portfolio has outperformed the market\nceqMINVAR = mean(CC_IND_RET_MINVAR) - 0.5*var(CC_IND_RET_MINVAR);\nadjsharpeMINVAR = sharpeMINVAR*(1+skewness(CC_IND_RET_MINVAR)/6*sharpeMINVAR-(kurtosis(CC_IND_RET_MINVAR)-3)/24*sharpeMINVAR^2);\nsharpeMAXSHARPE = mean(CC_IND_RET_MAXSHARPE)/std(CC_IND_RET_MAXSHARPE);\nbetaMAXSHARPE = cov(CC_IND_RET_MAXSHARPE,IND_RET(end-size(CC_IND_RET_EW,1)+1:end,4))/var(IND_RET(end-size(CC_IND_RET_EW,1)+1:end,4)); %MArket - SP100\nbetaMAXSHARPE = betaMAXSHARPE(2,1);\ntreynorMAXSHARPE = mean(IND_RET)/betaMAXSHARPE;\nmsquaredMAXSHARPE = mean(CC_IND_RET_MAXSHARPE)*(std(IND_RET(end-size(CC_IND_RET_EW,1)+1:end,4))/std(CC_IND_RET_MAXSHARPE)) - mean(IND_RET(end-size(CC_IND_RET_EW,1)+1:end,4));\njalphaMAXSHARPE = mean(CC_IND_RET_MAXSHARPE) - betaMAXSHARPE*mean(IND_RET(end-size(CC_IND_RET_EW,1)+1:end,4));\nceqMAXSHARPE = mean(CC_IND_RET_MAXSHARPE) - 0.5*var(CC_IND_RET_MAXSHARPE);\nadjsharpeMAXSHARPE = sharpeMAXSHARPE*(1+skewness(CC_IND_RET_MAXSHARPE)/6*sharpeMAXSHARPE-(kurtosis(CC_IND_RET_MAXSHARPE)-3)/24*sharpeMAXSHARPE^2);\nsharpeRP = mean(CC_IND_RET_RP)/std(CC_IND_RET_RP);\nbetaRP = cov(CC_IND_RET_RP,IND_RET(end-size(CC_IND_RET_EW,1)+1:end,5))/var(IND_RET(end-size(CC_IND_RET_EW,1)+1:end,5)); %MArket - SP100\nbetaRP = betaRP(2,1);\ntreynorRP = mean(IND_RET)/betaRP;\nmsquaredRP = mean(CC_IND_RET_RP)*(std(IND_RET(end-size(CC_IND_RET_EW,1)+1:end,4))/std(CC_IND_RET_RP)) - mean(IND_RET(end-size(CC_IND_RET_EW,1)+1:end,4)); % (!!!) not only the rank, but also whether beats the market (beats if positINV_VOLAe)\njalphaRP = mean(CC_IND_RET_RP) - betaRP*mean(IND_RET(end-size(CC_IND_RET_EW,1)+1:end,4)); % if the Jensen's alpha is positINV_VOLAe, then portfolio has outperformed the market\nceqRP = mean(CC_IND_RET_RP) - 0.5*var(CC_IND_RET_RP);\nadjsharpeRP = sharpeRP*(1+skewness(CC_IND_RET_RP)/6*sharpeRP-(kurtosis(CC_IND_RET_RP)-3)/24*sharpeRP^2);\nsharpeIV = mean(CC_IND_RET_IV)/std(CC_IND_RET_IV);\nbetaIV = cov(CC_IND_RET_IV,IND_RET(end-size(CC_IND_RET_EW,1)+1:end,4))/var(IND_RET(end-size(CC_IND_RET_EW,1)+1:end,4));\nbetaIV = betaIV(2,1);\ntreynorIV = mean(IND_RET)/betaIV;\nmsquaredIV = mean(CC_IND_RET_IV)*(std(IND_RET(end-size(CC_IND_RET_EW,1)+1:end,4))/std(CC_IND_RET_IV)) - mean(IND_RET(end-size(CC_IND_RET_EW,1)+1:end,4)); % (!!!) not only the rank, but also whether beats the market (beats if positINV_VOLAe)\njalphaIV = mean(CC_IND_RET_IV) - betaIV*mean(IND_RET(end-size(CC_IND_RET_EW,1)+1:end,4)); % if the Jensen's alpha is positINV_VOLAe, then portfolio has outperformed the market\nceqIV = mean(CC_IND_RET_IV) - 0.5*var(CC_IND_RET_IV);\nadjsharpeIV = sharpeIV*(1+skewness(CC_IND_RET_IV)/6*sharpeIV-(kurtosis(CC_IND_RET_IV)-3)/24*sharpeIV^2);\nsharpeMAXRET_CVAR = mean(CC_IND_RET_MAXRET_CVAR)/std(CC_IND_RET_MAXRET_CVAR);\nbetaMAXRET_CVAR = cov(CC_IND_RET_MAXRET_CVAR,IND_RET(end-size(CC_IND_RET_EW,1)+1:end,4))/var(IND_RET(end-size(CC_IND_RET_EW,1)+1:end,4));\nbetaMAXRET_CVAR = betaMAXRET_CVAR(2,1);\ntreynorMAXRET_CVAR = mean(IND_RET)/betaMAXRET_CVAR;\nmsquaredMAXRET_CVAR = mean(CC_IND_RET_MAXRET_CVAR)*(std(IND_RET(end-size(CC_IND_RET_EW,1)+1:end,4))/std(CC_IND_RET_MAXRET_CVAR)) - mean(IND_RET(end-size(CC_IND_RET_EW,1)+1:end,4)); % (!!!) no only the rank, but also whether beats the market (beats if positINV_VOLAe)\njalphaMAXRET_CVAR = mean(CC_IND_RET_MAXRET_CVAR) - betaMAXRET_CVAR*mean(IND_RET(end-size(CC_IND_RET_EW,1)+1:end,4)); % if the Jensen's alpha is positINV_VOLAe, then portf_cvarolio has outperfomed the market\nceqMAXRET_CVAR = mean(CC_IND_RET_MAXRET_CVAR) - 0.5*var(CC_IND_RET_MAXRET_CVAR);\nadjsharpeMAXRET_CVAR = sharpeMAXRET_CVAR*(1+skewness(CC_IND_RET_MAXRET_CVAR)/6*sharpeMAXRET_CVAR-(kurtosis(CC_IND_RET_MAXRET_CVAR)-3)/24*sharpeMAXRET_CVAR^2);\nsharpeMINVAR_CVAR = mean(CC_IND_RET_MINVAR_CVAR)/std(CC_IND_RET_MINVAR_CVAR);\nbetaMINVAR_CVAR = cov(CC_IND_RET_MINVAR_CVAR, IND_RET(end-size(CC_IND_RET_EW,1)+1:end,4))/var(IND_RET(end-size(CC_IND_RET_EW,1)+1:end,4));\nbetaMINVAR_CVAR = betaMINVAR_CVAR(2,1);\ntreynorMINVAR_CVAR = mean(IND_RET)/betaMINVAR_CVAR;\nmsquaredMINVAR_CVAR = mean(CC_IND_RET_MINVAR_CVAR)*(std(IND_RET(end-size(CC_IND_RET_EW,1)+1:end,4))/std(CC_IND_RET_MINVAR_CVAR)) - mean(IND_RET(end-size(CC_IND_RET_EW,1)+1:end,4)); % (!!!) no only the rank, but also whether beats the market (beats if positINV_VOLAe)\njalphaMINVAR_CVAR = mean(CC_IND_RET_MINVAR_CVAR) - betaMINVAR_CVAR*mean(IND_RET(end-size(CC_IND_RET_EW,1)+1:end,4)); % if the Jensen's alpha is positINV_VOLAe, then portf_cvarolio has outperfomed the market\nceqMINVAR_CVAR = mean(CC_IND_RET_MINVAR_CVAR) - 0.5*var(CC_IND_RET_MINVAR_CVAR);\nadjsharpeMINVAR_CVAR = sharpeMINVAR_CVAR*(1+skewness(CC_IND_RET_MINVAR_CVAR)/6*sharpeMINVAR_CVAR -(kurtosis(CC_IND_RET_MINVAR_CVAR)-3)/24*sharpeMINVAR_CVAR^2);\n% sharpeCOMB = mean(CC_IND_RET_ALL_COMB_CEQ,1)./std(CC_IND_RET_ALL_COMB_CEQ,1);\n% betaCOMB = cov(CC_IND_RET_ALL_COMB_CEQ,IND_RET(end-size(CC_IND_RET_EW,1)+1:end,4))/var(IND_RET(end-size(CC_IND_RET_EW,1)+1:end,4)); %MArket - SP100\n% betaCOMB = betaCOMB(2,1);\n% treynorCOMB = mean(IND_RET)/betaCOMB;\n% msquaredCOMB = mean(CC_IND_RET_ALL_COMB_CEQ)*(std(IND_RET(end-size(CC_IND_RET_EW,1)+1:end,4))/std(CC_IND_RET_ALL_COMB_CEQ)) - mean(IND_RET(end-size(CC_IND_RET_EW,1)+1:end,4)); % (!!!) not only the rank, but also whether beats the market (beats if positive)\n% jalphaCOMB = mean(CC_IND_RET_ALL_COMB_CEQ) - betaCOMB*mean(IND_RET(end-size(CC_IND_RET_EW,1)+1:end,4)); % if the Jensen's alpha is positive, then portfolio has outperformed the market\n% ceqCOMB = mean(CC_IND_RET_ALL_COMB_CEQ,1) - 0.5*var(CC_IND_RET_ALL_COMB_CEQ,1);\n% adjsharpeCOMB = sharpeCOMB*(1+skewness(CC_IND_RET_ALL_COMB_CEQ, 1)-3)/24*sharpeCOMB^2);\nsharpeMD = mean(CC_IND_RET_MD,1)./std(CC_IND_RET_MD,1);\nbetaMD = cov(CC_IND_RET_MD,IND_RET(end-size(CC_IND_RET_EW,1)+1:end,4))/var(IND_RET(end-size(CC_IND_RET_EW,1)+1:end,4)); %MArket - SP100\nbetaMD = betaMD(2,1);\ntreynorMD = mean(IND_RET)/betaMD;\nmsquaredMD = mean(CC_IND_RET_MD)*(std(IND_RET(end-size(CC_IND_RET_EW,1)+1:end,4))/std(CC_IND_RET_MD)) - mean(IND_RET(end-size(CC_IND_RET_EW,1)+1:end,4)); % (!!!) not only the rank, but also whether beats the market (beats if positINV_VOLAe)\njalphaMD = mean(CC_IND_RET_MD) - betaMD*mean(IND_RET(end-size(CC_IND_RET_EW,1)+1:end,4)); % if the Jensen's alpha is positINV_VOLAe, then portfolio has outperformed the market\nceqMD = mean(CC_IND_RET_MD,1) - 0.5*var(CC_IND_RET_MD,1);\nadjsharpeMD = sharpeMD*(1+skewness(CC_IND_RET_MD,1)/6*sharpeMD-(kurtosis(CC_IND_RET_MD,1)-3)/24*sharpeMD^2);\n% sharpeCOMBNAIVE = mean(CC_IND_RET_ALL_COMB_NAIVE,1)./std(CC_IND_RET_ALL_COMB_NAIVE,1);\n% betaCOMBNAIVE = cov(CC_IND_RET_ALL_COMB_NAIVE,IND_RET(end-size(CC_IND_RET_EW,1)+1:end,4))/var(IND_RET(end-size(CC_IND_RET_EW,1)+1:end,4)); %MArket - SP100\n% betaCOMBNAIVE = betaCOMBNAIVE(2,1);\n% treynorCOMBNAIVE = mean(IND_RET)/betaCOMBNAIVE;\n% msquaredCOMBNAIVE = mean(CC_IND_RET_ALL_COMB_NAIVE)*(std(IND_RET(end-size(CC_IND_RET_EW,1)+1:end,4))/std(CC_IND_RET_ALL_COMB_NAIVE)) - mean(IND_RET(end-size(CC_IND_RET_EW,1)+1:end,4)); % (!!!) not only the rank, but also whether beats the market (beats if positive)\n% jalphaCOMBNAIVE = mean(CC_IND_RET_ALL_COMB_NAIVE) - betaCOMBNAIVE*mean(IND_RET(end-size(CC_IND_RET_EW,1)+1:end,4)); % if the Jensen's alpha is positive, then portfolio has outperformed the market\n% ceqCOMBNAIVE = mean(CC_IND_RET_ALL_COMB_NAIVE,1) - 0.5*var(CC_IND_RET_ALL_COMB_NAIVE,1);\n% adjsharpeCOMBNAIVE = sharpeCOMBNAIVE*(1+skewness(CC_IND_RET_ALL_COMB_NAIVE)/6*sharpeCOMBNAIVE-(kurtosis(CC_IND_RET_ALL_COMB_NAIVE)-3)/24*sharpeCOMBNAIVE^2);\nAWT_END_mat = cell2mat(AWT_END);\nAWT_END_mat(isnan(AWT_END_mat)) = 0;\nAWT_IND_END_mat = cell2mat(AWT_IND_END);\nAWT_IND_END_mat(isnan(AWT_IND_END_mat)) = 0;\nAWT_CVAR_END_mat = cell2mat(AWT_CVAR_END);\nAWT_CVAR_END_mat(isnan(AWT_CVAR_END_mat)) = 0;\nAWT_CVAR_mat = cell2mat(AWT_CVAR);\nAWT_CVAR_mat(isnan(AWT_CVAR_mat)) = 0;\nturnoverMAXRET = sum(sum(abs(cell2mat(AWT(:,2:end))-AWT_END_mat(:,1:end-1))))/(length(cell2mat(AWT_END))-1);\nturnoverIND = 0;\nTURNOVER = [turnoverIND; turnoverIND_MAXSHARPE;...\nturnoverMAXSHARPE; ...\nturnoverMINVAR_CVAR; turnoverMD];%; turnoverCOMBNAIVE;\nCUMRET = [cumretIND; cumretIND_MAXSHARPE;...\ncumretMAXSHARPE; cumretRP; ...\ncumretMINVAR_CVAR; cumretMD];%; cumretCOMBNAIVE;\nSHARPE = [sharpeIND; sharpeIND_MAXSHARPE; sharpeEW; ...\nsharpeMINVAR_CVAR; sharpeMD];%; sharpeCOMBNAIVE;\n\nCEQ = [ceqIND; ceqIND_EW; ceqIND_MAXSHARPE; ceqEW; ceqMINVAR; ...\nceqMINVAR_CVAR; ceqMD];%; ceqCOMBNAIVE;\nADJSHARPE = [adjsharpeIND; adjsharpeIND_MAXSHARPE; adjsharpeEW; ...\nadjsharpeMINVAR_CVAR; adjsharpeMD];%; adjsharpeCOMBNAIVE;\n\n%% Save results to tex tables\ninput.data = [CUMRET(:,end), SHARPE, ADJSHARPE CEQ, TURNOVER ];\ninput.tableColLabels = {'CW', 'SR', 'CEQ', 'Turnover'};\ninput.tableRowLabels = {'SP100','EW Trad Assets ','MV-S Trad assets','EW',...\n'RR-max ret','MinVar','MV- S','ERC', 'MinCVAR', 'MD'};%, 'COMB NA\\\"IVE', 'COMB'}\ninput.transposeTable = 0;\ninput.dataFormat = {'%.3f'};\ninput.dataNanString = '-';input.tableColumnAlignment = 'r';\ninput.tableBorders = 0;\ninput.tableCaption = strcat('Performance rebalancing_', rebal_freq, '_liquidity constraint_',liquidity_const);\ninput.makeCompleteLatexDocument = 0;\nlatex = latexTable(input);\n%% Plots of strategies out-of-sample\nSTRATRGY_TITLE = {'RR - Max ret', 'MV - S', 'MinVar', 'ERC', 'MinCVaR', 'MD'};%, 'COMB NAIVE', 'COMB'}\nclose all\nymin = -1.5;\nymax = 7;\nxmin = DATE(end-length(cell2mat(DATE_OUT')));\nxmax = DATE(end);\npfig = figure\nfor n = 5:size(CUMRET,1)\nsubplot(4,2,n-4)\nplot(DATE(end-length(cell2mat(DATE_OUT')):end),[1,CUMRET(1,:)],'Color', 'k','LineWidth',2)%Index\ndatetick('x','mmmyy')\nhold on\nplot(DATE(end-length(cell2mat(DATE_OUT')):end),[1,CUMRET(n,:)],'Color', 'r','LineWidth',2)%Benchmark\ntitle(STRATRGY_TITLE{n-4})\nylim([ymin ymax])\nxlabel('Date')\nylabel('Cum Wealth')\nhold off\nend\n%set(pfig, 'Position', [10 1000 625]);\nsavefig(pfig, strcat('CUMRET_liquidity_constraint_',liquidity_const,'rebal',rebal_freq,num2str(insample_width),'_',num2str(length(CC_TICK)),'.fig'));\norient(pfig,'portrait');\n%fig.PaperPositionMode = 'manual';\npfig_pos = pfig.PaperPosition;\npfig.PaperSize = [pfig_pos(3)\nsaveas(pfig, strcat('CUMRET_liquidity_constraint_',liquidity_const,'rebal',rebal_freq,num2str(insample_width),'_',num2str(length(CC_TICK)),'.pdf'))\n\n", "output_sequence": "Calculates: Sharpe ratio, Adjusted Sharpe ratio, Turnover, CEQ for 8 portfolios with cryptocurrencies"}, {"input_sequence": "Author: Ya Qian and Chen Zhang ; function [A BB] = coeff(yn,maturity,lamda,mu_q,omega)\n\n%define the NS factor loading\nc1 = ones(120, 1);\nfor i = 1:120\nc1(i) = -i;\n\nc2(i) = -(1 - exp(-lamda*i))/(lamda);\nend;\nBB = [c1 c2 c3];\n%define adjust term\nA=zeros(120,1);\nfor i = 2:120\nA(i,1)=A(i-1,1)+BB(i-1,:)*mu_q+1/2*BB(i-1,:)*omega*BB(i-1,:)';\nend\n%define regression coefficient\nBB(i,1) = -BB(i,1)/i;\nA=A(maturity,:);\n", "output_sequence": "Estimate the arbitrage free Nelson Siegel model"}, {"input_sequence": "Author: Ya Qian and Chen Zhang ; function gb = gamm_rnd(nrow,ncol,m,k)\n% PURPOSE: a matrix of random draws from the gamma distribution\n%---------------------------------------------------\n% USAGE: r = gamm_rnd(nrow,ncol,m,k)\n% where: nrow,ncol = the size of the matrix drawn\n% m = a parameter such that the mean of the gamma = m/k\n% note: m=r/2, k=2 equals chisq r random deviate\n% RETURNS:\n% r = an nrow x ncol matrix of random numbers from the gamma distribution\n% --------------------------------------------------\n% SEE ALSO: gamm_inv, gamm_cdf\n% NOTE: written by: Michael Gordy, 15 Sept 1993\n% mbgordy@athena.mit.edu\n% REFERENCES: Luc Devroye, Non-Uniform Random Variate Generation,\n% New York: Springer Verlag, 1986, ch 9.3-6.\nif nargin ~= 4\nerror('Wrong # of arguments to gamm_rnd');\nend;\ngb=zeros(nrow,ncol);\nif m<=1\n% Use RGS algorithm by Best, p. 426\nc=1/m;\nt=0.07+0.75*sqrt(1-m);\nb=1+exp(-t)*m/t;\nfor i1=1:nrow\naccept=0;\nwhile accept==0\nu=rand; v=b*u;\nif v<=1\nx=t*(v^c);\naccept=((w<=((2-x)/(2+x))) | (w<=exp(-x)));\nelse\nx=-log(c*t*(b-v));\ny=x/t;\naccept=(((w*(m+y-m*y))<=1) | (w<=(y^(m-1))));\nend\ngb(i1,i2)=x;\nend\nelse\n% Use Best's rejection algorithm XG, p. 410\nb=m-1;\nc=3*m-0.75;\nu=rand;\nw=u*(1-u); y=sqrt(c/w)*(u-0.5);\nx=b+y;\nif x >= 0\nz=64*(w^3)*v*v;\naccept=(z<=(1-2*y*y/x)) ...\n| (log(z)<=(2*(b*log(x/b)-y)));\nend\ngb=gb/k;\n\n", "output_sequence": "Estimate the arbitrage free Nelson Siegel model"}, {"input_sequence": "Author: Ya Qian and Chen Zhang ; \n% This procedure is the standard Linear Gaussian State Space Model\n% Using Kalman Filter\n%\nfunction [XNS Xsam Ptt] = ssp2(yy,Zt,mu,F,Q,R,smo)\n\nif (nargin <= 5); smo = 0; end\nbigt = size(yy,1);\nDD = eye(K);\n%% Start Recursion (ref: P378 Hamilton)\nXNS = zeros(K,bigt);\nXini = zeros(K,1); % use 0 as the initial news\nXini(1,1)=0;\nXt_t = Xini;\nvecP = (eye(K^2) - kron(F,F))\\eye(K^2) * Q(:); % vec(P_1|0)\n%vecP=ones(K*K,1)*10^7;\nPt_t = reshape(vecP,K,K); % P_1|0\nfor t = 1:bigt;\n%% Predicting\nXt_lag = F * Xt_t;\nPt_lag = F*Pt_t*F' + DD*Q*DD';\nitat_lag = yy(t,:)' - Zt * Xt_lag - mu;\nft_lag = Zt*Pt_lag*Zt' + R * eye(yn);\n\n%% updating:\nKalg = Pt_lag* Zt'*(ft_lag\\eye(yn)); %% Kalman gain\nXt_t = Xt_lag + Kalg * itat_lag;\nXNS(:,t) = Xt_t;\nPt_t = (Pt_t + Pt_t')/2;\nPtt(:,t) = Pt_t(:);\nend;\n%Ptt\nif smo == 1\nXSM = XNS;\nfor sm = 1 : (bigt - 1)\nPt1_t = reshape(Ptl(:,bigt+1 - sm),K,K);\nXSM(:, bigt - sm) = XNS(:,bigt-sm) + Pt0_t * F' * inv(Pt1_t)* (XSM(:, bigt + 1 - sm) - F * XNS(:,bigt-sm));\nPT = Pt0_t + Pt0_t * F' * inv(Pt1_t)* (Pt1_T - Pt1_t) * inv(Pt1_t)' * F * Pt0_t';\n% PT = chol(PT)' * chol(PT);\nPtT(:, bigt - sm) = PT(:);\nend\nXNS = real(XSM);\nend\nXsam = XNS;\nfor j = 1:bigt\nXsam(:,j) = normrnd(XNS(:,j),real(sqrt(diag(reshape(Ptt(:,j),K,K)))));\n", "output_sequence": "Estimate the arbitrage free Nelson Siegel model"}, {"input_sequence": "Author: Ya Qian and Chen Zhang ; function [s2] = likelihood(y,x,A,BB,h)\ns2=0;\nfor i=1:size(y,1)\ns2=s2+(y(i,:)'- BB*x(:,i) -A)'*(y(i,:)'- BB*x(:,i)-A);\nend\n", "output_sequence": "Estimate the arbitrage free Nelson Siegel model"}, {"input_sequence": "Author: Ya Qian and Chen Zhang ; function w = wish_rnd(sigma,v)\n% PURPOSE: generate random wishart matrix\n%------------------------------------------------\n% USAGE: w = wish_rnd(sigma,n)\n% Where: sigma = symmetric pds input matrix\n% n = degrees of freedom parameter\n% RETURNS:\n% w = random wishart_n(sigma)\n% distributed matrix\n% REFERENCES: Gelman, Carlin, Stern, Rubin, Bayesian Data\n% Analysis, (1995,96) pages 474, 480-481.\n% written by:\n% Aki Vehtari\n% Helsinki University of Technology\n% Lab. of Computational Engineering\n% P.O.Box 9400\n% FIN-02015 HUT\n% FINLAND\n% Aki.Vehtari@hut.fi\nif nargin ~= 2\nerror('Wrong # of arguments to wish_rnd');\nend;\n[n k] = size(sigma);\nif n ~= k\nerror('wish_rnd: requires a square matrix');\nelseif n < k\nwarning('wish_rnd: n must be >= k+1 for a finite distribution');\n[t,p]=chol(sigma);\nif p < 0\nerror('wish_rnd: matrix must be a positive definite');\nend\ny = t'*randn(n,v);\nw = y*y';\n", "output_sequence": "Estimate the arbitrage free Nelson Siegel model"}, {"input_sequence": "Author: Joanna Janczura, Awdesch Melzer ; % clear variables and close windows\nclear all\nclc\nstep=10;\nx=(1:8*step)/step;\ny1=gampdf(x,1,2);\nfigure(1)\nplot(x,y1,'k','LineWidth',2);\nhold on\nplot(x,y2,':r','LineWidth',2);\nxlabel('x','FontSize',16,'FontWeight','Bold');\nset(gca,'Ytick',[0.0:0.1:0.6],'YTickLabel',[0.0:0.1:0.6],'FontSize',16,'FontWeight','Bold')\ntitle('Gamma densities','FontSize',16,'FontWeight','Bold');\nylim([-0.01 0.6]);\nset(gca,'LineWidth',1.6,'FontSize',16,'FontWeight','Bold');\nbox on\n% to save the plot in pdf or png please uncomment next 2 lines:\n%print -painters -dpdf -r600 STFloss04_01.pdf\nfigure(2)\nsemilogy(x,y1,'k','LineWidth',2);\nylim([10e-4, 10e-1]);\n%print -painters -dpdf -r600 STFloss04_02.pdf\nfigure(3)\nx=(1:25*step)/step;\ny1=lognpdf(x,2,1);\nplot(x,y1,'k' ,'LineWidth',2);\nxlim([0 25]);\nylim([-0.01 0.6])\ntitle('Log-normal densities','FontSize',16,'FontWeight','Bold');\nset(gca,'Xtick',[0:5:25],'XTickLabel',[0:5:25],'FontSize',16,'FontWeight','Bold')\n%print -painters -dpdf -r600 STFloss04_03.pdf\nfigure(4)\nsemilogy(x,y1,'k' ,'LineWidth',2);\nylim([10e-4 10e-1])\n%print -painters -dpdf -r600 STFloss04_04.pdf\n", "output_sequence": "Plots three sample gamma pdfs, Gamma(alpha, beta), on linear and Semi-logarithmic scales. Note, that the first one (black solid line) is an exponential law, while the last one (dashed blue line) is a chi-square distribution with nu=6 degrees of freedom. It also plots three sample log-normal pdfs, LogN(mu,sigma), on linear and semi-logarithmic plots. For small sigma the log-normal distribution resembles the Gaussian."}, {"input_sequence": "Author: Joanna Janczura, Awdesch Melzer ; # clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# gamma densities\nstep = 10\nx = (1:(8 * step))/step\ny1 = dgamma(x, shape = 1, scale = 2)\n# linear plot\nplot(x, y1, col = \"black\", type = \"l\", lwd = 2, xlab = \"x\", ylab = \"PDF(x)\", ylim = c(-0.01, 0.6))\ntitle(\"Gamma densities\")\nlines(x, y2, col = \"red3\", lty = 3, lwd = 2)\n# semi-logarithmic plot\ndev.new()\nplot(x, y1, log = \"y\", col = \"black\", type = \"l\", lwd = 2, xlab = \"x\", ylab = \"PDF(x)\", ylim = c(0.001, 1))\npar(new = T)\nplot(x, y2, type = \"l\", log = \"y\", axes = F, frame = F, ylab = \"\", xlab = \"\", col = \"red3\", lty = 3, lwd = 2, ylim = c(0.001,\n1))\nplot(x, y3, log = \"y\", type = \"l\", axes = F, frame = F, ylab = \"\", xlab = \"\", col = \"blue3\", lty = 2, lwd = 2, ylim = c(0.001,\n# log-normal densities\nx = (1:(25 * step))/step\ny1 = dlnorm(x, 2, 1)\ntitle(\"Log-normal densities\")\n", "output_sequence": "Plots three sample gamma pdfs, Gamma(alpha, beta), on linear and Semi-logarithmic scales. Note, that the first one (black solid line) is an exponential law, while the last one (dashed blue line) is a chi-square distribution with nu=6 degrees of freedom. It also plots three sample log-normal pdfs, LogN(mu,sigma), on linear and semi-logarithmic plots. For small sigma the log-normal distribution resembles the Gaussian."}, {"input_sequence": "Author: Joanna Janczura, Awdesch Melzer ; # clear all variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\nmixexppdf = function(x, alpha, beta1, {\n# MIXEXPPDF Mixed exponential probability density function (pdf). Y = MIEXEXPPDF(X,ALPHA,BETA1,BETA2) returns the pdf of\n# the mixed exponential distribution with mixing probability A and distributions parameters BETA1, evaluated at the\n# values in X. For CONTROL=0 the error message is displayed, if the parmeters are negative or a>1. The default values for\n# A, BETA1, are 0.5, 1, 2.\nif (missing(alpha) == T)\nalpha = 0.5\nif (missing(beta1) == T)\nbeta1 = 1\nif (missing(beta2) == T)\nbeta2 = 2\nif (beta1 <= 0) {\nstop(\"Non-positive beta1! Please insert a positive beta1!\")\n}\nif (beta2 <= 0) {\nstop(\"Non-positive beta2! Please insert a positive beta2!\")\nif (alpha <= 0) {\nstop(\"Alpha lesser or equal 0! Please insert alpha between 0 and 1!\")\nif (alpha >= 1) {\nstop(\"Alpha greater or equal 1! Please insert alpha between 0 and 1!\")\n\ny = matrix(0, dim(x)[1],\npos = x > 0\ny[pos] = alpha * beta1 * exp(-beta1 * x[pos]) + (1 - alpha) * beta2 * exp(-beta2 * x[pos])\nreturn(cbind(y))\n}\nstep = 10\nx = cbind((1:(8 * step))/step)\ny1 = dexp(x, 1/3)\ny3 = mixexppdf(x, 0.5, 1)\nplot(x, y1, col = \"black\", type = \"l\", lwd = 2, xlab = \"x\", ylab = \"PDF(x)\", ylim = c(-0.01, 0.8))\ntitle(\"Mixture of two exponential densities\")\nlines(x, y2, col = \"red3\", lty = 3, lwd = 2)\ndev.new()\nplot(x, y1, log = \"y\", col = \"black\", type = \"l\", lwd = 2, xlab = \"x\", ylab = \"PDF(x)\", frame = T, ylim = c(0.01, 1))\npar(new = T)\nplot(x, y2, log = \"y\", axes = F, frame = F, col = \"red3\", type = \"l\", lty = 3, lwd = 2, ylab = \"\", xlab = \"\", ylim = c(0.01,\n1))\nplot(x, y3, log = \"y\", axes = F, frame = F, col = \"blue3\", type = \"l\", lty = 2, lwd = 2, ylab = \"\", xlab = \"\", ylim = c(0.01,\ntitle(\"Mixture of two exponential semi-log densities\")\n", "output_sequence": "Plots a probability density function (pdf) of a mixture of two exponential distributions with mixing parameter a = 0.5 superimposed on the pdfs of the component distributions as well as semi-logarithmic plots. In the semi-logarithmic plot, the exponential pdfs are straight lines with slopes -beta. Note, the curvature of the pdf of the mixture if two exponentials. Requires mixexppdf.m to run the program (see quantnet)."}, {"input_sequence": "Author: Joanna Janczura, Awdesch Melzer ; function y=mixexppdf(x,alpha,beta1,beta2,control)\n%MIXEXPPDF Mixed exponential probability density function (pdf).\n% Y = MIEXEXPPDF(X,ALPHA,BETA1,BETA2) returns the pdf of the mixed\n% exponential distribution with mixing probability A and distributions\n% parameters BETA1, evaluated at the values in X.\n% For CONTROL=0 the error message is displayed, if the parmeters are\n% negative or a>1.\n%\n% The default values for A, BETA1, and CONTROL are 0.5, 1, 2, 0\n% respectively.\nif nargin<5\ncontrol=0;\nend\nif nargin<2\na=.5;\nif nargin<3\nb1=1;\nif nargin<4\nb2=2;\nif(control==0)\nif beta1<=0\nerror('Non-positive beta1!');\nend\nif beta2<=0\nerror('Non-positive beta2!');\nend\nif alpha<=0\nerror('Alpha lesser or equal 0!');\nif alpha>=1\nerror('Alpha greater or equal 1!');\n\ny=zeros(size(x));\npos=find(x>0);\ny(pos)=alpha*beta1*exp(-beta1*x(pos))+(1-alpha)*beta2*exp(-beta2*x(pos));\n", "output_sequence": "Plots a probability density function (pdf) of a mixture of two exponential distributions with mixing parameter a = 0.5 superimposed on the pdfs of the component distributions as well as semi-logarithmic plots. In the semi-logarithmic plot, the exponential pdfs are straight lines with slopes -beta. Note, the curvature of the pdf of the mixture if two exponentials. Requires mixexppdf.m to run the program (see quantnet)."}, {"input_sequence": "Author: Joanna Janczura, Awdesch Melzer ; % clear all variables and close windows\nclear all\nclc\nstep = 10;\nx=(1:8*step)/step;\ny1 = exppdf(x,3);\ny3 = mixexppdf(x,0.5,0.3,1);\nfigure(1)\nplot(x,y1,'k','LineWidth',2);\nhold on\nplot(x,y2,':r','LineWidth',2);\nset(gca,'LineWidth',1.6,'FontSize',16,'FontWeight','Bold');\nbox on\nxlabel('x','FontSize',16,'FontWeight','Bold');\ntitle('Mixture of two exponential densities','FontSize',16,'FontWeight','Bold');\nset(gca,'Ytick',[0.0:0.2:1],'YTickLabel',{0.0,0.2,0.4,0.6,0.8,1.0},'FontSize',16,'FontWeight','Bold')\nylim([-0.01 0.8]);\n% to save the plot in pdf or png please uncomment next 2 lines:\n% print -painters -dpdf -r600 STFloss03_01.pdf\nfigure(2)\nsemilogy(x,y1,'k','LineWidth',2);\ntitle('Mixture of two exponential semi-log densities','FontSize',16,'FontWeight','Bold');\nylim([10e-3 10e-1]);\n% print -painters -dpdf -r600 STFloss03_02.pdf\n", "output_sequence": "Plots a probability density function (pdf) of a mixture of two exponential distributions with mixing parameter a = 0.5 superimposed on the pdfs of the component distributions as well as semi-logarithmic plots. In the semi-logarithmic plot, the exponential pdfs are straight lines with slopes -beta. Note, the curvature of the pdf of the mixture if two exponentials. Requires mixexppdf.m to run the program (see quantnet)."}, {"input_sequence": "Author: Pavel Cizek, Wolfgang K. Haerdle, Rafal Weron ; # Clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# Please change working directory setwd('C:/...')\ndata <- read.delim2(\"SP1997-2005s.txt\")\ntime <- (1:length(data[, 1]))\ndat0 <- data[, 1] - c(mean(data[, 1]))\ndat0 <- dat0/sd(dat0)\ntimet <- (time - 1078)/250 + 2001\nplot(timet[time >= 1075], dat0[time >= 1075], xaxp = c(2001, 2005, 4), xlab = \"Time\", ylab = \"Log-returns\", type = \"l\")\n", "output_sequence": "Plots the daily log-returns of S&P 500 in the years  2001 to 2005."}, {"input_sequence": "Author: Janusz Miskiewicz, Awdesch Melzer ; function retval = umlp(x,y)\n% unidirextional minimum length path algorithm\n% x - the distance matrix\n% y - root of the chain (the number of column)\n%\n% The rsult is presentes as a set of links between nodes\n% Author: Janusz Mi\u015bkiewicz, email: jamis@ift.uni.wroc.pl\n[n,m]=size(x);\nnet=zeros(n-1,3);\nlicz=0;\nonnet(y)=1;\nmaxx=10*max(max(x));\nsmax=maxx*eye(size(x));\nx=x+smax;\nwhile (licz<n-1)\nminx=min(x(y,:));\nit=find(x(y,:)==minx);\nif (length(it) > 1)\ntmp=1;\nwhile ((onnet(it(tmp))==1) && (tmp < length(it)))\ntmp=tmp+1;\nend;\nif (onnet(it(tmp))==0)\nii=it(tmp);\nit=[];\nelse\nii=it(1);\nend;\nif (onnet(it)==0 )\nlicz=licz+1;\nnet(licz,1)=y;\nx(it,y)=maxx;\ny=it;\n\nif ((onnet(it)==1) && (onnet(y)==1))\nend;\nretval=net;\n", "output_sequence": "Presents the analysis of the network evolution. The procedure generate networks in moving time window and calculate the frequency of connections between elements of the chosen WIG 20 companies (gwp.csv). The unidirectional minimum length path is constructed. Requires umlp.m, ultra.m and {Statistical Toolbox} from Matlab to run the quantlet."}, {"input_sequence": "Author: Janusz Miskiewicz, Awdesch Melzer ; # clear history\nrm(list = ls(all = TRUE))\ngraphics.off()\n########################################### Subroutine ultra(x) ############\nultra = function(x) {\n# Ultrametric distance between time series. x - time series matrix\nh = nrow(x)\nretval = sqrt(abs(0.5 * (matrix(1, k, k) - cor(x))))\nreturn(retval)\n}\n########################################### Subroutine umlp(x,y) ##########\numlp = function(x, y) {\n# unidirextional minimum length path algorithm x - the distance matrix y - root of the chain (the number of column) The\n# rsult is presentes as a set of links between nodes\n\nn = nrow(x)\nnet = matrix(0, n - 1, 3)\nlicz = 0\nonnet[y] = 1\nmaxx = 10 * max(x)\nsmax = maxx * diag(nrow = nrow(x), ncol = ncol(x))\nx = x + smax\nwhile (licz < n - 1) {\nminx = min(x[y, ])\nit = which(x[y, ] == minx, arr.ind = T)\n\nif (length(it) > 1) {\ntmp = 1\nwhile ((onnet[it[tmp]] == 1) && (tmp < length(it))) {\ntmp = tmp + 1\n}\nif (onnet[it[tmp]] == 0) {\nii = it(tmp)\n} else {\nii = it[1]\n}\nif (onnet[it] == 0) {\nlicz = licz + 1\nnet[licz, 1] = y\nx[y, it] = maxx\nif ((onnet[it] == 1) && (onnet[y] == 1)) {\n}\nretval = net\n########################################### Main calculation #############\ndata = read.table(\"gwp.csv\", header = T) # load data\ndata = diff(log(data)) # log return\ndl_szer = nrow(data)\npodmioty = ncol(data)\nczes = matrix(0, podmioty,\nwindow = 100 #time window size\n# moving time window\nwynik = matrix(0, dl_szer - window - 1, 5)\nfor (t in 1:(dl_szer - window - 1)) {\nwindow_data = data[t:(t + window - 1), ]\nwind_dist = ultra(window_data)\nwind_umlp = umlp(wind_dist, 5)\nfor (i in 1L:(podmioty - 1)) {\nif (wind_umlp[i, 1] < wind_umlp[i, 2]) {\nczes[wind_umlp[i, 1], wind_umlp[i, 2]] = czes[wind_umlp[i, 1], wind_umlp[i, 2]] + 1\n} else {\nczes[wind_umlp[i, 2], wind_umlp[i, 1]] = czes[wind_umlp[i, 2], wind_umlp[i, 1]] + 1\nwind_umlp = numeric()\ncompanies = c(\"WIG20 \", \"ACP \", \"BIO \", \"BRE \", \"BZW \", \"CEZ \", \"CPS \", \"GTN \", \"GTC \", \"KGH \", \"LTS \", \"PBG \", \"PEO \", \"PKN \",\n\"PKO \", \"PXM \", \"TPS \", \"TVN \")\nfreq = round(1e+05 * czes/sum(sum(czes)))/1000\nfreqTEMP = freq\nrownames(freqTEMP) = companies\nfreqTEMP\n\n", "output_sequence": "Presents the analysis of the network evolution. The procedure generate networks in moving time window and calculate the frequency of connections between elements of the chosen WIG 20 companies (gwp.csv). The unidirectional minimum length path is constructed. Requires umlp.m, ultra.m and {Statistical Toolbox} from Matlab to run the quantlet."}, {"input_sequence": "Author: Janusz Miskiewicz, Awdesch Melzer ; % clear history\nclear all\nclc\ndata = load('gwp.csv'); % load data\ndata = diff(log(data)); % log return\n[dl_szer,podmioty] = size(data);\nczes = zeros(podmioty);\nwindow = 100; %time window size\n% moving time window\nwynik = zeros(dl_szer - window-1,5);\nfor t=1:(dl_szer - window-1)\nwindow_data = data(t:(t+window-1),:);\nwind_dist = ultra(window_data);\nfor i=1:podmioty-1\nif (wind_umlp(i,1)<wind_umlp(i,2))\nczes(wind_umlp(i,1),wind_umlp(i,2))=czes(wind_umlp(i,1),wind_umlp(i,2))+1;\nelse\nczes(wind_umlp(i,2),wind_umlp(i,1))=czes(wind_umlp(i,2),wind_umlp(i,1))+1;\nend;\nend;\nwind_umlp = [];\nend;\ncompanies = [' ','WIG20 ','ACP ','BIO ','CEZ ','GTN ','KGH ','LTS ','PBG ','TPS '];\nfreq = round(100000*czes/sum(sum(czes)))/1000;\nfreqTEMP = [companies1(1:18,:),num2str(freq(:,:))];\ndisp(' WIG20 ACP BIO BRE BZW CEZ CPS GTN KGH LTS PBG PEO PKN PXM TPS TVN ')\ndisp(freqTEMP)\n", "output_sequence": "Presents the analysis of the network evolution. The procedure generate networks in moving time window and calculate the frequency of connections between elements of the chosen WIG 20 companies (gwp.csv). The unidirectional minimum length path is constructed. Requires umlp.m, ultra.m and {Statistical Toolbox} from Matlab to run the quantlet."}, {"input_sequence": "Author: Janusz Miskiewicz, Awdesch Melzer ; function retval=ultra(x)\n% Ultrametric distance between time series.\n% x - time series matrix\n%\n% Author: Janusz Mi\u015bkiewicz, email: jamis@ift.uni.wroc.pl\n[h,k]=size(x);\nretval= sqrt(abs(0.5*(ones(k)-corr(x))));\n", "output_sequence": "Presents the analysis of the network evolution. The procedure generate networks in moving time window and calculate the frequency of connections between elements of the chosen WIG 20 companies (gwp.csv). The unidirectional minimum length path is constructed. Requires umlp.m, ultra.m and {Statistical Toolbox} from Matlab to run the quantlet."}, {"input_sequence": "Author: Zografia Anastasiadou, Rafal Weron ; % clear variables and close windows\nclear all\nclc\ncmd = [1 2];\nif ismember(2,cmd),\nf = figure(2);\nsubplot(1,2,1)\n[x1,y1] = stabpdf_fft(2,1,0,0,6);\nsemilogy(x1,y1,'b-','linewidth',2)\nhold on\nsemilogy(x2,y2,'-.','color',[0 .5 0],'linewidth',1)\nhold off\nxlabel('x')\nlegend('alpha=2','alpha=1.9','alpha=1.5','alpha=0.5','Location','South')\nset(gca,'ylim',[1e-4 .8]);\nsubplot(1,2,2)\n[x1,y1] = stabpdf_fft(1.2,2,0,0,5);\nplot(x1,y1,'b-','linewidth',2)\nplot(x2,y2,'-.','color',[0 .5 0],'linewidth',1)\nplot(x3,y3,'r--','linewidth',1)\nlegend('beta=0','beta=-1','beta=0.5','beta=1','Location','South')\nset(gca,'xtick',-5:2.5:5,'ylim',[0 .16])\n\nprint(f,'-dpsc2','STF2stab02.ps')\n", "output_sequence": "STFstab02.R creates a) a plot of stable probability density function for different betas b) a plot of probability density functions of Gaussian, Cauchy, Levy distributions. STFstab02.m shows a semi-logarithmic plot of symmetric (beta=mu=0) stable densities for four values of alpha and a plot of stable densities for alpha = 1.2 and four values of beta. Requires the \"stabpdf_fft.m\" function."}, {"input_sequence": "Author: Zografia Anastasiadou, Rafal Weron ; function [x,y]=stabpdf_fft(alpha,sigma,beta,mu,xmax,n,S0,mult)\n%STABPDF_FFT Stable probability density function (pdf) via FFT.\n% [X,Y] = STABPDF_FFT(ALPHA,SIGMA,BETA,MU,XMAX,N) returns the stable pdf\n% ((X,Y) pairs) with characteristic exponent, ALPHA, scale, SIGMA,\n% skewness, BETA, and location MU at 2^N evenly spaced values in\n% [-XMAX,XMAX] in the S0 parametrization. Default values for XMAX and N\n% are 20 and 12 respectively.\n% [X,Y] = STABPDF_FFT(ALPHA,SIGMA,BETA,MU,XMAX,N,0) returns the stable\n% pdf in the S parametrization.\n%\n% For .999 < ALPHA < 1.001 the stable pdf is calculated using the formula\n% for ALPHA = 1, otherwise numerical errors creep in.\n% Due to the nature of FFT, values away from the center may be underestimated.\n% For this reason STABPDF_FFT calculates the stable pdf on the interval\n% [-XMAX,XMAX]*2^MULT and then truncates it to the original interval.\n% The default value of MULT is 4, however, for better accuracy use MULT>4.\n% The full syntax is [X,Y] = STABPDF_FFT(ALPHA,SIGMA,BETA,XMAX,N,PARAM,MULT).\n% Reference(s):\n% [1] R.Weron (2004) \"Computationally intensive Value at Risk\n% calculations\", in \"Handbook of Computational Statistics: Concepts and\n% Methods\", eds. J.E. Gentle, W. Haerdle, Y. Mori, Springer, Berlin,\n% 911-950.\n% Copyright (c) 1996-2009 by The Hugo Steinhaus Center\n% 1996.07.01 Rafal Weron\n% 2009.03.27 Rafal Weron\nif nargin < 8,\nmult = 4;\nend\nif nargin < 7,\nS0 = 1;\nif nargin < 6,\nn = 12;\nif nargin < 5;\nxmax = 20;\n% calculate pdf on a twice larger interval\nn = n+mult;\nxmax = xmax*(2^mult);\nM = 2^n;\nR = pi/xmax;\ndt = 1/(R*M);\n\nxx = (-2^(n-1)+.5:(2^(n-1)-.5))/(2^n*dt);\n% stable characteristic function\nif abs(alpha-1) < .001,\nif S0,\nmu = mu - beta*sigma*2/pi*log(sigma);\nend\nyy = exp(-sigma*(abs(xx)).*(1+i*beta.*sign(xx).*(2/pi).*log((abs(xx)))) + i*mu*xx);\nelse\nif S0,\nmu = mu - beta*sigma*tan(0.5*pi*alpha);\nyy = exp(-sigma^alpha.*(abs(xx)).^alpha.*(1-i*beta.*sign(xx).*tan((pi.*alpha)/2)) + i*mu*xx);\nend;\n% FFT\nyy1 = [yy((2^(n-1)+1):2^n), yy(1:2^(n-1))];\nz = real( fft(yy1) )/(2*pi)*R;\n% stable density\nx = (2*pi)*((0:1:(M-1))/(M*R)-1/(2*R));\ny = [z((2^(n-1)+1):2^n), z(1:2^(n-1))];\n% shrink to the original interval\nT = find((x<=xmax/(2^mult)) & (x>=-xmax/(2^mult)));\nx = x(T);\n", "output_sequence": "STFstab02.R creates a) a plot of stable probability density function for different betas b) a plot of probability density functions of Gaussian, Cauchy, Levy distributions. STFstab02.m shows a semi-logarithmic plot of symmetric (beta=mu=0) stable densities for four values of alpha and a plot of stable densities for alpha = 1.2 and four values of beta. Requires the \"stabpdf_fft.m\" function."}, {"input_sequence": "Author: Zografia Anastasiadou, Rafal Weron ; rm(list = ls(all = TRUE))\ngraphics.off()\n# setwd('C:/...')\ninstall.packages(\"fBasics\")\nlibrary(stabledist)\nlibrary(fBasics)\nx <- c(-50:50)/10\nalpha <- 1.2\nbeta <- c(0, 0.5, 1)\n# stable pdfs\nw1 <- dstable(x, alpha, beta = beta[1], pm = 1)\nplot(x, w1, type = \"l\", main = \"Dependence on beta\", xlab = \"x\", ylab = \"PDF(x)\", cex.axis = 2, cex.lab = 1.4, cex.main = 2,\nlwd = 3)\nlines(x, w2, col = \"red\", lwd = 3, lty = 3)\n###################################\n# Gaussian, Cauchy, Levy pdfs\nw5 <- dnorm(x)\nw6 <- dcauchy(x, location = 0, scale = 1)\nw7 <- matrix(0, length(x))\nfor (i in 1:length(x)) {\nif (x[i] > 0) {\nw7[i] <- dstable(x[i], alpha = 0.5, beta = 1, pm = 1)\n}\n}\ndev.new()\nplot(x, w7, type = \"l\", main = \"Gaussian, Cauchy and Levy distributions\", xlab = \"x\", ylab = \"PDF(x)\", cex.axis = 2, cex.lab = 1.4,\ncex.main = 2, col = \"blue\", lwd = 3, lty = 2)\nlines(x, w5, lwd = 3)\nlines(x, w6, col = \"red\", lwd = 3, lty = 3)\n", "output_sequence": "STFstab02.R creates a) a plot of stable probability density function for different betas b) a plot of probability density functions of Gaussian, Cauchy, Levy distributions. STFstab02.m shows a semi-logarithmic plot of symmetric (beta=mu=0) stable densities for four values of alpha and a plot of stable densities for alpha = 1.2 and four values of beta. Requires the \"stabpdf_fft.m\" function."}, {"input_sequence": "Author: Awdesch Melzer ; function [y] = BondOnlyCoupon(C,D,T,r,lambda,parlambda,distr,params,Tmax,N)\nif(lambda ~= 0 && lambda ~= 1 && lambda~=2)\nerror('BondOnlyCoupon: Lambda must be either 0,1 or 2.');\nend\nif(length(C) ~=1)\nerror('BondOnlyCoupon: coupon payments C needs to be a scalar');\nif(length(r) ~=1)\nerror('BondOnlyCoupon: discount rate needs to be a scalar');\nif(length(D)==1)\nerror('BondOnlyCoupon: threshold level D needs to be a vector ');\nif(length(T)==1)\nerror('BondOnlyCoupon: time to expiry T needs to be a vector ');\nx = simNHPPALP(lambda,parlambda,distr,params,Tmax,N);\nTl = length(T);\ny = zeros(Tl*Dl,3);\ni = 1; %loop (times to maturity)\nwyn = 0;\nwhile(i<=Tl)\nwhile(k<=N)\ntraj = [x(:,k,1),x(:,k,2)];\nif (traj(length(traj(find(traj(:,1)<=T(i)),1)),2)<=D(j))\nwyn = wyn + (1-exp(-r*T(i)))/r;\nelse\nwyn = wyn + (1-exp(-r*traj(length(traj(find(traj(:,2)<=D(j)))),1)))/r;\nend\nk = k + 1;\nend\ny((i-1)*Dl+j,1) = T(i);\nwyn = 0;\nj = j + 1;\nend\nj = 1;\ni = i + 1;\nend\n", "output_sequence": "Plots the CAT bond price, for the bond paying only coupons, for the Burr claim amounts and a non-homogeneous Poisson process governing the flow of losses."}, {"input_sequence": "Author: Awdesch Melzer ; function y=Burrrnd(alpha,lambda,tau,n,m)\n%BURRRND Random arrays from Burr distribution.\n% R = BURRRND(ALPHA,LAMBDA,TAU,N,M) returns an M-by-N array of random numbers\n% chosen from the Burr distribution with parameters ALPHA, LAMBDA, TAU.\n%\n% The default values for the parameters ALPHA, LAMBDA, TAU, M, N are\n% 1, 2, 1, respectively.\n% BURRRND uses the inversion method.\nif nargin<5\nm=1;\nend\nif nargin<4\nn=1;\nif nargin<3\ntau=2;\nif nargin<2\nlambda=1;\nif nargin<1\nalpha=1;\n\ny=(lambda*(rand(n,m).^(-1/alpha)-1)).^(1/tau);\n", "output_sequence": "Plots the CAT bond price, for the bond paying only coupons, for the Burr claim amounts and a non-homogeneous Poisson process governing the flow of losses."}, {"input_sequence": "Author: Awdesch Melzer ; function [y] = simNHPPALP(lambda,parlambda,distrib,params,T,N)\n\nif(lambda ~= 0 && lambda ~= 1 && lambda~=2)\nerror('simNHPPALP: Lambda must be either 0,1 or 2.');\nend\nif(T <= 0 || (length(T))~=1)\nerror('simNHPPALP: T must be a positive scalar.');\nif(N <= 0 || (length(N))~=1)\nerror('simNHPPALP: N must be a positive scalar.');\nif(length(parlambda)~=3 && (lambda)~=1)\nerror('simNHPPALP: for lambda 0 or 2, parlambda must be a 3 x 1 vector.');\nif(length(parlambda)~=2 && (lambda)==1)\nerror('simNHPPALP: for lambda 1, parlambda must be a 2 x 1 vector.');\nif((strcmp(distrib,'Burr') || strcmp(distrib,'mixofexps')) && (length(params)~=3))\nerror('simNHPPALP: for Burr and mixofexps distributions, params must be a 3 x 1 vector.');\nif((strcmp(distrib,'gamma') || strcmp(distrib,'lognormal')|| || strcmp(distrib,'Weibull')) && (length(params))~=2)\nerror('simNHPPALP: for gamma, lognormal, Pareto and Weibull distributions, params must be a 2 x 1 vector.');\nif(strcmp(distrib,'exponential') && (length(params))~=1)\nerror('simNHPPALP: for exponential distribution, params must be a scalar.');\nif(strcmp(distrib, 'exponential')==0 && strcmp(distrib, 'gamma')==0 && strcmp(distrib, 'mixofexps')==0 && strcmp(distrib,'Weibull')==0 && strcmp(distrib, 'lognormal')==0 && strcmp(distrib,'Pareto')==0 && strcmp(distrib,'Burr')==0)\nerror('simNHPPALP: distribs should be: exponential, gamma, mixofexps, Weibull, lognormal, Pareto or Burr');\npoisproc = simNHPP(lambda,parlambda,T,N);\nrpp = size(poisproc,1);\nlosses = zeros(rpp,cpp);\nswitch distrib\ncase 'Burr'\ni = 1;\nwhile(i<=N)\naux = min(find(poisproc(:,i,1)==T,i,'first'));\nif(aux>2)\nlaux = cumsum(Burrrnd(params(1),params(2),params(3),aux/2-1,1));\nlosses(3:aux,i) = laux(ceil((1:(aux-2))/2));\nif(aux<rpp)\nlosses((aux+1):rpp,i) = laux(length(laux))*ones(rpp-aux,1);\nend\nelse\nlosses(:,i)=zeros(rpp,1);\nend\ni = i + 1;\nend\ncase'exponential'\naux = min(find(poisproc(:,i,1)==T,i,'first'));\nlaux = cumsum(exprnd(1/params(1),aux/2-1,1));\nlosses(3:aux,i) = laux(ceil((1:aux-2)/2));\ncase 'gamma'\nlaux = cumsum(gamrnd(params(1),(1/params(2)),aux/2-1,1));\nlosses(:,i) = zeros(rpp,1);\ncase 'lognormal'\nlaux = cumsum(lognrnd(params(1),params(2),aux/2-1,1));\ncase 'mixofexps'\nlaux = cumsum(mixexprnd(params(1),params(2),params(3),aux/2-1,1));\ncase 'Pareto'\nlaux = cumsum(Paretornd(params(1),params(2),aux/2-1,1));\ncase 'Weibull'\nlaux=cumsum(wblrnd(params(1)^(-1/params(2)),params(2),aux/2-1,1));\ny = zeros(size(poisproc));\ny(:,:,1) = poisproc(:,:,1);\ny(:,:,2) = losses;\nend\n", "output_sequence": "Plots the CAT bond price, for the bond paying only coupons, for the Burr claim amounts and a non-homogeneous Poisson process governing the flow of losses."}, {"input_sequence": "Author: Awdesch Melzer ; rm(list = ls(all = TRUE))\ngraphics.off()\n########################## SUBROUTINES ##########################\nBondOnlyCoupon = function(C, D, T, r, lambda, parlambda, distr, params, Tmax, N) {\n# computes price of the CAT bond paying only coupons for the given claim amount distribution and the non-homogeneous Poisson\n# process governing the flow of losses ---------------------------------------------------------------------------- y =\n# BondOnlyCoupon(C,D,T,r,lambda,parlambda,distr,params,Tmax,N)\n# ---------------------------------------------------------------------------- Input: Parameter: C Definition: scalar,\n# coupon payments (cease at the threshold time or Tmax) Parameter: D Definition: n1 x 1 vector, threshold level Parameter: T\n# Definition: n2 x 1 vector, time to expiry Parameter: r Definition: scalar, continuously-compounded discount rate\n# Parameter: lambda Definition: scalar, intensity function, if lambda=0, a sine function if lambda=1, a linear function if\n# lambda=2, a sine square function Parameter: parlambda Definition: n x 1 vector, parameters of the intensity function\n# lambda (n=2 for lambda=1, n=3 otherwise) Parameter: distrib Definition: string, claim size distribution Parameter: params\n# Definition: n x 1 vector, parameters of the claim size distribution n = 1 (exponential) n = 2 (gamma, lognormal, Pareto,\n# Weibull) n = 3 (Burr, mixofexps) Parameter: Tmax Definition: scalar, time horizon Parameter: N Definition: scalar, number\n# of trajectories ---------------------------------------------------------------------------- Output: Parameter: y\n# Definition: m x 3 matrix, the first column are times to bond's expiration, the second threshold levels and the third\n# corresponding prices of the bond ----------------------------------------------------------------------------\n\nif (lambda != 0 && lambda != 1 && lambda != 2) {\nstop(\"BondOnlyCoupon: Lambda must be either 0,1 or 2.\")\n}\nif (length(C) != 1) {\nstop(\"BondOnlyCoupon: coupon payments C needs to be a scalar\")\nif (length(r) != 1) {\nstop(\"BondOnlyCoupon: discount rate needs to be a scalar\")\nif (length(D) == 1) {\nstop(\"BondOnlyCoupon: threshold level D needs to be a vector \")\nif (length(T) == 1) {\nstop(\"BondOnlyCoupon: time to expiry T needs to be a vector \")\nx = simNHPPALP(lambda, parlambda, distr, params, Tmax, N)\nTl = length(T)\ny = matrix(0, Tl * Dl, 3)\ni = 1 #loop (times to maturity)\nwyn = 0\nwhile (i <= Tl) {\nwhile (k <= N) {\ntraj = cbind(x[, k, 1], x[, k, 2])\nif (traj[length(traj[which(traj[, 1] <= T[i]), 1]), 2] <= D[j]) {\nwyn = wyn + (1 - exp(-r * T[i]))/r\n} else {\nwyn = wyn + (1 - exp(-r * traj[length(traj[which(traj[, 2] <= D[j])]), 1]))/r\n}\nk = k + 1\n}\ny[(i - 1) * Dl + j, 1] = T[i]\nwyn = 0\n}\nj = 1\nreturn(y)\n}\nBurrrnd = function(alpha, lambda, tau, n, m) {\n# BURRRND Random arrays from Burr distribution. --------------------------------------------------------------------- R =\n# BURRRND(ALPHA,LAMBDA,TAU,N,M) returns an M-by-N array of random numbers chosen from the Burr distribution with parameters\n# ALPHA, LAMBDA, TAU. --------------------------------------------------------------------- The default values for the\n# parameters ALPHA, LAMBDA, TAU, M, N are 1, 2, 1, respectively. BURRRND uses the inversion method.\n# ---------------------------------------------------------------------\nif (missing(m)) {\nm = 1\nif (missing(n)) {\nn = 1\nif (missing(tau)) {\ntau = 2\nif (missing(lambda)) {\nlambda = 1\nif (missing(alpha)) {\nalpha = 1\nu = matrix(0, n, m)\nfor (i in 1:m) {\nu[, i] = (lambda * (runif(n, 0, 1)^(-1/alpha) - 1))^(1/tau)\ny = u\nmixexprnd = function(p, beta1, n, m) {\n# MIXEXPRND Random arrays from the mixed exponential distribution.\n# --------------------------------------------------------------------- Y = MIXEXPRND(P,BETA1,BETA2,N,M) returns an M-by-N\n# array of random numbers chosen from the mixed exponential distribution with parameters P, BETA1,\n# --------------------------------------------------------------------- The default values for A, BETA1, N, M are\n# 0.5, 1, 2, 1, respectively. MIXEXPRND uses the exponential number generator.\nif (missing(p)) {\np = 0.5\nif (missing(beta1)) {\nbeta1 = 1\nif (missing(beta2)) {\nbeta2 = 2\ny = rexp(n * m, rate = (1/beta2))\naux = which(runif(n * m, 0, 1) <= p)\nif (!missing(aux)) {\ny[aux] = rexp(length(aux), 1/beta1)\ny = matrix(y, n, m)\nsimHPP = function(lambda, T, N) {\n# SIMHPP Homogeneous Poisson process. --------------------------------------------------------------------- Y =\n# SIMHPP(lambda,T,N) generates N trajectories of the homogeneous Poisson process with intensity LAMBDA. T is the time\n# horizon. ---------------------------------------------------------------------\nif (lambda <= 0 || length(lambda) != 1) {\nstop(\"simHPP: Lambda must be a positive scalar.\")\nif (T <= 0 || length(T) != 1) {\nstop(\"simHPP: T must be a positive scalar.\")\nif (N <= 0 || length(N) != 1) {\nstop(\"simHPP: N must be a positive scalar.\")\nEN = rpois(N, lambda * T)\nym = matrix(T, 2 * max(EN) + 2, N)\ny = tmp\ny[, , 1] = ym\ny[, , 2] = matrix(1, 2 * max(EN) + 2, 1) %*% t(EN)\ni = 1\nwhile (i <= N) {\nif (EN[i] > 0) {\nttmp = c(sort(T * runif(EN[i])))\ny[1:(2 * EN[i] + 1), i, 1] = c(0, ttmp[ceiling((1:(2 * EN[i]))/2)])\n} else {\ny[1, i, 1] = 0\ny[1:(2 * EN[i] + 2), i, 2] = c(0, floor((1:(2 * EN[i]))/2), EN[i])\nsimNHPP = function(lambda, parlambda, T, N) {\n# SIMNHPP Non-homogeneous Poisson process. --------------------------------------------------------------------- Y =\n# SIMNHPP(lambda,parlambda,T,N) generates N trajectories of the non-homogeneous Poisson process with intensity specified by\n# LAMBDA (0 - sine function, 1 - linear function, 2 - sine square function) with paramters in PARLAMBDA. T is the time\n# horizon. The function usues thining method. ---------------------------------------------------------------------\n# lambda: scalar, intensity function, sine function (lambda=0), linear function (lambda=1) or sine square function\n# (lambda=2) parlambda: n x 1 vector, parameters of the intensity function lambda (n=2 for lambda=1, n=3 otherwise) T:\n# scalar, time horizon N: scalar, number of trajectories\na = parlambda[1]\nif (lambda == 0) {\nd = parlambda[3]\nJM = simHPP(a + b, T, N)\n} else if (lambda == 1) {\nJM = simHPP(a + b * T, N)\n} else if (lambda == 2) {\nrjm = nrow(JM)\nyy = array(0, c(rjm, N, 2))\nyy[, , 1] = matrix(T, nrow = rjm, ncol = N)\nmaxEN = 0\npom = JM[, i, 1][JM[, i, 1] < T]\npom = pom[2 * (1:(length(pom)/2))]\nR = runif(NROW(pom))\nif (lambda == 0) {\nlambdat = (a + b * sin(2 * pi * (pom + d)))/(a + b)\nif (lambda == 1) {\nlambdat = (a + b * pom)/(a + b * T)\n} else {\nif (lambda == 2) {\nlambdat = (a + b * sin(2 * pi * (pom + d))^2)/(a + b)\npom = pom[R < lambdat]\nEN = NROW(pom)\nmaxEN = max(maxEN, EN)\nyy[1:(2 * EN + 1), i, 1] = c(0, rep(pom, each = 2))\nyy[2:(2 * EN), i, 2] = c(floor((1:(2 * EN - 1))/2))\nyy[(2 * EN + 1):rjm, i, 2] = matrix(EN, nrow = rjm - 2 * EN, ncol = 1)\nyy = yy[1:(2 * maxEN + 2), , ]\nreturn(yy)\nParetornd = function(alpha, lambda, n, m) {\n# PARETORND Random arrays from Pareto distribution. --------------------------------------------------------------------- Y\n# = PARETORND(ALPHA,LAMBDA,N,M) returns an M-by-N array of random numbers chosen from the Pareto distribution with\n# parameters ALPHA, LAMBDA. The default values for ALPHA, LAMBDA, N, M 1, respectively. PARETORND uses the\n# inversion method. ---------------------------------------------------------------------\nu[, i] = lambda * (runif(n, 0, 1)^(-1/alpha) - 1)\nsimNHPPALP = function(lambda, parlambda, distrib, params, T, N) {\n# generates aggregate loss process driven by the non-homogeneous Poisson process.\n# --------------------------------------------------------------------- y = simNHPPALP(lambda,parlambda,distrib,params,T,N)\n# array, generated process - max is the maximum number of jumps for all generated trajectories\n# --------------------------------------------------------------------- Input: Parameter: lambda Definition: scalar,\n# intensity function, sine function (lambda=0), linear function (lambda=1) or sine square function (lambda=2) Parameter:\n# parlambda Definition: n x 1 vector, parameters of the intensity function lambda (n=2 for lambda=1, n=3 otherwise)\n# Parameter: distrib Definition: string, claim size distribution Parameter: params Definition: n x 1 vector, parameters of\n# the claim size distribution n = 1 (exponential) n = 2 (gamma, lognormal, Pareto, Weibull) n = 3 (Burr, mixofexps)\n# Parameter: T Definition: scalar, time horizon Parameter: N Definition: scalar, number of trajectories\nstop(\"simNHPPALP: Lambda must be either 0,1 or 2.\")\nif (T <= 0 || (length(T)) != 1) {\nstop(\"simNHPPALP: T must be a positive scalar.\")\nif (N <= 0 || (length(N)) != 1) {\nstop(\"simNHPPALP: N must be a positive scalar.\")\nif (length(parlambda) != 3 && (lambda) != 1) {\nstop(\"simNHPPALP: for lambda 0 or 2, parlambda must be a 3 x 1 vector.\")\nif (length(parlambda) != 2 && (lambda) == 1) {\nstop(\"simNHPPALP: for lambda 1, parlambda must be a 2 x 1 vector.\")\nif ((distrib == \"Burr\" || distrib == \"mixofexps\") && (length(params)) != 3) {\nstop(\"simNHPPALP: for Burr and mixofexps distributions, params must be a 3 x 1 vector.\")\nif ((distrib == \"gamma\" || distrib == \"lognormal\" || distrib == \"Pareto\" || distrib == \"Weibull\") && (length(params)) !=\n2) {\nstop(\"simNHPPALP: for gamma, lognormal, Pareto and Weibull distributions, params must be a 2 x 1 vector.\")\nif (distrib == \"exponential\" && (length(params)) != 1) {\nstop(\"simNHPPALP: for exponential distribution, params must be a scalar.\")\nif (distrib != \"exponential\" && distrib != \"gamma\" && distrib != \"mixofexps\" && distrib != \"Weibull\" && distrib != \"lognormal\" &&\ndistrib != \"Pareto\" && distrib != \"Burr\") {\nstop(\"simNHPPALP: distribs should be: exponential, gamma, mixofexps, Weibull, lognormal, Pareto or Burr\")\npoisproc = simNHPP(lambda, parlambda, T, N)\nrpp = dim(poisproc)[1]\nlosses = matrix(0, rpp, cpp)\nif (distrib == \"Burr\") {\ni = 1\nwhile (i <= N) {\naux = min(as.matrix(which(poisproc[, 1] == T))) #[1:i,])\nif (aux > 2) {\nlaux = cumsum(Burrrnd(params[1], params[2], aux/2 - 1, 1))\nlosses[3:aux, i] = laux[ceiling((1:(aux - 2))/2)]\nif (aux < rpp) {\nlosses[(aux + 1):rpp, i] = matrix(laux[length(laux)], rpp - aux, 1)\nlosses[, i] = rep(0, rpp)\ni = i + 1\n} else if (distrib == \"exponential\") {\nlaux = cumsum(rexp(aux/2 - 1, rate = 1/params[1]))\nlosses[3:aux, i] = laux[ceiling((1:aux - 2)/2)]\n} else if (distrib == \"gamma\") {\nlaux = cumsum(rgamma(aux/2 - 1, shape = params[1], rate = params[2], scale = (1/params[2])))\n} else if (distrib == \"lognormal\") {\nlaux = cumsum(rlnorm(aux/2 - 1, meanlog = params[1], sdlog = params[2]))\n} else if (distrib == \"mixofexps\") {\nlaux = cumsum(mixexprnd(params[3], params[1], aux/2 - 1, 1))\n} else if (distrib == \"Pareto\") {\nlaux = cumsum(Paretornd(params[1], params[2], aux/2 - 1, 1))\n} else if (distrib == \"Weibull\") {\nlaux = cumsum(rweibull(aux/2 - 1, scale = params[1]^(-1/params[2]), shape = params[2]))\nif (N == 1) {\ny = array(0, dim(poisproc))\ny[, 1] = poisproc[, 1]\n} else {\ny[, , 1] = poisproc[, , 1]\ny[, , 2] = losses\n########################## MAIN PROGRAM ##########################\n# parlambda = 34.2 # HPP\nparlambda = c(35.32, 2.32 * 2 * pi, -0.2) # NHPP1\ndistr = \"Burr\" # 'lognormal'\n# params = c(18.3806,1.1052) # lognormal\nparams = c(0.4801, 3.9495 * 1e+16, 2.1524) # Burr\ndata = read.table(\"ncl.dat\")\nA = mean(data[, 3]) * (34.2/4)\nC = 0.06\nna = 41 # default 41\nD = seq(A, length = na, by = (12 * A - A)/(na - 1))\nB = 0.25\nnb = 41 # default 41\nT = seq(B, length = nb, by = (8 * B - B)/(nb - 1))\nTmax = max(T)\nlambda = 0\nN = 100 # default 1000\nr = log(1.025)\nd1 = BondOnlyCoupon(C, D, T, r, lambda, parlambda, distr, params, Tmax, N)\ny = d1[, 1]\nx = d1[, 2]/1e+09\nz = d1[, 3]\ndata = data.frame(cbind(x, y, z))\nrequire(lattice)\npar.set = list(axis.line = list(col = \"transparent\"), clip = list(panel = \"off\"))\nwireframe(z ~ x + y, data = data, screen = list(z = 30, x = -60), drape = TRUE, colorkey = F, ticktype = \"detailed\", scales = list(arrows = FALSE,\ncol = \"black\", distance = 1, tick.number = 8, cex = 0.7, x = list(labels = round(seq(min(x), max(x), length = 11),\ny = list(labels = round(seq(min(y), max(y), length = 11), z = list(labels = round(seq(min(z), max(z), length = 11),\n2))), xlab = list(\"\", rot = 30, cex = 1.2), ylab = list(\"\", rot = -40, cex = 1.2), zlab = list(\"\", rot = 95, cex = 1.1),\npar.settings = par.set)\n", "output_sequence": "Plots the CAT bond price, for the bond paying only coupons, for the Burr claim amounts and a non-homogeneous Poisson process governing the flow of losses."}, {"input_sequence": "Author: Awdesch Melzer ; function y = simHPP(lambda,T,N)\n% SIMHPP Homogeneous Poisson process.\n% Y = SIMHPP(lambda,T,N) generates N trajectories of the\n% homogeneous Poisson process with intensity LAMBDA. T is the time\n% horizon.\nif lambda <= 0\nerror('simHPP: Lambda must be a positive real number');\nend\nif T <= 0\nerror('simHPP: T must be a positive real number');\nEN = poissrnd(lambda*T,N,1);\nmax(EN) ;\ny=T*ones(max(EN)+1,N);\ni=1;\nwhile(i<=N)\ny(1,i)=0;\nif EN(i)>0\ny(2:EN(i)+1,i)=sort(T*rand(EN(i),1));\nend\ni=i+1;\n", "output_sequence": "Plots the CAT bond price, for the bond paying only coupons, for the Burr claim amounts and a non-homogeneous Poisson process governing the flow of losses."}, {"input_sequence": "Author: Awdesch Melzer ; close all\nclear all\nclc\n%parlambda = 34.2 % HPP\nparlambda = [35.32,2.32*2*pi,-0.2]; % NHPP1\ndistr = 'Burr'; % \"lognormal\"\n%params = c(18.3806,1.1052) % lognormal\nparams = [0.4801,3.9495*1e16,2.1524]; % Burr\ndata = load('ncl.dat');\nA = mean(data(:,3))*(34.2/4);\nC = 0.06;\nna = 41; % default 41\nAE = 12*A;\nD = A:(12*A-A)/(na-1):AE;\nB = 0.25;\nnb = 41; % default 41\nBE = 8*B;\nT = B:(8*B-B)/(nb-1):BE;\nTmax = max(T);\nlambda = 0;\nN = 100; % default 1000\nr = log(1.025);\nd1 = BondOnlyCoupon(C,D,T,r,lambda,parlambda,distr,params,Tmax,N);\ny = d1(:,1);\nyy = reshape(y,na,nb);\n% Plot\nmesh(xx,yy,zz)\nxlim([min(x),max(x)])\n\nset(gca,'FontSize',16,'LineWidth',2,'FontWeight','bold');\n% print -painters -dpdf -r600 STFcat05.pdf\n", "output_sequence": "Plots the CAT bond price, for the bond paying only coupons, for the Burr claim amounts and a non-homogeneous Poisson process governing the flow of losses."}, {"input_sequence": "Author: Awdesch Melzer ; function [y] = simNHPP(lambda,parlambda,T,N)\n\n% SIMNHPP Non-homogeneous Poisson process.\n% Y = SIMNHPP(lambda,parlambda,T,N) generates N trajectories of the\n% non-homogeneous Poisson process with intensity specified by LAMBDA\n% (0 - sine function, 1 - linear function, 2 - sine square function)\n% with paramters in PARLAMBDA. T is the time horizon. The function\n% usues thining method.\na = parlambda(1);\nif (a<=0)\nerror('simNHPP: a must be a positive real number');\nend\nif (a+b<= 0)\nerror('simNHPP: b does not fulfill special condition');\nif (T <= 0)\nerror('simNHPP: T must be a positive real number');\nif (lambda == 0)\nc = parlambda(3);\nJM = simHPP(a+b,T,N);\nelseif( lambda==1)\nJM = simHPP(a+b*T,T,N);\nelseif(lambda==2)\n\nrjm = size(JM,1);\ny = ones(rjm,N,2);\ny(:,:,1) = T*y(:,:,1);\nmaxEN = 0;\nwhile(i<=N)\nJMind = find(JM(:,i,1)<T);\npom = JM(JMind,i,1);\npoml = length(pom);\npom = pom(2*(1:(poml-1)/2));\nU = unifrnd(0,1,length(pom),1);\n\nif(lambda == 0)\nlambdat = (a+b*sin(2*pi*(pom+c)))/(a+b);\nelseif(lambda == 1)\nlambdat = (a+b*pom)/(a+b*T);\nelseif(lambda == 2)\nlambdat = (a+b*sin(2*pi*(pom+c)).^2)/(a+b);\nend\npomind = find(U<lambdat);\nEN = length(pom);\nmaxEN = max([maxEN,EN]);\ny(1:(2*EN+1),i,1) = [0;pom(ceil((1:(2*EN))/2))];\ny((2*EN+1):rjm,i,2) = EN*ones((rjm-2*EN),1);\ni = i+1;\ny = y(1:(2*maxEN+2),:,:);\n", "output_sequence": "Plots the CAT bond price, for the bond paying only coupons, for the Burr claim amounts and a non-homogeneous Poisson process governing the flow of losses."}, {"input_sequence": "Author: Zografia Anastasiadou, Awdesch Melzer ; rm(list = ls(all = TRUE))\n# setwd('C:/...')\ninstall.packages(\"abind\")\nlibrary(abind)\n# generates a homogeneous Poisson process with intensity lambda\nsimHPP <- function(lambda, T, N) {\n# lambda: scalar, intensity of the Poisson process T: scalar, time horizon N: scalar, number of trajectories\nEN <- rpois(N, lambda * T)\ny <- matrix(T, nrow = 2 * max(EN) + 2, ncol = N) * matrix(1, nrow = 2 * max(EN) + 2, ncol = N)\nyy <- abind(y, matrix(1, nrow = 2 * max(EN) + 2, ncol = N) * EN, along = 3)\ni = 1\nwhile (i <= N) {\nif (EN[i] > 0) {\nyy[1:(2 * EN[i] + 1), i, 1] <- c(0, rep(sort(T * runif(EN[i])), each = 2))\n} else {\nyy[1, i, 1] = 0\n}\nyy[1:(2 * EN[i] + 2), i, 2] <- c(0, floor((1:(2 * EN[i]))/2), EN[i])\ni = i + 1\n}\nreturn(yy)\n}\n# generates a non-homogeneous Poisson process with intensity lambda\nsimNHPP <- function(lambda, parlambda, T, N) {\n# lambda: scalar, intensity function, sine function (lambda=0), linear function (lambda=1) or sine square function\n# (lambda=2) parlambda: n x 1 vector, parameters of the intensity function lambda (n=2 for lambda=1, n=3 otherwise) T:\n# scalar, time horizon N: scalar, number of trajectories\na <- parlambda[1]\nif (lambda == 0) {\nc <- parlambda[3]\nJM <- simHPP(a + b, T, N)\n} else {\nif (lambda == 1) {\nJM <- simHPP(a + b * T, N)\nif (lambda == 3) {\nJM <- simHPP(a + b * T, N)\n}\nrjm <- nrow(JM)\nyy <- abind(matrix(T, nrow = rjm, ncol = N), matrix(0, nrow = rjm, ncol = N), along = 3)\nmaxEN = 0\npom <- JM[, i, 1][JM[, i, 1] < T]\npom <- pom[2 * (1:(length(pom)/2))]\nR <- runif(NROW(pom))\nif (lambda == 0) {\nlambdat <- (a + b * sin(2 * pi * (pom + c)))/(a + b)\nif (lambda == 1) {\nlambdat <- (a + b * pom)/(a + b * T)\n} else {\nif (lambda == 3) {\nlambdat <- (a + b * sin(2 * pi * (pom + c))^2)/(a + b)\n}\npom <- pom[R < lambdat]\nEN <- NROW(pom)\nmaxEN <- max(maxEN, EN)\nyy[1:(2 * EN + 1), i, 1] <- c(0, rep(pom, each = 2))\nyy[2:(2 * EN), i, 2] <- c(floor((1:(2 * EN - 1))/2))\nyy[(2 * EN + 1):rjm, i, 2] <- matrix(EN, nrow = rjm - 2 * EN, ncol = 1)\nyy <- yy[1:(2 * maxEN + 2), , ]\n# generates aggregate loss process driven by the non-homogeneous Poisson process for lognormal distribution\nsimNHPPALP <- function(lambda, parlambda, params, T, N) {\n# (lambda=2) parlambda: n x 1 vector, parameters of the intensity function lambda (n=2 for lambda=1, n=3 otherwise) params:\n# n x 1 vector, parameters of the lognormal distribution # T: scalar, time horizon N: scalar, number of trajectories\nif (N == 1) {\npoisproc <- simNHPP(lambda, parlambda, T, N)\npoisproc <- abind(matrix(poisproc[, 1]), matrix(poisproc[, 2]), along = 3)\nrpp <- nrow(poisproc)\nlosses <- matrix(0, nrow = rpp, ncol = cpp)\naux <- min(which(poisproc[, i, 1] == T))\nif (aux > 2) {\nlaux <- cumsum(rlnorm(aux/2 - 1, params[1],\nlosses[3:aux, i] <- rep(laux, each = 2)\nif (aux < rpp) {\nlosses[(aux + 1):rpp, i] <- laux[NROW(laux)] * matrix(1, nrow = rpp - aux)\nlosses[, i] <- matrix(0, nrow = rpp)\ny <- abind(poisproc[, , 1], losses)\nreturn(y)\nset.seed(2)\nlambda1 <- 0 # intensity\nparlambda1 <- c(35.32, 2.32 * 2 * pi, -0.2) # parameters of intensity function\nparams1 <- c(18.3806, 1.1052) # parameters of the lognormal distribution\nT1 <- 10 # time\nN1 <- 1 # trajectories\nN2 <- 100\ny1 <- simNHPPALP(lambda1, parlambda1, T1, N1) # aggregate loss process for lognormal distribution\ny1[, 2] <- y1[, 2]/1e+09\nx <- read.table(\"ncl.dat\")\nt <- rep(x[, 2], each = 2)\nt1 <- c(0, t, T1)\nPCS <- cumsum(x[, 3])/1e+09\nPCS1 <- rep(PCS, each = 2)\nPCS1 <- c(0, 0, PCS1)\nz <- cbind(t1, PCS1)\n# mean of aggregate loss process\nt2 <- (0:(100 * T1))/100\nRP <- exp(params1[1] + params1[2]^2/2) * (parlambda1[1] * t2 - parlambda1[2]/2/pi * (cos(2 * pi * (t2 + parlambda1[3])) - cos(2 *\npi * parlambda1[3])))\nme <- cbind(t2, RP/1e+09)\n# computes quantiles of trajectories\nquantilelines <- function(data, step, perc) {\n# data: n x m x 2 array, data, where n is the length of trajectories and m the number of trajectories step: scalar, time\n# interval between points at which the quantiles are computed perc: s x 1 vector, orders of quantiles\nN <- ncol(data)\nbegin <- data[1, 1, 1]\nnumofpoints <- (theend - begin)/step + 1\nvecstep <- seq(begin, theend, step)\ny <- matrix(0, nrow = 1, ncol = NROW(perc))\nwhile (i <= numofpoints) {\nj = 1\nvecval = 0\nwhile (j <= N) {\naux1 = data[, j, 1]\npos = sum(aux1 <= vecstep[i])\nif (pos < R) {\nvecval <- c(vecval, aux2[pos] + (vecstep[i] - aux1[pos]) * (aux2[pos + 1] - aux2[pos])/(aux1[pos + 1] - aux1[pos]))\nvecval <- c(vecval, aux2[pos])\nj = j + 1\ny <- rbind(y, quantile(vecval[2:(N + 1)], perc))\ny = cbind(vecstep, y[2:(numofpoints + 1), ])\nstep1 <- 0.05 #time\nperc1 <- c(0.05, 0.95) #quantiles\n# quantiles of trajectories\nqq1 <- quantilelines(simNHPPALP(lambda1, parlambda1, T1, N2), step1, perc1[1]) # 0.05 quantile of trajectories\nqq1 <- cbind(qq1[, 1], qq1[, 2]/1e+09)\nplot(y1, type = \"l\", col = \"blue\", ylim = c(0, 120), xlab = \"Years\", ylab = \"Aggregate loss process (USD billion)\", cex.lab = 1.4,\ncex.axis = 1.4, lwd = 2)\nlines(me, col = \"red\", lty = 2, lwd = 2)\nlines(qq1[1:200, ], col = \"brown\", lty = 3, lwd = 2)\nabline(h = 60, lwd = 3)\n", "output_sequence": "Produces the plot of an aggregate loss process together with a real-world trajectory, two sample quantile lines and mean of the process."}, {"input_sequence": "Author: Zografia Anastasiadou, Awdesch Melzer ; function y = simHPP(lambda,T,N)\n% SIMHPP Homogeneous Poisson process.\n% Y = SIMHPP(lambda,T,N) generates N trajectories of the\n% homogeneous Poisson process with intensity LAMBDA. T is the time\n% horizon.\nif lambda <= 0\nerror('simHPP: Lambda must be a positive real number');\nend\nif T <= 0\nerror('simHPP: T must be a positive real number');\nEN = poissrnd(lambda*T,N,1);\nmax(EN) ;\ny=T*ones(max(EN)+1,N);\ni=1;\nwhile(i<=N)\ny(1,i)=0;\nif EN(i)>0\ny(2:EN(i)+1,i)=sort(T*rand(EN(i),1));\nend\ni=i+1;\n", "output_sequence": "Produces the plot of an aggregate loss process together with a real-world trajectory, two sample quantile lines and mean of the process."}, {"input_sequence": "Author: Zografia Anastasiadou, Awdesch Melzer ; clear all\nclose all\nclc\nRandStream.setGlobalStream(RandStream('mt19937ar','seed',99));\nlambda = 0; % default Poisson process NHPP1\nparlambda = [35.32,2.32*2*pi,-0.2]; % NHPP1\ndistr = 'lognormal'; % default distribution\nparams = [18.3806,1.1052];\nT = 10; % max time (years)\nN = 100; % number of trajectories in NHPPALP, default 5000\nqu = [0.05,0.95]; % quantiles\nstep = 0.05; % step in quantile plot\n%=========== aggregate process - single realization\ny = simNHPPALP(lambda,parlambda,distr,params,T,1);\ny(:,:,2) = y(:,:,2)/1e+9;\n%=========== real PCS trajectory\nc = load('ncl.dat');\nt2 = c(ceil((1:2*size(c(:,2),1))/2),2);\nt2 = [0;t2;T];\nPCS = cumsum(c(:,3))/1e+9;\nPCS2 = PCS(ceil((1:2*size(PCS,1))/2));\nPCS2 = [0;0;PCS2];\nz = [t2,PCS2];\n%=========== mean of aggregate loss process (only for NHPP1 and lognormal loss size distribution)\nt = [(0:100*T)/100]';\nRP = exp(params(1)+params(2).^2/2).*(parlambda(1)*t-parlambda(2)/2/pi*(cos(2*pi*(t+parlambda(3))) - cos(2*pi*parlambda(3))));\nme = [t,RP/1e+9];\n%=========== quantiles\nvqu = quantilelines(simNHPPALP(lambda,parlambda,distr,params,T,N),step,qu');\n%=========== plot\nplot(y(:,:,1),y(:,:,2),'b-')\nhold on\nplot(me(:,1),me(:,2),'r-.')\nplot(z(:,1),z(:,2),'g-','LineWidth',1.5)\nplot(vqu(:,1),vqu(:,2)/1e+9, 'm--')\nxlim([0,10])\nxlabel('Years','FontSize',16,'FontWeight','bold')\nylabel('Aggregate loss process (USD billion)','FontSize',16,'FontWeight','bold')\nhold off\nbox on\nset(gca,'FontSize',16,'LineWidth',2,'FontWeight','bold');\n% print -painters -dpdf -r600 STFcat02.pdf\n", "output_sequence": "Produces the plot of an aggregate loss process together with a real-world trajectory, two sample quantile lines and mean of the process."}, {"input_sequence": "Author: Zografia Anastasiadou, Awdesch Melzer ; function [y] = simNHPPALP(lambda,parlambda,distrib,params,T,N)\n\nif(lambda ~= 0 && lambda ~= 1 && lambda~=2)\nerror('simNHPPALP: Lambda must be either 0,1 or 2.');\nend\nif(T <= 0 || (length(T))~=1)\nerror('simNHPPALP: T must be a positive scalar.');\nif(N <= 0 || (length(N))~=1)\nerror('simNHPPALP: N must be a positive scalar.');\nif(length(parlambda)~=3 && (lambda)~=1)\nerror('simNHPPALP: for lambda 0 or 2, parlambda must be a 3 x 1 vector.');\nif(length(parlambda)~=2 && (lambda)==1)\nerror('simNHPPALP: for lambda 1, parlambda must be a 2 x 1 vector.');\nif((strcmp(distrib,'Burr') || strcmp(distrib,'mixofexps')) && (length(params)~=3))\nerror('simNHPPALP: for Burr and mixofexps distributions, params must be a 3 x 1 vector.');\nif((strcmp(distrib,'gamma') || strcmp(distrib,'lognormal')|| || strcmp(distrib,'Weibull')) && (length(params))~=2)\nerror('simNHPPALP: for gamma, lognormal, Pareto and Weibull distributions, params must be a 2 x 1 vector.');\nif(strcmp(distrib,'exponential') && (length(params))~=1)\nerror('simNHPPALP: for exponential distribution, params must be a scalar.');\nif(strcmp(distrib, 'exponential')==0 && strcmp(distrib, 'gamma')==0 && strcmp(distrib, 'mixofexps')==0 && strcmp(distrib,'Weibull')==0 && strcmp(distrib, 'lognormal')==0 && strcmp(distrib,'Pareto')==0 && strcmp(distrib,'Burr')==0)\nerror('simNHPPALP: distribs should be: exponential, gamma, mixofexps, Weibull, lognormal, Pareto or Burr');\npoisproc = simNHPP(lambda,parlambda,T,N);\nrpp = size(poisproc,1);\nlosses = zeros(rpp,cpp);\nswitch distrib\ncase 'Burr'\ni = 1;\nwhile(i<=N)\naux = min(find(poisproc(:,i,1)==T,i,'first'));\nif(aux>2)\nlaux = cumsum(Burrrnd(params(1),params(2),params(3),aux/2-1,1));\nlosses(3:aux,i) = laux(ceil((1:(aux-2))/2));\nif(aux<rpp)\nlosses((aux+1):rpp,i) = laux(length(laux))*ones(rpp-aux,1);\nend\nelse\nlosses(:,i)=zeros(rpp,1);\nend\ni = i + 1;\nend\ncase'exponential'\naux = min(find(poisproc(:,i,1)==T,i,'first'));\nlaux = cumsum(exprnd(1/params(1),aux/2-1,1));\nlosses(3:aux,i) = laux(ceil((1:aux-2)/2));\ncase 'gamma'\nlaux = cumsum(gamrnd(params(1),(1/params(2)),aux/2-1,1));\nlosses(:,i) = zeros(rpp,1);\ncase 'lognormal'\nlaux = cumsum(lognrnd(params(1),params(2),aux/2-1,1));\ncase 'mixofexps'\nlaux = cumsum(mixexprnd(params(1),params(2),params(3),aux/2-1,1));\ncase 'Pareto'\nlaux = cumsum(Paretornd(params(1),params(2),aux/2-1,1));\ncase 'Weibull'\nlaux=cumsum(wblrnd(params(1)^(-1/params(2)),params(2),aux/2-1,1));\ny = zeros(size(poisproc));\ny(:,:,1) = poisproc(:,:,1);\ny(:,:,2) = losses;\nend\n", "output_sequence": "Produces the plot of an aggregate loss process together with a real-world trajectory, two sample quantile lines and mean of the process."}, {"input_sequence": "Author: Zografia Anastasiadou, Awdesch Melzer ; function [y] = quantilelines(data,step,perc)\nif (size(step,1)~=1||step<=0)\nerror('quantiles: step must be a positive scalar.');\nend\nif (size(perc,2)~=1||size(perc,1)==1)\nerror('quantiles: perc must be n x 1 vector.');\nif(exist('perc')==0)\nperc =(1:9)/10;\nend\nN = size(data,2);\nbegin = data(1,1,1);\nnumofpoints = (theend-begin)/step+1;\nlast = begin+(numofpoints-1)*step;\nvecstep = (begin:step:last)';\ny = zeros(1,size(perc,1));\ni = 1;\nwhile(i<=numofpoints)\nj = 1;\nvecval = 0;\nwhile(j<=N)\naux1 = data(:,j,1);\npos = sum(aux1<=vecstep(i));\nif(pos<R)\nvecval=[vecval;aux2(pos)+(vecstep(i)-aux1(pos))*(aux2(pos+1)-aux2(pos))/(aux1(pos+1)-aux1(pos))];\nelse\nvecval=[vecval;aux2(pos)];\nend\nj = j + 1;\nend\ny = [y;(quantile(vecval(2:N+1),perc))'];\ni = i + 1;\ny = [vecstep,y(2:numofpoints+1,:)];\n", "output_sequence": "Produces the plot of an aggregate loss process together with a real-world trajectory, two sample quantile lines and mean of the process."}, {"input_sequence": "Author: Zografia Anastasiadou, Awdesch Melzer ; function [y] = simNHPP(lambda,parlambda,T,N)\n\n% SIMNHPP Non-homogeneous Poisson process.\n% Y = SIMNHPP(lambda,parlambda,T,N) generates N trajectories of the\n% non-homogeneous Poisson process with intensity specified by LAMBDA\n% (0 - sine function, 1 - linear function, 2 - sine square function)\n% with paramters in PARLAMBDA. T is the time horizon. The function\n% usues thining method.\na = parlambda(1);\nif (a<=0)\nerror('simNHPP: a must be a positive real number');\nend\nif (a+b<= 0)\nerror('simNHPP: b does not fulfill special condition');\nif (T <= 0)\nerror('simNHPP: T must be a positive real number');\nif (lambda == 0)\nc = parlambda(3);\nJM = simHPP(a+b,T,N);\nelseif( lambda==1)\nJM = simHPP(a+b*T,T,N);\nelseif(lambda==2)\n\nrjm = size(JM,1);\ny = ones(rjm,N,2);\ny(:,:,1) = T*y(:,:,1);\nmaxEN = 0;\nwhile(i<=N)\nJMind = find(JM(:,i,1)<T);\npom = JM(JMind,i,1);\npoml = length(pom);\npom = pom(2*(1:(poml-1)/2));\nU = unifrnd(0,1,length(pom),1);\n\nif(lambda == 0)\nlambdat = (a+b*sin(2*pi*(pom+c)))/(a+b);\nelseif(lambda == 1)\nlambdat = (a+b*pom)/(a+b*T);\nelseif(lambda == 2)\nlambdat = (a+b*sin(2*pi*(pom+c)).^2)/(a+b);\nend\npomind = find(U<lambdat);\nEN = length(pom);\nmaxEN = max([maxEN,EN]);\ny(1:(2*EN+1),i,1) = [0;pom(ceil((1:(2*EN))/2))];\ny((2*EN+1):rjm,i,2) = EN*ones((rjm-2*EN),1);\ni = i+1;\ny = y(1:(2*maxEN+2),:,:);\n", "output_sequence": "Produces the plot of an aggregate loss process together with a real-world trajectory, two sample quantile lines and mean of the process."}, {"input_sequence": "Author: Joanna Janczura, Awdesch Melzer ; % ---------------------------------------------------------------------\n% clear variables and close windows\nclear all\nclc\nstep=20;\n% Burr densities\nx=(1:144*step)/step;\ny1=Burrpdf(x,0.5,2,1.5);\nfigure(1)\nplot(x,y1,'k','LineWidth',2);\nhold on\nplot(x,y2,':r','LineWidth',2);\nxlim([0,8])\nxlabel('x','FontSize',16,'FontWeight','Bold');\ntitle('Burr densities','FontSize',16,'FontWeight','Bold');\nylim([0 1.2]);\nset(gca,'LineWidth',1.6,'FontSize',16,'FontWeight','Bold');\nbox on\n% to save the plot in pdf or png please uncomment next 2 lines:\nprint -painters -dpdf -r600 STFloss06_01.pdf\n% Weibull densities\ny1=wblpdf(x,1.^(-1./0.5),0.5);\nfigure(2)\nxlim([0,5])\ntitle('Weibull densities','FontSize',16,'FontWeight','Bold');\nylim([-0.01 1.2]);\nprint -painters -dpdf -r600 STFloss06_02.pdf\n% Burr double-logarithmic densities\nfigure(3)\nloglog(x,y1,'k','LineWidth',2);\nylim([10e-5, 10e-1]);\n%print -painters -dpdf -r600 STFloss06_03.pdf\n% Weibull double-logarithmic densities\nfigure(4)\nsemilogy(x,y1,'k','LineWidth',2);\nylim([10e-4 10e-1]);\n%print -painters -dpdf -r600 STFloss06_04.pdf\n", "output_sequence": "Plots three sample Burr pdfs, Burr(alpha, lambda, tau), on linear and double-logarithmic scales and three sample Weibull pdfs, Weib(beta, tau), on a linear and semi-logarithmic scales. We can see that for tau<1 the tails are much heavier and they look like power-law. Requires Burrpdf.m to run the program (see quantnet)."}, {"input_sequence": "Author: Joanna Janczura, Awdesch Melzer ; function y=Burrpdf(x,alpha,lambda,tau)\n%BURRPDF Burr probability density function (pdf).\n% Y = BURRPDF(X,ALPHA,LAMBDA,TAU) returns the pdf of the Burr\n% distribution with parameters ALPHA,LAMBDA,TAU, evaluated at the values in X.\n% For CONTROL=0 the error message is displayed, if the parmeters are\n% negative.\n%\n% The default values for the parameters ALPHA, LAMBDA, TAU, CONTROL are\n% 1, 2, 0, respectively.\nif nargin<4\ntau=2;\nend\nif nargin<3\nlambda=1;\nend\nif nargin<2\nalpha=1;\nif nargin<1\nerror('stats:normpdf:TooFewInputs','Input argument X is undefined.');\nif tau<=0\nerror('Non-positive tau!');\nif lambda<=0\nerror('Non-positive sigma!');\nif alpha<=0\nerror('Non-positive alpha!');\ny=zeros(size(x));\npos=find(x>0);\ny(pos)=tau*alpha*lambda^alpha*x(pos).^(tau-1).*(lambda+x(pos).^tau).^(-alpha-1);\n", "output_sequence": "Plots three sample Burr pdfs, Burr(alpha, lambda, tau), on linear and double-logarithmic scales and three sample Weibull pdfs, Weib(beta, tau), on a linear and semi-logarithmic scales. We can see that for tau<1 the tails are much heavier and they look like power-law. Requires Burrpdf.m to run the program (see quantnet)."}, {"input_sequence": "Author: Joanna Janczura, Awdesch Melzer ; # clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\nBurrpdf = function(x, alpha, lambda, tau) {\n# BURRPDF Burr probability density function (pdf). Y = BURRPDF(X,ALPHA,LAMBDA,TAU) returns the pdf of the Burr distribution\n# with parameters ALPHA,LAMBDA,TAU, evaluated at the values in X. For CONTROL=0 the error message is displayed, if the\n# parmeters are negative. The default values for the parameters ALPHA, LAMBDA, TAU, CONTROL are 1, 2, 0, respectively.\nif (missing(tau) == TRUE) {\ntau = 2\n}\nif (missing(lambda) == TRUE) {\nlambda = 1\nif (missing(alpha) == TRUE) {\nalpha = 1\nif (missing(x) == TRUE) {\nstop(\"stats:normpdf:TooFewInputs! Input argument X is undefined.\")\nif (lambda <= 0) {\nstop(\"Non-positive lambda!\")\nif (alpha <= 0) {\nstop(\"Non-positive alpha!\")\n\nx = cbind(x)\ny = matrix(0, dim(x)[1],\npos = x > 0\ny[pos] = tau * alpha * lambda^alpha * x[pos]^(tau - 1) * (lambda + x[pos]^tau)^(-alpha - 1)\n}\nstep = 20\n# Burr densities\nx = (1:(144 * step))/step\ny1 = Burrpdf(x, 0.5, 2, 1.5)\n# Burr linear plot\nplot(x, y1, col = \"black\", type = \"l\", lwd = 2, xlab = \"x\", ylab = \"PDF(x)\", ylim = c(0, 1.2), xlim = c(0, 8))\ntitle(\"Burr densities\")\nlines(x, y2, col = \"red3\", lty = 3, lwd = 2)\n# Burr double-logarithmic densities\ndev.new()\nplot(x, y1, log = \"xy\", col = \"black\", type = \"l\", lwd = 2, xlab = \"x\", ylab = \"PDF(x)\", ylim = c(1e-04, 1), xlim = c((0.1),\n(100)))\npar(new = T)\nplot(x, y2, type = \"l\", log = \"xy\", axes = F, frame = F, ylab = \"\", xlab = \"\", col = \"red3\", lty = 3, lwd = 2, ylim = c(1e-04,\n1), xlim = c((0.1), (100)))\nplot(x, y3, type = \"l\", log = \"xy\", axes = F, frame = F, ylab = \"\", xlab = \"\", col = \"blue3\", lty = 2, , lwd = 2, ylim = c(1e-04,\n# Weibull densities\ny1 = dweibull(x, shape = 0.5, scale = (1^(-1/0.5)))\n# Weibull linear plot\nplot(x, y1, col = \"black\", type = \"l\", lwd = 2, xlab = \"x\", ylab = \"PDF(x)\", ylim = c(0, 1.2), xlim = c(0, 5))\ntitle(\"Weibull densities\")\n# Weibull semi-logarithmic plot\nplot(x, y1, log = \"y\", col = \"black\", type = \"l\", lwd = 2, xlab = \"x\", ylab = \"PDF(x)\", ylim = c(0.001, 1), xlim = c(0, 5))\nplot(x, y2, type = \"l\", log = \"y\", axes = F, frame = F, ylab = \"\", xlab = \"\", col = \"red3\", lty = 3, lwd = 2, ylim = c(0.001,\n1), xlim = c(0, 5))\nplot(x, y3, type = \"l\", log = \"y\", axes = F, frame = F, ylab = \"\", xlab = \"\", col = \"blue3\", lty = 2, , lwd = 2, ylim = c(0.001,\ntitle(\"Weibull densities\")\n", "output_sequence": "Plots three sample Burr pdfs, Burr(alpha, lambda, tau), on linear and double-logarithmic scales and three sample Weibull pdfs, Weib(beta, tau), on a linear and semi-logarithmic scales. We can see that for tau<1 the tails are much heavier and they look like power-law. Requires Burrpdf.m to run the program (see quantnet)."}, {"input_sequence": "Author: Marius Sterling ; #!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Mon May 27 14:49:33 2019\n@author: ms\nimport os\nimport json_tricks\nimport numpy as np\nimport sys\nsys.path.append('/home/ms/github/ob_nw')\nfrom LOB.LOB_keras_train_class import LOB_keras_train_class\n# %%\nPATH_results = '/home/ms/github/ob_nw/results/innvestigate_analyse/'\nMODEL_NAME = 'LOB_keras_model17a'\nPATH_NN_MODEL = os.path.join(PATH_NN, MODEL_NAME)\nmodel = 'params_N'\npath_save = '.'\nyhats = {i: dict() for i in ['train', 'valid', 'test']}\nys_dict = {}\nfor f in {'___'.join(f.split('___')[:2]) for f in os.listdir(PATH_NN_MODEL)\nif 'z___' in f and model in f}:\nparams, time_start = f.split('___')\npath_nn_model = PATH_NN_MODEL\nf = sorted([i for i in os.listdir(path_nn_model) if (\nparams + '___' in i and time_start in i)\nand 'model' in i\nkey=lambda x:\nint(x.replace(f, '').replace('___model_', '')\n.replace('.h5', '')))[-1]\nf = os.path.join(path_nn_model, f)\nprint(f)\na = LOB_keras_train_class(os.path.basename(path_nn_model))\na.set_model_params(os.path.join('LOB/params_files', params + '.json'))\nif hasattr(a.model_params, 'set_submodel_weights'):\na.model_params['load_model'] = f\na.model_params['tcn']['base']['dropout_rate'] = 0\na.set_up_model()\nfor data_split in ['train', 'valid', 'test']:\nids = range(0, len(a.gen[data_split]), 1)\nxs = np.concatenate([a.gen[data_split][i][0] for i in ids])\nif 'ys' not in locals() or data_split not in ys_dict:\nys = np.concatenate([a.gen[data_split][i][1] for i in ids])\nys_dict.update({data_split: ys})\nyhats[data_split].update({params: a.model.predict(x=xs, verbose=1)})\ndel xs, ids\ndel ys\nbreak\nfor k, v in yhats.items():\nwith open(f'{path_save}/preds_T_yhats_{k}.json', 'w') as outfile:\njson_tricks.dump(v, outfile)\nfor k, v in ys_dict.items():\nwith open(f'{path_save}/preds_T_ys_{k}.json', 'w') as outfile:\nfor data_split in ['train', 'valid', 'test']:\nprint(data_split)\nres = {}\nwith open(f'{path_save}/preds_T_yhats_{data_split}.json') as json_file:\nyhats = json_tricks.load(json_file)\nwith open(f'{path_save}/preds_T_ys_{data_split}.json') as json_file:\nys = json_tricks.load(json_file)\nfor k, v in yhats.items():\nmse = np.mean((v - ys) ** 2, axis=0)\nmde = 1 - np.mean((ys > 0) == (v > 0), axis=0)\nres.update({k: {'mse': mse, 'mde': mde}})\nwith open(f'{path_save}/preds_T_errors_{data_split}.json', 'w') as outfile:\njson_tricks.dump(res, outfile)\n", "output_sequence": "Joint Limit order book ask and bid price predition for multiple predicton horizons with a neural networks. This script computes the MSE loss for the trained model for the fixed order book level application for training, validation and test dataset."}, {"input_sequence": "Author: Marius Sterling ; #!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Mon May 27 14:49:33 2019\n@author: ms\nimport os\nimport numpy as np\nfrom scipy.stats import jarque_bera\nimport matplotlib.pyplot as plt\nimport sys\nsys.path.append('..')\nfrom LOB.LOB_keras_train_class import LOB_keras_train_class\n# %%\nPATH_NN = 'results/nn_model_training/'\nMODEL_NAME = 'LOB_keras_model17a'\nPATH_NN_MODEL = os.path.join(PATH_NN, MODEL_NAME)\npath_save = '.'\n# %% Seting up model\nmodels = [\nsorted([f for f in os.listdir(PATH_NN_MODEL)\nif m.rstrip('.json') + '___' in f and 'model' in f and '.h5' in f],\nkey=lambda x: int(x.split(\"___\")[2].replace('model_', '').\nreplace(\".h5\", \"\").replace('_TEST', '')))[-1]\nfor m in ['params_L10z']]\nf = models[0]\nf = os.path.join(PATH_NN_MODEL, f)\na = LOB_keras_train_class(os.path.basename(PATH_NN_MODEL))\na.set_model_params(\nos.path.join('LOB/params_files',\nos.path.basename(f).split('___')[0] + '.json'))\nif hasattr(a.model_params, 'set_submodel_weights'):\na.model_params['load_model'] = f\na.model_params['lob_model'][\"targets_standardize_by_sqrt_time\"] = False\na.model_params['lob_model']['log_return_method'] = 'level_1_prices'\na.model_params['lob_model']['pred_horizon'] = [i for i in range(1, 61)]\na.model_params['lob_model'][\"targets_stacked\"] = True\n'train': [\n'Data/Matlab/2015-07-06.mat',\n]\n}\na.set_up_model()\n# %% Computation of h-step log returns\ngen = a.gen['train']\nres = [gen[i][1] for i in range(len(gen))]\nres = np.concatenate(res, axis=0)\nph = res.shape[1] // 2\n# %% Ratio of zero h-step log returns\ntmp = (res == 0).mean(axis=0)[::2] * 100\nplt.plot(range(1, len(tmp) + 1), tmp, c='green', label='bid')\ntmp = (res == 0).mean(axis=0)[1::2] * 100\nplt.plot(range(1, len(tmp) + 1), tmp, c='red', label='ask')\nplt.xlabel('$h$')\nplt.ylabel('Ratio of zero $h$-step returns in %')\nplt.ylim([0, 90])\nfor type in ['pdf', 'png']:\nplt.savefig(f'{path_save}/log_returns_zero_ratio.{type}',\nbbox_inches='tight', dpi=300)\nplt.show()\n# %% Standard deviation of h-step log returns\ntmp = res\nplt.plot(range(1, ph + 1), tmp[:, ::2].std(axis=0), c='red', label='ask')\nplt.ylim([0, tmp.std(axis=0).max() + 0.01])\nnorm = np.arange(1, ph + 1)**0.5\nplt.plot(range(1, ph + 1), tmp[:, ::2].std(axis=0) / norm, linestyle='dashed',\nc='red', label='ask2')\nplt.plot(range(1, ph + 1), tmp[:, 1::2].std(axis=0) / norm, linestyle='dashed',\nc='green', label='bid2')\nplt.ylabel('Standard Deviation of $h$-step log return')\nplt.savefig(f'{path_save}/log_returns_std.{type}',\n# %% Standard deviation standardized by $h^0.5$ of h-step log returns\nplt.plot(range(1, ph + 1), tmp[:, ::2].std(axis=0) / norm, c='red',\nlabel='ask')\nplt.plot(range(1, ph + 1), tmp[:, 1::2].std(axis=0) / norm, c='green',\nlabel='bid')\n# plt.ylim([tmp.std(axis=0).min()*0, tmp.std(axis=0).max() + 0.1])\nplt.ylabel('Sample Standard Deviation / $\\\\sqrt{h}$')\nplt.savefig(f'{path_save}/log_returns_std_by_time.{type}',\n# %% Mean h-step log returns\nplt.plot(range(1, ph + 1), tmp[:, ::2].mean(axis=0), c='red', label='ask')\nlinestyle='dashed', c='green', label='bid2')\nplt.plot(range(1, ph + 1), tmp[:, 0::2].mean(axis=0) / norm,\nlinestyle='dashed', c='red', label='ask2')\nplt.ylabel('Mean $h$-step log returns')\nplt.savefig(f'{path_save}/log_returns_mean.{type}',\n# %% Skewness of h-step log-returns\ntmp -= tmp.mean(axis=0)\nplt.plot(range(1, ph + 1), (tmp[:, ::2] ** 3).mean(axis=0),\nc='red', label='ask')\nplt.plot(range(1, ph + 1), (tmp[:, 1::2] ** 3).mean(axis=0),\nc='green', label='bid')\nplt.ylabel('Skewness of $h$-step log returns')\nplt.savefig(f'{path_save}/log_returns_skewness.{type}',\n# %% Kurtosis of h-step log-returns\nplt.plot(range(1, ph + 1), (tmp[:, ::2] ** 4).mean(axis=0),\nplt.plot(range(1, ph + 1), np.arange(1, ph + 1) * 0 + 3, c='gray',\nlabel='zero')\nplt.ylim([2.5, 100])\nplt.yscale('log')\nplt.ylabel('Kurtosis of $h$-step log returns')\nplt.savefig(f'{path_save}/log_returns_kurtosis.{type}',\n# %% Bera Jarque normality statistic\ntmp = np.array([jarque_bera(res[:, i])[0] for i in range(res.shape[1])])\nplt.plot(range(1, ph + 1), tmp[::2], c='red', label='ask')\nplt.ylabel('Jarque-Bera statistic for $h$-step log returns')\nplt.savefig(f'{path_save}/log_returns_normality.{type}',\n# %% Correlation\ncor_ask = np.array([corr(res[:, i], res[:, 0])[0] for i in\nrange(0, res.shape[1], 2)])\ncor_bid = np.array([corr(res[:, i], res[:, 1])[0] for i in\nrange(1, res.shape[1], 2)])\ncor_ask_bid = np.array([corr(res[:, i], res[:, 1])[0] for i in\nrange(0, res.shape[1], 2)])\ncor_bid_ask = np.array([corr(res[:, i], res[:, 0])[0] for i in\nrange(1, res.shape[1], 2)])\nplt.plot(range(0, ph), cor_ask, c='red', label='ask')\nlinestyle='dashed')\nplt.plot(range(0, ph), cor_bid_ask, c='green', label='bid2',\nplt.xlabel('Time difference $h$')\nplt.ylabel('Correlation of returns')\nplt.savefig(f'{path_save}/log_returns_correlation.{type}',\n", "output_sequence": "Joint Limit order book ask and bid price predition for multiple predicton horizons with a neural networks. This script analysis and visualizes properties of the limit order book ask and bid prices."}, {"input_sequence": "Author: Georg Keilbar ; rm(list = ls())\nsetwd(\"~/Dropbox/InteractivefixedEffect/2020_Projection_IFE/Code/Simulation\")\nlibrary(splines)\nlibrary(mgcv)\nlibrary(gsynth)\n##simulation function\nsimulation = function(S, N, T, Q, K){\n\n#1.1 result matrix\nresult2 = result3 = vector(length=4)\n#1.2 fix regression coefficients\nbeta = c(2,1,-1)\n#1.3 simulate regressors\nset.seed(1)\nxbar = matrix(rnorm(N*Q,1,0.5),N,Q)\nset.seed(2)\npi = array(rnorm(N*T*Q,sd=0.5),dim=c(N,T,Q))\nx = array(0,dim=c(N,T,Q))\nfor (t in 1:T){\nx[,t,] = pi[,t,]+xbar\n}\n#1.4 generate functions g_kq\nset.seed(3)\na = matrix(runif(K*Q,-1,1),K,Q)\nset.seed(4)\nb = matrix(runif(K*Q,-1,1),K,Q)\ng = matrix(0,N,K)\ng[,1] = a[1,1]*xbar[,1]^2+b[1,2]*xbar[,2]\n#1.5 get basis functions, projection matrix, residual maker matrix\n#J = floor(sqrt(N))\nJ = 3\ndesignX = array(0,dim =c(N,J,Q))\nfor (q in 1: Q){\ndesignX[,,q] = bs(xbar[,q], intercept = T, degree=J-1)\nphi = matrix(0,N,J*Q)\nfor (i in 1:N){\nphi[i,] = as.vector(designX[i,,])\nA = solve(t(phi) %*% phi + 0.00001*diag(J*Q))\nphi = cbind(xbar,xbar^2)\nA = solve(t(phi) %*% phi)\nP = phi %*% A %*% t(phi)\nM = diag(N) - P\nfor (s in 1:S){\nprint(paste(\"s = \",s))\n#1.6 simulate factors\n#F = VAR.sim(diag(0.5,K,K),T,include=\"none\",varcov=diag(0.75,K))\n#2.1 simulate error term u_it, idiosyncratic effects gamma_i\nu = matrix(rnorm(N*T,0,1),N,T)\ngamma = matrix(0,N,K)\n#u = matrix(rt(N*T,df=10),N,T)\n#2.2 generate y\ny = matrix(0,N,T)\nfor(i in 1:N){\ny[i,] = x[i,,]%*%beta + as.matrix(F,T,K)%*% (g[i,]+gamma[i]) + u[i,]\n}\n#2.3 Projection-based IFE estimator\nsumaa = matrix(0, Q, Q)\nfor(t in 1:T){\naa = (t(x[,t,])%*%M%*%x[,t,])\nsumaa = aa+ sumaa\n}\nbetahat = solve(sumaa)%*%sumbb\n\n#2.4 Estimation of interactive fixed effect components\nytilde = matrix(0,N,T)\nfor (t in 1:T){\nytilde[,t] = y[,t]-x[,t,]%*%betahat\nFhat = eigen(t(ytilde)%*%P%*%ytilde)$vectors[,1:K]*sqrt(T)\nGhat = 1/T * P%*%ytilde%*%Fhat\n\nFhat_PCA = eigen(t(ytilde)%*%ytilde)$vectors[,1:K]*sqrt(T)\nlambdahat_PCA = 1/T * ytilde%*%Fhat_PCA\nF0 = sqrt(T)*eigen(F%*%t(g)%*%g%*%t(F))$vector[,1:K]\nG0 = 1/T*g%*%t(F)%*%F0\nFmax = max(abs(F0-Fhat))\nFFrob = sqrt(sum((F0-Fhat)^2))/sqrt(T)\nGmax = max(abs(G0-Ghat))\nGFrob = sqrt(sum((G0-Ghat)^2))/sqrt(N)\nresult2[1] = result2[1] + Fmax\nFmax_PCA = max(abs(F0-Fhat_PCA))\nFFrob_PCA = sqrt(sum((F0-Fhat_PCA)^2))/sqrt(T)\nlambdamax_PCA = max(abs(G0+gamma-lambdahat_PCA))\nlambdaFrob_PCA = sqrt(sum((G0+gamma-lambdahat_PCA)^2))/sqrt(N)\nresult3[1] = result3[1] + Fmax_PCA\nresult2 = result2/S\nresult = rbind(result2,result3)\nreturn(result)\n}\n##1\n#T=10 fixed\nresults = array(0,dim=c(11,2,4))\nresults[1,,] = simulation(1000,25,10,3,3)\n#F max error\npdf(file=\"FMax_T10.pdf\",width=4,height=3)\npar(mar=c(2,2,1,1))\nplot(results[,1,1]~c(25,seq(50,500,50)),type=\"l\",ylim=c(0,4.2),col=\"red\",lwd=3,xlab=\"\",ylab=\"\")\nlines(results[,2,1]~c(25,seq(50,500,50)),col=\"blue\",lty=\"dashed\",lwd=3)\ndev.off()\n#F Frobenius error\npdf(file=\"FFrob_T10.pdf\",width=4,height=3)\nplot(results[,1,2]~c(25,seq(50,500,50)),type=\"l\",ylim=c(0,2),col=\"red\",lwd=3,xlab=\"\",ylab=\"\")\nlines(results[,2,2]~c(25,seq(50,500,50)),col=\"blue\",lty=\"dashed\",lwd=3)\n#G max error\npdf(file=\"GMax_T10.pdf\",width=4,height=3)\nplot(results[,1,3]~c(25,seq(50,500,50)),type=\"l\",ylim=c(0,3.5),col=\"red\",lwd=3,xlab=\"\",ylab=\"\")\nlines(results[,2,3]~c(25,seq(50,500,50)),col=\"blue\",lty=\"dashed\",lwd=3)\n#G Frobenius error\npdf(file=\"GFrob_T10.pdf\",width=4,height=3)\nplot(results[,1,4]~c(25,seq(50,500,50)),type=\"l\",ylim=c(0,1.5),col=\"red\",lwd=3,xlab=\"\",ylab=\"\")\nlines(results[,2,4]~c(25,seq(50,500,50)),col=\"blue\",lty=\"dashed\",lwd=3)\n##2\n#T=50 fixed\nresults2 = array(0,dim=c(11,2,4))\nresults2[1,,] = simulation(1000,25,50,3,3)\npdf(file=\"FMax_T50.pdf\",width=4,height=3)\nplot(results2[,1,1]~c(25,seq(50,500,50)),type=\"l\",ylim=c(0,4.2),col=\"red\",lwd=3,xlab=\"\",ylab=\"\")\nlines(results2[,2,1]~c(25,seq(50,500,50)),col=\"blue\",lty=\"dashed\",lwd=3)\npdf(file=\"FFrob_T50.pdf\",width=4,height=3)\nplot(results2[,1,2]~c(25,seq(50,500,50)),type=\"l\",ylim=c(0,2),col=\"red\",lwd=3,xlab=\"\",ylab=\"\")\nlines(results2[,2,2]~c(25,seq(50,500,50)),col=\"blue\",lty=\"dashed\",lwd=3)\npdf(file=\"GMax_T50.pdf\",width=4,height=3)\nplot(results2[,1,3]~c(25,seq(50,500,50)),type=\"l\",ylim=c(0,3.5),col=\"red\",lwd=3,xlab=\"\",ylab=\"\")\nlines(results2[,2,3]~c(25,seq(50,500,50)),col=\"blue\",lty=\"dashed\",lwd=3)\npdf(file=\"GFrob_T50.pdf\",width=4,height=3)\nplot(results2[,1,4]~c(25,seq(50,500,50)),type=\"l\",ylim=c(0,1.5),col=\"red\",lwd=3,xlab=\"\",ylab=\"\")\nlines(results2[,2,4]~c(25,seq(50,500,50)),col=\"blue\",lty=\"dashed\",lwd=3)\n##3\n#N=200 fixed\nresults3 = array(0,dim=c(10,2,4))\nresults3[1,,] = simulation(1000,200,10,3,3)\npdf(file=\"FMax_N200.pdf\",width=4,height=3)\nplot(results3[,1,1]~seq(10,100,10),type=\"l\",ylim=c(0,5),col=\"red\",lwd=3,xlab=\"\",ylab=\"\")\nlines(results3[,2,1]~seq(10,100,10),col=\"blue\",lty=\"dashed\",lwd=3)\npdf(file=\"FFrob_N200.pdf\",width=4,height=3)\nplot(results3[,1,2]~seq(10,100,10),type=\"l\",ylim=c(0,2),col=\"red\",lwd=3,xlab=\"\",ylab=\"\")\nlines(results3[,2,2]~seq(10,100,10),col=\"blue\",lty=\"dashed\",lwd=3)\npdf(file=\"GMax_N200.pdf\",width=4,height=3)\nplot(results3[,1,3]~seq(10,100,10),type=\"l\",ylim=c(0,3.5),col=\"red\",lwd=3,xlab=\"\",ylab=\"\")\nlines(results3[,2,3]~seq(10,100,10),col=\"blue\",lty=\"dashed\",lwd=3)\npdf(file=\"GFrob_N200.pdf\",width=4,height=3)\nplot(results3[,1,4]~seq(10,100,10),type=\"l\",ylim=c(0,1.5),col=\"red\",lwd=3,xlab=\"\",ylab=\"\")\nlines(results3[,2,4]~seq(10,100,10),col=\"blue\",lty=\"dashed\",lwd=3)\n", "output_sequence": "Simulates data and evaluates the performance of the estimators for the regression parameters and for the interactive fixed effect components."}, {"input_sequence": "Author: Thijs Benschop ; ## Clear history\nrm(list = ls(all = TRUE))\ngraphics.off()\n## Install and load packages\nlibraries = c(\"DEoptim\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# Set working directory\nsetwd(\"\")\n# Function to evaluate the loglikelihood, 2 states, both iid normal\ncondLogLikPar = function(theta, series){\n# calculates the conditional loglikelihood for a Markov Switching model\n# with 2 states\n# Arguments\n# p11 transition probability to stay in state 1\n# mu1, mean of distribution in resp. state 1 and 2\n# returns : value of conditional log-likelihood\np11 = theta[1]\n\nn = length(series) # number of obs.\np = cbind(c(p11, 1-p22), c(1-p11, p22)) # transition matrix\ndzeta = cbind(rep(0, n), rep(0, n))\nf = cbind(rep(0, n))\neta = cbind(rep(0,n), rep(0,n))\ndzetaInit = c((1-p[2,2])/(2-p[1,1]-p[2,2]),\n# startvalue for iterations, assuming the Markov chain is ergodic\n# alternative dzetaInit = c(0.5, 0.5)\nfor (i in 1:n){\n# Evaluate the densities under the two regimes\n############### create if else for different functional forms ##############\neta[i, 1] = dnorm(x=series[i], mean=mu1, sd=sigma1)\n# Evaluate the conditional density of the ith observation\nif (i == 1){\nf[i] = t(p %*% c(eta[i,1], eta[i,2])) %*% dzetaInit\n}\nelse{\nf[i] = t(p %*% c(eta[i,1], eta[i,2])) %*% c(dzeta[i-1, 1], dzeta[i-1, 2])\n# Evaluate the state probabilities\nif(i==1){\ndzeta[i, 1] = dzetaInit[1]\ndzeta[i, 1] = (eta[i,1] * (p[,1] %*% c(dzeta[i-1, 1], dzeta[i-1, 2]))) /f[i]\n}\nlogf = sum(log(f[5:n]))\ncat(logf, \" \")\nflush.console()\noutput = cbind(eta, f, dzeta)\nreturn(list(dzeta))\n}\ncondLogLik = function(theta, series){\np11 = theta[1]\nn = length(series) # number of obs.\np = cbind(c(p11, 1-p22), c(1-p11, p22)) # transition matrix\ndzeta = cbind(rep(0, n), rep(0, n))\nf = cbind(rep(0, n))\neta = cbind(rep(0,n), rep(0,n))\nif(is.nan(logf)==TRUE){\nlogf = 99999999\ncat(\"Error: not a number\", \"/n\")\nflush.console()\n#cat(logf, \" \")\n#flush.console()\nreturn(-logf)\n# Read-in data\ndata = read.table(\"dataCO2.txt\", header = TRUE)\nlogRetFirstPeriod = data[2:725,5]\n## Other optimization alghorithms\n# p11, p22, mu1, sigma1,\nlowParDE = c(0, 0, -0.3, 0, 0 )\ncontrolDE = list(NP = 60, itermax = 2500)\ntestDE = DEoptim(fn = condLogLik,\nlower = lowParDE,\nseries = logRetFirstPeriod,\ncontrol = controlDE)\n# Result\n#Iteration: 2500 bestvalit: -1719.999284 bestmemit: 0.985531 0.976359 0.001405 -0.003738 0.016150 0.033617\n#parDE = c(0.982090, 0.992281, -0.004157, 0.000865, 0.000052)\nparDE = c(0.985531, 0.976359, 0.001405, -0.003738, 0.016150, 0.033617)\n# Plot dzeta, prob in high regime and plot of series\n# in-sample period\ndzeta = condLogLikPar(theta = parDE,\nseries = data[2:725, 5]) #calculate dzeta\ndzeta1 = dzeta[[1]][,1]\npar(mfrow = c(2,1))\njan08 = as.Date(\"01/01/08\", \"%d/%m/%y\")\nDatePlot = as.Date(data$Date[6:725], ,\"%d/%m/%y\")\nplot(dzeta1[5:724]~DatePlot,\ntype = \"l\",\nylim = c(0.0,1.0),\nlwd = 2,\nxlab = \"Date\",\nylab = \"Regime probabilities\",\nxaxs = \"i\",\nxlim = c(jan08, dec10))\nplot(data[6:725,5]~DatePlot,\nlwd = 1.2,\nxlim = c(jan08, dec10),\nylab = \"Log returns\",\nylim = c(-0.11, 0.11))\nllh = condLogLik(parDE, logRetFirstPeriod)\ntestnlm = nlm(f = condLogLik,\np = c(0.7, 0.8, mean(logRetFirstPeriod),\nseries = logRetFirstPeriod,\niterlim = 100)\ntestOptim = optim(f = condLogLik,\np = c(0.7, 0.8, mean(logRetFirstPeriod),\nseries = logRetFirstPeriod)\nparValues = testnlm$estimate\n# loop testing different starting values for p11 and p22\nevalMat1 = matrix(rep(0, 121), 11,\n# loop testing different starting values for p11 and p22\nfor (i in 0:10){\nreestimate =nlm(f = condLogLik,\np = c(0.7, 0.8, mean(logRetFirstPeriod),\nseries = logRetFirstPeriod,\niterlim = 100)\nevalMat1[i+1, j+1] = reestimate$estimate[1]\nAIC = 2*(test2$minimum) + 2*6\n#########################################################\n#2# reestimation, recursive\n#matrix with estimate, error, estimated variance\nrecursivePar = parValues\n# reestimate the parameters for the longer time series\nfor (i in 726:1183){\nreestMSNormal = nlm(f = condLogLik,\np = prevPar,\nseries = data[(2:i),5],\niterlim = 100)\nrecursivePar = as.data.frame(cbind(recursivePar, reestMSNormal$estimate))\nprevPar = reestMSNormal$estimate\ncat(\"iteration: \", i, \"\\n\")\n# load recursivePar\nrecursivePar = read.table(\"reestRecurMSNormal.txt\", header=TRUE)\nllh = condLogLik(theta = c(recursivePar[,1]),\nseries = data[2:725, 5])\nllhcor = llh/724*720\nAIC = 2*llhcor + 2*6\n# calculate point and density estimates, confidence intervals\npointForecastRec = rep(0, 458)\nfor (i in 726:1183){\n# Forecasting values for i\n# calculate dzeta[i], sigma[i]\ninfo = condLogLikPar(theta = recursivePar[,(i-725)],\nseries = data[2:(i-1), 5]) #calculate dzeta and sigma\ncurPar = recursivePar[,(i-725)] # estimated parameters\n# [1] p11,#[2] mu2 ,#[5] sigma1,#[6] sigma2\ndzeta1Cur = info[[1]][(i-2),1]\np1 = dzeta1Cur * curPar[1] + dzeta2Cur * (1-curPar[2]) # prob of being in state 1 in forecast\n#point estimate\nmean1 = curPar[3]\npointForecastRec[i-725] = p1 * mean1 + p2 * mean2\np1Forecast[i-725] = p1\nsigma2Forecast[i-725] = p1 * curPar[5]^2 + p2 * curPar[6]^2\n##\nplot(c2Forecast,\ntype = \"l\",\nylim = c(-0.1, 0.2))\npar(new=TRUE)\nplot(c1Forecast,\nylim = c(-0.1, 0.2),\nxaxt = \"n\",\nann = FALSE)\nplot(data$logRet[726:1183],\ntype = \"l\",\nylim = c(-0.1, 0.2,),\nxaxt = \"n\",\nann = FALSE)\nplot(pointForecastRec,\nylim = c(-0.1, 0.2),\ncol = \"red\",\nabline(h=mean(data$logRet[726:1183]))\nestErrorRecur = pointForecastRec[1:458] - data$logRet[726:1183]\nrecursiveMSE = (1/length(estErrorRecur)) * sum(estErrorRecur^2)\nrecursiveMSE\nresPlot = cbind(estErrorRecur, sigma2Forecast)\n# logreturns and predicted 95 percent confidence intervals\n# calculate 2,5 and 97,5 quantiles\nconfInterval = cbind(rep(0, 458), rep(0, 458))\nfor (i in 1:458){\nconfInterval[i,1] = qnorm(0.025, mean=pointForecastRec[i], sd=sqrt(sigma2Forecast[i]))\nconf = cbind(confInterval, pointForecastRec)\n# Plot series within estimated confidence intervals\nplot(confInterval[,1],\nylim = c(-0.2, 0.2))\npar(new=TRUE)\nplot(confInterval[,2],\npar(mfrow=c(2,1))\nplot(p1Forecast,\nylim = c(0,1))\ntype = \"l\")\nylim = c(-0.3, 0.3),\ncol = \"red\")\nylim = c(-0.3, 0.3))\ntype = \"l\",\nylim = c(-0.3, 0.3),\ncol = \"red\")\nplot(estErrorRecur,\n# Density transformation\nu = c(rep(0, 458))\nfor (i in 1:length(u)){\nu[i] = pnorm(data$logRet[i+725],\nmean = pointForecastRec[i],\nsd = sqrt(sigma2Forecast[i]))\nhist(u, breaks=20)\n# test on uniformity\n#Kolmogorov Smirnov\n", "output_sequence": "Estimates a Markov switching autoregressive model and plots the estimated states for CO2 spot price data from 2008 to 2011"}, {"input_sequence": "Author: Thijs Benschop ; ## Clear history\nrm(list = ls(all = TRUE))\ngraphics.off()\n## Install and load packages\nlibraries = c(\"DEoptim\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# Set working directory\nsetwd(\"\")\n# Function to evaluate the loglikelihood, 2 states\ncondLogLikPar = function(theta, series){\n# calculates the conditional loglikelihood for a Markov Switching model\n# with 2 states\n# Arguments\n# p11 transition probability to stay in state 1\n# mu1, mean of distribution in resp. state 1 and 2\n# returns : value of conditional log-likelihood\np11 = theta[1]\nalpha01 = theta[5]\nbeta12 = theta[10]\n\nif ((alpha11 + beta11) >= 1 || (alpha12 + beta12) >= 1){logf = -9999999}\n# add restrictions on alpha 1 and beta 1\nelse{\nn = length(series) # number of obs.\np = cbind(c(p11, 1-p22), c(1-p11, p22)) # transition matrix\ndzeta = cbind(rep(0, n), rep(0, n))\nf = cbind(rep(0, n))\neta = cbind(rep(0,n), rep(0,n))\ndzetaInit = c((1-p[2,2])/(2-p[1,1]-p[2,2]),\n# startvalue for iterations, assuming the Markov chain is ergodic\n# alternative dzetaInit = c(0.5, 0.5)\n# creating sigma and epsilon vectors\nsigma1 = cbind(rep(0,n))\nsigma1[1] = sd(series)\nepsilon21 = (series-c1)^2\n#for (i in 2:n){\n#sigma1[i] = sqrt(alpha01 + (alpha11 * epsilon21[i-1]) + (beta11 * (sigma1[i-1])^2))\n#}\n#sigma1 = sqrt(sigma21)\nfor (i in 1:n){\n\n# Evaluate the densities under the two regimes\n############### create if else for different functional forms ##############\neta[i, 1] = dnorm(x=series[i], mean=c1, sd=sigma1[i])\n# Evaluate the conditional density of the ith observation\nif (i == 1){\nf[i] = t(p %*% c(eta[i,1], eta[i,2])) %*% dzetaInit\n}\nelse{\nf[i] = t(p %*% c(eta[i,1], eta[i,2])) %*% c(dzeta[i-1, 1], dzeta[i-1, 2])\n# Evaluate the state probabilities\nif(i==1){\ndzeta[i, 1] = dzetaInit[1]\ndzeta[i, 1] = (eta[i,1] * (p[,1] %*% c(dzeta[i-1, 1], dzeta[i-1, 2]))) /f[i]\n# Calculating sigma2\nif(i == 1){\nsigma1[i+1] = sqrt(alpha01 + (alpha11 * epsilon21[i]) + (beta11 * (dzetaInit[1]*(sigma1[i])^2 + dzetaInit[2]*(sigma2[i])^2)))\n}\nsigma1[i+1] = sqrt(alpha01 + (alpha11 * epsilon21[i]) + (beta11 * (dzeta[i,1]*(sigma1[i])^2 + dzeta[i,2]*(sigma2[i])^2)))\n}\nlogf = sum(log(f) * rep(1,n))\nif(is.nan(logf)){\nlogf = -9999999\ncat(\"Error not a number!!!\", \"\\n\")\n#output = cbind(eta, f, dzeta)\ncat(logf, \"\\n\")\n#cat(p11, \" \", p22, \" \", c1, \" \", c2, \"\\n \", alpha01, \" \", alpha02, \" \", alpha11, \" \", alpha12, \"\\n \", beta11, \" \", beta12, \"\\n \", logf, \"\\n\")\nflush.console()\nreturn(list(dzeta, sigma1, epsilon21,\n}\n# Function to evaluate the loglikelihood, 2 states, both GARCH(1,1)\ncondLogLik = function(theta, series){\n#cat(logf, \"\\n\")\n#flush.console()\nreturn(-logf)\n# Read-in data\ndata = read.table(\"dataCO2.txt\", header=TRUE)\n#Coefficient(s) of GARCH(1,1) without regime switching:\n# constant, alpha0, beta1\n# mu omega alpha1 beta1\n#-2.6460e-04 4.6171e-06 7.2568e-02 9.1988e-01\n# Test function with values of GARCH(1,1)\ntest = condLogLik(theta = c(0.5, -0.00026460, 0.0000046171, 0.072568, 0.91988, series = data[2:725, 5])\ntest\n#testa = condLogLik(theta = parDE, series = logRetFirstPeriod)\n#Plot state probs\n#dzetaTest = condLogLikDzeta(theta = parValues, series = data[2:725, 5])\n#plot(dzetaTest[,1], type=\"l\")\n## Defining constaints\n# Matrix with lienar combinations of parameters (p11. p22, c1, alpha01, beta11,\n# (k x p) matrix, k constraints, p parameters\n#\nconstraintMat = matrix(c(0, 0,\n0,\n), 8, 10)\nconstraintMat\nconstraintVec =c(0, 0, -1,\nlogRetFirstPeriod = data$logRet[2:725]\n# p11, p22, c1, alpha01, beta11,\n## Other optimization alghorithms\n# deoptim\nlibrary(DEoptim)\n# p11, p22, c1, alf01, beta12,\nlowParDE = c(0, 0, -0.3, 0, 0)\ncontrolDE = list(NP = 120, itermax = 500)\ntestDE = DEoptim(fn=condLogLik, lower=lowParDE, upper=upParDE, series=logRetFirstPeriod, control=controlDE)\n# Result\n#Iteration: 414 bestvalit: -1749.445659 bestmemit: 0.982090 0.992281 -0.004157 0.000865 0.103816 0.001251 0.723247 0.716569\n#Iteration: 699bestvalit -1749,399347\n#parDE = c(0.979766, 0.987543, -0.004397, 0.000905, 0.091176, 0.000013, 0.765748, 0.611604)\nllh = condLogLik(parDE, logRetFirstPeriod)\nllhcor = llh/724*720\nAIC = 2*llhcor + 2*10\n# unconditonal standard deviation\n# state 1\nsqrt(parDE[5]/(1-parDE[7]-parDE[9])) #falsh herum?\n# state 2\nsqrt(parDE[6]/(1-parDE[8]-parDE[10]))\n# unconditional probability\n(1-parDE[1])/(2-parDE[1]-parDE[2])\n# nlminb\nstartPar = c(0.8, 0.2, -0.00026460, 0.0000046171, 0.072568, 0.91988,\nlowPar = c(0,0, -1, 0, 0)\ntestNlminb = nlminb(start = startPar,\nobjective = condLogLik,\nseries = logRetFirstPeriod,\nupper = upPar)\ntestNlminb$par\n# neldermead\nlibrary(neldermead)\nfminsearch(fun=condLogLik, x0=startPar)\n#?nlminb\n#dfoptim\n#Unconstrained optimization\ntest6 = optim(c(0.8, 0.2, -0.00026460, 0.0000046171, 0.072568, 0.91988, condLogLik, series = logRetFirstPeriod)\ntest6b = optim(test6$par, condLogLik, series = logRetFirstPeriod)\ninitPar = c(0.8, 0.2, -0.00026460, 0.0000046171, 0.072568, 0.91988,\n# Constrained optimization\ntest7 = constrOptim(initPar, ui=constraintMat, condLogLik, series = logRetFirstPeriod, grad=NULL)\ncbind(test7$par, test8$par,\n####\n#results of test 18 [1] -1749.773\nparValues = test18$par\nparValues = c(9.561845e-01,6.832274e-01,9.352358e-04,-1.301391e-02,5.633237e-07,3.251843e-05,7.237832e-02,7.220622e-02,9.201030e-01, 9.202217e-01)\nfinal = condLogLik(theta = parValues,\nseries = data[2:725, 5])\ndzetaFinal = condLogLikDzeta(theta = parValues,\nseries = data[2:725, 5])\nplot(dzetaFinal[,1], type=\"l\")\n#different starting values\n#test9 = constrOptim(c(0.5, 0.5, -0.00026460, 0.0000046171, 0.01, 0.98, ui=constraintMat, condLogLik, series = logRetFirstPeriod, grad=NULL)\n#########################################################\n### point forecast for period 2011-2012 (726-1183)\n#2# reestimation, recursive\n#matrix with estimate, error, estimated variance\nrecursivePar = parDE\n# reestimate the parameters for the longer time series\nfor (i in 726:1183){\nreestMSGARCH11 = constrOptim(prevPar,\nui = constraintMat,\nrecursivePar = cbind(recursivePar, reestMSGARCH11$par)\nprevPar = reestMSGARCH11$par\ncat(\"iteration: \", i, \"/n\")\n# load recursivePar\nrecursivePar = read.table(\"reestRecurMSGARCH11.txt\", header=TRUE)\n# calculate point and density estimates, confidence intervals\npointForecastRec = rep(0, 458)\ndzeta1 = rep(0, 458)\n# Forecasting values for i\n# calculate dzeta[i], sigma[i]\ninfo = condLogLikPar(theta = recursivePar[,(i-725)], series = data[2:(i-1), 5]) #calculate dzeta and sigma\ncurPar = recursivePar[,(i-725)] # estimated parameters\ndzeta1Cur = info[[1]][(i-2),1]\ndzeta1[i-725] = dzeta1Cur\np1 = dzeta1Cur * curPar[1] + dzeta2Cur * (1-curPar[2]) # prob of being in state 1 in forecast\n#point estimate\npointForecastRec[i-725] = p1 * curPar[3] + p2 * curPar[4]\np1Forecast[i-725] = p1\nc1Forecast[i-725] = curPar[3]\nsigma2Cur = dzeta1Cur * (info[[2]][(i-2)])^2 + dzeta2Cur * (info[[3]][(i-2)])^2\nsigma2Forecast[i-725] = p1 * (curPar[5] + (curPar[7] * info[[4]][(i-2)]) + curPar[9]*sigma2Cur) + p2 * (curPar[6] + (curPar[8] * info[[5]][(i-2)]) + curPar[10]*sigma2Cur)\ncat(\"iteration: \", i, \"\\n\")\nplot(c2Forecast,\ntype = \"l\",\nylim = c(-0.15, 0.2))\npar(new=TRUE)\nplot(c1Forecast,\ntype = \"l\" ,\n#mean(data$logRet[726:1183])\nestErrorRecur = pointForecastRec - data$logRet[726:1183]\nresPlot = cbind(estErrorRecur, sigma2Forecast)\n#write.table(resPlot, \"resMSGARCH11.txt\")\nrecursiveMSE = (1/length(estErrorRecur)) * sum((estErrorRecur^2))\nrecursiveMSE\n# logreturns and predicted 95 percent confidence intervals\n# calculate 2,5 and 97,5 quantiles\nconfInterval = cbind(rep(0, 458), rep(0, 458))\nfor (i in 1:458){\nconfInterval[i,1] = qnorm(0.025, mean=pointForecastRec[i], sd=sqrt(sigma2Forecast[i]))\nconf = cbind(confInterval, pointForecastRec)\n#write.table(conf, \"confMSGARCH11.txt\")\nplot(pointForecastRec,\nylim = c(-0.005, 0.005))\n# Plot dzeta, prob in high regime and plot of series\n# in-sample period\ndzeta = condLogLikPar(theta = parDE, series = data[2:725, 5]) #calculate dzeta\ndzeta1 = dzeta[[1]][,1]\npar(mfrow=c(2,1))\njan08 = as.Date(\"01/01/08\", \"%d/%m/%y\")\nDatePlot = as.Date(data$Date[2:725], , \"%d/%m/%y\")\nplot(dzeta1~DatePlot,\nylim = c(0.0,1.0),\nlwd = 1.2,\nxlab = \"Date\",\nylab = \"Regime probabilities\",\nxaxs = \"i\",\nxlim = c(jan08, dec10))\nplot(data[2:725,5]~DatePlot,\nxlim = c(jan08, dec10),\nylab = \"Log returns\",\nylim = c(-0.11, 0.11))\nabline(h = parDE[3], col= \"red\", lwd = 1.2)\nprob = cbind(dzeta1, dzeta2)\n#write.table(prob, \"probMSGARCH11.txt\")\n# calculate the conditional estimated mean\nmeanCond = dzeta1 * parDE[3] + dzeta2 * parDE[4]\nplot(meanCond,\ncol = \"red\")\nplot(data[2:725,5],\ntype = \"l\")\nplot(data[726:1183,5],\nylim = c(-0.2, 0.2))\nplot(confInterval[2:458,1],\nylim = c(-0.2, 0.2),\nplot(estErrorRecur,\n# Density transformation\nu = c(rep(0, 458))\nfor (i in 1:length(u)){\nu[i] = pnorm(data$logRet[i+725],\nmean = pointForecastRec[i],\nsd = sqrt(sigma2Forecast[i]))\nhist(u, breaks=20)\n# test on uniformity\n#Kolmogorov Smirnov\nks.test(u, \"punif\")\n", "output_sequence": "Estimates a Markov switching autoregressive model and plots the estimated states for CO2 futures price data from 2008 to 2011"}, {"input_sequence": "Author: Awdesch Melzer ; rm(list = ls(all = TRUE))\ngraphics.off()\n############################ SUBROUTINES ################################ Complex array generation\ncompl = function(re, im) {\nif (missing(re)) {\nstop(\"compl: no composed object for real part\")\n}\nif (missing(im)) {\nim = 0 * (re <= Inf)\nif (nrow(matrix(re)) != nrow(matrix(im))) {\nstop(\"compl: dim(re)<>dim(im)\")\nz = list()\nz$re = re\nreturn(z)\n}\nVaRcgfDG = function(t, par) {\n# cumulant generating function (cgf) for the class of quadratic forms of Gaussian vectors. Complex array generation\ncompl = function(re, im) {\nif (missing(re)) {\nstop(\"compl: no composed object for real part\")\n}\nif (missing(im)) {\nim = 0 * (re <= Inf)\nif (nrow(matrix(re)) != nrow(matrix(im))) {\nstop(\"compl: dim(re)<>dim(im)\")\nz = list()\nz$re = re\nreturn(z)\ns = compl(par$theta * t$re, par$theta * t$im)\ncmul = function(x, y) {\n# Complex multiplication\nre = x$re * y$re - x$im * y$im\ncdiv = function(x, y) {\n# Complex division\nw = y$re^2 + y$im^2\nre = (x$re * y$re + x$im * y$im)/w\ncln = function(x) {\n# Complex natural logarithm\nre = log(x$re^2 + x$im^2)/2\nim = atan2(x$im, x$re)\ncsub = function(x, y) {\n# Complex subtraction two arrays of complex numbers\nre = x$re - y$re\ni = 1\nm = length(par$lambda)\nwhile (i <= m) {\n# 1-lambda*t:\nomlt = compl(1 - par$lambda[i] * t$re, -par$lambda[i] * t$im)\ntmp = cmul(t, t)\ntmp = compl(par$delta[i]^2 * tmp$re, par$delta[i]^2 * tmp$im)\ntmp = csub(tmp, cln(omlt))\ns = compl(s$re + 0.5 * tmp$re, s$im + 0.5 * tmp$im)\ni = i + 1\nreturn(s)\ncexp = function(x) {\n# Complex exponential\nre = exp(x$re) * cos(x$im)\nVaRcharfDG = function(t, par) {\n# computes the characteristic function for the class of quadratic forms of Gaussian vectors.\nt = compl(-t$im, t$re) # 1i*t\nr = cexp(VaRcgfDG(t, par))\n############################ Main Program ############################\nXFGVaRcharfDGtest = function(par, n, xlim) {\n# Complex array generation\ndt = (xlim[2] - xlim[1])/(n - 1)\nt = xlim[1] + (0:(n - 1)) * dt\nr = VaRcharfDG(compl(t, t * 0), par)\nz1 = cbind(t, r$re)\nplot(z1, type = \"l\", col = \"blue3\", lwd = 2, ylab = \"Y\", xlab = \"X\", ylim = c(min(r$re, r$im), max(r$re, r$im)))\nlines(z2, col = \"red3\", lwd = 2)\ntitle(\"Characteristic function\")\ntheta = 0\ndelta = c(0)\nlambda = c(1.4142)\npar = list()\npar$theta = theta\npar$lambda = lambda\nXFGVaRcharfDGtest(par, 300, c(-40, 40))\n", "output_sequence": "Plots the real (blue line) and the imaginary part (red line) of the characteristic function for a distribution, which is close to a chi^2 distribution with one degree of freedom."}, {"input_sequence": "Author: Awdesch Melzer ; # nadaraya watson kernel smoothing\nsker = function(x, y, h, N, f, v) {\n# kernel regression smoothing for different kernel functions\n\n# kernel functions\nkern = function(x, f) {\nx = as.matrix(x)\nif (f == \"gau\") {\n# gaussian kernel\ny = dnorm(x)\n} else if (f == \"qua\") {\n# quadric / biweight kernel\ny = 0.9375 * (abs(x) < 1) * (1 - x^2)^2\n} else if (f == \"cosi\") {\n# cosine kernel\ny = (pi/4) * cos((pi/2) * x) * (abs(x) < 1)\n} else if (f == \"tri\") {\n# triweight kernel\ny = 35/32 * (abs(x) < 1) * (1 - x^2)^3\n} else if (f == \"tria\") {\n# triangular kernel\ny = (1 - abs(x)) * (abs(x) < 1)\n} else if (f == \"uni\") {\n# uniform kernel\ny = 0.5 * (abs(x) < 1)\n} else if (f == \"spline\") {\ny = 0.5 * (exp(-abs(x)/sqrt(2))) * (sin(abs(x)/sqrt(2) + pi/4))\n}\nreturn(y)\n}\n# Default parameters\nif (missing(N)) {\nN = 100\nif (missing(f)) {\nf = \"qua\"\nif (missing(v) == F) {\nv = sort(v)\nr.n = length(x)\nif (missing(h)) {\nstop(\"There is no enough variation in the data. Regression is meaningless.\")\nr.h = h\nr.x = seq(min(x), max(x), length = N)\nr.f = matrix(0, N)\nfor (k in 1:N) {\nz = kern((v[k] - x)/h, f)\nr.f[k] = sum(z * y)/sum(z)\nreturn(list(mx = v, mh = r.f))\n}\n# Implied volatility surface estimation (IVS, gamma2, nu2)\nIVSurface = function(VarEst, huse, dataday) {\n# fix bandwidth\nhuse = huse\nif (missing(\"VarEst\")) {\n# just estimate the function xmin = c(0.85, 0.047222) # February xmax = c(1.15, 0.047222)\nxmin = c(0.9, 0.047222)\n\nN = c(50, 1)\nxstep = (xmax - xmin)/N\nIVS = matrix(1, prod(N), 3)\nfor (i in 1:2) {\nIVS[, i] = seq(xmin[i], xstep[i], length = N[1])\n} else {\n# estimate the variance\ndata = dataday\n# to estimate variance it is good to reduce the grid data = data[which(data[,7]>0.89),] # February data =\n# data[which(data[,7]<1.12),]\ndata = data[which(data[, 7] > 0.91), ] # January\ndim(data)\ndata = data[, 1:9]\ndata[, 9] = 0 * data[, 9]\nIVS = cbind(data[, 7], data[, 4], matrix(1, nrow(data), 1))\ni = 1\nI = nrow(IVS)\nI\nwhile (i <= I) {\nx = t(IVS[i, 1:2])\nh <<- h # set global parameter\nx <<- x\nsol = nmgolden(LSKernIVSEst, 0.01, 0.2, 0.85, 0.01, data, h, x)\nIVS[i, 3] = sol$xmin\nif (missing(\"VarEst\") == 0) {\n# if a variance is to be estimated\ndata[i, 8] = sol$xmin\ni = i + 1\ngamma2 = 0\nif (missing(\"VarEst\") == 0) {\n# if a variance is to be estimated\ndataZZ = data\ntmp = VarEstLSKS(data)\ngamma2 = tmp$gamma2\nreturn(list(IVS = IVS, gamma2 = gamma2, nu2 = nu2))\n# Golden section search\nnmgolden = function(func, a, b, c, xtol, data, h, x) {\ndeftol = 1e-08\nfa = 0\nfa = func(a, data, h, x)\nK = (3 - sqrt(5))/2 # the golden ratios\nR = 1 - K\nx0 = a\nif (abs(c - b) > abs(b - a)) {\nx1 = b\nx2 = b + K * (c - b)\nx2 = b\nx1 = b - K * (b - a)\nf1 = 0\nf1 = func(x1, data, h, x)\nwhile (abs(x3 - x0) > xtol * (abs(x1) + abs(x2))) {\nif (f2 < f1) {\nx0 = x1\nx2 = R * x1 + K * x3\nf1 = f2\nf2 = func(x2, data, h, x)\n} else {\nx3 = x2\nx1 = R * x2 + K * x0\nf2 = f1\nf1 = func(x1, data, h, x)\nif (f1 < f2) {\nxmin = x1\nreturn(list(xmin = xmin, fmin = fmin))\n# general Black and Scholes fomula\nBlackScholes = function(S, K, r, sigma, tau, task, d) {\nif (missing(d)) {\nd = matrix(0, length(S), 1)\nb = r - d\ny = (log(S/K) + (b - (sigma^2)/2) * tau)/(sigma * sqrt(tau))\n# first part computes Call option, second the put:\np = (exp((b - r) * tau) * S * pnorm(y + sigma * sqrt(tau)) - K * exp(-r * tau) * pnorm(y)) * (task == 1) + (K * exp(-r *\ntau) * pnorm(-y) - exp((b - r) * tau) * S * pnorm(-y - sigma * sqrt(tau))) * (task == 0)\nif (any(p < 0)) {\np[(p < 0)] = 0\nreturn(p)\n# Variance estimation with Least Squares Kernel Smoothing\nVarEstLSKS = function(data) {\nS = data[, 1]\nsigmahat = data[, 8]\n# compute A, B, D we do not need to adjust for moneyness as the spot cancels out:\nA = oprice - BlackScholes(S, K, r, sigmahat, tau, task)\nB = tmp$vega\ngamma2 = (mean(-B^2 + A * D))^2\nnu2 = mean((A^2) * (B^2)) * 25/49\nreturn(list(gamma2 = gamma2, nu2 = nu2))\n# intint K(u)^2 dudv = 25/49 for quartic Kernel\n# Black and Scholes Vega and Volga functions proc(vega, volga)\nBSVegaVolga = function(S, K, r, sigma, tau, task, d) {\nd1 = (log(S/K) + (b + (sigma^2)/2) * tau)/(sigma * sqrt(tau))\nvega = S * sqrt(tau) * dnorm(d1) * exp(-d * tau)\nvolga1 = S * exp(-d * tau) * tau * dnorm(d1) * ((d1^2)/(sigma * sqrt(tau)) - d1)\nreturn(list(vega = vega, volga = volga))\n# Least Squares Kernel Estimation of Implied Volatility proc(Q) =\nLSKernIVSEst = function(sigma, data, h, x) {\nsigmaB = sigma\ndata = data # (S, K, r, sigma, tau, task)\n# blow sigma up to fit with other data\nsigma = matrix(sigma, nrow(data), 1)\n# umoney and utau\nu = ((kappa - x[1])/h[1])\nKern = 0.9375 * (abs(u) < 1) * (1 - u^2)^2 # quartic kernel\nresid = oprice/S - BlackScholes(S, K, r, sigma, tau, task)/S\nQ = sum((resid^2) * Kern)\nreturn(Q)\n############################# MAIN COMPUTATION ############################\n# to use please choose data source and bandwidth dataday = read.table('FebruaryData.dat')\ndataday = read.table(\"JanuaryData.dat\")\n# choose bandwidth\nhfeb = 0.017\n# huse is the bandwidth finally used\nhuse = hjan\n# huse = hfeb\ntemp = IVSurface(\"VarEst\", huse, dataday) # {Smile, gamma2, nu2}\nSmile = temp$IVS\nSmile = cbind(Smile[, 1], Smile[, 3])\ndataUnCorr = dataday\nxUnCorr = cbind(dataUnCorr[, 7], dataUnCorr[, 8])\nopPrices = cbind(dataUnCorr[, 7], (dataUnCorr[, 1]))\n# these numbers are precomputed as their computation is time consuming\ngamma2 = 29176326722.4258\nnu2 = 1482489.0485\n# estimate densities on the grid of the Smile\nSmile = Smile[order(Smile[, 1]), ]\nhdens = 0.0365\nfh = sker(x = seq(min(dataday[, 7]), max(dataday[, 7]), length = length(dataday[, 7])), y = dataday[, 7], h = hdens, N = length(Smile[,\n1]), f = \"qua\", v = Smile[, 1])\nsigma2band = nu2/(gamma2 * fh$mh)\nn = nrow(dataday)\nh = 0.025\n# confidence bands\nalpha = 0.05\nsh = sker(x = seq(min(dataday[, 7]), max(dataday[, 7]), length = length(dataday[, 7])), y = dataday[, 7]^2, h = hdens, N = length(Smile[,\ncalpha = qnorm(1 - alpha/2)\nmrg = calpha * sqrt(hdens * sh$mh/((n * h) * density(Smile[, 1], bw = h, cut = 1, kernel = \"biweight\", n = length(Smile[, 1]))$y))\nclo = cbind(fh$mx, (Smile[, 2] - mrg))\nplot(xUnCorr, col = \"blue3\", ylim = c(min(clo[, 2]), max(cup[, 2])), xlab = \"Moneyness\", ylab = \"\")\ntitle(\"Smile\")\npoints(Smile, pch = 16, cex = 0.3, col = \"red3\")\npoints(clo, pch = \".\")\ndev.new()\nplot(opPrices, xlab = \"Moneyness\", ylab = \"\")\ntitle(\"Put and call prices per moneyness\")\n\n", "output_sequence": "Performes a Least Squares Kernel Smoothing technique for implied volatility smile estimation. To achieve good forecast of asset price volatility the Gaussian shaped vega function is used as weight. A quartic kernel is employed. Given global convexity, the Golden section search is implemented with estimation tolerance 10e-5. The data consists of January and February 2001 tick statistics of the DAX futures contracts and DAX index options. Since the data are transaction data containing potential misprints, a filter in deleting all observations whose implied volatility is bigger than 0.7 and less than 0.1 is applied. Plots of observed option price data are presented. From the lower left to upper right put prices are displayed, from upper left to lower right (normalized) call prices. Moreover, graphs of least squares kernel smoothed implied volatility smile for 17 days to expiry (January 02, 2001) with bandwidth = 0.025 and 95 percent confidence bands and 14 days to expiry (February 02, 2001) with bandwidth = 0.015 and 95 percent confidence bands are shown. Regression smoothing is made employing a Nadaraya Watson kernel regression estimate for quartic kernel"}, {"input_sequence": "Author: Germar Kn\u00f6chlein, Awdesch Melzer ; close all\nclc\n% load data\nPL = load('XFGPL.dat');\n% main computation\n[m n] = size(PL); % m = number of rows\nt = 1;\nExc = 0;\nwhile (t<=m)\nq = quantile(PL(t,:)',0.99); % 99% quantile of PL\nif (q<MMPL(t,1)) % counts exceeding values of q in MMPL\nExc = Exc+1;\nend\nt = t+1;\nend\nExc = Exc/m\n", "output_sequence": "Counts 99% Value-at-Risk exceedences over a profit and loss function for basic simulation data."}, {"input_sequence": "Author: Germar Kn\u00f6chlein, Awdesch Melzer ; rm(list = ls(all = TRUE))\ngraphics.off()\n# load data\nPL = read.table(\"XFGPL.dat\")\n# main computation\nm = nrow(PL)\nt = 1\nExc = 0\nwhile (t <= m) {\nq = quantile(t(PL[t, ]), p = 0.99, type = 1) # 99% quantile\nif (q < MMPL[t, 1]) {\n# counts exceeding values of q in MMPL\nExc = Exc + 1\n}\nt = t + 1\n}\nExc = Exc/m\nExc\n", "output_sequence": "Counts 99% Value-at-Risk exceedences over a profit and loss function for basic simulation data."}, {"input_sequence": "Author: Georg Keilbar, Yanfen Zhang ; rm(list = ls())\nlibrary(xtable)\n#################\n# Set directory #\nsetwd(\"~/Dropbox/Cointegration and CC Dynamics/New Code\")\n##############\n# Simulation #\nsimulation = function(T,M,S,p=2,k=1,r=1){\nalpha0 = c(-0.2,0.2)\nGamma = matrix(0.05,p,p)\nbeta = c(1,-1)\ngamma0 = 0.1\n\ngammahat=rep(0,S)\nalphahat=matrix(0,p,S)\nresult = matrix(0,p+1,2)\nfor (s in 1:S){\n\nset.seed(s)\nx0 = rnorm(p)\ndx = x = matrix(0,T,p)\ndx[1,] = alpha0 %*% t(beta) %*% x0 + rnorm(p,sd=0.05)\nx[1,] = x0 + dx[1,]\nfor (t in 2:T){\nset.seed(t*s)\ndx[t,] = alpha0 %*% t(beta) %*% x[t-1,] * as.numeric((1+tanh(gamma0*t(beta)%*%x[t-1,]))) + t(dx[t-1,] %*% Gamma) + rnorm(p,sd=0.05)\nx[t,] = x[t-1,] + dx[t,]\n}\ndx_1 = rbind(rep(0,p),dx[-T,])\nz = beta%*% t(rbind(x0,x[-T,]))\ngamma = matrix(seq(-1,1,length.out=M),r,M)\nMSE = rep(0, M)\nfor (m in 1:M){\nG = 1 + tanh(gamma[,m] %*% z)\nzG = as.numeric(z*G)\nfit = lm(dx[,j]~cbind(zG,dx_1))\nMSE[m] = mean(lm(dx[,j]~cbind(zG,dx_1))$residuals^2)/p + MSE[m]\nalphahat[j,s] = lm(dx[,j]~cbind(zG,dx_1))$coeff[2]\n}\ngammahat[s] = gamma[,which(MSE==min(MSE))]\n}\nresult[1,1] = mean(gammahat-gamma0)\nresult[2,1] = mean(alphahat[1,]-alpha0[1])\nreturn(result)\n}\nRMSE250 = simulation(250,41,1000)\nRMSE = cbind(RMSE250[,2],RMSE500[,2],RMSE1000[,2],RMSE2000[,2])\nxtable(RMSE,digits=4)\n", "output_sequence": "Simulation and Evaluation of the Co-Intensity VECM."}, {"input_sequence": "Author: Georg Keilbar, Yanfen Zhang ; rm(list = ls())\nlibrary(readxl)\nsetwd(\"~/Dropbox/Cointegration and CC Dynamics/New Code\")\n#List of all cryptocurrencies from coinmarketcap\nlist = crypto_list(coin = NULL, start_date = '20190131', end_date = '20190131',\ncoin_list = NULL)\n#Scrape the largest N currencies\nN = 17\nstart = '20170101'\nend = FALSE\ndata = crypto_history(limit = N, start_date = start, end_date = end, coin_list = NULL, sleep = NULL)\nwrite.csv(data,file=\"cryptodata.csv\")\n#Merge the largest 10 cryptocurrency with at lease 2 1/2 year of history which are not tied to USD (manual selection)\nprice = na.omit(reshape(data[,c(\"date\",\"symbol\",\"close\")],idvar=\"date\",timevar=\"symbol\",direction=\"wide\")[,c(1,2,3,4,5,8,9,10,13,14,17)])\n#Take logs\ndate = price[,1]\nlogprice = log(as.matrix(price[,-1]))\n#First differences\nlogreturn = diff(logprice)\n#Export to csv\nwrite.csv(file=\"logprice.csv\",cbind(date,as.data.frame(logprice)))\n", "output_sequence": "Scrape and merge the prices of the N largest cryptocurrencies over a specified time period"}, {"input_sequence": "Author: Rui Ren ; import numpy as np\nimport matplotlib as mpl\nimport pandas as pd\npd.set_option('display.width', 500)\npd.set_option('display.notebook_repr_html', True)\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nsns.set_context(\"poster\")\n#import pymc3 as pm\nimport math\nfrom scipy.stats import multivariate_normal as mv\n# hyperparameters\nprior_std = 8\n# True parameter\nK = 3\nmu = []\nfor i in range(K):\nmu.append(np.random.normal(0, prior_std))\nprint(mu)\nvar = 1\nvar_arr = [1, 1]\n# Plot variables\ncolor = ['blue', 'green', 'orange', 'yellow', 'red']\nn = 1000\n# Simulated data\nmixture_selection_matrix = np.identity(K)\nN = np.random.choice(K, n)\nfrom collections import defaultdict\ndata = []\ndata_resp=defaultdict(list)\nplt.figure(1)\nfor i in range(n):\n# pick category\nmean = np.dot(mixture_selection_matrix[N[i]], mu)\n# get point from distribution\nx = np.random.normal(mean, cov)\ndata.append(x)\ndata_resp[N[i]].append(x)\n#figure\nfor k in range(K):\nplt.hist(data_resp[k], bins=30, color=color[k], histtype='step', lw=2)\nplt.hist(data, color='k', bins=90, alpha=0.2) #, alpha=0.2 colors transparency\nplt.grid(False)\nplt.show()\n# Variational Inference - CAVI\ndef VI(K, prior_std, n, data):\n# Initialization\nmu_mean = []\nfor i in range(K):\nmu_mean.append(np.random.normal(0, prior_std))\nc_est = np.zeros((n, K)) # 1000*3 matrix\nfor i in range(n):\nc_est[i, np.random.choice(K)] = 1\n# Initiate CAVI iterations\nwhile (True):\nmu_mean_old = mu_mean[:]\n# mixture model parameter update step\nfor j in range(K):\nnr = 0\nfor i in range(n):\nnr += c_est[i, j] * data[i]\nmu_mean[j] = nr / ((1 / prior_std ** 2) + dr)\n# categorical vector update step\nfor i in range(n):\ncat_vec = []\nfor j in range(K):\ncat_vec.append(math.exp(mu_mean[j] * data[i] - (mu_var[j] + mu_mean[j] ** 2) / 2))\nfor k in range(K):\nc_est[i, k] = cat_vec[k] / np.sum(np.array(cat_vec))\n# compute ELBO or check for convergence of variational factors - here we do the latter\nif np.dot(np.array(mu_mean_old) - np.array(mu_mean), - np.array(mu_mean)) < 0.000001:\nbreak\n# sort the values since the k mixture components dont maintain any ordering across multiple initializations\nmixture_components = list(zip(mu_mean, mu_var))\nmixture_components.sort()\nreturn mixture_components, c_est\n# Run the CAVI algorithm\nmixture_components, c_est = VI(K, prior_std, n, data)\nprint(K,\"||\",mixture_components, \"||\", c_est)\nplt.hist(data_resp[k], bins=30, color=color[k], alpha=0.3, density=True)\nvals=np.random.normal(mixture_components[k][0], 1, size=1000)\nsns.kdeplot(vals, color='k', linewidth=1.6);\n", "output_sequence": "Use Variational Bayes (VB) to approximate the density of Gaussian Mixture Models (GMM)"}, {"input_sequence": "Author: Daniel Jacob ; install.packages(\"xgboost\", repos=c(\"http://dmlc.ml/drat/\", getOption(\"repos\")), type=\"source\")\nvec.pac= c(\"SuperLearner\", \"gbm\", \"glmnet\",\"ranger\",\"nnet\",\"caret\")\nlapply(vec.pac, require, character.only = TRUE)\n#Learner Library:\nlearners <- c( \"SL.glmnet\",\"SL.xgboost\", \"SL.ranger\",\"SL.nnet\")\n#CV Control for the SuperLearner\ncontrol <- SuperLearner.CV.control(V=5)\nS_learner <- function(data,covariates,learners){\n\ndata$ID <- c(1:nrow(data))\nscore_S <- matrix(0,nrow(data),1)\nset.seed(1234)\nfolds <- createFolds(data$d,k=5)\nfor(f in 1:(length(folds))){\n\nif(f == 1){\ndata1 <- data[c(folds[[5]],folds[[2]],folds[[3]],folds[[4]]),]\ndf_main <- data[folds[[1]],]\n}\nif(f == 2){\ndata1 <- data[c(folds[[1]],folds[[5]],folds[[3]],folds[[4]]),]\ndf_main <- data[folds[[2]],]\nif(f == 3){\ndata1 <- data[c(folds[[1]],folds[[2]],folds[[5]],folds[[4]]),]\ndf_main <- data[folds[[3]],]\nif(f == 4){\ndata1 <- data[c(folds[[1]],folds[[2]],folds[[3]],folds[[5]]),]\ndf_main <- data[folds[[4]],]\nif(f == 5){\ndata1 <- data[c(folds[[1]],folds[[2]],folds[[3]],folds[[4]]),]\ndf_main <- data[folds[[5]],]\ndf_aux <- data1\nX_train <- (df_aux[,c(covariates,\"d\")])\n# Train a regression model using the covariates and the treatment variable\nm_mod <- SuperLearner(Y = df_aux$y, X = X_train, SL.library = learners,\nverbose = FALSE, method = \"method.NNLS\",cvControl = control)\n# Set treatment variable to 0\nX_test_0 <- (df_main[,c(covariates,\"d\")])\nX_test_0$d <- 0\n# Set treatment variable to 1\nX_test_1 <- (df_main[,c(covariates,\"d\")])\nX_test_1$d <- 1\n# Estimate the CATE as the difference between the model with different treatment status\nscore_S[,1][df_main$ID] = predict(m_mod,X_test_1)$pred - predict(m_mod,X_test_0)$pred\n}\nreturn(score_S)\n}\n", "output_sequence": "Single model method to estimate the conditional average treatment effect (CATE) via a variety of machine learning (ML) methods."}, {"input_sequence": "Author: Daniel Jacob ; install.packages(\"xgboost\", repos=c(\"http://dmlc.ml/drat/\", getOption(\"repos\")), type=\"source\")\nvec.pac= c(\"SuperLearner\", \"gbm\", \"glmnet\",\"ranger\",\"caret\")\nlapply(vec.pac, require, character.only = TRUE)\n#Learner Library:\nlearners <- c( \"SL.glmnet\",\"SL.xgboost\", \"SL.ranger\",\"SL.nnet\")\n#CV Control for the SuperLearner\ncontrol <- SuperLearner.CV.control(V=5)\nIPW_learner <- function(data,covariates,learners){\ndata$ID <- c(1:nrow(data))\n\n##### # 5-fold sample splitting\n# Sample splitting\nset.seed(1234)\nfolds <- createFolds(data$d,k=5)\nfor(f in 1:(length(folds))){\nif(f == 1){\ndata1 <- data[c(folds[[5]],folds[[2]],folds[[3]],folds[[4]]),]\ndf_main <- data[folds[[1]],]\n}\nif(f == 2){\ndata1 <- data[c(folds[[1]],folds[[5]],folds[[3]],folds[[4]]),]\ndf_main <- data[folds[[2]],]\nif(f == 3){\ndata1 <- data[c(folds[[1]],folds[[2]],folds[[5]],folds[[4]]),]\ndf_main <- data[folds[[3]],]\nif(f == 4){\ndata1 <- data[c(folds[[1]],folds[[2]],folds[[3]],folds[[5]]),]\ndf_main <- data[folds[[4]],]\nif(f == 5){\ndata1 <- data[c(folds[[1]],folds[[2]],folds[[3]],folds[[4]]),]\ndf_main <- data[folds[[5]],]\ndf_aux <- data1\n## IPW-learner\n# Train a classification model to get the propensity scores\np_mod <- SuperLearner(Y = df_aux$d, X = df_aux[,covariates], newX = df_main[,covariates], SL.library = learners,\nverbose = FALSE, method = \"method.NNLS\", family = binomial(),cvControl = control)\np_hat <- p_mod$SL.predict\np_hat = ifelse(p_hat<0.025, 0.025, ifelse(p_hat>.975,.975, p_hat)) # Overlap bounding\n# Transformed outcome\ny_to <- df_main$y * df_main$d/p_hat - df_main$y * (1-df_main$d)/(1-p_hat)\n## Collect all pseudo outcomes\npseudo_all[,1][df_main$ID] <- y_to\n}\nres_combined_ipw <- matrix(NA,nrow(data),5)\n# loop\nfor(l in 1:10){\nset <- seq(from=1, to=nrow(data)+1,by=(nrow(data)/10))\nset\nif(l<=5){\nipw_mod_cf <- SuperLearner(Y = pseudo_all[(set[l]:set[l+1]-1),1],X=data[(set[l]:set[l+1]-1),covariates],\nnewX = data[(set[6]:set[11]-1),covariates], SL.library = learners,\nverbose = FALSE, method = \"method.NNLS\",cvControl = control)\n\nscore_ipw_1_cf <- ipw_mod_cf$SL.predict\nres_combined_ipw[(set[6]:set[11]-1),l] <- score_dr_1_cf\n}\nif(l>5){\nipw_mod_cf <- SuperLearner(Y =pseudo_all[(set[l]:set[l+1]-1),1],X=data[(set[l]:set[l+1]-1),covariates],\nnewX =data[(set[1]:set[6]-1),covariates], SL.library = learners,\nverbose = FALSE, method = \"method.NNLS\",cvControl = control)\nscore_ipw_0_cf <- ipw_mod_cf$SL.predict\nres_combined_ipw[(set[1]:set[6]-1),(l-5)] <- score_dr_0_cf\n\nscore_ipw_oob <- rowMeans(res_combined_ipw)\nreturn(score_ipw_oob)\n", "output_sequence": "Inverse probability weighting (transformed-outcome) method to estimate the conditional average treatment effect (CATE) via a variety of machine learning (ML) methods."}, {"input_sequence": "Author: Ya Qian ; #Close windows and clear variables\ngraphics.off()\nrm(list = ls(all=TRUE))\n# install and load packages\nlibraries = c(\"fields\", \"graphics\", \"stats\", \"Matrix\", \"ggplot2\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n#whole sample connectedness\ndatafile = c(\"beta_L_median.csv\", \"beta_L_lowertail.csv\",\"beta_L_uppertail.csv\")\nconn_tot = matrix(0,1,3)\nfor (s in 1:length(datafile)){\nconn_total = read.csv(datafile[s], header = TRUE, sep = \",\", dec = \".\")\nindustrynames = read.csv(\"industrynames.csv\")\nconn_total = conn_total [ ,-1]\nrownames(conn_total) = industrynames[ ,2]\nadjac = data.matrix(conn_total)\ndiag(adjac) = 0\nadjac = abs(adjac)\nconn_tot[,s] = sum(adjac)/49\n}\n\n#moving window connectedness\ndatafile_mw = c(\"mw/beta_L_median \", \"mw/beta_L_lowertail \", \"mw/beta_L_uppertail \")\nindustrynames = read.csv(\"industrynames.csv\")\nconn_mw = matrix(0,177,3)\nfor (i in 1:length(datafile_mw)){\nfor (s in 1:177){\ndatafile = paste(datafile_mw[i], s, \" .csv\", sep =\"\")\nnetworkcoeff = read.csv(datafile, header = TRUE, sep = \",\", dec = \".\")\nnetworkcoeff = networkcoeff[ ,-1]\nrownames(networkcoeff) = industrynames[ ,2]\nadjac = data.matrix(networkcoeff)\ndiag(adjac) = 0\nadjac = abs(adjac)\nconn_mw[s,i] = sum(adjac)/49\n}\ncolnames(conn_mw) = c('melevel','lolevel', 'uplevel')\ndateindex = seq(as.Date(\"1970/1/1\"), \"quarters\")\nconn_mw = as.data.frame(conn_mw)\nrownames(conn_mw) = dateindex\nwrite.csv(conn_mw, file = \"conn_mw.csv\")\npng(file=\"connect.png\",width = 400, height=320, bg = NA)\nplot(dateindex, conn_mw$melevel, col = 'black', type ='l', ylim = c(0,20), ylab = 'connectedness', xlab = 'date')\nlines(dateindex, conn_mw$lolevel, col = 'red')\n#ggplot(data = conn_mw, aes(x = dateindex)) +\n# geom_line(aes(y = melevel)) +\n# scale_color_manual(values = c('black', 'red', 'darkgreen'))+\n# labs(y = \"connectedness\")\n#plot the time series of connectedness\n#conn_mw_ts = ts(conn_mw, start=c(1970, 01), end=c(2014, 01), frequency=12)\n#png(file=\"out.png\",width = 400, height=320, bg = NA)\n#ts.plot(conn_mw_ts, gpars= list(col=c(\"blue\", \"red\", \"darkgreen\"))) # connect\n", "output_sequence": "Calculate the whole sample and moving window network connectedness of industry portfolios"}, {"input_sequence": "Author: Ya Qian ; #Close windows and clear variables\ngraphics.off()\nrm(list = ls(all=TRUE))\n# install and load packages\nlibraries = c(\"fields\", \"graphics\", \"stats\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n#setwd(\"/Users/qianya/Library/Mobile Documents/com~apple~CloudDocs/ffdata/test\")\n#Define the connectedness function\nconnectedness_cyc = function(A) {\nW = A^(1/3)\nfrom = colSums(A!=0)\nA2 = as.matrix((A>0)+0)\nconn_cyc = diag(W1)/(from*to-self)\nconn_cyc[is.na(conn_cyc)]= 0\nreturn(mean(conn_cyc))\n}\nconnectedness_mid = function(A) {\nW2 = W%*%t(W)%*%W\nconn_mid = diag(W2)/(from*to-self)\nconn_mid[is.na(conn_mid)]= 0\nreturn(mean(conn_mid))\nconnectedness_ins = function(A) {\nW3 = t(W)%*%W%*%W\nconn_ins = diag(W3)/(from*(from-1))\nconn_ins[is.na(conn_ins)]= 0\nreturn(mean(conn_ins))\nconnectedness_out = function(A) {\nW4 = W%*%W%*%t(W)\nconn_out = diag(W4)/(to*(to-1))\nconn_out[is.na(conn_out)]= 0\nreturn(mean(conn_out))\n#whoel sample connectedness\ndatafile = c(\"beta_L_median.csv\", \"beta_L_lowertail.csv\",\"beta_L_uppertail.csv\")\ncc = matrix(0,4,3)\ncolnames(cc) = c(\"median\", \"lowertail\",\nrownames(cc) = c( \"cycles\", \"middlemen\", \"ins\", \"out\")\nfor (s in 1:length(datafile)){\nclustercoeff = read.csv(datafile[s], header = TRUE, sep = \",\", dec = \".\")\nindustrynames = read.csv(\"industrynames.csv\")\nclustercoeff = clustercoeff[ ,-1]\nrownames(clustercoeff) = industrynames[ ,2]\nadjac = data.matrix(clustercoeff)\ndiag(adjac) = 0\nadjac = abs(adjac)\ncc[1,s] = connectedness_cyc(adjac)\nwrite.table(cc, file = \"wholeconnectedness.txt\", sep = '&')\n#moving window connectedness\ndatafile_mw = c(\"mw/beta_L_median \", \"mw/beta_L_lowertail \", \"mw/beta_L_uppertail \")\ncc_mw = matrix(0,177,12)\nfor (i in 1:length(datafile_mw)){\nfor (s in 1:177){\ndatafile = paste(datafile_mw[i], s, \" .csv\", sep =\"\")\nclustercoeff = read.csv(datafile, header = TRUE, sep = \",\", dec = \".\")\nclustercoeff = clustercoeff[ ,-1]\nrownames(clustercoeff) = industrynames[ ,2]\nadjac = data.matrix(clustercoeff)\ndiag(adjac) = 0\nadjac = abs(adjac)\ncc_mw[s,(4*(i-1)+1)] = connectedness_cyc(adjac)\n}\n#cc_mw_ave = colMeans(cc_mw)\n#write.table(cc_mw_ave, file = \"connectedness_mw.txt\", sep = '&')\ncolnames(cc_mw) = c('cyc_m','mid_m', 'ins_m', 'cyc_l','mid_l', 'ins_l', 'cyc_u','mid_u', 'ins_u', 'out_u')\ndateindex = seq(as.Date(\"1970/1/1\"), \"quarters\")\ncc_mw = as.data.frame(cc_mw)\nwrite.csv(cc_mw, \"cc_mw.csv\")\npng(file=\"ccplot_median.png\",width = 400, height=320, bg = NA)\nplot(dateindex, cc_mw$cyc_m, col = 'blue', type ='l', ylim = c(0,0.25), ylab = 'clustering coefficient', xlab = 'date')\nlines(dateindex, cc_mw$mid_m, col = 'red')\ndev.off()\npng(file=\"ccplot_lowertail.png\",width = 400, height=320, bg = NA)\nplot(dateindex, cc_mw$cyc_l, col = 'blue', type ='l', ylim = c(0,0.25), ylab = 'clustering coefficient', xlab = 'date')\nlines(dateindex, cc_mw$mid_l, col = 'red')\npng(file=\"ccplot_uppertail.png\",width = 400, height=320, bg = NA)\nplot(dateindex, cc_mw$cyc_u, col = 'blue', type ='l', ylim = c(0,0.25), ylab = 'clustering coefficient', xlab = 'date')\nlines(dateindex, cc_mw$mid_u, col = 'red')\npng(file=\"cyclesplot.png\",width = 400, height=320, bg = NA)\nlines(dateindex, cc_mw$cyc_l, col = 'red')\npng(file=\"middlemenplot.png\",width = 400, height=320, bg = NA)\nplot(dateindex, cc_mw$mid_m, col = 'blue', type ='l', ylim = c(0,0.25), ylab = 'clustering coefficient', xlab = 'date')\nlines(dateindex, cc_mw$mid_u, col = 'green')\npng(file=\"insplot.png\",width = 400, height=320, bg = NA)\nplot(dateindex, cc_mw$ins_m, col = 'blue', type ='l', ylim = c(0,0.25), ylab = 'clustering coefficient', xlab = 'date')\nlines(dateindex, cc_mw$ins_l, col = 'red')\npng(file=\"outplot.png\",width = 400, height=320, bg = NA)\nplot(dateindex, cc_mw$out_m, col = 'blue', type ='l', ylim = c(0,0.25), ylab = 'clustering coefficient', xlab = 'date')\nlines(dateindex, cc_mw$out_l, col = 'red')\n", "output_sequence": "Calculate the whole sample and moving window network clustering coefficients of industry portfolios"}, {"input_sequence": "Author: Joanna Tomanek, Lasse Groth, Sophie Burgard ; # clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\nset.seed(2)\n# parameter settings\nn = 50\nbeta = 0.5\n# Simulation of MA(1)-processes as the true values\nx = arima.sim(n, model = list(ma = beta), rand.gen = function(n) rnorm(n, 0, 1))\nx = as.matrix(x)\n# Estimated values for beta\ntheta = seq(-0.9, by = 0.01, 0.9)\nk = length(theta)\nl1 = matrix(1, k, 1)\nl2 = l1\nfor (i in 1:k) {\nb = theta[i] # beta estimate [i]\ng = diag((b^2 + 1), n, n)\nh = cbind(rbind(0, h1), 0)\ng = g + h + t(h) # Covariance matrix\nl1[i] = -(n/2) * log(2 * pi) - 0.5 * log(det(g)) - 0.5 * t(x) %*% solve(g) %*% x # exact Log likelihood\narcoeff = (-b)^(1:(n - 1)) # coefficients of AR(1) process for lag=2:10\ne = matrix(1, n, 1)\n\n# Approximation of errors\nfor (t in 2:n) {\ne[t] = x[t] + sum(t(arcoeff[1:(t - 1)]) * x[(t - 1):1])\n}\nl2[i] = -(n/2) * log(2 * pi) - 0.5 * sum(e^2) # Conditional log likelihood\n}\n# Plots\ndat1 = cbind(theta, l1)\nplot(dat1,\ncol = 4,\nxlab = \"Beta\",\nylab = \"log-Likelihood\",\nmain = paste(\"Likelihood Function of a MA(1) Process with n = \", n, sep=\"\"),\ntype = \"l\",\nlwd = 2)\ndat2 = cbind(theta, l2)\npoints(dat2,\ntype = \"l\",\nlwd = 2)\n#line indicating max. of exact likelihood\nabline(v = theta[which.max(l1)])\n#line indicating true parameter\nabline(v = beta,\nlty = 2)\n", "output_sequence": "Plots the exact and conditional likelihood function of an MA(1) (moving average) process."}, {"input_sequence": "Author: Joanna Tomanek, Lasse Groth, Sophie Burgard ; # ------------------------------------------------------------------------------\n# Published in: Statistics of Financial Markets II\n# ------------------------------------------------------------------------------\n# Description: The following code introduces the Box-Jenkins Method. First, it\n# simulates an ARMA process and tests it for a unit root with\n# Dickey Fuller Test. Next, the ACF and PACF of the series are\n# visually inspected and a function for identifying the best\n# ARMA model for a given time series according to the AIC\n# criterion is indroduced. Finally, the best model according to\n# the above function is being diagnosed.\n# Keywords: Box Jenkins, time series, AR, MA, ARMA\n# Author: Sophie Stadlinger, Karolina Stanczak\n# Submitted: 2016/06/17\n# Datafile: -\n# Input: -\n# Output: Order of the best ARMA model according to the AIC criterion for\n# a given time series. Time series plot, ACF and PACF plots of a\n# time series and its residuals.\n#########################################################\n############## Preparations ###################\n## - Installing & loading required packages\n## - Setting working directory\n# Function for installing & loading required packages.\nloadinstallpackages = function(packs){\nnew.packs = packs[!(packs %in% installed.packages()[, \"Package\"])]\nif (length(new.packs))\ninstall.packages(new.packs, dependencies = TRUE)\nsapply(packs, require, character.only = TRUE)\n}\n# Packages we will need for our code are inside \"packs\"\npacks = cbind(\"tseries\", \"quantmod\", \"scales\",\"kedd\")\nloadinstallpackages(packs)\n# Set working directory:\n# setwd()\n############### Time Series ###################\n# Simulate ARMA Process\nset.seed(231)\nts = arima.sim(n=5000, list(ar= c(0.68, -.32), ma= c(0.24)),\ninnov = rnorm(5000))\n## Visualize the data\n# Plot of the stochastic process\nplot(ts, type = \"l\", lwd = 1, ylab = \"y_t\",col = \"blue\",\nframe = T, axes = F, xlab = \"t\",\nmain = \"Simulated ARMA(2,1)-process\")\naxis(side = 2, at = seq(0, 2250, by = 250), las=1, lwd = 1, tck = 1,\ncol = \"black\", cex.axis = 0.8)\naxis(side = 1, lwd = 0.01, tck = -0.02, col = \"black\", cex.axis = 0.8)\n############ Box-Jenkins-Method ################\n########################\n# 1. Step\n# Identification\n# Function that checks for stationarity (requires \"tseries\" package):\n# Dickey Fuller Test\nis.stationary = function(data,\nalpha = 0.05\n){\noptions(warn=-1)\noutcome = adf.test(data)\nif (outcome$p.value < alpha){\nprint(sprintf(\"%s is stationary\",\nas.character(substitute(data))))\n}\nelse{\nprint(sprintf(\"WARNING! %s is not stationary\",\nas.character(substitute(data))))\n# Is stochastic process stationary?\nis.stationary(ts) #TRUE\n# Visual inspection of the ACF and PACF\npar(mfrow = c(1,2))\nacf.ts = acf(ts,lag=12, main = \"ACF\")\n# 2. Step\n# Estimation\n#### The following function finds the best ARMA model based on the AIC\n#### The function contains the estimation stage and the diagnostic\n#### checking stage of the Box Jenkins Method\nalpha = 0.05\nbest_arma = function(data, orderp, orderq){\n# Set up some values that will help us write the code:\nrangep = 1:orderp\nnr.of.models = (orderp) * (orderq+1)\n\n# Create a container for the AIC of the models\nn = 1 # n will help with the indexing\naic = matrix(rep(100000), ncol=nr.of.models, nrow=2)\n# Check different ARMA models\nfor(i in rangep){\noutput = arima(data, order = c(i, 0, j))\n# For testing ARMA(p,q) residuals, fitdf should be set to p+q (see\n# Box.test-help). Lag has to be larger than fitdf, so lag = p+q+1 when\n# testing residuals.\npvalue = Box.test(output$residuals, lag = i + j + 1, type = \"Ljung\",\nfitdf = i + j)$p.value\naic[1, n] = paste(\"ARMA(\", i, \",\", j, \")\", sep = \"\")\n\nif (pvalue >= alpha) {\naic[2, n] = output$aic\n} else {\nprint(sprintf\n(\"WARNING! The ARMA(%d, %d) model contains correlated residuals\",\ni, j))\n}\nn = n + 1\n}\n# Which model gives the lowest (= best) AIC?:\nmin.aic = min(as.numeric(aic[2, ]))\nbestmodel = print(paste(\"The lowest AIC is provided by\",\naic[1, as.numeric(aic[2, ]) == min.aic]))\nindex = as.numeric(unlist(strsplit\n(aic[1, as.numeric(aic[2, ]) == min.aic],\n\"[^[:digit:]]\")))\nindex = index[!is.na(index)]\np = index[1]\n# Calculating the best model\nbestmodeloutput = arima(data, order = c(p, 0, q))\nfinal = list(bestmodel = bestmodel, aic = aic,\nbestmodeloutput = bestmodeloutput)\nreturn(final)\narma_best = best_arma(ts, 3, 3)\n# 3. Step\n# Diagnostic\n# check if residuals are uncorrelated\nbestACFres = acf(arma_best$bestmodeloutput$residuals, lag = 12,\nmain = \"ACF Residuals\")\nbestPACFres = pacf(arma_best$bestmodeloutput$residuals, lag = 12,\nmain = \"PACF Residuals\" )\n", "output_sequence": "Plots the exact and conditional likelihood function of an MA(1) (moving average) process."}, {"input_sequence": "Author: Philipp Gsch\u00f6pf, Andrija Mihoci, Shi Chen, Lukas Borke ; \n# clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n\n# Load required packages\nlibraries = c(\"numDeriv\", \"VGAM\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# Define different distributions for comparison of quantiles and expectiles\n# Fdist1: the distribution such that the expectile equals the quantile\nFdist1 = function(x) {\nif (x < 0) {\n0.5 * (1 - (1 - 4/(4 + x^2))^0.5)\n} else {\n0.5 * (1 + (1 - 4/(4 + x^2))^0.5)\n}\n}\n# Fdist2: normal distribution\nFdist2 = function(x) {\npnorm(x)\n# Fdist2: Laplace distribution\nFdist3 = function(x) {\nplaplace(x)\n# Select different distributions (Fdist1, or Fdist3) as input\nF = function(x, Fdist=Fdist1) {\nsapply(x, Fdist)\nf = function(x) {\ngrad(F, x)\ntauexact = function(alpha) {\ninverse = function(f, lower = -100, upper = 100) {\nfunction(y) uniroot((function(x) f(x) - y), lower = lower, upper = upper)[1]\nquantileFun = inverse(F)\nq = as.numeric(quantileFun(alpha))\nLPM = function(x) {\nx * (f(x))\nLPMq = function(x) {\nintegrate(LPM, -Inf, x)\ntmp = as.numeric(LPMq(q)[1]) - q * alpha\ntmp/(2 * tmp + q)\nq_func = function(alpha, delta = 1) {\nq\nPartialMoment = function(x) {\nLPM = function(y) {\ny * (f(y))\nas.numeric(integrate(LPM, -Inf, x)[1])[1]\nG = function(x) {\n(PartialMoment(x) - x * F(x))/(2 * (PartialMoment(x) - x * F(x)) + (x))\ng = function(x) {\ngrad(G, x)\ne = function(alpha) {\ninverse = function(g, lower = -100, upper = 100) {\nfunction(y) uniroot((function(x) g(x) - y), lower = lower, upper = upper)[1]\nquantileFun = inverse(G)\nalphas = seq(0.01, 0.99, 0.01)\n#alphas[86] = 0.859 #additional assumption for laplace distribution\nquant = c(sapply(alphas, q_func))\ndiff = (quant - expect)\npar(mfrow=c(2,1))\nplot(alphas, quant, type = \"l\", col = \"blue\", lwd = 3, ylab=\"\", main = \"quantiles vs. expectiles\", xlim=c(0, 1), ylim=c(-4, 4) )\npoints(alphas, expect, type = \"l\", col = \"red\", ylab=\"\", lwd = 3)\nplot(alphas, diff, type = \"l\", col = \"blue\", lwd = 3, ylab=\"\", main = \"difference\", xlim=c(0, 1), ylim=c(-1.5, 1.5))\n", "output_sequence": "Calculates and compares the expectile and quantile functions for all risk levels under different distribution."}, {"input_sequence": "Author: Chen Huang ; libraries = c(\"mvtnorm\", \"hdm\", \"sandwich\", \"matrixStats\",\"doSNOW\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\nc1 = makeCluster(20) # core numbers\nregisterDoSNOW(c1)\nnboot = 5000\nrep = 1000\nT = 100 #sample size\nP = c(50,100,150) #number of equations\nb = 10 #nonzero entries\nd = 5\nsim_scene = function(nboot,T,p,K,d,b){\ncov = matrix(0,K,K)\nfor(i in 1:K){\ncov[i,j] = 0.5^(abs(i-j))\n}\nbeta.matrix = matrix(0, p, K)\nlambda.ga = 2*1.1*qnorm(1-0.1/(2*K*p))*sqrt(T)\nfor (i in 1:p){\nindx = ceiling(i/d)\nbeta.matrix[i, (d*(indx-1)+1):(indx*d)] = b\nY.data = matrix(0, T, p)\nSmatrix.boot = array(0,dim=c(p,K,nboot))\nc = rep(0, nboot)\n\nX = rmvnorm(T, rep(0,K), cov)\nY = X%*%beta.matrix[i,] + rnorm(T, 0, 1) #sd=1 or 0.5\nY.data[,i] = Y\nfit1 = rlasso(X, Y, penalty = list(homoscedastic = \"none\", lambda.start = 2*0.5*sqrt(T)*qnorm(1-0.1/(2*K))), post=FALSE)\nbeta = fit1$beta\nintercept = fit1$intercept\nres = Y - X %*% beta - intercept * rep(1, length(Y))\nfor (j in 1:nboot){\nres.boot = rnorm(T)\nfor (k in 1:K){\nSmatrix.boot[i,k,j] = sum(res.boot*X[,k]*res)/sqrt(T)\n}\n#### compute the penalty loadings\nfor(k in 1:K){\nload[i,k] = sd(X[,k]*res)\nfor(j in 1:nboot){\nc[j]=max(abs(Smatrix.boot[,,j]/load))\nlambda.boot = 2*quantile(c, 0.9)*sqrt(T)*1.1\nrmse = rmse.joint = l2 = l2.joint = matrix(0,p,2)\n# compare the in-sample fitting performance\nY = Y.data[,i]\nfit_hdm = rlasso(X, Y, penalty = list(X.dependent.lambda = TRUE, numSim = nboot))\nrmse[i,1] = sqrt(mean((X%*%beta.matrix[i,]-X%*%fit_hdm$beta)^2))\nl2[i,1] = sqrt(sum((fit_hdm$beta-beta.matrix[i,])^2))\nrmse[i,2] = sqrt(mean((X%*%beta.matrix[i,]-X%*%fit_hdm2$beta)^2))\nl2[i,2] = sqrt(sum((fit_hdm2$beta-beta.matrix[i,])^2))\nfit_hdm.joint = rlasso(X,Y,penalty = list(homoscedastic = \"none\", lambda.start = lambda.boot))\nrmse.joint[i,1] = sqrt(mean((X%*%beta.matrix[i,]-X%*%fit_hdm.joint$beta)^2))\nl2.joint[i,1] = sqrt(sum((fit_hdm.joint$beta-beta.matrix[i,])^2))\nrmse.joint[i,2] = sqrt(mean((X%*%beta.matrix[i,]-X%*%fit_hdm.joint2$beta)^2))\nl2.joint[i,2] = sqrt(sum((fit_hdm.joint2$beta-beta.matrix[i,])^2))\nlist(ratio.median = colMedians(rmse.joint)/colMedians(rmse), ratio.median2 = colMedians(l2.joint)/colMedians(l2))\n}\nfor(pp in 1:length(P)){\np = P[pp]\nresults = foreach(l=1:rep, .packages=c(\"mvtnorm\", \"Matrix\", \"hdm\", \"sandwich\", \"matrixStats\"), .inorder=FALSE) %dopar%{\nsim_scene(nboot=nboot,T=T,p=p,K=K,d=d,b=b)\nsave(results, file = paste(\"ratios_K\",K,\".dat\", sep = \"\"))\nratios.mean = ratios.median = ratios.mean2 = ratios.median2 = ratios.sd = ratios.sd2 = matrix(0, length(P),2)\nload(file = paste(\"ratios_K\",K,\".dat\", sep = \"\"))\nratio.median = ratio.median2 = numeric(0)\nfor(r in 1:rep){\nratio.median = rbind(ratio.median,results[[r]]$ratio.median)\nratios.mean[pp,] = colMeans(ratio.median)\nround(ratios.mean[,1], digits = 4)\n", "output_sequence": "Compares the prediction norm with the penalty level selected equation by equation or jointly over equations in a regression system under iid data setting."}, {"input_sequence": "Author: Chen Huang ; libraries = c(\"mvtnorm\", \"hdm\", \"Matrix\", \"sandwich\", \"matrixStats\", \"doSNOW\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\nc1 = makeCluster(20) # core numbers\nregisterDoSNOW(c1)\nnboot = 5000\nrep = 1000\nT = 100 #sample size\nP = c(50,100,150) #number of equations\nb = 10 #nonzero entries\nd = 5\nBn = c(4,6,8,10) #length of block\nRho = c(0.1,1)\ndf = 8 #4\nsim_scene = function(nboot,T,p,K,rho,Bn,df,d,b){\nbeta.matrix = matrix(0, p, K)\nlambda.ga = 2*1.1*qnorm(1-0.1/(2*K*p))*sqrt(T)\nSmatrix.boot = array(0,dim=c(p,K,nboot))\nload = matrix(0,p,K)\nc = rep(0, nboot)\nfor (i in 1:p){\nindx = ceiling(i/d)\nbeta.matrix[i, (d*(indx-1)+1):(indx*d)] = b\n}\nY.data = matrix(0, T, p)\n\ninnov = numeric(0)\nfor(i in 1:K){\ninnov = cbind(innov, rt(T+1000+1, df)/sqrt(df/(df-2)))\neta = matrix(0,T+1000,K)\nfor(t in (1+1):(T+1+1000)){\neta[t-1,] = innov[t,]*sqrt(0.8*innov[t-1,]^2+0.2)\nM = A = list()\nfor(j in 1:(1000+1)){\nM[[j]] = matrix(rnorm(K*K), nrow=K, ncol=K)\nA[[j]] = (j-1+1)^(-rho-1)*M[[j]]\nX = matrix(0,nrow=T,ncol=K)\nfor(t in 1:(T)){\nfor(tt in 1:(1000+1)){\nX[t,] = X[t,] + A[[tt]]%*%eta[t-tt+1000+1,]\n}\ninnov2 = numeric(0)\nfor(i in 1:p){\ninnov2 = cbind(innov2, rt(T+1+1000, df)/sqrt(df/(df-2)))\neta2 = matrix(0,T+1000,p)\neta2[t-1,] = innov2[t,]*sqrt(0.8*innov2[t-1,]^2+0.2)\nM2 = A2 = list()\nM2[[j]] = matrix(rnorm(p*p), nrow=p, ncol=p)\nA2[[j]] = (j-1+1)^(-rho-1)*M2[[j]]\neps = matrix(0,nrow=T,ncol=p)\neps[t,] = eps[t,] + A2[[tt]]%*%eta2[t-tt+1000+1,]\nrmse = rmse.joint = l2 = l2.joint = array(0,dim=c(p,length(Bn),2))\nratio.median = ratio.median2 = matrix(0,nrow=length(Bn),ncol=2)\nfor(bb in 1:length(Bn)){\nbn = Bn[bb]\nln = floor(T/bn) #number of blocks\nfor (i in 1:p){\nY = X%*%beta.matrix[i,] + eps[,i] #rnorm(T, 0, 1) #sd=1 or 0.5\nY.data[,i] = Y\nfit1 = rlasso(X, Y, penalty = list(homoscedastic = \"none\", lambda.start = 2*0.5*sqrt(T)*qnorm(1-0.1/(2*K))), post=FALSE)\nbeta = fit1$beta\nintercept = fit1$intercept\nres = Y - X %*% beta - intercept * rep(1, length(Y))\nfor (j in 1:nboot){\nres.boot = rnorm(ln)\nfor (k in 1:K){\nSmatrix.boot[i,k,j] = sum(c(rep(res.boot, each=bn),rep(rnorm(1),T-ln*bn))*X[,k]*res)/sqrt(T)\n}\n#### compute the penalty loadings\nfor(k in 1:K){\nload[i,k] = sqrt(lrvar(X[,k]*res)*T)\n\nfor(j in 1:nboot){\nc[j]=max(abs(Smatrix.boot[,,j]/load))\nlambda.boot = 2*quantile(c, 0.9)*sqrt(T)*1.1\n# compare the in-sample fitting performance\nY = Y.data[,i]\nfit_hdm2 = rlasso(X, Y, penalty = list(X.dependent.lambda = FALSE))\nrmse[i,bb,] = sqrt(mean((X%*%beta.matrix[i,]-X%*%fit_hdm2$beta)^2))\nl2[i,bb,] = sqrt(sum((fit_hdm2$beta-beta.matrix[i,])^2))\nfit_hdm.joint = rlasso(X,Y,penalty = list(homoscedastic = \"none\", lambda.start = lambda.boot))\nrmse.joint[i,bb,1] = sqrt(mean((X%*%beta.matrix[i,]-X%*%fit_hdm.joint$beta)^2))\nl2.joint[i,bb,1] = sqrt(sum((fit_hdm.joint$beta-beta.matrix[i,])^2))\nrmse.joint[i,bb,2] = sqrt(mean((X%*%beta.matrix[i,]-X%*%fit_hdm.joint2$beta)^2))\nl2.joint[i,bb,2] = sqrt(sum((fit_hdm.joint2$beta-beta.matrix[i,])^2))\nratio.median[bb,] = colMedians(rmse.joint[,bb,])/colMedians(rmse[,bb,])\nlist(ratio.median = ratio.median, = ratio.median2)#}\n}\nfor(rr in 1:length(Rho)){\nrho = Rho[rr]\nfor(kk in 1:length(P)){\np = P[kk]\nresults = foreach(l=1:rep, .packages=c(\"mvtnorm\", \"Matrix\", \"hdm\", \"sandwich\", \"matrixStats\"), .inorder=FALSE) %dopar%{\nsim_scene(nboot=nboot,T=T,p=p,K=K,rho=rho,Bn=Bn,df=df,d=d,b=b)\nsave(results, file = paste(\"ratios_rho\",rho,\"_K\",K,\".dat\", sep = \"\"))\nratios.mean = ratios.median = ratios.mean2 = ratios.median2 = ratios.sd = ratios.sd2 = list()\nratios.mean[[rr]] = ratios.median[[rr]] = ratios.mean2[[rr]] = ratios.median2[[rr]] = ratios.sd[[rr]] = ratios.sd2[[rr]] = array(0,dim=c(length(Bn),length(P),2))\nload(file = paste(\"ratios_rho\",rho,\"_K\",K,\".dat\", sep = \"\"))\nratio.median = ratio.median2 = array(0,dim=c(rep,length(Bn),2))\nfor(r in 1:rep){\nratio.median[r,,] = results[[r]]$ratio.median\nfor(bb in 1:length(Bn)){\nratios.mean[[rr]][bb,kk,] = colMeans(ratio.median[,bb,])\n}\nround(ratios.mean[[1]][,,1], digits = 4)\n", "output_sequence": "Compares the prediction norm with the penalty level selected equation by equation or jointly over equations in a regression system with dependent data."}, {"input_sequence": "Author: Xiu Xu ; function [output] = mqRQobjectiveFunction(Beta0, y, THETA, empiricalQuantile, OUT)\n[T,N]=size(y);\nq = zeros(T,N); q(1,:) = empiricalQuantile;\nA = reshape(Beta0, N, 1+2*N);\nc=A(:,1)'; a=A(:,2:N+1)'; b=A(:,N+2:1+2*N)';\nfor t = 2:T\nq(t,:) = c + abs(y(t-1,:))*a + q(t-1,:)*b;\nend\nS = (y - q).*(THETA*ones(T,N) - (y<q));\nS = sum(S,2); S = mean(S);\nif OUT == 1, output = S;\n", "output_sequence": "do the adaptive estimation for localising multivariate CAViaR model, under two risk cases at quantile level 1%."}, {"input_sequence": "Author: Xiu Xu ; function Out = Estimation_loglik(y, alpha)\n[Beta, fval] = mvmqcaviar(y, alpha);\nOut = - log(fval);\n", "output_sequence": "do the adaptive estimation for localising multivariate CAViaR model, under two risk cases at quantile level 1%."}, {"input_sequence": "Author: Xiu Xu ; function q = SAVloop(BETA, y, empiricalQuantile)\nT = length(y);\nq = zeros(T,1); q(1) = empiricalQuantile;\nfor t = 2:T\nq(t) = BETA(1) + BETA(2) * abs(y(t-1)) + BETA(3) * q(t-1);\n", "output_sequence": "do the adaptive estimation for localising multivariate CAViaR model, under two risk cases at quantile level 1%."}, {"input_sequence": "Author: Xiu Xu ; function [forecast_q, q] = mqRQobjectiveFunction_forecast(Beta0, y, THETA, empiricalQuantile)\n[T,N]=size(y);\nq = zeros(T,N); q(1,:) = empiricalQuantile;\nA = reshape(Beta0, N, 1+2*N);\nc=A(:,1)'; a=A(:,2:N+1)'; b=A(:,N+2:1+2*N)';\nfor t = 2:T\nq(t,:) = c + abs(y(t-1,:))*a + q(t-1,:)*b;\nend\nforecast_q = c + abs(y(t-1,:))*a + q(T,:)*b;\n", "output_sequence": "do the adaptive estimation for localising multivariate CAViaR model, under two risk cases at quantile level 1%."}, {"input_sequence": "Author: Xiu Xu ; function [output_k, qt_hat, yt_k] = Adaptive_estimation_hat(y, alpha, z_cd)\n% 1. choose the maximium available interval based on the choosed criticals\nIntervals = load('Intervals');\nK = size(z_cd, 1) + 1;\nk = 1;\nLR = 1;\ncritical = 1000;\nL_T = 1000 * ones(size(z_cd, 1),\nwhile ( LR <= critical && k < K)\ntic;\nJ_k = (Intervals(k + 1, 4) : - 1 : Intervals(k + 1, 3))';\nL_A_k = arrayfun(@(x) Estimation_loglik(y(Intervals(k + 1, 5) : 1 : x, :), alpha), J_k);\nJ_k1 = J_k + 1;\nL_B_k = arrayfun(@(x) Estimation_loglik(y(x : 1 : Intervals(1, 10), :), alpha), J_k1);\nL_I_k = Estimation_loglik(y(Intervals(k + 1, 5) : 1 : Intervals(1, 10), :), alpha) * ones(length(J_k), 1);\nL_T_k0 = L_A_k + L_B_k - L_I_k;\nLR = max(L_T_k0);\nL_T(k,:) = LR;\ncritical = max(z_cd(k,:));\nk = k + 1;\ntoc;\nend\n% 2. select the adaptive interval\nLT_lag = L_T * ones(1, size(z_cd, 2)) - z_cd;\nif (max(LT_lag(:,1) >= 0)) > 0\nk_hat_c = find((LT_lag(:,1) >= 0), 1, 'first');\nelse\nk_hat_c = size(z_cd, 1) + 1;\nif (max(LT_lag(:,2) >= 0)) > 0\nk_hat_d = find((LT_lag(:,2) >= 0), 1, 'first');\nk_hat_d = size(z_cd, 1) + 1;\nk_hat = [k_hat_c, k_hat_d];\nn_k_hat_c = Intervals(k_hat_c, 2);\nn_k_hat = [n_k_hat_c,\nc_r = 0.90; d_r = 0.99; % Risk power\nr_k = [c_r, d_r];\n% 3. get the estimated theta\ny_k_esti_c = y(Intervals(k_hat_c, 3) : 1 : Intervals(1, 10), :);\n[Beta_c, fval_c, forecast_q_c, q_c] = mvmqcaviar_forecast(y_k_esti_c, alpha);\nBeta_hat = [Beta_c, Beta_d];\nfval_hat = [fval_c, fval_d];\nforecast_q_hat = [reshape(forecast_q_c, 2, 1), reshape(forecast_q_d, 2, 1)];\nq_hat = zeros(2*size(y, 1), size(y, 2));\nq_hat(1: 2*size(q_c, 1), = reshape(q_c, 2*size(q_c, 1),\n% 4. out-of-sample t+1 estimated quantile\nalphav =[alpha, alpha];\nL_T_k = [L_T, L_T];\noutput_k = [alphav; r_k; k_hat; Beta_hat; L_T_k; fval_hat; forecast_q_hat];\ny2 = [reshape(y, size(y, 1)*size(y, 2), 1) reshape(y, size(y, 1)*size(y, 2), 1)];\nqt_hat = [alphav; r_k; q_hat];\n", "output_sequence": "do the adaptive estimation for localising multivariate CAViaR model, under two risk cases at quantile level 1%."}, {"input_sequence": "Author: Xiu Xu ; function [Beta, fval, forecast_q, q] = mvmqcaviar_forecast(y, THETA)\n[num_y, N]=size(y);\noptions = optimset('Display','off', 'MaxFunEvals', 1000, 'TolFun', 1e-10, 'TolX', 1e-7);\nREP = 50; warning('off')\nWIN = min(100, num_y);\nfor i=1:N, ysort(:,i) = sortrows(y((1:WIN),i)); end\nfor i=1:N, empiricalQuantile(1,i) = ysort(max(1, round(WIN*THETA)),i); end\nfor i = 1:N, c(:,i) = CAViaR_estim(y(:,i), THETA); end\nw = c(1,:)'; a = diag(c(2,:)); b = diag(c(3,:));\nBeta0 = [w; a(:); b(:)];\nfor i =1:size(Beta0,2)\n[Beta(:,i),fval(1,i),exitflag(1,i)] = fminsearch('mqRQobjectiveFunction', Beta0(:,i), options, y, THETA, empiricalQuantile, 1); % it does not work with OUT=2\nfor ii = 1:REP\n[Beta(:,i),fval(1,i),exitflag(1,i)] = fminsearch('mqRQobjectiveFunction', Beta(:,i), options, y, THETA, empiricalQuantile, 1);\nif exitflag(1,i) == 1, break, end\nend\nend\nI = find(fval == min(fval));\nBeta = Beta(:,I); exitflag = exitflag(1,I); fval = fval(:,I);\n[forecast_q, q] = mqRQobjectiveFunction_forecast(Beta, y, THETA, empiricalQuantile);\n", "output_sequence": "do the adaptive estimation for localising multivariate CAViaR model, under two risk cases at quantile level 1%."}, {"input_sequence": "Author: Xiu Xu ; function BetaHat = CAViaR_estim(y, THETA)\nREP = 30;\nnInitialVectors = [1000, 3];\nnInitialCond = 1;\nMaxFunEvals = 500;\noptions = optimset('LargeScale', 'off', 'HessUpdate', 'dfp', 'MaxFunEvals', ...\nMaxFunEvals, 'display', 'off', 'MaxIter', 'TolFun', 1e-6, 'TolX', 1e-6);\nwarning('off', 'verbose')\nnum_y = size(y, 1);\nWIN = min(100, num_y);\nysort = sortrows(y(1:WIN), 1);\nempiricalQuantile = ysort(max(1, round(WIN*THETA)));\ninitialTargetVectors = unifrnd(0, 1, nInitialVectors);\nRQfval = zeros(nInitialVectors(1), 1);\nfor i = 1:nInitialVectors(1)\nRQfval(i) = RQobjectiveFunction(initialTargetVectors(i,:), 1, y, THETA, empiricalQuantile);\nend\nResults = [RQfval, initialTargetVectors];\nSortedResults = sortrows(Results,1);\nBestInitialCond = SortedResults(1:nInitialCond,2:4);\nBeta = zeros(size(BestInitialCond)); fval = Beta(:,1); exitflag = Beta(:,1);\nfor i = 1:size(BestInitialCond,1)\n[Beta(i,:), fval(i,1), exitflag(i,1)] = fminsearch('RQobjectiveFunction', BestInitialCond(i,:), ...\noptions, 1, y, THETA, empiricalQuantile);\nfor it = 1:REP\nif exitflag(i,1) == 1, break, end\n[Beta(i,:), fval(i,1), exitflag(i,1)] = fminsearch('RQobjectiveFunction', Beta(i,:), ...\noptions, 1, y, THETA, empiricalQuantile);\nend\nSortedFval = sortrows([fval, Beta, exitflag, BestInitialCond], 1);\nBetaHat = SortedFval(1, 2:4)';\n", "output_sequence": "do the adaptive estimation for localising multivariate CAViaR model, under two risk cases at quantile level 1%."}, {"input_sequence": "Author: Xiu Xu ; clear all; clc;\nload('Data.mat');\nn = length(y);\nfirst = 521;\nz_001 = load('z_k_001');\nz = z_001;\nfor t = first : 1 : n\ndisp(t)\ntic;\n[output_k, qt_hat, yt_k] = adaptive_estimation_hat(y(t - first + 1 : t, :), alpha, z);\nLoutput_001_c(:, t - first + 1) = [t; output_k(:, 1)];\ntoc;\nend\nsave('LVARVaR_001.mat');\nclear all; clc;\nload('LVARVaR_001.mat')\nfigure;\nsubplot(2, 1, 1); plot(Loutput_001_c(5, :), '-b', 'LineWidth', 1.5); title('DAX');\nylabel('Length'); ylim([30, 500]); % xlim([1, 2348]);\nYear = {'2007', '2015'}; hold on; set(gca, 'xtick', [1 523 1045 1566 2088]);\nLength = {'60', '120', '480'}; hold on;\nset(gca, 'ytick', [60 120 240 300 420 480]);\n% set(gca, 'xticklabel', Year);\nsubplot(2, 1, 2); plot(Loutput_001_d(5, :), '-b', 'LineWidth', 1.5);\nset(gca, 'xticklabel', Year); set(gca, 'yticklabel', Length);\nsaveas(gcf, 'adaptive_estimation_001_figure', 'fig');\n", "output_sequence": "do the adaptive estimation for localising multivariate CAViaR model, under two risk cases at quantile level 1%."}, {"input_sequence": "Author: Xiu Xu ; function output = RQobjectiveFunction(BETA, OUT, y, THETA, empiricalQuantile)\nq = SAVloop(BETA, y, empiricalQuantile);\nHit = (y < q) - THETA;\nRQ = -Hit'*(y - q);\nif OUT == 1\noutput = RQ;\nelseif OUT ==2\noutput = [q, Hit];\nelse error('Wrong output selected. Choose OUT = 1 for RQ, or OUT = 2 for [q, Hit].')\n", "output_sequence": "do the adaptive estimation for localising multivariate CAViaR model, under two risk cases at quantile level 1%."}, {"input_sequence": "Author: Xiu Xu ; function [Beta, fval, q, exitflag] = mvmqcaviar(y, THETA)\n[num_y, N]=size(y);\noptions = optimset('Display','off', 'MaxFunEvals', 1000, 'TolFun', 1e-10, 'TolX', 1e-7);\nREP = 50; warning('off')\nWIN = min(100, num_y);\nfor i=1:N, ysort(:,i) = sortrows(y((1:WIN),i)); end\nfor i=1:N, empiricalQuantile(1,i) = ysort(max(1, round(WIN*THETA)),i); end\nfor i = 1:N, c(:,i) = CAViaR_estim(y(:,i), THETA); end\nw = c(1,:)'; a = diag(c(2,:)); b = diag(c(3,:));\nBeta0 = [w; a(:); b(:)];\nfor i =1:size(Beta0,2)\n[Beta(:,i),fval(1,i),exitflag(1,i)] = fminsearch('mqRQobjectiveFunction', Beta0(:,i), options, y, THETA, empiricalQuantile, 1); % it does not work with OUT=2\nfor ii = 1:REP\n[Beta(:,i),fval(1,i),exitflag(1,i)] = fminsearch('mqRQobjectiveFunction', Beta(:,i), options, y, THETA, empiricalQuantile, 1);\nif exitflag(1,i) == 1, break, end\nend\nend\nI = find(fval == min(fval));\nBeta = Beta(:,I); exitflag = exitflag(1,I); fval = fval(:,I);\n", "output_sequence": "do the adaptive estimation for localising multivariate CAViaR model, under two risk cases at quantile level 1%."}, {"input_sequence": "Author: Elisabeth Bommes ; # Proper prior cleaning of text might improve results!\n# Source of textfile: http://www.nasdaq.com/aspx/stockmarketnewsstoryprint.aspx?storyid=3-disastrous-mistakes-mcdonalds-should-regret-cm406246\n# Please install packages if not installed yet!\nimport os\nimport types\nfrom textblob import TextBlob, Word\nfrom nltk.corpus import stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer as lemma\n### Functions\n# Function to convert Penn-Treebank POS tags to simplified (WordNet) POS tags\ndef posWN(posTB):\nif posTB.startswith('J'):\nreturn 'a'\nelif posTB.startswith('V'):\nreturn 'v'\nelif posTB.startswith('N'):\nreturn 'n'\nelif posTB.startswith('R'):\nreturn 'r'\nelif posTB.startswith('A'):\nreturn 's'\nelse:\nreturn ''\n# POS function\ndef pos(blob):\ntok = [token[0] for token in blob.pos_tags]\ntokW = [Word(token) for token in tok]\ntokn = len(tok)\nposTB = [pos[1] for pos in blob.pos_tags]\nposW = [posWN(TB) for TB in posTB]\nreturn posW\n# Tokenizer function\ndef token(blob):\nreturn tok\n# Lemmatizer function\ndef lem(article):\nfrom nltk.stem.wordnet import WordNetLemmatizer as lemma\ntokn = len(article.blob.words)\nif tokn == posn:\nwords = article.blob.words\nwords = [token[0] for token in article.blob.pos_tags]\ntokn = len(words)\n\nlems = []\nfor j in range(0, tokn):\nif article.pos[j] == '':\nverb = words[j].lemmatize('v')\nif len(verb) == len(noun):\nlems.append(words[j])\nelif len(verb) < len(noun):\nlems.append(verb)\nelse:\nlems.append(noun)\n\nlems.append(words[j].lemmatize(article.pos[j]))\nlems = [token.lower() for token in lems]\narticle.lem = lems\nreturn article\n# Counter and stopwords remover\ndef cntr(lems, stpw):\nfrom collections import Counter\ncnt = Counter(lems)\ncnt = [(cnt[entry], entry) for entry in cnt]\ncnt.sort()\ncnt.reverse()\nwordfreq = []\nfor i in range(0, len(cnt)):\nif cnt[i][1] not in stpw:\nwordfreq.append(cnt[i])\nreturn wordfreq\n# Set working directory\nos.chdir('')\n# Read text file\ntxt = ''\nwith io.open('mcd_long.txt', 'r', encoding = 'UTF-8', errors = 'ignore') as infile:\nfor line in infile:\ntxt = txt + line\ntxt = TextBlob(txt)\n### Basics\n# Print first sentence\nprint('First sentence: ' + str(txt.sentences[0]) + '\\n')\n# Print first word\nprint('First word: ' + str(txt.words[0]) + '\\n')\n# Print first part of speech tag\nprint('POS tagging: ' + str(txt.tags[0]) + '\\n')\n### Tokens and POS tags\ntxttok = token(txt)\n### WordNet lemmatizer\n# Create namespace\narticle = types.SimpleNamespace()\n# Assign parts of namespace\narticle.blob = txt\n# Lemmatize\narticle = lem(article)\n# Show part of results\nprint('Lemmatization')\nprint('Original word: ' + str(txttok[1]))\nprint('Lemma: ' + str(article.lem[1]) + '\\n')\n### Count words and remove stopwords\nstpw = stopwords.words('english')\nstpw.extend(\"'\")\ncnt = cntr(article.lem, stpw)\nprint('Most frequent lemmas:')\n# Show 10 most frequent lemmas\nfor ntry in range(0, 10):\nprint(cnt[ntry][1] + ': ' + str(cnt[ntry][0]))\n", "output_sequence": "Shows basic functionalities of natural language processing: sentence and word tokenization, part-of-speech tagging, lemmatization, stopword removal and word counting. POS-tags are converted to WordNet compatible tags such that the WordNet lemmatizer can be used. Please install required Python packages before usage: os, io, types, collections, textblob, nltk."}, {"input_sequence": "Author: Elisabeth Bommes ; ## Clear history\nrm(list = ls(all = TRUE))\ngraphics.off()\n## Install and load packages\nlibraries = c(\"leaflet\", \"htmlwidgets\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# Set working directory\nsetwd(\"\")\n# Read data\ndata.df = readRDS(\"TXTGermanBrewerieslonlat.RDS\")\n# Score\ndata.df$Avg = as.numeric(data.df$Avg)\ndata.df = data.df[data.df$Avg != 0,]\ndata.df$NoBeer = as.numeric(data.df$NoBeer)\ndata.df$NoBeer.size = data.df$NoBeer/max(data.df$NoBeer) * 20 + 1\n# Classes for color coding\ndata.df$Avg.class = 0\ndata.df$Avg.class[data.df$Avg <= 4] = 2\ndata.df$Avg.class = as.factor(data.df$Avg.class)\n# Color palette\npal = colorFactor(colorRampPalette( c(\"green\", \"red\"), space = \"rgb\")(4),\ndomain = c(\"1\", \"2\", \"4\"))\n# Plot\nm = leaflet(data = data.df) %>% setView(lng = 10.3833, lat = 50.5167, zoom = 6)\nm %>% addProviderTiles(\"OpenStreetMap.HOT\") %>%\naddCircleMarkers(lng = ~lon, lat = ~lat, radius = ~data.df$NoBeer.size,\npopup = ~paste(Brewery, \": \", NoBeer, \" beers\", Avg, \" as avg score\",sep = \"\"),\ncolor = ~pal(Avg.class),\nstroke = FALSE, fillOpacity = 0.8)\n", "output_sequence": "Plots locations and customer ratings of German breweries. Red indicates bad scores while green indicates good scores. The size of each bubble represents the number of beers that are produced by an individual brewery."}, {"input_sequence": "Author: Elisabeth Bommes ; # Clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# Set WD\npath.path = \"\"\nsetwd(path.path)\n# Install and load Packages\nlibraries = c(\"locfit\", \"KernSmooth\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# Temporarily save plotting options\nparmar = par(\"mar\")\nmfrow = par(\"mfrow\")\n# Functions\nconf.plot = function(x, y, lex, pn, ylim, xlim){\nbw = dpill(x, y)\nif(bw < 0.001) bw = 0.001;\n\nylab = paste(lex, \"Simulated Volatility\", sep = \" \")\nxlab = paste(lex, pn, \"Proportion, h =\", round(bw, 3), sep = \" \")\n\n# Remove some very extreme sentiment values on x axis (!) (e.g = 0.1 > 0.9999 quantile)\n# (otherwise no nonparam. regression possible)\nmin.x2 = 0\nmax.x2 = quantile(x, 0.9999)\ny = y[x < max.x2]\n# Compute confidence bands\nllr = locfit(y ~ lp(x, h = bw, deg = 1), kern = \"gauss\", deg = 1)\nbands = scb(y ~ lp(x, h = bw, deg = 1), type = 1, ev = lfgrid(100),\nsimul = TRUE, kern = \"gauss\", deg = 1)\n# Plot\nplot(NULL, pch = 20, cex = 0.5, col = \"#00000010\",\nylab = ylab, xlab = xlab, cex.axis = 0.9, tck = - 0.03, mgp = c(1.5, 0.3, 0),\ncex.lab = 1, ylim = ylim, xlim = xlim)\npoints(x, y, pch = '.', col = \"#00000010\")\nlines(llr, col = \"blue\", lwd = 2)\nlines(bands$x, bands$lower, col = \"red\", lwd = 2, lty = \"dashed\")\n}\n# Read data\nsim.df = readRDS(\"TXTSimulation.RDS\")\n##### Plot for global ad hoc inspection\n# Set global limits of y and x axis here to make plots comparable\nlimits.y = c(1.45, 1.65)\n# Set plotting parameters\npar(mar = c(3.0, 3.1, 1, 1.7))\npar(mfrow = c(2, 3))\n# Negative Proportion Plots\npn = \"Negative\"\n# BL Neg\nx = sim.df$BL_Neg_Proportion\ny = exp(sim.df$BL_SimulatedVolatility) # Note: we compare on exponential scale\nlex = \"BL\"\nconf.plot(x, y, lex, pn, limits.y,\n# LM Neg\nx = sim.df$LM_Neg_Proportion\ny = exp(sim.df$LM_SimulatedVolatility)\nlex = \"LM\"\n# MPQA Neg\nx = sim.df$MPQA_Neg_Proportion\ny = exp(sim.df$MPQA_SimulatedVolatility)\nlex = \"MPQA\"\n# Positive Proportion Plots\npn = \"Positive\"\nx = sim.df$BL_Pos_Proportion\ny = exp(sim.df$BL_SimulatedVolatility)\n# LM Pos\nx = sim.df$LM_Pos_Proportion\n# MPQA Pos\nx = sim.df$MPQA_Pos_Proportion\n# Return to previous plotting settings\npar(mar = parmar)\npar(mfrow = c(1, 1))\n", "output_sequence": "Plots simulation results of stock volatility based on the regression results of the entire panel. In the panel model, sentiment projections of three different sentiment lexica have been used. Furthermore, the mean curve and corresponding confidence bands are plotted to investigate the asymmetric reaction of volatility given positive and negative sentiment."}, {"input_sequence": "Author: Elisabeth Bommes ; import os\nfrom nltk.tokenize import word_tokenize\n# Set working directory\nos.chdir('')\n### Functions\ndef rempct(s):\npunct_numb = '\"#$%&\\()*+,./:;<=>?@[\\\\]^_`{|}~\u20ac$' + '0123456789' + \"'\"\nswopct = \"\"\nfor letter in s:\nif letter not in punct_numb:\nswopct += letter.lower()\nelse:\nswopct += '\nreturn swopct\ndef cleaner(txt):\ntxt = str(txt.encode('ascii', 'ignore'))\ntxt = txt.replace(\"\\\\n\",\" \")\nreturn(txt)\ndef wordcount(words, dct):\nfrom collections import Counter\ncounting = Counter(words)\ncount = []\nfor key, value in counting.items():\nif key in dct:\ncount.append([key, value])\n\nreturn count\ndef negwordcount(words, dct, negdct, lngram):\nfrom nltk.util import ngrams\nmid = int(lngram/2)\nng = ngrams(words, lngram)\nnglist = []\nfor grams in ng:\nnglist.append(grams)\nkeeper = []\nn = len(nglist)\ni = 1\nfor grams in nglist:\nif n - i < int(lngram / 2):\nmid = mid + 1\nif grams[mid] in dct:\nfor j in grams:\nif j in negdct:\nkeeper.append(grams[mid])\nbreak\ni = i + 1\ncount = wordcount(keeper, dct)\n# Read text file\ntxt = ''\nwith io.open('mcd_short.txt', 'r', encoding = 'UTF-8', errors = 'ignore') as infile:\nfor line in infile:\ntxt = txt + line\n# Additional input\nnegdct = [\"n't\", 'not', 'never', 'no', 'neither', 'nor', 'none']\nlngram = 7\n### Read in LM lexicon\n# Negative\nndct = ''\nwith io.open('bl_neg.csv', 'r', encoding = 'utf-8', errors = 'ignore') as infile:\nndct = ndct + line\nndct = ndct.split('\\n')\nndct = [rempct(entry) for entry in ndct]\nndct = [e for e in ndct if e]\n# Positive\npdct = ''\nwith io.open('bl_pos.csv', 'r', encoding = 'utf-8', errors = 'ignore') as infile:\npdct = pdct + line\npdct = pdct.split(\"\\n\")\npdct = [rempct(entry) for entry in pdct]\npdct = [e for e in pdct if e]\n### Count positive and negative words\ntxt = cleaner(txt)\n# Tokenize -> Words\ntxt = word_tokenize(txt)\n# Number of words in article\nnwords = len(txt)\n# Count words in lexicon\nnwc = wordcount(txt, ndct)\n# Count negated words in lexicon\nnwcneg = negwordcount(txt, ndct, negdct, lngram)\n# Total number of positive/negative words\nntot = 0\nfor i in range(0, len(nwc)):\nntot += nwc[i][1]\nptot = 0\nfor i in range(0, len(pwc)):\nptot += pwc[i][1]\n# Print results\nprint('Positive words:')\nprint(str(pwc[i][0]) + ': ' + str(pwc[i][1]))\nprint('Total number of positive words: ' + str(ptot))\nprint('\\n')\nprint('Negative words:')\nprint(str(nwc[i][0]) + ': ' + str(nwc[i][1]))\nprint('Total number of negative words: ' + str(ntot))\n", "output_sequence": "Counts positive and negative words using the lexicon by Liu et al. Please install required Python packages before usage: os, io, collections, nltk."}, {"input_sequence": "Author: Elisabeth Bommes ; # install and load packages\nlibraries = c(\"devtools\",\n\"showtext\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x, repos = \"http://cran.uni-muenster.de/\")\n})\n# install ebmisc\nlibrary(devtools)\nif (!(\"ebmisc\" %in% installed.packages())) {\ndevtools::install_github(\"ebommes/ebmisc\")\n}\nlapply(c(libraries, \"ebmisc\"), require, character.only = TRUE)\n# add font\nfont.add.google(\"Karla\", \"karla\")\nshowtext.auto()\n# initialize\nn = 5000\nsigma = 1\neta_1 = 1/1000\nconst = rep(mu, n)\n# generate random values\nset.seed(123)\nx = rnorm(n, mu, sigma)\nt = sample(1:n, n, replace = FALSE)\n# initialize theta vectors\ntheta_1 = c(x[t[1]], rep(0, length(x) - 1))\n# optimize theta as estimator for mean via SGD\nfor(i in 2:n){\neta_opt = 0.5 * (1 / i)\ntheta_opt[i] = theta_opt[i - 1] - (2 * eta_opt * (theta_opt[i - 1] - x[t[i]]))\n# plot\ndev.new(width = 5.5, height = 3)\npar(mar = c(3, 4.1, 1, 2.1))\nplot(theta_opt,\nlwd = 2,\ncol = \"black\",\nbty = 'l',\nxaxt = \"n\",\nxlim = c(0, n),\nylim = c(min(theta_1, theta_2, theta_opt),\nlines(const, lty = 1, lwd = 2, col = colorme(\"grey\"),\nxlim = c(1, n))\nlines(theta_1, lwd = 2, col = \"#1959BE\")\nlines(theta_3, lwd = 2, col = colorme(\"orange\"))\nbox(lwd = 2, bty = 'l', col = colorme(\"grey\"))\nmtext(expression(theta[i]),\nside = 2,\ncex = 1.2,\ncol = colorme(\"grey\"),\nlas = 1,\nat = max(theta_1, theta_2, theta_opt))\nmtext(\"i\",\nside = 1,\ncol = colorme(\"grey\"),\nfamily = \"karla\",\nlas = 1)\naxis(side = 1,\ntick = FALSE,\ncex.lab = 1.5,\nline = -0.8,\ncol.axis = colorme(\"grey\"),\nfamily = \"karla\",\nlas = 1)\naxis(side = 2,\n", "output_sequence": "Optimizes least squares loss function via stochastic gradient descent. The results of each iteration are plotted for different choices of eta. eta = 1/i (black), eta = 1/1000 (blue), eta = 1/1500 (green), eta = 1/2000 (orange), eta = 1/2500 (purple)"}, {"input_sequence": "Author: Franziska Schulz, Brenda L\u00f3pez Cabrera ; ## Biodiesel\nData = read.delim(\"Biodiesel.txt\")\nData$Date = as.Date(Data$Date)\n# Rapeseed\ndates = seq(as.Date(\"2013/01/31\"), by = \"-3 months\")\nfor (i in seq(4, length(dates), 4)) {\ndates[i] = dates[i] - 1\n}\nx = read.table(\"Rapeseed.txt\",\nskip = 1,\nstringsAsFactors = FALSE,\nheader = TRUE,\ncolClasses = rep(c(\"Date\", \"numeric\"), 31))\nmat = data.frame(date = seq(as.Date(\"2012/10/26\"), by = \"-1 week\"),\nF1 = NA,\nstringsAsFactors = FALSE)\ndiffmat = outer(dates, mat$date, \"-\")\ndiffmat[diffmat < 0] = Inf\nindex = apply(diffmat, 2, function(x) {\nwhich.min(x)\n})\nmat$T1 = dates[index]\nfor (j in 1:nrow(mat)) {\ntemp = x[, 2 * index[j] - 1] %in% mat$date[j]\nif (sum(temp) != 0) {\nmat$F1[j] = x[x[, 2 * index[j] - 1] %in% mat$date[j], 2 * index[j]]\n}\ntemp = x[, 2 * (index[j] - 1) - 1] %in% mat$date[j]\nmat$F2[j] = x[x[, 2 * (index[j] - 1) - 1] %in% mat$date[j], 2 * (index[j] - 1)]\nmat$spot = mat$F1/exp(as.numeric((mat$T1 - mat$date)/7) * log(mat$F2/mat$F1) * 1/(as.numeric((mat$T2 - mat$T1)/7)))\nmat = mat[nrow(mat):1, ]\n## Crude Oil\nspot.co = read.table(\"CrudeOil.txt\",\nsep = \"\\t\",\nstringsAsFactors = FALSE,\nheader = TRUE)\nspot.co = spot.co[nrow(spot.co):1, ]\nspot.co$Date = as.Date(spot.co$Date)\nData.new = data.frame(Date = spot.co$Date[1:467],\nCrude.Oil = spot.co$Price[1:467],\nRapeseed = mat$spot[30:496],\nBiodiesel = Data$Biodiesel[45:511])\nData = Data.new\nData$Biodiesel = na.spline(Data$Biodiesel, na.rm = TRUE) # interpolate missing values using cubic splines\nData$Rapeseed = Data$Rapeseed/0.6\nData$Crude.Oil = Data$Crude.Oil * 6.28981\n# logarithmic Prices\nlp = log(Data[, 2:4])\n## Rapeseed\nweek = 1:52\ntemp = as.matrix(lp$Rapeseed)\ntemp = rbind(temp, matrix(NaN, nrow = 1, ncol = 1))\nYN1 = matrix(data = temp, nrow = 52, byrow = FALSE)\nY1 = rowMeans(YN1, na.rm = TRUE)\nhx1 = median(abs(week - median(week)))/0.6745 * (4/(3 * 52))^0.2\nh1 = sqrt(hy1 * hx1)\nnp = npreg(Y1 ~ week, regtype = \"ll\", bws = h1, ckertype = \"gaussian\")\nseason1 = fitted(np)\nseason11 = rep(season1, 9)\nresid11 = (temp - season11)\nRapeseed = resid11[1:467]\nweek = 1:52\ntemp = as.matrix(lp$Crude.Oil)\ntemp = rbind(temp, matrix(NaN, nrow = 1, ncol = 1))\nYN1 = matrix(data = temp, nrow = 52, byrow = FALSE)\nY1 = rowMeans(YN1, na.rm = TRUE)\nhx1 = median(abs(week - median(week)))/0.6745 * (4/3/52)^0.2\nh1 = sqrt(hy1 * hx1)\nh1 = npregbw(Y1 ~ week, regtype = \"ll\", method = \"cv.ls\")\nnp = npreg(Y1 ~ week, regtype = \"ll\", bws = h1, ckertype = \"gaussian\")\nseason1 = fitted(np)\nseason11 = rep(season1, 9)\nresid11 = (temp - season11)[1:467]\nCrude.Oil = resid11\n## Biodiesel\nweek = 1:52\ntemp = as.matrix(lp$Biodiesel)\ntemp = rbind(temp, matrix(NaN, nrow = 1, ncol = 1))\nYN1 = matrix(data = temp, nrow = 52, byrow = FALSE)\nY1 = rowMeans(YN1, na.rm = TRUE)\nhx1 = median(abs(week - median(week)))/0.6745 * (4/3/52)^0.2\nh1 = sqrt(hy1 * hx1)\nh1 = npregbw(Y1 ~ week, regtype = \"ll\", method = \"cv.ls\")\nnp = npreg(Y1 ~ week, regtype = \"ll\", bws = h1, ckertype = \"gaussian\")\nBiodiesel = resid11\nData_deseas = data.frame(Crude.Oil, Rapeseed, Biodiesel)\nrownames(Data_deseas) = Data$Date\nlp = Data_deseas\nrownames(lp) = Data$Date\n", "output_sequence": "Shows a plot of the raw data and deseasonalized log prices of crude oil, biodiesel and rapeseed"}, {"input_sequence": "Author: Franziska Schulz, Brenda L\u00f3pez Cabrera ; # Clear memory\nrm(list = ls())\ngraphics.off()\n# Set working directory\n#setwd(\"...\")\n# install and load libraries\nlibraries = c(\"car\",\n\"abind\",\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries,require,quietly=TRUE,character.only=TRUE)\nsource(\"VolLinkagesData.R\")\n# Plot 1\npar(mfrow = c(1, 2))\nplot(Data$Date,\nData$Crude.Oil,\ncex.axis = 1.2,\ntype = \"l\",\nylab = \"Price\",\nylim = c(0, 1500))\nlines(Data$Date,\nData$Biodiesel,\ntype = \"l\",\nlwd = 3,\nData$Rapeseed,\nlwd = 4,\nlegend(\"topleft\",\nc(\"Crude Oil\", \"Biodiesel\", \"Rapeseed\"),\ny.intersp = 0.5,\ncex = 1.2,\nlty = c(1, 2, 3),\nlp$Crude.Oil,\nylab = \"Price\")\nlp$Biodiesel,\n", "output_sequence": "Shows a plot of the raw data and deseasonalized log prices of crude oil, biodiesel and rapeseed"}, {"input_sequence": "Author: Awdesch Melzer ; rm(list = ls(all = TRUE))\ngraphics.off()\n############################ SUBROUTINES ################################ Complex array generation\ncompl = function(re, im) {\nif (missing(re)) {\nstop(\"compl: no composed object for real part\")\n}\nif (missing(im)) {\nim = 0 * (re <= Inf)\nif (nrow(matrix(re)) != nrow(matrix(im))) {\nstop(\"compl: dim(re)<>dim(im)\")\nz = list()\nz$re = re\nreturn(z)\n}\nVaRcgfDG = function(t, par) {\n# cumulant generating function (cgf) for the class of quadratic forms of Gaussian vectors. Complex array generation\ncompl = function(re, im) {\nif (missing(re)) {\nstop(\"compl: no composed object for real part\")\n}\nif (missing(im)) {\nim = 0 * (re <= Inf)\nif (nrow(matrix(re)) != nrow(matrix(im))) {\nstop(\"compl: dim(re)<>dim(im)\")\nz = list()\nz$re = re\nreturn(z)\ns = compl(par$theta * t$re, par$theta * t$im)\ncmul = function(x, y) {\n# Complex multiplication\nre = x$re * y$re - x$im * y$im\ncdiv = function(x, y) {\n# Complex division\nw = y$re^2 + y$im^2\nre = (x$re * y$re + x$im * y$im)/w\ncln = function(x) {\n# Complex natural logarithm\nre = log(x$re^2 + x$im^2)/2\nim = atan2(x$im, x$re)\ncsub = function(x, y) {\n# Complex subtraction two arrays of complex numbers\nre = x$re - y$re\ni = 1\nm = length(par$lambda)\nwhile (i <= m) {\n# 1-lambda*t:\nomlt = compl(1 - par$lambda[i] * t$re, -par$lambda[i] * t$im)\ntmp = cmul(t, t)\ntmp = compl(par$delta[i]^2 * tmp$re, par$delta[i]^2 * tmp$im)\ntmp = csub(tmp, cln(omlt))\ns = compl(s$re + 0.5 * tmp$re, s$im + 0.5 * tmp$im)\ni = i + 1\nreturn(s)\ncexp = function(x) {\n# Complex exponential\nre = exp(x$re) * cos(x$im)\nVaRcharfDG = function(t, par) {\n# computes the characteristic function for the class of quadratic forms of Gaussian vectors.\nt = compl(-t$im, t$re) # 1i*t\nr = cexp(VaRcgfDG(t, par))\n############################ Main Program ############################\nXFGVaRcharfDGtest = function(par, n, xlim) {\n# Complex array generation\ndt = (xlim[2] - xlim[1])/(n - 1)\nt = xlim[1] + (0:(n - 1)) * dt\nr = VaRcharfDG(compl(t, t * 0), par)\nz1 = cbind(t, r$re)\nplot(z1, type = \"l\", col = \"blue3\", lwd = 2, ylab = \"Y\", xlab = \"X\", ylim = c(min(r$re, r$im), max(r$re, r$im)))\nlines(z2, col = \"red3\", lwd = 2)\ntitle(\"Characteristic function\")\ntheta = 0\ndelta = c(0)\nlambda = c(1.4142)\npar = list()\npar$theta = theta\npar$lambda = lambda\nXFGVaRcharfDGtest(par, 300, c(-40, 40))\n", "output_sequence": "Plots the real (blue line) and the imaginary part (red line) of the characteristic function for a distribution, which is close to a chi^2 distribution with one degree of freedom."}, {"input_sequence": "Author: Sabine Bertram ; #!/usr/bin/python\n# -*- coding: utf-8 -*-\n################################################################################\n# Learning of sentence embeddings\n# input: modelData: a list of sentences, split into words\n# output: sentenceModel: a gensim model containing the embeddings and additional info\nimport os\nimport pickle\nimport gensim, logging\n# import model training data\ncwd = os.getcwd()\ninputPath = os.path.normpath(os.path.join(cwd, \"..\", 'Regulatory_Complexity_Preprocessing'))\nwith open(os.path.join(inputPath, 'modelData'),'rb') as f:\ndata = pickle.load(f)\n# convert into labeled sentences\nlabeledData = []\nfor c, s in enumerate(data):\nlabel = \"_\".join(s)\nlabeledSentence = gensim.models.doc2vec.LabeledSentence(words=s, tags=[label])\nlabeledData.append(labeledSentence)\n#doc2vec training\n###################\n# screen output\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n# parameters\nmethod = 1 # use distributed memory model\nfeatures = 100 # sent vector dimensionality\ncount = 3 # Minimum word count\nthreads = 4 # Number of threads to run in parallel\ncontext = 5 # Context window size\ndownsampling = 1e-3 # Downsample setting for frequent words\nhierarch = 0 # no hierarchical sampling\nneg = 5 # negative sample\nepochs = 10 # number of epochs in NN\n# training\nmodel = gensim.models.doc2vec.Doc2Vec(labeledData, dm = method, size = features, window = context, min_count = count, workers = threads, sample = downsampling, hs = hierarch, negative = neg, iter = epochs)\nmodel.init_sims(replace=True)\nmodel.save('sentenceModel')\n", "output_sequence": "Computes sentence embeddings according to Le et al. (2014) doc2vec using gensim"}, {"input_sequence": "Author: Ostap Okhrin, Yafei Xu ; rm(list = ls(all = TRUE))\n# set the parameters and draw samples\nn = 100\nx = runif(min = -1, max = 1, n) # draw 100 random numbers from a uniform CDF\ny = rnorm(n, mean = 0, sd = 0.1) + x\nx = c(x, 1.2)\nb1 = var(x, y) / var(x)\nb0 = mean(y) - b1 * mean(x)\n# do the right scatter plot\nplot(x, y, type = \"p\", pch = 19, col = \"red3\")\nlines(x = c(-1, 1), y = c(b0 - b1, b0 + b1), lwd = 2, col = \"blue3\")\nabline(a = 0, b = 0, v = seq(-1, 1, by = 0.25), h = seq(0, 10, by = 2),\ncol = \"gray\", lty = 3)\n# compute correlations of Pearson, Kendall and Spearman\nprint(cor(x, y))\nprint(cor(x, y, method = \"kendall\"))\n", "output_sequence": "COPcorrelation2 gives a plot of 100 scatter points generated respectively by uniform distribution and normal distribution with a drift generated by the aforementioned uniform distribution. Also the three different correlations are computed, the correlations of Pearson, Kendall and Spearman."}, {"input_sequence": "Author: Ostap Okhrin, Yafei Xu ; # ------------------------------------------------------------------------------\n# Book: COP\n# ------------------------------------------------------------------------------\n# Quantlet: COPhac5varsmpscatter\n# Description: COPhac5varsmpscatter gives a plot of pairwise scatter plot of an\n# HAC sample with 5 dimensions and a Gumbel generator. The\n# parameter for X3, X4 equals 2.005 and for ((X3, X4, X5), X2,\n# X1) it is 1.005. According to these structure we construct an HAC\n# with a Gumbel generator. Then 1500 random numbers are drawn from\n# this HAC. Hence, for every variable we obtain 1500 numbers. Then\n# for each pair of two variables we can obtain a scatter plot.\n# Usage: -\n# Output: COPhac5varsmpscatter returns a plot of pairwise scatter plots\n# of an HAC sample with 5 dimensions and a Gumbel generator.\n# Example:\n# See also: -\n# Keywords: HAC, Hierarchical Archimedean Copula, simulation, Gumbel\n# Author: Ostap Okhrin, Yafei Xu\n# install.packages(\"HAC\") # run this line to install neccesary package\nrm(list = ls(all = TRUE))\n# setwd(\"C: / ...\") # please change your working directory\nlibrary(HAC)\nG2 = hac(type = 1, tree = list(list (\"X3\", 2.005),\n\"X2\", 1.005))\nsample = rHAC(n = 1500 , hac = G2)\n# do plot\n", "output_sequence": "COPhac5varsmpscatter gives a plot of pairwise scatter plot of an HAC sample with 5 dimensions and a Gumbel generator. The parameter for X3, X4, X4 equals 2.005 and for ((X3, X4, X5), X2, X1) it is 1.005. According to these structure we construct an HAC with a Gumbel generator. Then 1500 random numbers are drawn from this HAC. Hence, for every variable we obtain 1500 numbers. Then for each pair of two variables we can obtain a scatter plot."}, {"input_sequence": "Author: Ostap Okhrin, Yafei Xu ; rm(list = ls(all = TRUE))\n# replace the path of the working directory if necessary\n# setwd(\"C:/R\")\nfile.name = \"COPapp1prices.csv\" # read data\nX = read.csv(file.name, header = FALSE) # read data set\nrownames(X) = X[, 1]\nX = X[, -1]\nwhere.put = which(diff(as.numeric(format(as.Date(rownames(X), \"%d.%m.%Y\"),\n\"%Y%m\"))) != 0)\nlabels = format(as.Date(rownames(X), \"%d.%m.%Y\"), \"%b %Y\")\n# do plot\ndev.new(width = 8, height = 8)\nlayout(matrix(1:3, nrow = 3, byrow = T))\n# plot 1\npar(mai = (c(0.0, 0.55, 0.2, 0.1) + 0.3))\nplot(as.numeric(X[, 1]), type = \"l\", lwd = 3,\nylab = paste(\"AAPL \", \"(Price)\", sep = \"\"),\naxes = F, frame = T, cex.lab = 1.2)\naxis(2, cex.axis = 1.2)\n# plot 2\npar(mai = (c(0.1, 0.55, 0.1, + 0.3))\nplot(as.numeric(X[, 2]), type = \"l\", lwd = 3,\nylab = paste(\"HP \", \"(Price)\", sep = \"\"), axes = F,\nframe = T, cex.lab = 1.2)\n# plot 3\npar(mai = (c(0.3, 0.55, 0.0, 0.1) + c(0.1, 0.3, 0.3)))\nplot(as.numeric(X[, 3]), type = \"l\", lwd = 3,\nylab = paste(\"MSFT \", \"(Price)\", sep = \"\"),\ncex.axis = 1.2, cex.lab = 1.2, frame = T, axes = F)\n", "output_sequence": "COPhac5varsmpscatter gives a plot of pairwise scatter plot of an HAC sample with 5 dimensions and a Gumbel generator. The parameter for X3, X4, X4 equals 2.005 and for ((X3, X4, X5), X2, X1) it is 1.005. According to these structure we construct an HAC with a Gumbel generator. Then 1500 random numbers are drawn from this HAC. Hence, for every variable we obtain 1500 numbers. Then for each pair of two variables we can obtain a scatter plot."}, {"input_sequence": "Author: Ostap Okhrin, Yafei Xu ; # replace the path of the working directory if necessary\n# setwd(\"C:/R\")\nd = read.csv(\"COPdax140624.csv\") # pls download the pertinent data set.\nDateInput = as.Date(d[, 1])\nnumOfDate = as.numeric(as.Date(c(DateInput)))\nnewDF = data.frame(d, numOfDate)\nsortNewDF = newDF[order(newDF[, 3]),]\nnewDF2 = data.frame(sortNewDF, seq(1, length(newDF[, 3]),\nlength.out = length(newDF[, 3])))\nPt = newDF[, 2]\nP1 = Pt[ - length(Pt)]\nDAXreturn = log(P1/P2)\nDAXreturn = data.frame(DAXreturn,length(DAXreturn):1)\nDAXreturn = DAXreturn[order(DAXreturn[, 2]),]\nDAXreturn = DAXreturn[, 1] * 100\nr = DAXreturn[ - which(DAXreturn >= 6.5 | DAXreturn <= - 6.5)]\n# sample of Gaussian mu = 0.0002113130, sigmaSquare = 0.0002001865\nrNorm = 100 * rnorm(length(r), mean = 0.0002113130, sd = sqrt(0.0002001865))\n# do plot\nhist1 = hist(r, prob = TRUE, 12, freq = F, breaks = 24)\nplot(seq(-6.5, 6.5, length.out = 10), seq(0, .5, length.out = 10),\nylab = \"Density\", xlab = \"DAX Returns and Normal Simulations\", col = \"White\",\nmain = \"Histogram of DAX Returns and Normal Simulations\")\nplot(hist1, col = rgb(0, 0, 1, 1 / 4), xlim = c(-6, 6), freq = F, add = T)\nbox()\n", "output_sequence": "COPdaxnormhist gives histogram of DAX returns compared to the normal CDF. Here the normal simulations are based on the parameters that mean = 0.0002 and standard deviation = 0.0141. In the plot, the blue straps plus the purple straps stand for the DAX returns' histogram, and the pink straps plus the purple straps stand for the histogram of the normal simulations with the aforementioned parameters. And the purple straps stand for the over- lapped parts of the both histograms. Here the number of the points of the time series used in the computation is 5771, which is the same number as the number of the normal simulations"}, {"input_sequence": "Author: Alexander Schade, Fabian Karst, Zhasmina Gyozalyan ; Python 3.7.3 (v3.7.3:ef4ec6ed12, Mar 25 2019, 22:22:05) [MSC v.1916 64 bit (AMD64)] on win32\nType \"help\", \"copyright\", \"credits\" or \"license()\" for more information.\n>>> # -*- coding: utf-8 -*-\n\"\"\"\nThe following file will extract the SEC fillings from the EDGAR webpage for all the companies that have submitted a file\nto the SEC during a certain timeframe, 2002 to 2018 in our case. First, the full index links will be dowloaded, from recurring\n8-K, 10-Q or 10-K files among many other. Thereafter, only the direct link to 10-K files will be dowloaded as there\nare the one of interest.\nBased on the 10-K links, we will extract the full html files by withdrawing the tables and keeping only the full texts.\nWe clean the dataset by withdrawing stop words, certain key words as \"company\" or the years of interests or by withdrawing\ncertain html tags. Finally, we count the numbers of positive and negative words in each 10-K file based on two different libraries:\nthe one from Loughran and McDonald (2016) and the one from Bing Liu. Finally, a polarity score is extracted based on the\nnumber of positive and negative words from each 10-K file.\n\nATTENTION: The given code below is extremely computational intensive and therefore should not be exectued diretly without\nusing an appropriate machine or slicing the workload in different steps. We decided to exectute the code below year by year,\ni.e. by changing line 93 and 95 to be the same year, ony by one, and it took us approximately a week to run the 17 years\nwith a CPU of 4GB and Virtual Memory up to 150GB. Therefore, we highly discourage to exectue the code below for an entire\ntimeframe on only one machine and advice to cut the process by year to be able to run it on several machines\nto be efficient.\n#Import libraries needed\nimport os\nimport pandas as pd\nimport requests\nimport sqlite3\nfrom sqlalchemy import create_engine\nfrom bs4 import BeautifulSoup\nfrom urllib.request import urlopen\nimport re\nimport nltk\nnltk.download('all')\nimport pickle\nimport nltk.data\nfrom nltk.tokenize import RegexpTokenizer, sent_tokenize\n#set working directory\nPATH = r\"YOUR PATH HERE\"\nos.chdir(PATH)\n# function to get the text from the html links via BeautifulSoup\ndef url_to_text(url):\nresp = requests.get(url)\nsoup = BeautifulSoup(resp.text, 'html.parser')\nfor table in soup.find_all('table'):\ntable.decompose()\ntext = soup.get_text()\nreturn text\n#Function to clean the dataset\ndef clean_text_round2(text):\n# convert to lower case\ntext = text.lower()\ntext = re.sub(r'(\\t|\\v)', '', text)\n# remove \\xa0 which is non-breaking space from ISO 8859-1\ntext = re.sub(r'\\xa0', ' ', text)\n# remove newline feeds (\\n) following hyphens\ntext = re.sub(r'(-+)\\n{2,}', r'\\1', text)\n# remove hyphens preceded and followed by a blank space\ntext = re.sub(r'\\s-\\s', '', text)\n# replace 'and/or' with 'and or'\ntext = re.sub(r'and/or', r'and or', text)\n# tow or more hypens, periods, or equal signs, possiblly followed by spaces are removed\ntext = re.sub(r'[-|\\.|=]{2,}\\s*', r'', text)\n# all underscores are removed\ntext = re.sub(r'_', '', text)\n# 3 or more spaces are replaced by a single space\ntext = re.sub(r'\\s{3,}', ' ', text)\n# three or more line feeds, possibly separated by spaces are replaced by two line feeds\ntext = re.sub(r'(\\n\\s*){3,}', '\\n\\n', text)\n# remove hyphens before a line feed\ntext = re.sub(r'-+\\n', '\\n', text)\n# replace hyphens preceding a capitalized letter with a space\ntext = re.sub(r'-+([A-Z].*)', r' \\1', text)\n# remove capitalized or all capitals for the months\ntext = re.sub(r'(January|February|March|April|May|June|July|August|September|October|November|December|JANUARY|FEBRUARY|MARCH|APRIL|MAY|JUNE|JULY|AUGUST|SEPTEMBER|OCTOBER|NOVEMBER|DECEMBER)', '', text)\n# remove years\ntext = re.sub(r'2000|2001|2002|2003|2004|2005|2006|2007|2008|2009|2010|2011|2012|2013|2014|2015|2016|2017|2018|2019', '', text)\n# remove words million and company\ntext = re.sub(r'million|company', '', text)\n# remove line feeds\ntext = re.sub('\\n', ' ', text)\n#replace single line feed \\n with single space\ntext = re.sub(r'\\n', ' ', text)\n#Initiate the years\nstart_year = 2002\nquarter = 4\nend_year = 2018\nyears = list(range(start_year, end_year))\nquarters = ['QTR1',\nhistory = [(y, q) for y in years for q in quarters]\n#Look into the time frame wanted\nfor i in range(1, quarter + 1):\nhistory.append((end_year, 'QTR%d' % i))\nurls = ['https://www.sec.gov/Archives/edgar/full-index/%d/%s/crawler.idx' % (x[0], x[1]) for x in history]\nurls.sort()\n#Pass via sqlite3 as is more efficient\ncon = sqlite3.connect('edgar_htm_idx.db')\ncur = con.cursor()\ncur.execute('DROP TABLE IF EXISTS idx')\ncur.execute('CREATE TABLE idx (conm TEXT, type TEXT, cik TEXT, date TEXT, path TEXT)')\n#Go get the required data from the EDGAR index file\nfor url in urls:\nlines = requests.get(url).text.splitlines()\ncikloc = lines[7].find('CIK')\nnameloc = lines[7].find('Company Name')\nrecords = [tuple([line[:typeloc].strip(), line[typeloc:cikloc].strip(),\nline[dateloc:urlloc].strip(), for line in lines[9:]]\ncur.executemany('INSERT INTO idx VALUES (?, ?)', records)\nprint(url, 'downloaded and wrote to SQLite')\n\ncon.commit()\n#After having passed by SQL, save it to a csv\nengine2 = create_engine('sqlite:///edgar_htm_idx.db')\nwith engine2.connect() as conn, conn.begin():\ndata_csv= pd.read_sql_table('idx', conn)\n\n# Take only the 10-K files we are interested in\nfiles = ['10-K']\ndata_10K = data_csv[data_csv['type'].isin(files)]\n#rearange the dataset\ndata_10K = data_10K[['cik','conm','type','date','path']]\n#save in csv\ndata_10K.to_csv('edgar_htm_idx.csv')\n#Transform data into a list\nfirms = data_10K['conm']\nfirms =firms.values.tolist()\n#Create a dataset only with the paths\npaths = data_10K.path\n#Initialise the loop to extract the links\nfinal_link_appended=[]\nn=1\nfor path in paths:\nfirst_link='None'\nurl = path\n#Inspect element to find how the 10K files are stored - in <div> and then find type and the link right before that\n#this contains the actual 10 K file\nhtml = urlopen(url)\nsoup = BeautifulSoup(html, 'lxml')\na_tags = soup.find_all('div')\nlink1=path.split('/')\nlink3=\"https://\"+link1[1]+link1[2]+'/'\nfirst_link = soup.find_all(string='10-K')\nfor i in first_link:\nif i=='10-K':\nend_link=i.find_previous('a')\nend1=str(end_link)\npattern = re.compile(r'Archives.+\\w.+htm\"')\nmatch = pattern.search(end1)\n\nif match:\nformatted_link=format(match.group())\nformatted_link[:-1]\nfinal_link=link3+formatted_link[:-1]\nprint(final_link)\nn=n+1\nfinal_link_appended.append(final_link)\n\nurls=final_link_appended\n#Save all the links to the 10-K files into a csv file\nsaved_url = pd.DataFrame(urls)\nsaved_url.to_csv('urls.csv')\n#Get the texts files from the urls\nurls = final_link_appended\ntexts = [url_to_text(url) for url in urls]\n#save them into a csv file if needed\nsaved_texts = pd.DataFrame(texts)\nsaved_texts.to_csv('texts.csv')\nstopWordsFile = PATH + r'\\Dictionaries\\StopWords_Generic.txt'\n#Loading stop words dictionary for removing stop words\nwith open(stopWordsFile ,'r') as stop_words:\nstopWords = stop_words.read().lower()\nstopWordList = stopWords.split('\\n')\nstopWordList[-1:] = []\n#Tokenizeing module and filtering tokens using stop words list, removing punctuations\ndef tokenizer(text):\ntokenizer = RegexpTokenizer(r'\\w+')\ntokens = tokenizer.tokenize(text)\nfiltered_words = list(filter(lambda token: not in stopWordList, tokens))\nreturn filtered_words\n#Based on the dictionary of Loughran and McDonald (2016)\n# Calculating positive score\ndef positive_word_LM(text):\nnumPosWords = 0\nrawToken = tokenizer(text)\nfor word in rawToken:\nif word in pos_dict_LM:\nnumPosWords += 1\nsumPos = numPosWords\nreturn sumPos\n# Calculating Negative score\ndef negative_word_LM(text):\nnumNegWords=0\nif word in neg_dict_LM:\nnumNegWords -=1\nsumNeg = sumNeg * -1\nreturn sumNeg\n#Based on the dictionary of Bing Liu\ndef positive_word_B(text):\nif word in pos_dict_B:\ndef negative_word_B(text):\nif word in neg_dict_B:\n# Calculating polarity score\ndef polarity_score(positiveScore, negativeScore):\npol_score = (positiveScore - negativeScore) / ((positiveScore + negativeScore) + 0.000001)\nreturn pol_score\n# Function to count the words\ndef total_word_count(text):\ntokens = tokenizer(text)\nreturn len(tokens)\n# Calculating Average sentence length\n# It will calculated using formula --- Average Sentence Length = the number of words / the number of sentences\n\ndef average_sentence_length(text):\nsentence_list = sent_tokenize(text)\ntotalWordCount = len(tokens)\ntotalSentences = len(sentence_list)\naverage_sent = 0\nif totalSentences != 0:\naverage_sent = totalWordCount / totalSentences\naverage_sent_length= average_sent\nreturn round(average_sent_length)\n#Import the negative and positive dictionaries\n##### Words from Loughran and McDonald\n# negative\nneg_dict_LM = \"\"\nneg_dict_LM = pd.read_csv(PATH + r\"\\Dictionaries\\lm_negative.csv\",encoding = 'ISO-8859-1', names=[\"lm_negative\"])[\"lm_negative\"].values.tolist()\nneg_dict_LM = str(neg_dict_LM)\n# positive\npos_dict_LM = \"\"\npos_dict_LM = pd.read_csv(PATH + r\"\\Dictionaries\\lm_positive.csv\", encoding = 'ISO-8859-1', names=[\"lm_positive\"])[\"lm_positive\"].values.tolist()\npos_dict_LM = str(pos_dict_LM)\n##### Words from Bing Liu\n# negative\nneg_dict_B = \"\"\nneg_dict_B = pd.read_csv(PATH + r\"\\Dictionaries\\bl_negative.csv\", encoding = 'ISO-8859-1', names=[\"bl_negative\"])[\"bl_negative\"].values.tolist()\nneg_dict_B = str(neg_dict_B)\npos_dict_B = \"\"\npos_dict_B = pd.read_csv(PATH + r\"\\Dictionaries\\bl_positive.csv\",encoding = 'ISO-8859-1', names=[\"bl_positive\"])[\"bl_positive\"].values.tolist()\npos_dict_B = str(pos_dict_B)\n# If the process of collection and textual analysis were different, here dowload the urls\n#urls = pd.read_csv(PATH + r\"\\urls.csv\", index_col = 0, names=[\"url\"])[\"url\"].values.tolist()[1:]\n# Retrive all the cik numbers from the urls\nciks = []\nfor url in urls:\ncik = []\npath = url\nlink1 = path.split('/')\ncik = link1[6]\nciks.append(cik)\nprint(\"ciks fetched\")\n# Pickle the data to save them on the computer\nfor idx, val in enumerate(ciks):\nwith open(PATH + r\"\\pickle\" + val+\"val\" + \".pkl\", \"wb\") as f:\nif idx != len(ciks)-1:\npickle.dump(texts[idx], f)\nelse:\npickle.dump(texts[idx-1], f)\n#Open the pickle files previously saved on the computer\ndata = {}\nfor _, val in enumerate(ciks):\nwith open(PATH + r\"\\pickle\" + val +\"val\"+ \".pkl\", \"rb\") as f:\ndata[val] = pickle.load(f)\n\n# Change to a pd.DataFrame\ndf = pd.DataFrame.from_dict(data, orient = \"index\")\ndf1 = pd.DataFrame(df[0].apply(clean_text_round2))\n# Create the variables for the textual analysis\ndf1['word_count'] = df1.iloc[:,0].apply(total_word_count)\ndf1['polarity_LM'] = np.vectorize(polarity_score)(df1['positive_LM'],df1['negative_LM'])\ndf1['positive_B'] = df1.iloc[:,0].apply(positive_word_B)\ndf1['polarity_B'] = np.vectorize(polarity_score)(df1['positive_B'],df1['negative_B'])\ndf1['average_sentence_lenght'] = df1.iloc[:,0].apply(average_sentence_length)\ndf1.to_csv(PATH + r\"\\results\\textual_analyis.csv\")\nprint(\"done\")\n", "output_sequence": "Collect textual data from companies annual reports, perform sentiment analysis, report polarity scores."}, {"input_sequence": "Author: Alexander Schade, Fabian Karst, Zhasmina Gyozalyan ; # -*- coding: utf-8 -*-\n\"\"\"\nCreated on Thu Nov 14 06:44:13 2019\n@author: Fabian Karst\nInput: all fillings.csvs (Collecting_Preprocessing_Analyzing_10K_files.py)\nbl_data_processed.csv, (Preprocessing_WIDS.py)\nOutput: filings_data_processed.csv (csv containing the filings with the same structure as all other data.\nPurpose: Transform the filings data into a format which can be used for further processing\nimport os\ncsv.field_size_limit(100000000)\nimport pandas as pd\nimport numpy as np\nimport glob\n# Set working directory\nos.chdir('D:\\Programming\\Python\\SmartDataAnalytics\\Project\\Preprocessing_filings')\ndf = [pd.read_csv(\"preprocesseddata//ratio//bl_data_processed.csv\", low_memory=False, index_col = 0), pd.read_csv(\"preprocesseddata//ratio//cf_data_processed.csv\", low_memory=False, index_col = 0), pd.read_csv(\"preprocesseddata//ratio//is_data_processed.csv\", low_memory=False, index_col = 0), pd.read_csv(\"preprocesseddata//ratio//lbl_data_processed.csv\", low_memory=False, index_col = 0)]\ndf.append(df[0][[\"fyear\", \"cik\", \"state\", \"gsector\"]])\ndf[4][\"word_count\"] = (np.nan * len(df[4]))\ndf[4][\"average_sentence_lenght\"] = (np.nan * len(df[4]))\nstats = pd.DataFrame([], columns=[\"year\", \"total\", \"found\", \"word_count\", \"positive_LM\", \"negative_B\", \"polarity_B\", \"average_sentence_lenght\"])\nfor file in glob.glob(\"filings/*.csv\"):\nyear = int(file[-8:-4])\n\nwith open(r\"filings\\textual_analyis_2002.csv\", encoding=\"utf-8\") as file:\ncsv_data = csv.reader(file, delimiter=',')\nnext(csv_data)\nfor row in csv_data:\nif not df[4][(df[4]['cik'] == float(row[0])) & (df[4]['fyear'] == year)].empty:\ndf[4].loc[(df[4]['cik'] == float(row[0])) & (df[4]['fyear'] == year), \"average_sentence_lenght\"] = row[9]\nif sum((df[4]['cik'] == row[0]) & (df[4]['fyear'] == year)) > 1:\nprint(\"Error, multiple matches for cik and year\")\nprint(df[4][(df[4]['cik'] == row[0]) & (df[4]['fyear'] == year)])\nprint(\"Result: {} companies of {} found.\".format(sum((df[4]['word_count'].notnull()) & (df[4]['fyear'] == year)), sum(df[4]['fyear'] == year)))\nword_count = np.asarray(df[4].loc[(df[4][\"fyear\"] == year) & (df[4][\"word_count\"].notnull()), \"word_count\"], dtype=np.float).mean()\nstats.append(pd.DataFrame([[year, sum((df[4]['word_count'].notnull()) & (df[4]['fyear'] == year)), sum(df[4]['fyear'] == year), word_count, positive_LM, polarity_LM, positive_B, polarity_B, average_sentence_lenght]], columns=[\"year\", \"total\", \"found\", \"word_count\", \"positive_LM\", \"negative_B\", \"polarity_B\", \"average_sentence_lenght\"]))\ndf[4].loc[(df[4][\"word_count\"].isnull()) & (df[4]['fyear'] == year), \"word_count\"] = word_count\ndf[4].loc[(df[4][\"average_sentence_lenght\"].isnull()) & (df[4]['fyear'] == year), \"average_sentence_lenght\"] = average_sentence_lenght\nstats.to_csv(\"statistic_average_filings_by_year.csv\")\ndf[4].to_csv(\"preprocesseddata//ratio//filings_data_processed.csv\")\n", "output_sequence": "Collect textual data from companies annual reports, perform sentiment analysis, report polarity scores."}, {"input_sequence": "Author: Julian Woessner ; #!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Tue Oct 29 12:54:51 2019\nThis file contains all code to parse the closing prices of the SP500\nconstituents from yahoo.finance\nCode found on\nhttps://pythonprogramming.net/sp500-company-price-data-python-programming-for-finance\n/?completed=/sp500-company-list-python-programming-for-finance/\nCode adapted to be used for required data download.\n@author: julianwossner\n@date: 20191117\n# In[1]:\n# Import packages\nimport bs4 as bs\nimport os\nimport pandas_datareader as data\nimport pickle\nimport requests\n# In[2]:\n# Define Function to parse tickers of SP500 companies\ndef save_sp500_tickers():\nresp = requests.get('http://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\nsoup = bs.BeautifulSoup(resp.text, 'lxml')\ntable = soup.find('table', {'class': 'wikitable sortable'})\ntickers = []\nfor row in table.findAll('tr')[1:]:\nticker = row.findAll('td')[0].text\ntickers.append(ticker)\n\nwith open(\"sp500tickers.pickle\",\"wb\") as f:\npickle.dump(tickers,f)\nreturn tickers\n# In[3]:\n# Parse tickers and timeseries from 2015 to 2019, then save as csv\n\ntickers = save_sp500_tickers()\ntickers = [s.replace(\"\\n\",\"\") for s in tickers]\n# Define start and data\nstart_date = '2015-01-01'\nSP500_data = data.DataReader(tickers, 'yahoo', start_date, end_date)\nSP500_close = SP500_data[\"Close\"]\n\n# Save as .csv file\nSP500_close.to_csv(\"SP500_price_data_15.csv\")\n# In[4]:\nstart_date = '2000-01-01'\nSP500_close = SP500_data[\"Close\"]\nSP500_close.to_csv(\"SP500_price_data_00.csv\")\n# In[ ]\n", "output_sequence": "Web scrape data for Crypto Assets and SP500 constituents from the internet. The tickers are scraped from wikipedia and timeseries data is downloaded."}, {"input_sequence": "Author: Julian Woessner ; #!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Wed Oct 30 14:17:33 2019\nThis file is to download crypto currency data with the bitfinex API.\nFound on https://blog.patricktriest.com/analyzing-cryptocurrencies-python/,\nCode slightly adapted in order to download the wanted crypto prices\n@author: julianwossner\n@date: 20191117\n# In[1]:\n# Import required packages\nimport os\nimport numpy as np\nimport pandas as pd\nimport pickle\nimport quandl\nfrom datetime import datetime\nimport time\nfrom pandas import DataFrame\nimport plotly.offline as py\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\npy.init_notebook_mode(connected = True)\n# In[2]:\n# Define Quandl Helper Function\ndef get_quandl_data(quandl_id):\n'''Download and cache Quandl dataseries'''\ncache_path = '{}.pkl'.format(quandl_id).replace('/','-')\ntry:\nf = open(cache_path, 'rb')\ndf = pickle.load(f)\nprint('Loaded {} from cache'.format(quandl_id))\nexcept (OSError, as e:\nprint('Downloading {} from Quandl'.format(quandl_id))\ndf = quandl.get(quandl_id, returns=\"pandas\")\ndf.to_pickle(cache_path)\nprint('Cached {} at {}'.format(quandl_id, cache_path))\nreturn df\n# Pull Kraken BTC price exchange data\nbtc_usd_datasets = get_quandl_data('BCHARTS/KRAKENUSD')\nbtc_usd_datasets = btc_usd_datasets[\"Close\"]\n# In[3]:\n# Define Function to merge columns into dataframe\ndef merge_dfs_on_column(dataframes, labels, col):\n'''Merge a single column of each dataframe into a new combined dataframe'''\nseries_dict = {}\nfor index in range(len(dataframes)):\nseries_dict[labels[index]] = dataframes[index][col]\n\nreturn pd.DataFrame(series_dict)\n# In[4]:\n# Define json data Helper Function\ndef get_json_data(json_url, cache_path):\n'''Download and cache JSON data, return as a dataframe.'''\ntry:\nprint('Loaded {} from cache'.format(json_url))\ndf = pd.read_json(json_url)\nprint('Cached {} at {}'.format(json_url, cache_path))\n# In[5]:\n# Parse crypto data and save to csv\n\nbase_polo_url = 'https://poloniex.com/public?command=returnChartData&currencyPair={}&start={}&end={}&period={}'\nstart_date = time.mktime(datetime(2015,1,1).timetuple()) # get data from the start of 2015 in seconds\nend_date = time.mktime(datetime(2019,10,30).timetuple()) # up until today\nperiod = 86400 # pull daily data (86,400 seconds per day)\ndef get_crypto_data(poloniex_pair):\n'''Retrieve cryptocurrency data from poloniex'''\njson_url = base_polo_url.format(poloniex_pair, start_date, end_date, period)\ndata_df = get_json_data(json_url, poloniex_pair)\ndata_df = data_df.set_index('date')\nreturn data_df\naltcoins = ['ETH','LTC','XRP','ETC','STR','DASH','SC','XMR','XEM']\naltcoin_data = {}\nfor altcoin in altcoins:\ncoinpair = 'BTC_{}'.format(altcoin)\ncrypto_price_df = get_crypto_data(coinpair)\naltcoin_data[altcoin] = crypto_price_df\n# Calculate USD Price as a new column in each altcoin dataframe\nfor altcoin in altcoin_data.keys():\naltcoin_data[altcoin]['price_usd'] = altcoin_data[altcoin]['close'] * btc_usd_datasets\n#Merge USD price of each altcoin into single dataframe\ncombined_df = merge_dfs_on_column(list(altcoin_data.values()), list(altcoin_data.keys()), 'price_usd')\n# Add BTC price to the dataframe\ncombined_df['BTC'] = btc_usd_datasets\n# Save as .csv file\ncombined_df.to_csv(\"crypto_prices.csv\")\n# In[ ]\n", "output_sequence": "Web scrape data for Crypto Assets and SP500 constituents from the internet. The tickers are scraped from wikipedia and timeseries data is downloaded."}, {"input_sequence": "Author: Oliver Kostorz ; \"\"\"\nThis file scrapes the news website finanzen.net and gathers specified articles,\nanalysis the words used and links them to the stocks return and volatility, following the publication of the article.\nStock data is retrieved through the Yahoo Finance API.\nCode was written by Oliver Kostorz during November 2019.\n\"\"\"\n#Import packages\nfrom bs4 import BeautifulSoup\nimport requests\nimport yfinance\nfrom stop_words import get_stop_words\nimport re\nimport datetime\nfrom fuzzywuzzy import process\nimport datetime as dt\nimport pandas\nimport numpy\nimport pickle\nimport os\n#Set working device\ntry:\nwd = os.path.join(os.getcwd(), 'SDA-Oliver-Kostorz-SMART-Sentiment-Analysis-master')\nexcept:\nprint('Please specify path to working directory manually.')\n\n###Functions\n#Clean html tags in string\ndef remove_html_tags(text):\nclean = re.compile('<.*?>')\nreturn re.sub(clean, '', text)\n#Get stock symbol\ndef getCompany(text):\nr = requests.get('https://api.iextrading.com/1.0/ref-data/symbols')\nstockList = r.json()\nreturn process.extractOne(text, stockList)[0]\n#Rounds time downwards to earlier five minute interval\ndef roundtime(time):\nwhile 1<2:\nif float(time.minute/5).is_integer() is True:\nbreak\ntime += dt.timedelta(minutes = -1)\nreturn time\n###Data mining\n#List containing relevant URLs for news-sample\nwith open(os.path.join(wd, 'links.txt'), \"rb\") as fp: # Unpickling\nurl_list = pickle.load(fp)\n#Add additional links to news articles to use for training the algorithm\n#Refer to Meta-Information for full guide and requirements\nurl_list.append()\npass\nwith open(os.path.join(wd, 'links.txt'), \"wb\") as fp: #Pickling\npickle.dump(url_list, fp)\n#Defines dataframe to collect relevant info\nvoi = {'word':[], 'frequency':[],\n'return(24h)':[], 'volatility(24h)':[]}\ndata = pandas.DataFrame(voi)\n#Define stopwords (not necessary but implemented for schooling purposes)\nstop_words = get_stop_words('german')\nstop_words.append('dpaafx')\n#Loop through all articles to extract relevant words and allocate SMART sentiment weights\nfault_counter = 0\nfor link in url_list:\nwebsite = requests.get(link)\nnews = website.content\nsoup = BeautifulSoup(news, 'lxml')\n###Data processing\n#Finanzen.net specific code to extract news' body\ncontainer = soup.find('div', class_='teaser teaser-xs color-news')\nparts_html = list()\nfor para in container.find_all('p', recursive=False):\nparts_html.append(para)\n\n#General text preparation code\nparts = remove_html_tags(str(parts_html))\nexpression = \"[^a-zA-Z \u00e4\u00fc\u00f6\u00df]\"\ntext_raw = re.sub(expression, '', str(parts))\ntext_raw_lower = text_raw.lower()\nword_tokens = text_raw_lower.split()\n#Deleting stopwords from text is not necessary with SMART method but included for educational purpose\nfiltered_sentence = list()\nfor w in word_tokens:\nif w not in stop_words:\nfiltered_sentence.append(w)\n\n#Finanzen.net specific code to extract stock's name\nname_section = soup.find('div', class_='chart-block relative')\nname_parts_html = list()\nfor para in name_section.find_all('a', recursive=False):\nname_parts_html.append(para)\nname_parts = remove_html_tags(str(name_parts_html))\nname = name_parts[1:len(name_parts)-18]\n#Finanzen.net specific code to extract time of news\ndate_section = soup.find(class_=\"pull-left mright-20\")\ndate = str(date_section)[33:49]\ndate_time = datetime.datetime.strptime(date, '%d.%m.%Y %H:%M')\n#Gathers stock's return history\ntry: #keeps code running if news date is faulty\ndate = date_time.strftime('%Y-%m-%d')\nrounded_time = roundtime(date_time)\nt_end = (date_time + datetime.timedelta(days=3)).strftime('%Y-%m-%d')\nsymbol = getCompany(name).get('symbol')\nstock_data = yfinance.Ticker(symbol)\nreturn_t = stock_data.history(start = t_start, end = t_end, interval = \"5m\")\n#Calculate return and volatility\nreturn_24 = 100*((((return_t.loc[(rounded_time).strftime('%Y-%m-%d %H:%M:%S') : t_end])[\"Open\"].iloc[77])-\n((return_t.loc[t_start : (rounded_time).strftime('%Y-%m-%d %H:%M:%S')])[\"Open\"].iloc[-1]))/((return_t.loc[t_start : (rounded_time).strftime('%Y-%m-%d %H:%M:%S')])[\"Open\"].iloc[-1]))\nvar_till_t = (numpy.var((return_t.loc[t_start : (rounded_time).strftime('%Y-%m-%d %H:%M:%S')])[\"Open\"])) # variance until news\nvolatility_24 = math.sqrt(var_after_t)/math.sqrt(var_till_t)\n#Save in dict\nfor word in filtered_sentence:\nif word in data.values:\ndata.set_value(data['word'] == word, 'return(24h)', #addressed cell\nround((((data.get_value(data.loc[data['word']==word].index[0], 2, takeable = True)*\ndata.set_value(data['word'] == word, 'volatility(24h)',\nround((((data.get_value(data.loc[data['word']==word].index[0], 3, takeable = True)*\ndata.set_value(data['word'] == word, 'frequency',\ndata.get_value(data.loc[data['word']==word].index[0], 1, takeable = True)+1)\nelse:\ndata = data.append({'word':word, 'frequency':1,\n'return(24h)':return_24, 'volatility(24h)':volatility_24, }, ignore_index=True)\nexcept:\nfault_counter = fault_counter + 1\ndata = data.sort_values(by = 'word', ascending = True)\nsuccess_counter = len(url_list)-fault_counter\nprint('Hello ' + os.getlogin() + ',')\nprint('within the last minutes, ' + str(success_counter) + ' news articles could be assessed for training of the algorithm.')\nprint('Unfortunately, ' + str(fault_counter) + ' articles did not fulfill the requirements for assessment and were not included in the calculation.')\nprint('Among the most common reasons for exclusion are:')\nprint('-Publication was too recent to gather enough return data')\nprint('-Article was published on weekends and might be outdated by the time the stock exchange opens again')\nprint('-Article was not published on Finanzen.net directly but rather refers to another website')\nprint('-Publication date is outside requestable return range (60 days)')\nprint('Please refer to the Meta-Information for further explanations.')\nprint('However, we will continue with the valid data.')\ndata.to_csv(os.path.join(wd, 'data.csv'), index = False, header = True)\n", "output_sequence": "This Quantlet scrapes the news website Finanzen.net for pre-specified articles that are linked to a specific stock. Afterwards, this Quantlet assigns sentiment weights to the words based on the stock movement followed by the publication."}, {"input_sequence": "Author: Julian Woessner ; #!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nThis file implements the HRP algorithm, the MinVAr portfolio and the IVP portf.\nfor the fulfillment of the 2019 SDA class in St.Gallen, CH.\nCode for HRP is based on Lopez de Prado, M. (2018). Advances in Financial\nMachine Learning. Wiley. The code has been adapted in order to be used with\npython 3 and the data set.\n\n@author: julianwossner\n@date: 20191116\n# In[1]:\n# Load modules\nimport os\npath = os.getcwd() # Set Working directory here\n# Import modules for Datastructuring and calc.\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nimport warnings\nfrom tqdm import tqdm\n# Modules for RHP algorithm\nimport matplotlib.pyplot as mpl\nimport scipy.cluster.hierarchy as sch\n# Modules for the network plot\nimport networkx as nx\nfrom networkx.convert_matrix import from_numpy_matrix\n# Modules for Markowitz optimization\nimport cvxopt as opt\nimport cvxopt.solvers as optsolvers\nwarnings.filterwarnings(\"ignore\") # suppress warnings in clustering\n# In[2]:\n# define functions for HRP and IVP\ndef getIVP(cov,**kargs):\n# Compute the inverse-variance portfolio\nivp=1./np.diag(cov)\nivp/=ivp.sum()\nreturn ivp\ndef getClusterVar(cov, cItems):\n# Compute variance per cluster\ncov_=cov.loc[cItems, cItems] # matrix slice\nw_=getIVP(cov_).reshape(-1,1)\ncVar=np.dot(np.dot(w_.T,cov_),w_)[0,0]\nreturn cVar\ndef getQuasiDiag(link):\n# Sort clustered items by distance\nlink=link.astype(int)\nsortIx=pd.Series([link[-1,0],link[-1,1]])\nnumItems=link[-1,3] # number of original items\nwhile sortIx.max() >=numItems:\nsortIx.index=range(0,sortIx.shape[0]*2,2) # make space\ndf0=sortIx[sortIx>=numItems] # find clusters\ni=df0.index;j=df0.values-numItems\nsortIx[i]=link[j,0] # item 1\ndf0=pd.Series(link[j,1], index=i+1)\nsortIx=sortIx.append(df0) # item 2\nsortIx.index=range(sortIx.shape[0]) # re-index\nreturn sortIx.tolist()\ndef getRecBipart(cov,sortIx):\n# Compute HRP alloc\nw=pd.Series(1,index=sortIx)\ncItems=[sortIx] # initialize all items in one cluster\nwhile len(cItems)>0:\ncItems=[i[j:k] for i in cItems for j,k in ((0,len(i)//2),(len(i)//2,\\\nlen(i))) if len(i)>1] # bi-section\nfor i in range(0,len(cItems),2): # parse in pairs\ncItems0=cItems[i] # cluster 1\ncVar0=getClusterVar(cov,cItems0)\nalpha=1-cVar0/(cVar0+cVar1)\nw[cItems0]*=alpha # weight 1\nreturn w\ndef correlDist(corr):\n# A distance matrix based on correlation, where 0<=d[i,j]<=1\n# This is a proper distance metric\ndist=((1-corr)/2.)**.5 # distance matrix\nreturn dist\ndef plotCorrMatrix(path, corr, labels=None):\n# Heatmap of the correlation matrix\nif labels is None:labels=[]\nmpl.pcolor(corr)\nmpl.yticks(np.arange(.5,corr.shape[0]+.5),labels)\nmpl.savefig(path,dpi=300, transparent=True)\nmpl.clf();mpl.close() # reset pylab\nreturn\n# In[3]:\n# define function for MinVar portfolio\n\n# The MIT License (MIT)\n#\n# Copyright (c) 2015 Christian Zielinski\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\ndef min_var_portfolio(cov_mat, allow_short=False):\n\"\"\"\nComputes the minimum variance portfolio.\nNote: As the variance is not invariant with respect\nto leverage, it is not possible to construct non-trivial\nmarket neutral minimum variance portfolios. This is because\nthe variance approaches zero with decreasing leverage,\ni.e. the market neutral portfolio with minimum variance\nis not invested at all.\nParameters\n----------\ncov_mat: pandas.DataFrame\nCovariance matrix of asset returns.\nallow_short: bool, optional\nIf 'False' construct a long-only portfolio.\nIf 'True' allow shorting, i.e. negative weights.\nReturns\n-------\nweights: pandas.Series\nOptimal asset weights.\nif not isinstance(cov_mat, pd.DataFrame):\nraise ValueError(\"Covariance matrix is not a DataFrame\")\nn = len(cov_mat)\n\nP = opt.matrix(cov_mat.values)\nq = opt.matrix(0.0, (n, 1))\n# Constraints Gx <= h\nif not allow_short:\n# x >= 0\nG = opt.matrix(-np.identity(n))\nelse:\nG = None\n# Constraints Ax = b\n# sum(x) = 1\nA = opt.matrix(1.0, (1, n))\n# Solve\noptsolvers.options['show_progress'] = False\nsol = optsolvers.qp(P, q, G, h, A, b)\nif sol['status'] != 'optimal':\nwarnings.warn(\"Convergence problem\")\n\n# Put weights into a labeled series\nweights = pd.Series(sol['x'], index=cov_mat.index)\nreturn weights\n# In[4]:\n# Define functions for network graphs\n#Function to plot Network plots\ndef plotNetwork(path,corr):\n# Transform it in a links data frame\n#links=corr.stack().reset_index()\n#Build graph\ncorr=Corr_mat\nadj_matrix = corr\nconstits_latest = corr.index\n# remove self-loops\nadj_matrix = np.where((adj_matrix<=1.000001) & (adj_matrix>=0.99999),0,adj_matrix)\n# replace values that are below threshold\n# create undirected graph from adj_matrix\ngraph = from_numpy_matrix(adj_matrix, parallel_edges=False, create_using= nx.Graph())\n# set names to crypots\ngraph = nx.relabel.relabel_nodes(graph, dict(zip(range(len(constits_latest)), constits_latest)))\npos_og = nx.circular_layout(graph, scale=2)\nfor p in pos: # raise text positions\nif pos[p][1]>1:\npos[p][1] += 0.15\nelif pos[p][0]<0:\nelse:\npos[p][0]+=0.3\nplt = mpl.figure(figsize = (5,5))\nnx.draw(graph, pos_og, with_labels= False)\nnx.draw_networkx_labels(graph, pos)\n\nplt.savefig(path,dpi=300 ,transparent=True)\nmpl.clf();mpl.close()\n# In[5]:\n# Loading and structuring crypto data sets\nCrypto = pd.read_csv(\"crypto_prices.csv\") #load csv\nCrypto = Crypto[(~Crypto.isnull()).all(axis=1)] # Deleting empty rows\nCrypto[\"date\"] = Crypto['date'].map(lambda x: str(x)[:-9]) # Removing timestamp\nCrypto = Crypto.rename(columns = {\"date\":\"Date\"})\nCrypto = Crypto.replace(to_replace = 0, method = \"ffill\")\nPrice_data_univ=Crypto\n#Price_data_univ = pd.merge(SP500, Crypto, on='Date', how='inner')#rename column\nPrice_data_univ = Price_data_univ.set_index(\"Date\") # define Date as index\n# Calculating returns\nReturn_data_univ = Price_data_univ.pct_change() #calculate daily returns\nReturn_data_univ = Return_data_univ.drop(Return_data_univ.index[range(0,1)])\n# Calculating covariance matrix\nCov_mat = Return_data_univ.cov() # Covariance matrix of the return matrix\n# In[6]:\n# Heatmap and network analysis of corr. matrix\n# Plotting Correlation matrix heatmap\nplotCorrMatrix(path+\"/Corr_Heatmap_Crypto_unsorted\",Corr_mat)\n# network plot of correlation matrix\nplotNetwork(path+\"/Corr_Network_Crypto_unsorted.png\", Corr_mat)\n# Sort correlation matrix\ndist=correlDist(Corr_mat)\nlink=sch.linkage(dist,'single')\nsortIx=getQuasiDiag(link)\nsortIx=Corr_mat.index[sortIx].tolist() # recover labels\nCorr_sorted=Corr_mat.loc[sortIx,sortIx] # reorder\n# Plot sorted correlation matrix\nplotCorrMatrix(path+\"/Corr_Heatmap_Crypto_sorted\",Corr_sorted)\n# Plot dendogram of the constituents\n#2) Cluster Data\nmpl.figure(num=None, figsize=(20, 10), dpi=300, facecolor='w',\ndn = sch.dendrogram(link, labels = dist.columns)\nmpl.savefig(path+\"/Dendrogram_Crypto.png\", transparent = True, dpi = 300)\nmpl.clf();mpl.close() # reset pylab\n# In[7]:\n#Function to calculate the HRP portfolio weights\ndef HRPportf(cov,corr):\n#1) Cluster covariance matrix\ndist=correlDist(corr)\nlink=sch.linkage(dist,'single')\nsortIx=getQuasiDiag(link)\nsortIx=corr.index[sortIx].tolist() # recover labels\n#2) Allocate capital according to HRP\nweights_hrp=getRecBipart(cov,sortIx)\nreturn weights_hrp\n# In[8]:\n# Compute the weights for the Markowitz MinVar and the HRP portfolio and the\n# IVP portfolio\nw_HRP=np.array([HRPportf(Cov_mat,Corr_mat).index,HRPportf(Cov_mat,Corr_mat).round(3)])\nw_HRP=pd.DataFrame(np.transpose(w_HRP))\nw_HRP.columns = [\"Asset\",\"Weights HRP\"]\nw_MinVar= np.array([min_var_portfolio(Cov_mat).index,min_var_portfolio(Cov_mat).round(3)])\nw_MinVar=pd.DataFrame(np.transpose(w_MinVar))\nw_MinVar.columns = [\"Asset\",\"Weights MinVar\"]\nw_IVP= np.array([Cov_mat.index, getIVP(Cov_mat).round(3)])\nw_IVP=pd.DataFrame(np.transpose(w_IVP))\nw_IVP.columns = [\"Asset\",\"Weights IVP\"]\nWeights = pd.merge(w_MinVar,w_IVP,\\\non=\"Asset\", how = \"inner\")\nWeights = pd.merge(Weights,w_HRP,\\\nprint(Weights.to_latex(index=False)) # Latex table output\n# In[9]:\n# Backtesting the three optimisation methods for Crypto dataset\n# Function to calculate the weigths in sample and then test out of sample\ndef Backtest_Crypto(returns, rebal = 30): # rebal = 30 default rebalancing after 1 month\nnrows = len(returns.index)-rebal # Number of iterations without first set to train\nrets_train = returns[:rebal]\ncov,corr = rets_train.cov(),\nw_HRP=np.array([HRPportf(cov,corr).index,HRPportf(cov,corr)])\nw_HRP=pd.DataFrame(np.transpose(w_HRP))\nw_HRP.columns = [\"Asset\",\"Weights HRP\"]\nw_MinVar= np.array([cov.index,min_var_portfolio(cov)])\nw_MinVar=pd.DataFrame(np.transpose(w_MinVar))\nw_MinVar.columns = [\"Asset\",\"Weights MinVar\"]\nw_IVP= np.array([cov.index, getIVP(cov)])\nw_IVP=pd.DataFrame(np.transpose(w_IVP))\nw_IVP.columns = [\"Asset\",\"Weights IVP\"]\n\nWeights = pd.merge(w_MinVar, w_IVP, on=\"Asset\", how = \"inner\")\nWeights = Weights.drop(Weights.columns[0],axis=1).to_numpy()\nportf_return = pd.DataFrame(columns=[\"MinVar\",\"IVP\",\"HRP\"], index = range(nrows))\nfor i in tqdm(range(rebal,nrows+rebal)):\nif i>rebal and i<nrows-rebal and i % rebal == 0: # Check for rebalancing date\nrets_train = returns[i-rebal:i]\ncov,corr = rets_train.cov(),\nw_HRP=np.array([HRPportf(cov,corr).index,HRPportf(cov,corr)])\nw_HRP=pd.DataFrame(np.transpose(w_HRP))\nw_HRP.columns = [\"Asset\",\"Weights HRP\"]\n\nw_MinVar= np.array([cov.index,min_var_portfolio(cov)])\nw_MinVar=pd.DataFrame(np.transpose(w_MinVar))\nw_MinVar.columns = [\"Asset\",\"Weights MinVar\"]\nw_IVP= np.array([cov.index, getIVP(cov)])\nw_IVP=pd.DataFrame(np.transpose(w_IVP))\nw_IVP.columns = [\"Asset\",\"Weights IVP\"]\nWeights = pd.merge(w_MinVar, w_IVP, on=\"Asset\", how = \"inner\")\nWeights = Weights.drop(Weights.columns[0],axis=1).to_numpy()\n\nportf_return[\"MinVar\"][i-int(rebal):i-int(rebal)+1]=np.average(returns[i:i+1].to_numpy(),\\\nweights = Weights[:,0].reshape(len(returns.columns),1).ravel(), axis = 1)\nportf_return[\"IVP\"][i-int(rebal):i-int(rebal)+1]=np.average(returns[i:i+1].to_numpy(),\\\nweights = Weights[:,1].reshape(len(returns.columns),1).ravel(), axis = 1)\nportf_return[\"HRP\"][i-int(rebal):i-int(rebal)+1]=np.average(returns[i:i+1].to_numpy(),\\\nweights = Weights[:,2].reshape(len(returns.columns),1).ravel(), axis = 1)\n\nreturn portf_return\n# In[10]:\n# Calculate the backtested portfolio returns\nportf_rets = Backtest_Crypto(Return_data_univ, rebal=90)\nportf_rets2 = 1+portf_rets\nrebal = 90\n# Calculate index\nportf_index = pd.DataFrame(columns=[\"MinVar\",\"IVP\",\"HRP\"], \\\nindex = Return_data_univ.index)[rebal:]\nportf_index[0:1] = 100\nfor i in range(1,len(portf_index.index)):\nportf_index[\"MinVar\"][i:i+1] = float(portf_index[\"MinVar\"][i-1:i]) * float(portf_rets2[\"MinVar\"][i-1:i])\nindex = portf_index.plot.line()\nmpl.savefig(path+\"/Index_crypto.png\", transparent = True, dpi = 300)\n# Calculate portfolio return and portfolio variance\nmean_MinVar = stats.mstats.gmean(np.array(portf_rets2[\"MinVar\"], dtype=float))-1\n# Daily Standard deviation\nstd_MinVar = portf_rets[\"MinVar\"].std()\n# Sharpe ratios\nSR_MinVar = mean_MinVar/std_MinVar\nSR_IVP = mean_IVP/std_IVP\nPerf_figures = pd.DataFrame([[mean_MinVar, mean_IVP,mean_HRP], [std_MinVar, \\\nstd_IVP, std_HRP],[SR_MinVar,SR_IVP,SR_HRP]], \\\nindex =['Mean', 'Std', 'SR'], columns = [\"MinVar\",\"IVP\",\"HRP\"])\nprint(Perf_figures.to_latex(index=True)) # Latex table output\n# In[11]:\n# Analysis for the SP500 universe from 2015 onwards\n# Data import\nSP500 = pd.read_csv(\"SP500_price_data_15.csv\") #load csv\n#Deleting empty columns\nSP500 = SP500.replace(to_replace = 0, method = \"ffill\")\nPrice_data_univ = SP500\nPrice_data_univ = Price_data_univ.set_index(\"Date\") # define Date as index\n# Drop columns with incomplete data\nPrice_data_univ = Price_data_univ.drop([\"AMCR\",\"BF.B\",\"BKR\",\"BRK.B\",\"CTVA\", \\\n\"DOW\",\"FOX\",\"FOXA\",\"FTV\",\"HPE\",\"KHC\", \\\n# Calculating returns and deleting columns that contain 0\n#Return_data_univ = Return_data_univ.fillna(0)\n# In[12]:\nplotCorrMatrix(path+\"/Corr_Heatmap_SP500_15_unsorted\",Corr_mat)\ndn = sch.dendrogram(link, labels = dist.columns, leaf_rotation=90., \\\nleaf_font_size=8.) # font size for the x axis labels)\nmpl.savefig(path+\"/Dendrogram_SP500.png\", transparent = True, dpi = 1200)\n# In[13]:\n# Backtesting function for stocks data\ndef Backtest_SP500(returns, rebal = 30): # rebal = 30 default rebalancing after 1 month\n# In[14]:\nportf_rets_15 = Backtest_SP500(Return_data_univ, rebal=90) # Change rebalancing frequency\nportf_rets_2 = 1+portf_rets_15\nportf_index[\"MinVar\"][i:i+1] = float(portf_index[\"MinVar\"][i-1:i]) * float(portf_rets_2[\"MinVar\"][i-1:i])\nmpl.savefig(path+\"/Index_SP500_15_90.png\", transparent = True, dpi = 300)\n# Daily average return\nmean_MinVar = stats.mstats.gmean(np.array(portf_rets_2[\"MinVar\"], dtype=float))-1\nstd_MinVar = portf_rets_15[\"MinVar\"].std()\n# In[15]:\n# Analysis for the SP500 universe from 2000 onwards: Data import\nSP500 = pd.read_csv(\"SP500_price_data_00.csv\") #load csv\n#SP500 = SP500[[col for col in SP500.columns if SP500.loc[:,col].notna().any()]]\nPrice_data_univ = Price_data_univ.drop([\"AAL\",\"AAP\",\"ABBV\",\"ACN\",\"ADS\",\"AIZ\",\\\n\"ALGN\",\"ALLE\",\"AMCR\",\"AMP\",\"ANET\",\"ANTM\"\\\n\"NWS\"], axis=1)\n# In[16]:\nportf_rets = Backtest_SP500(Return_data_univ, rebal=90) # Change rebalancing frequency\nmpl.savefig(path+\"/Index_SP500_00_90.png\", transparent = True, dpi = 300)\n# In[17]:\n# Changing the rebalancing frequency\nportf_rets_30 = Backtest_SP500(Return_data_univ, rebal=30) # Monthly\nportf_rets_30_2 = 1+portf_rets_30\nportf_rets_180 = Backtest_SP500(Return_data_univ, rebal=180) # half-year\nportf_rets_180_2 = 1+portf_rets_180\nportf_rets_360 = Backtest_SP500(Return_data_univ, rebal=360) # yearly\nportf_rets_360_2 = 1+portf_rets_360\n#Print indices\nrebal = 30\nportf_index[\"MinVar\"][i:i+1] = float(portf_index[\"MinVar\"][i-1:i]) * float(portf_rets_30_2[\"MinVar\"][i-1:i])\nmpl.savefig(path+\"/Index_SP500_00_30.png\", transparent = True, dpi = 300)\nrebal = 180\nportf_index[\"MinVar\"][i:i+1] = float(portf_index[\"MinVar\"][i-1:i]) * float(portf_rets_180_2[\"MinVar\"][i-1:i])\nmpl.savefig(path+\"/Index_SP500_00_180.png\", transparent = True, dpi = 300)\nrebal = 360\nportf_index[\"MinVar\"][i:i+1] = float(portf_index[\"MinVar\"][i-1:i]) * float(portf_rets_360_2[\"MinVar\"][i-1:i])\nmpl.savefig(path+\"/Index_SP500_00_360.png\", transparent = True, dpi = 300)\n# Calculate performance figures\nmean_MinVar = stats.mstats.gmean(np.array(portf_rets_30_2[\"MinVar\"], dtype=float))-1\nstd_MinVar = portf_rets_30[\"MinVar\"].std()\nmean_MinVar = stats.mstats.gmean(np.array(portf_rets_180_2[\"MinVar\"], dtype=float))-1\nstd_MinVar = portf_rets_180[\"MinVar\"].std()\nmean_MinVar = stats.mstats.gmean(np.array(portf_rets_360_2[\"MinVar\"], dtype=float))-1\nstd_MinVar = portf_rets_360[\"MinVar\"].std()\n# In[18]:\n# Create Mixed portfolios with stocks and crypto\nSP500 = SP500.drop([\"AAL\",\"AAP\",\"ABBV\",\"ACN\",\"ADS\",\"AIZ\",\\\nSP500 = SP500.set_index(\"Date\")\nSP500_sample = SP500.sample(10,axis = 1,random_state=1) # randomly select 10 stocks from SP500\nPrice_data_univ2 = pd.merge(SP500_sample, Crypto, on='Date', how='inner')#rename column\nPrice_data_univ2 = Price_data_univ2.set_index(\"Date\") # define Date as index\nReturn_data_univ2 = Price_data_univ2.pct_change() #calculate daily returns\nReturn_data_univ2 = Return_data_univ2.drop(Return_data_univ2.index[range(0,1)])\nCov_mat2 = Return_data_univ2.cov() # Covariance matrix of the return matrix\n# In[19]:\nplotCorrMatrix(path+\"/Corr_Heatmap_Mixed_unsorted\",Corr_mat2)\ndist=correlDist(Corr_mat2)\nsortIx=Corr_mat2.index[sortIx].tolist() # recover labels\nCorr_sorted2=Corr_mat2.loc[sortIx,sortIx] # reorder\nplotCorrMatrix(path+\"/Corr_Heatmap_Mixed_sorted\",Corr_sorted2)\n# Cluster Data\nmpl.savefig(path+\"/Dendrogram_Mixed.png\", transparent = True, dpi = 300)\n# In[20]:\nportf_rets_30 = Backtest_SP500(Return_data_univ2, rebal=30) # Monthly\nportf_rets_90_2 = 1+portf_rets_90\nindex = Return_data_univ2.index)[rebal:]\nindex = portf_index.plot.line( rot=25)\nmpl.savefig(path+\"/Index_Mixed_16_30.png\", transparent = True, dpi = 300)\nindex = portf_index.plot.line(rot=25)\nmpl.savefig(path+\"/Index_Mixed_16_180.png\", transparent = True, dpi = 300)\nportf_index[\"MinVar\"][i:i+1] = float(portf_index[\"MinVar\"][i-1:i]) * float(portf_rets_90_2[\"MinVar\"][i-1:i])\nmpl.savefig(path+\"/Index_Mixed_16_90.png\", transparent = True, dpi = 300)\nmean_MinVar = stats.mstats.gmean(np.array(portf_rets_90_2[\"MinVar\"], dtype=float))-1\nstd_MinVar = portf_rets_90[\"MinVar\"].std()\n# In[ ]:\n", "output_sequence": "Implementation of three different portfolio optimisation methods. The classical Markowitz Minimum Variance technique is compared to the Inverse Variance Portfolio and a new approach called Hierarchical Risk Parity (HRP). HRP combines graph theory and machine learning techniques to determine the optimal allocation to assets based on the information contained in the covariance matrix. Strategies are applied to Crypto Currency Assets and Stocks and evaluated on risk/return profile."}, {"input_sequence": "Author: Thomas Haslwanter ; \"\"\"Logistic Regression\nA logistic regression is an example of a \"Generalized Linear Model (GLM)\".\nThe input values are the recorded O-ring data from the space shuttle launches\nbefore 1986, and the fit indicates the likelihood of failure for an O-ring.\nTaken from\nhttp://www.brightstat.com/index.php?option=com_content&task=view&id=41&Itemid=1&limit=1&limitstart=2\n\"\"\"\n# author: Thomas Haslwanter, date: Sept-2021\n# Import standard packages\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport pandas as pd\nimport seaborn as sns\n# additional packages\nimport sys\nsys.path.append(os.path.join('..', '..', 'Utilities'))\ntry:\n# Import formatting commands if directory \"Utilities\" is available\nfrom ISP_mystyle import setFonts, showData\n\nexcept ImportError:\n# Ensure correct performance otherwise\ndef setFonts(*options):\nreturn\ndef showData(*options):\nplt.show()\nfrom statsmodels.formula.api import glm\nfrom statsmodels.genmod.families import Binomial\nsns.set_context('poster')\ndef getData() -> np.ndarray:\n\"\"\"Get the data\nReturns\n-------\ndata : the info from the space shuttle launches\n\"\"\"\ninFile = 'challenger_data.csv'\ndata = np.genfromtxt(inFile, skip_header=1, usecols=[1, 2],\nmissing_values='NA', delimiter=',')\n# Eliminate NaNs\ndata = data[~np.isnan(data[:, 1])]\nreturn data\ndef prepareForFit(inData: np.ndarray) -> pd.DataFrame:\n\"\"\" Make the temperature-values unique, and count the number of failures\nand successes.\nParameters\n----------\ndf :\n# Create a dataframe, with suitable columns for the fit\ndf = pd.DataFrame()\ndf['temp'] = np.unique(inData[:,0])\ndf['failed'] = 0\ndf.index = df.temp.values\n# Count the number of starts and failures\nfor ii in range(inData.shape[0]):\ncurTemp = inData[ii,0]\ndf.loc[curTemp,'total'] += 1\nif curVal == 1:\ndf.loc[curTemp, 'failed'] += 1\nelse:\ndf.loc[curTemp, 'ok'] += 1\nreturn df\ndef logistic(x: np.ndarray, beta:float, alpha:float=0) -> np.ndarray:\n\"\"\" Logistic Function \"\"\"\nreturn 1.0 / (1.0 + np.exp(np.dot(beta, x) + alpha))\ndef showResults(challenger_data: np.ndarray, model) -> None:\n\"\"\" Show the original data, and the resulting logit-fit\nParamters\n---------\nchallenger_data : input data\nmodel : model results (statsmodels.genmod.generalized_linear_model.GLM)\ntemperature = challenger_data[:,0]\n# First plot the original data\nplt.figure()\nsetFonts()\nsns.set_style('darkgrid')\nnp.set_printoptions(precision=3, suppress=True)\nplt.scatter(temperature, failures, s=200, color=\"k\", alpha=0.5)\nplt.yticks([0, 1])\nplt.ylabel(\"Damage Incident?\")\nplt.xlabel(\"Outside Temperature [F]\")\nplt.title(\"Defects of the Space Shuttle O-Rings vs temperature\")\nplt.tight_layout\n# Plot the fit\nx = np.arange(50, 85)\nalpha = model.params[0]\ny = logistic(x, beta, alpha)\nplt.plot(x,y,'r')\nplt.xlim([50, 85])\noutFile = 'ChallengerPlain.png'\nshowData(outFile)\nif __name__ == '__main__':\ninData = getData()\ndfFit = prepareForFit(inData)\n# fit the model\n# --- >>> START stats <<< ---\nmodel = glm('ok + failed ~ temp', data=dfFit, family=Binomial()).fit()\n# --- >>> STOP stats <<< ---\nprint(model.summary())\nshowResults(inData, model)\n", "output_sequence": "Logistic Regression A logistic regression is an example of a <Generalized Linear Model (GLM)>.\nThe input values are the recorded O-ring data from the space shuttle launches before 1986, and the fit indicates the likelihood of failure for an O-ring.\nTaken from http://www.brightstat.com/index.php?option=com_content&task=view&id=41&Itemid=1&limit=1&limitstart=2"}, {"input_sequence": "Author: Thomas Haslwanter ; \"\"\"Simple linear models\n- \"model_formulas\" is based on examples in Kaplan's book \"Statistical Modeling\".\n- \"polynomial_regression\" shows how to work with simple design matrices\n\"\"\"\n# author: Thomas Haslwanter, date: Sept-2021\n# Import standard packages\nimport numpy as np\nimport pandas as pd\n# additional packages\nfrom statsmodels.formula.api import ols\nimport statsmodels.regression.linear_model as sm\nfrom statsmodels.stats.anova import anova_lm\nfrom typing import List\ndef model_formulas() -> float:\n\"\"\" Define models through formulas\n\nReturns\n-------\nF : Test statistic for the cubic model\n\"\"\"\n# Get the data:\n# Development of world record times for the 100m Freestyle (men/women)\ndata = pd.read_csv('swim100m.csv')\n# Different models\nmodel1 = ols(\"time ~ sex\", data).fit() # one factor\nmodel3 = ols(\"time ~ sex * year\", data).fit() # two factors with interaction\n# Model information\nprint((model1.summary()))\n# ANOVAs\nprint('----------------- Results ANOVAs: Model 1 -----------------------')\nprint((anova_lm(model1)))\nprint('--------------------- Model 2 -----------------------------------')\nprint((anova_lm(model2)))\nprint('--------------------- Model 3 -----------------------------------')\nmodel3Results = anova_lm(model3)\nprint(model3Results)\n# Just to check the correct run\nreturn model3Results['F'][0] # should be 156.1407931415788\ndef polynomial_regression() -> List[float]:\n\"\"\" Define the model directly through the design matrix.\nSimilar to MATLAB's \"regress\" command.\n\nReturns\n-------\nparams : coefficients for the quadratic model\n\"\"\"\n# Generate the data: a noisy second order polynomial\n# To get reproducable values, I provide a seed value\nnp.random.seed(987654321)\nt = np.arange(0,10,0.1)\ny = 4 + 3*t + 2*t**2 + 5*np.random.randn(len(t))\n# --- >>> START stats <<< ---\n# Make the fit. Note that this is another \"OLS\" than the one in\n# \"model_formulas\", as it works directly with the design matrix!\nM = np.column_stack((np.ones(len(t)), t, t**2))\nres = sm.OLS(y, M).fit()\n# --- >>> STOP stats <<< ---\n# Display the results\nprint('Summary:')\nprint((res.summary()))\nprint(('The fit parameters are: {0}'.format(str(res.params))))\nprint('The confidence intervals are:')\nprint((res.conf_int()))\nreturn res.params # should be [ 4.74244177, 2.60675788, 2.03793634]\nif __name__ == '__main__':\nmodel_formulas()\npolynomial_regression()\n", "output_sequence": "Simple linear models. - <model_formulas> is based on examples in Kaplan's book <Statistical Modeling>. - <polynomial_regression> shows how to work with simple design matrices, like MATLAB's <regress> command."}, {"input_sequence": "Author: Thomas Haslwanter ; \"\"\" Demonstration of linear regression using pingouin \"\"\"\n# author: Thomas Haslwanter, date: Aug-2021\n# Import standard packages\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n# ... for the 3d plot ...\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import cm\nimport os\nsys.path.append(os.path.join('..', '..', 'Utilities'))\ntry:\n# Import formatting commands if directory \"Utilities\" is available\nfrom ISP_mystyle import setFonts, showData\n\nexcept ImportError:\n# Ensure correct performance otherwise\ndef setFonts(*options):\nreturn\ndef showData(*options):\nplt.show()\nif __name__ == '__main__':\n# Generate some data\nnp.random.seed(12345)\nx = np.random.randn(100)*30\nz = 3 + 0.4*x + 0.05*y + 10*np.random.randn(len(x))\n# Put them into a DataFrame\ndf = pd.DataFrame({'x':x, 'y':y, 'z':z})\n# Simple linear regression\nresults = pg.linear_regression(df.x, df.z)\nprint(results.round(2))\nprint(f'p = {results.pval.values[1]:4.1e}\\n')\n# Multiple linear regression\nresults = pg.linear_regression(df[['x', 'y']], df['z'])\n# Show the data\nfig, axs = plt.subplots(1,2, figsize=(6,3))\ndf.plot('x', 'z', kind='scatter', ax=axs[0])\np = np.polyfit(df.x, df.z, 1)\nx = np.linspace(df.x.min(), df.x.max(), 100)\naxs[0].plot(x, np.polyval(p, x), color='C1', ls='dashed')\nplt.tight_layout()\nout_file = 'regression_pg.jpg'\nshowData(out_file)\n# --------- 3D plot ---------------\nx = np.linspace(df.x.min(),df.x.max(),101)\n(X,Y) = np.meshgrid(x,y)\nZ = results.coef[0] + results.coef[1]*X + results.coef[2]*Y\n# Set the color\nmyCmap = cm.GnBu_r\n# If you want a colormap from seaborn use:\n#from matplotlib.colors import ListedColormap\n#myCmap = ListedColormap(sns.color_palette(\"Blues\", 20))\n# Plot the figure\nfig = plt.figure()\nax = fig.gca(projection='3d')\nsurf = ax.plot_surface(X,Y,Z, cmap=myCmap, rstride=2,\nlinewidth=0, antialiased=False, alpha=0.3)\nax.view_init(20,-120)\nax.set_xlabel('X')\nfig.colorbar(surf, shrink=0.6)\nax.plot(df.x, df.y, 'o')\nout_file = 'regression_3d.jpg'\n# plt.show()\n", "output_sequence": "Simple linear models. - <model_formulas> is based on examples in Kaplan's book <Statistical Modeling>. - <polynomial_regression> shows how to work with simple design matrices, like MATLAB's <regress> command."}, {"input_sequence": "Author: Thomas Haslwanter ; \"\"\" Practical demonstration of central limit theorem for uniform distribution \"\"\"\n# author: Thomas Haslwanter, date: Sept-2021\n# Import standard packages\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\n# additional packages\nimport sys\nsys.path.append(os.path.join('..', '..', 'Utilities'))\ntry:\n# Import formatting commands if directory \"Utilities\" is available\nfrom ISP_mystyle import setFonts, showData\n\nexcept ImportError:\n# Ensure correct performance otherwise\ndef setFonts(*options):\nreturn\ndef showData(*options):\nplt.show()\n# Formatting options\nsns.set(context='poster', style='ticks', palette='muted')\n# Input data\nndata = 100000\nnbins = 50\ndef show_as_histogram(axis, data, title):\n\"\"\" Subroutine showing a histogram and formatting it \"\"\"\naxis.hist( data, bins=nbins)\naxis.set_xticks([0, 0.5, 1])\naxis.set_title(title)\ndef main():\n\"\"\"Demonstrate central limit theorem.\"\"\"\nsetFonts(24)\n# Generate data\ndata = np.random.random(ndata)\n# Show three histograms, side-by-side\nfig, axs = plt.subplots(1,3)\nshow_as_histogram(axs[0], data, 'Random data')\nshow_as_histogram(axs[1], np.mean(data.reshape((ndata//2, 2 )), axis=1),\n'Average over 2')\nshow_as_histogram(axs[2], np.mean(data.reshape((ndata//10,10)), axis=1),\n'Average over 10')\n# Format them and show them\naxs[0].set_ylabel('Counts')\nplt.tight_layout()\nshowData('CentralLimitTheorem.png')\nif __name__ == '__main__':\nmain()\n", "output_sequence": "Practical demonstration of the central limit theorem, based on the uniform distribution"}, {"input_sequence": "Author: Xiu Xu ; clear all;\nclc;\nfirmnum = 10;\nBankname = [\n'Bank of America';\n' Citygroup ';\n' Goldman Sachs ';\n' UBS '\n];\nfigure\nbank = [3 9];\nfor i = 1 : length(bank)\nk = bank(i);\n[data, d] = xlsread('Bank.xlsx', ['Bank_' num2str(k)], 'A4:K2091');\nCDS = log(data);\nbname = Bankname(k, :);\nsubplot(1, 2, i)\nCDSPlot(CDS', bname)\nend\nsaveas(gcf, 'CDS1', 'png');\nbanks = [1 2 4 5];\nfor i = 1 : 4\nk = banks(i);\nsubplot(2, 2, i)\nsaveas(gcf, 'CDS2', 'png');\nbanks = [6 7 8 10];\n", "output_sequence": "Plot the 3D CDS curves of the selected 10 banks."}, {"input_sequence": "Author: Xiu Xu ; function [] = CDSPlot(Res, bname)\nmesh(Res);\nset(gca, 'XTick', [266, 788, 1309, 1831])\nset(gca, 'XTicklabel', ['2009'; '2015'])\nx1 = xlabel('Year');\nhold on;\nset(gca, 'YTick', [1, 6, 10])\nset(gca, 'YTicklabel', [' 6M'; ' 5Y'; '30Y'])\nx2 = ylabel('Maturity');\nzlabel('Value')\nset(x1, 'Rotation', 20);\nylim([0, size(Res, 1)]);\ntitle(char(bname))\nset(gca, 'FontName', 'Times New Roman', 'FontSize', 12);\nend\n", "output_sequence": "Plot the 3D CDS curves of the selected 10 banks."}, {"input_sequence": "Author: Darius Jonda ; # Libraries\nlibrary(foreign)\n\n# Functions\n## Inputs KNHANES nutrition survey dataset and seperates into existing\n## food groups + seperates Alcohol, Kimchi, White Rice, Coffee, Bread into\n## new food groups\nFoodGroupAdd = function(df = NULL, year = NULL, agefilter = NULL) {\ndf_fg = upper(df) %>%\nmutate(FOOD_GROUP = as.numeric(substr(N_FCODE, 1, 2)))\n\nif(!is.null(agefilter)) {\ndf_fg = df_fg %>%\nfilter(AGE > agefilter)\n}\ndf_fg_wname = merge(df_fg, foodgroup_db,\nby.x = \"FOOD_GROUP\",\nmutate(NAME = as.character(NAME))\nif (year == 98) {\nalc = as.factor(c(15031:15062))\nkimchi = paste0(\"0\", c(6045:6057))\ncoffee = as.character(c(15012:15018))\nbread = paste0(\"0\", c(1049:1071))\ndf_fg_wname$NAME[trim(df_fg_wname$N_FCODE) %in% alc] = \"Alcohol\"\n} else if (year == 14) {\nalc = as.character(c(15026:15060))\nkimchi = paste0(\"0\", c(6057:6070))\ncoffee = as.character(c(15083:15088))\nbread = paste0(\"0\", c(1053:1076))\nreturn(df_fg_wname)\n}\n## Summarizes the kcal intake by foodgroups for each individual first and\n## for the entire population second.\nFoodGroupRank = function(df = NULL, year = NULL) {\ndf1 = FoodGroupAdd(df, year) %>%\ngroup_by(ID, FOOD_GROUP, NAME) %>%\nsummarise(DAILY_INTAKE_KCAL = sum(NF_EN))\ndfreturn = df1 %>%\ngroup_by(FOOD_GROUP, NAME) %>%\nsummarise(DailYIntGram = sum(DAILY_INTAKE_KCAL)/length(unique(df1$ID))) %>%\narrange(desc(DailYIntGram))\nreturn(dfreturn)\n## Inputs Dataframe and returns same Dataframe with UPPERCASE variable names\nupper = function(df) {\nnames(df) = toupper(names(df))\ndf\n## trim function removes unnecessary blank spaces\ntrim = function(x) gsub(\"^\\\\s+|\\\\s+$\", \"\", x)\n# Read in data\n## read in 24 h recall examination files\nfiles = list.files(pattern = \"_24RC\")\nfor (file in files) {\ntd = as.data.frame(read.spss(paste0(file)), stringsAsFactors = F)\ntd_name = substr(file, 0, 9)\nassign(td_name, td)\n# Analysis\n## Creates lookup table for foodgroups\nfoodgroup_db = data.frame(\nID = 1:18,\nNAME = c(\"Grains\", \"Potatoes and Starch\", \"Sugars\", \"Legumes\",\n\"Seeds and Nuts\", \"Other Vegetables\", \"Mushrooms\",\n\"Fruits\", \"Meats\", \"Eggs\", \"Fish\", \"Seeweads\",\n\"Milk and Dairy Products\", \"Fat and Oils\",\n\"Beverages\", \"Seasonings\", \"Processed Foods\", \"Others\"))\n## Applies FoodGroupRank function on KNHANES Nutrition Survey datasets from\n## 1998 and 2015\nfoodgroup98 = FoodGroupRank(df = HN98_24RC, year = 98)\n", "output_sequence": "Summarizes food group intake by kilocalories from the 1998 and 2015 KNHANES datasets."}, {"input_sequence": "Author: Li-Shan Huang, Wolfgang Karl H\u00e4rdle ; ## clear history\nrm(list = ls(all = TRUE))\ngraphics.off()\n## install and load packages\nlibraries = c(\"mgcv\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n## count number of data points around each grid point\ncountdata = function(xdata, xgrid, h) {\nnsize = length(xdata)\ngridlength = length(xgrid)\nncount = rep(0, gridlength)\nfor (i in 1:gridlength) {\nncount[i] = sum(ifelse(abs(xdata - xgrid[i]) <= h, 1, 0))\n}\nncount\n}\n# Epanechnikov kernel function\nepkernel = function(x) {\ntemp = 0.75 * (1 - x * x)\ntemp[x <= -1] = 0\ntemp\n# smoother matrix in Huang and Chen (2008) Epanechnikov kernel normlizing &\n# symmatric & sum to one\nproj.ep.symmetric = function(x, xgrid, h) {\ndatalength = length(x)\ngridint = xgrid[2] - xgrid[1]\nlocalproj = matrix(0, ncol = datalength, nrow = datalength)\nfor (i in 1:datalength) {\nbigK[i, ] = (epkernel(((x[i] - xgrid)/h))/h)\nadjvect = as.numeric(1/bigK %*% rep(gridint, gridlength))\nbigKK = (diag(adjvect, nrow = datalength, ncol = datalength) %*% bigK)\n\nkweight = diag(bigKK[, i], ncol = datalength)\nbigX = cbind(rep(1, length(x)), (x - xgrid[i]))\nlocalH = bigX %*% (solve(t(bigX) %*% kweight %*% bigX)) %*% t(bigX) %*% kweight\nlocalproj = localproj + kweight %*% localH\nlocalproj = localproj * gridint\ndefree = sum(diag(localproj))\nlist(Hstar = localproj, defree = defree)\n## analysis of deviance function\n## using Epanechnikov kernel, xgrid must be equally spaced testing no effect of x\nanodev.logit.chisq = function(y, x, xgrid, h) {\nglobal.glm = glm(y ~ 1, family = binomial(link = \"logit\"))\nd.dev = rep(0, gridlength)\nd.coeff = matrix(0, nrow = gridlength, ncol = 2)\nx.local = x - xgrid[i]\nkweights = bigKK[, i]\n\nd.glm = glm(y ~ x.local, family = binomial(link = \"logit\"), weights = kweights)\nd.dev[i] = d.glm$dev\n# i loop\nintD = sum(d.dev) * gridint\nteststat = -(intD - global.glm$dev)\nlist(intD = intD, teststat = teststat)\n# under H1, a=0.5, 0.75, 1\na = 0.5\nset.seed(20150130)\n# a=0.5 n=100 set.seed(20150130) a=0.5 n=200 set.seed(201206225)\nsamplen = 100\nNsim = 5000\n# try 7 values of bandwidth, 0.1, 0.25, and 0.3\nhlength = 7\n# initialization\nchi.teststat = matrix(0, nrow = Nsim, ncol = hlength)\n# integrated deviance\nintDall = matrix(0, nrow = Nsim, ncol = hlength)\n# AICc values for 7 values of bandwidth\nAICc = matrix(0, nrow = Nsim, ncol = hlength)\n# bandwidth select by AICc and its p-value\nAICch = rep(0, Nsim)\n# Horowitz and Spokoiny (2001) set the initial value as the smallest bandwidth\nadaptiveh = rep(0.1, Nsim)\n# for gam in mgcv\ngam.p0 = rep(0, Nsim)\n# range of t set as [0,1]\ntgrid = seq(0, 1, 0.005) #length 201\nfor (j in 1:Nsim) {\n# random design check if every neighborhood has at least 3 points at smallest h\ncheckdata = rep(0, length(tgrid))\nwhile (any(checkdata <= 2)) {\nxt = c(runif((samplen - 2)), 0, 1)\nt.range = max(xt) - min(xt)\n# check if there is sufficient data around each grid point when h=0.1\ncheckdata = countdata(xt, tgrid, 0.1)\n# print(any(checkdata<=2))\nbeta0 = -1\neta = beta0 + a * cos(2 * pi * xt)\np0 = exp(eta)/(1 + exp(eta))\ny0 = as.integer(rbinom(samplen, 1, p0))\nt.order = order(xt)\nd4 = data.frame(xt[t.order], y0[t.order])\nnames(d4) = c(\"xt\", \"y0\")\nhchoice = c(0.1, 0.12, 0.25, 0.3)\nAICcmin = 10\nfor (hi in 1:hlength) {\nh = hchoice[hi]\n# call the function\nchi.teststat[j, hi] = anodev.logit.chisq(d4$y0, d4$xt, tgrid, h)$teststat\n# get the smoother matrix\nd4.H = proj.ep.symmetric(d4$xt, tgrid, h)\n# get the trace of smoother matrix\ndegfee[j, hi] = d4.H$defree\n# calculate AICc and find AICcmin\nAICc[j, hi] = log(intDall[j, hi]/samplen) + 2 * (degfee[j, hi] + 1)/(samplen -\ndegfee[j, hi] - 2)\nif (AICc[j, hi] < AICcmin) {\nAICcmin = AICc[j, hi]\nAICch[j] = h\n}\n# proposed chi-square test statistic p-value\npvalue[j, hi] = 1 - pchisq(chi.teststat[j, hi], (degfee[j, hi] - 1))\n# find the smallest pvalue based on Horowitz and Spokoiny (2001)\nif (pvalue[j, hi] < adaptivepvalue[j]) {\nadaptivepvalue[j] = pvalue[j, hi]\n# end hi bandiwdth\nfindaicc = (hchoice == AICch[j])\nAICcpvalue[j] = pvalue[j, findaicc]\n# mgcv package\nb.gam = gam(d4$y0 ~ s(d4$xt), method = \"REML\", family = binomial(link = \"logit\"))\ngam.p0[j] = summary(b.gam, p.type = 0)$s.pv\ngam.edf[j] = summary(b.gam)$edf\ncat(j, \"/\")\n# j loop\nprint(samplen)\nprint(\"random\")\ntestresults1 = round(matrix(c(hchoice, colSums(pvalue <= 0.05)/Nsim), nrow = 2, ncol = 7,\nbyrow = TRUE), 6)\ntestresults1\ntestresults2 = round(matrix(c(mean(AICch), sd(AICch), sum(AICcpvalue < 0.05)/Nsim,\nmean(adaptiveh), sum(adaptivepvalue < 0.05)/Nsim), nrow = 2, byrow = TRUE),\n6)\ntestresults2\ndegresults = round(matrix(c(mean(degfee[, 1]), sd(degfee[, 1]), mean(degfee[, 2]),\nsd(degfee[, 2]), mean(degfee[, 3]), sd(degfee[, 3]), mean(degfee[, 4]), sd(degfee[,\nmean(degfee[, 7]), sd(degfee[, 7])), nrow = 2, byrow = FALSE), 6)\ndegresults\nprint(table(AICch))\nprint(table(adaptiveh))\nprint(\"gam\")\nprint(sum(gam.p0 <= 0.05))/Nsim\nprint(summary(gam.edf))\n", "output_sequence": "Computes the percent of rejection under H1 in example 1 (in the JBES paper Analysis of Deviance for Hypothesis Testing in Generalized Partially Linear Models)."}, {"input_sequence": "Author: Li-Shan Huang, Wolfgang Karl H\u00e4rdle ; # clear history\nrm(list = ls(all = TRUE))\ngraphics.off()\n# install and load packages\nlibraries = c(\"mgcv\", \"MASS\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# count number of data points around each grid point\ncountdata = function(xdata, xgrid, h) {\n# logit data need to careful about y\nnsize = length(xdata)\ngridlength = length(xgrid)\nncount = rep(0, gridlength)\nfor (i in 1:gridlength) {\nxnhd = ifelse(abs(xdata - xgrid[i]) <= h, 1, 0)\nncount[i] = sum(xnhd)\n}\nncount\n}\n# Epanechnikov kernel function\nepkernel = function(x) {\ntemp = 0.75 * (1 - x * x)\ntemp[x <= -1] = 0\ntemp\n# smoother matrix in Huang and Chen (2008) Epanechnikov kernel normlizing &\n# symmatric & sum to one\nproj.ep.symmetric = function(x, xgrid, h) {\ndatalength = length(x)\ngridint = xgrid[2] - xgrid[1]\nlocalproj = matrix(0, ncol = datalength, nrow = datalength)\nfor (i in 1:datalength) bigK[i, ] = (epkernel(((x[i] - xgrid)/h))/h)\n\nadjvect = as.numeric(1/bigK %*% rep(gridint, gridlength))\nbigKK = (diag(adjvect, nrow = datalength, ncol = datalength) %*% bigK)\nkweight = diag(bigKK[, i], ncol = datalength)\nbigX = cbind(rep(1, length(x)), (x - xgrid[i]))\nlocalH = bigX %*% (solve(t(bigX) %*% kweight %*% bigX)) %*% t(bigX) %*%\nkweight\nlocalproj = localproj + kweight %*% localH\nlocalproj = localproj * gridint\ndefree = sum(diag(localproj))\nlist(Hstar = localproj, defree = defree)\n####### analysis of deviance function for logistic partially linear model ##########\n####### using Epanechnikov kernel, xgrid must be equally spaced testing no effect of\n####### xt x1 x2 linear part xt nonparametric part\nanodev.gplm.logit.chisq = function(y, x1, tgrid, h) {\ng.glm = glm(factor(y) ~ x1 + x2 + xt, family = binomial(link = \"logit\"))\n# get initial value for parameters\nbcoeff = g.glm$coeff[2:3]\ng0.glm = glm(factor(y) ~ x1 + x2, family = binomial(link = \"logit\"))\nsamplen = length(y)\n# normalizing weights integrated over tgrid to 1\ngridlength = length(tgrid)\ngridint = tgrid[2] - tgrid[1]\nbigK = matrix(0, ncol = gridlength, nrow = samplen)\nfor (i in 1:samplen) bigK[i, ] = (epkernel(((xt[i] - tgrid)/h))/h)\nbigKK = (diag(adjvect, nrow = samplen, ncol = samplen) %*% bigK)\n# use local likelihood\nd4.dev = rep(0, length(tgrid))\nd4.coeff = matrix(0, nrow = length(tgrid), ncol = 2)\nnitrt = 0\nbdiff = 100\n# iteration begins\nwhile ((bdiff > 0.001) & (nitrt < 30)) {\nnitrt = nitrt + 1\nfor (j in 1:length(tgrid)) {\nxt.local = xt - tgrid[j]\nkweights = bigKK[, j]\n\n# keep parametric part fixed, and fit local linear for xt set maxit=5, it is\n# enough to update the values offset(I(1.3 * X1))\nd4.glm = glm(factor(y) ~ offset(bcoeff[1] * x1) + offset(bcoeff[2] *\nx2) + xt.local, family = binomial(link = \"logit\"), weights = kweights,\ncontrol = glm.control(maxit = 5))\nd4.dev[j] = d4.glm$dev\nd4.linear[, j] = (d4.glm$coeff[1] + d4.glm$coeff[2] * xt.local) *\nkweights\n}\n# j loop grid\n\nthetastar = (apply(d4.linear, 1, sum) * gridint)\n# keep nonparametric part fixed, and fit linear terms for x1 and x2 set\n# maxit=5, it is enough to update the values\npara.glm = glm(factor(y) ~ -1 + x1 + x2 + offset(thetastar), family = binomial(link = \"logit\"),\ncontrol = glm.control(maxit = 5))\nbdiff = sum(abs(para.glm$coeff - bcoeff))\nbcoeff = para.glm$coeff\n# end of while\n# usually, the algorithm converges\nnotconv = 0\n# if nitrt==30 then non-convergence\nif (nitrt == 30) {\nprint(\"ERROR\")\nnotconv = 1\nintDev = sum(d4.dev) * gridint\nteststat0 = -(intDev - g0.glm$dev)\nd4.H = proj.ep.symmetric(xt, tgrid, h)\ndegfree = d4.H$defree\nchipvalue = 1 - pchisq(teststat0, degfree - 1)\nlist(intDev = intDev, teststat0 = teststat0, bcoeff = bcoeff, thetastar = thetastar,\ndegfree = degfree, chipvalue = chipvalue, notconv = notconv)\n############################################################# logistic partial linear cos example 4 cos a=1.5 n=200\nset.seed(20121003)\n# a=1.5 n=100 set.seed(20150208) cos a=1 n=200 set.seed(20121024) cos a=1\n# n=200 set.seed(20121019)\n# sample size 100 and 200\nsamplen = 200\nNsim = 5000\n# try 5 values of bandwidth\nhchoice = c(0.15, 0.2, 0.4)\nhlength = length(hchoice)\nnlnrcov = 2\n# grid points on [-0.5,1]\ntgrid = seq(-0.5, 1, 0.005) #length 301\n# initialization\nintD = matrix(0, nrow = Nsim, ncol = hlength)\n# to check convergence\nconv = matrix(1, nrow = Nsim, ncol = hlength)\nmhat = matrix(0, nrow = samplen, ncol = (hlength * Nsim))\n# set the initial value as the smallest bandwidth\nAICch = rep(0.15, Nsim)\nAICcpvalue = rep(0.99999, Nsim)\n# Horowitz and Spokoiny (2001) set the initial value as the smallest bandwidth\nadaptiveh = rep(0.15, Nsim)\n# for gam in mgcv\ngam.p0 = rep(0, Nsim)\na = 1.5\nfor (k in 1:Nsim) {\ncheckdata = rep(0, length(tgrid))\n# to simulate correlated data rho=0.3\nvarmat = matrix(c(1, 0.3, 1), 2, 2)\n########## random design check if every neighborhood has at least 5 points at smallest\n########## h #############\nwhile (any(checkdata <= 5)) {\nxdata = mvrnorm(samplen, rep(0, 2), varmat)\nxtmp = pnorm(xdata[, 2])\nxt = 1.5 * (xtmp - min(xtmp))/(max(xtmp) - min(xtmp)) - 0.5\n# so that xt is unfirom on (-0.5,1) with min=-0.5 and max=1\ncheckdata = countdata(xt, tgrid, 0.15)\nx2 = 0.5 * xdata[, 1]\nx1 = ifelse((runif(samplen, -1, 1)) > 0, 1, -1)\nt.order = order(xt)\nxt = xt[t.order]\nincvec = rep(1, samplen)\nxtt = cbind(incvec, xt)\n# least squares projection matrix of intercept and xt\nproj.li.xt = xtt %*% (solve(t(xtt) %*% xtt)) %*% t(xtt)\n# to make z1 (x1.th) and z2 (x2.th) orthogonal to xt\nx1.th = (diag(1, nrow = samplen, ncol = samplen) - proj.li.xt) %*% x1\nbeta1 = 0.1\ntruemt = a * cos(2 * pi * xt)\neta = -1 + beta1 * x1.th + beta2 * x2.th + truemt\np0 = exp(eta)/(1 + exp(eta))\ny0 = as.integer(rbinom(samplen, 1, p0))\nt.range = max(xt) - min(xt)\nx2xtcorr[k] = cor(x2, xt)\nd4 = data.frame(x1.th, x2.th, xt, y0)\nnames(d4) = c(\"x1.th\", \"xt\", \"y0\")\ntgrid = seq(-0.5, 1, 0.005) #length 301\nAICcmin = 10\nfor (hi in 1:hlength) {\nh = hchoice[hi]\n# call the function\nsemilogit = anodev.gplm.logit.chisq(d4$y0, d4$x1.th,\ntgrid, h)\n# proposed chi-square test statistic\nteststat[k, hi] = semilogit$teststat0\nmdiff[k, hi] = sum((semilogit$thetastar - truemt)^2)/samplen\n# if non-convergent\nif (semilogit$notconv == 1)\nconv[k, hi] = 0\ndegfee[k, hi] = semilogit$degfree\nbb[k, ((hi * 2 - 1):(hi * 2))] = semilogit$bcoeff\n# b1, b2 for h=0.15, then b1, b2 for h=0.2, ...\nmhat[, ((k - 1) * hlength + hi)] = semilogit$thetastar\n# mhat for h1, ... h5\n# calculate AICc and find AICcmin\nAICc[k, hi] = log(intD[k, hi]/samplen) + 2 * (degfee[k, hi] + 1)/(samplen -\ndegfee[k, hi] - 2)\nif ((AICc[k, hi] < AICcmin) && (conv[k, hi] == 1)) {\nAICcmin = AICc[k, hi]\nAICch[k] = h\n# find the smallest pvalue based on Horowitz and Spokoiny (2001)\nif ((pvalue[k, hi] < adaptivepvalue[k]) && (conv[k, hi] == 1)) {\nadaptivepvalue[k] = pvalue[k, hi]\n# end hi bandiwdth use convergent h to select AICch\nif (any(conv[k, ] == 1)) {\nfindaicc = (hchoice == AICch[k])\nAICcpvalue[k] = pvalue[k, findaicc]\n} else {\ncat(\"all h values fail to converge\\n\")\n# mgcv package\nb.gam = gam(d4$y0 ~ d4$x1.th + d4$x2.th + s(d4$xt), method = \"REML\", family = binomial(link = \"logit\"))\ngam.p0[k] = summary(b.gam, p.type = 0)$s.pv\ncat(k, \"/\")\n# k loop\ntestresults1 = round(matrix(c(hchoice, colSums(pvalue <= 0.05)/Nsim), nrow = 2,\nncol = hlength, byrow = TRUE), 6)\ntestresults1\ntestresults2 = round(matrix(c(mean(AICch), sd(AICch), sum(AICcpvalue < 0.05)/Nsim,\nmean(adaptiveh), sum(adaptivepvalue < 0.05)/Nsim), nrow = 2,\nbyrow = TRUE), 6)\ntestresults2\ndegresults = round(matrix(c(mean(degfee[, 1]), sd(degfee[, 1]), mean(degfee[,\n2]), sd(degfee[, 2]), mean(degfee[, 3]), sd(degfee[, 3]), mean(degfee[, 4]),\n6)\ndegresults\nprint(table(AICch))\nprint(table(adaptiveh))\nprint(\"gam\")\nprint(sum(gam.p0 <= 0.05))/Nsim\nprint(summary(gam.edf))\nprint(sd(x2xtcorr))\n#### check convergence for fixed bandwidth\nprint(colSums(conv))\ntestresults3 = round(matrix(c(hchoice, ((colSums((pvalue * conv > 0) & (pvalue *\nconv <= 0.05)))/colSums(conv))), nrow = 2, ncol = hlength, byrow = TRUE),\nprint(testresults3)\n#### chcek convergence for the smallest bandwidth\nprint(sum(conv[, 1]))\ntestresults4 = rep(0, hlength)\ntestresults4[1] = sum((pvalue[, 1] * conv[, 1]) <= 0.05)/sum(conv[, 1])\nprint(round(matrix(c(hchoice, testresults4), nrow = 2, ncol = hlength, byrow = TRUE),\n6))\n", "output_sequence": "Computes the percent of rejection in example 4 when z1 and z2 are orthogonal to x (in the JBES paper Analysis of Deviance for Hypothesis Testing in Generalized Partially Linear Models)."}, {"input_sequence": "Author: Dennis Koehn, Linxi Wang, Mingyang Li ; ########################## Encode Time Variables #######################\ninclude_quarter_dummies = function(X_com) {\nX_com$SoldSecondQuartal = ifelse(X_com$MoSold %in% 4:6, 1, 0)\nX_com$MoSold = NULL\nreturn(X_com)\n}\n", "output_sequence": "Process the raw data for further analysis, including impute NA values, convert category variables into numeric."}, {"input_sequence": "Author: Dennis Koehn, Linxi Wang, Mingyang Li ; ########################################## Feature Engineering #####################################################\nlibraries = \"plyr\"\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# replace NA with None if NA mean that there is None\nreplace_NA_zero = function(x) {\nx = factor(x, levels = c(levels(x), 0))\nx[is.na(x)] = 0\nreturn(x)\n}\n# replace variable with the standard rating scheme i.e. Ex,Gd,TA,Fa,Po with numeric ratings\nreplace_standard_ratings = function(x) {\nx = replace_NA_zero(x)\nx = revalue(x, c(Ex = 5, Gd = 4, TA = 3, Fa = 2, Po = 1))\n# replace basement ratings with numerical ratings\nunifinish_dummy = function(X_com) {\nX_com$BsmtFinType1_unfinished = ifelse(X_com$BsmtFinType1 == \"Unf\", 1, 0)\nreturn(X_com)\n# function to replace ratings for Bsmt..\nreplace_base_ratings = function(x) {\nx = revalue(x, c(GLQ = 5, ALQ = 4, BLQ = 3, Rec = 2, LwQ = 1, Unf = NA))\n# function to replace all ratings that are given in string into numeric rating scores\nreplace_ratings = function(X_com) {\n# replace standard rating schema\nname_standard_rating = c(\"PoolQC\", \"GarageCond\", \"FireplaceQu\", \"KitchenQual\",\n\"HeatingQC\", \"BsmtCond\", \"ExterCond\", \"ExterQual\")\nidx_standard = which(colnames(X_com) %in% name_standard_rating)\nX_replace = X_com[, idx_standard]\nX_replaced = data.frame(lapply(X_replace, replace_standard_ratings))\nX_com = cbind(X_com[, -c(idx_standard)], X_replaced)\n\n# replace basement schema (Bsmt...)\nX_com = unifinish_dummy(X_com)\nname_base_rating = c(\"BsmtFinType1\",\nidx_base = which(colnames(X_com) %in% name_base_rating)\nX_replace = X_com[, idx_base]\nX_replaced = data.frame(lapply(X_replace, replace_base_ratings))\nX_com = cbind(X_com[, -c(idx_base)], X_replaced)\n# replace NA in variable where they actuale mean none instead of na, we replace value with 0\nname_na_means_none = c(\"MiscFeature\", \"Fence\", \"GarageFinish\", \"Alley\")\nidx_na_means_none = which(colnames(X_com) %in% name_na_means_none)\nX_replace = X_com[, idx_na_means_none]\nX_replaced = data.frame(lapply(X_replace, replace_NA_zero))\nX_com = cbind(X_com[, -c(idx_na_means_none)], X_replaced)\n# replace ratings in BsmtExposure\nX_com$BsmtExposure = revalue(X_com$BsmtExposure, c(Gd = 3, Av = 2, Mn = 1, No = 0))\n", "output_sequence": "Process the raw data for further analysis, including impute NA values, convert category variables into numeric."}, {"input_sequence": "Author: Dennis Koehn, Linxi Wang, Mingyang Li ; ################### delete near zero variance #######################\nlibrary(caret)\ndelect_nz_variable = function(X) {\nnear_zero_variance = nearZeroVar(X)\nreturn(X[, -near_zero_variance])\n}\n", "output_sequence": "Process the raw data for further analysis, including impute NA values, convert category variables into numeric."}, {"input_sequence": "Author: Dennis Koehn, Linxi Wang, Mingyang Li ; ################### Quick Data Cleaning ####################################\nlibraries = c(\"Hmisc\", \"mice\", \"VIM\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# function to compute the mode of a variable (used to impute categorical variables) input: x -\n# a variable output: mode of the variable x as a character\nMode = function(x) {\nux = unique(x)\nas.character(ux[which.max(tabulate(match(x, ux)))])\n}\n# function to impute a variable (numerical with median and categoricals with mode) input: x -\n# a variable output: x with imputed NAs by either the mean or the mode\nmedian_mode_impute = function(x) {\nif (any(is.na(x))) {\nif (is.numeric(x) & !is.factor(x)) {\nx = impute(x, fun = mean)\nreturn(as.numeric(x))\n} else {\nx = impute(x, fun = Mode)\nreturn(as.factor(x))\n}\n}\nreturn(x)\n# function to impute a whole feature matrix (numerical with median and categoricals with mode)\n# input: X - a data.frame output: data.frame X with imputed NAs by either the mean or the mode\nnaive_imputation = function(X) {\ntemp = X\n# apply function on all variable using lappy\ntemp = data.frame(temp, stringsAsFactors = FALSE)\ntemp = as.data.frame(lapply(temp, median_mode_impute))\nreturn(temp)\n# usage\nX_imputed = naive_imputation(X_com)\n# function to impute a whole feature matrix using the mice package defaults: numerical with\n# pmm and categoricals with log/poly reg, This take a while. The imputed data set can be found\nmice_imputation = function(x) {\ntemp = mice(x, MaxNWts = 5000) # MaxNWt from the nnet package\ntemp = complete(temp, 1)\n", "output_sequence": "Process the raw data for further analysis, including impute NA values, convert category variables into numeric."}, {"input_sequence": "Author: Dennis Koehn, Linxi Wang, Mingyang Li ; #################### scaling data for regression model and pca ################\n# this function performs min(0) max(1) scaling meaning that all numeric are scale between 0\n# and 1 note: scaling will be bad if no outlier detection was performed previously input: a\n# variable output: a scale version of the variable in case it is numerical\nmin_max_scaling = function(x) {\nif (is.numeric(x)) {\nreturn((x - min(x))/(max(x) - min(x)))\n} else {\nreturn(x)\n}\n}\n# this function normalizes the data i.e. x - mu/sigma note: scaling will be bad if no outlier\n# detection was performed previously input: a variable output: a scale version of the variable\n# in case it is numerical\ngaussian_scaling = function(x) {\nreturn(scale(x, center = TRUE, scale = TRUE))\n# this function scales the all numerical variable in the feature matrix input: X-no_outliers -\n# feature matrix without NAs and outliers scale_method - a string that determines the scaling\n# method, if 'min_max' then min_max_scaling function is applied else gaussian_scaling is\n# applied output: scale feature matrix\nscale_data = function(X_no_outliers, scale_method) {\nif (scale_method == \"min_max\") {\nreturn(as.data.frame(lapply(X_no_outliers, min_max_scaling))[, -1]) # without id variable\nif (scale_method == \"gaussian\") {\nreturn(as.data.frame(lapply(X_no_outliers, gaussian_scaling))[, -1])\n", "output_sequence": "Process the raw data for further analysis, including impute NA values, convert category variables into numeric."}, {"input_sequence": "Author: Dennis Koehn, Linxi Wang, Mingyang Li ; ######################## Function to convert a categorical variable into dummies ######################## It also\n######################## filters out dummies that are linear dependent to other according to the vif score to avoid\n######################## multicolinearity assumes that you have installed the following libraries\nlibraries = c(\"dummies\", \"usdm\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\ncat_to_dummy = function(x, vif_threshold = 10) {\n# check if variable is not numeric or has less than 5 levels else convert\nif (!any(is.numeric(x)) | length(unique(x)) <= 4) {\n# convert into dummy and exclude randomly drawn index\nx = as.data.frame(dummy(x))\n# get number of column after hot encoding\ninitial_colnumber = ncol(x)\n# check if more than one dummy remains\nif (ncol(x) > 2) {\n# drop dummy that are linear dependent with other variables exclude from the usdm package\n# delets all dummies with a higher score than the vif_threshold\nx = exclude(x, vifstep(x, th = vif_threshold))\n# if no column was delete we delete one randomnly to avoid dummy trap\nif (ncol(x) == initial_colnumber) {\nrandom_drop = sample(1:length(unique(x)), 1)\nx = x[, -random_drop]\n}\nif (ncol(x) == 2) {\nx = x[, -1]\n}\nreturn(x)\n}\n", "output_sequence": "Process the raw data for further analysis, including impute NA values, convert category variables into numeric."}, {"input_sequence": "Author: Dennis Koehn, Linxi Wang, Mingyang Li ; ############################ data processing ##################################\n### setwd('F:/PHD/IRTG/courses/SPL/Quantlet/data_processing')\nrm(list = ls())\ngraphics.off()\nbasic_preprocessing = function(X_com, y, scaler = \"gaussian\") {\nsource(\"replace_ratings.R\")\nsource(\"convert_categoricals.R\")\nsource(\"impute_data.R\")\nsource(\"encode_time_variables.R\")\nsource(\"impute_outliers.R\")\nsource(\"delete_nearzero_variables.R\")\nX_ratings = replace_ratings(X_com)\nX_imputed = naive_imputation(X_ratings)\nX_no_outlier = data.frame(lapply(X_imputed, iqr_outlier))\nX_time_encoded = include_quarter_dummies(X_no_outlier)\nX_scaled = scale_data(X_time_encoded, scale_method = scaler)\nX_encoded = data.frame(lapply(X_scaled, cat_to_dummy))\nX_com = delect_nz_variable(X_encoded)\n# remerge train data\nidx_train = c(1:length(y))\ntrain = cbind(X_com[idx_train, ], y)\nreturn(list(train = train, X_com = X_com, test = test)) # return without id\n}\ntrain = read.csv(\"ames_train.csv\", header = T)\n# split target variable and feature matrix\ny = train[, \"SalePrice\"] # target variable SalePrice\nX = train[, -81] # feature matrix without target variable\n# merge test and train features to get the complete feature matrix\nX_com = rbind(X, test)\n# apply preprocessing\nbasic_data = basic_preprocessing(X_com, y)\n# get the complete input matrix\nX_com = basic_data$X_com\nwrite.csv(X_com, \"inputMatrix_preprocessed.csv\")\n# get the training data with labels\ntrain = basic_data$train\nwrite.csv(train,\"train_preprocessed.csv\")\n# get the test data to be predicted in the same format\ntest = basic_data$test\nwrite.csv(train,\"test_preprocessed.csv\")\n# remove redundant variables and functions\nall_var = ls()\nall_var = all_var[all_var != \"basic_data\"]\nrm(list = c(all_var, \"all_var\"))\n### save workplace for further use\nsave.image(\"basic_processing.RData\")\n", "output_sequence": "Process the raw data for further analysis, including impute NA values, convert category variables into numeric."}, {"input_sequence": "Author: Dennis Koehn, Linxi Wang, Mingyang Li ; ######################### impout outlier with upper and lower bounds #####################3\niqr_outlier = function(x) {\nif (is.numeric(x)) {\nbp = boxplot(x)\nlower = bp$stats[1]\nx[x > upper] = upper\n}\nreturn(x)\n}\n# usage X_no_outlier = as.data.frame(lapply(X_imputed, iqr_outlier))\nz_outlier = function(x) {\nz = scale(x, center = TRUE, scale = TRUE)\nif (abs(z) > 3) {\n\n", "output_sequence": "Process the raw data for further analysis, including impute NA values, convert category variables into numeric."}, {"input_sequence": "Author: Jonas Klein ; # Clean up the workspace\nrm(list = ls())\nif(length(sessionInfo()$otherPkgs) > 0)\nlapply(paste(\"package:\", names(sessionInfo()$otherPkgs), sep = \"\"), detach,\ncharacter.only = TRUE, unload = TRUE, force = TRUE)\n# Load the raw data set for CA\nrawdat = \"P00000001-CA.csv\"\n# read data with header as the first row, fill the missing cell to produce a\n# complete empty column on the right\ndat0 = read.csv(rawdat, header = FALSE, fill = TRUE, stringsAsFactors = FALSE)\n# delete the empty column\ndat0 = dat0[, -ncol(dat0)]\n# make header out of the first row\nnames(dat0) = as.vector(unlist(dat0[1, ]))\n# delete the first row\ndat0 = dat0[-1,]\n# fix the numbering for the rows, which would otherwise start from 2\nrow.names(dat0) = NULL\n# Keep only relevant variables and drop the rest\ndat0 = dat0[, c(\"cand_nm\", \"contbr_nm\", \"contbr_occupation\",\n\"contb_receipt_amt\",\n\n# Better names for manipulation\nnames(dat0) = c(\"cand\", \"name\", \"zip\", \"occu\", \"amount\", \"date\")\n# Fix the zip codes as they are in 9 digit representation; we need 5 digits only\ndat0$zip = substr(dat0$zip, 1, 5)\n# No need for the candidates first name\ncand = strsplit(dat0$cand, \", \")\ndat0$cand = sapply(cand, \"[[\", 1)\n# Adjust variable types\ndat0$cand = as.factor(dat0$cand)\ndat0$zip = as.numeric(dat0$zip)\ndat0$occu = as.factor(dat0$occu)\ndat0$amount = as.numeric(dat0$amount)\n# dat0$date = as.Date(dat0$date, \"%d-%B-%y\")\n# you might have to adjust your local time settings of your R session. Otherwise\n# you might end up with lots of NAs\n# Sys.setlocale(\"LC_TIME\", \"en_US.UTF-8\")\n# Drop NA values\ndat0 = na.omit(dat0)\n# remove all zips that do not belong to CA\ndat0 = dat0[dat0$zip >= 90000 & dat0$zip <= 96162,]\n# Delete all negative donations, i.e. refunds\ndat0 = dat0[dat0$amount > 0,]\n# Add the party (D = democrats, R = republicans, TP = third party) of each\n# candidate to the data set\ndat0$party = \"TP\"\ndat0$party[dat0$cand %in% c(\"Bush\", \"Carson\", \"Christie\", \"Cruz\", \"Fiorina\",\n\"Gilmore\", \"Graham\", \"Huckabee\", \"Jindal\", \"Kasich\", \"Pataki\", \"Paul\",\n\"Perry\", \"Rubio\", \"Santorum\", \"Trump\", \"Walker\")] = \"R\"\ndat0$party[dat0$cand %in% c(\"Clinton\", \"Lessig\", \"O'Malley\", \"Sanders\",\n\"Webb\")] = \"D\"\n# save the prepared data set to a file\nwrite.csv(dat0, \"dat0.csv\", row.names = FALSE)\n", "output_sequence": "Prepares the FEC data set for California, i.e. the broken data set P00000001-CA.csv is loaded and transformed into a proper data frame. Moreover, column names and variable types are adjusted, invalid or NA values deleted and some more data manipulation is done."}, {"input_sequence": "Author: Maria Culjak ; getwd()\n\ninstall.packages(\"writexl\")\nlibrary(\"writexl\")\ninstall.packages(\"RND\")\nlibrary(\"RND\")\n##set the work directory\nsetwd(\"/repository\")\n##reading data\ndax=read.table(file=\"aexoptions.txt\",h=T)\ndim(dax)\nnames(dax)\n##clearing raw data - expiration date and observation date set\ndax.srpanj2m=subset(dax,(expiration==\"21.9.2018\")&(openint.c.23.03>0)&(openint.p.23.03>0)&(bid.c.23.03>0)&(bid.p.23.03>0))\ndim(dax.srpanj2m)\n## calls, strikes, puts\ncalls=(dax.srpanj2m[,\"bid.c.23.03\"]+dax.srpanj2m[,\"ask.c.23.03\"])/2\nstrike=dax.srpanj2m[,\"strike\"]\nmatplot(strike,cbind(puts,calls),type=\"b\",pch=19)\nlegend(\"topright\",c(\"Puts\",\"Calls\"),col=c(\"Black\",\"Red\"),pch=19)\nte=182/365\ns0=534.09\nextract.rates(calls=calls,puts=puts,k=strike,s0=s0,te=te)\nhf=read.table(file=\"hf219.txt\",header=T)\nstart=proc.time()\nMOE1(market.calls=calls,market.puts=puts,s0=s0,call.strikes=strike,put.strikes=strike,te=te,r=-0.00371,y=0,file.name=\"est\",lambda=1)\nend=proc.time()\nend - start\nMOE1<-function (market.calls, call.strikes, market.puts, put.strikes,\ncall.weights = 1, put.weights = 1, lambda = 1, s0, r, te,\ny, file.name = \"myfile\")\n{\nstrikes = intersect(call.strikes, put.strikes)\nif (length(strikes) < 10)\nstop(\"You must have at least 10 common strikes between the calls and puts.\")\nmln.obj = extract.mln.density(r = r, y = y, te = te, s0 = s0,\nmarket.calls = market.calls, call.strikes = call.strikes,\ncall.weights = call.weights, = put.weights,\nlambda = lambda, hessian.flag = F)\nmln.alpha.1 = mln.obj$alpha.1\nmln.meanlog.2 = mln.obj$meanlog.2\new.obj = extract.ew.density(r = r, y = y, te = te, s0 = s0,\nmarket.calls = market.calls, call.strikes = call.strikes,\ncall.weights = call.weights, lambda = lambda, hessian.flag = F)\new.sigma = ew.obj$sigma\nshimko.obj = extract.shimko.density(market.calls = market.calls,\ncall.strikes = call.strikes, r = r, y = y, te = te, s0 = s0,\nlower = -10, upper = +10)\na0 = shimko.obj$implied.curve.obj$a0\nmin.x = min(put.strikes, call.strikes)\nx = seq(from = min.x, to = max.x, length.out = 100)\ny.mln = dmln(x = x, alpha.1 = mln.alpha.1, meanlog.1 = mln.meanlog.1,\nmeanlog.2 = mln.meanlog.2, sdlog.1 = mln.sdlog.1, sdlog.2 = mln.sdlog.2)\ny.ew = dew(x = x, r = r, y = y, te = te, s0 = s0, sigma = ew.sigma,\nskew = ew.skew, kurt = ew.kurt)\ny.shimko = dshimko(r = r, te = te, s0 = s0, k = x, y = y,\na0 = a0, a1 = a1, a2 = a2)\nmax.y = max(y.mln, y.ew, y.shimko) * 1.05\nif (!is.numeric(max.y))\nmax.y = 1\ncut.off = (min(x) + max(x))/2\nmax.ind = which.max(y.mln)\nif (x[max.ind] > cut.off)\nlegend.location = \"topleft\"\npar(mar = c(5, 5, 5))\nmatplot(x, cbind(y.mln, y.ew, y.shimko), type = \"l\",\ncol = c(\"black\", \"blue\", \"red\"), xlab = \"Strikes\",\nylab = \"Density\", lwd = c(2, 2, 2), lty = c(1,\n1, 1), cex.axis = 1.25, cex.lab = 1.25, ylim = c(0,\nmax.y))\nlegend(legend.location, legend = c(\"MLN\", \"EE\", \"SM\", \"TD\"), col = c(\"black\", \"blue\",\n\"red\", \"green\"), lwd = c(2, 2, 2,2), lty = c(1,\n1, 1,1), bty = \"n\", cex = 1.25)\nlines(density(hf[,\"aex\"], bw=20, kernel = \"gaussian\"), col =\"green\")\nmln.predicted.puts = price.mln.option(r = r, te = te, y = y,\nk = put.strikes, alpha.1 = mln.alpha.1, meanlog.1 = mln.meanlog.1,\nmeanlog.2 = mln.meanlog.2, sdlog.1 = mln.sdlog.1, sdlog.2 = mln.sdlog.2)$put\nmln.predicted.calls = price.mln.option(r = r, te = te, y = y,\nk = call.strikes, alpha.1 = mln.alpha.1, meanlog.1 = mln.meanlog.1,\nmeanlog.2 = mln.meanlog.2, sdlog.1 = mln.sdlog.1, sdlog.2 = mln.sdlog.2)$call\new.predicted.puts = price.ew.option(r = r, te = te, s0 = s0,\nk = put.strikes, y = y, sigma = ew.sigma, skew = ew.skew,\nkurt = ew.kurt)$put\new.predicted.calls = price.ew.option(r = r, te = te, s0 = s0,\nk = call.strikes, y = y, sigma = ew.sigma, skew = ew.skew,\nkurt = ew.kurt)$call\nshimko.predicted.puts = numeric(length(put.strikes))\nfor (i in 1:length(put.strikes)) {\nshimko.predicted.puts[i] = price.shimko.option(r = r,\nte = te, s0 = s0, k = put.strikes[i], y = y, a0 = a0,\n}\nshimko.predicted.calls = numeric(length(put.strikes))\nfor (j in 1:length(put.strikes)) {\nshimko.predicted.calls[j] = price.shimko.option(r = r,\nte = te, s0 = s0, k = call.strikes[j], y = y, a0 = a0,\ntmp.data.calls = cbind(market.calls, call.strikes, mln.predicted.calls,\nshimko.predicted.calls)\ntmp.data.puts = cbind(market.puts, put.strikes, mln.predicted.puts,\nshimko.predicted.puts)\ntmp.data=rbind(tmp.data.calls,tmp.data.puts)\ncolnames(tmp.data) = c(\"market\", \"strikes\", \"mln\", \"ew\", \"shimko\")\ndata.calls = as.data.frame(tmp.data)\n#write_xlsx(data.calls, file = paste(file.name, \"est.xlsx\",\n# sep = \"\"), sheetName=\"estimatedprices\", append=FALSE)\ntmp.parameters=rbind(mln.alpha.1, mln.meanlog.1, ew.sigma, ew.skew, ew.kurt, a0, a2)\ntmp.parameters=as.data.frame(tmp.parameters)\n#write_xlsx(tmp.parameters, file = paste(file.name, \"est.xlsx\",\n# sep = \"\"), sheetName=\"parameters\", append=TRUE)\ntmp.densit=cbind(x,y.mln, y.ew, y.shimko)\ntmp.densit=as.data.frame(tmp.densit)\n#write_xlsx(tmp.densit, file = paste(file.name, \"est.xlsx\",\n# sep = \"\"), sheetName=\"densities\", append=TRUE)\nwrite_xlsx(list(data.calls = data.calls, tmp.parameters = tmp.parameters, tmp.densit = tmp.densit), \"est.xlsx\")\nout = list(mln.alpha.1 = mln.alpha.1,\nmln.meanlog.1 = mln.meanlog.1, = mln.meanlog.2,\new.sigma = ew.sigma, ew.skew = ew.skew, ew.kurt = ew.kurt,\na0 = a0, a1 = a1, a2 = a2)\nout\n}\n##kernel density calibration\nhf=read.table(file=\"hf178.txt\",header=T)\nplot(density(hf[,\"mib\"], kernel = \"gaussian\", bw=700))\nwarnings()\n", "output_sequence": "Predictive accuracy analysis of option pricing models using high frequency data"}, {"input_sequence": "Author: Maria Culjak ; # -*- coding: utf-8 -*-\n\"\"\"\nCreated on Wed Apr 3 10:53:03 2019\n\n@author: Philipp Schneider, Miriam K\u00f6berl\n# treasure island wordcloud, based on the christmas song wordcloud by verani.\n#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\nCreated on Sun May 6 19:10:18 2018\n@author: verani\n#Please install these modules before you run the code\n#!pip install wordcloud\n#############################################################\n# Part I: Pirates wordcloud\nimport matplotlib.pyplot as plt\nimport re\nfrom os import path\nfrom wordcloud import WordCloud, STOPWORDS\nimport numpy as np\n# Please make sure that all the required files are in your working directory\n# or change the working directory to your path!\nd = os.getcwd()\nraw_text= open(path.join(d, 'schatzinselutf8.txt'), encoding = \"utf8\").read()\nraw_text= raw_text.replace(\"\\n\",\" \")\ncleantextprep = str(raw_text)\n# keep only letters, numbers and whitespace\nexpression = \"[^a-zA-Z0-9 ]\"\ncleantextCAP = re.sub(expression, '', cleantextprep) # apply regex\ncleantext = cleantextCAP.lower() # lower case\ntext_file = open(\"Output_total.txt\", \"w\")\ntext_file.write(str(cleantext))\ntext_file.close()\nroot_path = os.getcwd()\n# Read the whole text.\nwith open(path.join(root_path, 'Output_total.txt'), 'r', encoding='utf-8', errors='ignore') as outout_file:\ntext = outout_file.readlines()\n# Mask\nxmas_tree_pic = np.array(Image.open(path.join(root_path, \"pirate-ship.jpg\")))\n# Optional additional stopwords\nstopwords = set(STOPWORDS)\n# Construct Word Cloud\n# no backgroundcolor and mode = 'RGBA' create transparency\nwc = WordCloud(max_words=100, mask=xmas_tree_pic,\nstopwords=stopwords, mode='RGBA', background_color=None)\n# Pass Text\nwc.generate(text[0])\n# store to file\nwc.to_file(path.join(root_path, \"wordcloud_treasure_island.png\"))\nplt.imshow(wc, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()\n# Part II: Alpha Centauri wordcloud\n#\n# Please change the working directory to your path!\nwith open(path.join(root_path, 'BlurbsEdit.txt'), 'r', encoding='utf-8', errors='ignore') as outout_file:\nsuns_pic = np.array(Image.open(path.join(root_path, \"peace3.jpg\")))\nstopwords.add(\"will\")\nwc = WordCloud(max_words=1000, mask=suns_pic,\nwc.to_file(path.join(root_path, \"alphacentauriwords.png\"))\n# to show the picture\nplt.axis(\"off\")#\n", "output_sequence": "Predictive accuracy analysis of option pricing models using high frequency data"}, {"input_sequence": "Author: Anna Pacher, Patrick Plum, Kathrin Spendier ; # -*- coding: utf-8 -*-\n\"\"\"\nCreated on Wed Apr 3 08:34:12 2019\n@author: Anna Pacher, Patrick Plum, Kathrin Spendier\n# importing the necessery modules\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nimport csv\nfrom os import path\nimport numpy as np\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\n# Please change the working directory to your path!\nroot_path = os.getcwd()\n#import german stopwords (these are words that will be excluded from the wordcloud,\n#for instance articles and prepositions)\nstopwords = stopwords.words('german')\n# file object is created\n# here it can be chosen which text should be displayed\nfile_ob = open(path.join(root_path, 'musil.txt'), encoding='utf-8')\n# 1. text: Chapter one and two of Musil's book \"Der Mann ohne Eigenschaften\" taken form http://musilonline.at/musiltext/\n# 2. https://www.songtexte.com/songtext/nena/99-luftballons-63dcfa57.html\n# 3. taken from http://www.gedichte-zitate.com/gedichte.html\n\n# reader object is created\nreader_ob = csv.reader(file_ob)\n# contents of reader object is stored .\n# data is stored in list of list format.\nreader_contents = list(reader_ob)\n# empty string is declare\ntext = \"\"\n# iterating through list of rows\nfor row in reader_contents :\n\n# iterating through words in the row\nfor word in row :\n# concatenate the words\ntext = text + \" + word\n\n\n# Mask\nman_pic = np.array(Image.open(path.join(root_path, \"manwithhat.jpg\")))\n#balloon = np.array(Image.open('balloon.jpeg'))\n#star = np.array(Image.open(path.join(root_path, \"star.jpeg\")))\n#########################################\n# remove stopwords from WordCloud, show 200 words in the wordcloud .\nwordcloud = WordCloud(width=480, height=480, max_words=200,\nstopwords=stopwords,\nmask= man_pic,\nmode='RGBA',background_color=None ).generate(text)\n# plot the WordCloud image\nplt.figure()\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.margins(x=0, y=0)\nplt.show()\n# store to file\nwordcloud.to_file(\"musil_words.png\")\nwordcloud = WordCloud(width=480, height=480,\nstopwords=stopwords,\nmask= man_pic,\nbackground_color=\"violet\").generate(text)\n", "output_sequence": "Creates a word cloud for one of the three given texts (an Austrian novel - \"Der Mann ohne Eigenschaften\" by Robert Musil, a songtext - \"99 Luftballons\" by Nena or a poem - \"Gemeinsam\" by Hanna Schnyders) in the shape of a man with hat, a balloon or a star. A word cloud is a data visualization technique used for representing text data in which the size of each word indicates its frequency or importance."}, {"input_sequence": "Author: Anna Pacher, Patrick Plum, Kathrin Spendier ; # 20190505 Schneider Philipp\n# Statistisches Lernen\n# Abschlussprojekt\n\nrm(list=ls())\n# Workspace leeren.\ninstall.packages(\"randomForest\",\"e1071\",\"gmodels\")\nlibrary(randomForest)\nlibrary(e1071)\nlibrary(gmodels)\nurl=\"http://freakonometrics.free.fr/german_credit.csv\"\ncredit=read.csv(url, header = TRUE, sep = \",\")\nstr(credit)\nF=c(1,2,4,5,7,8,9,10,11,12,13,15,16,17,18,19,20)\n# Diese Attribute sind kategorisch, werden aber noch wie numerische behandelt.\nfor(i in F) credit[,i]=as.factor(credit[,i])\n# Wandelt sie in kategorische Attribute um.\nset.seed(23)\nselection = sample(1:1000,700,replace=FALSE)\ntrain = credit[selection,]\n# Dies ergibt die Trainingsdaten.\ntest = credit[-selection,]\n# Der Rest wird als Testdatensatz definiert.\nmodel1 = randomForest(Creditability ~ .,data = train)\n# Das erste Random Forest Modell wird mit Standardeinstellungen gebildet.\npred1 = predict(model1, test, type=\"class\")\ntable(pred1==test[,1])/300\n# Rate an inkorrekt / korrekt klassifizierten Objekten.\nCrossTable(x = test[,1], y = pred1, prop.chisq = FALSE)\nmodel1$importance\ncredit = credit[,1:14]\n# Die unwichtigen Attribute werden entfernt.\ntrain2 = credit[selection,]\nmtrygrid=c(4,5,6,7,8,9,10)\nngrid=c(500,600,700,800,900,1000)\n# ACHTUNG!\n# Die Tune-Funktion kann einige Minuten in Anspruch nehmen!\ntune.out=tune(randomForest, Creditability~., data=train2, ranges=list(mtry=mtrygrid,ntree=ngrid))\ntune.out\n# Das Ergebnis der Gittersuche mit Crossvalidation.\nmodel2 = randomForest(Creditability~ ., data = train2, mtry = 10, ntree = 500)\n# Das zweite Modell wird nun mit den gefundenen Parametern gebildet.\npred2 = predict(model2, test2, type=\"class\")\ntable(pred2 == test2[,1])/300\n#Konfusionsmatrix:\nCrossTable(x = test2[,1], y = pred2, prop.chisq = FALSE)\n", "output_sequence": "Creates a word cloud for one of the three given texts (an Austrian novel - \"Der Mann ohne Eigenschaften\" by Robert Musil, a songtext - \"99 Luftballons\" by Nena or a poem - \"Gemeinsam\" by Hanna Schnyders) in the shape of a man with hat, a balloon or a star. A word cloud is a data visualization technique used for representing text data in which the size of each word indicates its frequency or importance."}, {"input_sequence": "Author: Sigbert Klinke ; # please use \"Esc\" key to jump out the run of Shiny app\n# clear history and close windows\n# rm(list=ls(all=TRUE))\ngraphics.off()\n\n# General settings\nlibraries = c(\"shiny\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# please set working directory setwd('C:/...')\n# windows setwd('/Users/...') #\n# mac os setwd('~/...') # linux\n##############################################################################\n# Default parameters\nmmstat.getValues = function(local, ...) {\nret <<- list(...)\nfor (name in names(ret)) {\nif (is.null(ret[[name]]) || (length(ret[[name]]) == 0)) {\nif (is.null(local[[name]])) {\nstopif(is.null(mmstat$UI[[name]]$value) &&\npaste0(\"mmstat.getValues: no default value(s) for \\\"\", name, \"\\\"\"))\nif (is.null(mmstat$UI[[name]]$value)) {\nret[[name]] = mmstat$UI[[name]]$selected\n} else {\nret[[name]] = mmstat$UI[[name]]$value\n}\n} else {\nret[[name]] = local[[name]]\n}\nif (mmstat$UI[[name]]$call == \"mmstat.sliderInput\") {\nif ((compareVersion(mmstat$shiny, \"0.11\") >= 0) &&\n!is.null(mmstat$UI[[name]]$ticks)) {\nret[[name]] = ret[[name]] + 1\n}\nret\n}\n# Datasets and variables\nmmstat.getDataNames = function(...) {\nis.binary = function(v) {\nx = unique(v)\nlength(x) - sum(is.na(x)) == 2L\n}\n\nfiles = list(...)\nif (length(files) == 0)\nreturn(names(mmstat$dataset)[1])\ndataset = vector(\"list\", length(files))\nnames(dataset) = files\nmmstat$dataset <<- dataset\nfor (i in seq(files)) {\nfile = files[[i]]\nif (is.null(mmstat$dataset$file)) {\ndata = readRDS(paste0(file, \".rds\"))\nif (class(data) == \"data.frame\") {\nallvars = names(data)\nmmstat$dataset[[file]] <<- list(data = data,\nallvars = allvars,\nnumvars = allvars[sapply(data, is.numeric)],\nif (class(data) == \"ts\") {\nallvars = colnames(data)\ngettext(names(mmstat$dataset), \"name\")\nmmstat.getVarNames = function(dataname, vartype, which = NULL) {\nvars = NULL\nif (vartype == \"numeric\")\nvars = \"numvars\"\nif (vartype == \"ordered\")\nvars = \"ordvars\"\nif (vartype == \"factor\")\nvars = \"facvars\"\nif (vartype == \"binary\")\nvars = \"binvars\"\nstopif(is.null(vars),\n\"mmstat.getVarNames: Variable type \\\"%s\\\" unknown\")\nif (is.null(which))\nreturn(gettext(mmstat$dataset[[dataname]][[vars]], \"name\"))\nif (length(which == 1))\nreturn(gettext(mmstat$dataset[[dataname]][[vars]][which]))\nmmstat.getDatasets = function(...) {\nfiles = list(...)\ndataset = vector(\"list\", length(files))\nnames(dataset) = files\nmmstat$dataset <<- dataset\nfor (i in seq(files)) {\nfile = files[[i]]\ndata = readRDS(paste0(file, \".rds\"))\nif (class(data) == \"data.frame\") {\nallvars = names(data)\nmmstat$dataset[[file]] <<- list(data = data,\nallvars = allvars,\nnumvars = allvars[sapply(data, is.numeric)],\nif (class(data) == \"ts\") {\nallvars = colnames(data)\nnumvars = allvars)\nnames(dataset)[1]\nmmstat.attrVar = function(var, type, index = NULL) {\nret = var\nif (is.null(index))\nindex = 1:length(var$values)\nelse {\nret$values = var$values[index]\nret$n = length(index)\nif (type == \"numvars\") {\nif (is.null(var$jitter))\nret$jitter = runif(length(index)) else\nret$jitter = var$jitter[index]\nret$mean = mean(var$values[index], na.rm = T)\nret$quart = quantile(var$values[index], c(0.25, 0.75), na.rm = T)\nret$max = max(var$values[index], na.rm = T)\nif ((type == \"binvars\") || (type == \"ordvars\") || (type == \"facvars\")) {\nret$tab = table(var$values[index], useNA = \"ifany\")\nret$prop = prop.table(ret$tab)\nmmstat.getDatasetNames = function() {\ngettext(mmstat$dataset[[name]]$allvars, \"name\")\nmmstat.getVar = function(dataname = NULL, varname = NULL, vartype = NULL, na.action = na.omit) {\nmatch.name = function(needle, haystack) {\nif (is.null(needle))\nreturn(NA)\nreturn(match(needle, haystack))\n# which variable type do we need\nif (is.null(vartype))\nvartype = mmstat$vartype\ndeftype = c(\"numvars\", \"binvars\", \"numeric\",\n\"binary\", \"ordered\", \"factor\")\ntype = pmatch(vartype, deftype)\nstopif(is.na(type), paste0(\"mmstat.getVar: Unknown variable type: \", vartype))\ntype = deftype[type]\nif (type == \"numeric\")\ntype = \"ordvars\"\nif (type == \"factor\")\ntype = \"binvars\"\n# get dataset\npos = match.name(dataname, names(mmstat$dataset))\nif (is.na(pos))\npos = 1 # assume that all datasets contain\ndataname = names(mmstat$dataset)[pos]\nif (is.null(mmstat$dataset[[dataname]]))\nstop(sprintf(gettext(\"Data file '%s' is missing\"), dataname))\n# get variable\npos = match.name(varname, mmstat$dataset[[dataname]][[type]])\npos = 1\n# assume that every datasets contains a variable of correct type\nvarname = mmstat$dataset[[dataname]][[type]][pos]\n# get var values\ndataset = mmstat$dataset[[dataname]]$data\nif (class(dataset) == \"data.frame\")\nvalues = na.action(dataset[[varname]])\nif (class(dataset) == \"ts\")\nvalues = na.action(dataset[, varname])\nvar = list(values = values, n = length(values), name = varname,\nsub = paste(gettext(\"Dataset:\"), gettext(dataname)),\nxlab = gettext(varname),\nvar = mmstat.attrVar(var, type)\nvar\nmmstat.dec = function(x, ord = NULL) {\norder = order(x)\ndf = diff(x[order])\n# take only positive differences\ndf = min(df[df > 0])\nif (!is.null(ord))\norder = order(order[ord])\nlist(decimal = max(0, ceiling(-log10(df))), order = order)\nmmstat.axis = function(side, range, at, labels, ...) {\nat = pretty(range)\ndec = mmstat.dec(at)\naxis(side, at = at, labels = sprintf(\"%.*f\", dec$decimal, at), ...)\nmmstat.baraxis = function(side, range, at, labels, ...) {\npos = 1 + pretty(range)\naxis(side, at = at[pos], labels = labels[pos], ...)\nmmstat.merge = function(range1, range2) {\nreturn(c(min(range1, range2), max(range1, range2)))\nmmstat.range = function(...) {\nranges = list(...)\nisn = sapply(ranges, is.numeric)\nif (all(isn)) {\nmins = sapply(ranges, min)\n} else {\nmins = maxs = NA\nreturn(c(min(mins), max(maxs)))\nmmstat.round.up = function(x, digits = 0) {\nxr = round(x, digits)\ntf = (xr < x)\nxr[tf] = xr[tf] + 10^(-digits)\nxr\nmmstat.round.down = function(x, digits = 0) {\ntf = (xr > x)\nxr[tf] = xr[tf] - 10^(-digits)\nmmstat.pos = function(minmax, pos) {\nmin(minmax) + diff(minmax) * pos\nmmstat.ticks = function(nin, nmin = 3, tin = 11) {\nnmax = nin\nnt = tin - 1\nrepeat {\nn = nmin * exp((0:nt)/nt * log(nmax/nmin))\npow = 10^trunc(log10(n))\nssd[pow == 1] = 0\nssd[(ssd > 2) & (ssd < 8)] = 5\nfsd[ssd > 7] = fsd[ssd > 7] + 1\nnret = fsd * pow + ssd * pow/10\nif (nret[nt + 1] > nmax)\nnret[nt + 1] = nret[nt + 1] - pow[nt + 1]/2\nif (length(unique(nret)) == nt + 1)\nreturn(nret)\nnt = nt - 1\nmmstat.math = function(txt) {\ndollar = strsplit(txt, \"&\", fixed = T)[[1]]\nif (length(dollar) < 2)\nreturn(txt)\nres = paste0(\"expression(paste(\\\"\", dollar[1], \"\\\"\")\nfor (i in 2:length(dollar)) {\npercent = strsplit(dollar[i], \";\", fixed = T)[[1]]\nlp = length(percent)\nif (lp == 1)\nres = paste0(res, \",\\\"\", percent[1], \"\\\"\")\nelse {\nif (lp > 2)\npercent[2] = paste(percent[2:lp], sep = \";\")\nres = paste0(res, \",\", percent[1], \",\\\"\", percent[2], \"\\\"\")\nres = paste0(res, \"))\")\neval(parse(text = res))\nis.ASCII = function(txt) {\nall(charToRaw(txt) <= as.raw(127))\nstopif = function(cond, txt) {\nif (cond) stop(txt)\nmmstat.html = function(file, ...) {\nstopif(!file.exists(file), sprintf(\"File '%s' does not exist\", file))\nhtml = paste0(readLines(file), collapse = \"\")\nstopif(!is.ASCII(html), sprintf(\"File '%s' contains non-ASCII symbols\", file))\nargs = list(...)\ncond = sapply(args, length)\nstopif(!all(cond), paste(\"mmstat.html - Zero length arguments:\",\npaste(names(args)[!cond],\ncollapse = \", \")))\nif (length(args)) {\nstopif(any(sapply(args, is.null)),\n\"One or more arguments contain a NULL\")\nargs$fmt = html\nhtml = do.call(\"sprintf\", args)\nreturn(html)\nmmstat.plotTestRegions = function(crit, xlim, cex, close = F, col = \"black\",\nlabel = NULL, pos = 1) {\nlines(xlim, c(ylim[1], col = col)\nif (close) {\nlines(c(xlim[1], xlim[1]), ylim, col = col)\ncu = max(crit[1], xlim[1])\nif (crit[1] >= xlim[1]) {\nlines(c(cu, cu), ylim, col = col)\ntext((cu + xlim[1])/2, mean(ylim), mmstat.math(\"\\\"&H[1];\\\"\"),\ncex = cex, col = col)\nco = min(crit[2], xlim[2])\nif (crit[2] <= xlim[2]) {\nlines(c(co, co), ylim, col = col)\ntext((co + xlim[2])/2, mean(ylim), mmstat.math(\"\\\"&H[1];\\\"\"),\ntext((co + cu)/2, mean(ylim), mmstat.math(\"\\\"&H[0];\\\"\"), cex = cex, col = col)\nif (!is.null(text)) {\nif (pos == 2)\ntext(xlim[1], mmstat.pos(ylim, -0.25), label, col = col, cex = cex, pos = 4)\nif (pos == 4)\ntext(xlim[2], mmstat.pos(ylim, -0.25), label, col = col, cex = cex, pos = 2)\nmmstat.htest = function(...) {\naddnames = function(txt1, txt2) {\nif (length(txt1)) {\ncont = txt2 %in% txt1\nret = c(txt1, txt2[!cont])\n} else {\nret = txt2\nret\nhtest = list(method = list(attr = NULL, names = \"\", fmt = \"%s\", lines = 0),\nalternative = list(attr = NULL,\nnames = \"Alternative:\",\nnull.value = list(attr = \"names\",\nnames = vector(\"character\", 0),\ndata.name = list(attr = NULL, names = \"Data:\",\nfmt = \"%.4f\",\nconf.int = list(attr = \"conf.level\",\nlines = 1),\nstatistic = list(attr = \"names\",\nfmt = \"%.0f\",\np.value = list(attr = NULL,\nnames = \"p-value:\",\ntests = list(...)\nnhtest = names(htest)\nnrow = vector(\"numeric\", length(htest))\nlines = 0\nfor (j in seq(nhtest)) {\nname = nhtest[j]\nattr = htest[[nhtest[j]]]$attr\nif (!is.null(attr)) {\n# find all names\nfor (i in seq(tests)) {\nhtest[[name]]$names = addnames(htest[[name]]$names,\nattr(tests[[i]][[name]], attr))\n# grab all values\nnrow[j] = length(htest[[name]]$names)\nhtest[[name]]$tab = matrix(\"\", nrow = nrow[j], ncol = length(tests))\nfor (i in seq(tests)) {\ntelem = tests[[i]][[name]]\nif (!is.null(telem)) {\nhtest[[name]]$tab[1, i] = sprintf(htest[[name]]$fmt, telem)\n} else if (attr == \"conf.level\") {\nhtest[[name]]$tab[match(as.character(attr(telem, attr)),\nhtest[[name]]$names), i] = paste0(\"[\", round(telem[1], 4), \",\nround(telem[2], 4), \"]\")\n} else {\nhtest[[name]]$names), i] = sprintf(htest[[name]]$fmt, telem)\n}\nif (attr == \"conf.level\") {\nhtest[[name]]$names = sprintf(\"%.1f%% CI\", 100 *\nas.numeric(htest[[name]]$names))\nlines = lines + htest[[name]]$lines\ntab = matrix(\"\", nrow = sum(nrow) + lines, ncol = 1 + length(tests))\npos = 1\nname = nhtest[j]\nlen = length(htest[[name]]$names)\ntab[pos:(pos + len - 1), 1] = htest[[name]]$names\ntab[pos:(pos + len - 1),\n2:(1 + length(tests))] = htest[[name]]$tab\npos = pos + len + htest[[name]]$lines\nmaxlen = apply(nchar(tab), 2, max)\nfor (j in seq(tab)) {\nif (j <= nrow(tab))\ntab[j] = sprintf(\"%-*s\", maxlen[1 + ((j - 1) %/% nrow(tab))], tab[j])\npaste(apply(tab, 1, paste, collapse = \" \"), collapse = \"n\")\nmmstat.sliderInput = function(...) {\niapply = function(l) {\nret = l\nif (is.list(l)) {\nif (!is.null(ret$name) && (ret$name == \"input\")) {\nret$attribs[[\"data-values\"]] = ticks\n} else {\nfor (i in seq(ret)) ret[[i]] = iapply(ret[[i]])\nif ((compareVersion(mmstat$shiny, \"0.11\") >= 0) &&\n!is.null(args$ticks) && length(args$ticks)) {\nticks = paste0(args$ticks, collapse = \",\")\nargs$ticks = T\nhtml = do.call(\"sliderInput\", args)\nhtml = iapply(html)\nhtml\nmmstat.ui.elem = function(inputId, type, ...) {\nfound = F\nelem = list(...)\nelem$inputId = inputId\nelem$type = type\nshinytypes = c(\"actionButton\",\n\"checkboxInput\",\nmmstattypes = c(\"sampleSize\",\n\"drawSample\",\n\"confidenceLevel\",\npos = pmatch(type, shinytypes)\nif (!is.na(pos)) {\nfound = T\nif (elem$type == \"actionButton\") {\nif (is.null(elem$value))\nelem$value = 0\nelem$call = shinytypes[pos]\npos = pmatch(type, mmstattypes)\nif (elem$type == \"sampleSize\") {\nif (is.null(elem$label))\nelem$label = gettext(\"Sample size (n)\")\nelem$call = \"mmstat.sliderInput\"\nelem$step = 1\nelem$value = as.numeric(compareVersion(mmstat$shiny, \"0.11\") < 0)\nif (elem$type == \"drawSample\") {\nelem$call = \"actionButton\"\nelem$label = gettext(\"Draw sample\")\nif (elem$type == \"testHypotheses\") {\nelem$call = \"radioButtons\"\nelem$label = gettext(\"Choose test type\")\nelem$choices = gettext(c(\"two.sided\", \"less\", \"greater\"), \"name\")\nif (is.null(elem$value))\nelem$value = \"two.sided\"\nif (elem$type == \"significance\") {\nelem$call = \"mmstat.sliderInput\"\nif (is.null(elem$ticks))\nelem$ticks = c(0.1, 0.25, 1, 2, 5, 10, 20)\nelem$label = HTML(gettext(\"Select significance\nlevel (&alpha;)\"))\nelem$step = 1\nelem$max = length(elem$ticks)\nelem$value = 6 - as.numeric(compareVersion(mmstat$shiny, \"0.11\") >= 0)\nif (elem$type == \"confidenceLevel\") {\nelem$ticks = c(80, 85, 90, 99.5, 99.9)\nif (is.null(elem$label))\nelem$label = HTML(gettext(\"Select confidence level (1-&alpha;)\"))\nelem$step = 1\nelem$max = length(elem$ticks)\nelem$value = 4 - as.numeric(compareVersion(mmstat$shiny, \"0.11\") >= 0)\nif (elem$type == \"dataSet\") {\nelem$call = \"selectInput\"\nelem$label = gettext(\"Select a data set\")\nif (is.null(elem$choices))\nelem$choices = mmstat.getDataNames(gsub(\".rds$\", \"\",\nlist.files(pattern = \"*.rds\")))\nelem$value = mmstat.getDataNames()\nif (elem$type == \"variable1\") {\nelem$label = gettext(\"Select a variable\")\nelem$choices = mmstat.getVarNames(1, elem$vartype)\nif (elem$type == \"variableN\") {\nelem$call = \"selectInput\"\nelem$multiple = T\nelem$label = gettext(\"Select variable(s)\")\nif (elem$type == \"fontSize\") {\nelem$label = gettext(\"Font size\")\nif (is.null(elem$min))\nelem$min = 1\nif (is.null(elem$max))\nelem$max = 1.5\nif (is.null(elem$step))\nelem$step = 0.05\nelem$value = elem$min\nif (elem$type == \"speedSlider\") {\nelem$call = \"mmstat.sliderInput\"\nelem$label = list(NULL)\nelem$min = 0\nstopif(!found, sprintf(\"mmstat.ui.elem: Type \\\"%s\\\" unknown\", type))\nif (is.null(elem$update))\nelem$update = paste0(\"update\", ucfirst(elem$call))\nmmstat$UI[[inputId]] <<- elem\nmmstat.ui.call = function(inputId, ...) {\nelem = mmstat$UI[[inputId]]\nwhat = elem$call\nargs = list(...)\nfor (name in names(elem)) {\nif (is.null(args[[name]]))\nargs[[name]] = elem[[name]]\nargs$call = NULL\nif ((what == \"selectInput\") || (what == \"checkboxGroupInput\"))\nargs$value = NULL\ndo.call(what, args)\nmmstat.ui.update = function(inputId, ...) {\nwhat = elem$update\nif ((what == \"updateSelectInput\") ||\n(what == \"updateCheckboxGroupInput\"))\nmmstat.lang = function(deflang = NULL) {\npof = list.files(pattern = \"*.po$\")\nif (is.null(deflang)) {\nlang = sapply(strsplit(pof, \".\", fixed = T),\nfunction(elem) {elem[1]})\npat = paste0(\"_\", lang, \"$\")\npath = getwd()\npath = strsplit(path, \"/\", fixed = T)[[1]]\npos = -1\nfor (i in seq(pat)) {\np = grep(pat[i], path)\nif (length(p)) {\np = max(p)\nif (p > pos) {\nlind = i\nlind = match(paste0(deflang, \".po\"), pof)\nif (is.na(lind))\nlind = (-1)\nmsgfn = ifelse(lind > 0, pof[lind], \"default.po\")\nmmstat.warn(!file.exists(msgfn), sprintf(\"File '%s' does not exist\", msgfn))\nmsg = paste(readLines(msgfn), collapse = \" \")\nmsgid = regmatches(msg, gregexpr(\"msgid\\\\s*\\\".*?\\\"\", msg))\ntmp = strsplit(msgid[[1]], \"\\\"\")\nmsgid = sapply(tmp, function(vec) {\npaste0(vec[2:length(vec)])\n})\nmsgstr = regmatches(msg, gregexpr(\"msgstr\\\\s*\\\".*?\\\"\", msg))\ntmp = strsplit(msgstr[[1]], \"\\\"\")\nmsgstr = sapply(tmp, function(vec) {\nmmstat$messages <<- list(id = msgid, str = msgstr)\nhtmlTable = function(tab, vars = NULL, lines = NULL, cex = 1) {\nhtml = sprintf(\"<table style=\\\"text-align:right;width:100%%;\nfont-size:%i%%;\\\">\",\nas.integer(100 * cex))\nif (length(vars) == 2)\nhtml = paste0(html,\nsprintf(\"<tr><td colspan=\\\"%i\\\">\",1 + ncol(tab)),\nsprintf(gettext(\"Columns: %s\"), gettext(vars[1])),\n\"</td></tr>\")\nwidth = as.integer(100/(1 + ncol(tab)))\nhtml = paste0(html, sprintf(\"<TR><TD style=\\\"width:%i%%\\\"></TD>\",\nwidth))\nhtml = paste0(html, paste0(sprintf(\"<TD style=\\\"width:%i%%;\nbackground-color:#AAAAAA\\\"><b>\",\nwidth),\nsprintf(\"%s\", colnames(tab)),\nhtml = paste0(html, \"</TR>\")\nalign = ifelse(is.na(suppressWarnings(as.numeric(rownames(tab)))),\n\"left\", \"right\")\nfor (i in seq(nrow(tab))) {\nhtml = paste0(html, sprintf(\"<TR><TD style=\\\"text-align:%s;\nbackground-color:\n#AAAAAA\\\"><b>%s</b></TD>\",\nalign[i], rownames(tab)[i]))\nif (i%%2 == 0) {\nhtml = paste0(html, paste0(\"<TD style=\n\\\"background-color:\nsprintf(\"%i\", tab[i, ]),\nhtml = paste0(html, paste0(\"<TD>\", sprintf(\"%i\", tab[i, ]),\nhtml = paste0(html, \"</TR>\")\nhtml = paste0(html, sprintf(\"<tr><td colspan=\\\"%i\\\"\nstyle=\\\"text-align:left;\\\">\",\n1 + ncol(tab)),\nsprintf(gettext(\"Rows: %s\"), vars[2]),\n\"</td></tr>\",\nsprintf(\"<tr><td colspan=\\\"%i\\\"><hr></td></tr>\",\n1 + ncol(tab)))\nfor (i in seq(lines)) html = paste0(html,\nsprintf(\"<tr><td colspan=\n\\\"%i\\\" style=\n\\\"text-align:left;\\\">\",\nlines[i], \"</td></tr>\")\nhtml = paste0(html, \"</table>\")\nucfirst = function(txt) {\nreturn(paste0(toupper(substr(txt, 1, 1)), substring(txt, 2)))\ngettext = function(msg, utype = \"vector\") {\ntype = pmatch(utype, c(\"name\", \"numeric\"))\nif (is.na(type)) {\nret = paste0(ifelse(mmstat$debug > 2, \"?\", \"\"), msg)\npos = match(msg, mmstat$messages$id)\nind = (1:length(pos))[!is.na(pos)]\nret[ind] = mmstat$messages$str[pos[ind]]\n} else if (type == 1) {\nret = as.list(msg)\nnames(ret) = gettext(msg)\n} else if (type == 2) {\nret = as.list(seq(msg))\nreturn(ret)\n# Logging\nmmstat.getLog = function(session) {\nif (!mmstat$debug)\nreturn(\"\")\ninvalidateLater(100, session)\npaste0(\"<hr>\", paste(mmstat$log, collapse = \"<br>\"))\nmmstat.log = function(txt) {\nif (mmstat$debug > 1)\nmmstat$log <<- cbind(sprintf(\"%s (Info): %s\",\ndate(), txt),\nmmstat.warn = function(cond, txt) {\nif ((mmstat$debug > 0) && cond)\nmmstat$log <<- cbind(sprintf(\"%s (Warn): %s\",\nmmstat.input = function(input) {\nif (mmstat$debug > 1) {\nni = names(input)\nfor (i in seq(ni)) {\nout = capture.output(input[[ni[i]]])\nout[1] = paste0(ni[i], \": \", out[1])\nfor (j in seq(out))\nmmstat$log <<- cbind(sprintf(\"%s (Input): %s\",\ndate(), out[j]), mmstat$log)\n# Main\nmmstat = list(debug = 1,\nshiny = sessionInfo()$otherPkgs$shiny$Version,\ncol = list(daquamarine = \"#1B9E77\",\ndorange = \"#D95F02\",\n1, 2.5, 5, 10, 20),\nUI = vector(\"list\", 0))\nmmstat.log(\"Starting app\")\n", "output_sequence": "Shows the pdf and the cdf of the normal distribution. Mean and variance can be changed interactively. The user can also plot the exponential distribution with adjustable parameter lambda."}, {"input_sequence": "Author: Sigbert Klinke ; # ------------------------------------------------------------------------------\n# Name of Quantlet: MMSTATdistribution_normal\n# ------------------------------------------------------------------------------\n# Published in: MMSTAT\n# Description: Shows the pdf and the cdf of the normal distribution. Mean and variance can be changed interactively.\n# The user can also plot the exponential distribution with adjustable parameter lambda.\n# Keywords: distribution, plot, cdf, exponential, visualization, parameter, interactive, parametric, normal,\n# normal-distribution, mean, variance, density\n# See also: norm, SFEDaxReturnDistribution, norm2, MSEedfnormal, MMSTATtime_series_1, MMSTATlinreg, MMSTATconfmean,\n# MMSTATconfi_sigma, MMSTATassociation, MMSTAThelper_function, MMSTATdistribution_exponential\n# Author : Sigbert Klinke\n# Code Editor: Yafei Xu\n# Submitted: 21/08/2015\n# Usage: MMSTAThelper_function\n# Output: Interactive shiny application\n# Example: Shows the density function of the normal distribution with mean = 0 and variance = 1.\n# ------------------------------------------------------------------------------\n\n# please use \"Esc\" key to jump out of the Shiny app\nrm(list = ls(all = TRUE))\ngraphics.off()\n# please set working directory setwd('C:/...')\n# setwd('~/...') # linux/mac os\n# setwd('/Users/...') # windows\nsource(\"MMSTAThelper_function.r\")\n############################### SUBROUTINES ##################################\n### assistant function #######################################################\nmmstat.getValues = function (local, ...) {\nret <<- list(...)\nfor (name in names(ret)) {\nif (is.null(ret[[name]]) || (length(ret[[name]])==0)) {\nif (is.null(local[[name]])) {\nstopif (is.null(mmstat$UI[[name]]$value),\npaste0('no default value(s) for \"', name, '\"'))\nret[[name]] = mmstat$UI[[name]]$value\n} else {\nret[[name]] = local[[name]]\n}\n}\nret\n}\n### server ###################################################################\ngv = list(dist = \"mmstat\", xlim = NULL, ylim = NULL)\ndistrc = gettext(c(\"EXP\", \"NORM\"), \"name\")\ndistrd = \"NORM\"\nradioc = gettext(c(\"PDF\", \"CDF\"), \"name\")\nmmstat$UI = list(distribution = list(inputId = \"distribution\",\nlabel = gettext(\"Select a probability\ndistribution\"),\nexp.lambda = list(inputId = \"exp.lambda\",\nlabel = HTML(gettext(\"Parameter (&lambda;)\")),\nmin = 0.1,\nnorm.mu = list(inputId = \"norm.mu\",\nlabel = HTML(gettext(\"Mean (&mu;)\")),\nmin = -3,\nnorm.sigma2 = list(inputId = \"norm.sigma2\",\nlabel = HTML(gettext(\"Variance (&sigma;<sup>2</sup>)\")),\npdforcdf = list(inputId = \"pdforcdf\",\nlabel = gettext(\"Show\"),\nrefit = list(inputId = \"refit\",\nlabel = gettext(\"Refit axes\")),\nserver = shinyServer(function(input, output, session) {\n\noutput$EXPUI = renderUI({\nHTML(mmstat.html(gettext(\"INTRO_EXP\")))\n})\noutput$NORMUI = renderUI({\nHTML(mmstat.html(gettext(\"INTRO_NORM\")))\noutput$exp.lambdaUI = renderUI({\nmmstat.log(\"exp.lambdaUI\")\nargs = mmstat$UI$exp.lambda\ndo.call(\"sliderInput\", args)\noutput$norm.muUI = renderUI({\nmmstat.log(\"norm.muUI\")\nargs = mmstat$UI$norm.mu\noutput$norm.sigma2UI = renderUI({\nmmstat.log(\"norm.sigma2UI\")\nargs = mmstat$UI$norm.sigma2\noutput$distributionUI = renderUI({\nmmstat.log(\"distributionUI\")\nargs = mmstat$UI$distribution\nargs$value = NULL\ndo.call(\"selectInput\", args)\noutput$pdforcdfUI = renderUI({\nargs = mmstat$UI$pdforcdf\ndo.call(\"radioButtons\", args)\noutput$refitUI = renderUI({\nargs = mmstat$UI$refit\ndo.call(\"actionButton\", args)\noutput$cexUI = renderUI({\nmmstat.log(\"cexUI\")\nargs = mmstat$UI$cex\nrefit = reactive({\ninput$refit\ngv$dist <<- \"mmstat\"\noutput$distPlot = renderPlot({\nrefit()\ninp = mmstat.getValues(NULL,\ncex = input$cex,\ndistribution = input$distribution,\n\nif (gv$dist != inp$distribution) {\nif ((gv$dist == \"NORM\") && (inp$distribution == \"EXP\")) {\n# switch from NORM to EXP\nupdateSliderInput(session, inputId = \"exp.lambda\", value = inp$norm.sigma2)\nif ((gv$dist == \"EXP\") && (inp$distribution == \"NORM\")) {\n# switch from EXP to NORM\nupdateSliderInput(session, inputId = \"norm.sigma2\", value = inp$exp.lambda)\ngv$xlim <<- c(0, 0)\ngv$dist <<- inp$distribution\nswitch(inp$distribution, EXP = {\ngv$xlim <<- mmstat.merge(gv$xlim, c(0, qexp(0.999, inp$exp.lambda)))\nx = (0:300)/300 * gv$xlim[2]\nif (inp$pdforcdf == \"PDF\") {\nheight = dexp(x, inp$exp.lambda)\ngv$ylim <<- mmstat.merge(gv$ylim, c(0, height))\nplot(x, height,\ntype = \"l\",\nmain = mmstat.math(sprintf(gettext(\"Density function f(x) of EX(&lambda;=%.1f)\"),\ninp$exp.lambda)),\ncex.axis = inp$cex,\ngv$ylim <<- mmstat.merge(gv$ylim, c(0, 1))\nheight = pexp(x, inp$exp.lambda)\nmain = mmstat.math(sprintf(gettext(\"Cumulative distribution function F(x) of EX(&lambda;=%.1f)\"),\nabline(h = 0, col = \"gray70\", lty = \"dashed\")\n}, NORM = {\nsd = sqrt(inp$norm.sigma2)\ngv$xlim <<- mmstat.merge(gv$xlim, c(qnorm(0.001, inp$norm.mu, sd),\nqnorm(0.999, inp$norm.mu, sd)))\nx = gv$xlim[1] + (0:300)/300 * diff(gv$xlim)\nheight = dnorm(x, inp$norm.mu, sd)\nmain = mmstat.math(sprintf(gettext(\"Density function f(x) of N(&mu;=%.1f,&sigma^2;=%.1f)\"),\ninp$norm.mu,\nabline(v = 0, col = \"gray70\")\nheight = pnorm(x, inp$norm.mu, sd)\nmain = mmstat.math(sprintf(gettext(\"Cumulative distribution function F(x) of N(&mu;=%.1f,&sigma^2;=%.1f)\"),\ncex.main = 1.2 * inp$cex,\n})\noutput$logText = renderText({\nmmstat.getLog(session)\n})\n### ui #######################################################################\nui = shinyUI(fluidPage(\ndiv(class = \"navbar navbar-static-top\",\ndiv(class = \"navbar-inner\",\nfluidRow(column(4, div(class = \"brand pull-left\",\ngettext(\"Continuous probability\ndistributions\"))),\ncolumn(2, checkboxInput(\"showdist\",\ngettext(\"Choose distribution\"),\ncolumn(2, checkboxInput(\"showparameter\",\ngettext(\"Distribution parameter\"),\ncolumn(2, checkboxInput(\"showfunction\",\ngettext(\"Choose function\"),\ncolumn(2, checkboxInput(\"showoptions\",\ngettext(\"Options\"),\nsidebarLayout(\nconditionalPanel(\ncondition = 'input.showdist',\nuiOutput(\"distributionUI\")\n),\ncondition = \"input.distribution=='EXP'\",\nconditionalPanel(\ncondition = 'input.showparameter',\nuiOutput(\"exp.lambdaUI\")\n)\ncondition = \"input.distribution=='NORM'\",\nuiOutput(\"norm.muUI\"),\ncondition = 'input.showfunction',\nbr(),\nuiOutput(\"pdforcdfUI\"),\ncondition = 'input.showoptions',\nhr(),\nuiOutput(\"cexUI\")\n)\n),\n\nmainPanel(plotOutput(\"distPlot\"))\nhtmlOutput(\"logText\")\n))\n### shinyApp #################################################################\nshinyApp(ui = ui, server = server)\n", "output_sequence": "Shows the pdf and the cdf of the normal distribution. Mean and variance can be changed interactively. The user can also plot the exponential distribution with adjustable parameter lambda."}, {"input_sequence": "Author: Sigbert Klinke ; # ------------------------------------------------------------------------------\n# Name of Quantlet: MMSTATscatterplot\n# ------------------------------------------------------------------------------\n# Published in: MMSTAT\n# Description: Shows either a 2D or 3D scatterplot. One has also the option to compute Pearson's correlation and\n# Spearman's rank correlation for variables of the data sets CARS, DECATHLON and USCRIME.\n# Keywords: plot, scatterplot, correlation, data visualization, estimation, parameter, interactive,\n# uscrime, 3D, US crime data set\n# Usage: MMSTAThelper_function\n# Output: Interactive shiny application\n# Example: Shows the 2-dimensional scatterplot for the variables PRICE and MILEAGE in the data set CARS.\n# See also: COPcorrelation1, COPhac4firmsscatter, SFE5dim, MMSTATtime_series_1, MMSTATlinreg,\n# MMSTATconfmean, MMSTATconfi_sigma, MMSTATassociation, MMSTAThelper_function\n# Author : Sigbert Klinke\n# Code Editor: Yafei Xu\n# Datafiles: CARS.rds, DECATHLON.rds, USCRIME.rds\n\n# please use \"Esc\" key to jump out of the Shiny app\nrm(list = ls(all = TRUE))\ngraphics.off()\n# please set working directory setwd('C:/...')\n# setwd('~/...') # linux/mac os\n# setwd('/Users/...') # windows\nlibraries = c(\"scatterplot3d\", \"lattice\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\nsource(\"MMSTAThelper_function.r\")\n############################### SUBROUTINES ##################################\nmmstat$vartype = \"numvars\"\nmmstat.ui.elem(\"graph\", \"radioButtons\",\nlabel = gettext(\"Select a scatterplot type\"),\nchoices = gettext(c(\"2D.SCATTERPLOT\", \"3D.SCATTERPLOT\",\n\"SCATTERPLOTMATRIX\"),\nselected = \"2D.SCATTERPLOT\")\nmmstat.ui.elem(\"permute\", \"actionButton\", label = gettext(\"Cycle axes\"))\nmmstat.ui.elem(\"coeff\", \"checkboxGroupInput\",\nlabel = gettext(\"Show coefficient(s)\"),\nchoices = gettext(c(\"SHOW.BRAVAIS.PEARSON\", \"SHOW.SPEARMAN\"),\n\"name\"),\nvalue = character())\nmmstat.ui.elem(\"dataset\", \"dataSet\",\nchoices = mmstat.getDataNames(\"USCRIME\", \"CARS\", \"DECATHLON\"))\nmmstat.ui.elem(\"variableSelect\", \"variableN\",\nvartype = \"numeric\",\nselected = mmstat$dataset[[1]]$numvars[1:2])\nmmstat.ui.elem(\"cex\", \"fontSize\")\nbuildList = function(depth, v) {\nif (depth) {\nret = buildList(depth - 1, v)\nmat = matrix(NA, nrow = length(v) * nrow(ret), ncol = ncol(ret) + 1)\nfor (i in seq(v)) {\nind1 = (1 + (i - 1) * nrow(ret)):(i * nrow(ret))\nind2 = 1 + (1:ncol(ret))\nmat[ind1, ind2] = ret\nmat[(1 + (i - 1) * nrow(ret)):(i * nrow(ret)), 1] = v[i]\n}\nind = apply(mat, 1, function(x, m) {\nlength(unique(x)) == m\n}, m = depth + 1)\nreturn(mat[ind, ])\n}\nreturn(as.matrix(v, ncol = 1))\n}\nupper.panel = function(x, y, cex, coeff) {\nif (length(coeff)) {\nusr = par(\"usr\")\non.exit(par(usr))\npar(usr = c(0, 1, 0, 1))\nr = abs(cor(x, y))\ntxt = format(c(r, 0.123456789), digits = digits)[1]\ntxt = paste0(prefix, txt)\nif (missing(cex.cor))\ncex.cor = 0.8/strwidth(txt)\ntext(0.5, 0.5, txt, cex = cex.cor * r)\n} else {\npoints(x, y, cex = 0.75, pch = 19)\nct = 0\nlast.permute = 0\nserver = shinyServer(function(input, output, session) {\n\noutput$graphUI = renderUI({\nmmstat.ui.call(\"graph\")\n})\noutput$coeffUI = renderUI({\nmmstat.ui.call(\"coeff\")\noutput$permuteUI = renderUI({\nmmstat.ui.call(\"permute\")\noutput$datasetUI = renderUI({\nmmstat.ui.call(\"dataset\")\noutput$cexUI = renderUI({\nmmstat.ui.call(\"cex\")\noutput$variableSelectUI = renderUI({\ninp = mmstat.getValues(NULL, dataset = input$dataset)\nret = mmstat.ui.call(\"variableSelect\",\nchoices = mmstat.getVarNames(inp$dataset, \"numeric\"))\nret\nobserve({\nupdateSelectInput(session, \"variableSelect\",\nchoices = mmstat.getVarNames(inp$dataset, \"numeric\"),\ncycleList = reactive({\ninp = mmstat.getValues(NULL, graph = input$graph,\nvariableSelect = input$variableSelect)\nmaxdepth = length(inp$variableSelect)\nif (inp$graph == \"2D.SCATTERPLOT\")\nmaxdepth = 2\nif (inp$graph == \"3D.SCATTERPLOT\")\nmaxdepth = 3\nif (inp$graph == \"SCATTERPLOTMATRIX\")\nmaxdepth = min(maxdepth, 6)\nreturn(buildList(maxdepth - 1, inp$variableSelect))\noutput$scatterPlot = renderPlot({\ninp = mmstat.getValues(NULL,\ndataset = input$dataset,\nclist = cycleList()\nif (nrow(clist) == 0) {\nplot(0, 0,\nxlim = c(-0.1, 0.1),\ntype = \"n\",\ntext(0, 0, gettext(\"Please select two, three or more variables!\"),\ncex = 1.5 * inp$cex)\n} else {\nif (inp$permute > last.permute) {\n# button was pressed\nlast.permute <<- inp$permute\nct <<- ct + 1\n}\nct <<- ct%%nrow(clist)\nvarx = mmstat.getVar(isolate(inp$dataset), varname = clist[ct + 1, 1],\nna.action = na.pass)\nif (varx$name == clist[ct + 1, 1]) {\nif (inp$graph == \"2D.SCATTERPLOT\") {\nvary = mmstat.getVar(inp$dataset,\nvarname = clist[ct + 1, 2],\nplot(varx$values,\nxlab = gettext(clist[ct + 1, 1]),\ncex.axis = inp$cex,\npch = 19)\nif (length(inp$coeff)) {\nif (inp$coeff[i] == \"SHOW.BRAVAIS.PEARSON\") {\nmain[i] = sprintf(\" &r[xy];=%+.2f\",\ncor(varx$values, use = \"c\"))\n}\nif (inp$coeff[i] == \"SHOW.SPEARMAN\") {\nmain[i] = sprintf(\" &r[s];=%+.2f\",\ncor(varx$values,\nuse = \"c\", method = \"s\"))\n}\ntitle(main = mmstat.math(paste(main, collapse = \", \")))\n}\nif (inp$graph == \"3D.SCATTERPLOT\") {\nvary = mmstat.getVar(isolate(inp$dataset),\nvarname = clist[ct + 1, 2], na.action = na.pass)\nvarz = mmstat.getVar(isolate(inp$dataset),\nvarname = clist[ct + 1, 3], na.action = na.pass)\nscatterplot3d(varx$values, vary$values,\npch = 19,\ncex.axis = 0.8 * inp$cex,\nhighlight.3d = T,\nxlab = gettext(clist[ct + 1, 1]),\nif (length(inp$coeff))\ntitle(main = gettext(\"Coefficients are not available\nfor 3D Scatterplot\"))\nif (inp$graph == \"SCATTERPLOTMATRIX\") {\ndat = data.frame(varx$values)\nfor (i in 2:length(clist[ct + 1, ])) {\nvar = mmstat.getVar(inp$dataset,\nvarname = clist[ct + 1, i],\ndat[, i] = var$values\ncolnames(dat) = gettext(clist[ct + 1, ])\nsplom(~dat,\npch = 19,\ncex = 1/sqrt(ncol(dat)),\nvarname.cex = 1.5 * inp$cex/inp$cex/(ncol(dat)^(1/3)),\naxis.text.cex = ifelse(ncol(dat) < 5, inp$cex/(ncol(dat)^(1/3)), 0),\ncoeff = inp$coeff,\ncex.coeff = 1.5 * inp$cex/(ncol(dat)^(1/3)),\npanel = function(x, y, i, j, cex, coeff, cex.coeff, ...) {\nif (length(coeff)) {\npanel.points(x, y, cex = cex, pch = 19, col = \"black\")\n} else {\nmain = rep(\"\", length(inp$coeff))\nif (inp$coeff[i] == \"SHOW.BRAVAIS.PEARSON\") {\nmain[i] = sprintf(\" &r[xy];=%+.2f\", cor(x, y, use = \"c\"))\n}\nif (inp$coeff[i] == \"SHOW.SPEARMAN\") {\nmain[i] = sprintf(\" &r[s];=%+.2f\", cor(x, y, use = \"c\",\nmethod = \"s\"))\n}\nxpos = mean(range(x))\nc(1, 3, 5)/(2 * length(coeff))\nfor (i in seq(main)) panel.text(xpos,\nypos[i],\n}\npanel.points(x, y, cex = cex, pch = 19, col = \"black\")\n}\noutput$logText = renderText({\nmmstat.getLog(session)\n### ui #######################################################################\nui = shinyUI(fluidPage(\ndiv(class=\"navbar navbar-static-top\",\ndiv(class = \"navbar-inner\",\nfluidRow(column(4, div(class = \"brand pull-left\",\ngettext(\"Scatterplots and correlation\"))),\ncolumn(2, checkboxInput(\"showgraph\",\ngettext(\"Graphics\"),\ncolumn(2, checkboxInput(\"showcoeff\",\ngettext(\"Coefficients\"),\ncolumn(2, checkboxInput(\"showdata\",\ngettext(\"Data choice\"),\ncolumn(2, checkboxInput(\"showoptions\",\ngettext(\"Options\"),\n\nsidebarLayout(\nconditionalPanel(\ncondition = 'input.showgraph',\nuiOutput(\"graphUI\"),\n),\ncondition = 'input.showcoeff',\nhr(),\nuiOutput(\"coeffUI\")\ncondition = 'input.showdata',\nuiOutput(\"datasetUI\"),\ncondition = 'input.showoptions',\nuiOutput(\"cexUI\")\n)\nmainPanel(plotOutput(\"scatterPlot\"))),\nhtmlOutput(\"logText\")\n))\n### shinyApp #################################################################\nshinyApp(ui = ui, server = server)\n", "output_sequence": "Shows either a 2D or 3D scatterplot. One has also the option to compute Pearson's correlation and Spearman's rank correlation for variables of the data sets CARS, DECATHLON and USCRIME."}, {"input_sequence": "Author: Sigbert Klinke ; # please use \"Esc\" key to jump out the run of Shiny app\n# clear history and close windows\n# rm(list=ls(all=TRUE))\ngraphics.off()\n\n# General settings\nlibraries = c(\"shiny\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# please set working directory setwd('C:/...')\n# windows setwd('/Users/...') #\n# mac os setwd('~/...') # linux\n##############################################################################\n# Default parameters\nmmstat.getValues = function(local, ...) {\nret <<- list(...)\nfor (name in names(ret)) {\nif (is.null(ret[[name]]) || (length(ret[[name]]) == 0)) {\nif (is.null(local[[name]])) {\nstopif(is.null(mmstat$UI[[name]]$value) &&\npaste0(\"mmstat.getValues: no default value(s) for \\\"\", name, \"\\\"\"))\nif (is.null(mmstat$UI[[name]]$value)) {\nret[[name]] = mmstat$UI[[name]]$selected\n} else {\nret[[name]] = mmstat$UI[[name]]$value\n}\n} else {\nret[[name]] = local[[name]]\n}\nif (mmstat$UI[[name]]$call == \"mmstat.sliderInput\") {\nif ((compareVersion(mmstat$shiny, \"0.11\") >= 0) &&\n!is.null(mmstat$UI[[name]]$ticks)) {\nret[[name]] = ret[[name]] + 1\n}\nret\n}\n# Datasets and variables\nmmstat.getDataNames = function(...) {\nis.binary = function(v) {\nx = unique(v)\nlength(x) - sum(is.na(x)) == 2L\n}\n\nfiles = list(...)\nif (length(files) == 0)\nreturn(names(mmstat$dataset)[1])\ndataset = vector(\"list\", length(files))\nnames(dataset) = files\nmmstat$dataset <<- dataset\nfor (i in seq(files)) {\nfile = files[[i]]\nif (is.null(mmstat$dataset$file)) {\ndata = readRDS(paste0(file, \".rds\"))\nif (class(data) == \"data.frame\") {\nallvars = names(data)\nmmstat$dataset[[file]] <<- list(data = data,\nallvars = allvars,\nnumvars = allvars[sapply(data, is.numeric)],\nif (class(data) == \"ts\") {\nallvars = colnames(data)\ngettext(names(mmstat$dataset), \"name\")\nmmstat.getVarNames = function(dataname, vartype, which = NULL) {\nvars = NULL\nif (vartype == \"numeric\")\nvars = \"numvars\"\nif (vartype == \"ordered\")\nvars = \"ordvars\"\nif (vartype == \"factor\")\nvars = \"facvars\"\nif (vartype == \"binary\")\nvars = \"binvars\"\nstopif(is.null(vars),\n\"mmstat.getVarNames: Variable type \\\"%s\\\" unknown\")\nif (is.null(which))\nreturn(gettext(mmstat$dataset[[dataname]][[vars]], \"name\"))\nif (length(which == 1))\nreturn(gettext(mmstat$dataset[[dataname]][[vars]][which]))\nmmstat.getDatasets = function(...) {\nfiles = list(...)\ndataset = vector(\"list\", length(files))\nnames(dataset) = files\nmmstat$dataset <<- dataset\nfor (i in seq(files)) {\nfile = files[[i]]\ndata = readRDS(paste0(file, \".rds\"))\nif (class(data) == \"data.frame\") {\nallvars = names(data)\nmmstat$dataset[[file]] <<- list(data = data,\nallvars = allvars,\nnumvars = allvars[sapply(data, is.numeric)],\nif (class(data) == \"ts\") {\nallvars = colnames(data)\nnumvars = allvars)\nnames(dataset)[1]\nmmstat.attrVar = function(var, type, index = NULL) {\nret = var\nif (is.null(index))\nindex = 1:length(var$values)\nelse {\nret$values = var$values[index]\nret$n = length(index)\nif (type == \"numvars\") {\nif (is.null(var$jitter))\nret$jitter = runif(length(index)) else\nret$jitter = var$jitter[index]\nret$mean = mean(var$values[index], na.rm = T)\nret$quart = quantile(var$values[index], c(0.25, 0.75), na.rm = T)\nret$max = max(var$values[index], na.rm = T)\nif ((type == \"binvars\") || (type == \"ordvars\") || (type == \"facvars\")) {\nret$tab = table(var$values[index], useNA = \"ifany\")\nret$prop = prop.table(ret$tab)\nmmstat.getDatasetNames = function() {\ngettext(mmstat$dataset[[name]]$allvars, \"name\")\nmmstat.getVar = function(dataname = NULL, varname = NULL, vartype = NULL, na.action = na.omit) {\nmatch.name = function(needle, haystack) {\nif (is.null(needle))\nreturn(NA)\nreturn(match(needle, haystack))\n# which variable type do we need\nif (is.null(vartype))\nvartype = mmstat$vartype\ndeftype = c(\"numvars\", \"binvars\", \"numeric\",\n\"binary\", \"ordered\", \"factor\")\ntype = pmatch(vartype, deftype)\nstopif(is.na(type), paste0(\"mmstat.getVar: Unknown variable type: \", vartype))\ntype = deftype[type]\nif (type == \"numeric\")\ntype = \"ordvars\"\nif (type == \"factor\")\ntype = \"binvars\"\n# get dataset\npos = match.name(dataname, names(mmstat$dataset))\nif (is.na(pos))\npos = 1 # assume that all datasets contain\ndataname = names(mmstat$dataset)[pos]\nif (is.null(mmstat$dataset[[dataname]]))\nstop(sprintf(gettext(\"Data file '%s' is missing\"), dataname))\n# get variable\npos = match.name(varname, mmstat$dataset[[dataname]][[type]])\npos = 1\n# assume that every datasets contains a variable of correct type\nvarname = mmstat$dataset[[dataname]][[type]][pos]\n# get var values\ndataset = mmstat$dataset[[dataname]]$data\nif (class(dataset) == \"data.frame\")\nvalues = na.action(dataset[[varname]])\nif (class(dataset) == \"ts\")\nvalues = na.action(dataset[, varname])\nvar = list(values = values, n = length(values), name = varname,\nsub = paste(gettext(\"Dataset:\"), gettext(dataname)),\nxlab = gettext(varname),\nvar = mmstat.attrVar(var, type)\nvar\nmmstat.dec = function(x, ord = NULL) {\norder = order(x)\ndf = diff(x[order])\n# take only positive differences\ndf = min(df[df > 0])\nif (!is.null(ord))\norder = order(order[ord])\nlist(decimal = max(0, ceiling(-log10(df))), order = order)\nmmstat.axis = function(side, range, at, labels, ...) {\nat = pretty(range)\ndec = mmstat.dec(at)\naxis(side, at = at, labels = sprintf(\"%.*f\", dec$decimal, at), ...)\nmmstat.baraxis = function(side, range, at, labels, ...) {\npos = 1 + pretty(range)\naxis(side, at = at[pos], labels = labels[pos], ...)\nmmstat.merge = function(range1, range2) {\nreturn(c(min(range1, range2), max(range1, range2)))\nmmstat.range = function(...) {\nranges = list(...)\nisn = sapply(ranges, is.numeric)\nif (all(isn)) {\nmins = sapply(ranges, min)\n} else {\nmins = maxs = NA\nreturn(c(min(mins), max(maxs)))\nmmstat.round.up = function(x, digits = 0) {\nxr = round(x, digits)\ntf = (xr < x)\nxr[tf] = xr[tf] + 10^(-digits)\nxr\nmmstat.round.down = function(x, digits = 0) {\ntf = (xr > x)\nxr[tf] = xr[tf] - 10^(-digits)\nmmstat.pos = function(minmax, pos) {\nmin(minmax) + diff(minmax) * pos\nmmstat.ticks = function(nin, nmin = 3, tin = 11) {\nnmax = nin\nnt = tin - 1\nrepeat {\nn = nmin * exp((0:nt)/nt * log(nmax/nmin))\npow = 10^trunc(log10(n))\nssd[pow == 1] = 0\nssd[(ssd > 2) & (ssd < 8)] = 5\nfsd[ssd > 7] = fsd[ssd > 7] + 1\nnret = fsd * pow + ssd * pow/10\nif (nret[nt + 1] > nmax)\nnret[nt + 1] = nret[nt + 1] - pow[nt + 1]/2\nif (length(unique(nret)) == nt + 1)\nreturn(nret)\nnt = nt - 1\nmmstat.math = function(txt) {\ndollar = strsplit(txt, \"&\", fixed = T)[[1]]\nif (length(dollar) < 2)\nreturn(txt)\nres = paste0(\"expression(paste(\\\"\", dollar[1], \"\\\"\")\nfor (i in 2:length(dollar)) {\npercent = strsplit(dollar[i], \";\", fixed = T)[[1]]\nlp = length(percent)\nif (lp == 1)\nres = paste0(res, \",\\\"\", percent[1], \"\\\"\")\nelse {\nif (lp > 2)\npercent[2] = paste(percent[2:lp], sep = \";\")\nres = paste0(res, \",\", percent[1], \",\\\"\", percent[2], \"\\\"\")\nres = paste0(res, \"))\")\neval(parse(text = res))\nis.ASCII = function(txt) {\nall(charToRaw(txt) <= as.raw(127))\nstopif = function(cond, txt) {\nif (cond) stop(txt)\nmmstat.html = function(file, ...) {\nstopif(!file.exists(file), sprintf(\"File '%s' does not exist\", file))\nhtml = paste0(readLines(file), collapse = \"\")\nstopif(!is.ASCII(html), sprintf(\"File '%s' contains non-ASCII symbols\", file))\nargs = list(...)\ncond = sapply(args, length)\nstopif(!all(cond), paste(\"mmstat.html - Zero length arguments:\",\npaste(names(args)[!cond],\ncollapse = \", \")))\nif (length(args)) {\nstopif(any(sapply(args, is.null)),\n\"One or more arguments contain a NULL\")\nargs$fmt = html\nhtml = do.call(\"sprintf\", args)\nreturn(html)\nmmstat.plotTestRegions = function(crit, xlim, cex, close = F, col = \"black\",\nlabel = NULL, pos = 1) {\nlines(xlim, c(ylim[1], col = col)\nif (close) {\nlines(c(xlim[1], xlim[1]), ylim, col = col)\ncu = max(crit[1], xlim[1])\nif (crit[1] >= xlim[1]) {\nlines(c(cu, cu), ylim, col = col)\ntext((cu + xlim[1])/2, mean(ylim), mmstat.math(\"\\\"&H[1];\\\"\"),\ncex = cex, col = col)\nco = min(crit[2], xlim[2])\nif (crit[2] <= xlim[2]) {\nlines(c(co, co), ylim, col = col)\ntext((co + xlim[2])/2, mean(ylim), mmstat.math(\"\\\"&H[1];\\\"\"),\ntext((co + cu)/2, mean(ylim), mmstat.math(\"\\\"&H[0];\\\"\"), cex = cex, col = col)\nif (!is.null(text)) {\nif (pos == 2)\ntext(xlim[1], mmstat.pos(ylim, -0.25), label, col = col, cex = cex, pos = 4)\nif (pos == 4)\ntext(xlim[2], mmstat.pos(ylim, -0.25), label, col = col, cex = cex, pos = 2)\nmmstat.htest = function(...) {\naddnames = function(txt1, txt2) {\nif (length(txt1)) {\ncont = txt2 %in% txt1\nret = c(txt1, txt2[!cont])\n} else {\nret = txt2\nret\nhtest = list(method = list(attr = NULL, names = \"\", fmt = \"%s\", lines = 0),\nalternative = list(attr = NULL,\nnames = \"Alternative:\",\nnull.value = list(attr = \"names\",\nnames = vector(\"character\", 0),\ndata.name = list(attr = NULL, names = \"Data:\",\nfmt = \"%.4f\",\nconf.int = list(attr = \"conf.level\",\nlines = 1),\nstatistic = list(attr = \"names\",\nfmt = \"%.0f\",\np.value = list(attr = NULL,\nnames = \"p-value:\",\ntests = list(...)\nnhtest = names(htest)\nnrow = vector(\"numeric\", length(htest))\nlines = 0\nfor (j in seq(nhtest)) {\nname = nhtest[j]\nattr = htest[[nhtest[j]]]$attr\nif (!is.null(attr)) {\n# find all names\nfor (i in seq(tests)) {\nhtest[[name]]$names = addnames(htest[[name]]$names,\nattr(tests[[i]][[name]], attr))\n# grab all values\nnrow[j] = length(htest[[name]]$names)\nhtest[[name]]$tab = matrix(\"\", nrow = nrow[j], ncol = length(tests))\nfor (i in seq(tests)) {\ntelem = tests[[i]][[name]]\nif (!is.null(telem)) {\nhtest[[name]]$tab[1, i] = sprintf(htest[[name]]$fmt, telem)\n} else if (attr == \"conf.level\") {\nhtest[[name]]$tab[match(as.character(attr(telem, attr)),\nhtest[[name]]$names), i] = paste0(\"[\", round(telem[1], 4), \",\nround(telem[2], 4), \"]\")\n} else {\nhtest[[name]]$names), i] = sprintf(htest[[name]]$fmt, telem)\n}\nif (attr == \"conf.level\") {\nhtest[[name]]$names = sprintf(\"%.1f%% CI\", 100 *\nas.numeric(htest[[name]]$names))\nlines = lines + htest[[name]]$lines\ntab = matrix(\"\", nrow = sum(nrow) + lines, ncol = 1 + length(tests))\npos = 1\nname = nhtest[j]\nlen = length(htest[[name]]$names)\ntab[pos:(pos + len - 1), 1] = htest[[name]]$names\ntab[pos:(pos + len - 1),\n2:(1 + length(tests))] = htest[[name]]$tab\npos = pos + len + htest[[name]]$lines\nmaxlen = apply(nchar(tab), 2, max)\nfor (j in seq(tab)) {\nif (j <= nrow(tab))\ntab[j] = sprintf(\"%-*s\", maxlen[1 + ((j - 1) %/% nrow(tab))], tab[j])\npaste(apply(tab, 1, paste, collapse = \" \"), collapse = \"n\")\nmmstat.sliderInput = function(...) {\niapply = function(l) {\nret = l\nif (is.list(l)) {\nif (!is.null(ret$name) && (ret$name == \"input\")) {\nret$attribs[[\"data-values\"]] = ticks\n} else {\nfor (i in seq(ret)) ret[[i]] = iapply(ret[[i]])\nif ((compareVersion(mmstat$shiny, \"0.11\") >= 0) &&\n!is.null(args$ticks) && length(args$ticks)) {\nticks = paste0(args$ticks, collapse = \",\")\nargs$ticks = T\nhtml = do.call(\"sliderInput\", args)\nhtml = iapply(html)\nhtml\nmmstat.ui.elem = function(inputId, type, ...) {\nfound = F\nelem = list(...)\nelem$inputId = inputId\nelem$type = type\nshinytypes = c(\"actionButton\",\n\"checkboxInput\",\nmmstattypes = c(\"sampleSize\",\n\"drawSample\",\n\"confidenceLevel\",\npos = pmatch(type, shinytypes)\nif (!is.na(pos)) {\nfound = T\nif (elem$type == \"actionButton\") {\nif (is.null(elem$value))\nelem$value = 0\nelem$call = shinytypes[pos]\npos = pmatch(type, mmstattypes)\nif (elem$type == \"sampleSize\") {\nif (is.null(elem$label))\nelem$label = gettext(\"Sample size (n)\")\nelem$call = \"mmstat.sliderInput\"\nelem$step = 1\nelem$value = as.numeric(compareVersion(mmstat$shiny, \"0.11\") < 0)\nif (elem$type == \"drawSample\") {\nelem$call = \"actionButton\"\nelem$label = gettext(\"Draw sample\")\nif (elem$type == \"testHypotheses\") {\nelem$call = \"radioButtons\"\nelem$label = gettext(\"Choose test type\")\nelem$choices = gettext(c(\"two.sided\", \"less\", \"greater\"), \"name\")\nif (is.null(elem$value))\nelem$value = \"two.sided\"\nif (elem$type == \"significance\") {\nelem$call = \"mmstat.sliderInput\"\nif (is.null(elem$ticks))\nelem$ticks = c(0.1, 0.25, 1, 2, 5, 10, 20)\nelem$label = HTML(gettext(\"Select significance\nlevel (&alpha;)\"))\nelem$step = 1\nelem$max = length(elem$ticks)\nelem$value = 6 - as.numeric(compareVersion(mmstat$shiny, \"0.11\") >= 0)\nif (elem$type == \"confidenceLevel\") {\nelem$ticks = c(80, 85, 90, 99.5, 99.9)\nif (is.null(elem$label))\nelem$label = HTML(gettext(\"Select confidence level (1-&alpha;)\"))\nelem$step = 1\nelem$max = length(elem$ticks)\nelem$value = 4 - as.numeric(compareVersion(mmstat$shiny, \"0.11\") >= 0)\nif (elem$type == \"dataSet\") {\nelem$call = \"selectInput\"\nelem$label = gettext(\"Select a data set\")\nif (is.null(elem$choices))\nelem$choices = mmstat.getDataNames(gsub(\".rds$\", \"\",\nlist.files(pattern = \"*.rds\")))\nelem$value = mmstat.getDataNames()\nif (elem$type == \"variable1\") {\nelem$label = gettext(\"Select a variable\")\nelem$choices = mmstat.getVarNames(1, elem$vartype)\nif (elem$type == \"variableN\") {\nelem$call = \"selectInput\"\nelem$multiple = T\nelem$label = gettext(\"Select variable(s)\")\nif (elem$type == \"fontSize\") {\nelem$label = gettext(\"Font size\")\nif (is.null(elem$min))\nelem$min = 1\nif (is.null(elem$max))\nelem$max = 1.5\nif (is.null(elem$step))\nelem$step = 0.05\nelem$value = elem$min\nif (elem$type == \"speedSlider\") {\nelem$call = \"mmstat.sliderInput\"\nelem$label = list(NULL)\nelem$min = 0\nstopif(!found, sprintf(\"mmstat.ui.elem: Type \\\"%s\\\" unknown\", type))\nif (is.null(elem$update))\nelem$update = paste0(\"update\", ucfirst(elem$call))\nmmstat$UI[[inputId]] <<- elem\nmmstat.ui.call = function(inputId, ...) {\nelem = mmstat$UI[[inputId]]\nwhat = elem$call\nargs = list(...)\nfor (name in names(elem)) {\nif (is.null(args[[name]]))\nargs[[name]] = elem[[name]]\nargs$call = NULL\nif ((what == \"selectInput\") || (what == \"checkboxGroupInput\"))\nargs$value = NULL\ndo.call(what, args)\nmmstat.ui.update = function(inputId, ...) {\nwhat = elem$update\nif ((what == \"updateSelectInput\") ||\n(what == \"updateCheckboxGroupInput\"))\nmmstat.lang = function(deflang = NULL) {\npof = list.files(pattern = \"*.po$\")\nif (is.null(deflang)) {\nlang = sapply(strsplit(pof, \".\", fixed = T),\nfunction(elem) {elem[1]})\npat = paste0(\"_\", lang, \"$\")\npath = getwd()\npath = strsplit(path, \"/\", fixed = T)[[1]]\npos = -1\nfor (i in seq(pat)) {\np = grep(pat[i], path)\nif (length(p)) {\np = max(p)\nif (p > pos) {\nlind = i\nlind = match(paste0(deflang, \".po\"), pof)\nif (is.na(lind))\nlind = (-1)\nmsgfn = ifelse(lind > 0, pof[lind], \"default.po\")\nmmstat.warn(!file.exists(msgfn), sprintf(\"File '%s' does not exist\", msgfn))\nmsg = paste(readLines(msgfn), collapse = \" \")\nmsgid = regmatches(msg, gregexpr(\"msgid\\\\s*\\\".*?\\\"\", msg))\ntmp = strsplit(msgid[[1]], \"\\\"\")\nmsgid = sapply(tmp, function(vec) {\npaste0(vec[2:length(vec)])\n})\nmsgstr = regmatches(msg, gregexpr(\"msgstr\\\\s*\\\".*?\\\"\", msg))\ntmp = strsplit(msgstr[[1]], \"\\\"\")\nmsgstr = sapply(tmp, function(vec) {\nmmstat$messages <<- list(id = msgid, str = msgstr)\nhtmlTable = function(tab, vars = NULL, lines = NULL, cex = 1) {\nhtml = sprintf(\"<table style=\\\"text-align:right;width:100%%;\nfont-size:%i%%;\\\">\",\nas.integer(100 * cex))\nif (length(vars) == 2)\nhtml = paste0(html,\nsprintf(\"<tr><td colspan=\\\"%i\\\">\",1 + ncol(tab)),\nsprintf(gettext(\"Columns: %s\"), gettext(vars[1])),\n\"</td></tr>\")\nwidth = as.integer(100/(1 + ncol(tab)))\nhtml = paste0(html, sprintf(\"<TR><TD style=\\\"width:%i%%\\\"></TD>\",\nwidth))\nhtml = paste0(html, paste0(sprintf(\"<TD style=\\\"width:%i%%;\nbackground-color:#AAAAAA\\\"><b>\",\nwidth),\nsprintf(\"%s\", colnames(tab)),\nhtml = paste0(html, \"</TR>\")\nalign = ifelse(is.na(suppressWarnings(as.numeric(rownames(tab)))),\n\"left\", \"right\")\nfor (i in seq(nrow(tab))) {\nhtml = paste0(html, sprintf(\"<TR><TD style=\\\"text-align:%s;\nbackground-color:\n#AAAAAA\\\"><b>%s</b></TD>\",\nalign[i], rownames(tab)[i]))\nif (i%%2 == 0) {\nhtml = paste0(html, paste0(\"<TD style=\n\\\"background-color:\nsprintf(\"%i\", tab[i, ]),\nhtml = paste0(html, paste0(\"<TD>\", sprintf(\"%i\", tab[i, ]),\nhtml = paste0(html, \"</TR>\")\nhtml = paste0(html, sprintf(\"<tr><td colspan=\\\"%i\\\"\nstyle=\\\"text-align:left;\\\">\",\n1 + ncol(tab)),\nsprintf(gettext(\"Rows: %s\"), vars[2]),\n\"</td></tr>\",\nsprintf(\"<tr><td colspan=\\\"%i\\\"><hr></td></tr>\",\n1 + ncol(tab)))\nfor (i in seq(lines)) html = paste0(html,\nsprintf(\"<tr><td colspan=\n\\\"%i\\\" style=\n\\\"text-align:left;\\\">\",\nlines[i], \"</td></tr>\")\nhtml = paste0(html, \"</table>\")\nucfirst = function(txt) {\nreturn(paste0(toupper(substr(txt, 1, 1)), substring(txt, 2)))\ngettext = function(msg, utype = \"vector\") {\ntype = pmatch(utype, c(\"name\", \"numeric\"))\nif (is.na(type)) {\nret = paste0(ifelse(mmstat$debug > 2, \"?\", \"\"), msg)\npos = match(msg, mmstat$messages$id)\nind = (1:length(pos))[!is.na(pos)]\nret[ind] = mmstat$messages$str[pos[ind]]\n} else if (type == 1) {\nret = as.list(msg)\nnames(ret) = gettext(msg)\n} else if (type == 2) {\nret = as.list(seq(msg))\nreturn(ret)\n# Logging\nmmstat.getLog = function(session) {\nif (!mmstat$debug)\nreturn(\"\")\ninvalidateLater(100, session)\npaste0(\"<hr>\", paste(mmstat$log, collapse = \"<br>\"))\nmmstat.log = function(txt) {\nif (mmstat$debug > 1)\nmmstat$log <<- cbind(sprintf(\"%s (Info): %s\",\ndate(), txt),\nmmstat.warn = function(cond, txt) {\nif ((mmstat$debug > 0) && cond)\nmmstat$log <<- cbind(sprintf(\"%s (Warn): %s\",\nmmstat.input = function(input) {\nif (mmstat$debug > 1) {\nni = names(input)\nfor (i in seq(ni)) {\nout = capture.output(input[[ni[i]]])\nout[1] = paste0(ni[i], \": \", out[1])\nfor (j in seq(out))\nmmstat$log <<- cbind(sprintf(\"%s (Input): %s\",\ndate(), out[j]), mmstat$log)\n# Main\nmmstat = list(debug = 1,\nshiny = sessionInfo()$otherPkgs$shiny$Version,\ncol = list(daquamarine = \"#1B9E77\",\ndorange = \"#D95F02\",\n1, 2.5, 5, 10, 20),\nUI = vector(\"list\", 0))\nmmstat.log(\"Starting app\")\n", "output_sequence": "Shows either a 2D or 3D scatterplot. One has also the option to compute Pearson's correlation and Spearman's rank correlation for variables of the data sets CARS, DECATHLON and USCRIME."}, {"input_sequence": "Author: Shi Chen ; rm(list = ls(all = TRUE))\ngraphics.off()\n# install and load packages\nlibraries = c(\"zoo\", \"tseries\", \"xts\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# load dataset\nload(file = \"crix.RData\")\n# plot of crix\n# plot(as.xts(crix), type=\"l\", auto.grid=FALSE, main = NA)\nplot(crix1, ylab = NA, xlab = NA)\n# plot of crix return\nret = diff(log(crix1))\n# plot(as.xts(ret), type=\"l\", auto.grid=FALSE, main = NA)\nplot(ret, ylab = NA, xlab = NA)\nmean(ret)\nsd(ret)\n# histogram of price\nhist(crix, col = \"grey\", breaks = 40, freq = FALSE)\nlines(density(crix), lwd = 2)\npar(mfrow = c(1, 2))\n# histogram of returns\nhist(ret, col = \"grey\", breaks = 20, freq = FALSE, ylim = c(0, 25), xlab = NA)\nlines(density(ret), lwd = 2)\nmu = mean(ret)\nsigma = sd(ret)\nx = seq(-4, 4, length = 100)\ncurve(dnorm(x, mean = mean(ret), sd = sd(ret)), add = TRUE, col = \"darkblue\",\nlwd = 2)\n# qq-plot\nqqnorm(ret)\nqqline(ret, col = \"blue\", lwd = 3)\n# normality test\nret1 = diff(log(crix))\nks.test(ret1, \"pnorm\", mean(ret), sd(ret))\nshapiro.test(ret1)\n", "output_sequence": "Produces econometric analysis results using CRIX data."}, {"input_sequence": "Author: Shi Chen ; rm(list = ls(all = TRUE))\ngraphics.off()\n# install and load packages\nlibraries = c(\"FinTS\", \"tseries\", \"forecast\", \"fGarch\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# load dataset\nload(file = \"crix.RData\")\nret = diff(log(crix1))\n# vol cluster\nfit202 = arima(ret, order = c(2, 0, 2))\npar(mfrow = c(1, 1))\nres = fit202$residuals\n# different garch model\nfg11 = garchFit(data = res, data ~ garch(1, 1))\nsummary(fg11)\nfg12 = garchFit(data = res, data ~ garch(1, 2))\nsummary(fg12)\nfg21 = garchFit(data = res, data ~ garch(2, 1))\nsummary(fg21)\nfg22 = garchFit(data = res, data ~ garch(2, 2))\nsummary(fg22)\n# residual plot\nreszo = zoo(fg11@residuals, order.by = index(crix1))\nplot(reszo, ylab = NA, lwd = 2)\npar(mfrow = c(1, 2))\nfg11res2 = fg11@residuals\nacfres2 = acf(fg11res2, lag.max = 20, ylab = \"Sample Autocorrelation\",\nmain = NA, lwd = 2)\npacfres2 = pacf(fg11res2, lag.max = 20, ylab = \"Sample Partial Autocorrelation\",\nmain = NA, lwd = 2, ylim = c(-0.5, 0.5))\nfg12res2 = fg12@residuals\nacfres2 = acf(fg12res2, lag.max = 20, ylab = \"Sample Autocorrelation\",\n# qq plot\nplot(fg11, which = 13) #9,10,11,13\n# kp test\nset.seed(100)\nx = rnorm(200)\n# Do x and y come from the same distribution?\nks.test(x, fg11@residuals)\n", "output_sequence": "Produces GARCH estimation results using ARIMA model residuals."}, {"input_sequence": "Author: Lasse Groth ; # ------------------------------------------------------------------------------ Book: SFS\n# convergence to infinity for the stable distributed random variables is higher than for standard normal variables. Plots\n# the convergence rate of maximum for n random variables with a standard normal cdf and with a 1.1-stable cdf. Refers to\n# exercise 16.2 in SFS. ------------------------------------------------------------------------------ Usage: -\n# ------------------------------------------------------------------------------ Output: Plots of the convergence rate of\n# maxima for standard normal cdf and 1.1-stable cdf.\n# ------------------------------------------------------------------------------ Example: -\ninstall.packages(\"QRM\")\n# Load library\nlibrary(QRM)\n# Close all plots and clear variables\ngraphics.off()\nrm(list = ls(all = TRUE))\nn0 = 10\nn = 10000/n0\nflag = 1\nm = n/n0\ndat1 = dat2 = matrix(, 100, 2)\nfor (i in 1:m) {\n\ndat1[i, 1] = i * 10\ndat1[i, 2] = max(rnorm(i, mean = 0, sd = 1)) #for normal distributions\ndat2[i, 1] = i * 10\ndat2[i, 2] = max(rstable(i, 1.1, 0)) #for stable distributions\n}\nplot(dat1[, 2], xlab = (\"n\"), ylab = (\"M(n)\"), col = \"blue\")\ntitle(\"Limit of M(n) for normal cdf\")\ndev.new()\nplot(dat2[, 2], xlab = (\"n\"), ylab = (\"M(n)\"), col = \"blue\")\ntitle(\"Limit of M(n) for stable cdf\")\n", "output_sequence": "Shows the rate of convergence to infinity for the stable distributed random variables is higher than for standard normal variables. Plots the convergence rate of maximum for n random variables with a standard normal cdf and with a 1:1-stable cdf. Refers to exercise 16.2 in SFS. Requires the quantlet \"stabrnd.m\" for MatLab."}, {"input_sequence": "Author: Lasse Groth ; function [x] = stabrnd(alpha, beta, c, delta, m, n)\n% STABRND.M\n% Stable Random Number Generator (McCulloch 12/18/96)\n%\n% x = stabrnd(alpha, beta, c, delta, m, n);\n% Returns m x n matrix of iid stable random numbers with\n% characteristic exponent alpha in [.1,2], skewness parameter\n% beta in [-1,1], scale c > 0, and location parameter delta.\n% Based on the method of J.M. Chambers, C.L. Mallows and B.W.\n% Stuck, \"A Method for Simulating Stable Random Variables,\"\n% JASA 71 (1976): 340-4.\n% Encoded in MATLAB by J. Huston McCulloch, Ohio State\n% University Econ. Dept. (mcculloch.2@osu.edu). This 12/18/96\n% version uses 2*m*n calls to RAND, and does not rely on\n% the STATISTICS toolbox.\n% The CMS method is applied in such a way that x will have the\n% log characteristic function\n% log E exp(ixt) = i*delta*t + psi(c*t),\n% where\n% psi(t) = -abs(t)^alpha*(1-i*beta*sign(t)*tan(pi*alpha/2))\n% for alpha ~= 1,\n% = -abs(t)*(1+i*beta*(2/pi)*sign(t)*log(abs(t))),\n% for alpha = 1.\n% With this parameterization, the stable cdf S(x; alpha, beta,\n% c, delta) equals S((x-delta)/c; alpha, beta, 1, 0). See my\n% \"On the parametrization of the afocal stable distributions,\"\n% _Bull. London Math. Soc._ 28 (1996): 651-55, for details.\n% When alpha = 2, the distribution is Gaussian with mean delta\n% and variance 2*c^2, and beta has no effect.\n% When alpha > 1, the mean is delta for all beta. When alpha\n% <= 1, the mean is undefined.\n% When beta = 0, the distribution is symmetrical and delta is\n% the median for all alpha. When alpha = 1 and beta = 0, the\n% distribution is Cauchy (arctangent) with median delta.\n% When the submitted alpha is > 2 or < .1, or beta is outside\n% [-1,1], an error message is generated and x is returned as a\n% matrix of NaNs.\n% Alpha < .1 is not allowed here because of the non-negligible\n% probability of overflows.\n% If you're only interested in the symmetric cases, you may just\n% set beta = 0 and skip the following considerations:\n% When beta > 0 (< 0), the distribution is skewed to the right\n% (left).\n% When alpha < 1, delta, as defined above, is the unique fractile\n% that is invariant under averaging of iid contributions. I\n% call such a fractile a \"focus of stability.\" This, like the\n% mean, is a natural location parameter.\n% When alpha = 1, either every fractile is a focus of stability,\n% as in the beta = 0 Cauchy case, or else there is no focus of\n% stability at all, as is the case for beta ~=0. In the latter\n% cases, which I call \"afocal,\" delta is just an arbitrary\n% fractile that has a simple relation to the c.f.\n% When alpha > 1 and beta > 0, med(x) must lie very far below\n% the mean as alpha approaches 1 from above. Furthermore, as\n% alpha approaches 1 from below, med(x) must lie very far above\n% the focus of stability when beta > 0. If beta ~= 0, there\n% is therefore a discontinuity in the distribution as a function\n% of alpha as alpha passes 1, when delta is held constant.\n% CMS, following an insight of Vladimir Zolotarev, remove this\n% discontinuity by subtracting\n% beta*c*tan(pi*alpha/2)\n% (equivalent to their -tan(alpha*phi0)) from x for alpha ~=1\n% in their program RSTAB, a.k.a. RNSTA in IMSL (formerly GGSTA).\n% The result is a random number whose distribution is a contin-\n% uous function of alpha, but whose location parameter (which I\n% call zeta) is a shifted version of delta that has no known\n% interpretation other than computational convenience.\n% The present program restores the more meaningful \"delta\"\n% parameterization by using the CMS (4.1), but with\n% beta*c*tan(pi*alpha/2) added back in (ie with their initial\n% tan(alpha*phi0) deleted). RNSTA therefore gives different\n% results than the present program when beta ~= 0. However,\n% the present beta is equivalent to the CMS beta' (BPRIME).\n% Rather than using the CMS D2 and exp2 functions to compensate\n% for the ill-condition of the CMS (4.1) when alpha is very\n% near 1, the present program merely fudges these cases by\n% computing x from their (2.4) and adjusting for\n% beta*c*tan(pi*alpha/2) when alpha is within 1.e-8 of 1.\n% This should make no difference for simulation results with\n% samples of size less than approximately 10^8, and then\n% only when the desired alpha is within 1.e-8 of 1, but not\n% equal to 1.\n% The frequently used Gaussian and symmetric cases are coded\n% separately so as to speed up execution.\n% Additional references:\n% V.M. Zolotarev, _One Dimensional Stable Laws_, Amer. Math.\n% Soc., 1986.\n% G. Samorodnitsky and M.S. Taqqu, _Stable Non-Gaussian Random\n% Processes_, Chapman & Hill, 1994.\n% A. Janicki and A. Weron, _Simulaton and Chaotic Behavior of\n% Alpha-Stable Stochastic Processes_, Dekker, 1994.\n% J.H. McCulloch, \"Financial Applications of Stable Distributons,\"\n% _Handbook of Statistics_ Vol. 14, forthcoming early 1997.\n% Errortraps:\nif alpha < .1 | alpha > 2\ndisp('Alpha must be in [.1,2] for function STABRND.')\nalpha\nx = NaN * zeros(m,n);\nreturn\nend\nif abs(beta) > 1\ndisp('Beta must be in [-1,1] for function STABRND.')\nbeta\n% Generate exponential w and uniform phi:\nw = -log(rand(m,n));\nphi = (rand(m,n)-.5)*pi;\n% Gaussian case (Box-Muller):\nif alpha == 2\nx = (2*sqrt(w) .* sin(phi));\nx = delta + c*x;\n% Symmetrical cases:\nif beta == 0\nif alpha == 1 % Cauchy case\nx = tan(phi);\nelse\nx = ((cos((1-alpha)*phi) ./ w) .^ (1/alpha - 1) ...\n.* sin(alpha * phi) ./ cos(phi) .^ (1/alpha));\nend\n% General cases:\nelse\ncosphi = cos(phi);\nif abs(alpha-1) > 1.e-8\nzeta = beta * tan(pi*alpha/2);\naphi = alpha * phi;\nx = ((sin(aphi) + zeta * cos(aphi)) ./ cosphi) ...\n./ (w .* cosphi)) .^ ((1-alpha)/alpha);\nbphi = (pi/2) + beta * phi;\nx = (2/pi) * (bphi .* tan(phi) - beta * log((pi/2) * w ...\n.* cosphi ./ bphi));\nif alpha ~= 1\nx = x + beta * tan(pi * alpha/2);\nend\n% Finale:\nx = delta + c * x;\nreturn\n", "output_sequence": "Shows the rate of convergence to infinity for the stable distributed random variables is higher than for standard normal variables. Plots the convergence rate of maximum for n random variables with a standard normal cdf and with a 1:1-stable cdf. Refers to exercise 16.2 in SFS. Requires the quantlet \"stabrnd.m\" for MatLab."}, {"input_sequence": "Author: Lasse Groth ; % ---------------------------------------------------------------------\n% Book: SFS\n% ---------------------------------------------------------------------\n% Quantlet: SFSmsr1\n% Description: SFSmsr1 shows the rate of convergence to infinity for\n% the stable distributed random variables is higher than\n% for standard normal variables. Plots the convergence rate of\n% maximum for n random variables with a standard normal cdf\n% and with a 1.1-stable cdf. Requires the quantlet\n% \"stabrnd.m\".\n% Inputs: none\n% Output: Plots of the convergence rate of maxima for standard normal\n% cdf and 1.1-stable cdf.\n% Example: -\n% Author: Lasse Groth 20090728\n%\nclear\nclc\nclose all\nn0 = 10;\nn = 10000 / n0;\nflag = 1;\nm = n/n0;\nfor i=1:m\n\ndat1(i,1) = i*10;\ndat1(i,2) = max(randn(i,1)); %for normal distributions\ndat2(i,1) = i*10;\ndat2(i,2 ) = max(stabrnd(1.1, 0, 1, i, 1) );%for stable distributions\n\nend\nplot(dat1(:,1),dat1(:,2),'o')\ntitle('Limit of M(n) for normal cdf','FontSize',16,'FontWeight','Bold')\nxlabel('n','FontSize',16,'FontWeight','Bold')\nset(gca,'LineWidth',1.6,'FontSize',16,'FontWeight','Bold');\nbox on\n% to save the plot in pdf or png please uncomment next 2 lines:\n% print -painters -dpdf -r600 SFSmsr1_1.pdf\nfigure\nplot(dat2(:,1),dat2(:,2),'o')\ntitle('Limit of M(n) for stable cdf','FontSize',16,'FontWeight','Bold')\n% print -painters -dpdf -r600 SFSmsr1_2.pdf\n", "output_sequence": "Shows the rate of convergence to infinity for the stable distributed random variables is higher than for standard normal variables. Plots the convergence rate of maximum for n random variables with a standard normal cdf and with a 1:1-stable cdf. Refers to exercise 16.2 in SFS. Requires the quantlet \"stabrnd.m\" for MatLab."}, {"input_sequence": "Author: Lasse Groth ; % ---------------------------------------------------------------------\n% Book: SFS\n% ---------------------------------------------------------------------\n% Quantlet: SFStailGEV\n% Description: SFStailGEV fits Generalized Extreme Value Distribution\n% to the negative log-returns of portfolio (Bayer, BMW,\n% Siemens), time period: from 1992-01-01 to 2006-09-21,\n% and produces QQ-plot and PP-plot.\n% Usage: SFStailGEV\n% Inputs: None\n% Output: QQ, PP plot with Generalized Extreme Value Distribution.\n% Example: -\n% Author: Barbara Choros 20080709\nfunction SFStailGEV\nclc;\nclose all;\na = load('Bay9906_close_2kPoints.txt','-ascii');\nd = a+b+c;\nx = log(d(1:end-1))-log(d(2:end));%negative log-returns\nT = length(x);\nn = 20;\nk = T/n;\nfor j=1:k\nr = x((j-1)*n+1:j*n);\nz(j) = max(r);\nend;\nparmhat = gevfit(z);\nw = sort(z);\nK = parmhat(1);%shape parameter\nt = (1:k)/(k+1);\ny = gevinv(t,K,sigma,mu);\nhold on\nplot(y,y,'r','LineWidth',2)\nscatter(w,y,'.')\ntitle('QQ plot, Generalized Extreme Value Distribution')\nhold off\n%---------------------------------------------------------------------\nfigure\ny = gevcdf(w,K,sigma,mu);\nscatter(y,t,'.')\ntitle('PP plot, Generalized Extreme Value Distribution')\n", "output_sequence": "Fits a Generalized Extreme Value Distribution to the negative log-returns of a portfolio (Bayer, BMW, Siemens) for the time period from 1992-01-01 to 2006-09-21 and produces a QQ-plot and a PP-plot. Corresponds to exercise 16.5 in SFS."}, {"input_sequence": "Author: Lasse Groth ; # ------------------------------------------------------------------------------ Book: SFS\n# ------------------------------------------------------------------------------ Description: SFStailGEV fits a Generalized\n# Extreme Value Distribution to the negative log-returns of a portfolio (Bayer, BMW, Siemens) for the time period from\n# 1992-01-01 to 2006-09-21 and produces a QQ-plot and a PP-plot. Corresponds to exercise 16.5 in SFS.\n# ------------------------------------------------------------------------------ Usage: -\n# Generalized Extreme Value Distribution. ------------------------------------------------------------------------------\n# Example: - ------------------------------------------------------------------------------ Author: Lasse Groth 20091013\n# Load library\ninstall.packages(\"QRM\")\nlibrary(QRM)\n# Close all plots and clear variables\ngraphics.off()\nrm(list = ls(all = TRUE))\n# Set working directory and load datasets\nsetwd(\"C:/...\")\na = read.table(\"Bay9906_close_2kPoints.txt\")\nd = a + b + c #Create the portfolio\nlg = dim(d)\nx = log(d[-lg[1], ]) - log(d[-1, ]) #Negative log-return\n# Determine the Block Maxima data\nT = length(x)\nn = 20\nk = T/n\nz = matrix(, , )\nfor (j in 1:k) {\n\nr = x[((j - 1) * n + 1):(j * n)]\nz[j] = max(r)\n}\nw = sort(z)\nGEV = fit.GEV(z) #Fit the Generalized Extreme Value Distribution\nK = GEV$par.ests[1] #shape parameter\nt = (1:k)/(k + 1)\ny1 = qGEV(t, K, mu, sigma)\n# Plot the QQ plot\ndev.new()\nplot(w, y1, col = \"blue\", pch = 23, bg = \"blue\", xlab = c(\"\"), ylab = c(\"\"))\nlines(y1, y1, type = \"l\", col = \"red\", lwd = 2)\ntitle(\"QQ plot, Generalized Extreme Value Distribution\")\n# Plot the PP plot\nplot(y2, t, col = \"blue\", pch = 23, bg = \"blue\", xlab = c(\"\"), ylab = c(\"\"))\nlines(y2, y2, type = \"l\", col = \"red\", lwd = 2)\ntitle(\"PP plot, Generalized Pareto Distribution\")\n", "output_sequence": "Fits a Generalized Extreme Value Distribution to the negative log-returns of a portfolio (Bayer, BMW, Siemens) for the time period from 1992-01-01 to 2006-09-21 and produces a QQ-plot and a PP-plot. Corresponds to exercise 16.5 in SFS."}, {"input_sequence": "Author: Lasse Groth ; # ------------------------------------------------------------------------------ Book: SFS\n# empirical mean excess function, the mean excess function of generalized Pareto distribution, the mean excess function of\n# Pareto distribution with parameter estimated with Hill estimator for the negative log-returns of portfolio (Bayer, BMW,\n# Siemens), time period: from 1992-01-01 to 2006-09-21. Refers to exercise 16.8 in SFS.\n# ------------------------------------------------------------------------------ Usage: -\n# ------------------------------------------------------------------------------ Output: Plot of the empirical mean excess\n# function, the mean excess function of generalized Pareto distribution, the mean excess function of Pareto distribution\n# with parameter estiamted with Hill estimator.\n# ------------------------------------------------------------------------------ Example: -\n# Load libraries\ninstall.packages(c(\"matlab\", \"QRM\"))\nlibrary(matlab)\n# Close all plots and clear variables\ngraphics.off()\nrm(list = ls(all = TRUE))\n# Set working directory and load datasets\nsetwd(\"C:/...\")\na = read.table(\"Bay9906_close_2kPoints.txt\")\nd = a + b + c #Create the portfolio\nlg = dim(d)\nx = log(d[-lg[1], ]) - log(d[-1, ]) #Negative log-returns\nn = length(x)\nx = sort(x, decreasing = TRUE)\nm = 100\nx1 = x[1:m]\n# empirical mean excess function\nt = x[1:(m + 1)] #t must be >0\nMEF = matrix(, , )\nfor (i in 1:length(t)) {\ny = x[find(x > t[i])]\nMEF[i] = mean(y - t[i])\n}\nplot(t, MEF, type = \"l\", col = \"blue\", lwd = 3, ylim = c(0.005, 0.04), xlab = \"u\", ylab = \"e(u)\")\ntitle(\"Mean Excess Functions\")\n# mean excess function of generalized Pareto distribution, theorem 18.8\nk = 100\nGPD = fit.GPD(x, nextremes = k, type = \"ml\", information = \"observed\")\nK = GPD$par.ests[1]\ngpme = (sigma + K * (t - mean(t)))/(1 - K)\nlines(t, gpme, lwd = 3)\n# Hill estimator, mean excess function of Pareto distribution\nalphaH = (mean(log(x1)) - log(x1[k]))^(-1)\nsigmaH = x1[k] * (k/n)^(1/alphaH)\ngp1me = t/(alphaH - 1)\nlines(t, gp1me, col = \"red\", lwd = 3, lty = 5)\n", "output_sequence": "Plots the empirical mean excess function, the mean excess function of generalized Pareto distribution, the mean excess function of Pareto distribution with parameter estimated with Hill estimator for the negative log-returns of portfolio (Bayer, BMW, Siemens), time period: from 1992-01-01 to 2006-09-21. Refers to exercise 16.8 in SFS."}, {"input_sequence": "Author: Lasse Groth ; % ---------------------------------------------------------------------\n% Book:\n% ---------------------------------------------------------------------\n% Quantlet: SFSmeanExcessFun\n% Description: SFSmeanExcessFun plots the empirical mean excess\n% function, the mean excess function of generalized\n% distribution with parameter estiamted with Hill\n% estimator for the negative log-returns of portfolio\n% (Bayer, BMW, Siemens), time period: from 1992-01-01 to\n% 2006-09-21.\n% Usage: SFSmeanExcessFun\n% Inputs: None\n% Output: Plot of the empirical mean excess function, the mean\n% excess function of generalized Pareto distribution, the\n% parameter estiamted with Hill estimator.\n% Example:\n% Author: Barbara Choros 20080730\nclc;\nclose all;\na = load('Bay9906_close_2kPoints.txt','-ascii');\nd = a+b+c;\nx = log(d(1:end-1))-log(d(2:end));%negative log-returns\nn = length(x)\nx = sort(x,'descend');%from positive losses to negative profits\nk = 100;\nx1 = x(1:k,:);\n%empirical mean excess function\nt = x(1:k+1);%t must be >0\nfor i=1:length(t)\ny = x(find( x > t(i)));\nMEF(i) = mean(y-t(i));\nend\nplot(t,MEF,'Linewidth',1.5)\n%mean excess function of generalized Pareto distribution, theorem 18.8\ntheta = x(k+1);\nz = x(1:k)-theta;\nparams = gpfit(z);\nK = params(1)\ngpme = (sigma+K*(t-mean(t)))./(1-K);\nhold on\nplot(t,gpme,'k','Linewidth',1.5,'Linestyle',':')\n%Hill estimator, mean excess function of Pareto distribution\nalphaH = (mean(log(x1))-log(x1(k)))^(-1)\nsigmaH = x1(k)*(k/n)^(1/alphaH);\ngp1me = t./(alphaH-1)\nplot(t,gp1me,'r','Linewidth',1.5,'Linestyle','--')\ntitle('Mean Excess Functions')\nxlabel('u')\nhold off\n", "output_sequence": "Plots the empirical mean excess function, the mean excess function of generalized Pareto distribution, the mean excess function of Pareto distribution with parameter estimated with Hill estimator for the negative log-returns of portfolio (Bayer, BMW, Siemens), time period: from 1992-01-01 to 2006-09-21. Refers to exercise 16.8 in SFS."}, {"input_sequence": "Author: Xun Gong ; import requests\nfrom bs4 import BeautifulSoup as soup\nimport json\nimport pandas as pd\nimport os\n# from utils import ThreadWorker\n# returns a list of yelp restaurant names based on search page\ndef yelp_get_biz_names(find_loc = 'Berlin', cflt = 'Chinese', page = 0): ##Default test args\n# Argument page equals 1 by default\nif page == 0:\n# Visit the home page if page equals 0\nyelp_url = 'https://www.yelp.com/search?find_loc=%s&cflt=%s' % (find_loc,cflt)\nelse:\n# Change url by different argument\nyelp_url = 'https://www.yelp.com/search?find_loc=%s&cflt=%s&start=' % (find_loc,cflt) + str((page)*10)\nurl_request = requests.get(yelp_url)\nurl_content = url_request.content\nparsed_content = soup(url_content,\"lxml\")\nbiz_content = parsed_content.find_all('a', {\"class\":\"biz-name js-analytics-click\"})\nbiz_list = []\nfor biz in biz_content:\nbiz_href = biz.get('href')\nif 'adredir?' not in biz_href:\nbiz_list.append(biz_href)\nelse:\ncontinue\nreturn biz_list\n# Define the function to read the first page of reviews of each biz\ndef yelp_get_biz_reviews(biz):\n# get single biz url\nyelp_biz_url = 'https://www.yelp.com/%s' % biz\nurl_request = requests.get(yelp_biz_url)\n# Using BeautifulSoup to parse webpage source code\nparsed_content = soup(url_content, \"lxml\")\n# Finding all the <script type=\"application/ld+json\"> tag content\ncontainers = parsed_content.find('script', {\"type\": \"application/ld+json\"})\ncontainers_content = containers.contents[0]\n# Deserialized the content to python dictionary\ncontainers_content_dict = json.loads(containers_content)\n# initial empty list to load the data\nreview_list = []\n# get all the reviews in the content dict\nreviews = containers_content_dict['review']\n# extract useful attributes in each review and to review_list\nfor item in reviews:\nrestr_name = containers_content_dict['name']\nrestr_rating = containers_content_dict['aggregateRating']['ratingValue']\nreview_date = item['datePublished']\nreview_list.append(\n[restr_name, restr_cuis, float(restr_rating), restr_price_range,\nint(review_rating), review_date, review_desc])\nreturn review_list\nbiz_berlin_chi_20 = []\n# get the biz names of the first two pages\nfor i in range(0, 2):\nbiz_berlin_chi_20.extend(yelp_get_biz_names('Berlin', 'Chinese', i))\nprint(biz_berlin_chi_20)\nbiz_berlin_chi_20_reviews = []\nfor biz in biz_berlin_chi_20:\nbiz_berlin_chi_20_reviews.extend(yelp_get_biz_reviews(biz))\nreview_info_df = pd.DataFrame(biz_berlin_chi_20_reviews, columns=['Restaurant Name', 'Cuisine', 'Rating', 'Price Range','Review Rating', 'Review Date', 'Review'])\nreview_info_df.to_csv(os.getcwd() + '\\\\Reviews_cn.csv', index=False)\n", "output_sequence": "Crawl user-published reviews on Yelp and apply text mining and machine learing techniques using Python. Web Scraping and Word Cloud, Sentiment Analysis using Textblob, Sentiment Classification using SVM"}, {"input_sequence": "Author: Xun Gong ; # further information about wordcloud go to https://github.com/amueller/word_cloud\nfrom wordcloud import WordCloud\nimport pandas as pd\nimport matplotlib.pyplot as plt\nreviews_df = pd.read_csv(\"Reviews.csv\", sep=',', encoding='cp1251')\ndescription_list = reviews_df.Review.values\ntext = str(description_list)\n# lower max_font_size\nwordcloud = WordCloud(max_font_size=40).generate(text)\nplt.figure()\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()\n", "output_sequence": "Crawl user-published reviews on Yelp and apply text mining and machine learing techniques using Python. Web Scraping and Word Cloud, Sentiment Analysis using Textblob, Sentiment Classification using SVM"}, {"input_sequence": "Author: Xun Gong ; import pandas as pd\nimport matplotlib.pyplot as plt\nfrom textblob import TextBlob\nfrom itertools import *\nfrom matplotlib import rcParams\nrcParams.update({'figure.autolayout': True})\nreviews_df = pd.read_csv('Reviews_cn.csv', encoding = 'cp1252')\n# print(reviews_df.shape)\n# get only reviews text and rating stars\nreviews_df = reviews_df[['Review Rating', 'Review']]\nprint(reviews_df.head())\n# print(reviews_df['Review'][0])\n# details = TextBlob(reviews_df['Review'][0])\n# print(details.sentiment)\ntb_score = []\nstars = []\nfor ind,review in islice(reviews_df.iterrows(),271):\ndetails = TextBlob(review['Review'])\ntb_score.append(details.sentiment.polarity)\nstars.append(review['Review Rating'])\nstars_tbscore = pd.DataFrame()\nstars_tbscore['Review Rating'] = stars\nstars_tbscore['senti_value'] = tb_score\nax = stars_tbscore.boxplot(by=['Review Rating'], figsize=(10,6))\nplt.style.use('seaborn-white')\nax.get_figure().suptitle(\"\")\nax.set_title('Average sentiment score and Star rating - Using Textblob.')\nax.set_xlabel('Yelp Review Rating')\nax.set_ylabel('Average sentiment score')\nax.grid(False)\nfig1 = plt.gcf()\nplt.show()\nfig1.savefig(\"boxplot.png\", transparent = True)\n", "output_sequence": "Crawl user-published reviews on Yelp and apply text mining and machine learing techniques using Python. Web Scraping and Word Cloud, Sentiment Analysis using Textblob, Sentiment Classification using SVM"}, {"input_sequence": "Author: Ranqing Song ; # clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# install and load packages\nlibraries = c(\"stats\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# parameter settings\nn1 = 100\nbeta = 0.8\n# simulation of MA(1)-processes\nset.seed(123)\nx1 = arima.sim(n = n1, list(ma = beta), innov = rnorm(n1))\n# Plot\npar(mfrow = c(2, 1))\nplot.ts(x1, col = \"blue\", ylab = \"y(t)\")\ntitle(paste(\"MA(1) Process, n =\", n1))\npar(mfg = c(2, 1))\nplot.ts(x2, col = \"red\", ylab = \"y(t)\")\ntitle(paste(\"MA(1) Process, n =\", n2))\n", "output_sequence": "Plots two realizations of an MA(1) (moving average) process with MA coefficient beta,  simulate from an arima model with innovations n1 and n2."}, {"input_sequence": "Author: Ranqing Song ; # clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# parameter settings\nn = 100\nbeta = 0.5\n#generate time-series data set\nset.seed(123)\nx = arima.sim(n = n, list(ma = beta), innov = rnorm(n))\nx = matrix(x)\n#generate candidates for estimated beta, -1<beta<1\nbetahat = seq(from = -0.99, to = 0.99, by = 0.02)\nk= 100\nli = c(1:k)\ne[1] = x[1]\n# likelihood function, assuming standard normal distributed errors\nfor (i in 1:k){\nb = betahat[i]\ngamma0 = diag(1+b^2, n, n)\ngamma1 = cbind(0, gamma1)\ngamma = gamma0 + gamma1 + tgamma1\nbetacoef = (-b) ^ (1:(n-1))\n#unconditional maximal likelihood function\nli[i] = -n/2 * log(2*pi) - 1/2 * log(det(gamma)) - 1/2 * t(x) %*% solve(gamma) %*% x\n#error terms\nfor (j in 2:n){\ne[j] = x[j] + sum(betacoef[1:(j-1)] * x[(j-1):1, 1])\n}\n#conditional maximal likelihood function\ncli[i] = -n/2 * log(2*pi) - 1/2 * log(det(gamma)) - 1/2 * sum(e^2)\n}\n#plot the likelihood functions\noutput = cbind(betahat, li, cli)\nplot(output[,c(1,2)], col = 4, xlab = \"Beta\", ylab = \"log-Likelihood\",type = \"l\", lwd = 2,\nmain = paste(\"likelihood function of a MA(1) Process with n=\", n, sep = \"\"))\nabline(v = output[which.max(li),1], col = \"blue\")\npoints(output[, c(1,3)], type = \"l\", col = 2, lty = 2, lwd = 2)\nabline(v = output[which.max(cli),1], col = \"red\")\n", "output_sequence": "Plots two realizations of an MA(1) (moving average) process with MA coefficient beta,  simulate from an arima model with innovations n1 and n2."}, {"input_sequence": "Author: Ranqing Song ; # clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# parameter settings\nn = 1000\nbeta = 0.5\n#generate time-series data set\nset.seed(123)\nx = arima.sim(n = n, list(ma = beta), innov = rnorm(n))\nx = matrix(x)\n#generate candidates for estimated beta, -1<beta<1\nbetahat = seq(from = -0.99, to = 0.99, by = 0.02)\nk= 100\nli = c(1:k)\ne[1] = x[1]\n# likelihood function, assuming standard normal distributed errors\nfor (i in 1:k){\nb = betahat[i]\ngamma0 = diag(1+b^2, n, n)\ngamma1 = cbind(0, gamma1)\ngamma = gamma0 + gamma1 + tgamma1\nbetacoef = (-b) ^ (1:(n-1))\n#unconditional maximal likelihood function\nli[i] = -n/2 * log(2*pi) - 1/2 * log(det(gamma)) - 1/2 * t(x) %*% solve(gamma) %*% x\n#error terms\nfor (j in 2:n){\ne[j] = x[j] + sum(betacoef[1:(j-1)] * x[(j-1):1, 1])\n}\n#conditional maximal likelihood function\ncli[i] = -n/2 * log(2*pi) - 1/2 * log(det(gamma)) - 1/2 * sum(e^2)\n}\n#plot the likelihood functions\noutput = cbind(betahat, li, cli)\nplot(output[,c(1,2)], col = 4, xlab = \"Beta\", ylab = \"log-Likelihood\",type = \"l\", lwd = 2,\nmain = paste(\"likelihood function of a MA(1) Process with n=\", n, sep = \"\"))\nabline(v = output[which.max(li),1], col = \"blue\")\npoints(output[, c(1,3)], type = \"l\", col = 2, lty = 2, lwd = 2)\nabline(v = output[which.max(cli),1], col = \"red\")\n", "output_sequence": "Plots two realizations of an MA(1) (moving average) process with MA coefficient beta,  simulate from an arima model with innovations n1 and n2."}, {"input_sequence": "Author: Anna-Helena Mihov, Hao Cheng, Liv Jantzen ; #import all relevant libraries and install packages\nlibrary(Rcmdr)\nlibrary(sandwich)\nattach(mtcars)\n\n#set working directory\n#setwd('C:/Users/Helen/Desktop/FinalVersion-SSPL')\n# read file with economic data\nEconData = read.table(\"EconData.txt\", header=TRUE,\nsep = \"\\t\")\n#Column names give to variables\ncolnames(EconData) = c('Date','EXUS','EX', 'EXP',\n'IMP', 'GDP_EU','GDP_US',\nEconData = na.omit(EconData)\n# set date format\n", "output_sequence": "reads in a textfile and prepares the dataframe."}, {"input_sequence": "Author: Zografia Anastasiadou ; rm(list=ls(all=TRUE))\n#setwd(\"C:/...\")\n\n#install.packages(\"QRM\")\nlibrary(QRM)\n\nn = 100\nr = c(0.5, 0, -0.5)\nx1 = seq(0.001, 5.001, by = 0.05)\ndist1 = cbind(x1, dGPD(x1, r[1], 1))\nx2 = c(0, sp*(1:n)/n)\ndist2 = cbind(x2, dGPD(x2, r[2], 1))\nx3 = seq(0.001, 100*(-1/r[3]/n)+0.001, -1/r[3]/n)\ndist3 = cbind(x3, dGPD(x3, r[3],1))\nplot(dist1, type = \"l\", col = \"blue\", lty = 4, lwd = 3, xlab = \"\", ylab = \"\",\nmain = \"Generalized Pareto Densities\")\nlines(dist2, lwd = 3)\n", "output_sequence": "Generates generalized Pareto densities for different shape parameters and plots the resulting densities."}, {"input_sequence": "Author: Zografia Anastasiadou ; hold on\nn = 100;\nsp = 5;\nr = [0.5; 0; -0.5];\n\nx1 = 0.001:0.05:5.001;\ndist1 = [x1' gppdf(x1,r(1),1)'];\nplot(dist1(:, 1), dist1(:, 2), 'b', 'LineStyle', '-.', 'LineWidth', 2)\n\nx2 = [0; (sp*(1:n)/n)'];\ndist2 = [x2, gppdf(x2,r(2),1)];\nplot(dist2(:,1), dist2(:,2),'k','LineWidth',2)\nx3 = 0.001:-1/r(3)/n:100*(-1/r(3)/n) + 0.001;\nx3\ndist3 = [x3' gppdf(x3, r(3), 1)'];\nplot(dist3(:, 1), dist3(:, 2), 'r', 'LineStyle', '--', 'LineWidth',2 )\ntitle('Generalized Pareto Densities')\nxlim([0 5])\n", "output_sequence": "Generates generalized Pareto densities for different shape parameters and plots the resulting densities."}, {"input_sequence": "Author: Barbara Choros ; function MSRvar_pot(x)\nclc;\nclose all;\na = load('Bay9906_close_2kPoints.txt','-ascii');\nd = a + b + c;\nx = d(2:end)-d(1:end-1);\nx = - x;\nT = length(x);\nh = 250;\np = 0.95;\nfor i = 1:T-h\ny = x(i:i+h-1);\n[var(i),ksi(i),beta(i),u(i)]=var_pot(y,h,p,q);\nend\nsave ('VaR9906_pot_Portf.txt','var','-ascii');\nfunction [var, ksi, beta, u] = var_pot(y,h,p,q);\nN = floor(h*q);\nys = sort(y,'descend');\nu = ys(N+1);\nparams = gpfit(z);\nksi = params(1);\n", "output_sequence": "Computes Value-at-Risk with Peaks Over Treshold model with generalized Pareto distribution for data from Siemens and BMW (1999 - 2006)."}, {"input_sequence": "Author: Barbara Choros ; function MSRvar_coptStudent\nclc;\nwx = 1;\nx = load('Bay9906_close_2kPoints.txt','-ascii');\ndofx = 6;\ny = load('Sie9906_close_2kPoints.txt','-ascii');\ndofy = 5;\nv = var_coptStudent(x,y,wx,wy,dofx,dofy);\nsave('VaR9906_ctS_BaySie_2kPoints.txt','v','-ascii');\n% ---------------------------------------------------------------------\nx = load('Bmw9906_close_2kPoints.txt','-ascii');\ndofx = 7\ny = load('Vow9906_close_2kPoints.txt','-ascii');\ndofy = 8;\nsave('VaR9906_ctS_BmwVow_2kPoints.txt','v','-ascii');\n% ---------------------------------------------------------------------\nx = load('Sie9906_close_2kPoints.txt','-ascii');\ndofx = 5\nsave('VaR9906_ctS_SieVow_2kPoints.txt','v','-ascii');\nfunction VaR = var_coptStudent(x,y,wx,wy,dofx,dofy);\nrx = log(x(2:end)./x(1:end-1));\nmx = mean(rx);\nrx = (rx-mx)/stx;\nry = log(y(2:end)./y(1:end-1));\nmy = mean(ry);\nry = (ry-my)/sty;\nrx = tcdf(rx,dofx);\nh = 250;\nalpha = 0.05;\nT = length(rx);\nfor i = 1:T-h\na = rx(i:i+h-1);\n[r,v] = MSRtcopulaparam(a,b);\nU = copularnd('t',r,v,n);\nux = tinv(U(:,1),dofx);\nux = ux.*stx+mx;\nL = wx.*x(i+h).*(exp(ux)-1)+wy.*y(i+h).*(exp(uy)-1);\nstats = sort(L);\nVaR(i) = stats(alpha*n+1);\n", "output_sequence": "Computes Value-at-Risk with t-Student copula model for Bayer and Siemens data (1999 - 2006)."}, {"input_sequence": "Author: Barbara Choros ; function [r,v] = MSRtcopulaparam(x,y)\nT = length(x);\nz = [x';y'];\nN = size(z,1);\nu = unif_var(z);\nRcml = zeros(N,N);\n\nfor i = 1:N-1\nfor j = i+1:N\nRcml(i,j) = corr(u(i,:)', u(j,:)', 'type', 'Kendall');\nRcml(j,i) = Rcml(i,j);\nend;\nRcml(i,i) = 1;\nend;\nRcml(N,N) = 1;\nRcml = sin(pi/2.*Rcml);\nr = Rcml(1,2);\nv = 3;\nloglike1 = log_dens_t(v, Rcml, u);\nv = v + 1;\nloglike2 = log_dens_t(v, Rcml, u);\nwhile loglike1 < loglike2\nv = v+1;\nloglike2 = log_dens_t(v, Rcml, u);\nif v == 30 break; end;\nv = v-1;\nfunction s = unif_var(x)\nN = size(x,1);\nfor i = 1:N\nfor j = 1:T\ndys(i,j) = sum(x(i,:)<= x(i,j));\ns = dys./(T+1);\nfunction s = log_dens_t(v,R,u)\nsi = size(u);\ny = tinv(u,v);\ng = T*log(gamma((v+n)/2)/gamma(v/2))-T*n*log(gamma((v+1)/2)/gamma(v/2))-T/2*log(abs(det(R)));\na = sum(log(1+y.^2./v));\nb = sum(a);\nfor i = 1:T\nc(i) = log(1+(y(:,i)'*inv(R)*y(:,i))./v);\nend\nc = sum(c);\n", "output_sequence": "Computes Value-at-Risk with t-Student copula model for Bayer and Siemens data (1999 - 2006)."}, {"input_sequence": "Author: Barbara Choros ; function Y = smoothn(X,sz,filt,std)\n%SMOOTHN Smooth N-D data\n% Y = SMOOTHN(X, SIZE) smooths input data X. The smoothed data is\n% retuirned in Y. SIZE sets the size of the convolution kernel\n% such that LENGTH(SIZE) = NDIMS(X)\n%\n% Y = SMOOTHN(X, SIZE, FILTER) Filter can be 'gaussian' or 'box' (default)\n% and determines the convolution kernel.\n% Y = SMOOTHN(X, SIZE, FILTER, STD) is a vector of standard deviations\n% one for each dimension, when filter is 'gaussian' (default is 0.65)\n% $Author: ganil $\n% $Date: 2001/09/17 18:54:39 $\n% $Revision: 1.1 $\n% $State: Exp $\nif nargin == 2,\nfilt = 'b';\nelseif nargin == 3,\nstd = 0.65;\nelseif nargin>4 | nargin<2\nerror('Wrong number of input arguments.');\nend\n% check the correctness of sz\nif ndims(sz) > 2 | min(size(sz)) ~= 1\nerror('SIZE must be a vector');\nelseif length(sz) == 1\nsz = repmat(sz,ndims(X));\nelseif ndims(X) ~= length(sz)\nerror('SIZE must be a vector of length equal to the dimensionality of X');\n% check the correctness of std\nif filt(1) == 'g'\nif length(std) == 1\nstd = std*ones(ndims(X),1);\nelseif ndims(X) ~= length(std)\nerror('STD must be a vector of length equal to the dimensionality of X');\nend\nstd = std(:)';\n% check for appropriate size\npadSize = (sz-1)/2;\nif ~isequal(padSize, floor(padSize)) | any(padSize<0)\nerror('All elements of SIZE must be odd integers >= 1.');\n% generate the convolution kernel based on the choice of the filter\nfilt = lower(filt);\nif (filt(1) == 'b')\nsmooth = ones(sz)/prod(sz); % box filter in N-D\nelseif (filt(1) == 'g')\nsmooth = ndgaussian(padSize,std); % a gaussian filter in N-D\nelse\nerror('Unknown filter');\n% pad the data\nX = padreplicate(X,padSize);\n% perform the convolution\nY = convn(X,smooth,'valid');\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nfunction h = ndgaussian(siz,std)\n% Calculate a non-symmetric ND gaussian. Note that STD is scaled to the\n% sizes in SIZ as STD = STD.*SIZ\nndim = length(siz);\nsizd = cell(ndim,1);\nfor i = 1:ndim\nsizd{i} = -siz(i):siz(i);\ngrid = gridnd(sizd);\nstd = reshape(std.*siz,[ones(1,ndim) ndim]);\nstd(find(siz==0)) = 1; % no smoothing along these dimensions as siz = 0\nstd = repmat(std,2*siz+1);\nh = exp(-sum((grid.*grid)./(2*std.*std),ndim+1));\nh = h/sum(h(:));\nfunction argout = gridnd(argin)\n% exactly the same as ndgrid but it accepts only one input argument of\n% type cell and a single output array\nnin = length(argin);\nnout = nin;\nfor i=nin:-1:1,\nargin{i} = full(argin{i}); % Make sure everything is full\nsiz(i) = prod(size(argin{i}));\nif length(siz)<nout, siz = [siz ones(1,nout-length(siz))]; end\nargout = [];\nfor i=1:nout,\nx = argin{i}(:); % Extract and reshape as a vector.\ns = siz; s(i) = []; % Remove i-th dimension\nx = reshape(x(:,ones(1,prod(s))),[length(x) s]); % Expand x\nx = permute(x,[2:i 1 i+1:nout]);% Permute to i'th dimension\nargout = cat(nin+1,argout,x);% Concatenate to the output\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nfunction b=padreplicate(a, padSize)\n%Pad an array by replicating values.\nnumDims = length(padSize);\nidx = cell(numDims,1);\nfor k = 1:numDims\nM = size(a,k);\nonesVector = ones(1,padSize(k));\nidx{k} = [onesVector 1:M M*onesVector];\nb = a(idx{:});\n", "output_sequence": "Calculates upper tail dependence for various thresholds k for any combination of BMW, Siemens, VW and Bayer based on data from 1999 to 2006."}, {"input_sequence": "Author: Barbara Choros ; function MSRlambda_smoothing_utd\nclose all;\nclc;\nx1 = load('BMW9906_standLogRet.dat');\nlambdaU = lambda_smoothing_utd(x1, x2)\nsave('lambdaS_BmwVow1.txt', 'lambdaU', '-ascii');\nx1 = load('Sie9906_standLogRet.dat');\nsave('lambdaS_SieVow1.txt', 'lambdaU', '-ascii');\nx1 = load('Bay9906_standLogRet.dat');\nsave ('lambdaS_BaySie1.txt', 'lambdaU', '-ascii');\nfunction x = lambda_smoothing_utd(u, v);\nT = length(u);\ns1 = tiedrank(u);\nfor k = 1:T-1\nC = sum((s1 > (T - k)).*(s2 > (T - k)));\nx(k) = C/k;\nend\nn = size(x, 2);\n[b, n] = bandwith(0.005, n);\nx = x(1:n);\nx = smoothn(x, b, 'box');\nfunction [b, n] = bandwith(alpha, n)\nb = floor(alpha*n);\nif mod(b, 2) == 0\nodd = 799:400:4799;\nf = find(odd<n);\n", "output_sequence": "Calculates upper tail dependence for various thresholds k for any combination of BMW, Siemens, VW and Bayer based on data from 1999 to 2006."}, {"input_sequence": "Author: Barbara Choros-Tomczyk , Wolfgang K. H\u00e4rdle  ; rm(list = ls(all = TRUE))\n#setwd(\"C:/...\")\n\n#install.packages(\"evd\")\nlibrary(evd)\nset.seed(123)\nn = 150\nxf1 = rgumbel(n)\nxf = sort(xf1)\nt = c(1:n)/(n + 1)\ndat = cbind(pgumbel(xf), t)\ndat2 = cbind(t, t)\nplot(dat, col = \"blue\", pch = 20, xlab = \"\", ylab = \"\", main = \"CDFs of Random Variables\")\n", "output_sequence": "Produces a PP plot of the pseudo random variables with Gumbel distribution against theoretical Gumbel distribution."}, {"input_sequence": "Author: Barbara Choros-Tomczyk , Wolfgang K. H\u00e4rdle  ; clear\nclose all\nclc\n\nn = 150;\nxf1 = evrnd(0, 1, n, 1);\nxf = sort(xf1);\nt = (1:n)/(n + 1);\ndat = [evcdf(xf), t'];\ndat2 = [t', t'];\nhold on\nscatter(dat(:, 1), dat(:, 2), '.')\nplot(dat2(:, 1), dat2(:, 2), 'r', 'LineWidth', 2)\nhold off\ntitle('CDFs of Random Variables')\n", "output_sequence": "Produces a PP plot of the pseudo random variables with Gumbel distribution against theoretical Gumbel distribution."}, {"input_sequence": "Author: Lei Fang ; normalization = function(theta) {\ntheta.temp = colSums(theta)\nfor (i in 1:loop.31) {\nnam10 = paste(\"normal.theta\", names.31[i], 1, sep = \".\")\nassign(nam10, loop.31 * theta[i, 1]/theta.temp[1])\nnam11 = paste(\"normal.theta\", names.31[i], 3, sep = \".\")\nassign(nam11, loop.31 * theta[i, 3]/theta.temp[3])\nnam12 = paste(\"normal.theta\", names.31[i], 2, sep = \".\")\nassign(nam12, theta[i, 2] - theta.temp[2]/loop.31)\nnam14 = paste(\"normal.theta\", names.31[i], sep = \".\")\nassign(nam14, c(eval(parse(text = paste(\"normal.theta\", names.31[i], 1, sep = \".\"))), eval(parse(text = paste(\"normal.theta\",\nnames.31[i], 2, sep = \".\"))), eval(parse(text = paste(\"normal.theta\", names.31[i], 3, sep = \".\")))))\n}\nll = list()\nll[[i]] = c(eval(parse(text = paste(\"normal.theta\", names.31[i], sep = \".\"))))\nnormal.theta = do.call(rbind, ll)\nreturn(normal.theta)\n}\n", "output_sequence": "Normalizes optimal shape variation parameters theta estimated from MuPoMo_optimization, and will be called in MuPoMo_main_multipop."}, {"input_sequence": "Author: Lining Yu ; rm(list = ls())\ngraphics.off()\n# set the working directory setwd('C:/...')\nlibraries = c(\"vars\", \"stats\", \"tseries\", \"quantmod\", \"tsDyn\", \"dygraphs\", \"urca\",\n\"xtable\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\ndata = read.csv(\"FRM_VIX_SRISK_GT.csv\")\ndt = as.Date(data[, 1], format = \"%d/%m/%Y\")\nFRM = as.matrix(data[, 2])\nadf.test(FRM)\nSRISK = as.matrix(data[, 4])\nadf.test(SRISK)\n########### scale variables #################\nSRISK = (SRISK - min(SRISK))/(max(SRISK) - min(SRISK))\n########### plot FRM and SRISK ################\nplot(dt, FRM, ylab = \"\", xlab = \"\", pch = 16, col = \"white\", , cex.axis = 2, font.axis = 2,\ntype = \"l\")\nlines(dt, FRM, col = \"black\", lwd = 8)\n########## Correlation analysis #############\ncor(FRM, SRISK, method = \"pearson\")\n####### Causality FRM and SRISK ###############\n### from FRM to SRISK ###\nlm_SF = lm(SRISK ~ FRM)\nsummary(lm_SF)\nres_lm_SF = resid(lm_SF)\nres.ADF <- ur.df(res_lm_SF, type = \"none\", selectlags = \"AIC\")\nsummary(res.ADF)\n### from SRISK to FRM ###\nlm_FS = lm(FRM ~ SRISK)\nsummary(lm_FS)\nres_lm_FS = resid(lm_FS)\nres.ADF <- ur.df(res_lm_FS, type = \"none\", selectlags = \"AIC\")\n", "output_sequence": "compare the FRM and SRISK by using correlation analysis and causality analysis, plot the FRM and SRISK series"}, {"input_sequence": "Author: Li Sun ; # clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\nx = read.table(\"yield_US3month9808.txt\") # load data\nt = 1:dim(x)[1] # time\n# Plot\nplot(t, t(x), type = \"l\", xlab = \"Observation days\", ylab = \"Yield(%)\", col = \"blue3\",\nframe = TRUE)\nabline(h = seq(0, 7, by = 1), lty = \"dotted\", lwd = 0.5, col = \"grey\")\n", "output_sequence": "Shows a 3-month U.S. Treasury bill daily yield from 1998 to 2008 as an approximation of the short rate."}, {"input_sequence": "Author: Andrija Mihoci, Awdesch Melzer ; import numpy as np\nimport pandas as pd\nimport sklearn\nfrom sklearn.linear_model import LinearRegression\ndf = pd.read_csv(\"FSE_LSE_2014.dat\",header=None)\nD = df.iloc[:,0] # date\nr = np.log(S).diff(1).dropna() # r(t) = log(S(t)) - log(S(t-1))\nn = r.shape[0]*r.shape[1] # sample size\nd = r.shape[1] # columns or r\n# Right tail index regression and Hill estimator\nm1 = 10 # m1 largest observations\n#Sort each column\nr_sorted=[]\nfor col in r.columns:\nr_sorted.append(r[col].sort_values(ascending=False).values)\nr_sorted = pd.DataFrame(r_sorted).T\nx1 = np.log(r_sorted.head(m1))\ny1 = pd.DataFrame(np.repeat(np.log((np.arange(m1).astype(float)+1)/n),d).reshape((-1,d)))\nx2 = np.log(r_sorted.head(m2))\ny2 = pd.DataFrame(np.repeat(np.log((np.arange(m2).astype(float)+1)/n),d).reshape((-1,d)))\nls1=[]\nhill1=[]\n#loop over the columns of the return data frame and fit regression\nfor i in range(d):\nlr=LinearRegression()\n#reshape data, fit the regression model, append coefficients to list\nls1.append(-lr.fit(x1.iloc[:,i].values.reshape((-1,1)),y1.iloc[:,i].values.reshape((-1,1))).coef_.flatten())\n\nhill1.append(1/(np.mean(x1.iloc[:m1-1, i]) - x1.iloc[m1-1, i]))\n#assemble coefficients to a data frame\nY = pd.DataFrame([ls1,ls2,hill1,hill2]).T.astype(float).round(2)\nY.columns=[\"LS_m1\", \"LS_m2\", \"Hill_m1\",\nprint(Y)\n", "output_sequence": "Reads the date, DAX index values, stock prices of 20 largest companies at Frankfurt Stock Exchange (FSE), FTSE 100 index values and stock prices of 20 largest companies at London Stock Exchange (LSE) and computes the Least Square (LS) and the Hill estimators of the tail index for all 42 analysed return processes."}, {"input_sequence": "Author: Andrija Mihoci, Awdesch Melzer ; # clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# install and load packages\nlibraries = c(\"matlab\", \"pracma\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# load data for FSE and LSE\nDS = read.csv(\"FSE_LSE_2014.dat\")\nD = DS[, 1] # date\nr = diff(as.matrix(log(S))) # r(t) = log(S(t)) - log(S(t-1))\nn = length(r) # sample size\n# Right tail index regression and Hill estimator\nm1 = 10 # m1 largest observations\nrsorted = apply(r, 2, sort, decreasing = TRUE)\nx1 = log(rsorted[1:m1, ])\ny1 = matrix(rep(log(c(1:m1)/n), d), ncol = d)\nx2 = log(rsorted[1:m2, ])\ny2 = matrix(rep(log(c(1:m2)/n), d), ncol = d)\nls1 = ls2 = hill1 = hill2 = numeric(d)\nfor (i in 1:d){\nls1[i] = lm(y1[, i] ~ x1[, i])$coef[2]\nhill1[i] = 1/(mean(x1[1:(m1-1), i]) - x1[m1, i])\n}\nY = cbind(-ls1, -ls2, hill1,\ncolnames(Y) = c(\"LS_m1\", \"Hill_m1\",\n", "output_sequence": "Reads the date, DAX index values, stock prices of 20 largest companies at Frankfurt Stock Exchange (FSE), FTSE 100 index values and stock prices of 20 largest companies at London Stock Exchange (LSE) and computes the Least Square (LS) and the Hill estimators of the tail index for all 42 analysed return processes."}, {"input_sequence": "Author: Andreas Golle, Awdesch Melzer ; # clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# install and load packages\nlibraries = c(\"lattice\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# parameter settings\ns1 = 50 # lower bound of Asset Price\nt2 = 1 # upper bound of Time to Maturity\nK = 100 # exercise price\nr = 0.01 # interest rate\nsig = 0.35 # volatility\nd = 0 # dividend rate\nb = r - d # cost of carry\nsteps = 60\nmeshgrid = function(a, b) {\nlist(x = outer(b * 0, a, FUN = \"+\"), y = outer(b, a * 0, FUN = \"+\"))\n}\nfirst = meshgrid(seq(t1, t2, -(t1 - t2)/(steps - 1)), seq(t1, t2, -(t1 - t2)/(steps - 1)))\ntau = first$x\nsecond = meshgrid(seq(s1, s2, -(s1 - s2)/(steps - 1)), seq(s1, s2, -(s1 - s2)/(steps - 1)))\ndump2 = second$x\n# Black-Scholes formula for the option price and the ultima\nd1 = (log(S/K) + (r - d - sig^2/2) * tau)/(sig * sqrt(tau))\nd2 = d1 - sig * sqrt(tau)\nvega = S * exp(-d * tau) * dnorm(d1) * sqrt(tau)\nvomma = vega * (d1 * d2/sig)\nultima = vomma * (1/sig) * (d1 * d2 - d1/d2 - d2/d1 - 1)\n# plot\ntitle = bquote(expression(paste(\"Strike price is \", .(K), \", interest rate is \",\n.(r), \", dividend rate is \", .(d), \", annual volatility is \", .(sig))))\nwireframe((ultima/1e+06) ~ tau * S, drape = T, ticktype = \"detailed\", main = expression(paste(\"Ultima as function of the time to maturity \",\ntau, \" and the asset price S\")), sub = title, scales = list(arrows = FALSE,\ncol = \"black\", distance = 1, tick.number = 8, cex = 0.7, x = list(labels = round(seq(t1,\nt2, length = 11), y = list(labels = round(seq(s1, s2, length = 11),\n1)), z = list(labels = round(seq(min(ultima/1e+06), max(ultima/1e+06),\nlength = 11), 4))), xlab = list(expression(paste(\"Time to Maturity \",\ntau)), rot = 30, cex = 1.2), ylab = list(\"Asset Price S\", rot = -40, cex = 1.2),\nzlab = list(\"Ultima\", rot = 95, cex = 1.1))\n", "output_sequence": "Plots the Ultima of a call option (Ultima or DvommaDvol) as a function of the time to maturity and the asset price. Ultima is divided by 1,000,000 to reflect a one-percentage point change in volatility."}, {"input_sequence": "Author: Wolfgang K. H\u00e4rdle ; %Clear loaded variables and close graphics\nclear all\nclc\np = 0.5;\nbsamplem = binornd(1, 0.5, n, 1000); %Random generation\nx1 = (mean(bsamplem) - p)/sqrt(p*(1 - p)/n);\n[bden x] = ksdensity(x1); %Compute kernel density estimate\n%Plot kernel density\nhold on\nplot(x, bden, 'Color', 'b', 'LineWidth', 4)\nset(gca, 'FontWeight', 'bold')\nxlabel('1000 Random Samples', 'FontSize', 16)\nylabel('Estimated and Normal Density', 'Fontsize', 16)\nylim([0, 0.45])\nbox on\nplot(x, normpdf(x, 0, 1), 'Color', 'r', 'LineWidth', 4)\nset(gca, 'LineWidth', 1.6)\nset(gca, 'TickLength', [0.015, 0.015])\nstr = sprintf('Asymptotic Distribution, n = %g ', n);\n", "output_sequence": "Illustrates the (univariate) Central Limit Theorem (CLT). n*1000 sets of n-dimensional Bernoulli samples are generated and used to approximate the distribution of t = sqrt(n)*(mean(x)-mu)/sigma -> N(0,1). The estimated density (red) of t is shown together with the standard normal (green)."}, {"input_sequence": "Author: Wolfgang K. H\u00e4rdle ; # clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# install and load packages\nlibraries = c(\"KernSmooth\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# parameter settings\np = 0.5\n# Random generation of the binomial distribution with parameters 1000*n and 0.5\nbsample = rbinom(n * 1000, 1, p)\n# Create a matrix of binomial random variables\nbsamplem = matrix(bsample, n, 1000)\n#Compute kernel density estimate\nbden = bkde((colMeans(bsamplem) - p)/sqrt(p * (1 - p)/n))\n# Plot\nplot(bden, col = \"green\", type = \"l\", lty = 1, lwd = 4, xlab = \"1000 Random Samples\",\nylab = \"Estimated and Normal Density\", cex.lab = 1.7, cex.axis = 2, ylim = c(0,\n0.4)) #Plot kernel density\nlines(bden$x, dnorm(bden$x), col = \"red\", lty = 1, lwd = 4)\ntitle(paste(\"Asymptotic Distribution, n =\", n))\n", "output_sequence": "Illustrates the (univariate) Central Limit Theorem (CLT). n*1000 sets of n-dimensional Bernoulli samples are generated and used to approximate the distribution of t = sqrt(n)*(mean(x)-mu)/sigma -> N(0,1). The estimated density (red) of t is shown together with the standard normal (green)."}, {"input_sequence": "Author: Ostap Okhrin ; # clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# install and load packages\nlibraries = c(\"copula\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# Main computation\nlayout(matrix(c(1, 4, 2, 5, 3, 6), 2, 3, byrow = FALSE))\npar(mar = c(2, 2, 2), pty = \"s\")\n# Uniform marginal distributions\ntMVD = mvdc(tCopula(param = 0.5, df = 3, dim = 3), margins = c(\"unif\",\nparamMargins = list(list(min = 0, max = 1), list(min = 0, max = 1), list(min = 0,\nmax = 1)))\nX = rMvdc(tMVD, n = 5000)\nplot(X[, 1], X[, 2], xlab = \"\", ylab = \"\", cex = 0.5, pch = 19, cex.axis = 2, xlim = c(0,\n1), ylim = c(0, 1))\ntMVD = mvdc(tCopula(param = 0.5, df = 3, dim = 3), margins = c(\"unif\",\nX = rMvdc(tMVD, n = 5000)\nplot(X[, 1], X[, 3], xlab = \"\", ylab = \"\", cex = 0.5, pch = 19, cex.axis = 2)\n# t3 marginal distributions\ntMVD = mvdc(tCopula(param = 0.5, df = 3, dim = 3), margins = c(\"t\", \"t\"), paramMargins = list(list(df = 3),\nlist(df = 3), list(df = 3)))\nplot(X[, 1], X[, 2], xlab = \"\", ylab = \"\", cex = 0.5, pch = 19, cex.axis = 2)\n", "output_sequence": "Produces scatterplots of Monte Carlo sample (5000 realisations) of pseudo random variable with uniform (top) and marginal distributions (bottom). Dependence structure is given by t-copula with df=3 and parameter=0.5."}, {"input_sequence": "Author: Felix Jung ; # clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\nReadConsoleInput = function(prompt.message, bounds) {\ninput = NA\nwhile (!is.numeric(input)) {\ninput = as.numeric(sub(\",\", \".\", readline(prompt = prompt.message)))\nif (!missing(bounds)) {\nif (length(bounds) > 1) {\nif (input < bounds[1] | input > bounds[2]) {\ninput = NA\ncat(\"Error: value must lie between\", bounds[1], \"and\", bounds[2],\n\"n\")\n} else {\nif (input < bounds[1]) {\ncat(\"Error: value must lie at or above\", bounds[1], \"n\")\n}\n}\nreturn(input)\n}\n# parameter settings\nK = 210\nr = 0.04545\ntau = 0.5\nsig = 0.25\n# Check whether user wants to use defaults\nuse.defaults = TRUE\ncat(\"The default option parameters are:n\")\ncat(\"S =\", S, \"n\")\nwhile (TRUE) {\nuser.input = readline(\"Would you like to use these default values (y/n)? \")\nif (!(user.input %in% c(\"y\", \"n\"))) {\ncat(\"Invalid input: please use y or n to answer the question.n\")\n} else {\nif (user.input == \"y\") {\nuse.defaults = TRUE\n} else {\nuse.defaults = FALSE\nbreak\nif (use.defaults == FALSE) {\nS = ReadConsoleInput(\"Please enter a stock price S: \", bounds = c(0))\nset.seed(0)\n# Compute call price from approximation a\ny = (log(S/K) + (b - (sig^2)/2) * tau)/(sig * sqrt(tau))\nba = 0.332672527\nt1 = 1/(1 + ba * y)\nt2 = 1/(1 + ba * (y + sig * sqrt(tau)))\na1 = 0.17401209\na2 = -0.04793922\na3 = 0.373927817\nnorma1 = 1 - (a1 * t1 + a2 * (t1^2) + a3 * (t1^3)) * exp(-y * y/2)\ncall.price.a = exp(-(r - b) * tau) * S * norma2 - exp(-r * tau) * K * norma1\n# Compute call price from approximation b\nbb = 0.231641888\nt1 = 1/(1 + bb * y)\nt2 = 1/(1 + bb * (y + sig * sqrt(tau)))\na1 = 0.127414796\na2 = -0.142248368\na3 = 0.71070687\na4 = -0.726576013\na5 = 0.530702714\nnormb1 = 1 - (a1 * t1 + a2 * (t1^2) + a3 * (t1^3) + a4 * (t1^4) + a5 * (t1^5)) *\nexp(-y^2/2)\nnormb2 = 1 - (a1 * t2 + a2 * (t2^2) + a3 * (t2^3) + a4 * (t2^4) + a5 * (t2^5)) *\nexp(-(y + sig * sqrt(tau))^2/2)\ncall.price.b = exp(-(r - b) * tau) * S * normb2 - exp(-r * tau) * K * normb1\n# Compute call price from approximation c\na1 = 0.09979268\na2 = 0.04432014\na3 = 0.0096992\na4 = -9.862e-05\na5 = 0.00058155\nt1 = abs(y)\nt2 = abs(y + sig * sqrt(tau))\nnormc1 = 1/2 - 1/(2 * (1 + a1 * t1 + a2 * t1^2 + a3 * t1^3 + a4 * t1^4 + a5 * t1^5)^8)\nif (y < 0) {\nnormc1 = 0.5 - normc1\n} else {\nnormc1 = 0.5 + normc1\nnormc2 = 1/2 - 1/(2 * (1 + a1 * t2 + a2 * t2^2 + a3 * t2^3 + a4 * t2^4 + a5 * t2^5)^8)\nif (y + sig * sqrt(tau) < 0) {\nnormc2 = 0.5 - normc2\ncall.price.c = exp(-(r - b) * tau) * S * normc2 - exp(-r * tau) * K * normc1\n# Compute call price from approximation d (Taylor expansion)\nn = 0\nsum1 = 0\nwhile (n <= 12) {\nsum1 = sum1 + (-1)^n * y^(2 * n + 1)/(factorial(n) * 2^n * (2 * n + 1))\nsum2 = sum2 + (-1)^n * (y + sig * sqrt(tau))^(2 * n + 1)/(factorial(n) * 2^n *\n(2 * n + 1))\nnormd1 = 0.5 + sum1/sqrt(2 * pi)\ncall.price.d = exp(-(r - b) * tau) * S * normd2 - exp(-r * tau) * K * normd1\n# Return option prices\ncat(\"Price of European Call norm-a: \")\nprint(call.price.a)\ncat(\"Price of European Call norm-b: \")\nprint(call.price.b)\ncat(\"Price of European Call norm-c: \")\nprint(call.price.c)\ncat(\"Price of European Call norm-d: \")\nprint(call.price.d)\n", "output_sequence": "Computes the Black-Scholes price of a European call option using different approximations of the normal distribution. Optionally, option parameters may be given interactively as user input."}, {"input_sequence": "Author: Felix Jung ; clear all\nclose all\nclc\nformat long\n% user inputs parameters\ndisp('Please input spot S, strike K, and interest r as: [230, 210, 0.04545]') ;\ndisp(' ') ;\npara = input('[spot price, strike price, interest rate]=');\nwhile length(para) < 3\ndisp('Not enough input arguments. Please input in 1*3 vector form like [230, 210, 0.04545] or [230 210 0.04545]');\npara = input('[spot price, strike price, interest rate] = ');\nend\n% spot price\nS = para(1);\n% strike price\nK = para(2);\n% interest rate\nr = para(3);\ndisp('Please input cost of carry b, and volatility sig, remaining time tau as: [0.04545, 0.25, 0.5]') ;\npara2 = input('[cost of carry b, volatility sig, tau]=') ;\nwhile length(para2) < 3\ndisp('Not enough input arguments. Please input in 1*3 vector form like [0.04545, 0.25, 0.5] or [0.04545 0.25 0.5]');\npara2 = input('[cost of carry b, volatility sig, tau] = ');\n% cost of carry, volatility, remaining time\nb = para2(1);\n% main computation\ny = (log(S / K) + (b - (sig^2) / 2) * tau ) / (sig * sqrt(tau));\n% norm a\nba = 0.332672527;\nt1 = 1 / (1 + ba * y);\nt2 = 1 / (1 + ba *(y + sig * sqrt(tau)));\na1 = 0.17401209;\na2 = -0.04793922;\na3 = 0.373927817;\nnorma1 = 1 - (a1 * t1 + a2 * (t1^2) + a3 * (t1^3)) * exp(-y .* y / 2);\nca = exp(-(r - b) * tau) * S * norma2 - exp(-r * tau) * K * norma1;\n% norm b\nbb = 0.231641888;\nt1 = 1 / (1 + bb * y);\nt2 = 1 / (1 + bb * (y + sig * sqrt(tau)));\na1 = 0.127414796;\na2 = -0.142248368;\na3 = 0.71070687;\na4 = -0.726576013;\na5 = 0.530702714;\nnormb1 = 1 - (a1 * t1 + a2 * (t1^2) + a3 * (t1^3) + a4 * (t1^4) + a5 * (t1^5)) * exp(-y^2 / 2);\ncb = exp(-(r - b) * tau) * S * normb2 - exp(-r * tau) * K * normb1;\n% norm c\na1 = 0.09979268;\na5 = 0.00058155;\nt1 = abs(y);\nt2 = abs(y + sig * sqrt(tau));\nnormc1 = 1.0 / 2.0 - 1.0 / ( 2.0 * ( (1.0 + a1 * t1 + a2 * t1^2 + a3 * t1^3 + a4 * t1^4 + a5 * t1^5)^8 ));\nif y < 0\nnormc1 = 0.5 - normc1;\nelse\nnormc1 = 0.5 + normc1;\nnormc2 = 0.5 - 1.0 / (2.0 * ( (1.0 + a1 * t2 + a2 * t2^2 + a3 * t2^3 + a4 * t2^4 + a5 * t2^5)^8 ))\nif (y + sig * sqrt(tau)) < 0\nnormc2 = 0.5 - normc2;\ncc = exp(-(r - b) * tau) * S * normc2 - exp(-r * tau) * K * normc1;\n% norm d\nn = 0;\nsum1 = 0;\nwhile n <= 12\nsum1 = sum1 + ((-1)^n) * y^(2 * n + 1)/(factorial(n) * 2^n * (2 * n + 1));\nn = n + 1;\nnormd1 = 0.5 + sum1 / sqrt(2 * pi);\ncd = exp(-(r - b) * tau) * S * normd2 - exp(-r * tau) * K * normd1;\n% output\ndisp(' ');\ndisp('Price of European Call norm-a =')\ndisp(ca)\ndisp('Price of European Call norm-b =')\ndisp(cb)\ndisp('Price of European Call norm-c =')\ndisp(cc)\ndisp('Price of European Call norm-d =')\ndisp(cd)\n", "output_sequence": "Computes the Black-Scholes price of a European call option using different approximations of the normal distribution. Optionally, option parameters may be given interactively as user input."}, {"input_sequence": "Author: Joanna Tomanek, WK H\u00e4rdle ; clear\nclc\nclose all\n% user inputs parameters\ndisp('Please input lag value lag, value of alpha1 a as: [30, 0.9]') ;\ndisp(' ') ;\npara = input('[lag, a] = ');\nwhile length(para) < 2\ndisp('Not enough input arguments. Please input in 1*2 vector form like [30, 0.9] or [30 0.9]');\npara = input('[lag, a] = ');\nend\nlag = para(1);\n% main computation\nrandn('state', 0) % Start from a known state.\nx = randn(10000, 1); % 10000 Gaussian deviates ~ N(0, 1).\ny = filter(1, [1 -a], x); % Create an AR(1) process.\n", "output_sequence": "Plots the autocorrelation function of an AR(1) (autoregressive) process."}, {"input_sequence": "Author: Joanna Tomanek, WK H\u00e4rdle ; # clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# parameter settings, you might want to change these, check also the ACF if you enter e.g. a = -0.9\nlag = 30 # lag value\na = 0.9 # value of alpha_1\n# Plot\nplot(ARMAacf(ar = a, ma = numeric(0), lag.max = lag, pacf = FALSE), type = \"h\",\nxlab = \"lag\", ylab = \"acf\")\ntitle(\"Sample autocorrelation function (acf)\")\n", "output_sequence": "Plots the autocorrelation function of an AR(1) (autoregressive) process."}, {"input_sequence": "Author: Joanna Tomanek, WK H\u00e4rdle ; import pandas as pd\nimport numpy as np\nimport statsmodels\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.tsa.arima_process import ArmaProcess\nimport matplotlib\nimport matplotlib.pyplot as plt\n# parameter settings\nlag = 30 # lag value\nn = 1000\na = 0.9\nnp.random.seed(123)\n# Obtain MA(1) sample by sampling from a ARMA() model with no AR coefficient\nar1 = np.array([1,-a])\nMA_object1 = ArmaProcess(ar1,ma1)\nsimulated_data_1 = MA_object1.generate_sample(nsample=50000)\nar1 = np.array([1,a])\nMA_object2 = ArmaProcess(ar1,ma1)\nsimulated_data_2 = MA_object2.generate_sample(nsample=50000)\nf, axarr = plt.subplots(2, 1,figsize=(11, 6))\nplot_acf(simulated_data_1,lags=lag,ax=axarr[0],zero=False,alpha=None,title='Sample ACF of an AR(1) Process with '+r'$\\alpha_1=0.9$')\nplt.tight_layout()\nplt.savefig('SFEacfar1_py.png')\n", "output_sequence": "Plots the autocorrelation function of an AR(1) (autoregressive) process."}, {"input_sequence": "Author: Cindy Lamm, Karel Komor\u00e1d ; # clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\nSFEWienerProcess = function(dt, c, k) {\nk = floor(k) # makes sure number of path is integer\nif (dt <= 0 | k <= 0) {\nstop(\"Delta t and number of trajectories must be larger than 0!\")\n}\nl = 100\nn = floor(l/dt)\nt = seq(0, n * dt, by = dt)\nset.seed(0)\nz = matrix(runif(n * k), n, k)\nz = 2 * (z > 0.5) - 1 # scale to -1 or 1\nz = z * c * sqrt(dt) # to get finite and non-zero variance\nzz = apply(z, MARGIN = 2, FUN = cumsum)\nx = rbind(rep(0, k), zz)\n# Output\nmatplot(x, lwd = 2, type = \"l\", lty = 1, ylim = c(min(x), max(x)), col = 2:(k +\n1), main = \"Wiener process\", xlab = \"Time t\", ylab = expression(paste(\"Values of process \", X[t], \" delta\")))\n}\nSFEWienerProcess(dt = 0.5, c = 1, k = 5)\n", "output_sequence": "Generates and plots 5 paths of a Wiener process with c=1, delta_t=0.5."}, {"input_sequence": "Author: Cindy Lamm, Karel Komor\u00e1d ; clear\nclc\nclose all\n%% User inputs parameters\ndisp('Please input Delta t, Constant c, Trajectories as: [1, 3]') ;\ndisp(' ') ;\npara = input('[Delta t, Constant c, Trajectories]=');\nwhile length(para) < 3\ndisp('Not enough input arguments. Please input in 1 * 3 vector form like [1, 3] or [1 1 3]');\npara=input('[Delta t, Constant c, Trajectories]=');\nend\ndt = para(1);\n%% Main calculation\nl = 100;\nn = floor(l / dt);\nt = 0 : dt : n * dt;\nrng(1); %seed random number generator\nz = rand(n, k); %simulate U[0,1] r.v.'s\nz = 2 * (z > 0.5) - 1;\nz = z * c * sqrt(dt); %//to get finite and non-zero varinace\nzz = zeros(1, k);\nx = [zz; cumsum(z)];\nlistik = [t', x];\n%% Output\nhold on\nfor j = 1:k\ncolors = rand(1, 3);\nplot(listik(:, 1), listik(:, j+1), 'Color', 'Linewidth', 2)\ntitle('Wiener process')\nxlabel('Time t')\nylabel('Values of process X_t delta')\n", "output_sequence": "Generates and plots 5 paths of a Wiener process with c=1, delta_t=0.5."}, {"input_sequence": "Author: Li Sun, Awdesch Melzer ; # clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# install and load packages\nlibraries = c(\"neldermead\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\nVasimle = function(Params) {\nend = length(x)\ns1 = x[2:end]\ns2 = x[1:(end - 1)]\ndelta = 1/252\na = Params[1]\nn = length(s1)\nv = (sigma^2 * (1 - exp(-2 * a * delta)))/(2 * a)\nf = s1 - s2 * exp(-a * delta) - b * (1 - exp(-a * delta))\nlnL = (n/2) * log(2 * pi) + n * log(sqrt(v)) + sum((f/sqrt(v))^2)/2\nreturn(lnL)\n}\n# load data\nx = read.table(\"yield_US3month9808.txt\")\nx = x[1:2600, 1]/100\nn = length(x) - 1\ndelt = 1/252\n# Least square innitial estimation\nX1 = sum(x[1:(end - 1)])\nX4 = sum(x[1:(end - 1)] * x[2:end])\nX5 = sum(x[2:end]^2)\nc = (n * X4 - X1 * X2)/(n * X3 - X1^2)\nd = (X2 - c * X1)/n\nsd = sqrt((n * X5 - X2^2 - c * (n * X4 - X1 * X2))/n/(n - 2))\nlambda = -log(c)/delt\nmu = d/(1 - c)\nsigma = sd * sqrt(-2 * log(c)/delt/(1 - c^2))\nInitialParams = c(lambda, mu, sigma)\n# optimize the Likelihood function\noptions = optimset(method = \"fminsearch\", MaxIter = 300, MaxFunEvals = 300, Display = \"iter\",\nTolFun = c(1e-04), TolX = c(1e-04))\nyhat = fminsearch(Vasimle, x0 = InitialParams, options)\nResults = NULL\nResults$Params = yhat$x\nResults$Fval = -yhat$fval/n\nResults$Exitflag = yhat$Exitflag\na = yhat$x[1]\n# Estimates\nrbind(a, b, sigma)\nprint(paste(\"log-likelihood = \", Results$Fval))\n", "output_sequence": "SFEVasiml shows the estimation of Vasicek model using 2600 observations of the yields of the US 3 month treasury bill from 19980102 to 20080522."}, {"input_sequence": "Author: Li Sun, Awdesch Melzer ; function lnL = Vasimle(Params, X)\ns1 = X(2:end);\ndelta = 1 / 252;\na = Params(1);\nv = (sigma^2 * (1 - exp(-2 * a * delta))) / (2 * a);\nf = s1 - s2. * exp(-a * delta) - b * (1 - exp(-a * delta));\nlnL = (n / 2) * log(2 * pi) + n * log(sqrt(v))+sum((f / sqrt(v)).^2) / 2;\n", "output_sequence": "SFEVasiml shows the estimation of Vasicek model using 2600 observations of the yields of the US 3 month treasury bill from 19980102 to 20080522."}, {"input_sequence": "Author: Li Sun, Awdesch Melzer ; clear all\nclose all\nclc\nload yield_US3month9808.txt;\nX = yield_US3month9808(1:2600)/100;\nn = length(X)-1;\ndelt = 1/252;\n% Least square innitial estimation\nX1 = sum( X(1:end-1) );\nc = ( n*X4 - X1*X2 ) / ( n*X3 -X1^2 );\nsd = sqrt( (n*X5 - X2^2 - c*(n*X4 - X1*X2) )/n/(n-2) );\nlambda = -log(c)/delt;\nmu = d/(1-c);\nsigma = sd * sqrt( -2*log(c)/delt/(1-c^2) ) ;\nInitialParams = [lambda mu sigma];\n%optimize the Likelihood function\noptions = optimset('LargeScale', 'off', 'MaxIter', 300, 'MaxFunEvals', 300, 'Display', 'iter', 'TolFun', 1e-4, 'TolX', 1e-4, 'TolCon', 1e-4);\n[Params, Fval, Exitflag] = fminsearch(@(Params) Vasimle(Params,X), InitialParams,options);\nResults.Params = Params;\nResults.Exitflag = Exitflag;\nfprintf('\\n a = %+3.6f\\n b = %+3.6f\\n sigma = %+3.6f\\n', Params(1),\n", "output_sequence": "SFEVasiml shows the estimation of Vasicek model using 2600 observations of the yields of the US 3 month treasury bill from 19980102 to 20080522."}, {"input_sequence": "Author: Awdesch Melzer, Li Sun ; # clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# Load caplet data\ncap = read.table(\"cap.txt\")\n# Parameters (a, b, c, d) of time-homogeneous component of the volatility function\na = 0.001651\nb = 1.238157\nc = 0.001036\nd = 6.757801\nti = cap[, 1]\nh2 = -1/4 * (-2 * a^2 * c^2 - 2 * a * b * c - 8 * a * d * c^2 - b^2 - 8 * b *\nd * c + 2 * exp(-c * ti)^2 * a^2 * c^2 + 4 * a * b * exp(-c * ti)^2 * c^2 *\nti + 2 * a * b * exp(-c * ti)^2 * c + 8 * exp(-c * ti) * a * d * c^2 +\n2 * b^2 * exp(-c * ti)^2 * c^2 * ti^2 + 2 * b^2 * exp(-c * ti)^2 * c *\nti + b^2 * exp(-c * ti)^2 + 8 * b * d * exp(-c * ti) * c^2 * ti + 8 * b *\nd * exp(-c * ti) * c - 4 * d^2 * c^3 * ti)/c^3\ng = ((sig_b^2) * ti)/h2\n(k = sqrt(g))\n# Plot\nplot(sig_b, col = \"blue\", type = \"l\", lwd = 2, xlab = c(\"Time to Maturity\"),\n", "output_sequence": "Plots the Black implied volatility structure of EUR caplets."}, {"input_sequence": "Author: ['Maria Grith', 'Awdesch Melzer'] ; \nclear\nclc\nclose all\n\nload ('volsurfdata2.dat')\nx = volsurfdata2;\nx(:,7) = x(:,1)./x(:,2);\nPrice = x(:,1);\niv = blsimpv(Price, Strike, Rate, Time, Value,[],[], [],Class);\nfirstmon = 0.8;\nlastmat = 1;\nstepwidth = [0.02 1/52];\nlengthmon = ceil((lastmon-firstmon)/stepwidth(1));\nmongrid = linspace(0.8,1.2,lengthmon+1);\n[MON, MAT] = meshgrid(mongrid,matgrid);\ngmon = lengthmon+1;\nuu = size(x);\nbeta = zeros(gmat,gmon);\nj = 1;\nwhile (j<gmat+1);\nk = 1;\nwhile (k<gmon+1);\ni = 1;\nX = zeros(v,3);\nwhile (i<v+1);\nX(i,:) = [1,x(i,7)-MON(j,k), x(i,4)-MAT(j,k)];\ni = i+1;\nend\nY = iv;\nW = zeros(v,v); %Kernel matrix\n\nu1 = (x(i,7)-MON(j,k))/h1;\naa = 15/16*(1-u1^2)^2*(abs(u1) <= 1)/h1;\nW(i,i) = aa*bb;\nest = inv(X'*W*X)*X'*W*Y;\nbeta(j,k) = est(1);\nk = k+1;\nend\nj = j+1;\nend\nIV = beta;\nex1 = find(x(:,4)>1);\nex2 = find(x(:,7)<0.8 |x(:,7)>1.2);\nex = [ex1;ex2];\nx(ex,:) = [];\nPrice = x(:,1);\niv = blsimpv(Price, Strike, Rate, Time, Value,[],[], [],Class);\nsurf(MON,MAT,IV)\ncolormap hsv\nalpha(0.3)\nhold on\nscatter3(mon, Time,iv, 'filled')\nxlabel('Moneyness')\nylabel('Time to Maturity')\nzlabel('Implied Volatility')\n", "output_sequence": "Produces a graphic visualisation of the implied volatility surface of the DAX option. The original options are shown as blue points."}, {"input_sequence": "Author: ['Maria Grith', 'Awdesch Melzer'] ; # clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n\n# install and load packages\nlibraries = c(\"lattice\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# Black-Scholes Function\nBS = function(S, K, Time, r, sig, type) {\nd1 = (log(S/K) + (r + sig^2/2) * Time)/(sig * sqrt(Time))\nd2 = d1 - sig * sqrt(Time)\nif (type == 1) {\nvalue = S * pnorm(d1) - K * exp(-r * Time) * pnorm(d2)\n}\nif (type == 0) {\nvalue = K * exp(-r * Time) * pnorm(-d2) - S * pnorm(-d1)\nreturn(value)\n}\n# Function to find BS Implied Volatility using Bisection Method\nblsimpv = function(S, K, Time, r, market, type) {\nsig = 0.2\nsig.down = 0.001\ncount = 0\nerr = BS(S, K, Time, r, sig, type) - market\n\n# repeat until error is sufficiently small or counter hits 1000\nwhile (abs(err) > 1e-05 && count < 1000) {\nif (err < 0) {\nsig.down = sig\nsig = (sig.up + sig)/2\n} else {\nsig.up = sig\nsig = (sig.down + sig)/2\n}\nerr = BS(S, K, Time, r, sig, type) - market\ncount = count + 1\n# return NA if counter hit 1000\nif (count == 1000) {\nreturn(NA)\n} else {\nreturn(sig)\n# load data\nx = read.table(\"volsurfdata2.dat\")\n# define variables\nx[, 7] = x[, 1]/x[, 2] # define moneyness\nPrice = x[, 1]\nn = length(x[, 1])\n# calculate implied volatility\niv = rep(0, n)\nfor (i in 1:n) {\niv[i] = blsimpv(S = Price[i], K = Strike[i], Time = Time[i], r = Rate[i], market = Value[i],\ntype = Class[i])\nfirstmon = 0.8\nlastmat = 1\nstepwidth = c(0.02, 1/52)\nlengthmon = ceiling((lastmon - firstmon)/stepwidth[1])\nmongrid = seq(0.8, 1.2, length = c(lengthmon + 1))\n# grid function\nmeshgrid = function(a, b) {\nlist(x = outer(b * 0, a, FUN = \"+\"), y = outer(b, a * 0, FUN = \"+\"))\n# compute grid\ngridone = meshgrid(mongrid, matgrid)\nMON = gridone$x\ngmon = lengthmon + 1L\nuu = dim(x)\nv = uu[1]\n# calculate the implied volatility surface\nbeta = matrix(0, gmat, gmon)\nj = 1L\nwhile (j < gmat + 1L) {\nk = 1L\nwhile (k < gmon + 1L) {\ni = 1L\nX = matrix(0, v, 3)\nwhile (i < (v + 1L)) {\nX[i, ] = c(1, x[i, 7] - MON[j, k], x[i, 4] - MAT[j, k])\ni = i + 1\nh2 = 0.75\nW = matrix(0, v, v) # Kernel matrix\ni = 1L\nu1 = (x[i, 7] - MON[j, k])/h1\naa = 15/16 * (1 - u1^2)^2 %*% (abs(u1) <= 1)/h1\nW[i, i] = aa %*% bb\nest = solve(t(X) %*% W %*% X) %*% t(X) %*% W %*% Y\nbeta[j, k] = est[1]\nk = k + 1L\nIV = beta\n# select points for elimination\nex1 = which(x[, 4] > 1) # Time to maturity > 1\nex2 = which(x[, 7] < 0.8 | x[, 7] > 1.2) # moneyness < 0.8 or > 1.2\nex = c(ex1, ex2)\nxnew = x\nxnew = xnew[-ex, ] # eliminate data points\n# redefine variables\nPrice = xnew[, 1]\nn = length(xnew[, 1])\n# calculate implied volatility for original options\niv[i] = blsimpv(Price[i], Strike[i], Time[i], Value[i], Class[i])\n# define points\npts = cbind(mon, Time, iv)\n# plot\nwireframe(IV ~ MON + MAT, drape = T, ticktype = \"detailed\", pts = pts, main = \"\",\naspect = c(1, 1), scales = list(arrows = FALSE, y = list(labels = seq(0, 1, 0.2)),\nx = list(labels = seq(0.8, 1.4, 0.1)), z = list(labels = seq(0.2, 0.5, 0.05))),\nylab = list(\"Time to Maturity\", rot = -38), xlab = list(\"Moneyness\", rot = 33),\nzlab = list(\"Implied Volatility\", rot = 94), zlim = c(0.25, 0.45), panel.3d.wireframe = function(x,\ny, z, xlim, xlim.scaled, pts, drape = drape,\n...) {\npanel.3dwire(x, y, z, xlim = xlim, ylim = ylim, zlim = zlim, xlim.scaled = xlim.scaled,\nylim.scaled = ylim.scaled, = zlim.scaled, drape = TRUE, ...)\n\npanel.3dscatter(pts[, 1], pts[, 2], pts[, 3], xlim = xlim, ylim = ylim, zlim = zlim,\nxlim.scaled = xlim.scaled, = ylim.scaled, = zlim.scaled,\ntype = \"p\", col = c(4), lwd = 3, cex = 1, pch = c(19), .scale = TRUE,\n...)\n", "output_sequence": "Produces a graphic visualisation of the implied volatility surface of the DAX option. The original options are shown as blue points."}, {"input_sequence": "Author: Joanna Tomanek, Lasse Groth, WK H\u00e4rdle ; # clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# parameter settings\nn = 100\nbeta = 0.5\n# Simulation of MA(1)-processes as the true values\nx = arima.sim(n, model = list(ar = 0, d = 0, ma = beta), rand.gen = function(n) rnorm(n,\n0, 1))\nx = as.matrix(x)\n# Estimated values for beta\nk = 19\ntheta = seq(-0.9, by = 0.1, 0.9)\nl1 = matrix(1, k, 1)\nl2 = l1\nfor (i in 1:k) {\nb = theta[i] # beta estimate [i]\ng = diag((b^2 + 1), n, n)\nh = cbind(rbind(0, h1), 0)\ng = g + h + t(h) # Covariance matrix\nl1[i] = -(n/2) * log(2 * pi) - 0.5 * log(det(g)) - 0.5 * t(x) %*% solve(g) %*%\nx # exact Log likelihood\narcoeff = (-b)^(1:(n - 1)) # coefficients of AR(1) process for lag=2:10\ne = matrix(1, n, 1)\n\n# Approximation of errors\ne[1] = x[1]\nfor (t in 2:10) {\ne[t] = x[t] + sum(t(arcoeff[1:(t - 1)]) * x[(t - 1):1])\n}\nl2[i] = -(n/2) * log(2 * pi) - 0.5 * sum(e^2) # Conditional log likelihood\n}\n# Plots\ndat1 = cbind(theta, l1)\nplot(dat1, col = 4, xlab = \"Beta\", ylab = \"log-Likelihood\", main = \"likelihood function of an MA(1) Process\",\ntype = \"l\", lwd = 2)\ndat2 = cbind(theta, l2)\npoints(dat2, type = \"l\", col = 2, lty = 2, lwd = 2)\n", "output_sequence": "Plots the exact and conditional likelihood function of an MA(1) (moving average) process."}, {"input_sequence": "Author: Wolfgang K. Haerdle, Konstantin Haeusler ; # clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# install and load packages, install 'fExtremes' package for the Gumbel distribution\nlibraries = c(\"fExtremes\", \"evd\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) )\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\nset.seed(1234)\n\n# global parameter settings\nn = 100\ny = (1:n) / (n + 1)\n# choose EVT cdf for the plot header only\n# sel = \"Weibull\"\n# sel \"Weibull\"\n# x = rweibull(n, shape=2, scale = 1) # you may change these params\n# sel \"Frechet\"\n# x = rfrechet(n, loc = 0, scale = 1, shape = 1) # you may change these params\n# sel \"Gumbel\"\nx = rgumbel(n, loc=0, scale=1) # you may change these params\n# now calc the order statistics of the generated rv's\nx = sort(x)\npx= pnorm(x) # calc the probas on the standard normal scale\n# png( paste0(\"SFEevt\", sel, \".png\") )\npar(pty=\"s\")\nplot(y, y, col = \"red\", type = \"line\", lwd = 2.5, main = paste( \"PP Plot of Extreme Value - \", sel),\nxlab = \"X\", ylab = \"Y\", xaxt = \"n\", yaxt = \"n\", asp=1, xlim=c(0,1),\naxis(1, at = seq(0, 1, 0.2))\npoints(px, y, col = \"blue\", pch = 19, cex = 0.8)\n# dev.off()\n", "output_sequence": "Shows the normal probability plot (PP Plot) of the pseudo random variables with extreme value distributions: Weibull, Frechet and Gumbel."}, {"input_sequence": "Author: Wolfgang K. Haerdle, Konstantin Haeusler ; clear all\nclose all\nclc\nn = 100;\n% Gumbel\ngumb1 = gevrnd(0, 1, 0, 100, 1);\ngumb2 = sort(gumb1);\ngumb = normcdf(gumb2, 0, 1);\nt = (1 : n) / (n + 1);\nhold on\nfigure(1)\nplot(t, t, 'r', 'LineWidth', 2)\nscatter(gumb, t, '.', 'b')\nt = 0 : 0.2 : 1;\nset(gca, 'YTick', t)\ntitle('PP Plot of Extreme Value - Gumbel','FontSize', 16, 'FontWeight', 'Bold')\nbox on\nset(gca, 'FontSize', 16, 'LineWidth', 2, 'FontWeight', 'bold');\nhold off\n% print -painters -dpdf -r600 SFEevt2_01.pdf\n% Frechet\nfrec1 = gevrnd(0.5, 0.5, 1, 100, 1);\nfrec2 = sort(frec1);\nfrec = normcdf(frec2, 0, 1);\nfigure(2)\nscatter(frec, t, '.', 'b')\nxlim([0 1])\ntitle('PP Plot of Extreme Value - Frechet', 'FontSize', 16, 'FontWeight', 'Bold')\n% print -painters -dpdf -r600 SFEevt2_02.pdf\n% Weibull\nweib1 = gevrnd(-0.5, 0.5, -1, 100, 1);\nweib2 = sort(weib1);\nweib = normcdf(weib2, 0, 1);\nfigure(3)\nscatter(weib, t, '.', 'b')\ntitle('PP Plot of Extreme Value - Weibull', 'FontSize', 16, 'FontWeight', 'Bold')\n% print -painters -dpdf -r600 SFEevt2_03.pdf\n", "output_sequence": "Shows the normal probability plot (PP Plot) of the pseudo random variables with extreme value distributions: Weibull, Frechet and Gumbel."}, {"input_sequence": "Author: Awdesch Melzer, Li Sun ; % clear history\nclear all\n% load data file\nT = readtable('yield_US3month9808.txt', 'ReadVariableNames', false);\n% convert table to array\nA = table2array(T);\n% collect input\nModel.Data = A / 100;\nModel.delta = 1 / 250;\nModel.n = length(Model.Data);\n% least square initial estimation\nx2 = Model.Data(1:end - 1);\nxbar_1 = mean(x1);\nx3 = x1 - xbar_1;\ny1 = dot(x3, x4) / length(x1);\na = 252 * log(y1 / y2);\ngama = exp(a / 252);\nb = (xbar_1 - gama * xbar_2) / (gama - 1);\ny3 = x1 - b * (gama - 1) - gama * x2;\ny4 = (b / (2 * a)) * (gama - 1)^2 + (gama / a) * (gama - 1) * x2;\nsig = sum(y3.^2 ./ y4) / length(x1);\na = -a;\nsigma = sqrt(sig);\n% optimize the Likelihood function\nInitialParams = [a b sigma];\noptions = optimset('LargeScale', 'off', 'MaxIter', 300, 'MaxFunEvals', 300, 'Display',...\n'iter', 'TolFun', 1e-4, 'TolX', 1e-4, 'TolCon', 1e-4);\n% use fminsearch to find optimum\n[Params, Fval, Exitflag] = fminsearch(@(Params) CIRml(Params, Model), InitialParams, options);\n% define output\nResults.Params = Params;\nResults.Fval = -Fval / Model.n;\nResults.Exitflag = Exitflag;\n% print results\nfprintf('n a = %+3.6fn b = %+3.6fn sigma = %+3.6fn', Params(1),\nfprintf('log-likelihood = %+3.6fn', -Fval / Model.n);\n", "output_sequence": "Calculates MLE estimates of the parameters of the Calibrating Interest Rate Model using the yields of the US 3 month treasury bill (1998-2008)."}, {"input_sequence": "Author: Awdesch Melzer, Li Sun ; # clear history\nrm(list = ls(all = TRUE))\ngraphics.off()\n# install and load packages\nlibraries = c(\"neldermead\", \"Bessel\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# set working directory\n# setwd('C:/...')\n# load data\ndata = read.table(\"yield_US3month9808.txt\")\n# Log-likelihood function of CIR model\nCIRml = function(Params){\nlData = Model$Data;\nend = Model$n\nDataF = lData[1:end - 1];\na = Params[1];\nc = 2 * a/(sigma^2 * (1 - exp(-a * Model$delta)));\nu = c * exp(-a * Model$delta) * DataF;\nv = c * DataL;\nq = 2 * a * b / sigma^2 - 1;\nz = 2 * sqrt(u * v);\nbf = besselI(z,q,TRUE);\nlnL = -(Model$n - 1) * log(c) - sum(-u - v + 0.5 * q * log(v / u) + log(bf) + z);\nreturn(lnL)\n}\n# MAIN CALCULATION\n# Model\nModel = NULL\nModel$Data = unlist(data) / 100;\nModel$delta = 1 / 252;\nModel$n = length(Model$Data);\nend = Model$n\n# Least square innitial estimation\nx2 = Model$Data[1:(end - 1)];\nxbar_1 = mean(x1);\nx3 = x1 - xbar_1;\ny1 = sum(x3 * x4) / length(x1);\na = 252 * log(y1 / y2);\ngama = exp(a / 252);\nb = (xbar_1 - gama * xbar_2)/(gama - 1);\ny3 = x1 - b * (gama - 1) - gama * x2;\ny4 = (b / (2 * a)) * (gama - 1)^2 + (gama / a) * (gama - 1) * x2;\nsig = sum(y3^2 / y4) / length(x1);\na = -a ;\nsigma = sqrt(sig);\n# collect initial parameters\nInitialParams = c(a, b, sigma);\n# optimize the Likelihood function\noptions = optimset(method = 'fminsearch', MaxIter = 300, MaxFunEvals = 300, Display = 'iter', TolFun = c(1e-4), TolX = c(1e-4))\nyhat = fminsearch(CIRml, x0 = InitialParams, options);\nResults = NULL\nResults$Params = neldermead.get(yhat, \"xopt\");\nResults$Exitflag = yhat$exitflag;\na = Results$Params[1]\n# Estimates\nrbind(a, b, sigma)\nprint(paste(\"log-likelihood = \", Results$Fval))\n", "output_sequence": "Calculates MLE estimates of the parameters of the Calibrating Interest Rate Model using the yields of the US 3 month treasury bill (1998-2008)."}, {"input_sequence": "Author: Awdesch Melzer, Li Sun ; function lnL = CIRml(Params, Model)\n% define input parameters\nlData = Model.Data;\nDataF = lData(1:end - 1);\na = Params(1);\n\n% compute relevant parameters for the log-likelihood function\nc = 2 * a / (sigma^2 * (1 - exp(-a * Model.delta)));\nu = c * exp(-a * Model.delta) * DataF;\nv = c * DataL;\nq = 2 * a * b / sigma^2 - 1;\nz = 2 * sqrt(u .* v);\nbf = besseli(q, z, 1);\n% compute log-likelihood\nlnL = -(Model.n - 1) * log(c) - sum(-u - v + 0.5 * q * log(v ./ u) + log(bf) + z);\n", "output_sequence": "Calculates MLE estimates of the parameters of the Calibrating Interest Rate Model using the yields of the US 3 month treasury bill (1998-2008)."}, {"input_sequence": "Author: Awdesch Melzer ; # clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# install and load packages\nlibraries = c(\"pracma\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\nbvnIntegrand = function(theta, b1, b2) {\n# Bivariate normal distribution | SUBROUNTINE of mvncdf() Integrand is\n# exp(-(b1^2 + b2^2 - 2*b1*b2*sin(theta))/(2*cos(theta)^2) )\nsintheta = sin(theta)\ncossqtheta = cos(theta)^2 # always positive\nintegrand = exp(-((b1 * sintheta - b2)^2/cossqtheta + b1^2)/2)\nreturn(integrand)\n}\nmvncdf = function(b, mu, sigma) {\n# MVNCDF Multivariate normal cumulative distribution function. P = MVNCDF(B,\n# MU, SIGMA) returns the joint cumulative probability using numerical\n# integration. B is a vector of values, MU is the mean parameter vector, and\n# SIGMA is the covariance matrix.\nn = NROW(b)\nb = as.matrix(b)\nif (NCOL(b) != length(mu)) {\nstop(\"The first two inputs must be vectors of the same length.\")\n}\n# Rho = sigma/(sqrt(diag(sigma))%*%t(sqrt(diag(sigma))))\nrho = sigma[2]\nif (rho > 0) {\np1 = pnorm(apply(b, 2, min))\np1[any(is.nan(b), 2)] = NaN\n} else {\np1 = pnorm(b[, 1]) - pnorm(-b[, 2])\np1[p1 < 0] = 0 # max would drop NaNs\nif (abs(rho) < 1) {\nloLimit = asin(rho)\np2 = numeric()\nfor (i in 1:n) {\nb1 = b[i, 1]\np2[i] = integral(function(x) bvnIntegrand(x, b1, b2), xmin = loLimit,\nxmax = hiLimit, method = \"Kronrod\", reltol = 1e-10)\n}\np2 = rep(0, length(p1))\np = p1 - p2/(2 * pi)\nreturn(p)\nlowerTrLossGauss = function(sqrtBCbef, R, defProb, LAP) {\nC = qnorm(defProb, 0, 1)\nNinvK = qnorm(LAP/(1 - R), 0, 1)\nA = (C - sqrt(1 - sqrtBCbef^2) * NinvK)/sqrtBCbef\nSigma = matrix(c(1, -sqrtBCbef, 1), 2, 2)\nMu = c(0, 0)\nEL1 = mvncdf(cbind(C, -A), Mu, Sigma)\nEL2 = pnorm(A)\nLowerETL = EL1 + EL2 * LAP/(1 - R)\nreturn(LowerETL)\n", "output_sequence": "Computes the lower expected tranche loss."}, {"input_sequence": "Author: Christian M. Hafner, Ying Chen ; import pandas as pd\nimport numpy as np\nfrom statsmodels.graphics.tsaplots import plot_pacf\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.arima_process import arma_generate_sample\n# parameter settings\nlags = 30 # lag value\nn = 1000\nalphas = np.array([0.])\n# add zero-lag and negate alphas\nar = np.r_[1, -alphas]\nma1 = np.r_[1, np.array([0.5,0.4])]\nf, axarr = plt.subplots(2, 2,figsize=(11, 6))\nsimulated_data_1 = arma_generate_sample(ar=ar, ma=ma1, nsample=n)\nplot_pacf(simulated_data_1,lags=lags, ax=axarr[0, 0],title='Sample ACF Plot with '+r'$\\beta_1$='+str(ma1[1])+' and '+r'$\\beta_2$='+str(ma1[2]))\naxarr[0, 0].set_xlabel('lags')\nsimulated_data_2 = arma_generate_sample(ar=ar, ma=ma2, nsample=n)\nplot_pacf(simulated_data_2,lags=lags, ax=axarr[0, 1],title='Sample ACF Plot with '+r'$\\beta_1$='+str(ma2[1])+' and '+r'$\\beta_2$='+str(ma2[2]))\naxarr[0, 1].set_xlabel('lags')\nsimulated_data_3 = arma_generate_sample(ar=ar, ma=ma3, nsample=n)\nplot_pacf(simulated_data_3,lags=lags, ax=axarr[1, 0],title='Sample ACF Plot with '+r'$\\beta_1$='+str(ma3[1])+' and '+r'$\\beta_2$='+str(ma3[2]))\naxarr[1, 0].set_xlabel('lags')\nsimulated_data_4 = arma_generate_sample(ar=ar, ma=ma4, nsample=n)\nplot_pacf(simulated_data_4,lags=lags, ax=axarr[1,1],title='Sample ACF Plot with '+r'$\\beta_1$='+str(ma4[1])+' and '+r'$\\beta_2$='+str(ma4[2]))\naxarr[1, 1].set_xlabel('lags')\nplt.tight_layout()\nplt.savefig('SFEpacfma2_py.png')\n", "output_sequence": "Plots the partial autocorrelation function of an MA(2) (moving average) process."}, {"input_sequence": "Author: Christian M. Hafner, Ying Chen ; # clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# parameter settings\nlag = 30 # lag value\nb1 = 0.5 # value of beta_1\n# Plot\nplot(ARMAacf(ar = numeric(0), ma = c(b1, b2), lag.max = lag, pacf = TRUE), type = \"h\",\nxlab = \"lag\", ylab = \"pacf\")\ntitle(\"Sample partial autocorrelation function (pacf)\")\n", "output_sequence": "Plots the partial autocorrelation function of an MA(2) (moving average) process."}, {"input_sequence": "Author: Dedy D. Prastyo, Franziska Schulz ; clear all\nclose all\nclc\ndisp('Please input [Probability, Number n] as: [0.5, 15]') ;\ndisp(' ') ;\npara = input('[Probability, Number n] = ');\nwhile length(para) < 2\ndisp('Not enough input arguments. Please input in 1 * 2 vector form like [0.5, 15] or [0.5 15]');\npara = input('[Probability, Number n] = ');\nend\np = para(1);\nx = 1:n;\nsubplot(2, 1, 1)\ny = binopdf(x, n, p);\nfor i = 1:length(x)\nline([i, i],[0, y(i)], 'LineWidth', 5)\ntitle('Binomial distribution')\nsubplot(2, 1, 2)\nz = binocdf(x, n, p);\nline([i - 1, i], [z(i), 'LineWidth', 3, 'Color', 'r')\n% plot(x, z, '+')\ndisp('Please input [Value of x, Probability, Number n] as: [5, 0.5, 15]') ;\nwhile length(para) < 3\ndisp('Not enough input arguments. Please input in 1 * 2 vector form like [5, 0.5, 15] or [5 0.5 15]');\npara = input('[Value of x, Probability, Number n] = ');\nx1 = para(1);\ndisp(' ')\ndisp('Binomial distribution for the specified x, p, n')\ndisp('P(X = x) = f(x) =')\nbinopdf(x1, n1, p1)\ndisp('P(X <= x) = F(x) =')\nbinocdf(x1, n1, p1)\ndisp('P(X >= x) = 1 - F(x - 1) =')\n1 - binocdf(x1 - 1, n1, p1)\ndisp('P(X < x) = P(X <= x) - P(X = x) = P(X <= x - 1) = F(x - 1) =')\nbinocdf(x1 - 1, n1, p1)\ndisp('P(X > x) = P(X >= x) - P(X = x) = P(X >= x + 1) = 1 - F(x) =')\n", "output_sequence": "Plots the density and distribution function of a binomial distributed random variable with different parameters and calculates probabilities."}, {"input_sequence": "Author: Dedy D. Prastyo, Franziska Schulz ; # clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# parameter settings\npara = c(0.5, 15)\np = para[1] # Probability\nn = para[2] # Number n\nx = 1:n\n# plot\npar(mfrow = c(2, 1))\ny = dbinom(x, n, p)\nplot(x, y, col = \"blue3\", lwd = 5, type = \"h\", main = \"Binomial distribution\",\nylab = \"f(x)\")\nz = pbinom(x, n, p)\nplot(x, z, col = \"red3\", lwd = 3, type = \"s\", ylab = \"F(x)\")\npara = c(5, 0.5, 15)\nx1 = para[1] # Value of x\np1 = para[2] # Probability\nn1 = para[3] # Number n\nprint(\"Binomial distribution for the specified x, p, n\")\nprint(\"P(X=x) = f(x) =\")\ndbinom(x1, n1, p1)\nprint(\"P(X<=x) = F(x) =\")\npbinom(x1, n1, p1)\nprint(\"P(X>=x) = 1-F(x-1) =\")\n1 - pbinom(x1 - 1, n1, p1)\nprint(\"P(X<x) = P(X<=x) - P(X=x) = P(X<=x-1) = F(x-1) =\")\npbinom(x1 - 1, n1, p1)\nprint(\"P(X>x) = P(X>=x) - P(X=x) = P(X>=x+1) = 1 - F(x) =\")\n1 - pbinom(x1, n1, p1)\n", "output_sequence": "Plots the density and distribution function of a binomial distributed random variable with different parameters and calculates probabilities."}, {"input_sequence": "Author: Awdesch Melzer ; # clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# parameter settings\nn = 1000\nseed = 12\n# main computation\ny = NULL\ny[1] = seed\ni = 2\nwhile (i <= n) {\ny[i] = (a * y[i - 1] + b)%%M\ni = i + 1\n}\ny = y/M\n# output\nplot(y[1:(n - 2)], y[2:(n - 1)], col = \"black\", xlab = c(expression(U * bold(scriptstyle(atop(phantom(1),\n", "output_sequence": "Generates uniform random numbers using RANDU generator and produces a 2d plot of generated numbers where the pairs from the hyperplains can be visible. Shows that the points generated by the algorithm are lying on 5 straight lines, with c = 0; 1; 2; 3; 4."}, {"input_sequence": "Author: Maria Osipenko ; function [Call Put] = blsprice(S,K,r,sigma,tau)\n\nif tau == 0\nt = 1;\nelse\nt = 0;\nend\ny = (log(S/K)+(r-sigma^2/2)*tau)/(sigma*sqrt(tau)+t);\ncdfn = normcdf(y+sigma*sqrt(tau));\n\nif t ==0\nt_l = 1;\nCall = S*(cdfn*t_l+t)-K*exp(-r*tau)*normcdf(y)*t_l+t;\nPut = K*exp(-r*tau)*(normcdf(-y))*t_l+t-S*(normcdf(-y-sigma*sqrt(tau))*t_l+t);\n", "output_sequence": "Black-Scholes price function. It is used for calculation of put and call option prices in the Quantlet SFEbsprices."}, {"input_sequence": "Author: Ying Chen, Wee Song Chua, Wolfgang Karl Haerdle ; %% Plot sample cross correlation function between log-accumulated ..\n% volumes at best bid and ask price\n\n% load relevant data\nload('AEZS.mat')\n% declare vectors to store relevant data\nlvl1VolBid = [];\nlvl1LogAccVolAsk = [];\nfor t = 1:T\nfor i = 1:n\nlvl1VolAsk(i + (t - 1)*n) = askVol_org{i + (t - 1)*n}(1);\nend\nend\n% Sample cross-correlation plot\ncrosscorr(lvl1LogAccVolBid, lvl1LogAccVolAsk)\ntitle( {'Sample cross correlation function between log-accumulated ' ...\n'volumes',char( strcat(' at best bid and best ask price for',{' '}, ...\n", "output_sequence": "Estimates the sample cross correlation function between log-accumulated volumes at best bid and ask price for AEZS from 2rd Jan,2015 till 6th Mar,2015."}, {"input_sequence": "Author: Ying Chen, Wee Song Chua, Wolfgang Karl Haerdle ; %% Plot MAPE of VFAR versus Random Walk for multiple step forecast\n\n% load relevant data\nload('AEZSmultiforecast.mat')\n% MAPE plot\nplot(1:endh,MAPEhorizonVFAR,'b','LineWidth',1.5)\nhold on\nplot(1:endh,MAPEhorizonNAIVE,'--k','LineWidth',1.5)\nhold off\nxlim([0 endh])\nylim([lowYlim upYlim])\ntitle(ticker)\nylabel('MAPE')\n", "output_sequence": "Plot MAPE of VFAR versus Random Walk for multiple step forecasts for AEZS"}, {"input_sequence": "Author: Konstantin H\u00e4usler, Junjie Hu, Vanessa Emmanuela Guarino ; from pycoingecko import CoinGeckoAPI\ncg = CoinGeckoAPI()\nimport pandas as pd\nimport time\nfrom datetime import datetime\nfrom functools import reduce\nclass APIwrapper:\n\ndef __init__(self):\nself.socket = CoinGeckoAPI()\n\ndef coin_api(self, tries, delay, backoff, *args):\nfor n in range(tries):\ntry:\nreturn self.socket.get_coin_market_chart_range_by_id(*args)\n\nexcept (KeyboardInterrupt, SystemExit):\nraise\nexcept Exception as e:\ntime.sleep(delay)\ndelay *= backoff\nprint(f\"{e} occurred. New attempt in {delay} seconds\")\ncontinue\nraise NameError(f\"Failed within {tries} attempts\")\ndef get_coins_name_id(dataframe_coin):\ncoin_ids = dataframe_coin['id']\nreturn coin_ids, coin_names\ndef create_dataframe(coin, coin_id, minimum_date,\ndataframe = []\ntimeseries = []\nnames = [key for key, val in coin.items()]\nfor i in range([len(val) for key, val in coin.items()][0]):\ntry:\nvariables = [item[i][1] for item in coin.values()]\ndate = [datetime.fromtimestamp(item[i][0]/1000).strftime(\"%Y-%m-%d\") for item in coin.values()][0]\ndataframe.append(variables)\ntimeseries.append(date)\nminimum_date.append(min(timeseries))\n\nexcept (KeyboardInterrupt, SystemExit):\nexcept Exception as e:\ntime.sleep(10)\nprint(f\"Error: {e}, occurred for {coin_id}. New attempt in 5 seconds.\")\ncontinue\ndataframe = pd.DataFrame(dataframe, columns = names)\ndataframe[\"Datetime\"] = timeseries\ndataframe[\"Datetime\"] = pd.to_datetime(dataframe[\"Datetime\"])\ndataframe[\"Id\"] = coin_id\ndataframe = dataframe.set_index([\"Datetime\",\"Id\"])\nreturn minimum_date, timeseries, dataframe\ndef get_df_by_coin_time_range_to_csv(coin_ids, n_start = 0, n_end = 10,\nfrom_time = '1522195200', to_time = '1575158400', Save = False):\nframes = []\nminim_date = []\nid_coin = []\nfor coin in coin_ids[n_start : n_end]:\n\napi = APIwrapper()\ntmp = api.coin_api(10, 1, 2, coin,'usd', from_time, to_time)\nif [val for key, val in tmp.items()][0] == []:\ncontinue\nelse:\nid_coin.append(coin)\nminim_date, t, dataframe = create_dataframe(tmp, coin, minim_date,\nif Save == True:\ndataframe.to_csv(coin)\nprint(f\"{coin} correctly saved.\")\nframes.append(dataframe)\nelse:\ntime.sleep(5)\nreturn frames, minim_date, id_coin\ndef main():\napi = APIwrapper()\ncoinlist_df = pd.DataFrame(api.socket.get_coins_list())\ncoin_ids, coin_names = get_coins_name_id(coinlist_df)\nframes, minim_date, id_coin = get_df_by_coin_time_range_to_csv(coin_ids, n_start = 0, n_end = len(coin_ids),\nfrom_time = '1357041600', Save = True)\nif minim_date == []:\nprint(\"Coins data available only on minutely/hourly basis. Try with another sequence.\")\nelse:\nindex = pd.MultiIndex.from_product([pd.date_range(start= min(minim_date), end=max(maxim_date)).tolist(),\nid_coin], names =['Datetime', 'Id'])\nmatch_df = pd.DataFrame(index = index)\nfilter_type_frames = match_df.merge(reduce(lambda left,right: pd.concat( [left, right]), frames), how=\"outer\",\nleft_index=True, right_index=True).fillna(0)\nfilter_type_frames.to_csv(\"test.csv\")\nif __name__== \"__main__\":\nmain()\n", "output_sequence": "Scrapes Data on Market Capitalization, Price and Volume of all coins listed at CoinGecko.com, using the API provided by CoinGecko"}, {"input_sequence": "Author: Torsten van den Berg, Sophie Burgard ; #!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# CMBbubbles.py:\n# A simple wrapper for CMBbubbles.html for Quantnet compatibility.\n# Set HTML header (utf-8)\nhtml_header = 'Content-Type: text/html; charset=utf-8\\n\\n'\n# Open actual HTML\nhtml_content = open('CMBbubbles.html', 'r')\n# Output header and actual HTML content\nprint(html_header)\nprint(html_content.read())\n", "output_sequence": "Visualizes a collection of computers clustered by various means using D3.js and CoffeeScript. On hovering brief information of the specific computer appears (including producer, model name and picture). A live example can be found at cm.wiwi.hu-berlin.de/viz. Extended upon original work by Jim Vallandingham."}, {"input_sequence": "Author: Awdesch Melzer ; # clear variables and close windows\nrm(list=ls(all=TRUE))\ngraphics.off()\n# install packages\n# install.packages(\"simba\")\nlibrary(simba)\n# setwd(\"C:/...\") # please change working directory\nx = read.table(\"carmean2.dat\") # load data\nx = as.matrix(x[,2:9]) # retrieve Renault, Rover, Toyota\nx1719 = x[c(17:19),]\nx.mu = apply(x1719,2,mean) # column means\ny = matrix(0,nrow(x1719),ncol(x1719)) # empty matrix\n# fill binary matrix\nfor (i in 1:nrow(x1719)){ # if x(i,k)>x_bar(k): 1, else 0\nfor(k in 1:ncol(x1719)){\nif(x1719[i,k]>x.mu[k]){\ny[i,k]=1\n}else{\ny[i,k]=0\n}\n}\n# similarity coefficients for binary data\nsim(y,method=\"jaccard\") # jaccard\nsim(y,method=\"simplematching\") # simple matching\n", "output_sequence": "computes the Jaccard, simple matching and Tanimoto proximity coefficients for binary car data"}, {"input_sequence": "Author: Franziska Schulz, Brenda L\u00f3pez Cabrera ; fourier.series = function(t,terms,period)\n{\nn = length(t)\nX = matrix(,nrow=n,ncol=2*terms)\nfor(i in 1:terms)\n{\nX[,2*i-1] = sin(2*pi*i*t/period)\n}\ncolnames(X) = paste(c(\"S\",\"C\"),rep(1:terms,rep(2,terms)),sep=\"\")\nreturn(X)\n}\n# dsc = function(Load_list, WD, PH, n, k, hours){\n# PH = PH[1:n,]\n# DT_Load = array(NA, dim = length(Load_list))\n# season_forecast = matrix(ncol=1,nrow=hours)\n# for(i in 1:hours){\n# index = as.numeric(seq(from=i, to=(n+k)*hours, by=hours))\n# temp = Load_list[index]\n# t = 1:length(temp)\n# ltsc = lm(temp ~ t + WD[-(n+k+1),] + PH[-(n+k+1),] + fourier.series(t,4,365.25))\n# DT_Load[index] = ltsc$residuals\n#\n# }\n# return(DT_Load)\n# }\ndsc = function(Load_list, WD, PH, n, k, hours, p){\nDT_Load = array(NA, dim = length(Load_list))\nseason_forecast = matrix(ncol = 1, nrow = hours)\nfor(i in 1:hours){\nindex = as.numeric(seq(from = i, to = (n + k) * hours, by = hours))\ntemp = Load_list[index]\nt = 1:length(temp)\nltsc = lm(temp ~ t + WD[1:(n+k),] + PH[1:(n+k),] + fourier.series(t,4,365.25))\nDT_Load[index] = ltsc$residuals\nnew = as.data.frame(t(c(1, (max(t)+p), WD[(n+k+p),], fourier.series((max(t)+p), 4, 365.25))))\ncoeff = ltsc$coefficients\nseason_forecast[i,1]=sum(coeff * new)\nreturn(list(DT_Load,season_forecast))\nfpca = function(num, fit, x){\ndataResid = Data2fd(x,t(fit[,,num]))\nPCA_Resid = pca.fd(dataResid,nharm=4,centerfns=TRUE)\nreturn(PCA_Resid)\nplot.PC = function(ind, num, result){\nplot(result[1,][[num]][ind], ylab = paste(\"PC\" ,ind), xlab = \"Time of day\", cex.lab = 2, cex.axis = 2,lwd = 3, ylim = c(-2,2), xaxt = 'n')\nabline(h = 0, lty = 2)\naxis(1,c(0,0.25,0.5,0.75,1), c(\"00:00\",\"06:00\",\"12:00\",\"18:00\",\"24:00\"), cex.axis = 1.5)\nplot.score = function(ind, num, result){\nplot(result[3,][[num]][,ind], ylab = paste(expression(alpha) ,ind), xlab = \"Time\", cex.lab = 2, cex.axis = 2,lwd = 3, type= \"l\")\nseason.cov = function(x, days){\n# deseasonalizes covariates\nyear = 1:days\nmat = as.matrix(x)\nmod_year = days-length(x)%%days\nif(mod_year==max(year)){\nmod_year = 0\nmat = rbind(mat, matrix(NaN, nrow= mod_year, ncol=1) )\nYN1 = matrix(data=mat,nrow=days,byrow=FALSE)\nY1 = rowMeans(YN1,na.rm = TRUE) #mean over years of hourly demand\nhs = h.select(year,Y1, method=\"cv\")\ns = sm.regression(year,Y1,hs,eval.points=year, display='none')\nseason1 = s$estimate\nseason11 = rep(season1,dim(mat)[1]/days)\nresid = (mat-season11)[1:(length(mat)-mod_year)]\nreturn(resid)\nvar.model = function(num, scores, exo){\nendo = scores[num][[1]]\ncolnames(endo)<-c(\"s1\",\"s2\",\"s3\",\"s4\")\nmodel = VAR(endo, exogen = exo, lag.max = 30, type = \"const\", ic = \"AIC\")\nTemp = sapply(X = 1:4, FUN = function(ind){coef(model)[[ind]][\"Temperature\",]})\nreturn(list(Temp, Sun))\nvar.forecast = function(num, scores, exo, exo_fore, p){\nforecast = predict(model, n.ahead=p, dumvar=exo_fore)\nreturn(forecast)\nforecast.curve = function(num, result, forecast.score, season_forecast, p){\na_hat1 = forecast.score[[num]]$fcst$s1[p,1]\neval_points = seq(1/96,1,length=96)\nquant_fore = (eval.fd(eval_points,result[1,][[num]][1])*a_hat1+eval.fd(eval_points,result[1,][[num]][2])*\na_hat2+eval.fd(eval_points,result[1,][[num]][3])*a_hat3+eval.fd(eval_points,result[1,][[num]][4])*a_hat4)#+eval.fd(eval_points,pca[[3]])+\nforecast = quant_fore+season_forecast[k+p,]\nEVAL = function(Fore, Load, h, n, p){\nMSE = matrix(ncol = 96, nrow = (h))\nfor(k in 0:(h-p)){\nMSE[k+1,] = (Fore[(k+1),]-as.numeric(Load[(n+k+1),]))^2\nMAPE[k+1,] = t(abs(Fore[(k+1),]-as.numeric(Load[(n+k+1),]))/(as.numeric(Load[(n+k+1),])))\nresult = matrix(c(mean(sqrt(MSE),na.rm=TRUE), mean(MAPE,na.rm=TRUE)),ncol=2)\ncolnames(result) = c(\"RMSE\", \"MAPE\")\nreturn(result) # list(mean(sqrt(MSE),na.rm=TRUE), mean(MAPE,na.rm=TRUE))\nMSE.FDA = function(num, Fore, Load, tau, n, h, p){\nLoad_mat_reduced = as.matrix(Load[(n+1):(n+h),-1])\nDiff_Mat = Load_mat_reduced - as.matrix(Fore[,,num])\nMSE_FDA = abs(tau[num]-(Diff_Mat<=0))*Diff_Mat^2\nmean = mean(sqrt(MSE_FDA),na.rm=TRUE)\nsd = sd(sqrt(rowMeans(MSE_FDA)),na.rm=TRUE)\nresult = rbind(mean, sd)\nreturn(result)\n", "output_sequence": "Computes generalized quantiles of electicity demand and models their dynamics over time using FPCA and multivariate time-series techniques"}, {"input_sequence": "Author: Franziska Schulz, Brenda L\u00f3pez Cabrera ; ## Loads and processes Data of the TSO Amprion\nLoad = read.delim(\"Data/LoadTSO.txt\", sep=\";\") # change to LoadBU.txt for data of BU\nLoad_mat = reshape(Load[,-3],timevar=\"Uhrzeit\", idvar=\"Datum\", direction=\"wide\")\nLoad_mat[,1] = as.Date(Load_mat[,1])\nLoad_mat = Load_mat[order(Load_mat[, 1]),-98 ]\ntime = seq.Date(from=min(Load_mat[,1]),to=max(Load_mat[,1]),by=\"day\")\nLoad_mat = merge(as.data.frame(time),Load_mat,all.x=TRUE,by.x = 1, by.y = 1)\nrownames(Load_mat) = time\nLoad_pred = reshape(Load[,-4],timevar=\"Uhrzeit\", idvar=\"Datum\", direction=\"wide\")\nLoad_pred[,1] = as.Date(Load_pred[,1])\nLoad_pred = Load_pred[order(Load_pred[, 1]),-98 ]\nLoad_pred = merge(as.data.frame(time),Load_pred,all.x=TRUE,by.x = 1, by.y = 1)\nLoad_pred[,1] = time\nremove = c(seq.Date(from=as.Date(\"2010-01-01\"),to=as.Date(\"2010-01-05\"),by=\"day\"),\nLoad_mat[which(Load_mat<=10000,arr.ind = TRUE)] = NA\nLoad_mat[16,2:97] = (Load_mat[23,2:97])\nfor(i in 2:97){\na = which(is.na(Load_mat[,i]))\nweekm = a-rep(7,length(a))\nLoad_mat[a,i] = (Load_mat[weekm,i]+Load_mat[weekp,i])/2\n}\nLoad_mat = Load_mat[-c(which(Load_mat[,1]%in%remove)),]\nTemperature = read.delim(\"Data\\\\TempTSO.txt\", sep=\";\")\nSun_K = read.delim(\"Data\\\\SunKoeln.txt\", header=F)\nSun_K[,1] = as.Date(Sun_K[,1])\nSun_F = read.delim(\"Data\\\\SunFF.txt\", header=F)\nSun = rowMeans(cbind(Sun_K[,2],Sun_F[,2]))\nSun = Sun[-c(which(Sun_K[,1]%in%remove))]\nTemperature = Temperature[-c(which(Sun_K[,1]%in%remove)),2]\ndummy = cbind(as.numeric(Load_mat[,1] %in% as.Date(c(\"2010-04-02\", \"2010-04-05\",\n\"2010-11-01\",\nas.numeric(Load_mat[,1] %in% as.Date(c(\"2010-05-14\", \"2010-06-04\",\n\"2012-05-18\", \"2013-05-31\"))), as.numeric(Load_mat[,1] %in% as.Date(c(\"2010-10-03\",\n\"2011-05-01\"))), as.numeric(Load_mat[,1] %in% as.Date(c(\"2010-05-01\"))))\nrm(list=ls()[! ls() %in% c(\"Load\",\"Load_mat\",\"Temperature\",\"Sun\",\"Load_pred\",\"dummy\")])\n", "output_sequence": "Computes generalized quantiles of electicity demand and models their dynamics over time using FPCA and multivariate time-series techniques"}, {"input_sequence": "Author: Franziska Schulz, Brenda L\u00f3pez Cabrera ; # Models the dynamics of electricity consumption of the TSO Amprion\n# using functional data analysis of generalized quantiles\nrm(list = ls())\ngraphics.off()\n#setwd(\"...\")\nsource(\"FPCA_Electricity_Data.R\")\nlibraries = c(\"expectreg\",\"fda.usc\",\"sm\",\"vars\")\nlapply(libraries,function(x)if(!(x %in% installed.packages())){install.packages(x)})\nlapply(libraries,require,quietly=TRUE,character.only=TRUE)\n## Table 1: Summary Statistics\nstatistics = function(x){c(median(x),mean(x),sd(x),min(x),max(x))}\nstat = statistics(as.matrix(Load_mat[,-1]))\nnames(stat) = c(\"Median\",\"Mean\",\"SD\",\"Min\",\"Max\")\nstat\n## Plot 2: Temperature, Sunshine\npar(mar=c(5.2, 6.1, 2, 0.5))\npar(mfrow = c(2,1))\nplot(Temperature, type=\"l\", lwd = 3, cex.axis = 2,xaxt='n', cex.lab = 2,ylab =\"Temperature\", xlab = \"Time\")\naxis(1,c(175.5,526.5,877.5),c(\"2010\",\"2011\",\"2012\"),cex.axis=2)\nplot(Sun, type=\"l\", lwd = 3, cex.axis = 2,xaxt='n', cex.lab = 2, xlab = \"Time\")\n## Set parameter\nn = 702\ntau = c(0.01,0.05,0.25,0.5,0.75,0.95,0.99)\n## Subset for insample modeling\ntraining = Load_mat[1:n,-1]\ntraining_temp = Temperature[1:n] # 2 day forecast\ntraining_sun = Sun[1:n]\nLoad_list = (unlist(as.data.frame(t(training))))\n## DSC\ntempseq = as.numeric(rep(rep(c(1:7),each=1),length.out=n+k+2))\nWD = sapply(1:6, FUN = function(num){as.numeric(tempseq==num)})\nDT_Load = dsc(Load_list, WD, dummy, n = n, k = k, hours = 96, p = 1)[[1]]\nresid_data_yearly = matrix(DT_Load,nrow=96)\nx = seq(1/nrow(resid_data_yearly),1,length.out=nrow(resid_data_yearly)) # generate x\nfit = array(0,dim=c(ncol(resid_data_yearly),96,7))\n# Estimation of expectiles (this takes a while)\nfor(i in 1:ncol(resid_data_yearly)){\n(y = expectreg.ls(resid_data_yearly[,i]~rb(x,\"pspline\"),estimate=\"sheet\",expectiles=tau,smooth=\"gcv\"))\nfit[i,,] = y$fitted\n}\nresult = sapply(X = c(1: 7), FUN = fpca, fit = fit, x = x)\nscores = result[3,]\n## Plot 4: PC\npar(mfrow = c(2,2))\nsapply(1:4, plot.PC, num = 4, result = result)\n## Plot 4: Scores\npar(mfrow = c(4,1))\nsapply(1:4, plot.score, num = 4, result = result)\n## Table 2/3: Explained Variance\nvarprop = result[4,]\nvarprop\nresid_temp = season.cov(training_temp, 351)\nexo = cbind( resid_temp,resid_sun)\ncolnames(exo) = c(\"Temperature\",\"Sunshine\")\n## Table 4/5: VAR model\nmodel = sapply(c(1:7), FUN = var.model, scores = scores, exo = exo)\nrownames(model) = c(\"Temperature\", \"Sunshine\")\nmodel[,2]\n", "output_sequence": "Computes generalized quantiles of electicity demand and models their dynamics over time using FPCA and multivariate time-series techniques"}, {"input_sequence": "Author: Konstantin H\u00e4usler ; #clear work space\nrm(list = ls(all = TRUE))\ngraphics.off()\n# install and load packages\nlibraries = c(\"caret\",\"LogicReg\",\"e1071\",\"pROC\", \"gbm\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# set the working directory\nsetwd(\"~/Desktop/dateien/IRTG 1792/METIS/metis_presi/Rcode/\")\n# Read in data\ndata = read.csv(\"~/Desktop/dateien/IRTG 1792/METIS/metis_presi/Rcode/p2p.csv\")\ndata$status.bin <- data$status\ndata$status = as.character(factor(data$status))\n# Split into training and test data\nset.seed(1234)\nind = createDataPartition(y=data$status, p=0.8, list=F)\ntrain = data[ind,]\n#AdaBoost\nfit_ada = gbm(status ~ ratio002 + ratio003 + ratio004 + ratio005 + ratio006 +\nratio011 + ratio012 + DPO + DSO + turnover + ratio036 + ratio037 +\nratio039 + ratio040, distribution = \"adaboost\", data = train,\nn.trees = 1000,interaction.depth = 1, n.minobsinnode = 10, shrinkage = 0.1,\nbag.fraction = 0.5, train.fraction = 1, cv.folds = 0,\nkeep.data = TRUE, verbose = FALSE, class.stratify.cv = NULL)\n#variable importance\nsummary.gbm(fit_ada, las=1, xlim=c(0,25))\n#insights\nprint(fit_ada)\n#predictions on test set\npred_ada = predict(fit_ada, n.trees = 1000, newdata= test, type = \"response\")\n#treshold\nlabel <- function(data) {\ndata[data>=0.5] = 1\nreturn(data)\n}\nconfusion_inputs <- function(predictions, target){\nu = union(predictions, target)\nt = table(factor(predictions, u), factor(target, u))\nreturn(t)\npred_ada_conf = label(pred_ada)\nstatus_conf = label(test$status)\n#confusion matrix\nconfusionMatrix(confusion_inputs(pred_ada_conf, status_conf))\n#confusionMatrix(pred_ada, test$status)\n#ROC curve\nplot(roc(test$status, pred_ada,\nlevels=c(0, 1),\ndirection = \"<\"),\nidentity.lty = 2,\nprint.auc=TRUE,\n#auc.polygon=TRUE,\nlty = 1,\ncol = c(\"darkred\"),\nasp = NA)\n#stargazer(data, digits = 2, align = T)\n#logistic regression\nfit_log = glm(status.bin~ratio002 + ratio003 + ratio004 + ratio005 + ratio006 +\nratio039 + ratio040,family=\"binomial\",data=train)\n#prediction\npre_log = as.numeric(predict(fit_log,newdata=test,type=\"response\"))\nclass_log = factor(ifelse(pre_log>0.5,1,0))\n#roc curve\nplot(roc(test$status,pre_log),\nlevels=c(0, 1),\ndirection = \"<\",\nidentity.lty = 2,\nprint.auc=TRUE,\n#auc.polygon=TRUE,\nlty = 1,\ncol = c(\"darkred\"),\nasp = NA)\n", "output_sequence": "AdaBoost classification model for defaults of P2P loans. Compares the out-of-sample predictive performance to a baseline logistic regression model."}, {"input_sequence": "Author: Guo Li ; # please download the following package\nlibraries = c(\"lubridate\", \"poweRlaw\",\"igraph\",\"tables\",\"texreg\")\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# Part1: for figure 2,5,7 & Table 1 specify the path to the desired folder,\n# please set your own directory, e.g (C:/Users/Desktop/...)\n#setwd(\"~/Desktop/Data\")\n# pre-allocating for caculation of Power Law parameters\nXmin = rep(NA, 60)\n# Wealth distribution calculation loop (Fitted by Power Law model)\nfor (i in 1:length(Xmin)) {\nprint(i)\nData.PL = read.csv(paste(i, \".csv\", sep = \"\"),\nheader = T)[, 1]\nfit = power.law.fit(Data.PL, xmin = 1,\nstart = 2,\nP[i] = fit$KS.p\n}\n", "output_sequence": "Estimates the Power Law parameter Alpha as well as the goodness of fit of wealth distribution of Bitcoin with Xmin = 1 by default."}, {"input_sequence": "Author: David Steiner, Minh Nguyen ; # -*- coding: utf-8 -*-\n\"\"\"\nCreated on Mon Nov 18 20:57:26 2019\n\n@author: David\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport requests\nimport io\n# =============================================================================\n# LABELS / WALLETS\nwallets_all = pd.read_csv(\"../data/btc_wallets_new.csv\", index_col=False)\nwallets_known = pd.read_csv(\"../data/addresses_known_0.01_marketcap_2015-2020.csv\", index_col=False)\nwallets_known = pd.merge(wallets_known, wallets_all, on='address')\nwallets = wallets_all.append(wallets_predicted)\nwallets = wallets.drop_duplicates(subset='address', keep='first')\n#Statistics to all scraped wallets\nstats_wallets_all_categories = wallets_all.groupby(['category']).agg(\naddresses=('address', 'count'),\nentities =('owner', 'nunique')).sort_values(by=['addresses'], ascending=False)\nwith open('all_wallets_stats_1.tex','w') as tf:\ntf.write(stats_wallets_all_categories.to_latex())\ntmp = wallets_all[['owner', 'address']]\nstats_wallets_all_owners = tmp.groupby(['owner']).agg(['count'], as_index=False).reset_index()\n#Addresses per owner\nplt.figure(figsize = (15,40))\norder = wallets_all['address'].value_counts(ascending=False).index\nsns.countplot(y='owner', data=wallets_all, order = order)\nplt.savefig('addresses_per_owner.png', transparent=True)\n#Statistics to all relevant wallets (scraped and predicted)\n#Addresses per category known\nfig, ax = plt.subplots()\ngraph = sns.countplot(x='category', data=wallets_known, color=\"black\")\nplt.title('Labeled addresses per category of known addresses')\ngraph.set_xticklabels(graph.get_xticklabels())\nfor p in graph.patches:\nheight = p.get_height()\ngraph.text(p.get_x()+p.get_width()/2., height + 0.1,height ,ha=\"center\")\nplt.savefig('addresses_known_labeled_category.png', transparent=True)\n#Addresses per category predicted\ngraph = sns.countplot(x='category', data=wallets_predicted, color=\"black\")\nplt.title('Labeled addresses per category of predicted addresses')\nplt.savefig('addresses_predicted_labeled_category.png', transparent=True)\n#Addresses per category all\ngraph = sns.countplot(x='category', data=wallets_all, color=\"black\")\nplt.title('Labeled addresses per category of all known addresses')\nplt.savefig('addresses_all_labeled_category.png', transparent=True)\n# Transactions\ndef merge_tnx_wallets(tnx, wallets):\ntnx = tnx.drop(['sender_name', 'sender_category', 'receiver_name', 'receiver_category'], axis=1)\nwallets = wallets.drop_duplicates(subset='address', keep='last')\n\nsender = pd.merge(tnx, wallets, left_on='sender', right_on='address', how='left')\nsender.rename(columns = {\"owner\": \"sender_name\", \"category\":\"sender_category\"}, inplace = True)\nsender = sender.drop(['address'], axis=1)\n\nreceiver = pd.merge(tnx, wallets, left_on='receiver', right_on='address', how='left')\nreceiver.rename(columns = {\"owner\": \"receiver_name\", \"category\":\"receiver_category\"}, inplace = True)\nreceiver = receiver.drop(['address'], axis=1)\ntnx = pd.merge(sender, receiver, how='inner', on=['hash', 'block_timestamp', 'sender','receiver', 'date', 'btc', 'dollar', 'percent_marketcap', 'PriceUSD'])\n\nreturn tnx\ntnx = pd.read_csv(\"../data/transactions_0.01_marketcap_2015-2020.csv\", index_col=False) #1019673\n#Preprocessing\ntnx[\"btc\"] = tnx[\"btc\"].astype(int)\ntnx['block_timestamp'] = pd.to_datetime(tnx['block_timestamp'])\ntnx['date'] = pd.to_datetime(tnx['block_timestamp']).apply(lambda x: '{year}-{month}-{day}'.format(year=x.year, month=x.month, day=x.day))\ntnx = merge_tnx_wallets(tnx, wallets) #1.029 mio addresses\ntnx = tnx.groupby(['hash'], as_index=False).first() #121023 transactions\n# Remove Self Transactions\n'''transactions with single sender / self transactions'''\n#self transactions\ntmp = tnx[tnx.groupby('hash')['sender'].transform('size') == 1]\nself_transactions = tmp[tmp['sender'] == tmp['receiver']] #43484 self transactions\nsns.countplot(x='receiver_category', data=self_transactions)\nsns.countplot(y='receiver_name', data=self_transactions, order= self_transactions['receiver_name'].value_counts(ascending=False).index)\nsns.boxplot(x=self_transactions[\"dollar\"], y=self_transactions[\"receiver_name\"], showfliers=False, order = self_transactions.groupby(\"receiver_name\")[\"dollar\"].median().fillna(0).sort_values()[::-1].index)\n#Scatterplot (Date, Dollar, BTC)\nplt.figure(figsize = (20,20))\nplt.xlim(self_transactions['block_timestamp'].min(),\ncmap = sns.cubehelix_palette(dark=.3, light=.8, as_cmap=True)\nax = sns.scatterplot(x=\"block_timestamp\", y=\"dollar\",\nhue=\"receiver_category\", size=\"btc\",\npalette=\"Set2\",\ndata=self_transactions)\n'''grouped transactions without self transactions'''\ntnx_all = tnx.groupby(['hash'], as_index=False).first()\ntnx = pd.concat([tnx_all, self_transactions]).drop_duplicates(keep=False)\n# Sankey diagram\nfrom pySankey import sankey\n#tnx = pd.read_csv(\"../data/tmp/transactions_100BTC_labeled.csv\", index_col=False) #1019673\n#tnx = tnx.fillna('unknown')\ntnx.dtypes\nmode=\"filtered_transactions\"\n#mode=\"100BTC\"\nfilt = 'predicted'\n#filt = 'unknown'\n#Sankey diagram per entity\ngrouped = tnx.groupby(['hash'], as_index=False)[ 'sender_name', 'sender_category', 'receiver_category'].agg(['min']).reset_index() #n_transactions = 6.92 mio\ngrouped.columns = ['hash', 'sender_name', 'sender_category',\ndf = grouped[['hash', 'sender_name', 'receiver_name']]\ndf = df.groupby(['sender_name', 'receiver_name'], as_index=False).agg(['count']).reset_index()\ndf.columns = ['sender_name', 'receiver_name', 'count']\ndf1 = df.sort_values(by=['count'], ascending=False).reset_index(drop=True)\ndf1=df1[df1['count'] >= 10]\nsankey.sankey(\nleft=df1['sender_name'], rightWeight=df1['count'], aspect=5,\nfontsize=4, figure_name=\"sankey_\" + mode + \"_entity\")\ndf2 = df[df['sender_name'] != df['receiver_name']]\ndf2 = df2.sort_values(by=['count'], ascending=False).reset_index(drop=True)\nleft=df2['sender_name'], rightWeight=df2['count'], aspect=1,\nfontsize=4, figure_name=\"sankey_\" + mode + \"_entity_without_self_transactions\")\ndf2 = df[(df['sender_name'] != filt) & (df['receiver_name'] != filt)]\nfontsize=4, figure_name=\"sankey_\" + mode + \"_entity_without_unknown\")\ndf2 = df2[df2['sender_name'] != df2['receiver_name']]\nfontsize=4, figure_name=\"sankey_\" + mode + \"_entity_without_unknown_and_self_transactions\")\ndf2= df2[df2['count'] >= 10]\nfontsize=4, figure_name=\"sankey_\" + mode + \"_entity_without_unknown_and_self_transactions_and_10\")\n#Sankey diagram per category\ndf = grouped[['hash','sender_category', 'receiver_category']]\ndf = df.groupby(['sender_category', 'receiver_category'], as_index=False).agg(['count']).reset_index()\ndf.columns = ['sender_category', 'count']\ndf3 = df\ndf3 = df3.sort_values(by=['count'], ascending=False).reset_index(drop=True)\nleft=df3['sender_category'], rightWeight=df3['count'], aspect=5,\nfontsize=6, figure_name=\"sankey_\" + mode + \"_category\")\ndf4 = df[(df['sender_category'] != filt) & (df['receiver_category'] != filt)]\ndf4 = df4.sort_values(by=['count'], ascending=False).reset_index(drop=True)\nleft=df4['sender_category'], rightWeight=df4['count'], aspect=5,\nfontsize=6, figure_name=\"sankey_\" + mode + \"_category_without_unknown\")\ndf4 = df[df['sender_category'] != df['receiver_category']]\nfontsize=4, figure_name=\"sankey_\" + mode + \"_category_without_self_transactions\")\ndf4 = df4[df4['sender_category'] != df4['receiver_category']]\nfontsize=4, figure_name=\"sankey_\" + mode + \"_category_without_unknown_and_self_transactions\")\n# Exploration\n#Total Transactions per category\ngraph = sns.countplot(x='receiver_category', data=tnx)\nplt.title('Receiver addresses of all transactions > 10mio US$')\nplt.savefig('transactions_category_all.png', transparent=True)\n#Valid Transactions per category\nplt.title('Receiver addresses of valid transactions > 10mio US$ per category')\nplt.savefig('transactions_category_valid.png', transparent=True)\n#Dollar value per category\nax = sns.boxplot(x=tnx[\"receiver_category\"], y=tnx[\"dollar\"], showfliers=False)\n#Receiver transactions per owner\nfig, ax1 = plt.subplots(figsize=(20,10))\ngraph = sns.countplot(ax=ax1,x='receiver_name', data=tnx, order= tnx['receiver_name'].value_counts(ascending=False).index)\ngraph.set_xticklabels(graph.get_xticklabels(),rotation=90)\n#Distribution of transactions dollar value\nplt.figure(figsize = (10,10))\nsns.boxplot(x=tnx[\"dollar\"], y=tnx[\"receiver_name\"], order = tnx.groupby(\"receiver_name\")[\"dollar\"].median().fillna(0).sort_values()[::-1].index)\n#Distribution of transactions market cap value\nsns.boxplot(x=tnx[\"percent_marketcap\"], y=tnx[\"receiver_name\"], order = tnx.groupby(\"receiver_name\")[\"percent_marketcap\"].median().fillna(0).sort_values()[::-1].index)\nsns.boxplot(x=tnx[\"receiver_category\"], y=tnx[\"percent_marketcap\"], showfliers=False)\n#plt.xlim(tnx_valid['block_timestamp'].min(),\nplt.xlim(pd.to_datetime('2015-01-01 00:00:00+00:00'), tnx['block_timestamp'].max())\ndata=tnx)\n# Transaction type\n'''categorize by transactions type'''\ndate_start = '2015-01-01'\nall_days = pd.date_range(date_start, date_end, freq='D')\ndef preprocess_transaction_types(df, category):\ndf['date'] = pd.to_datetime(df['date'])\ndf = df[df['percent_marketcap'] >= 0.02]\ndf = df.groupby(['date']).sum()\ndf = df[df['percent_marketcap'] < 10]\ndf = df.reindex(all_days)\ndf['category'] = category\ndf = df.fillna(0)\nreturn df\n#tnx_valid.columns.values\ntmp = tnx[['hash', 'date', 'dollar','percent_marketcap', 'sender_name', 'sender_category',\nall_all = preprocess_transaction_types(tmp, 'all')\nexchange_exchange = preprocess_transaction_types(tmp[(tmp['sender_category'] == 'Exchange') & (tmp['receiver_category'] == 'Exchange')], 'exchange_exchange')\n#Addresses per category\ntnx_category = pd.concat([exchange_exchange, other_exchange, exchange_other, other_other])\ngraph = sns.countplot(x='category', data=tnx_category)\nplt.title('Transactions per transaction category')\nplt.savefig('transaction_types.png', transparent=True)\n#scatterplot by transaction type\ntmp = tnx_category.reset_index().dropna()\ntmp.rename(columns = {\"index\": \"date\"}, inplace = True)\nplt.xlim(pd.to_datetime('2015-01-01 00:00:00+00:00'), tmp['date'].max())\nax = sns.scatterplot(x=\"date\", y=\"dollar\",\nhue=\"category\",\n#scatterplots by transaction type\nsns.set(style=\"ticks\", color_codes=True)\ng = sns.FacetGrid(tmp, col=\"category\", palette=\"GnBu_d\", size=5, aspect=1.5)\ng.map(plt.scatter, \"date\", \"dollar\", alpha=.1)\ng.add_legend()\n# Price data\ndef get_price_data():\nresponse=requests.get('https://coinmetrics.io/newdata/btc.csv').content\nprice = pd.read_csv(io.StringIO(response.decode('utf-8')))\nprice['date'] = pd.to_datetime(price['date'])\nprice.set_index('date', inplace=True)\nprice = price[['PriceUSD']]\nprice['return'] = price.pct_change(1) * 100\nresponse=requests.get('https://www.cryptodatadownload.com/cdd/Bitstamp_BTCUSD_d.csv', verify=False).content\nresponse = response.decode('utf-8')\nresponse = \"\\n\".join(response.split(\"\\n\")[1:])\nprice_2 = pd.read_csv(io.StringIO(response))\nprice_2['volatility'] = (1 - (price_2['Low'] / price_2['High'])) * 100\nprice_2['date'] = pd.to_datetime(price_2['Date'])\nprice_2.set_index('date', inplace=True)\nprice = price.join(price_2)\nprice = price.loc[date_start:date_end]\nprice['dif_high_close'] = ((price['High'] - price['Close']) / price['Close']) * 100\nreturn price\nprice = get_price_data()\n# Time Series Chart\nplt.figure(figsize=(15,19))\nprice_top = plt.subplot2grid((10,4), (0, 0), rowspan=2, colspan=4)\nprice_top.plot(price.index, price['PriceUSD'])\nvolatility.plot(price.index, price['volatility'])\nchange_daily.axhline(y=0, color='r', linestyle='-')\ntnx_vol_1.plot(other_exchange.index, other_exchange['percent_marketcap'])\nprice_top.set_title('Value of BTC transactions per category (in percent of btc marketcap, aggregated per day)')\nprice_top.set_ylabel('Closing Price')\nvolatility.set_ylabel('Daily Volatility')\nchange_daily.set_ylabel('Daily Return')\ntnx_vol_1.set_ylabel('other_exchange')\nchange_daily.axes.get_xaxis().set_visible(False)\nplt.savefig('transaction_types_chart.png', transparent=True)\n# Analysis, Metrics\nfrom datetime import timedelta\ndef analytics(df, price, mode='normal'):\nn_days, return_m, return_0,return_1,return_2,vola_m,vola_0,vola_1,vola_2,diff_m,diff_0,diff_1,diff_2 = ([] for i in range(13))\ncol_names = []\ncol_names.extend(['cases', 'return-1', 'vola-1', 'dif-1','diff_0', 'diff_1',\n\nif mode == 'average':\nindex_name.append('AVERAGE')\nelse:\ntransaction_category = df['category'][0]\nindex_name.append(str(transaction_category))\nfor day in range(-1,3):\nif mode == 'average':\nreturn_whale = price['return'].mean()\n\ntx = len(df)\nn_days.extend([tx])\n\nelse:\ndf['date'] = df.index + timedelta(days=day)\ntmp = pd.merge(df, price, left_on='date', right_index=True, how='inner')\n\nwhale = tmp[tmp['dollar'] != 0]\nreturn_whale = whale['return'].mean()\ntx = len(tmp[tmp['dollar'] != 0])\nif day == -1:\nreturn_m.extend([return_whale])\nif day == 0:\nreturn_0.extend([return_whale])\n\nelif day == 1:\nreturn_1.extend([return_whale])\nelif day == 2:\nreturn_2.extend([return_whale])\nresult = pd.DataFrame(list(zip(n_days, return_m,return_0, return_1, return_2,vola_m, vola_0, diff_m, diff_2)), index = index_name, columns = col_names)\nreturn result\ndf = pd.DataFrame()\ndf = df.append(analytics(all_all, price, mode='average'))\ndf = df.append(analytics(other_exchange, price))\nwith open('final_metrics.tex','w') as tf:\n", "output_sequence": "Exploration of Scraped Addresses, Transaction Flow (Sankey Diagram), Transaction and Category Details, Time Series Chart and Transaction Categories. As well as analyzing the impact of different transaction categories on the Bitcoin Price through different metrics"}, {"input_sequence": "Author: Vladimir Georgescu, Jorge Patron, Song Song ; #pip install statsmodels==0.13.2\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nfrom KDEpy import FFTKDE\n# load data\nx = pd.read_csv(\"bank2.dat\", sep = \"\\s+\", header=None)\nx1 = x.iloc[0:100, 5]\n# Compute kernel density estimate\nfh1_x, = FFTKDE(bw=\"silverman\", kernel='biweight').fit(np.array(x1)).evaluate()\nfig, ax = plt.subplots(figsize=(10,10))\nax.plot(fh1_x, fh1_y, c=\"black\")\nplt.xlim(137, 143)\nplt.xlabel(\"Counterfeit / Genuine\")\nplt.ylabel(\"Density estimates for diagonals\")\nplt.title(\"Swiss bank notes\")\nplt.show()\n", "output_sequence": "Computes kernel density estimates of the diagonal of the genuine and forged swiss bank notes. The bandwidth parameter are chosen by Silverman rule of thumb."}, {"input_sequence": "Author: Vladimir Georgescu, Jorge Patron, Song Song ; # clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# install and load packages\nlibraries = c(\"KernSmooth\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# load data\nx = read.table(\"bank2.dat\")\nx1 = x[1:100, 6]\n# Compute kernel density estimate\nfh1 = bkde(x1, kernel = \"biweight\")\n# plot\nplot(fh1, type = \"l\", lwd = 2, xlab = \"Counterfeit / Genuine\",\nylab = \"Density estimates for diagonals\", col = \"black\", main = \"Swiss bank notes\",\nxlim = c(137, 143), ylim = c(0, 0.85))\nlines(fh2, lty = \"dotted\", lwd = 2, col = \"red3\")\n", "output_sequence": "Computes kernel density estimates of the diagonal of the genuine and forged swiss bank notes. The bandwidth parameter are chosen by Silverman rule of thumb."}, {"input_sequence": "Author: Jorge Patron, Vladimir Georgescu, Song Song, Awdesch Melzer ; # clear all variables\nrm(list = ls(all = TRUE))\ngraphics.off()\n# load data\nx = read.table(\"pullover.dat\")\n# plot\nplot(x[, 4], x[, 1], xlab = \"Sales Assistants (X4)\", ylab = \"Sales (X1)\", xlim = c(70,\n120), ylim = c(80, 240), frame = TRUE, axes = FALSE)\ntitle(\"Pullovers Data\")\naxis(side = 2, seq(80, 240, 40), seq(80, 240, 40))\n", "output_sequence": "Computes a two dimensional scatterplot of assistants and sales from the pullovers data set."}, {"input_sequence": "Author: Sergey Nasekin, Dedy D. Prastyo ; import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import linear_model\ndata = pd.read_csv(\"carc.dat\", sep = \"\\s+\", header=None)\ny = data[1]\nfor i in range(1, 13):\nglobals()[\"x\" + str(i)] = data[i+1]\nx = pd.DataFrame(data=[globals()[\"x\" + str(r)] for r in range(2, 13)]).transpose()\nfor j in [3, 4]:\ng = []\nfor v in x[j]:\ntry: g.append(int(v)+1)\nexcept ValueError:\ng.append(1)\nx[j] = g\nfor u in [5, 6, 12]:\nx[u] = x[u].astype(float)\nfor u in list(range(7, 12)) + [13]:\nx[u] = x[u].astype(np.int64)\nlasso_regress = linear_model.Lars(fit_intercept = True, normalize = True, fit_path=True).fit(np.array(x), np.array(y))\nlasso_path = np.sum(np.abs(lasso_regress.coef_path_.T), axis=1)\nlasso_path /= lasso_path[-1]\nfig, ax = plt.subplots(figsize = (15, 10))\nax.plot(lasso_path, lasso_regress.coef_path_.T)\nymin, ymax = plt.ylim()\nax.set_ylim([ymin,ymax])\nax.vlines(lasso_path, ymin, ymax)\nax.hlines(lasso_regress.coef_path_.T[-1], xmax-0.01, xmax)\nfor i in [6, 11, 9, 5, 3, 8, 7]:\nplt.text(xmax + 0.01, lasso_regress.coef_path_.T[-1][i-1], str(i), fontsize=12)\nfor i in range(0, len(lasso_path)):\nplt.text(lasso_path[i]-0.005, ymax+1000, str(i), fontsize=12)\n\nplt.title(\"LASSO\", y = 1.05, fontsize=14)\nplt.show()\n", "output_sequence": "Performs a standardized regression using the Lasso methodology. The estimates become nonzero at a point that means the variables enter the model equation sequentially as the scaled shrinkage parameter increases. The Lasso technique results in variable selection. Finally, the resulting Lasso estimates are plotted."}, {"input_sequence": "Author: Sergey Nasekin, Dedy D. Prastyo ; # clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# install and load packages\nlibraries = c(\"lars\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# load data\ndata = read.table(\"carc.dat\")\ny = data[, 2]\nx = cbind(x2, x3, x10, x12)\n(lasso.regress = lars(x, y, type = \"lasso\", normalize = TRUE, intercept = TRUE, max.steps = 1000))\nsummary(lasso.regress)\n# plot\nplot.lars(lasso.regress, lwd = 3)\n", "output_sequence": "Performs a standardized regression using the Lasso methodology. The estimates become nonzero at a point that means the variables enter the model equation sequentially as the scaled shrinkage parameter increases. The Lasso technique results in variable selection. Finally, the resulting Lasso estimates are plotted."}, {"input_sequence": "Author: Zografia Anastasiadou ; # clear all variables\nrm(list = ls(all = TRUE))\ngraphics.off()\n# install and load packages\nlibraries = c(\"Iso\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# Distance matrix with prior ordering of distances (delta_ij)\n# for cities Berlin, Dresden, Hamburg, Koblemz, Muenchen, Rostock\nber = c(0, 2, 4, 12, 3)\nkob = c(12, 10, 8, 0, 9, 13)\nros = c(3, 5, 1, 13, 0)\ndist = rbind(ber, dre, ham, kob, mue, ros)\n# Determine x, the coordinate matrix of the cities\n# a,b,h,i are matrices\na = (dist^2) * (-0.5)\ni = diag(6)\nu = rep(1, 6)\nh = i - (1/6 * (u %*% t(u)))\nb = h %*% a %*% h # Determine the inner product matrix\ne = eigen(b)\ng1 = cbind(e$vectors[, 1], e$vectors[, 2])\ng2 = diag(e$values[1:2])\nx = g1 %*% (g2^0.5) # Determine the coordinate matrix\n# Determine the distances between the cities from their coordinates (d_ij)\nd12 = ((x[1, 1] - x[2, 1])^2 + (x[1, 2] - x[2, 2])^2)^0.5\n# Create distance matrix\ndd = rbind(c(0, d12, d16), c(d12, 0, d23, d26), c(d13,\nd23, 0, d34, d36), c(d14, d24, 0, d45, d46), c(d15, d25,\n# Order the distance based on order in prior order in dist\nf = cbind(c(1:15), c(d36, d12, d26, d34, d45, d24, d15, d46, d35))\n\n# Use PAV (from PAVA package)\nf <- cbind(f, pava(f[,2]))\nl = cbind(c(15, 1), c(f[15,3], f[1,3]))\n# Plot\nplot(f[,c(1,3)], xlim = c(0, 16), ylim = c(0,16), xlab = \"Dissimilarity rank\", ylab = \"Distance\", main = \"Pool-Adjacent-Violator-Algorithm\",\ncex.axis = 1.2, cex.lab = 1.2, cex.main = 1.8)\nlines(f[,c(1,3)], lty = 2, lwd = 3, col = \"blue\")\nsegments(l[1, 1], l[1, 2], l[2, 1], l[2, 2], lwd = 3, col = \"blue\")\n", "output_sequence": "Computes the pool adjacent violator algorithm (PAV)."}, {"input_sequence": "Author: Zografia Anastasiadou ; %% clear all variables and console and close windows\nclear\nclc\nclose all\n%% data\nber = [0 2 4 12 11 3 ];\nkob = [12 10 8 0 9 13];\nmue = [11 7 15 9 0 14];\nros = [3 5 1 13 14 0 ];\ndist=[ber', dre', ham', kob', mue', ros'];\n%% matrices aa, bb, hh, ii\naa = (dist .^ 2) .* (-0.5);\nii = eye(6, 6);\nhh = ii - (1/6 * (u' * u));\nbb = hh * aa * hh; % Determine the inner product matrix\n[g1, g2] = eigs(bb, 2);\nx = g1 * (g2 .^ 0.5); % Determine the coordinate matrix\n%% Determine the dissimilarities\nd12 = ((x(1, 1) - x(2, 1))^2 + (x(1, 2) - x(2, 2))^2)^0.5;\nd14 = (d14 + d15)/2;\nd15 = d14;\nd16 = (d14 + d15 + d16 + d23 + d24 + d25 + d26)/7;\nd14 = d16;\nd35 = (d35 + d36)/2;\nd36 = d35;\ndd = [[0 d12 d16];\n[d13 d23 0 d34 d36];\nff = [[1:15]',...\n[d12;d13;d14;d15;d16;d23;d24;d25;d26;d34;d35;d36;d45;d46;d56]];\nll = [[15;1],[14.4;3.17]];\n%% plot\nscatter(ff(:, 1), ff(:, 2), 'k')\ntitle('Pool-Adjacent-Violator-Algorithm')\nxlabel('Rank')\nylabel('Distance')\nxlim([0 16])\nset(gca, 'box', 'on')\nfor i=1:14\nline([ff(i, 1) ff(i + 1, 1)], [ff(i, 2) ff(i + 1, 2)],...\n'Color', 'b', 'LineWidth', 1.5, 'LineStyle', '--')\nend\nline([ll(1,1) ll(2,1)],[ll(1,2) ll(2,2)],'Color','b','LineWidth',2)\n", "output_sequence": "Computes the pool adjacent violator algorithm (PAV)."}, {"input_sequence": "Author: Lukas Borke, Awdesch Melzer, Simon Trimborn ; # clear all variables\nrm(list = ls(all = TRUE))\ngraphics.off()\n# read data\nD = as.matrix(read.table(\"export_q_kw_141.dat\"))\n# take everything but ID\nE = D[, -1]\n# Quantlet IDs\nIDM = D[, 1]\n# transpose and norm to one column-wise, then a column equals the vector representation of a Qlet\nnorm.E = apply(t(E), 2, function(v) {\nv/sqrt(sum(v * v))\n})\nnorm.E[is.na(norm.E)] = 0\nnorm.H = apply(t(H), 2, function(v) {\nnorm.H[is.na(norm.H)] = 0\npar(mfrow = c(3, 1)) # set plot for 3 columns\n# BCS\nset.seed(12345) # set pseudo random numbers\nd = dist(t(norm.H)) # Euclidean norm\nclusBCS = kmeans(t(norm.H), 4) # kmeans for 4 clusters/centers\nmdsBCS = cmdscale(d, k = 2) # mds for 2 dimensions\nplot(mdsBCS, type = \"n\", xlab = \"\", ylab = \"\", main = \"Metric MDS for BCS quantlets\")\ntext(mdsBCS[, 1], mdsBCS[, 2], IDB, col = clusBCS$cluster)\n# MVA\nd = dist(t(norm.E)) # Euclidean norm\nclusMVA = kmeans(t(norm.E), 4) # kmeans for 4 clusters/centers\nmdsMVA = cmdscale(d, k = 2) # mds for 2 dimensions\nplot(mdsMVA, type = \"n\", xlab = \"\", ylab = \"\", main = \"Metric MDS for MVA quantlets\")\ntext(mdsMVA[, 1], mdsMVA[, 2], IDM, col = clusMVA$cluster)\n# Displaying Null-Vectors in cyan color\nD_T_141_null = apply(norm.E^2, 2, sum)\ntext(mdsMVA[D_T_141_null == 0, 1], mdsMVA[D_T_141_null == 0, 2], IDM[D_T_141_null ==\n0], col = 5)\n# All Qlets\nQall = as.matrix(read.table(\"export_q_kw_All.dat\"))\nIDall = Qall[, 1]\nnorm.E = apply(t(E), 2, function(v) {\nD_global = norm.E\nd = dist(t(D_global))\nclusAll = kmeans(t(D_global), 4)\nmdsAll = cmdscale(d, k = 2)\nplot(mdsAll, type = \"n\", xlab = \"\", ylab = \"\", main = \"Metric MDS for All quantlets\")\ntext(mdsAll[, 1], mdsAll[, 2], IDall, col = clusAll$cluster)\nD_global_null = apply(D_global^2, 2, sum)\ntext(mdsAll[D_global_null == 0, 1], mdsAll[D_global_null == 0, 2], IDall[D_global_null ==\n", "output_sequence": "The document similarity of quantlets is calculated based on their keywords. For this purpose quantlets are taken from the MVA book, BCS project and the whole Quantnet. First the keywords are transformed into the vector representation, basis model is used. Finally the k-means algorithm is applied for clustering (four clusters) and the data are represented via MDS (multidimensional scaling)."}, {"input_sequence": "Author: Zografia Anastasiadou ; # clear all variables\nrm(list = ls(all = TRUE))\ngraphics.off()\n# load data\nibm = read.csv(\"ibm.csv\")\n# compute returns for IBM\ny1 = ibm[, 5]\na = 0\nwhile (i <= 120) {\ni = i + 1\na[i] = (y1[i] - y1[i - 1])/y1[i]\n}\nx1 = a[2:121]\n# compute returns for Apple\ny2 = apple[, 5]\nb = 0\nb[i] = (y2[i] - y2[i - 1])/y2[i]\nx2 = b[2:121]\n# compute returns for Bank of America Corporation\ny3 = bac[, 5]\nd = 0\nd[i] = (y3[i] - y3[i - 1])/y3[i]\nx3 = d[2:121]\n# compute returns for Forward Industries\ny4 = ford[, 5]\nf = 0\nf[i] = (y4[i] - y4[i - 1])/y4[i]\nx4 = f[2:121]\n# compute returns for Consolidated Edison\ny5 = ed[, 5]\ng = 0\ng[i] = (y5[i] - y5[i - 1])/y5[i]\nx5 = g[2:121]\n# compute returns for Morgan Stanley\ny6 = ms[, 5]\nh = 0\nh[i] = (y6[i] - y6[i - 1])/y6[i]\nx6 = h[2:121]\nix = c(1:6) # choose assets to use\nx = cbind(x1, x2, x6) # MVAportfolio of all assets\ns1 = solve(cov(x)) # inverse of empirical variance\none = rep(1, length(ix)) # vector of ones\nc2 = (s1 %*% one)/rep(t(one) %*% s1 %*% one, length(s1 %*% one)) # c2 weight\nc1 = one/rep(sum(one), length(one)) # c1 weight\nq1 = x %*% c1 # Optimal MVAportfol returns\nt = c(1:120)\nd1 = cbind(t, q1)\n# plot\npar(mfrow = c(2, 1))\npar(oma = c(0, 0, 8))\nplot(d1, type = \"l\", col = \"blue\", ylab = \"Y\", xlab = \"X\", main = \"Equally Weighted Portfolio\",\nylim = c(-0.6, 0.6), cex.lab = 1.4, cex.axis = 1.4, cex.main = 1.4)\nmtext(\"Weights\", side = 4, line = 4, at = 1.2, las = 2, font = 1)\nmtext(toString(c(sprintf(\"%.3f\", c1[1]), \"IBM\")), side = 4, line = 4, at = 0.6, las = 2)\nlas = 2)\nmtext(toString(c(sprintf(\"%.3f\", c1[3]), \"BAC\")), side = 4, line = 4, at = 0.3, las = 2)\nplot(d2, type = \"l\", col = \"blue\", ylab = \"Y\", xlab = \"X\", main = \"Optimal Weighted Portfolio\",\nmtext(toString(c(sprintf(\"%.3f\", c2[1]), \"IBM\")), side = 4, line = 4, at = 0.6, las = 2)\nlas = 2)\n", "output_sequence": "Computes the optimal portfolio weights with monthly returns of six US firms from Jan 2000 to Dec 2009. The optimal portfolio is compared with an equally weighted one."}, {"input_sequence": "Author: Zografia Anastasiadou ; # clear all variables\nrm(list = ls(all = TRUE))\ngraphics.off()\n# load data\ndata = read.table(\"bostonh.dat\")\n# transform data\nxt = data\nxt[, 1] = log(data[, 1])\nxt[, 7] = (data[, 7]^(2.5))/10000\nxt[, 8] = log(data[, 8])\nxt[, 11] = exp(0.4 * data[, 11])/1000\nxt[, 13] = sqrt(data[, 13])\nxt[, 14] = log(as.numeric(data[, 14]))\ndata = xt[, -4]\nn1 = nrow(data)\nx = (data - matrix(mean(as.matrix(data)), n1, byrow = T))/matrix(sqrt((n1 - 1) *\napply(data, 2, var)/n1), n1, byrow = T) # standardizes the data\neig = eigen((n1 - 1) * cov(x)/n1) # spectral decomposition\ne = eig$values\nx1 = as.matrix(x - matrix(mean(as.matrix(x)), nrow(x), byrow = T))\nr1 = x1 %*% v\nr = cor(cbind(r1, x))\n# correlations between variables and pc's\nr12 = r[14:26, 1:2]\nr13 = cbind(r[14:26, 1], r[14:26, 3])\nr123 = r[14:26, 1:3]\n# plot\npar(mfrow = c(2, 2))\nucircle = cbind(cos((0:360)/180 * pi), sin((0:360)/180 * pi))\nplot(ucircle, type = \"l\", lty = \"solid\", col = \"blue\", xlab = \"First PC\", ylab = \"Second PC\",\nmain = \"Boston Housing\", cex.lab = 1.2, cex.axis = 1.2, cex.main = 1.6, lwd = 2)\nabline(h = 0, v = 0)\nlabel = c(\"X1\", \"X2\", \"X10\",\n\"X14\")\ntext(r12, label)\nplot(ucircle, type = \"l\", lty = \"solid\", col = \"blue\", xlab = \"Third PC\", ylab = \"Second PC\",\ntext(r32, label)\nplot(ucircle, type = \"l\", lty = \"solid\", col = \"blue\", xlab = \"First PC\", ylab = \"Third PC\",\ntext(r13, label)\nplot(ucircle, type = \"l\", lty = \"solid\", col = \"blue\", xlab = \"X\", ylab = \"Y\", cex.lab = 1.2,\ncex.axis = 1.2, lwd = 2)\ntext(r123, label)\n", "output_sequence": "Calculates and plots the correlations of the first three PCs with the original variables for the standardized Boston housing data."}, {"input_sequence": "Author: Song Song ; # clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# install and load packages\nlibraries = c(\"KernSmooth\", \"misc3d\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# load data\nxx = read.table(\"bank2.dat\")\nd = kde3d(xx[, 4], xx[, 5], xx[, 6], n = 15)\n# plot\ncontour3d(d$d, level = c(max(d$d[10, 10, ]) * 0.02, max(d$d[10, 10, ]) * 0.5, max(d$d[10,\n10, ]) * 1.3), fill = c(FALSE, TRUE), col.mesh = c(\"green\", \"red\", \"blue\"),\nengine = \"standard\", screen = list(z = 210, x = -40, y = -295), scale = TRUE)\ntitle(\"Swiss bank notes\")\n", "output_sequence": "Gives a contour plot of the kernel density estimate of variables X4, X5 and X6 of the Swiss bank notes."}, {"input_sequence": "Author: Song Song ; import pandas as pd\nimport numpy as np\nimport scipy.stats as st\nfrom mayavi import mlab\nxx = pd.read_csv(\"bank2.dat\", sep = \"\\s+\", header=None)\nd = st.gaussian_kde(np.array(xx.iloc[:, 3:]).T)\nfor i, j in zip([\"x\", \"y\", \"z\"], range(3, 6)):\nglobals()[i+\"_min\"] = min(np.array(xx.iloc[:, j]))-1\nxi, zi = np.mgrid[x_min:x_max:30j, y_min:y_max:30j,\ncoords = np.vstack([item.ravel() for item in [xi, yi, zi]])\ndensity = d(coords).reshape(xi.shape)\nfigure = mlab.figure('DensityPlot', bgcolor = (1, 1))\nmlab.contour3d(xi, yi, density, opacity=0.5)\nmlab.title(\"Swiss bank notes\", color = (0,0,0))\nmlab.axes(extent = [x_min, x_max, y_min, z_max],\nx_axis_visibility = False, y_axis_visibility= False, z_axis_visibility = False)\nmlab.show()\n", "output_sequence": "Gives a contour plot of the kernel density estimate of variables X4, X5 and X6 of the Swiss bank notes."}, {"input_sequence": "Author: Wolfgang K. Haerdle ; # clear all variables\nrm(list = ls(all = TRUE))\ngraphics.off()\nxx = seq(-5, 5, by = 0.1)\n# Pdf of t-distribution with df=3,\nplot(xx, df = 3), type = \"l\", ylim = c(0, 0.4), ylab = \"Y\", xlab = \"X\", col = \"green\",\nlwd = 3, cex.lab = 2, cex.axis = 2)\nlines(xx, dt(xx, df = 6), type = \"l\", col = \"blue\", lwd = 3)\nlegend(x = 2, y = 0.3, legend = c(\"t3\", \"t6\", \"t30\"), pch = c(20, 20, col = c(\"green\",\n\"blue\", \"red\"), bty = \"n\")\ntitle(\"PDF of t-distribution\")\n# Cdf of t-distribution with df=3,\ndev.new()\nplot(xx, df = 3), type = \"l\", ylab = \"Y\", xlab = \"X\", col = \"green\", lwd = 3,\ncex.lab = 2, cex.axis = 2)\nlines(xx, pt(xx, df = 6), type = \"l\", col = \"blue\", lwd = 3)\nlegend(x = -5, y = 0.74, legend = c(\"t3\", \"t6\", \"t30\"), pch = c(20, 20, col = c(\"green\",\ntitle(\"CDF of t-distribution\")\n", "output_sequence": "Plots three probability density functions and three cumulative density functions of the t-distribution with different degrees of freedom (t3 stands for t-distribution with degree of freedom 3, etc.)"}, {"input_sequence": "Author: Vladimir Georgescu, Jorge Patron, Song Song, Awdesch Melzer ; # clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# load data\nx = read.table(\"pullover.dat\")\ndat = cbind(x[, 2], x[, 1])\ncv = cov(dat)\nsxy = cv[2, 1]\nbeta = sxy/sxx\nalpha = mean(dat[, 2]) - beta * mean(dat[, 1])\ny = dat[, 2]\nyhat = alpha + beta * dat[, 1]\nyquer = NULL\nsteps = dim(x)[1]\nfor (i in 1:steps) {\nyquer[i] = mean(dat[, 2])\n}\n# Generating line1\nline11 = NULL\nm = 1\nline11[m] = dat[i, 1]\nm = m + 3\nline12[m] = yquer[i]\nline1 = cbind(line11, line12)\n# Generating line2\nline21 = NULL\nline21[m] = dat[i, 1] - 0.3\nline22[m] = yquer[i]\nline2 = cbind(line21, line22)\n# Generating line3\nline31 = NULL\nline31[m] = dat[i, 1] + 0.3\nline32[m] = yquer[i]\nline3 = cbind(line31, line32)\nt = (round(min(dat[, 1])) - 5):(round(max(dat[, 1])) + 5)\nfit = alpha + beta * t\nrl = cbind(t, fit)\naux = mean(dat[, 2]) * matrix(1, length(t),\naux1 = aux[, 1]\nybar = cbind(t, aux1)\n# Chart\nplot(dat[, 1], dat[, 2], xlab = \"Price (X2)\", ylab = \"Sales (X1)\", xlim = c(88,\n102), ylim = c(162, 198))\ntitle(\"Pullover Data\")\nlines(rl[, 1], rl[, 2], lwd = 2)\nlines(ybar[, 1], ybar[, 2], lty = \"dashed\", lwd = 2)\n# redline\nredline = rbind(line1[1, ], line1[2, ], line1[4, ], line1[5, ], line1[7, ], line1[8,\n], line1[28, ], line1[29, ])\ni = 1\ns = dim(redline)[1]\nwhile (i < s) {\nlines(c(redline[i, 1], redline[i + 1, 1]), c(redline[i, 2], redline[i +\n1, 2]), col = \"red3\", lwd = 2, lty = \"dashed\")\ni = i + 2\n# Greenline\ngreenline = rbind(line2[1, ], line2[3, ], line2[4, ], line2[6, ], line2[7, ], line2[9,\n], line2[28, ], line2[30, ])\nlines(c(greenline[i, 1], greenline[i + 1, 1]), c(greenline[i, 2], greenline[i +\n1, 2]), col = \"green4\", lwd = 2)\n# blueline\nblueline = rbind(line3[1, ], line3[3, ], line3[5, ], line3[6, ], line3[7, ], line3[8,\n])\nlines(c(blueline[i, 1], blueline[i + 1, 1]), c(blueline[i, 2], blueline[i + 1,\n2]), col = \"blue3\", lwd = 2, lty = 4)\n", "output_sequence": "Plots a section of the linear regression of the sales (X1) on price (X2) for the pullovers data. Graphical representation of the relationship: total variation = explained variation + unexplained variation."}, {"input_sequence": "Author: Lukas Borke, Awdesch Melzer, Simon Trimborn ; # clear all variables\nrm(list = ls(all = TRUE))\ngraphics.off()\nD = as.matrix(read.table(\"export_q_kw_All.dat\"))\n# take everything but ID\nE = D[, -1]\nIDall = D[, 1] # Quantlet IDs\n# transpose and norm to one columnwise, then a column equals the vector representation of a Qlet\nnorm.E = apply(t(E), 2, function(v) {\nv/sqrt(sum(v * v))\n})\nnorm.E[is.na(norm.E)] = 0\n# cache global Qlet matrix as norm, needed for following transformations\nD_global = norm.E\n# read vector matrix from BCS and norm it\nD = as.matrix(read.table(\"export_q_kw_310.dat\"))\nE = D[, -1] # extract everything but ID\n# transpose the BCS vector representation of the basis model into T model\n# (term-term-correlation) one column in D_T_310 is equivalent to the vector\n# representation of a Qlet in the T model\nD_T_310 = t(D_global) %*% norm.E\n# read vector matrix from MVA and norm it\nD = as.matrix(read.table(\"export_q_kw_141.dat\"))\nIDM = D[, 1] # set ID\n# transpose the MVA vector representation of the basis model into T model\n# (term-term-correlation) one column in D_T_141 is equivalent to the vector\nD_T_141 = t(D_global) %*% norm.E\n# MDS + kmeans for BCS\nset.seed(12345) # set pseudo random numbers\nd = dist(t(D_T_310)) # Euclidean norm\nclusBCS = kmeans(d, 4) # kmeans for 4 clusters/centers\nmdsBCS = cmdscale(d, k = 2) # mds for 2 dimensions\npar(mfrow = c(1, 2)) # set plot for 2 columns\n# Plot 1\nplot(mdsBCS, type = \"n\", xlab = \"\", ylab = \"\", main = \"Metric MDS for BCS quantlets\")\ntext(mdsBCS[, 1], mdsBCS[, 2], IDB, col = clusBCS$cluster)\n# MDS + kmeans for MVA\nd = dist(t(D_T_141))\nclusMVA = kmeans(d, 4)\nmdsMVA = cmdscale(d, k = 2)\n# Plot 2\nplot(mdsMVA, type = \"n\", xlab = \"\", ylab = \"\", main = \"Metric MDS for MVA quantlets\")\ntext(mdsMVA[, 1], mdsMVA[, 2], IDM, col = clusMVA$cluster)\n", "output_sequence": "The document similarity of quantlets is calculated based on their keywords. For this purpose quantlets are taken from the MVA book and BCS project. First the keywords are transformed into the vector representation. Then the scalar product is applied calculating so the similarity measure. The advanced term-term correlation model additionally uses the term-term correlation matrix between the terms of all documents. Finally the k-means algorithm with the Euclidean norm is applied for clustering (four clusters) and the data are represented via MDS (multidimensional scaling) showing metric MDS for BCS quantlets and metric MDS for MVA quantlets."}, {"input_sequence": "Author: Zografia Anastasiadou, Franziska Schulz ; # clear all variables\nrm(list = ls(all = TRUE))\ngraphics.off()\n# install and load packages\nlibraries = c(\"depth\", \"localdepth\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# parameter settings\nset.seed(101)\nn = 100 # number of observations\nr = matrix(c(3, 1), 2)\nf = 4/10\n# Generate data\nd = matrix(0, n, p) # initialize matrix\nd[, 1] = c(r[, 1] + runif((1 - f) * n), r[, 2] + runif(f * n)) # uniform numbers shifted by r\nd[, 2] = 6.28 * runif(n) # the second column has 100 random numbers from uniform distribution\nc = rbind(matrix(0, (1 - f) * n, 2), matrix(s, f * n, 2, byrow = TRUE)) # first 1-f rows equals to zero, last f rows equal to s\nx = matrix(0, n, p) # initialize matrix\nx[, 1] = c[, 1] + d[, 1] * cos(d[, 2]) # rotate data and shift by c\nx[, 2] = c[, 2] + d[, 1] * sin(d[, 2])\n# Compute the two-dimensional median\nm = med(x, method = \"Liu\")\nmm = m$median\n# compute the size of the simplices formed from x and return the corresponding\n# quantiles needed as input for the function localdepth\ntau = quantile.localdepth(x, probs = 0.1, size = TRUE, use = \"volume\")\n# compute the depth and the local depth for the set of points of x and the given threshold tau\nld = localdepth(x, tau = tau$quantile, method = \"simplicial\", use = \"volume\")\ndep = ld$depth # vector of the depth values\n# plot the result: 1 is a circle, 2 is a triangle, 3 is a cross, 4 is a X-symbol,\n# 5 is a rhombus, 6 is an inverted triangle, 15 is a filled rectangle, 16 is a\n# filled circle, 17 is a filled triangle, 18 is a filled rectangle\ncol = round(10 * dep/max(dep))\ncol = 4 * (col == 1) + 2 * (col == 2) + 1 * (col == 3) + 5 * (col == 4) + 6 * (col ==\n(col == 10)\nplot(x, pch = col, = \"blue\", xlab = \"X\", ylab = \"Y\", main = \"Simplicial depth\",\ncex.lab = 1.6, cex.axis = 1.6, cex.main = 1.8)\npoints(mm[1], mm[2], col = \"red\", pch = 8, cex = 3) # median is the big red star\n", "output_sequence": "Calculates and plots the simplicial depth for a two-dimensional, 10 point distribution according to depth."}, {"input_sequence": "Author: Zografia Anastasiadou, Awdesch Melzer ; # clear all variables\nrm(list = ls(all = TRUE))\ngraphics.off()\n# Load data\nx = read.table(\"uscomp2.dat\")\n# Without IBM and General Electric\nx = rbind(x[1:37, ], x[39, ], x[41:79, ])\ncolnames(x) = c(\"Company\", \"A\", \"MV\", \"P\", \"CF\", \"E\", \"Sector\")\nattach(x)\nSector = as.character(Sector)\nSector[1:2] = \"H\"\nx = x[, 2:7]\nn = nrow(x)\nx = (x - matrix(apply(x, 2, mean), n, 6, byrow = T))/matrix(sqrt((n - 1) * apply(x,\n2, var)/n), n, 6, byrow = T) # standardizes the data\neig = eigen((n - 1) * cov(x)/n) # spectral decomposition\ne = eig$values\nx = as.matrix(x) %*% v # principal components\nlayout(matrix(c(2, 1), 2, 1, byrow = T), c(2, 1), c(2, 1), T)\n# plot\nplot(e, xlab = \"Index\", ylab = \"Lambda\", main = \"Eigenvalues of S\")\nplot(-x[, 1], x[, 2], xlim = c(-2, 8), ylim = c(-8, 8), type = \"n\", xlab = \"PC 1\",\nylab = \"PC 2\", main = \"First vs. Second PC\")\ntext(-x[, 1], x[, 2], Sector)\n", "output_sequence": "Performs a PCA for the standardized US company data without IBM and General Motors. It shows the first two principal components and screeplot of the eigenvalues."}, {"input_sequence": "Author: Awdesch Melzer ; ppsib = function(px){\nn = NROW(px)\ncx = px - matrix(apply(px,2,mean),nrow=n,ncol=m,byrow=T)\nmu = rbind(apply(cx,2,mean),apply(cx^2,2,mean),apply(cx^3,2,mean),apply(cx^4,2,mean))\nk3 = mu[3]\nk4 = mu[4] - (3*(mu[2]^2))\nind = ((k3^2)+((k4^2)/4))/12\nreturn(ind)\n}\n### Example 1\n# set.seed(100) # initialize random generator\n# x = matrix(rnorm(100,0,1)) # generate a dataset with mean(x)=0 and var(x)=1\n# ppsib(x) # compute the index with Scott's rule\n### Example 2\n# plotindex1 = function(x) {\n# n = 50\n# phi = seq(0, by = 2 * pi/n, length = n)\n# ind = matrix(0, n, 1)\n# i = 0\n# while (i < n) {\n# i = i + 1\n# xp = x %*% c(cos(phi[i]), sin(phi[i]))\n# ind[i] = ppsib(xp)\n# matplot(phi, ind, type = \"l\", xlab = \"X\", ylab = \"Y\")\n# }\n# }\n# sphere = function(x) {\n# x = x - matrix(apply(x, 2, mean), nrow = NROW(x), ncol = NCOL(x), byrow = T)\n# s = svd(var(x))\n# s = s$u/matrix(sqrt(s$d), nrow(s$u), byrow = T)\n# x = as.matrix(x) %*% as.matrix(s)\n# return(x)\n# x = read.table(\"bank2.dat\")\n# x = sphere(x[, c(4, 6)])\n# plotindex1(x)\n# title(paste(\"Index function \"))\n", "output_sequence": "Computes the Sibson Jones index which considers the deviations from the normal density for univariate data."}, {"input_sequence": "Author: Zografia Anastasiadou ; %% clear all variables and console and close windows\nclear\nclc\nclose all\n%% the reported preference orderings\nx = (1:6)';\n% the estimated preference orderings according to the additive model (16.1)\n% and the metric solution (Table 16.6) in book\ny = [0.84, 3.16, 3.34, 5.66, 5.16]';\nz = [x, y];\n%% use PAV algorithm\ngp = pava(y);\ngp = [x, gp];\n%% plot\nfigure(1)\nhold on\nplot(gp, z, 'w')\nxlabel('Revealed rankings', 'FontSize', 12)\nbox on\nline(gp(:, 1), gp(:, 2), 'LineWidth', 2)\nscatter(x, y, 'MarkerEdgeColor', 'red', 'MarkerFaceColor', 'red')%'filled', true)\nxlim([0.5, 6.5])\nlabels = {'car1', 'car6'};\ntext(x, y - 0.1, labels, 'FontSize',12)\nhold off\n", "output_sequence": "Performs a monotone transformation to the estimated stimulus utilities of the car example by applying the pool-adjacent-violators (PAV) algorithm."}, {"input_sequence": "Author: Zografia Anastasiadou ; from sklearn.isotonic import IsotonicRegression\nimport matplotlib.pyplot as plt\n\n# the reported preference orderings\nx = list(range(1, 7))\n# the estimated preference orderings according to the additive model (16.1) and\n# the metric solution (Table 16.6) in MVA\ny = [0.84, 3.16, 3.34, 5.66, 5.16]\ngp = IsotonicRegression()\ny_gp = gp.fit_transform(x, y)\nfig, ax = plt.subplots(figsize = (7, 7))\nax.plot(x, y_gp, c = \"k\")\nfor i in range(0, len(y)):\nax.text(x[i]-0.05, y_gp[i]+0.1, \"car\" + str(i+1), fontsize = 14)\nplt.xlabel(\"revealed rankings\", fontsize = 14)\n", "output_sequence": "Performs a monotone transformation to the estimated stimulus utilities of the car example by applying the pool-adjacent-violators (PAV) algorithm."}, {"input_sequence": "Author: Zografia Anastasiadou ; # clear all variables\nrm(list = ls(all = TRUE))\ngraphics.off()\n# install and load packages\nlibraries = c(\"isotone\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# the reported preference orderings\nx = c(1:6)\n# the estimated preference orderings according to the additive model (16.1) and the metric solution (Table 16.6) in MVA\ny = c(0.84, 2.84, 3.16, 3.34, 5.66, 5.16)\nz = cbind(x, y)\n# PAV algorithm\ngp = gpava(x, y)\na = gp$z # the reported preference orderings\nb = gp$x # the estimated preference orderings after PAV algorithm\ngp = cbind(a, b)\n# Plot of car rankings\nplot(gp, z, type = \"n\", xlab = \"revealed rankings\", ylab = \"estimated rankings\",\nmain = \"Car rankings\", cex.lab = 1.4, cex.axis = 1.4, cex.main = 1.6)\nlines(a, b, lwd = 2)\npoints(x, y, pch = 19, col = \"red\")\ntext(x[1], y[1] - 0.1, \"car1\", font = 2)\n", "output_sequence": "Performs a monotone transformation to the estimated stimulus utilities of the car example by applying the pool-adjacent-violators (PAV) algorithm."}, {"input_sequence": "Author: Zografia Anastasiadou ; # clear all variables\nrm(list = ls(all = TRUE))\ngraphics.off()\n# load data\ndata = read.table(\"bostonh.dat\")\n# transform data\nxt = data\nxt[, 1] = log(data[, 1])\nxt[, 7] = (data[, 7]^(2.5))/10000\nxt[, 8] = log(data[, 8])\nxt[, 11] = exp(0.4 * data[, 11])/1000\nxt[, 13] = sqrt(data[, 13])\ndata = xt[, -4]\nda = scale(data) # standardize variables\nd = dist(da, \"euclidean\", p = 2) # euclidean distance matrix\nw = hclust(d, method = \"ward.D\") # cluster analysis with ward algorithm\ntree = cutree(w, 2) # define the clusters, tree=1 if cluster=1\n# the following two lines under comments are for price of Boston houses\n# tree=(xt[,14]>median(xt[,14]))+1\n# da=da[,1:12]\nt1 = subset(da, tree == 1)\nm1 = colMeans(t1) # mean of first cluster\ns = ((nrow(t1) - 1) * cov(t1) + (nrow(t2) - 1) * cov(t2))/(nrow(xt) - 2) # common variance matrix\nalpha = solve(s) %*% (m1 - m2) # alpha for the discrimination rule\n# APER for clusters of Boston houses\nmis1 = sum((t1 - m) %*% alpha < 0) # misclassified 1\naper = (mis1 + mis2)/nrow(xt) # APER (apparent error rate)\nalph = (da - matrix(m, nrow(da), byrow = T)) %*% alpha\nset.seed(1)\n# discrimination scores\np = cbind(alph, tree + 0.05 * rnorm(NROW(tree)))\ntree[tree == 1] = 16\ntr = tree\ntr[tr == 16] = \"red\"\n# plot of discrimination scores\nplot(p[, 1], p[, 2], pch = tree, col = tr, xaxt = \"n\", yaxt = \"n\", xlab = \"\", ylab = \"\",\nbty = \"n\")\nabline(v = 0, lwd = 3)\ntitle(paste(\"Discrimination scores\"))\n", "output_sequence": "Demonstrates maximum likelihood discrimination rule (ML rule) for the Boston housing data."}, {"input_sequence": "Author: Zografia Anastasiadou, Awdesch Melzer ; # clear all variables\nrm(list = ls(all = TRUE))\ngraphics.off()\n# install and load packages\nlibraries = c(\"gRapHD\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# load data\ndata1 = read.table(\"XFGvolsurf01.dat\") # 1 month maturity data\nS1 = cov(data1) * 1e+05 # Sample covariance*10^5 of 1 month maturity data\nS = rbind(S1, S2, S3) # Combine data by rows\nA = array(t(S), c(ncol(S1), 3)) # Create a 3d array\nn = c(253, 253) # Number of trading days in 1999\nN = n - 1\npreB = 1e-10 # Precision for maximum deviation of an element of matrix B\nmaxit = 15 # Maximal number of iteration\npreQ = 1e-10 # Precision for maximum deviation of an element of matrix Q\nmaxiter = 10 # Maximal number of iteration\np = dim(A)[1] # Covariances are pxp\nB = diag(p) # Initial value for B, the unit matrix\nf = 0\nrepeat {\nf = f + 1\nBold = B\nwhile (j <= p) {\nm = 1\nwhile (m < j) {\nBmj = cbind(B[, m], B[, j])\n(T = array(0, c(2, 2, k)))\nT[, , 1] = t(Bmj) %*% A[, , 1] %*% Bmj\nQ = cbind(c(1, 0), c(0, 1)) # orthogonal matrix to start with\ng = 0\ng = g + 1\nDelta1 = array(Q, c(2, 2, k)) * T * (array(Q, c(2, 2, k)))\ndiag(Delta1[, , 1])\nDelta = array(c(diag(Delta1[, , 1])), c(2, 1, 3))\na = t(Delta[, , 1])\nabc = matrix(cbind(a, b, c), 2, 3)\nabcd = t(abc)\nd = N * (abcd[, 1] - abcd[, 2])/(abcd[, 1] * abcd[, 2])\nTsum1 = array(0, c(2, 2, k))\nTsum1[, , 1] = d[1] * T[, , 1]\nf = sum(Tsum1[, 1, ][1, ])\nTsum = matrix(rbind(f, g, h, y), 2, 2)\ne = eigen(Tsum)\nQ = e$vectors # find eigenvectors of Tsum\nmaxim = max(abs(Q - Qold))\nif ((maxim < preQ) | (g > maxiter))\nbreak\n}\nJ = diag(p)\nJ[m, m] = Q[1, 1]\nB = B %*% J\nprint(m)\n}\nj = j + 1\n}\nmaximum = max(abs(B - Bold))\nif ((maximum < preB) | (f > maxit))\nbreak\n}\nlambda1 = array(t(B), c(p, p, k)) * A * (array(B, c(p, p, k)))\nlambda = array(c(diag(lambda1[, , 1])), c(p, 1, 3))\nlambda[, , 1] = c(diag(lambda1[, , 1]))\na1b1c1 = matrix(cbind(t(lambda[, , 1]), t(lambda[, , 2]), t(lambda[, , 3])), p, k)\n# Sort eigenvectors according to size of its corresponding eigenvalues\nu = rbind(t(a1b1c1), B)\nus = t(u)\nus = us[order(us[, 1]), ]\nuss = us[p:1, ]\nB = t(uss[, (k + 1):(k + p)])\nBB = t(B[, 1:k])\n# plot\nplot(BB[1, ], type = \"l\", lwd = 4, ylim = c(-1, 1), xlab = \"moneyness\", ylab = \"loading\",\nmain = \"PCP for CPCA, 3 eigenvectors\")\nlines(BB[2, ], type = \"l\", lwd = 3)\n# estimated population covariances\nV = array(0, c(p, p, k))\nV[, , 1] = uss[, 1] * diag(p)\npsi = array(0, c(p, p, k))\npsi[, , 1] = B %*% V[, , 1] %*% t(B)\n# Test statistic\nde = c(det(psi[, , 1]), det(psi[, , 2]), det(psi[, , 3]))\ntest = 2 * log(t(n - 1) %*% (de/det))\n# P-value\ndf = 1/2 * (k - 1) * p * (p - 1)\nt = 1 - pchisq(test, df)\n", "output_sequence": "Estimates a common principal components model for the implied volatility data and computes a likelihood ratio test."}, {"input_sequence": "Author: Vladimir Georgescu, Song Song, Jorge Patron ; # clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# load data\nx = read.table(\"carc.txt\")\n# parameter settings\nk = 0\nus = NULL\neurope = NULL\nM = x[, 2]\nfor (i in 1:dim(x)[1]) {\nif (x[i, 13] == 1) {\nk = k + 1\nus[k] = x[i, 2]\n} else if (x[i, 13] == 2) {\nl = l + 1\njapan[l] = x[i, 2]\n} else if (x[i, 13] == 3) {\nm = m + 1\neurope[m] = x[i, 2]\n}\n}\nm1 = mean(us)\nm2 = mean(japan)\nm3 = mean(europe)\n# plot\nboxplot(us, japan, europe, axes = FALSE, frame = TRUE)\naxis(side = 1, at = seq(1, 3), label = c(\"US\", \"JAPAN\", \"EU\"))\ntitle(\"Car Data\")\nlines(c(0.6, 1.4), c(m1, m1), lty = \"dotted\", lwd = 1.2)\n", "output_sequence": "Computes the five-number summary and boxplots for the mileage (X14 variable) of US, Japanese and European cars."}, {"input_sequence": "Author: Vladimir Georgescu, Song Song, Jorge Patron ; import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = pd.read_csv('carc.txt', sep=\"\\t\", header=None)\n# parameter settings\nk = 0\nM = x[1]\nus = x[x[12] == 1][1]\nm1 = us.mean()\nfig, ax = plt.subplots(figsize = (10, 10))\nax.boxplot([us, japan, europe], labels = [\"US\", \"JAPAN\", \"EU\"],\nmeanline = True, showmeans = True, patch_artist=True,\nboxprops=dict(facecolor = \"lightgrey\"), widths = 0.65,\nmedianprops=dict(color=\"black\", linewidth = 2.5),\nplt.title(\"Car Data\", fontsize = 20)\nplt.show()\npd.DataFrame(data = {\"value\": np.quantile(x[1], [0.025, 0.5, 0.975])},\n", "output_sequence": "Computes the five-number summary and boxplots for the mileage (X14 variable) of US, Japanese and European cars."}, {"input_sequence": "Author: Mengmeng Guo ; # Clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# load data\ndata = read.table(\"bostonh.dat\")\n# First we use the ANOVA model to do the regression\nx1 = data[, 1]\nfit = lm(x14 ~ x4 + x5 + x6 + x8 + x9 + x10 + x11 + x12 + x13)\nsummary(fit)\n# We replace x9 by x15, and also transform x4.\nn = length(x4)\nfor (i in 1:n) {\nif (x4[i] == 0)\n}\ny = 0\nif (x9[i] >= median(x9))\ny[i] = 1 else y[i] = -1\nx15 = y\nfit2 = lm(x14 ~ x4 + x5 + x6 + x8 + x10 + x11 + x12 + x13 + x15)\nsummary(fit2)\n# This is used for testing the interaction of x4 and x12, and also x4 and x15.\nfit3 = lm(x14 ~ x4 + x5 + x6 + x8 + x10 + x11 + x12 + x13 + x15 + x4 * x12 + x4 * x15)\nsummary(fit3)\n", "output_sequence": "Computes the ANCOVA model with Boston housing data. We add binary variable to the ANCOVA model, and try to check the effect of the new factors on the dependent variable."}, {"input_sequence": "Author: Zografia Anastasiadou ; # clear all variables\nrm(list = ls(all = TRUE))\ngraphics.off()\n# reads the bank data\nx = read.table(\"bank2.dat\")\nxg = x[1:100, ] # Group first 100 observations\nmg = colMeans(xg) # Determine the mean for the seperate groups and overall sample\nmf = colMeans(xf)\nm = (mg + mf)/2\nw = 100 * (cov(xg) + cov(xf)) # matrix w for within sum-of-squares\nd = mg - mf # Difference in means\na = solve(w) %*% d # Determine the factors for linear combinations\nyg = as.matrix(xg - matrix(m, nrow = 100, ncol = 6, byrow = T)) %*% a # Discriminant rule for genuine notes\nxgtest = yg\nsg = sum(xgtest < 0) # Number of misclassified genuine notes\nsf = sum(xftest > 0)\nfg = density(yg) # density of projection of genuine notes\n# plot\nplot(ff, lwd = 3, col = \"red\", xlab = \"\", ylab = \"Densities of Projections\", main = \"Densities of Projections of Swiss bank notes\",\nxlim = c(-0.2, 0.2), cex.lab = 1.2, cex.axis = 1.2, cex.main = 1.8)\nlines(fg, lwd = 3, col = \"blue\", lty = 2)\ntext(mean(yf), 3.72, \"Forged\", col = \"red\")\n", "output_sequence": "Performs a Fisher discrimination analysis of the Swiss bank notes, computes the misclassification rates for the whole dataset and displays nonparametric density estimates of the projected data."}, {"input_sequence": "Author: Zografia Anastasiadou ; import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom KDEpy import FFTKDE\n\nx = pd.read_csv(\"bank2.dat\", sep = \"\\s+\", header=None)\nxg = x[:100]\nmg = xg.mean(axis = 0)\nm = (mf + mg)/2\nw = 100 * (xg.cov() + xf.cov())\nd = mg - mf\na = np.linalg.inv(w) @ d\nyg = (xg - np.array([m]*100)) @ a\nxgtest = yg\nsg = sum(xgtest < 0) # Number of misclassified genuine notes\nsf = sum(xftest > 0)\nfg_x, fg_y = FFTKDE(bw=\"silverman\", kernel='gaussian').fit(np.array(yg)).evaluate()\n\nfig, ax = plt.subplots()\nax.plot(fg_x, fg_y, linestyle = \"dashed\")\nax.text(yf.mean()-0.03, 3.72, \"Forged\", color = \"r\")\nplt.title(\"Densities of Projections of Swiss bank notes\")\nplt.ylabel(\"Densities of Projections\")\nplt.show()\n", "output_sequence": "Performs a Fisher discrimination analysis of the Swiss bank notes, computes the misclassification rates for the whole dataset and displays nonparametric density estimates of the projected data."}, {"input_sequence": "Author: Zografia Anastasiadou ; % ---------------------------------------------------------------------\n% Book: MVA\n% ---------------------------------------------------------------------\n% Quantlet: MVAdisfbank\n% Description: MVAdisfbank performs a Fisher discrimination analysis\n% of the Swiss bank notes (bank2.dat), computes the\n% miss-classification rates for the whole dataset and\n% displays nonparametric density estimates of the\n% projected data.\n% Corresponds to example 13.6 in MVA.\n% Usage: -\n% Inputs: None\n% Output: Fisher discrimination analysis of the Swiss bank notes\n% (bank2.dat), miss-classification rates for the whole\n% dataset and plot of the nonparametric density\n% estimates of the projected data.\n% Example: -\n% Author: Wolfgang Haerdle, Vladimir Georgescu, Jorge Patron,\n% Song\n%% clear all variables and console and close windows\nclear\nclc\nclose all\n%% load data\nx = load('bank2.dat');\nxg = x(1:100,:); % Group first 100 observations\n% Determine the mean for the seperate groups and overall sample\nmg = mean(xg);\nmf = mean(xf);\nm = (mf+mg)/2;\nw = 100*(cov(xg)+cov(xf));\nd = mg-mf; % Difference in means\na = inv(w)*d'; % Determine the factors for linear combinations\nyg = (xg-repmat(m,100,1))*a; %Discriminant rule\n% Number of misclassified genuine notes\nsumg = 0;\nfor i=1:length(yg)\nif yg(i)<0\nsumg = sumg+1;\nend\nend\ndisp('Number of missclassified genuine notes');\ndisp(sumg)\n% Number of misclassified forged notes\nsumf = 0;\nfor i=1:length(yf)\nif yf(i)>0\nsumf = sumf+1;\ndisp('Number of missclassified forged notes');\ndisp(sumf)\n% Show densities of projections of genuine and counterfeit bank %notes by Fisher\u2019s discrimination rule.\n[f1,ygi1] = ksdensity(yg);\nhold on\ntitle('Swiss Bank Notes');\nylabel('Densities of Projections');\nxlim([-0.2 0.2]);\nplot(ygi1,f1,'color','b','LineWidth',2,'LineStyle','--');\ntext(-0.14,3.5,'Forged','Color','r');\nbox on\nhold off\n", "output_sequence": "Performs a Fisher discrimination analysis of the Swiss bank notes, computes the misclassification rates for the whole dataset and displays nonparametric density estimates of the projected data."}, {"input_sequence": "Author: Song Song ; # clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# install and load packages\nlibraries = c(\"KernSmooth\", \"graphics\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# load data\nxx = read.table(\"bank2.dat\")\n# Compute a kernel density estimates\ndj = bkde2D(xx[, 4:5], bandwidth = 1.06 * c(sd(xx[, 4]), sd(xx[, 5])) * 200^(-1/5))\nd1 = bkde(xx[, 4], gridsize = 51)\ndp = (d1$y) %*% t(d2$y)\n# plot\npersp(d1$x, d2$x, dp, box = FALSE, main = \"Joint estimate\")\n", "output_sequence": "Gives plots of the product of univariate and joint kernel density estimates of variables X4 and X5 of the Swiss bank notes."}, {"input_sequence": "Author: Zografia Anastasiadou, Awdesch Melzer ; # clear all variables\nrm(list = ls(all = TRUE))\ngraphics.off()\n# load data\nx = read.table(\"food.dat\")\np = ncol(x)\nx = x[, 2:p]\nx1 = sqrt((n - 1) * apply(x, 2, var)/n)\nx2 = x - matrix(apply(as.matrix(x), 2, mean), nrow = n, ncol = p - 1, byrow = T)\nx = as.matrix(x2/matrix(x1, nrow = n, ncol = p - 1, byrow = T)) # standardizes the data matrix\n# compute eigenvalues\ne = eigen(x %*% t(x)/n)\ne1 = e$values\ne2 = e$vectors\na = e2[, 1:2]\nw = -a * sqrt(matrix(e1[1:2], nrow(a), byrow = TRUE))\n# Plot 1: the representation of the individuals\ndev.new()\nplot(w, type = \"n\", xlab = \"First Factor - Families\", ylab = \"Second Factor - Families\",\nmain = \"French Food data\", cex.lab = 1.2, cex.axis = 1.2, cex.main = 1.8, lwd = 2)\ntext(w, c(\"MA2\", \"EM2\", \"MA3\", \"EM4\", \"MA5\",\n\"CA5\"), cex = 1.2)\nabline(h = 0, v = 0)\nsegments(w[1, 1], w[1, 2], w[2, 1], w[2, 2], lty = 3, lwd = 2)\ng = eigen(t(x) %*% x/n)\ng1 = g$values\ng2 = g$vectors\nb = g2[, 1:2]\nz = b * sqrt(matrix(g1[1:2], nrow(b), byrow = TRUE))\n# Plot 2: the representation of the variables\nucircle = cbind(cos((0:360)/180 * pi), sin((0:360)/180 * pi))\nplot(ucircle, type = \"l\", lty = \"solid\", col = \"blue\", xlim = c(-1.05, 1.05), ylim = c(-1.05,\n1.05), xlab = \"First Factor - Goods\", ylab = \"Second Factor - Goods\", main = \"French Food data\",\ncex.lab = 1.2, cex.axis = 1.2, cex.main = 1.8, lwd = 2)\nlabel = c(\"bread\", \"vegetables\", \"fruits\", \"meat\", \"poultry\", \"milk\", \"wine\")\ntext(z, label)\n", "output_sequence": "Performs a PCA for the standardized French food data and shows the first two principal components for the individuals and the variables. The normalization corresponds to that of Lebart/Morineau/Fenelon."}, {"input_sequence": "Author: Zografia Anastasiadou, Awdesch Melzer ; import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = pd.read_csv(\"food.dat\", sep = \"\\s+\", header=None)\nx = x.iloc[:, 1:]\nx1 = np.sqrt((len(x) - 1) * (x.std()**2)/len(x))\nx2 = x - np.tile(np.array(x.mean()), (len(x), 1))\nx = x2/x1\ne1, e2 = np.linalg.eig(x @ x.T/len(x))\na = e2[:, :2]\nw = a * np.sqrt(np.tile(e1[:2], (len(a), 1)))\nfood = [\"MA2\", \"EM2\", \"MA3\", \"EM4\", \"MA5\",\n\"EM5\", \"CA5\"]\n# Plot 1\nfig, ax = plt.subplots(figsize = (7, 7))\nax.scatter(w[:, 0], -w[:, 1], c = \"w\")\nfor i in range(len(food)):\nax.text(w[i, 0], -w[i, 1], food[i], fontsize = 12)\nax.vlines(0, -1, 1.2)\nj = 0\nfor i in range(1, 5):\nax.plot([w[j, 0], w[i, 0]], [-w[j, 1], -w[i, 1]], linestyle = \"dashed\", c = \"black\")\nj = np.abs(j-1)\nax.plot([w[3, 0], w[4, 0]], [-w[3, 1], -w[4, 1]], linestyle = \"dashed\", c = \"black\")\nax.set_xlim(-1.5, 1.1)\nax.set_xlabel(\"First Factor - Families\")\nplt.title(\"French Food data\", fontsize = 14)\nplt.show()\ng1, g2 = np.linalg.eig(x.T @ x/len(x))\nb = g2[:, :2]\nz = b * np.sqrt(np.tile(g1[:2], (b.shape[0], 1)))\n# Plot 2\nucircle = np.array([np.cos((np.arange(0, 361)/180)*np.pi),\nax.plot(ucircle[0], ucircle[1])\nax.scatter(-z[:, 0], z[:, 1], c = \"w\")\nlabel = [\"bread\", \"vegetables\", \"fruits\", \"meat\", \"poultry\", \"milk\", \"wine\"]\nfor i in range(len(label)):\nax.text(-z[i, 0], z[i, 1], label[i], fontsize = 12)\nax.axvline(0, c = \"k\")\nax.set_xlabel(\"First Factor - Goods\")\n", "output_sequence": "Performs a PCA for the standardized French food data and shows the first two principal components for the individuals and the variables. The normalization corresponds to that of Lebart/Morineau/Fenelon."}, {"input_sequence": "Author: ['Wolfgang Karl H\u00e4rdle', 'Ying-Yu Chen'] ; from matplotlib import animation\nfrom numpy import append, cos, linspace, pi, sin, zeros\nimport matplotlib.pyplot as plt\n# Big mouth bird parameters, last one is the eye\nparameters = [-40 + 30j, 20 + 30j, 30 + 16j, 40 + 30j, -30 + 20j]\ndef fourier(t, C):\nf = zeros(t.shape)\nfor k in range(len(C)):\nf += C.real[k] * cos(k * t) + C.imag[k] * sin(k * t)\nreturn f\ndef bird(t, p):\nnpar = 6\nCx = zeros((npar,), dtype='complex')\nCx[1] = p[0].real * 1j\nCy[1] = p[3].imag + p[0].imag * 1j\nCx[2] = p[1].real * 1j\nx = append(fourier(t, Cy), [p[4].imag])\nreturn x, y\ndef init_plot():\n# draw the body of the bird\n# create trunk\nx, y = bird(linspace(2 * pi + 0.9 * pi, 0.4 + 3.3 * pi, 1000), parameters)\nfor ii in range(len(y) - 1):\ny[ii] -= sin(((x[ii] - x[0]) * pi / len(y))) * sin(float(0)) * parameters[4].real\ntrunk.set_data(x, y)\nreturn trunk,\ndef move_trunk(i):\n# move trunk to new position (but don't move eye stored at end or array)\ny[ii] -= sin(((x[ii] - x[0]) * pi / len(y))) * sin(float(i)) * parameters[4].real\nfig, ax = plt.subplots()\n# initial the bird body\nx, y = bird(t=linspace(0.4 + 1.3 * pi, 2 * pi + 0.9 * pi, 1000), p=parameters)\nplt.plot(x, y, 'b.')\nplt.xlim([-75, 90])\nplt.axis('off')\ntrunk, = ax.plot([], [], 'b.') # initialize trunk\nani = animation.FuncAnimation(fig=fig,\nfunc=move_trunk,\n", "output_sequence": "Use complex numbers as parameters to draw a bird with big mouth (or called Toucan)."}, {"input_sequence": "Author: Youcef Tahari ; \"\"\"\nCreated on Thu Sep 22 16:15:46 2022\n@author: Youcef Tahari\n[Mandatory]\nHW2: Try web scraping with Taiwan\u2019s bike dataset\nDescription: Please use Taiwan\u2019s bike dataset to draw scatter plot. You can decide which region, which kind of bike (the common one in Taiwan is Ubike) and the objective of the analysis.\nI mapped the u-bikes avaiblable in Zhongshan district - Taipei\n\"\"\"\nimport pandas as pd\nimport requests\nimport json\nimport folium\n# download the ubike data of Hsinchu from https://data.gov.tw/dataset/67781\nresponse = requests.get(\"http://odws.hccg.gov.tw/001/Upload/25/OpenData/9059/59/67e94b18-b8ba-4c6a-9a2a-e66638a31895.json\")\ncontent = response.content\njson_tree = json.loads(content)\n# data precess\ndata = pd.DataFrame(json_tree)\ndata = data.rename(columns={\"\u7ad9\u9ede\u540d\u7a31\": \"Site Name\",\n\"\u7d93\u5ea6\": \"Longitude\",\n\"\u7ad9\u9ede\u4f4d\u7f6e\": \"Site Address\"})\n# data : 63 rows \u00d7 5 columns\ndata\n# Set the ceter of our map : NYCU managemant building\n# hsinchu_coords = [24.8138, 120.9675]\n# NYCU Location: [24.787178345630263, 120.99748615357079]\nLocation_coords = [24.7871, 120.9974]\n#Create the map\nmy_map = folium.Map(location = Location_coords, zoom_start = 13)\nfor i in range(0, 63):\nfolium.Marker([data['Latitude'][i], data['Longitude'][i]],\npopup = \"Site name: \" + data['Site Name'][i]). add_to(my_map)\n# Taipei Youbike\nresponse = requests.get(\"https://tcgbusfs.blob.core.windows.net/dotapp/youbike/v2/youbike_immediate.json\")\n#pprint.pprint(json_tree[1])\n# sno\uff1a\u7ad9\u9ede\u4ee3\u865f\u3001 sna\uff1a\u5834\u7ad9\u540d\u7a31(\u4e2d\u6587)\u3001 tot\uff1a\u5834\u7ad9\u7e3d\u505c\u8eca\u683c\u3001 sbi\uff1a\u5834\u7ad9\u76ee\u524d\u8eca\u8f1b\u6578\u91cf\u3001 sarea\uff1a\u5834\u7ad9\u5340\u57df(\u4e2d\u6587)\u3001 mday\uff1a\u8cc7\u6599\u66f4\u65b0\u6642\u9593\u3001\n# lat\uff1a\u7def\u5ea6\u3001 lng\uff1a\u7d93\u5ea6\u3001\n# ar\uff1a\u5730(\u4e2d\u6587)\u3001 sareaen\uff1a\u5834\u7ad9\u5340\u57df(\u82f1\u6587)\u3001 aren\uff1a\u5730\u5740(\u82f1\u6587)\u3001 bemp\uff1a\u7a7a\u4f4d\u6578\u91cf\u3001 act\uff1a\u5168\u7ad9\u7981\u7528\u72c0\u614b\ndata = pd.DataFrame(json_tree, columns = ['sna', 'lng', 'tot','sbi','bemp','act'])\ndata = data.rename(columns={\"sna\": \"SiteName\", \"lng\": \"Longitude\", \"lat\": \"Latitude\", \"sbi\": \"Remaining\"})\n# data : 1168 rows \u00d7 3 columns\n# Define coordinates of where we want to center our map\n# \u53f0\u5927\u7ba1\u9662 = [25.014138089260964, 121.5379950959105]\n#Zhongshan district [25.0526981, 121.5203977 ]\nmy_map = folium.Map(location = Location_coords, zoom_start = 17)\nfor i in range(0, 1168):\npopup = \"Site name: \" + data['SiteName'][i]). add_to(my_map)\n# Ramaining bicycles around me\nmy_map = folium.Map(location = Location_coords, zoom_start = 16.4, tiles='Stamen Toner')\n# data = data[(data.lat > 25.01) & (data.lat < 25.02) & (data.lng > 121.53) & (data.lng < 121.54) ]\ndata = data[data.Remaining > 0]\nfrom folium import plugins\nbike = plugins.MarkerCluster().add_to(my_map)\nfor lat, lng, label, in zip(data.Latitude, data.Remaining):\nfolium.Marker(\nlocation=[lat, lng],\nicon=folium.Icon(color='green', prefix='fa', icon='bicycle'),\npopup=label,\n).add_to(bike)\n# add incidents to map\nfolium.Circle(radius=500, location=[25.0141, 121.5379], popup='NTU Finance', color='crimson', fill=True,).add_to(my_map)\nfolium.Marker([25.0526981, 121.5203977 ], popup='NTU Finance',\nicon=folium.Icon(color='red', prefix='fa', icon='user')).add_to(my_map)\nmy_map.add_child(bike)\nmy_map.save('mymap.html')\nfrom html2image import Html2Image\nhti = Html2Image()\nhti.screenshot(\nhtml_file='mymap.html',\nsave_as='Bike_map.png'\n)\n", "output_sequence": "['Demonstrate getting data from webpage API and scraping data information from taipei public dataset', 'Find the nearest available youbikes in Zhongshan district']"}, {"input_sequence": "Author: ['TZU-YING TUNG'] ; import json\nimport pprint\nimport pandas as pd\nimport numpy\nimport matplotlib.pyplot as plt\nimport requests\nresponse = requests.get(\" https://datacenter.taichung.gov.tw/swagger/OpenData/9af00e84-473a-4f3d-99be-b875d8e86256\")\ncontent = response.content\njson_tree = json.loads(content)\nfor bike_rent_records in json_tree['retVal']:\nleftRatio = float(bike_rent_records[\"sbi\"]) / float(bike_rent_records[\"tot\"]) * 100\ndataframe = pd.DataFrame(json_tree['retVal'])\ndataframe['lat']=dataframe['lat'].astype(float)\nw = dataframe['lat']\nplt.scatter(w, v, k)\nplt.title('Scatter plot of Taichung U-bike')\nplt.xlabel('lat')\nplt.savefig('DEDA_410707007_HW2_Scraping with Taichung YouBike dataset')\nimport folium\nfrom folium.plugins import MarkerCluster\nimport numpy as np\nfrom datetime import datetime\nu_bike = folium.Map(location= [24.136807, 120.684875], zoom_start=13 )\nfor i in range(0,1202):\npop_text=str(dataframe['sarea'][i]) + '</br>' '\u5834\u7ad9\u540d\u7a31: ' +str(dataframe['sna'][i]) + '</br>' '\u5730\u5740: ' +str(dataframe['ar'][i]) + '</br>' '\u7e3d\u505c\u8eca\u683c: ' +str(dataframe['tot'][i]) + '</br>' '\u76ee\u524d\u8eca\u8f1b\u6578\u91cf: ' +str(int(dataframe['sbi'][i])) + '</br>' '\u53ef\u9084\u8eca\u4f4d\u6578: ' +str(dataframe['bemp'][i]) + '</br>' '\u66ab\u505c\u72c0\u614b: ' +str(dataframe['act'][i]) + '</br>' '\u8cc7\u6599\u66f4\u65b0\u6642\u9593: ' +str(datetime.strptime(str(dataframe['mday'][i]), '%Y%m%d%H%M%S'))\nif int(dataframe['act'][i])==0:\nfolium.Marker([dataframe['lat'][i], dataframe['lng'][i]], icon = folium.Icon(icon='wrench', color = 'black'),popup = folium.Popup(pop_text, max_width=1000)).add_to(u_bike)\nelif int(dataframe['sbi'][i])==0:\nfolium.Marker([dataframe['lat'][i], dataframe['lng'][i]], icon = folium.Icon(icon='remove', color = 'darkpurple'),popup = folium.Popup(pop_text, max_width=1000)).add_to(u_bike)\nelif int(dataframe['bemp'][i])==0:\nfolium.Marker([dataframe['lat'][i], dataframe['lng'][i]], icon = folium.Icon(icon='remove-sign', color = 'cadetblue'),popup = folium.Popup(pop_text, max_width=1000)).add_to(u_bike)\nelse :\nfolium.Marker([dataframe['lat'][i], dataframe['lng'][i]], icon = folium.Icon(icon='heart', color = 'beige'), popup = folium.Popup(pop_text, max_width=1000)).add_to(u_bike)\nu_bike.save(\"DEDA_410707007_HW2_Scraping with Taichung YouBike dataset.html\")\n", "output_sequence": "Using json module to unpack Taichung YouBik dataset and obtaining the instantaneous information of YouBike in Taichung city."}, {"input_sequence": "Author: ['David Alexander Behrens'] ; import pandas as pd\nfrom matplotlib import pyplot as plt\ndf_tmp[\"Date\"] = df_tmp[\"Date\"].astype(\"datetime64\")\ntitles = [\"BTC-USD\", \"ADA-USD\"]\ncolors = [\"red\", \"orange\", \"green\", \"saddlebrown\", \"darkviolet\"]\nfig, axs = plt.subplots(5,1, figsize=(12,12))\nfor i in range(5):\naxs[i].plot(df_tmp.index.values,df_tmp[coins[i]], color=colors[i])\naxs[i].set(xlabel=\"Date\", ylabel=\"USD\", title=titles[i])\naxs[i].xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n\nplt.tight_layout()\n#plt.savefig(\"Coins.png\")\n", "output_sequence": "['plot of prices in USD of five cryptocurrencies (BTC, ETH, BNB, XRP, ADA)']"}, {"input_sequence": "Author: ['Wolfgang Karl H\u00e4rdle', 'Ying-Yu Chen'] ; import pandas as pd\nimport bs4\nimport urllib\n# Define URL\n# \u201cI Have A Dream\u201d \u2014 Speech by Martin Luther King, Jr.\nurl = \"https://www.marshall.edu/onemarshallu/i-have-a-dream/\"\nraw_html = urllib.request.urlopen(url)\nparsed_html = bs4.BeautifulSoup(raw_html, \"lxml\")\ntext = parsed_html.find_all(\"div\", class_ = \"entry-content\")\n# Speech stored in list\ntextl = []\nfor i in text:\ntextl.append(i.get_text())\n# Convert to string\nimport re\n# Clean Speech Text\nexpression = \"[^a-zA-Z0-9 ]\" # keep only letters, numbers and whitespace\ncleantextCAP = re.sub(expression, \"\", cleantextprep) # replace empty string\ncleantext = cleantextCAP.lower()\n# Create Dictionary\ndat = list(cleantext.split())\ndict1 = {}\nfor i in range(len(dat)):\n#print(i)\nword = dat[i]\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\n# Unsorted speech constituents in dictionary as dict1\nkeys = list(dict1)\nfiltered_words = [word for word in keys if word not in stopwords.words('english')]\n# Finding word sequences in a dictionary ordered by frequency\ndef SequenceSelection(dictionary, length, startindex = 0):\n\n# Check Input\nif length > len(dictionary):\nreturn print(\"input length is too long\");\nelse:\nd = dictionary\nitems = [(v, k) for k, v in d.items()]\nitems.sort()\nitemsOut = [(k, v) for v, k in items]\nhighest = itemsOut[startindex:startindex + length]\ndd = dict(highest)\nwanted_keys = dd.keys()\ndictshow = dict((k, d[k]) for k in wanted_keys if k in d)\nreturn dictshow\nimport matplotlib.pyplot as plt\ndictionary = dictshow\nplt.bar((dictionary.keys()), dictionary.values(), color='g')\nplt.ylabel(\"frquency\")\nplt.title(\"Most frequent words\", {'fontsize' : 17})\nplt.savefig(\"Most frequent words.png\", transparent=True)\nimport pysentiment as ps\n# \u201cI Have A Dream\u201d \u2014 Speech by Martin Luther King, Jr.as cleantext\nhiv4 = ps.HIV4() # Use Harvard IV\ntokens = hiv4.tokenize(cleantext) # split string into constituents\nscore = hiv4.get_score(tokens)\nprint(score)\nimport pandas as pd\nd = [ [\"Positive\", 167],\n[\"Negative\", 85],\n[\"Polarity\", 0.32539682410556814],\n[\"Subjectivity\", 0.3829787228222208] ]\ndf = pd.DataFrame(d, columns = ['attribute ','score'])\nprint(df)\n", "output_sequence": "The files contain code for Python Webscraping and Sentiment Analysis by using Martin Luther King, Jr.'s speech \"I Have A Dream\" ."}, {"input_sequence": "Author: Lasse Groth ; # clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# install and load packages\nlibraries = c(\"QRMlib\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# load data\na = read.table(\"Bay9906_close_2kPoints.txt\")\nd = a + b + c # Create the portfolio\nlg = dim(d)\nx = log(d[-lg[1], ]) - log(d[-1, ]) # Negative log-return\n# Determine the Block Maxima data\nT = length(x)\nn = 20\nk = T/n\nz = matrix(, , )\nfor (j in 1:k) {\nr = x[((j - 1) * n + 1):(j * n)]\nz[j] = max(r)\n}\nw = sort(z)\nGEV = fit.GEV(z) # Fit the Generalized Extreme Value Distribution\nK = GEV$par.ests[1] # shape parameter\nt = (1:k)/(k + 1)\ny1 = qGEV(t, K, mu, sigma)\n# Plot the QQ plot\ndev.new()\nplot(w, y1, col = \"blue\", pch = 23, bg = \"blue\", xlab = c(\"\"), ylab = c(\"\"))\nlines(y1, y1, type = \"l\", col = \"red\", lwd = 2)\ntitle(\"QQ plot, Generalized Extreme Value Distribution\")\n# Plot the PP plot\nplot(y2, t, col = \"blue\", pch = 23, bg = \"blue\", xlab = c(\"\"), ylab = c(\"\"))\nlines(y2, y2, type = \"l\", col = \"red\", lwd = 2)\n", "output_sequence": "Fits a Generalized Extreme Value Distribution to the negative log-returns of a portfolio (Bayer, BMW, Siemens) for the time period from 1992-01-01 to 2006-09-21 and produces a QQ-plot and a PP-plot. Corresponds to exercise 16.5 in SFS."}, {"input_sequence": "Author: Ostap Okhrin ; # clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# install and load packages\nlibraries = c(\"rpanel\", \"fBasics\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\ne1 = new.env(parent = baseenv())\nassign(\"L\", c(2, 0, 1, 0), envir = e1)\ndensity.draw.alpha = function(panel) {\npanel$beta = e1$L[2]\ndensity.draw(panel)\n}\ndensity.draw.beta = function(panel) {\npanel$alpha = e1$L[1]\ndensity.draw.gamma = function(panel) {\ne1$L[1] = panel$alpha\ndensity.draw.exact(panel)\ndensity.draw.exact = function(panel) {\nmain.s = paste(\"alpha = \", panel$alpha, \"| beta = \", panel$beta, \"| gamma = \",\npanel$gamma, \"| delta =\", panel$delta)\nx = seq(-10, 10, 0.2)\ny = dstable(x, alpha = panel$alpha, beta = panel$beta, gamma = panel$gamma,\ndelta = panel$delta, pm = panel$parametrization)\n\nnf = layout(matrix(c(1, 2), 1, byrow = TRUE), c(1, 1), c(0.5, 0.5), TRUE)\npar(mai = (c(-0.05, 0.7, 0) + 0.1))\nplot(x, y, type = \"l\", xlim = c(-5, 5), ylim = c(0, 0.6), ylab = \"f(x)\", xlab = \"x\",\nlwd = 3, col = \"blue3\", main = main.s)\nabline(h = seq(0, 0.6, 0.1), v = seq(-5, 5, 1), lty = \"dotted\")\npar(mai = (c(0.7, 0.7, -0.05, 0) + 0.1))\nplot(x, log(y), type = \"l\", xlim = c(-10, 10), ylim = c(-10, 0), ylab = \"log(f(x))\",\nxlab = \"x\", lwd = 3, col = \"blue3\")\nabline(h = seq(-10, 0, 2), v = seq(-10, 10, 2), lty = \"dotted\")\npanel\n# this function defines at which values the densities are evaluated,\n# if an exact distribution is selected in the panel\ndensity.draw.exact.rb = function(panel) {\nif (panel$exact.dist.type == \"gaussian\") {\npanel$alpha = 2\ndensity.draw.exact(panel)\n} else if (panel$exact.dist.type == \"couchy\") {\npanel$alpha = 1\n} else if (panel$exact.dist.type == \"levy\") {\npanel$alpha = 0.5\n}\n# the panel is defined, i.e. the adjustable parameter\n# for example the different distributions\npanel = rp.control(size = c(320, 350), title = \"Stable Distributions\")\nrp.slider(panel, alpha, 0.01, 2, log = FALSE, action = density.draw.alpha, showvalue = TRUE,\ninitval = 2, resolution = 0.1, pos = c(10, 0, 300, 80))\nrp.slider(panel, beta, -1, log = FALSE, action = density.draw.beta, showvalue = TRUE,\ninitval = 0, resolution = 0.1, pos = c(10, 60, 300, 80))\nrp.slider(panel, gamma, 0.5, log = FALSE, action = density.draw.gamma, showvalue = TRUE,\ninitval = 1, resolution = 0.2, pos = c(10, 120, 300, 80))\nrp.slider(panel, delta, -2, log = FALSE, action = density.draw.delta, showvalue = TRUE,\ninitval = 0, resolution = 0.2, pos = c(10, 180, 300, 80))\nrp.radiogroup(panel, parametrization, c(0, 1, 2), labels = c(\"S0\", \"S1\", \"S2\"),\naction = density.draw, title = \"Parametrization\", initval = 0, pos = c(10,\n240, 145, 100))\nrp.radiogroup(panel, exact.dist.type, c(\"gaussian\", \"cauchy\", \"levy\"), action = density.draw.exact.rb,\ntitle = \"Exact Stable Distributions\", initval = \"gaussian\", pos = c(165, 240,\n145, 100))\n", "output_sequence": "Two plots of stable distributions are generated. The upper plot shows the density function and the lower it's log. The parameters can be adjusted by a panel, where the four parameter (alpha=(0,2]; beta=[-1,1]; gamma>0; delta=[-2,2]) of an alpha-stable distribution can be modified. The user can choose between three parametrization (S1; S2; S3) and the exact stable distributions Normal, Cauchy and Levy."}, {"input_sequence": "Author: Lasse Groth ; # clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# install and load packages\nlibraries = c(\"matlab\", \"QRMlib\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# load data\na = read.table(\"Bay9906_close_2kPoints.txt\")\nd = a + b + c # Create the portfolio\nlg = dim(d)\nx = log(d[-lg[1], ]) - log(d[-1, ]) # Negative log-returns\nn = length(x)\nx = sort(x, decreasing = TRUE)\nm = 100\nx1 = x[1:m]\n# empirical mean excess function\nt = x[1:(m + 1)] # t must be >0\nMEF = matrix(, , )\nfor (i in 1:length(t)) {\ny = x[find(x > t[i])]\nMEF[i] = mean(y - t[i])\n}\nplot(t, MEF, type = \"l\", col = \"blue\", lwd = 3, ylim = c(0.005, 0.04), xlab = \"u\",\nylab = \"e(u)\")\ntitle(\"Mean Excess Functions\")\n# mean excess function of generalized Pareto distribution, theorem 18.8\nk = 100\nGPD = fit.GPDb(x, nextremes = k, method = \"ml\", information = \"observed\")\nK = GPD$par.ests[1]\ngpme = (sigma + K * (t - mean(t)))/(1 - K)\nlines(t, gpme, lwd = 3)\n# Hill estimator, mean excess function of Pareto distribution\nalphaH = (mean(log(x1)) - log(x1[k]))^(-1)\nsigmaH = x1[k] * (k/n)^(1/alphaH)\ngp1me = t/(alphaH - 1)\n", "output_sequence": "Plots the empirical mean excess function, the mean excess function of generalized Pareto distribution, the mean excess function of Pareto distribution with parameter estimated with Hill estimator for the negative log-returns of portfolio (Bayer, BMW, Siemens), time period: from 1992-01-01 to 2006-09-21. Refers to exercise 16.8 in SFS."}, {"input_sequence": "Author: Xiu Xu, Shi Chen ; # clear history\nrm(list = ls(all = TRUE))\ngraphics.off()\n# install and load packages\nlibraries = c(\"sn\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\npar(mfrow = c(1, 3))\nx1 = seq(-3, 1, 0.001)\nplot(x1, dsn(x1, xi = 0, omega = 1, alpha = -5, log = FALSE), type = \"l\", lwd = 2.5,\nylab = \"PDF\", xlab = \"X\", col = \"chocolate3\", ylim = c(0, 0.9))\nabline(v = median(rsn(x1, xi = 0, omega = 1, alpha = -5)))\nplot(x2, dsn(x2, xi = 0, omega = 0.55, alpha = 0, log = FALSE), type = \"l\", lwd = 2.5,\nylab = \"\", xlab = \"X\", col = \"chartreuse4\", ylim = c(0, 0.9))\nabline(v = median(rsn(x2, xi = 0, omega = 0.55, alpha = 0)))\nplot(x3, dsn(x3, xi = 0, omega = 1, alpha = 5, log = FALSE), type = \"l\", lwd = 2.5,\nylab = \"\", xlab = \"X\", col = \"blue3\", ylim = c(0, 0.9))\nabline(v = median(rsn(x3, xi = 0, omega = 1, alpha = 5)))\n", "output_sequence": "Plots of left-skewed normal distribution (shape parameter alpha = - 5), symmetric normal and right-skewed normal distribution (shape parameter alpha = 5)."}, {"input_sequence": "Author: Chen Huang ; # clear all variables\nrm(list = ls(all = TRUE))\ngraphics.off()\ndlognormalLoss = function(x) {\ndlnorm(x, 0.5, 0.3) # the density function\n}\nplognormalLoss = function(x) {\nplnorm(x, 0.5, 0.3) # the distribution function\ncurve(plognormalLoss, 0, 4, xlab = \"Loss\", ylab = \"\", lwd = 2)\ncurve(dlognormalLoss, 0, 4, col = \" black\", lty = 2, lwd = 2, add = TRUE)\n# Fill in the part beyond VaR with shadow\nx = seq(2.42, 4, length = 100)\ny = dlognormalLoss(x)\nx = c(2.42, x)\ny = c(dlognormalLoss(4), y)\npolygon(x, y, col = \"grey\", border = \"grey\")\ntext(3.5, 0.2, expression(P(S > VaR[alpha]) == alpha))\narrows(3, 0.2, 2.7, 0.05, lwd = 2)\nmtext(expression(VaR[alpha]), side = 1, at = 2.42)\n# Mark the Expected shortfall\ny = plognormalLoss(x)\nx = c(0, x, 0)\ny = c(0.9, y, plognormalLoss(4))\npolygon(x, y, density = 20, col = \"grey\", border = \"black\", angle = 45)\ntext(0.3, 0.93, expression(1 - alpha == 0.9))\ntext(1.6, 0.95, expression(alpha * ES[alpha](X)))\ncurve(plognormalLoss, 0, 4, lwd = 2, add = TRUE)\ncurve(dlognormalLoss, 0, 4, col = \"black\", lty = 2, lwd = 2, add = TRUE)\nsegments(2.42, -0.25, 2.42, 0.9)\nlegend(x = 2.5, y = 0.5, c(\"PDF\", \"CDF\"), lty = c(2, 1))\n", "output_sequence": "Plots Value at Risk and Expected Shortfall in one figure and shows the relationship between VaR and ES."}, {"input_sequence": "Author: Alisa Kolesnikova (CRIX Data), Alla Petukhina (Crypto Data) ; setwd(\"~/Documents/PythiaR/CTD_Volatility/CC_Vola\")\n#install and load packages\nlibraries = c(\"readxl\", \"xtable\", \"igraph\", \"zoo\", \"igraph\",\n\"xts\", \"moments\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n## Load data Traditional assts\nTRAD_ASSETS = data.frame(read_excel(\"20171108Traditional_assets_indices_20130101_20171101.xlsx\", sheet = \"Sheet3\"))\nrow.names(TRAD_ASSETS) = as.Date(TRAD_ASSETS$Date, origin = \"1899-12-30\")\nTRAD_ASSETS$Date = NULL\n# CC prices, capitalization and volume\nCAP = read.delim(\"CC_CAP_20140101_20161231.txt\", sep = \";\")\nCAP$Date = as.Date(CAP$date, origin = \"1899-12-30\")\nCAP = CAP[order(CAP$Date), -1]\nrow.names(CAP) = CAP$Date\nCAP$Date = NULL\nCAP = CAP[row.names(CAP) > as.Date(\"2013-12-31\") & row.names(CAP) <\nas.Date(\"2017-01-01\"), ]\nPRICE$Date = as.Date(PRICE$date, origin = \"1899-12-30\")\nPRICE = PRICE[order(PRICE$Date), -1]\nrow.names(PRICE) = PRICE$Date\nPRICE$Date = NULL\nVOLUME$Date = as.Date(VOLUME$date, origin = \"1899-12-30\")\nVOLUME = VOLUME[order(VOLUME$Date), -1]\nPRICE = PRICE[row.names(PRICE) > as.Date(\"2013-12-31\") & row.names(PRICE) <\nas.Date(\"2017-01-01\"), ]\ncolnames(PRICE) = toupper(colnames(PRICE))\nyears = unique(format(as.Date(rownames(PRICE)), \"%Y\"))\n# Build annual prices TS\nCC_data = list()\nfor (year in years) {\nCC_data$PRICE[[year]] = PRICE[format(as.Date(rownames(PRICE)), \"%Y\") == year, ]\nCC_data$max_cc[[year]] = tail(sort(apply(CC_data$CAP[[year]], 2, mean, na.rm = T)), 10)\nCC_data$CC_PRICE_max[[year]] = subset(CC_data$PRICE[[year]],\nselect = names(CC_data$max_cc[[year]]))\nCC_data$TRAD_CC_PRICE_max[[year]] = merge(CC_data$CC_PRICE_max[[year]],\nTRAD_ASSETS[format(as.Date(rownames(TRAD_ASSETS)),\n\"%Y\") == year, ], by = \"row.names\")[, -1]\nCC_data$PRICE_max_without_na[[year]] = na.locf(CC_data$TRAD_CC_PRICE_max[[year]])\norder.by = as.Date(row.names(CC_data$PRICE[[year]])))\nCC_data$RET_max_xts[[year]] = diff(log(CC_data$PRICE_max_xts[[year]]),\nna.pad = TRUE)\nCC_data$COR_MAT[[year]] = data.frame(cor(CC_data$RET_max_xts[[year]]\n[, c(1:10, 14, 22)], use = \"pairwise.complete.obs\"))\nCOR_MAT_graph = CC_data$COR_MAT[[year]]\ndiag(COR_MAT_graph) = 0\nCC_data$COR_MAT_graph[[year]] = COR_MAT_graph\n}\n# For whole time period\nmax_cryptos = apply(CAP, 2, mean, na.rm = T)\nmax_cryptos = tail(sort(max_cryptos), 10)\nPRICE_max = subset(PRICE, select = names(max_cryptos))\nTRAD_CC_PRICE_max = merge(PRICE_max, TRAD_ASSETS, by = \"row.names\")[, -1]\n\n# Cleaning NaN\nPRICE_without_na = na.locf(PRICE)\nPRICE_max_without_na = na.locf(TRAD_CC_PRICE_max)\n# XTS creation\nPRICE_xts = xts(PRICE_without_na, order.by = as.Date(row.names(PRICE)))\n# Returns\nRET_xts = diff(log(PRICE_xts), na.pad = TRUE)\n####### CC Correlation tables and plots\ncolor = c(\"red3\", \"blue3\", \"darkorchid3\", \"goldenrod2\", \"chartreuse4\",\n\"palevioletred4\", \"steelblue3\", \"tan4\", \"black\",\"black\",\n\"black\",\"black\",\"black\",\"black\",\"black\",\"black\",\"black\",\"black\",\"black\",\n#add more black, if your fountain pen is out of ink and your plots are white\n# Build and save as pdf Bar plot for standart\n# deviation/volatility\n##grep(\"ETH\", colnames(RET_max_xts))\nStD_max = apply(RET_max_xts[, c(1:10, 14, 22)], 2, function(x) {\nsd(x, na.rm = T)\npdf(file = \"Volatility_max.pdf\")\nbarplot(StD_max, col = \"red3\", border = F)\ndev.off()\n# Build and save as pdf Bar plot for Sharpe ratios\nSharpe_max = apply(RET_max_xts[, c(1:10, 14, 22)], 2, function(x) {\nmean(x, na.rm = T)/sd(x, na.rm = T)\npdf(file = \"SHARPE_max.pdf\")\nbarplot(Sharpe_max, col = \"blue3\", border = F)\n# Save data\nsave.image(file = \"CC_MAX.RData\")\n", "output_sequence": "Produces volatility plots based on historical hf CRIX data"}, {"input_sequence": "Author: Fabio Martin ; import matplotlib\nmatplotlib.rcParams['animation.ffmpeg_path'] = 'C:/ffmpeg/bin/ffmpeg.exe'\n\"\"\"\nyou might want to use the following in terminal if the graphviz does not work:\nconda install -c conda-forge ffmpeg\nAll should be fine though if you use jupyter notebook\nmatplotlib.use('TKAgg')\nfrom matplotlib import animation\nfrom numpy import append, cos, linspace, pi, sin, zeros\nimport matplotlib.pyplot as plt\n# elephant parameters, last one is the eye\nparameters = [50-15j, 5+2j, -1-5j, -14-60j, 20-41j]\ndef fourier(t, C):\nf = zeros(t.shape)\nfor k in range(len(C)):\nf += C.real[k] * cos(k * t) + C.imag[k] * sin(k * t)\nreturn f\ndef elephant(t, p):\nnpar = 6\nCx = zeros((npar,), dtype='complex')\nCx[1] = p[0].real * 1j\nCy[1] = p[3].imag + p[0].imag * 1j\nCx[2] = p[1].real * 1j\nx = append(fourier(t, Cy), [p[4].imag])\nreturn x, y\ndef init_plot():\n# draw the body of the elephant\n# create trunk\nx, y = elephant(linspace(2 * pi + 0.9 * pi, 0.4 + 3.3 * pi, 1000), parameters)\nfor ii in range(len(y) - 1):\ny[ii] -= sin(((x[ii] - x[0]) * pi / len(y))) * sin(float(0)) * parameters[4].real\ntrunk.set_data(x, y)\nreturn trunk,\ndef move_trunk(i):\n# move trunk to new position (but don't move eye stored at end or array)\ny[ii] -= sin(((x[ii] - x[0]) * pi / len(y))) * sin(float(i)) * parameters[4].real\nfig, ax = plt.subplots()\n# initial the elephant body\nx, y = elephant(t=linspace(0.4 + 1.3 * pi, 2 * pi + 0.9 * pi, 1000), p=parameters)\nplt.plot(x, y, 'b.')\nplt.xlim([-75, 90])\nplt.axis('off')\ntrunk, = ax.plot([], [], 'b.') # initialize trunk\nani = animation.FuncAnimation(fig=fig,\nfunc=move_trunk,\nWriter = animation.writers['ffmpeg']\nmetadata = dict(title='Elephant Trunk Wiggling', artist='Jenny Beth\u00e4user')\nwriter = Writer(fps=30, metadata=metadata, bitrate=1800)\nani.save(filename='bulldog_trunk_wiggle.mp4', writer=writer)\nplt.show()\n", "output_sequence": "Wiggly Animal"}, {"input_sequence": "Author: Sergey Nasekin ; #Functions\nlibrary(\"foreach\")\nlibrary(\"quantreg\")\nlibrary(\"KernSmooth\")\nlibrary(\"doParallel\")\nkernelq = function(u){\ndnorm(u, mean = 0, sd = 1)\n}\n# M-type smoother\nlnrob = function(x, y, h, maxiter, x0 = seq(0,1, length.out = 100)){\n\nxx = sort(x0)\nxx = (xx - min(xx))/(max(xx) - min(xx))\nfv = xx\nx = (x - min(x))/(max(x)-min(x))\nfor (i in 1:length(xx)) {\nz = x - xx[i]\nwx = dnorm(z/h)\nr = rlm(y ~ z, weights = wx, method = \"M\", maxit = maxiter)\nu = r$wresid\nfv[i] = r$coefficients[[1]]\n}\n\"psi1\" = r$psi\nreturn( list(xx = xx, fv = fv, dv = dv, \"psi1\" = psi1) )\n# Quantile regression with specific tau\nlprq2 = function(x, y, h, tau, x0) {\nxx = sort(x0)\nx = (x - min(x)) / (max(x) - min(x))\n\nfor(i in 1:length(xx)){\nr = rq(y ~ z, tau = tau, weights = wx, method = \"br\")\nfv[i] = r$coef[1.]\nlist(xx = xx, fv = fv, dv = dv)\n# Quantile regression with random tau\nlprq3 = function(x, y, h, x0){\nxx = sort(x0)\nxx = (xx - min(xx)) / (max(xx) - min(xx))\nfv = xx\nx = (x - min(x)) / (max(x) - min(x))\ntau = runif(1)\nfor(i in 1:length(xx)) {\nr = rq(y ~ z, weights = wx, tau = runif(1), ci = FALSE)\n", "output_sequence": "Calculate and plot uniform bootstrap confidence bands for the ProShares UltraPro S&P500 (UPRO) LETF option implied volatility at the time-to-maturity 0.5 years"}, {"input_sequence": "Author: Sergey Nasekin ; # clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\nsetwd(\"~/Documents/R/conf_bands_correction\")\n#Load libraries\nlibraries = c(\"foreach\",\"MASS\",\"quantreg\",\"KernSmooth\",\"doParallel\",\"plyr\",\"ggplot2\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\nsource(\"cbands_funcs.r\")\n## Parameters\ncl = 7 # Number of cores to use\nB = 1000\nalpha = 0.05\ngridn = 100\nbeta = 3 #change here the beta for the LETF in question\n# Load data and apply moneyness scaling\nmonivdataLETF = read.table('mivttmdata_05_UPRO.csv',sep=',')\n#Clean the data\nxspy = as.matrix(monivdataSPY[,1])\nmoniv = data.frame(cbind(xspy,yspy))\ncolnames(moniv) = c('mon','iv')\nfreqs = count(moniv,vars = 'iv') # 'plyr' library needed\nfreqs = freqs[order(-freqs$freq),]\ngood_ivs = freqs$iv[freqs$freq < 12]\nyspy = as.matrix(moniv$iv[(moniv$iv %in% good_ivs) == TRUE])\netfvol = as.matrix(yspy)\nx = as.matrix(monivdataLETF[,1])\nmoniv = data.frame(cbind(x,y))\ngood_ivs = freqs$iv[freqs$freq < 3]\nx = as.matrix(moniv$mon[(moniv$iv %in% good_ivs) == TRUE])\nttm = as.matrix(monivdataLETF[,3])\nttm = as.matrix(ttm[(moniv$iv %in% good_ivs) == TRUE])\nScMonKf = (x/exp(-0.5*beta*(beta-1)*(mean(etfvol)^2)*ttm))^(1/beta)\nx = ScMonKf\n#Choose bandwidths\nn = nrow(x)\nhg = 0.15\ng = n^(-1/9)\n# Sort input data\ny = y[order(x)]\n# Scale x to [0, 1]\nxmin = min(x)\nx = (x - xmin) / (xmax - xmin)\nh = median(abs(x-median(x)))/0.6745*(4/3/n)^0.2\n# Initial fit\nyhat.h = lnrob(x, y, h = h, maxiter = 1000, x0 = x)\nehat = y - yhat.h$fv\nehh = median(abs(ehat-median(ehat)))/0.6745*(4/3/n)^0.2\nyhat.grid.h = lnrob(x, y, h = h, maxiter = 1000, x0 = seq(0, 1, length.out = gridn))\n# Empirical pdf of x at gridpoints\nfxd = bkde(x, gridsize = gridn, range.x = c(yhat.grid.h$xx[1], yhat.grid.h$xx[gridn]))\nfl = vector(length = gridn, mode = \"numeric\")\nfor (k in 1: gridn){\n# Conditional pdf f(e|x)at gridpoints\nnom = sum((kernelq((x - yhat.grid.h$xx[k]) / (hg)) *\nyhat.grid.h$psi1((y - yhat.grid.h$fv[k]), deriv = 1)))\ndenom = sum(kernelq((x - yhat.grid.h$xx[k])/(hg)))\nfl[k] = nom / denom\n\n# Conditional E(psi^2(e))\nnom = sum((kernelq((x - yhat.grid.h$xx[k])/(h)) *\nyhat.grid.h$psi1((y - yhat.grid.h$fv[k]), deriv = 0)^2))\ndenom = sum(kernelq((x - yhat.grid.h$xx[k])/(hg)))\nfll[k] = nom / denom\n}\nbandt = (fxd$y)^(1/2) * abs(fl / sqrt(fll))\n# Bootstrap\npack = c(\"MASS\", \"KernSmooth\", \"Rlab\", \"quantreg\")\ncl = makeCluster(cl)\nregisterDoParallel(cl)\nd = vector(length = B, mode = \"numeric\")\nd = foreach(i = 1:B, .packages = pack)%dopar%{\nestar = lprq3( yhat.h$xx, (y - yhat.h$fv), h = hg, x0 = yhat.grid.h$xx )\nystar = yhat.grid.g$fv + estar$fv\nfitstar = lnrob(yhat.grid.h$xx, ystar, h = h, maxiter = 50, x0 = yhat.grid.h$xx )\nd.m = max(abs(bandt*abs(fitstar$fv - yhat.grid.g$fv)))\nstopCluster(cl)\nd = unlist(d)\ndstar = quantile(d[d!=0], probs = 1 - alpha)\ndstar = dstar * {bandt}^(-1)\n# Construct asymptotic confidence bands\ncc = 1 / 4\nlambda = 1 / 2 / sqrt(pi)\ndelta = - log(h) / log(n)\ndd = sqrt(2 * delta * log(n)) + (2 * delta * log(n))^(-1/2) * log(cc / 2 / pi)\ncn = log(2) - log(abs(log(1 - alpha)))\nband = (n * h)^(- 1/2) * bandt^{-1} * (dd + cn * (2 * delta * log(n))^(-1/2)) * sqrt(lambda)\n# Scale back\nx = ( x * (xmax - xmin) ) + xmin\n# plot(x, y, xlab = \"Moneyness\", ylab = \"Implied volatility\", main = \"UPRO\")\n# lines(x.grid, yhat.grid.h$fv, lwd = 4, col = \"blue\")\n# lines(x.grid, (yhat.grid.h$fv - dstar), col = \"red\", lty = 2, lwd = 4)\n#Create a \"beautiful\" plot with ggplot2\nxyframe = data.frame(cbind(x,y))\ncolnames(xyframe) = c('moneyness','impvol')\nssoframe = data.frame(cbind(x.grid,yhat.grid.h$fv,yhat.grid.h$fv - dstar,yhat.grid.h$fv + dstar))\ncolnames(ssoframe) = c('mgrid','ivest','lowbnd','upbnd')\nbeauplot = ggplot() + geom_point(data = xyframe, aes(x = moneyness, y = impvol),size=1.5,colour=\"#666666\") +\ngeom_line(data = ssoframe, aes(x = mgrid, y = ivest),colour=\"#000099\",size=1.5) +\nbeauplot + ggtitle('UPRO')\n#write.table(ssoframe,file = 'sds_cleaned.csv',sep=',')\n", "output_sequence": "Calculate and plot uniform bootstrap confidence bands for the ProShares UltraPro S&P500 (UPRO) LETF option implied volatility at the time-to-maturity 0.5 years"}, {"input_sequence": "Author: Sergey Nasekin ; %% LOAD AND PREPARE DATA\nclc\nclear\nclose all\n%Pre-set model parameters\nKm = 3; %spline order for moneyness direction\nKt = 3;\ndim_mon = 70;\nikmon = 5;\ntol = 1e-06;\nmaxiter = 100;\nL = 3;\nlower_mon = 0.5;\n%Load data for SPY, SSO options and the Treasury yield curve data\nload SPYFULL\nload yrates1415\nSPY = SPYtomatlab;\nSPYdate_data = SPYtomatlab.t;\nSPYdates = unique(SPYdate_data);\n[BothDates,ind] = intersect(SSOdates,SPYdates);\nSPYIV = [SPY.TTM,SPY.LM,SPY.k_f,SPY.IV,SPY.t,SPY.K,SPY.L_t,SPY.r,SPY.C];\nSSOIV = [SSO.TTM,SSO.LM,SSO.IV,SSO.t,SSO.K,SSO.L_t,SSO.r,SSO.C,SSO.T];\nSSOIND = [];\nfor i = 1:length(ind)\nssoind = find(SSOdate_data == BothDates(ind(i)));\nSSOIND = [SSOIND;ssoind]; %#ok<AGROW>\nend\nSPYIND = [];\nspyind = find(SPYdate_data == BothDates(ind(i)));\nSPYIND = [SPYIND;spyind]; %#ok<AGROW>\nSPYIV = SPYIV(SPYIND,:);\ndate_data = SPYIV(:,5);\nDates = unique(date_data);\nMATNUM = [0; find(diff(DatNum) ~= 0)];\nT = length(MATNUM);\nttm_data = SPYIV(:,1);\n%remove duplicates\nIVCLEAN = [];\nfor i = 1:T\n%obtain data for each J_t\nif i == length(MATNUM)\nind = MATNUM(i)+1:length(mon_data);\nTTM_Jt = ttm_data(ind);\nelse\nind = MATNUM(i)+1:MATNUM(i+1);\nend\nDataMtx = SPYIV(ind,:);\nintmtn = [0; find(diff(TTM_Jt) ~= 0)];\nDUMTX = [];\nfor j = 1:length(intmtn)\nPaDaMtx = DataMtx(intmtn(j)+1:end,:);\nelse\nPaDaMtx = DataMtx(intmtn(j)+1:intmtn(j+1),:);\nend\n% extract the unique matrix for the step i\nColStrike = 6;\nDataCol = PaDaMtx(:,ColStrike);\n[~,indun,~] = unique(DataCol);\nDataUMtx = PaDaMtx(indun,:);\nDUMTX = [DUMTX;DataUMtx];\nIVCLEAN = [IVCLEAN;DUMTX];\nMONDATA = IVCLEAN(:,3);\nMonReduc = round(MONDATA,1);\nIVCLEAN = IVCLEAN(MonReduc <= upper_mon & MonReduc >= lower_mon,:);\nTTM_DATA = IVCLEAN(:,1);\nTtmReduc = round(TTM_DATA,1);\nIVCLEAN = IVCLEAN(TtmReduc <= 1,:);\ndate_data = IVCLEAN(:,5);\n%TRUE INTEREST RATE CALCULATION\n[~,rind,~] = intersect(yrates1415(:,1),Dates);\nRATEMAT = yrates1415(rind,2:6);\nmaturs = [1/12,3/12,6/12,1,2];\n%calculate risk-free interest rates for each maturity\nRATES = [];\nind = MATNUM(i)+1:length(TTM_DATA);\nTTM_Jt = TTM_DATA(ind);\nrates = RATEMAT(i,:)./100;\nR_Jt = interp1(maturs,rates,TTM_Jt,'pchip');\nRATES = [RATES;R_Jt];\nIVCLEAN(:,8) = RATES;\n% obtain the new \"tracking matrix\"\nIVDATA = IVCLEAN(:,4);\nDATES = unique(date_data);\nDatesStr = datestr(date_data);\niv_data = IVCLEAN(:,4);\nfor i = 1:length(date_data)\nif DatesStr(i,end) == '4'\nIVCLEAN(:,5) = datenum(DatesStr);\n% calculate days' breaks vector\nJTT = zeros(1,T);\nif i == T\nJt = length(ind);\nJTT(i) = Jt;\nwshift = 0;\nwwidth = 100;\n%% MAIN LOOP OF THE ROLLING-WINDOW STRATEGY\nRMSPE = zeros(1,T-wwidth);\nfor l = wwidth:T-1\n%construct the rolling-window dataset\n%for estimation\nIVMAT = IVCLEAN(1+wshift:MATNUM(l),:);\ndate_data = IVMAT(:,5);\nDates = unique(date_data);\nmatnum = [0; find(diff(DatNum) ~= 0)]; %this is another, rolling-window\nTw = length(matnum); %tracking matrix\nttm_data = IVMAT(:,1);\n% Marginally transform TTM data (because DSFM theory works\n% only on the interval (0,1) for the covariates!)\nttm_data = ksdensity(ttm_data,ttm_data,'function','cdf');\n%marginally transform MON data (the same reason as above!)\nmon_data = ksdensity(mon_data,mon_data,'function','cdf');\n%Produce knots' sequences for B-spline estimation\nknots_mon = [min(mon_data),min(mon_data),linspace(min(mon_data),...\nmax(mon_data),ikmon),max(mon_data),max(mon_data)];\nknots_ttm = [min(ttm_data),min(ttm_data),linspace(min(ttm_data),...\nmax(ttm_data),ikttm),max(ttm_data),max(ttm_data)];\nKK = (length(knots_mon)-Km)*(length(knots_ttm)-Kt);\nPHI = cell(1,Tw);\n%construct the Phi matrices for the rolling window\nfor i = 1:Tw\n%obtain data for each J_t\nif i == length(matnum)\nind = matnum(i)+1:length(mon_data);\nMON_Jt = mon_data(ind);\nJt = length(MON_Jt);\nTTM_Jt = ttm_data(ind);\nind = matnum(i)+1:matnum(i+1);\nJt = length(ind);\nJT(i) = Jt;\n%estimate splines\nUMON_Jt = unique(MON_Jt);\nMonMat = spcol(knots_mon,Km,UMON_Jt);\nPhi = zeros(KK, Jt);\nfor j = 1:Jt\nind_mon = find(UMON_Jt == MON_Jt(j));\ncombin = combvec(MonMat(ind_mon,:), TtmMat(ind_ttm,:));\nPhi(:,j) = prod(combin,1);\nPHI{i} = Phi;\nYY{i} = IV_Jt;\n%CREATE STARTING VALUES FOR Z\nCoefMat2 = [0.95 0;\n0.2 -0.3];\nCoefMat3 = [0.95 -0.2 0;\n0 0.8 0.1;\n\nCoefMat4 = [0.8 -0.2 0 0.3;\n0 0.8 -0.3 ;\n\nswitch L\ncase 2\nCoefMat = CoefMat2;\ncase 3\nCoefMat = CoefMat3;\ncase 4\nCoefMat = CoefMat4;\nvarspec = vgxset('n',L,'nAR',1,'AR',CoefMat,'Q',10e-5.*eye(L));\nZeta_start = vgxsim(varspec,Tw);\nCmat = eye(L) - (1/L)*ones(L,1)*ones(1,L); %centering matrix\nZeta_start = Zeta_start*Cmat; %set means to zero\nZeta_start = Zeta_start';\nZeta_start(2,50) = -0.0037;\nrank(Zeta_start)\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n%LOOPING WITH RE-TRIALS IN THE CASE OF NON-CONVERGENCE\ncount = 0;\nerr_count = 0;\nwhile count == err_count\ntry\n%Compute starting values for Newton iterations\nL = size(Zeta_start,1);\nZETA_start = reshape(Zeta_start,L*Tw,1).*100000;\n%set up the initial guess solution\nalpha_start = rand(KK*(L+1),1);\nSOL_OLD = [alpha_start;ZETA_start];\ndiffV = 10; % make sure the loop runs at least once\niter = 0;\n%THE NEWTON LOOP\nwhile (diffV > tol)\nif iter > maxiter\nerror('DSFM_RMSE:maxiter','The algorithm does not converge with the given maximum number of iterations')\nend\nalpha = SOL_OLD(1:KK*(L+1),1);\nZeta = [ones(1,Tw);reshape(ZETA,L,Tw)];\nAlpha = reshape(alpha,L+1,KK);\nAA = Alpha(2:end,:);\nfirst_el = zeros(KK*(L+1),Tw);\nF01 = cell(Tw,1);\nII = [zeros(1,L);eye(L)];\nF11 = cell(1,Tw);\nfor i = 1:Tw\nfirst_el(:,i) = kron( PHI{i}*PHI{i}', Zeta(:,i)*Zeta(:,i)' )*alpha;\nF01{i} = ( Zeta(:,i)'*Alpha*PHI{i}*PHI{i}'*AA' - YY{i}'*PHI{i}'*AA' )';\nf02 = zeros(Tw);\nf02_block = AA*PHI{i}*PHI{i}'*AA';\nf02_kron = kron(f02,f02_block);\nF11{i} = kron( PHI{i}*PHI{i}'*AA',Zeta(:,i) ) + kron( PHI{i}*PHI{i}'*Alpha'*Zeta(:,i), II ) ...\n- kron( PHI{i}*YY{i}, II);\nF10 = 2.*(sum(first_el,2) - sum(secd_el,2));\nF01 = 2.*cell2mat(F01);\nFBIG = [F10;F01];\n%compute the solution iteration\nSOL_NEW = SOL_OLD - (pinv(DFBIG))*FBIG;\ndiffV = max(abs(SOL_NEW - SOL_OLD))\nSOL_OLD = SOL_NEW;\niter = iter + 1\ncatch err\n%Re-simulate starting values for Z in the case of\n%non-convergence\nvarspec = vgxset('n',L,'nAR',1,'AR',CoefMat,'Q',10e-5.*eye(L));\nZeta_start = vgxsim(varspec,Tw);\nCmat = eye(L) - (1/L)*ones(L,1)*ones(1,L); %centering matrix\nZeta_start = Zeta_start*Cmat; %set means to zero\nZeta_start(2,50) = -0.0037;\nrank(Zeta_start)\nerr_count = err_count + 1;\n\n%END OF LOOPING WITH RE-TRIALS\nSOL_FIN = SOL_NEW;\nALPHA = SOL_FIN(1:KK*(L+1),1);\nALPHA = reshape(ALPHA,L+1,KK);\nmongrid = linspace(min(mon_data),max(mon_data),dim_mon);\nMONMAT = spcol(knots_mon,Km,mongrid);\nCOEF = zeros(length(knots_mon)-Km,length(knots_ttm)-Kt,L+1);\nfor i = 1:L+1\nCOEF(:,:,i) = reshape(ALPHA(i,:)',length(knots_mon)-Km,length(knots_ttm)-Kt);\n%obtain the estimated factor functions\nMHAT(:,:,i) = MONMAT*COEF(:,:,i)*TTMMAT';\n%Norming and orthogonalization of factor functions mhat and\n%coefficients zeta\ndu = (mongrid(2)-mongrid(1))*(ttmgrid(2)-ttmgrid(1));\ntempmat = 0*MHAT(:,:,1)+1;\ntempmat(2:(end-1),:) = 2*tempmat(2:(end-1),:);\n%Norming matrices\nGAMMA = zeros(L);\n%Numeric integration\nfor i = 1:L\ngamma(i) = sum(sum(tempmat.*MHAT(:,:,1).*MHAT(:,:,i+1)))*du/4;\nfor j = 1:L\nGAMMA(i,j) = sum(sum(tempmat.*MHAT(:,:,j+1).*MHAT(:,:,i+1)))*du/4;\n%Vectorize factor functions\nMHATMat = zeros(size(MHAT(:,:,1),1)*size(MHAT(:,:,1),2),L+1);\nfor i = 1:(L+1)\nMHATMat(:,i) = reshape(MHAT(:,:,i),size(MHAT(:,:,1),1)*size(MHAT(:,:,1),2),1);\n%Obtain normed coefficients Zeta (as in Fengler, p. 166, eq. 5.74)\nZeta_new = zeros(L,Tw);\nZeta_est = reshape(ZZETA,L,Tw);\nZeta_new(:,i) = (GAMMA^0.5)*( Zeta_est(:,i) + (GAMMA^(-1))*gamma );\nMHATMatZero = MHATMat(:,1)' -gamma'*GAMMA^(-1)*MHATMat(:,2:end)';\nMHATMatShort = GAMMA^(-0.5)*MHATMat(:,2:end)';\n%Create the B matrix for PCA transformation\nB = Zeta_new*Zeta_new';\n[Z,~] = eigs(B,L);\nZETA_FIN = zeros(L,Tw);\nZETA_FIN(:,i) = Z'*Zeta_new(:,i);\nMHATMatFin = Z'*MHATMatShort;\n%Obtain final factor functions\nMHAT_FIN = zeros(size(MHAT(:,:,1),1), L+1);\nif i == 1\nMHAT_FIN(:,:,i) = reshape(MHATMatZero, size(MHAT(:,:,1),1),...\nsize(MHAT(:,:,1),2));\ncontinue\nMHAT_FIN(:,:,i) = reshape(MHATMatFin(:,i-1), size(MHAT(:,:,1),1),...\nsize(MHAT(:,:,1),2));\n%Produce dynamic IV surfaces\nIV_DYN = zeros(size(MHAT(:,:,1),1), Tw);\nfor t = 1:Tw\nIV_DYN(:,:,t) = MHAT_FIN(:,:,1);\nfor ll = 1:L\nIV_DYN(:,:,t) = IV_DYN(:,:,t) + ZETA_FIN(ll,t).*MHAT_FIN(:,:,ll+1);\nend\n%NOW FORECAST THE IV_DYN AND TAKE OUT THE NECESSARY \"STRING\" OF IVs\nnumLags = 3;\nZ = ZETA_FIN';\nZPreSamp = Z(1:numLags,:);\nZSamp = Z(numLags+1:end,:);\nVARSpec = vgxvarx(vgxset('n',L,'Constant',true,'nAR',numLags),...\nZSamp,[],ZPreSamp); %no exogenous inputs here!\nhorizon = 1;\nForecastZ = vgxpred(VARSpec,horizon,[],Z);\n%obtain the forecasted IV surface\nIVDF = MHAT_FIN(:,:,1);\nfor ll = 1:L\nIVDF = IVDF + ForecastZ(ll).*MHAT_FIN(:,:,ll+1);\n%CALCULATING THE REAL-DATA NEXT-DAY IV FOR THE RMSPE\nDatMat_R = [MONDATA(MATNUM(l)+1:MATNUM(l+1)),...\n%RMSE CALCULATION PART\nTTM_t_R = round(TTM_DATA(MATNUM(l)+1:MATNUM(l+1)),1);\nTUt = unique(TTM_t_R);\n%Calculate the array of all real and forecast (sorted) IVs and (transformed and sorted) log-moneyness\nallIVLM = cell(length(TUt),1);\nfor i = 1:length(TUt)\nLM_t_R = DatMat_R(TTM_t_R == TUt(i),1);\nLMt_Sc_R = ksdensity(IVMAT(:,2),LM_t_R,'function','cdf');\nIVDF_Tgt_R = interp2(mongrid,ttmgrid,IVDF',LMt_Sc_R,TUt(i),'cubic');\n[LM_srtd_R,...\nivind_R] = sort(LMt_Sc_R,'ascend');\nallivlm = [LM_srtd_R';IVDF_Tgt_R(ivind_R);IV_t_R(ivind_R)'];\nallivlm = allivlm(:,~any(isnan(allivlm),1));\nallIVLM{i} = allivlm;\n%RMSPE CALCULATION\nSrmspe = zeros(1,length(TUt));\nSDivsr = 0;\nivm2 = allIVLM{i};\nif mod(length(ivt),2) == 0\ndp = length(ivt)/2 + 1;\nSrmspe(i) = sum((ivt(dp:end) - ivr(dp:end)).^2);\nSDivsr = SDivsr + length(ivt(dp:end));\nSrmspe = sqrt((1/SDivsr)*sum(Srmspe));\nRMSPE(l-wwidth+1) = Frmspe;\nIVStrs{l-wwidth+1} = allIVLM;\nwshift = wshift + JTT(l-wwidth+1);\n%% RMSPE plots\n%{\nTo plot this figure, you need to run the code above for L = 2,3,4\nand save the RMSPE arrays as RMSPE2, respectively\n%}\nrfig = figure;\nplot(DATES(wwidth+1:end,:), RMSPE2, 'LineWidth', 3)\nhold on\nplot(DATES(wwidth+1:end,:), RMSPE3, 'LineWidth', 3, 'Color', 'r')\ndatetick('x')\nylabel('RMSPE')\ntitle('RMSPE with L = 2, 3, 4')\nset(rfig, 'Position', [10 900 500])\nhold off\n", "output_sequence": "Computes and plots the root mean squared prediction error of the dynamic one-step-ahead forecast of the implied volatility surface of the SPY LETF call option. The estimation of the dynamic semiparametric factor model with B-spline basis is performed for this purpose"}, {"input_sequence": "Author: Sergey Nasekin ; # clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n#Load libraries\nlibraries = c(\"foreach\",\"MASS\",\"quantreg\",\"KernSmooth\",\"doParallel\",\"plyr\",\"ggplot2\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\nsource(\"cbands_funcs.r\")\n## Parameters\ncl = 7 # Number of cores to use\nB = 1000\nalpha = 0.05\ngridn = 100\n# Load data\nmonivdataSPY = read.table('mivttmdata_05_SPY.csv',sep=',')\nx = as.matrix(monivdataSPY[,1])\n#Clean the data\nmoniv = data.frame(cbind(x,y))\ncolnames(moniv) = c('mon','iv')\nfreqs = count(moniv,vars = 'iv') # 'plyr' library needed\nfreqs = freqs[order(-freqs$freq),]\ngood_ivs = freqs$iv[freqs$freq < 12]\nx = as.matrix(moniv$mon[(moniv$iv %in% good_ivs) == TRUE])\n#Choose bandwidths\nn = nrow(x)\nhg = 0.15\ng = n^(-1/9)\n# Sort input data\ny = y[order(x)]\n# Scale x to [0, 1]\nxmin = min(x)\nx = (x - xmin) / (xmax - xmin)\nh = median(abs(x-median(x)))/0.6745*(4/3/n)^0.2\n# Initial fit\nyhat.h = lnrob(x, y, h = h, maxiter = 1000, x0 = x)\nehat = y - yhat.h$fv\nehh = median(abs(ehat-median(ehat)))/0.6745*(4/3/n)^0.2\nyhat.grid.h = lnrob(x, y, h = h, maxiter = 1000, x0 = seq(0, 1, length.out = gridn))\n# Empirical pdf of x at gridpoints\nfxd = bkde(x, gridsize = gridn, range.x = c(yhat.grid.h$xx[1], yhat.grid.h$xx[gridn]))\nfl = vector(length = gridn, mode = \"numeric\")\nfor (k in 1: gridn){\n# Conditional pdf f(e|x)at gridpoints\nnom = sum((kernelq((x - yhat.grid.h$xx[k]) / (hg)) *\nyhat.grid.h$psi1((y - yhat.grid.h$fv[k]), deriv = 1)))\ndenom = sum(kernelq((x - yhat.grid.h$xx[k])/(hg)))\nfl[k] = nom / denom\n\n# Conditional E(psi^2(e))\nnom = sum((kernelq((x - yhat.grid.h$xx[k])/(h)) *\nyhat.grid.h$psi1((y - yhat.grid.h$fv[k]), deriv = 0)^2))\ndenom = sum(kernelq((x - yhat.grid.h$xx[k])/(hg)))\nfll[k] = nom / denom\n}\nbandt = (fxd$y)^(1/2) * abs(fl / sqrt(fll))\n# Bootstrap\npack = c(\"MASS\", \"KernSmooth\", \"Rlab\", \"quantreg\")\ncl = makeCluster(cl)\nregisterDoParallel(cl)\nd = vector(length = B, mode = \"numeric\")\nd = foreach(i = 1:B, .packages = pack)%dopar%{\nestar = lprq3( yhat.h$xx, (y - yhat.h$fv), h = hg, x0 = yhat.grid.h$xx )\nystar = yhat.grid.g$fv + estar$fv\nfitstar = lnrob(yhat.grid.h$xx, ystar, h = h, maxiter = 50, x0 = yhat.grid.h$xx )\nd.m = max(abs(bandt*abs(fitstar$fv - yhat.grid.g$fv)))\nstopCluster(cl)\nd = unlist(d)\ndstar = quantile(d[d!=0], probs = 1 - alpha)\ndstar = dstar * {bandt}^(-1)\n# Construct asymptotic confidence bands\ncc = 1 / 4\nlambda = 1 / 2 / sqrt(pi)\ndelta = - log(h) / log(n)\ndd = sqrt(2 * delta * log(n)) + (2 * delta * log(n))^(-1/2) * log(cc / 2 / pi)\ncn = log(2) - log(abs(log(1 - alpha)))\nband = (n * h)^(- 1/2) * bandt^{-1} * (dd + cn * (2 * delta * log(n))^(-1/2)) * sqrt(lambda)\n# Scale back\nx = ( x * (xmax - xmin) ) + xmin\n# plot(x, y, xlab = \"Moneyness\", ylab = \"Implied volatility\", main = \"SPY\")\n# lines(x.grid, yhat.grid.h$fv, lwd = 4, col = \"blue\")\n# lines(x.grid, (yhat.grid.h$fv - dstar), col = \"red\", lty = 2, lwd = 4)\n#Create a \"beautiful\" plot with ggplot2\nxyframe = data.frame(cbind(x,y))\ncolnames(xyframe) = c('moneyness','impvol')\nssoframe = data.frame(cbind(x.grid,yhat.grid.h$fv,yhat.grid.h$fv - dstar,yhat.grid.h$fv + dstar))\ncolnames(ssoframe) = c('mgrid','ivest','lowbnd','upbnd')\nbeauplot = ggplot() + geom_point(data = xyframe, aes(x = moneyness, y = impvol),size=1.5,colour=\"#666666\") +\ngeom_line(data = ssoframe, aes(x = mgrid, y = ivest),colour=\"#000099\",size=1.5) +\nbeauplot + ggtitle('SPY')\n", "output_sequence": "Calculate and plot uniform bootstrap confidence bands for the SPDR S&P 500 ETF (SPY) option implied volatility at the time-to-maturity 0.5 years"}, {"input_sequence": "Author: Sergey Nasekin ; #Functions\nlibrary(\"foreach\")\nlibrary(\"quantreg\")\nlibrary(\"KernSmooth\")\nlibrary(\"doParallel\")\nkernelq = function(u){\ndnorm(u, mean = 0, sd = 1)\n}\n# M-type smoother\nlnrob = function(x, y, h, maxiter, x0 = seq(0,1, length.out = 100)){\n\nxx = sort(x0)\nxx = (xx - min(xx))/(max(xx) - min(xx))\nfv = xx\nx = (x - min(x))/(max(x)-min(x))\nfor (i in 1:length(xx)) {\nz = x - xx[i]\nwx = dnorm(z/h)\nr = rlm(y ~ z, weights = wx, method = \"M\", maxit = maxiter)\nu = r$wresid\nfv[i] = r$coefficients[[1]]\n}\n\"psi1\" = r$psi\nreturn( list(xx = xx, fv = fv, dv = dv, \"psi1\" = psi1) )\n# Quantile regression with specific tau\nlprq2 = function(x, y, h, tau, x0) {\nxx = sort(x0)\nx = (x - min(x)) / (max(x) - min(x))\n\nfor(i in 1:length(xx)){\nr = rq(y ~ z, tau = tau, weights = wx, method = \"br\")\nfv[i] = r$coef[1.]\nlist(xx = xx, fv = fv, dv = dv)\n# Quantile regression with random tau\nlprq3 = function(x, y, h, x0){\nxx = sort(x0)\nxx = (xx - min(xx)) / (max(xx) - min(xx))\nfv = xx\nx = (x - min(x)) / (max(x) - min(x))\ntau = runif(1)\nfor(i in 1:length(xx)) {\nr = rq(y ~ z, weights = wx, tau = runif(1), ci = FALSE)\n", "output_sequence": "Calculate and plot uniform bootstrap confidence bands for the SPDR S&P 500 ETF (SPY) option implied volatility at the time-to-maturity 0.5 years"}, {"input_sequence": "Author: Alexander Hoelzer ; import pandas as pd\nimport csv\nfrom datetime import datetime, timedelta, time\nfrom google.colab import drive\nimport sys\nimport holidays\nimport pandas_market_calendars as mcal\n#This code merges the output files of SMLSMgetpanel.py and SMLSMprocessraw.py\n#Function to concatenate text\ndef concatenate_text(g):\nreturn ' '.join(g.Text)\n\n#Function to count words of text using regex\ndef count_words(Text):\nword_count = len(re.findall(r'\\w+', Text))\nreturn word_count\n# Function to check whether date is on a non-trading day or news article is published later than 4pm. If so, then return next trading day.\nholidaysUS = holidays.US()\nnyse = mcal.get_calendar('NYSE')\nstock_holidays = nyse.holidays()\nstock_holidays = list(pd.to_datetime(stock_holidays.holidays))\nstock_holidays = [x.date() for x in stock_holidays]\ndef check_tradingdayhour(day):\ntrading_day = day\nif trading_day.hour >= 16:\ntrading_day += timedelta(1)\n#Check if news article published on weekend. If so return monday. If day on holiday return next trading day\nwhile trading_day.weekday() in holidays.WEEKEND or trading_day in stock_holidays:\nreturn trading_day\ndata_text = pd.read_csv(\"DATA FILE PATH OF SMLSMprocessraw.py output\")\ndata_text[\"Date\"] = pd.to_datetime(data_text[\"Date\"])\ndata_text[\"Date\"] = [check_tradingdayhour(x) for x in data_text[\"Date\"]]\ndf[\"Date\"] = pd.to_datetime(df[\"Date\"])\ndf[\"Date\"] = [check_tradingdayhour(x) for x in df[\"Date\"]]\n#Merge rows with same ticker symbol and date\n#This can be done either for the processed text or unprocessed text. Adjust variable in concatenate_text function\nnew_df = data_text.groupby([\"Date\", \"Ticker\"]).apply(concatenate_text).to_frame(name = \"c_Text\")\ndf = df.merge(new_df, how= \"left\", left_on = [\"Date\", \"Ticker\"], right_on = [\"Date\", \"Ticker\"])\ndf[\"c_Text\"] = df[\"c_Text\"].fillna(\" \")\ndf[\"word_count\"] = [count_words(x) for x in df.c_Text]\ndf.drop_duplicates(inplace = True)\ndf = df.rename(columns={'c_Text': 'Text'})\ndf_columns = [\"Date\", \"Ticker\", \"Nasdaq\", \"Turnover\", \"Size\", \"BTM\",\"pref_alpha\", \"Text\", \"word_count\", \"AR\", \"Return\"]\ndf = df[df_columns]\n", "output_sequence": "This folder contains the quantlet SMLSMmergedata which merges the output file of quantlet SMLSMgetpanel and quantlet SMLSMprocessraw for the master thesis Supervised Machine Learning Sentiment Measures"}, {"input_sequence": "Author: Alexander Hoelzer ; import csv\nimport sys\nfrom scipy.sparse import csc_matrix, vstack\nfrom sklearn.ensemble import RandomForestRegressor\nimport pandas as pd\nimport numpy as np\n#Code and functions such as sparse_mat, get_ngrams and get_sparsematrix_and_car functions based on online appendix of paper by Frankel, Jennings and Lee (2021)\n#Function creates a memory efficient sparse matrix format. Input is the dictionary returned by n_grams function.\ndef sparse_mat(data):\nrow1 = []\n#Iterate through dictionary from get_sparsematrix_and_car to create sparse matrix (value in interation are all ngram counts of observation 'key').\nfor key, value in data.items():\nvalue_n = list(value.values())\nfor e, elem in enumerate(value_n):\ncolnum = e\nrow1.append(key)\nX = csc_matrix((data1, (row1, col1))) # Sparse matrix of rows (observations) and columns (independent variables)\nreturn X\n#Function to get one- and two-gram counts of text documents. This function will iterate over all text items in the dataset and return all unique one- and twograms.\n#Code from paper Frankel, Jennings and Lee (2021) (modified for own needs)\ndef get_ngrams(data):\nonegrams = []\nfor index, row in data.iterrows():\nsentences = row[\"Text\"].split('.')\n#### EXTRACT ALL ONE AND TWO WORD PHRASES #### Frankel, Jennings and Lee (2021)\nfor sentence in sentences:\nsentence = sentence.replace('.', '').strip()\nallwords = sentence.split(' ')\nfor w, word in enumerate(allwords):\nword0 = allwords[w]\ntry:\nword1 = allwords[w + 1]\nexcept Exception:\nword1 = ''\nif word0.strip() != '.' and word0.strip() != '':\nonegrams.append(word0)\nif word1.strip() != '.' and word1.strip() != '':\ntwogram = word0 + ' + word1\nn_grams_dict = {}\nuniqueonegrams = list(set(onegrams))\nuniquetwograms = sorted(uniquetwograms)\nngrams = uniqueonegrams + uniquetwograms\nreturn ngrams\n#The function get_sparsematrix_and_car uses the get_ngrams and sparse_mat function to create inputs for the Random Forest algorithm\n#The inputs of the function are either twice the training dataset or once the training dataset and once the test dataset.\n#The function first extracts all one- and two-grams of the training dataset (the first innput) via the get_ngrams function. Next, the function iterates over each text document of the second input data file to count occurences of one- and two-grams found in the first input file.\ndef get_sparsematrix_and_car(data_train, data_test):\n#Get one- and two-grams of data_train\nngram_list = get_ngrams(data_train)\nwrd_list = ngram_list\n#Initialize dependent variable list (CAR)\ncar = []\n#Initialize dictionary with pre-determined number of txt files. Later used for memory reasons.\nwrd_dictionary = dict.fromkeys(range(67))\ni = 0\nfor index, row in data_test.iterrows():\ncar.append(row[\"AR\"])\nprint(j)\n# Initialize dictionary within dictionary with keys according to all ngrams found in training dataset.\nwrd_dictionary[i] = dict.fromkeys(wrd_list, 0)\n#Count one- and two-grams found in data_train within data_test\n# Add count of found ngrams occurence to dictionary\nif word0 in wrd_dictionary[i].keys():\nwrd_dictionary[i][word0] = wrd_dictionary[i][word0] + 1\nif word0 + ' + word1 in wrd_dictionary[i].keys():\nwrd_dictionary[i][word0 + ' + word1] = wrd_dictionary[i][word0 + ' + word1] + 1\ni += 1\n\n#The following code creates a memory-efficient sparse matrix format every 67 observation and then vstacks them. Necessary due to computational capacities.\n#i and j necessary due to RAM overload. i serves as marker to create a sparse matrix every 67 observations.\n#j serves as marker to concatenate the sparse matrices and to stop the for loop when iterated of all items in data_test. In last iteration all empty key-value pairs of the dictionary need to be deleted\n#Attention: possible error can pop up when training or test dataset can be divided by 67, just replace 67 by any other number\nif j == len(data_test):\nkeys_to_remove = (j % 67)\nfor key in range(keys_to_remove, 67):\ndel wrd_dictionary[key]\nspar_mat_i = sparse_mat(wrd_dictionary)\nspar_mat = vstack((spar_mat, spar_mat_i))\nbreak\nif i % 67 == 0:\nif j != 67:\nspar_mat = vstack((spar_mat, spar_mat_i))\nwrd_dictionary = dict.fromkeys(range(67))\ni = 0\nspar_mat = spar_mat_i\nreturn spar_mat, car\n#Function to split dataset based on months.\ndef split_months(dt):\nreturn [dt[dt[\"ordered_month\"] == y] for y in dt[\"ordered_month\"].unique()]\ndata = pd.read_csv(\"DATA FILEPATH\")\n#Get rid of all observations without news articles\ndata_onlytext = data.dropna()\n#Prepare data for sliding window approach. Ordered Month dataframe column has value 1 for first month of dataset and e.g. 20 for 20th month of dataset\ndata_onlytext[\"Date\"] = pd.to_datetime(data_onlytext[\"Date\"])\ndata_onlytext[\"ordered_month\"] = [((x[1][\"Year\"]-2015)*12 + x[1][\"Month\"]) for x in data_onlytext.iterrows()]\ndata_splt_months = split_months(data_onlytext)\ni = -1\n#Sliding window approach\nfor _, month in enumerate(data_splt_months):\n\nnp.random.seed(9000)\ndata_train = pd.concat([data_splt_months[i], data_splt_months[i+1],\ndata_test = data_splt_months[i+3]\n#Get sparse matrices as input for ML models\nX_train, = get_sparsematrix_and_car(data_train, data_train)\n#Random Forest algorithm\nrf = RandomForestRegressor(n_estimators=1000, max_features='sqrt', n_jobs=-1)\nrf = rf.fit(X_train, y_train)\npred = rf.predict(X_test).tolist()\n#References:\n#Frankel, R., Jennings, J., and Lee, J. (2021). Disclosure sentiment: Machine learning vs. dictionary methods. Management Science\n", "output_sequence": "This folder contains 1 quantlet estimating sentiment measures based on the Random Forest algorithm with one- and two-gram as inputs for the master thesis Supervised Machine Learning Sentiment Measures"}, {"input_sequence": "Author: NW ; %% Kelly Bernoulli plots the optimal betting fraction given Bernoulli trials\n% First code in 2012\n%% Global commands\n% ver % verify the version of Matlab\nclear;clc; % clear\n% set global commands for font size and line width\nsize_font=14;\nset(0,'DefaultAxesFontSize',size_font,'DefaultTextFontSize',size_font);\nset(0,'defaultlinelinewidth',size_line)\n%% Kelly, classic\np=0.6; %winning prob\ng=@(f) (p*log(1+f)+(1-p)*log(1-f));\ng_neg=@(f) -(p*log(1+f)+(1-p)*log(1-f)); %negative, due to min algo\nmin=fminbnd(g_neg,0,1); % min bounds for f [0,1]\nh=figure();\nsubplot(1,2,1)\nezplot(g)\ntitle('');hold on;\nplot(min,g(min),'d')\nxlim([0 1])\ngrid;\nsubplot(1,2,2)\nxlim([0 (2*p-1)*2])\nylim([0 g(min)+0.003])\nhold off;\n%print(h,'-depsc','-r300','kelly_bernoulli')\n%% Kelly classic g(f,p)\nclear;\ng=@(f,p) (p*log(1+f)+(1-p)*log(1-f));\nk=@(f,p) 0;\nezsurf(g,[0,1,0,1]);\ntitle('')\n%ezsurf(k,[0,1,0,1])\n%print(h,'-depsc','-r300','kelly_bernoulli2')\n%{\nmymap = [[204 0 0]\n[255 0]\n[0 204 102]]./255;\ncolormap(mymap)\ncaxis([-2 1])\n%}\n", "output_sequence": "Kelly_Bernoulli illustrates growth-optimal betting under repeated Bernoulli trials"}, {"input_sequence": "Author: Mike Ellington ; % Estimate excess return of Apple Stock on S&P500 Return using Bayesian\n% regression with independent Normal inverse-Gamma prior and posterior\n% Introduction to Gibbs Sampling\nclc; clear;\naddpath('../src');\nseed=181219;\nrng(seed);\n% Set number of simulations and burn-in draws\nNsim=100000; burn=10000;\n% GET DATA on APPLE return, FF Market Risk Premium, and RF\n% Now Data in terms of excess return of Apple, and market risk premium\n% Data from January 2000-December 2018\ndata=xlsread('Example_1_data','Sheet1','B2:D229');\nR=data(:,1)-data(:,3);\nMKT=data(:,2);\nclear data;\n% PLOT THE DATA\nyearlab=(2000:(1/12):2018+(11/12))';\nfigure(1)\nplot(yearlab,R,'r-','LineWidth',1.5)\nhold on,\nplot(yearlab,MKT,'b-','LineWidth',1.5)\naxis tight\nylabel('%')\nxlabel('Time')\ntitle('Monthly Excess Returns of Apple and Market Risk Premium')\nlegend('Apple','MKT','Location','SouthOutside')\n%matlab2tikz('dat_ex1.tex')\n% Now define things in terms of Y and X as in slides\n%\nY=R; X=[ones(length(Y),1), MKT];\n[T,N]=size(X);\n% prior\nb0=[0,0]';iVb=eye(2)/100;\nv0=3; S0=1*(v0-1);\n% initialise Markov Chain\nB=(X'*X)\\X'*Y; % OLS ESTIMATES\nBOLS=B;\nsig2=sum((Y-X*B).^2)/T;\nsigOLS=sig2;\nstore_theta=zeros(Nsim,3); % STORE BOTH PARAMETER DRAWS AND SIG2\nK=Nsim+burn;\n% start Gibbs sampler\ntic;\nfor k=1:K\n% sample B\nDb=(iVb + X'*X/sig2)\\speye(2); % quick way to take inverse\nBhat=Db*(iVb*b0+X'*Y/sig2);\nC=chol(Db,'Lower');\nB=Bhat+C*randn(N,1);\n% sample sig2\ne=Y-X*B;\nsig2=1/gamrnd(v0+T/2,1/(S0+e'*e/2));\nif k>burn\nksave=k-burn;\nstore_theta(ksave,:)=[B' sig2];\nend\ntoc;\nthetahat=mean(store_theta); thetaCI=quantile(store_theta,[0.025, 0.975]);\nfigure(2)\nsubplot(1,2,1)\nhistogram(store_theta(:,1),25,'Facecolor','k','Edgecolor','k');\nxline(BOLS(1),'r-','\\alpha_{OLS}','LineWidth',2);\nxline(thetaCI(1,1),'b--','LineWidth',2);\nylabel('frequency')\nxlabel('bins')\nsubplot(1,2,2)\nhistogram(store_theta(:,2),25,'Facecolor','k','Edgecolor','k')\nxline(BOLS(2),'r-','B_{OLS}','LineWidth',2);\nxline(thetaCI(1,2),'b--','LineWidth',2);\n%matlab2tikz('hist_ex1.tex')\nalldraws = store_theta;\n%The function momentg is taken from LeSage's toolbox\n%it inputs all Gibbs draws and produces posterior\n%mean, standard deviation, nse and rne\n%it calculates what the book calls S(0) in various ways\n%see momentg.m for more details\nresult = momentg(alldraws);\nmeans=[result.pmean]';\n%calculate Geweke convergence diagnostic based on first .1\n%and last .4 of draws\nidraw1= round(.1*Nsim);\nresult = momentg(alldraws(1:idraw1,:));\nmeansa=[result.pmean]';\nidraw2= round(.6*Nsim)+1;\nresult = momentg(alldraws(idraw2:Nsim,:));\nmeansb=[result.pmean]';\ncd = (meansa - meansb)./(nsea+nseb);\n", "output_sequence": "Monthly Excess Returns of Apple and Market Risk Premium and Posterior distribution of alpha and beta"}, {"input_sequence": "Author: Shi Chen ; # ------------------------------------------------------------------------------\n# Project: MTS - Modeling of Term Structure for Inflation Estimation\n# ------------------------------------------------------------------------------\n# Quantlet: MTS_expinf\n# Description: Model-implied estimation of inflation expectation (grey) for\n# five countries. The 3-year IE is red and 5-year is dashed blue.\n# Keywords: bond, plot, estimation, time-series, curve, interest-rate,\n# graphical representation, visualization\n# See also:\n# Author: Shi Chen\n## clear history\nrm(list = ls(all = TRUE))\ngraphics.off()\n## load datafile\nload(\"IEdata.RData\")\n## Plots\npar(mfrow = c(3, 2), pty = \"m\")\nukts1 = ts(IEdata$uk.pi, frequency = 12, start = c(2006, 6))\nplot(ukts1, ylim = c(-2, 5), col = \"grey\", main = \"U.K. IE\", lwd = 2)\nlines(ukts2, col = \"red\", lwd = 2)\nlines(ukts3, col = \"blue\", lty = 2, lwd = 2)\ndets1 = ts(IEdata$de.pi, frequency = 12, start = c(2009, 6))\nplot(dets1, col = \"grey\", ylim = c(-1, 2), main = \"Germany IE\", lwd = 2)\nlines(dets2, col = \"red\", lwd = 2)\nlines(dets3, col = \"blue\", lty = 2, lwd = 2)\nfrts1 = ts(IEdata$fr.pi, frequency = 12, start = c(2006, 6))\nplot(frts1, col = \"grey\", ylim = c(-1, 4), main = \"France IE\", lwd = 2)\nlines(frts2, col = \"red\", lwd = 2)\nlines(frts3, col = \"blue\", lty = 2, lwd = 2)\nitts1 = ts(IEdata$it.pi, frequency = 12, start = c(2007, 6))\nplot(itts1, col = \"grey\", ylim = c(-1, 4), main = \"Italy IE\", lwd = 2)\nlines(itts2, col = \"red\", lwd = 2)\nlines(itts3, col = \"blue\", lty = 2, lwd = 2)\nswts1 = ts(IEdata$sw.pi, frequency = 12, start = c(2007, 4))\nplot(swts1, col = \"grey\", ylim = c(-1, 4), main = \"Sweden IE\", lwd = 2)\nlines(swts2, col = \"red\", lwd = 2)\nlines(swts3, col = \"blue\", lty = 2, lwd = 2)\n## Plot: comparison\npar(mfrow = c(1, 1), pty = \"m\")\nplot(ukts2, lty = 3, lwd = 3, col = \"red\", ylim = c(-1, 4), ylab = \"Inflation expectation\")\nlines(dets2, lty = 2, col = \"blue\", lwd = 3)\n", "output_sequence": "Compares the estimated three-year and five-year forecast of IE for each European country. The inflation expectation (IE) is estimated by model-implied BEIR and plotted in gray."}, {"input_sequence": "Author: Alisa Kolesnikova ; rm(list = ls(all = TRUE))\noptions(scipen = 999)\n# Load data\nload(\"vcrixdata.RData\")\n# install and load packages\nlibraries = c(\"lubridate\", \"zoo\", \"reshape2\", \"plyr\", \"MTS\", \"ggplot2\")\nlapply(libraries, function(x)\nif (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries,\nlibrary,\nquietly = TRUE,\ncharacter.only = TRUE)\n# Prepare data\ndata2 = data[, c(\"crypto_symbol\", \"price_usd\", \"date\")]\ndata2$date = as.Date(data2$date)\nweights$date = as.Date(weights$date)\n# Ensure correct variable names\ncolnames(weights) = c(\"crypto\", \"weights\", \"date\")\n# Select relevant components\ndata = data2[data2$crypto %in% weights$crypto, ]\nw = weights[order(weights$crypto), ]\n# Transform into time series format\nwtable = reshape( # for weights\nw,\nv.names = \"weights\",\nidvar = \"date\",\ntimevar = \"crypto\",\ndirection = \"wide\"\n)\nptable = reshape( # for prices\np,\nv.names = \"price\", # names of variables in the long format\nidvar = \"date\", # indicating time variable\nptable = ptable[order(ptable$date), ] # alphabetic ordering of cryptos\n# Ensure there are no NAs\nptable[, -1] = na.locf(ptable[, -1])\n# Convert prices into returns\nret = function(x) {\ndiff(log(x))\n}\nreturns = colwise(ret)(ptable[, -1])\nreturns$date = ptable$date[-1]\nreturns$dat = as.yearmon(returns$date, \"%y-%m\")\nwtable$date = NULL\n# Merge price and weight tables into the main dataset\nts = merge(returns, wtable, by = \"dat\")\nts$dat = NULL\n# Double check for missing values to avoid NA in var-covar matrix estimation\nts[, grepl(\"price\", names(ts))] =\nis.na(ts) = do.call(cbind, lapply(ts, is.infinite))\nts[is.na(ts)] = 0 # otherwise EWMA will return NAs\n# Estimating variance matrix with EWMA\nelem =\nlength(grep(x = colnames(ts), pattern = \"price\")) # number of cryptos\n# select lambda\nvolaest = EWMAvol(ts[, grepl(\"price\", names(ts))], lambda = 0.82)\n# estimation takes around takes 5 min\n# reorganise the list of var-covar matrices\nvol = volaest[1]\nv = vol[[1]]\nvar = c(1:nrow(ts))\nvv =\nlapply(1:nrow(v), function(x)\nmatrix(\nv[x, ],\nnrow = elem,\nbyrow = TRUE\n))\nww =\nas.matrix(ts[, grep(x = colnames(ts), pattern = \"weights\")]) # selecting weights\n# Plugging weights and var-covar matrix into formula\nfor (i in 1:nrow(ts)) {\nvar[i] = as.matrix(t(ww[i, ])) %*% vv[[i]] %*% ww[i, ]\n# Assembling vcrix dataset\nindex = data.frame(\"vola\" = sqrt(var), \"date\" = ts$date)\nindex$vcrix = c(1:nrow(index))\nindex$day = lubridate::day(index$date)\n# Setting the base value of VCRIX to 1000\nindex$vcrix[1] = 1000\nindex$divisor[1] = index$vola[1] / index$vcrix[1]\n# Calculating divisor\nfor (i in 2:nrow(index)) {\nif (index$day[i] == 1) {\nindex$vcrix[i] = index$vcrix[i-1]\nindex$divisor[i] = index$vola[i]/index$vcrix[i]\n} else {\nindex$divisor[i] = index$divisor[i-1]\nindex$vcrix[i] = index$vola[i]/index$divisor[i]\n}\n# Plot VCRIX\nplot(\nindex$date,\ntype = \"l\",\ncol = \"blue\",\nlwd = 2,\nxlab = \"Date\",\n", "output_sequence": "Contains the data preparation and the EWMA based derivation of VCRIX, volatility index for crypto-currencies on the basis of CRIX."}, {"input_sequence": "Author: Lara Vomfell ; # helper functions for visualisation\nrequire(extrafont)\n\n# enable ghostscript to embed fonts\nSys.setenv(R_GSCMD = \"C:/Program Files/gs/gs9.21/bin/gswin32c.exe\")\nloadfonts()\nplot_theme = function(...){\ntheme(text = element_text(family=\"Palatino Linotype\",\nsize=14,\naxis.title = element_text(size = rel(1.25)),\npanel.background = element_rect(fill=\"white\",\ncolour=NA),\npanel.border = element_rect(fill=NA,\ncolour=\"grey40\"),\npanel.grid.major = element_line(colour=\"grey92\",\nsize=0.25),\npanel.grid.minor = element_line(colour = \"grey92\",\nsize = 0.25),\nstrip.background = element_rect(fill=\"grey85\",\ncolour=\"grey20\"),\nlegend.key=element_rect(fill=\"white\",\ncolour=NA),\naxis.text = element_text(colour = \"black\"),\n...)\n}\ntheme_map = function(...) {\ntheme_minimal() +\ntheme(\naxis.line = element_blank(),\nplot.margin = unit(c(0,0,0,0),\"mm\"),\ntext=element_text(size=16, family=\"Palatino Linotype\"),\n...\n)\nis_outlier = function(x, quantil, above = T) {\nif (above == T){\nreturn(x > quantile(x, 1-quantil, na.rm=T) +\n1.5 * (quantile(x, 1-quantil, na.rm=T) - quantile(x, na.rm=T)))\n} else {\nreturn(x < quantile(x, 0.25, na.rm=T) -\n1.5 * (quantile(x, 1-quantil, na.rm=T) - quantile(x, na.rm=T)) |\nx > quantile(x, 1-quantil, na.rm=T) +\n}\nsavemap = function(name, plots, width=1400, height=2100){\nfname = paste0(\"C:/Users/Lara/Dropbox/Uni/thesis/thesis/thesis/Figures/\",name,\".png\")\npng(fname, width = width, height = height, res=120)\nplot(plots)\ndev.off()\nsaveplot = function(name, plots, width=965, height=600){\n# for each setting, create scatterplot of coefficients and save them\nscatterplots = function(results, setting, file, crimetype, out = F){\n# check crime types\nstopifnot(crimetype %in% c(\"violent\", \"property\"))\n\n# create and save plot for each setting\nfor (j in 1:length(setting)){\n# create data\nscatter_data = scatterplot_preparation(results,\nsetting = setting[j],\n# create basic point plot with minimal jittering\np = ggplot(scatter_data, aes(x=variables, y=plot)) +\ngeom_point(aes(color = factor(model)), size = 3.3, position=position_dodge(width=0.55)) +\nplot_theme(axis.title.x=element_blank(),\ntheme(legend.position=\"top\",\nplot.margin=unit(c(1,3.5,1,1), \"lines\"),\naxis.text = element_text(size = 16),\nscale_color_manual(values = c(\"#FCA007\", \"#D64B40\", \"#C20093\", \"#66277C\", \"#000004\"),\nguide = guide_legend(title = \"Models\",\ntitle.position = \"top\",\nlabels=c(\"CAR\", \"GLM\", \"SAR\")) +\ncoord_flip()\n#print(p)\n# set offset for labels\nif (crimetype == \"violent\"){\noffset = c(5.181118, 5.200335, 5.752347, 17.4157, 5.220357, 6.97816, 5.751898, 6.038212)\n} else if (crimetype == \"property\"){\noffset = c(9.736319, 7.493419, 2.983404, 5.206568, 3.578831,3.89061, 5.953528, 2.627495)\n}\n# ## can be optimised\n# ranges = ggplot_build(p)$layout$panel_ranges[[1]]$x.range\n#\n# offset = ranges[2] + (ranges[2] / abs(ranges[1])) * 0.1\n# if (j %in% c(4,7,8) & crimetype == \"property\") offset = offset - 0.2\n# add annotations: number of models the coef is significant in\n# for (i in 1:length(unique(scatter_data$variables))){\n# p = p + annotation_custom(\n# grob = textGrob(label = scatter_data$Freq[i], hjust = 0, gp = gpar(fontfamily =\"Palatino Linotype\",\n# size=12)),\n# xmin = scatter_data$variables[i],\n# ymin = offset[j],\n# }\n# Code to override clipping\n# gt = ggplot_gtable(ggplot_build(p))\n# gt$layout$clip[gt$layout$name == \"panel\"] = \"off\"\n#grid.draw(gt)\n# show plot (optional)\nif (out) p#grid.draw(gt)\n# save plot\nname = paste0(crimetype, \"_coefficients\", setting[j], \"experimental\")\nsaveplot(name=name, plots=p, width=1200, height=740)\nsaveggpairs = function(name, plots, width=965, height=600){\nfname = paste0(\"C:/Users/Lara/Dropbox/Uni/thesis/thesis/thesis/Figures/\", name, \".png\")\npng(fname, width = width, height = height, res = 120)\nprint(plots)\nPOI_plots = function(layer, category){\n# create plot for every category considered\nfor (i in 1:length(category)){\n# create upper-case title\ntitle = paste0(toupper(substr(category[i], 1, 1)),\nsubstr(category[i], 2, nchar(category[i])))\ncol = layer[, colnames(layer) == category[i]]\nmy_breaks = seq(min(col, na.rm=T), max(col, na.rm=T), by = 10)\nif (max(col, na.rm=T) > max(my_breaks)) my_breaks = c(my_breaks, + 10)\n# make plot\np = ggplot() + geom_polygon(data = layer, aes(x = long, y = lat,\ngroup = group,\ncolour = \"black\", size = 0.25) +\nlabs(x=NULL, y=NULL) +\ntheme_map() +\n#scale_x_continuous(expand = c(0, 0), limits = c(-74.04190, -73.70001)) +\nscale_fill_viridis(option = \"B\", direction = -1,\nbreaks = my_breaks,\nguide = guide_colorbar(direction = \"horizontal\",\ntitle.position = 'top',\ntheme(legend.position = \"bottom\",\nplot.margin=unit(c(0,0,0,0),\"mm\"))\nplotname = paste0(\"POI_\", title)\nsavemap(plotname, p, width = 600, height=900)\nscatterplot_preparation = function(results, setting, file){\ncoefs = lapply(1:length(file), function(x) results[[setting]][[x]]$coefficients)\ncoef_frame =\nt(do.call(rbind, coefs)) %>%\nas.data.frame(.) %>%\nset_colnames(names(results[[setting]])) %>%\n# set model names as variable\nmutate(variables = row.names(.)) %>%\ngather(., variables) %>%\nset_colnames(c(\"variables\",\"model\",\"value\"))\nSE_frame =\nt(do.call(rbind, SE)) %>%\nas.data.frame %>%\nset_colnames(c(\"variables\",\"model\",\"pvalue\"))\n# combine SE_frame and coef_frame\nfinal_data = data.frame(variables=coef_frame$variables,\nmodel=coef_frame$model,\n# set non-significant coefficients to NA\nfinal_data$plot = ifelse(final_data$pvalue < 0.05, final_data$value, NA)\n# count number of significant variables\nfinal_data =\ndata.frame(table(final_data$variables, is.na(final_data$plot))) %>%\nfilter(Var2 == F) %>%\nmutate(Freq = paste0(\"N = \", Freq)) %>%\nselect(Var1, Freq) %>%\nleft_join(final_data, ., by = c(\"variables\" = \"Var1\")) %>%\nmutate(helperstar = ifelse(grepl(\"0\", Freq), \"*\", \"\"),\nvariables = paste0(variables, helperstar),\nlevels = rev(variables[1:length(unique(variables))]))) %>%\nselect(-helperstar)\nreturn(final_data)\ntaxi_map = function(mapdata, taxidata, type, alpha_range = c(0.5, 0.95), size_range = c(0.134, 0.173),\nlow = \"#374c7c\", high = \"#171327\", save = T){\nif (type %in% c(\"pickup\", \"dropoff\")){\nx_coord = paste0(type, \"_long\")\n} else stop(\"Type must be one of: pickup, dropoff\")\np = ggplot() +\ngeom_polygon(data = mapdata, aes(x = long, y = lat, group = group), fill = \"grey50\") +\ngeom_point(data = taxidata, aes(x = get(x_coord), y = get(y_coord),\nalpha = count, size = count, color = count)) +\nscale_alpha_continuous(range = alpha_range, trans = \"log\", limits = range(taxidata$count)) +\nscale_color_gradient(low = low, high = high, trans = \"log\") +\ncoord_map(xlim = range(mapdata$long), ylim = range(mapdata$lat)) +\ntheme_map(legend.position=\"none\")\nif (save) savemap(paste0(type), p)\nreturn(p)\n", "output_sequence": "Takes a data file with census variables and another file with crime data to create correlation plots and maps"}, {"input_sequence": "Author: Lara Vomfell ; library(lubridate)\nlibrary(data.table)\nlibrary(dplyr)\nlibrary(viridis)\n\nsource(\"helper_visualisation.R\")\nload(\"final_frame_property15.Rdata\")\nfinal_frame_property[, violent := final_frame_violent[,occ]]\n# remove columns not needed for plotting\nfinal_frame_property[, c(\"gid\", \"week\",\n\"young_male\", \"white\", \"institution\",\n\"tweet_counts\", \"night_tweets\",\n\"ny\", \"kings\", \"bronx\", \"queens\",\n\"taxi1\",\n\"outdoorsA\", \"shopsA\", \"travelA\",\n\"residentialA\", \"entertainmentA\",\n\"uniA\", \"foodA\", \"professionalA\",\n\"nightlifeA\", \"H\", \"NTACode\") := NULL]\nsetcolorder(final_frame_property, c(\"occ\", \"violent\", \"log_night\", \"taxi\", \"population\", \"age\",\n\"male\", \"black\", \"asian\", \"hispanic\", \"vacancy\", \"female_head\",\n\"entertainment\", \"uni\",\"food\", \"professional\", \"nightlife\",\n\"outdoors\", \"shops\", \"travel\", \"residential\"))\nsetnames(final_frame_property, c(\"occ\", \"taxi\"), c(\"property\", \"taxi_dest\"))\np = ggpairs(final_frame_property,\ncolumns = names(final_frame_property),#[-c(13:ncol(final_frame_property))],\ndiag = list(continuous = wrap(\"barDiag\")),\nupper = list(continuous = wrap(\"cor\", size = 8, family = \"Palatino Linotype\",\ncolour=\"black\")),\nlower = list(continuous = wrap(\"points\", color = \"#595959\"))) +\nplot_theme()\nsaveggpairs(\"corrs_LALA\", p, width = 2500, height = 1500)\n# READ MAP DATA ------------------------------------------------\n# read shapefile\ntracts = sp:::spTransform(readOGR(\"D:/crime-data/nyc-taxi-data/nyct2010_15b/nyct2010.shp\", layer = \"nyct2010\"), CRS(\"+proj=longlat +datum=WGS84\"))\n# fortify as data.frame\ntracts@data$id = as.numeric(rownames(tracts@data)) + 1\nNY_layer = fortify(tracts, region = \"id\")\nNY_layer$id = as.numeric(NY_layer$id)\nNY_layer = inner_join(NY_layer, tracts@data, by = \"id\")\n# match to ID_mapping\nNY_layer = left_join(NY_layer, tractID, by = c(\"id\" = \"gid\"))\nrm(tracts)\npairs(census[, .(population, male, black, white, female_head)], # not happy with col selection yet\npch=\".\", # sets small values\nupper.panel = panel.cor, # print correlation\ndiag.panel = panel.hist, # show histogram\nlower.panel = panel.smooth,\ncol.smooth = \"red\",\nspan = 0.3) # shows lines, not happy with that\n# PLOT DATA -------------------------------------------------------\nplotData = left_join(NY_layer, census, by = c(\"id\" = \"gid\"))\n# plot map of racial differences excl. staten island\nplotData = plotData[plotData$BoroName != \"Staten Island\",]\np = ggplot() +\ngeom_polygon(data = plotData, aes(x = long, y = lat, group = group,\nfill = asian), color = \"black\", size = 0.25) +\nscale_fill_viridis(option = \"magma\", direction = -1, name = \"Percentage of Asian Population\",\nlabels = percent, limits = c(0,1), breaks = c(0, 0.25, 1),\nguide = guide_colorbar(direction = \"horizontal\",\ntitle.position = 'top',\nlabs(x=\"\", y=\"\") +\ntheme_map() +\ntheme(legend.position = \"bottom\") +\ncoord_map()\n\np\n# This part creates the neighbourhood plot\n# function converts nb object to a data.frame\nnb_to_df = function(nb, coords){\nx = coords[, 1]\nn = length(nb)\ncardnb = card(nb)\ni = rep(1:n, cardnb)\nj = unlist(nb)\nreturn(data.frame(x=x[i], xend=x[j],\ny=y[i], yend=y[j]))\n}\n# load map data\ncensus = read.csv(\"D:/crime-data/2010_NYC_Census_cleaned.csv\", stringsAsFactors = F)\n# read polygons to create neighbour matrix\ntracts = sp:::spTransform(readOGR(\"D:/crime-data/nyc-taxi-data/nyct2010_15b/nyct2010.shp\",\nlayer = \"nyct2010\"), CRS(\"+proj=longlat +datum=WGS84\"))\n# exclude Farway Rock and City Island\ntracts = subset(tracts, tracts@data$id %in% census$gid & !(tracts@data$id %in% c(1263, 2023, 2116)))\ncensus = census[rowSums(is.na(census)) == 0,]\ntracts = subset(tracts, tracts@data$id %in% census$gid)\nNY_layer$census = ifelse(NY_layer$id %in% census$gid, 1, NA)\n# create neighbours list\nneighbours = poly2nb(tracts, row.names = tracts$id)\ntrueCentroids = as.data.frame(gCentroid(tracts, byid=TRUE))\n# create distance-based neighbors\nnb_df = nb_to_df(neighbours, trueCentroids)\n# create plot\nlinkmap = ggplot() +\ngeom_polygon(data = NY_layer, aes(x = long, y = lat, fill = census, group = group),\ncolor = \"black\", size = 0.25) +\ncoord_map() +\nscale_x_continuous(expand = c(0, 0)) +\ntheme_map() +\nscale_fill_continuous(low=\"white\",\nhigh=\"white\",\nna.value=\"grey50\", guide=F) +\ngeom_segment(aes(x=x, xend=xend, y=y, yend=yend),\ndata = nb_df, colour = \"black\")\nsavemap(\"linkmap\", linkmap)\n", "output_sequence": "Takes a data file with census variables and another file with crime data to create correlation plots and maps"}, {"input_sequence": "Author: ['Min-Bin Lin'] ; import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport plotly\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n# define score function\ndef t_score(eps,v,sigma2):\nreturn ((v+1)/v)*(eps)/(sigma2 + np.power(eps,2)/v)\nnu_1=np.linspace(-5,5),t_score(np.linspace(-5,5),4,1)\nfig = make_subplots(specs=[[{\"secondary_y\": False}]])\nfig['layout'].update(height=800, width=800,\ntitle='',\nshowlegend=False,\nfont=dict(family='Times New Roman', size=24))\nfig.add_trace(go.Scatter(\ny=nu_1[1],\nline=dict(color='red', width=3)\n), secondary_y=False)\ny=nu_2[1],\nline=dict(color='blue', dash='dash', width=3)\nfig['layout']['xaxis'].update(title='e (residual)')\nfig.update_xaxes(range = [-6,6],showline=True, linewidth=1, linecolor='black', mirror=True,\ntickformat=\"%b\\n%Y\", showgrid=False)\nfig.update_yaxes(range = [-6,6],showline=True, linewidth=1, linecolor='black', mirror=True, showgrid=False)\nfig.update_layout({'plot_bgcolor': 'rgba(0,0,0,0)',\n'paper_bgcolor': 'rgba(0,0,0,0)'},\nfont_color='black',\nbargap=0.1)\nfig.add_shape(type='line', line=dict(dash='dot', color='black'), xref='paper', y0=0, y1=1, x0=0, x1=1)\n# fig.add_hline(y=0, line_dash='dot', line_color='black')\n# fig.show()\n# fig.write_image('./score.pdf')\n", "output_sequence": "illustration of score and degree of freedom."}, {"input_sequence": "Author: ['Min-Bin Lin'] ; import numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom statsmodels.tsa.stattools import grangercausalitytests\n# load data\ndai = pd.read_csv('index_df.csv')\ncrix = pd.read_csv('new_crix.csv').rename(columns={'price':'crix'})\neth = pd.read_csv('ETH-USD.csv').rename(columns={'Date':'date', 'Adj Close':'eth', 'Volume':'eth_vol'}).drop(columns=['Open','High', 'Low', 'Close'])\ndf_price = pd.merge(dai, crix, on='date')\ndf_return = df_price.apply(lambda x: x.div(x.shift(1))-1)\nfig = make_subplots(rows=2, cols=1,\nvertical_spacing=0)\nfig['layout'].update(height=800, width=1600,\ntitle='',\nshowlegend=False,\nfont=dict(family='Times New Roman', size=20))\nfig.add_trace(go.Scatter(x=df_return.index,\ny=df_return['hh_index'],\nline=dict(color='blue')),\nrow=1, col=1)\ny=df_return['crix'],\nrow=2, col=1)\nfig.update_xaxes(showline=True, linewidth=1,\nlinecolor='black',\ntickformat=\"%b\\n%Y\",\nshowgrid=False)\nfig.update_yaxes(showline=True, linewidth=1,\nmirror=True,\n# Update xaxis properties\nfig.update_xaxes(showticklabels=False, row=1, col=1)\nfig.update_xaxes(title='Date', row=2, col=1, dtick = 'M3')\n# Update yaxis properties\nfig.update_yaxes(title='',\nrow=1, col=1)\nfig.update_layout({'plot_bgcolor': 'rgba(0,0,0,0)',\n'paper_bgcolor': 'rgba(0,0,0,0)'},\nfont_color='black')\ndef create_corr_plot(series, date, save_path, plot_pacf=False):\ncorr_array = pacf(series.dropna(), alpha=0.05, nlags=40) if plot_pacf else acf(series.dropna(), alpha=0.05, nlags=40)\nlower_y = corr_array[1][:,0] - corr_array[0]\n\nfig = make_subplots(rows=2, cols=1, vertical_spacing=0.18)\n\nfig.add_trace(go.Scatter(y=series,\nx=date,\nline=dict(color='blue', width=1)\n),\nfig['layout'].update(height=800, width=1200,\n[fig.add_trace(go.Scatter(y=(0,corr_array[0][x]),\nx=(x,x),\nline=dict(color='black', width=2)\nrow=2, col=1)\nfor x in range(len(corr_array[0]))]\nfig.add_trace(go.Scatter(y=corr_array[0],\nx=np.arange(len(corr_array[0])),\nmode='markers',\nmarker=dict(color='blue', size=8)\nrow=2, col=1)\nfig.add_trace(go.Scatter(y=upper_y,\nline=dict(color='rgba(173, 216, 230,0.4)', width=1)\nfig.add_trace(go.Scatter(x=np.arange(len(corr_array[0])), y=lower_y,\nfillcolor='rgba(173, 216, 230,0.4)',\nfill='tonexty',\nline_color='rgba(173, 216, 230,0.4)'),\nfig.update_xaxes(title='Date', dtick = 'M3', row=1, col=1,)\nfig.update_xaxes(showline=True, linewidth=1,\nfig.update_layout({'plot_bgcolor': 'rgba(0,0,0,0)',\nfig.show()\ndef kpss_test(df):\nstatistic, p_value, n_lags, critical_values = kpss(df.values)\nprint(f'KPSS Statistic: {statistic}')\nprint(f'p-value: {p_value}')\nprint('Critial Values:')\nfor key, value in critical_values.items():\nprint(f' {key} : {value}')\ndef adf_test(df):\nresult = adfuller(df.values)\nprint('ADF Statistics: %f' % result[0])\nprint('Critical values:')\nfor key, value in result[4].items():\n### granger causality\ndef is_GrangerCause(data=None, maxlag=10):\n\"\"\"This function find if x2 Granger cause x1 vis versa \"\"\"\ngc = grangercausalitytests(data, maxlag=maxlag, verbose=False)\nfor i in range(maxlag):\nx=gc[i+1][0]\np1 = x['lrtest'][1] # pvalue for lr test\n\ncondition = ((p1 < 0.05 and p2 < 0.05) and (p3 < 0.05 and p4 < 0.05))\nif condition == True:\ncols = data.columns\nprint('Yes: {} Granger causes {}'.format(cols[0], cols[1]))\nprint('maxlag = {}\\nResults: {}'.format(i, x))\nbreak\nelse:\nif i == maxlag - 1:\ncols = data.columns\nis_GrangerCause(data = df_return[['dcs_index', 'eth_vol']])\nimport numpy as np\nimport seaborn as sns\nfrom scipy.spatial import distance\nclass ccm:\n\"\"\"\nchecking causality X -> Y\nArgs\nX: timeseries for variable X that could cause Y\ntau: time lag. default = 1\nE: shadow manifold embedding dimension. default = 2\nL: time period/duration to consider (longer = more data). default = length of X\ndef __init__(self, X, Y, tau, E, L=None):\n'''\ntau: time lag\nE: shadow manifold embedding dimension\nL: time period/duration to consider (longer = more data)\nWe're checking for X -> Y\nself.X = X\nif L == None:\nself.L = len(X)\nself.My = self.shadow_manifold(Y) # shadow manifold for Y (we want to know if info from X is in Y)\nself.t_steps, = self.get_distances(self.My) # for distances between points in manifold\ndef shadow_manifold(self, V):\n\"\"\"\nGiven\nV: some time series vector\ntau: lag step\nE: shadow manifold embedding dimension\nL: max time step to consider - 1 (starts from 0)\nReturns\n{t:[t, t-tau, t-2*tau ... t-(E-1)*tau]} = Shadow attractor manifold, dictionary of vectors\nV = V[:self.L] # make sure we cut at L\nM = {t:[] for t in range((self.E-1) * self.tau, self.L)} # shadow manifold\nv_lag = [] # lagged values\nfor t2 in range(0, self.E-1 + 1): # get lags, we add 1 to E-1 because we want to include E\nv_lag.append(V[t-t2*self.tau])\nM[t] = v_lag\nreturn M\n# get pairwise distances between vectors in the time series\ndef get_distances(self, M):\nArgs\nM: The shadow manifold from the time series\nt_steps: timesteps\ndists: n x n matrix showing distances of each vector at t_step (rows) from other vectors (columns)\nt_vec = [(k, v) for k,v in M.items()]\nt_steps = np.array([i[0] for i in t_vec])\ndists = distance.cdist(vecs, vecs)\nreturn t_steps, dists\ndef get_nearest_distances(self, t, t_steps, dists):\nArgs:\nt: timestep of vector whose nearest neighbors we want to compute\nt_teps: time steps of all vectors in the manifold M, output of get_distances()\ndists: distance matrix showing distance of each vector (row) from other vectors (columns). output of get_distances()\nE: embedding dimension of shadow manifold M\nReturns:\nnearest_timesteps: array of timesteps of E+1 vectors that are nearest to vector at time t\nt_ind = np.where(t_steps == t) # get the index of time t\ndist_t = dists[t_ind].squeeze() # distances from vector at time t (this is one row)\n# get top closest vectors\nnearest_inds = np.argsort(dist_t)[1:self.E+1 + 1] # get indices sorted, we exclude 0 which is distance from itself\nnearest_timesteps = t_steps[nearest_inds] # index column-wise, t_steps are same column and row-wise\nnearest_distances = dist_t[nearest_inds]\nreturn nearest_timesteps, nearest_distances\ndef predict(self, t):\nt: timestep at manifold of y, My, to predict X at same time step\nX_true: the true value of X at time t\nX_hat: the predicted value of X at time t using the manifold My\neps = 0.000001 # epsilon minimum distance possible\nt_ind = np.where(self.t_steps == t) # get the index of time t\ndist_t = self.dists[t_ind].squeeze() # distances from vector at time t (this is one row)\nnearest_timesteps, nearest_distances = self.get_nearest_distances(t, self.t_steps,\n# get weights\nu = np.exp(-nearest_distances/np.max([eps, nearest_distances[0]])) # we divide by the closest distance to scale\nw = u / np.sum(u)\n# get prediction of X\nX_true = self.X[t] # get corresponding true X\nX_cor = np.array(self.X)[nearest_timesteps] # get corresponding Y to cluster in Mx\nX_hat = (w * X_cor).sum() # get X_hat\n# DEBUGGING\n# will need to check why nearest_distances become nan\n# if np.isnan(X_hat):\n# print(nearest_timesteps)\nreturn X_true, X_hat\ndef causality(self):\nNone\n(r, p): how much X causes Y. as a correlation between predicted X and true X and the p-value (significance)\nX_true_list = []\nfor t in list(self.My.keys()): # for each time step in My\nX_true, X_hat = self.predict(t) # predict X from My\nX_true_list.append(X_true)\nx, y = X_true_list, X_hat_list\nr, p = pearsonr(x, y)\nreturn r, p\ndef visualize_cross_mapping(self):\nVisualize the shadow manifolds and some cross mappings\nf, axs = plt.subplots(1, 2, figsize=(24, 10))\nfor i, ax in zip((0, 1), axs): # i will be used in switching Mx and My in Cross Mapping visualization\n#===============================================\n# Shadow Manifolds Visualization\nX_lag, = [],\nfor t in range(1, len(self.X)):\nX_lag.append(self.X[t-self.tau])\nX_t, Y_t = self.X[1:], # remove first value\nax.scatter(X_t, X_lag, s=50, label='$M_x$',\ncolor='forestgreen')\nax.scatter(Y_t, Y_lag, s=50, label='$M_y$',\nc='#F49EC4')\n# Cross Mapping Visualization\nA, B = [(self.Y, self.X), self.Y)][i]\ncm_direction = ['Mx to My', 'My to Mx'][i]\nMa = self.shadow_manifold(A)\nt_steps_A, dists_A = self.get_distances(Ma) # for distances between points in manifold\n# Plot cross mapping for different time steps\ntimesteps = list(Ma.keys())\nfor t in np.random.choice(timesteps, size=3, replace=False):\nMa_t = Ma[t]\nnear_t_A, = self.get_nearest_distances(t, t_steps_A, dists_A)\nfor i in range(self.E+1):\n# points on Ma\nA_t = Ma[near_t_A[i]][0]\nax.scatter(A_t, A_lag, c='b', marker='s', s=150)\n# corresponding points on Mb\nB_t = Mb[near_t_A[i]][0]\nax.scatter(B_t, B_lag, c='r', marker='*', s=150)\n# connections\nax.plot([A_t, B_t], [A_lag, B_lag], c='r', linestyle=':')\n# ax.set_title(f'{cm_direction} cross mapping. time lag, tau = {self.tau}, E = 2')\n# ax.legend(prop={'size': 14})\nax.set_xlabel('$x{(t)}$, $y{(t)}$', size=30)\nax.patch.set_alpha(0)\nplt.show()\n# f.savefig('./cmm_ethvol_dai_2_2.pdf', format='PDF',facecolor=f.get_facecolor(), edgecolor='none')\ndef plot_ccm_correls(self):\nX: X time series\nNone. Just correlation plots between predicted X|M_y and true X\nX_My_true, = [],\nfor t in range(self.tau, self.L):\ntrue, pred = self.predict(t)\nX_My_true.append(true)\n# predicting X from My\nr, p = np.round(pearsonr(X_My_true, X_My_pred), 4)\nfig = plt.figure(figsize=(15, 15), dpi=80)\nplt.scatter(X_My_true, X_My_pred, s=30, c='blue')\nplt.xlabel('$x(t)$', size=30)\n# plt.title(f'tau={self.tau}, E={self.E}, Correlation coeff = {r}')\nplt.xlim([-0.5, 0.5])\nfrom matplotlib import rc\nfont = {'family':'serif','serif':['Times'],\n'size' : 30}\nccm1 = ccm(X=df_return['dcs_index'], Y=df_return['eth_vol'], tau=2, E=2)\nccm1.causality()\nccm1.visualize_cross_mapping()\ncorr_, p = ccm1.causality()\nX=df_return['dcs_index']\nY=df_return['eth']\nL_range = range(5, len(X), 10) # L values to test\ntau = 2\nXhat_My, = [], # correlation list\nfor L in L_range:\nccm_XY = ccm(X, Y, tau, E, L) # define new ccm object # Testing for X -> Y\nXhat_My.append(ccm_XY.causality()[0])\nprint('X->Y r', np.round(Xhat_My[-1], 2), 'p value', np.round(ccm_XY.causality()[1], 4))\n# plot convergence as L->inf. Convergence is necessary to conclude causality\nfig = make_subplots()\nfig['layout'].update(height=800, width=800,\nfont=dict(family='Times New Roman', size=24))\nfig.add_trace(go.Scatter(x=[*L_range],\ny=Xhat_My,\nline=dict(color='blue', width=2)))\ny=Yhat_Mx,\nline=dict(color='red',dash='dot', width=2)))\nfig.update_xaxes(title='L', dtick=50)\nfig.show()\nPOINTS = -1 ## Number of points to use, more can be slower to render.\n## -1 if all(but last).\nTAU = 3 ## Delay, integer\ntime_series = df_return['dcs_index'][:POINTS]\ndelay_coordinates1 = [\ntime_series[:-TAU if TAU else len(time_series)], # t-T\ntime_series[TAU:] # t\n]\ndelay_coordinates2 = [\ntime_series[TAU:-TAU if TAU else len(time_series)], # t-tau\ntime_series[2 * TAU:], # t\ntime_series[:-2 * TAU if TAU else len(time_series)] # t-2tau\n]\n\n## visulize embedding\nfig = make_subplots(rows=2, cols=2,\nvertical_spacing=0.1,\nspecs=[[{'type': 'xy', 'colspan': 2}, {'type': 'xy'}],\nrow_heights=[0.5,0.5]\n)\nfig['layout'].update(height=1000, width=1000,\nfont=dict(family='Times New Roman', size=14))\n# 3d scatter\nfig.add_trace(go.Scatter3d(x=delay_coordinates2[0],\ny=delay_coordinates2[1],\nmarker=dict(color='blue', size=5, opacity=1,\nline=dict(width=1, color='red'))),\nrow=2, col=2)\nfig.update_layout(template=\"none\", scene_camera_eye=dict(x=1.5, y=1.5, z=1))\nfig.update_layout( scene=dict(\nxaxis_title=u'x(t-\\u03C4)',\n# init time series\nfig.add_trace(go.Scatter(y=time_series,\nx=[*range(len(time_series))],\nmode='markers',\nmarker=dict(color='blue', size=5)),\n# 2d scatter\nfig.add_trace(go.Scatter(x=delay_coordinates1[0],\ny=delay_coordinates1[1],\nmarker=dict(color='blue', size=5,\nline=dict(width=0.1, color='red'))),\nfig.update_xaxes(title=u'x(t-\\u03C4)', row=2, col=1, range=[-0.4,0.8])\nlinecolor='black',\ntickformat=\"%b\\n%Y\",\nshowgrid=False)\nmirror=True,\n'paper_bgcolor': 'rgba(0,0,0,0)'},\nfont_color='black')\n# fig.update_xaxes(zerolinecolor='red')\n# fig.update_xaxes(zerolinecolor='red', row=2, col=1)\n# fig.write_image('images/taken_example.pdf')\n", "output_sequence": "Test autocorrelation and causality."}, {"input_sequence": "Author: Chengxiu Ling ; #' @title Approximations of VaR and ES with varying alpha and epsilon_alpha\n#' @call approximation.VaR.ES.epsilon.alpha_normal(index, scale, tau, alpha) gives\n#' gives approximation based on normal pre-supposed ideal model\n#' @call approximation.VaR.ES.epsilon.alpha_index(index, scale, tau, alpha)\n#' gives approximation based on the contamination model\n#' @param index = 'Normal', 'Laplace' and 'Power-like', indicating the choice of H\n#' @param scale = the scale parameter of the index function\n#' @param tau = the indice such that epsilon = alpha^tau\n#' @return The approximations and its theoretical values of VaR, ES\n#' @references see Theorem 2.3 a) and c)\n# clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# Standard double-sided exponential distribution with mean zero and variance 1\nplaplace = function(x) {\nlaplace = ifelse(x < 0, 1/2 * exp(sqrt(2) * x), 1 - 1/2 * exp(-sqrt(2) * x))\nreturn(laplace)\n}\n# Standard power tail distribution with \\gamma = 1/2, with mean zero and infinitely\n# variance, see Example 3.3\nppower_like = function(x) {\nPower_like = ifelse(x < 0, 1/2 * (1 + x/sqrt(4 + x^2)), 1 - 1/2 * (1 - x/sqrt(4 +\nx^2)))\nreturn(Power_like)\n# Give the mixture model with pre-supposed standard normal distribution and contamination model\n# being normal, Laplace or Power-like distribution with mean zoeo and scale parameter\nmix_cdf = function(x, index, epsilon, scale) {\nif (index == \"Normal\") {\nmix_cdf = (1 - epsilon) * pnorm(x) + epsilon * pnorm(x/scale)\n} else if (index == \"Laplace\") {\nmix_cdf = (1 - epsilon) * pnorm(x) + epsilon * plaplace(x/scale)\n} else {\nmix_cdf = (1 - epsilon) * pnorm(x) + epsilon * ppower_like(x/scale)\n}\nreturn(mix_cdf)\n# Calculate the lower partial moment at threshold x of mixture distribution\nLower.partial.moment = function(x, index, epsilon, scale) {\nLower.partial.moment = (1 - epsilon) * (-dnorm(x)) + epsilon * (-scale * dnorm(x/scale))\n1/2 * (sqrt(2)/scale * x - 1) * exp(sqrt(2)/scale * x))\nLower.partial.moment = (1 - epsilon) * (-dnorm(x)) + epsilon * (-scale * 2/sqrt(4 +\n(x/scale)^2))\nreturn(Lower.partial.moment)\n# Calculate the quantile at alpha level and fix epsilon contamination level\nmix_VaR = function(alpha, index, epsilon, scale) {\nff = function(x, alpha, index, epsilon, scale) mix_cdf(x, index, epsilon, scale) -\nalpha\nVaR = uniroot(ff, c(-20, 10), alpha, index, epsilon, scale)$root #the accurate alpha-th quantile\nreturn(VaR)\n# Calculate ES at alpha level and fix epsilon contamination level\nmix_ES = function(alpha, index, epsilon, scale) {\nquantile.epsilon = mix_VaR(alpha, index, epsilon, scale)\nmix_ES = alpha^(-1) * Lower.partial.moment(quantile.epsilon, index, epsilon, scale) #the accurate alpha-th Expected-Shortfall\nreturn(mix_ES)\n# Calculate the quantile at alpha level with epsilon = alpha^tau\nmix_VaR_tau = function(alpha, index, tau, scale) {\nepsilon = alpha^tau\n# Calculate ES at alpha level with epsilon = alpha^tau\nmix_ES_tau = function(alpha, index, tau, scale) {\n#' Gives approximation based on normal distribution, the pre-supposed ideal model\napproximation.VaR.ES.epsilon.alpha_normal = function(index, scale, tau, alpha) {\nsize.index = length(index)\nTheoretical_VaR.ES = matrix(rep(1, size.alpha * size.index * 2), nrow = size.alpha)\ncolnames(Theoretical_VaR.ES) = paste(c(rep(\"VaR of\", size.index), rep(\"ES of\",\nsize.index)), rep(index, 2))\ncolnames(Approximation_VaR.ES) = paste(c(rep(\"Approximated VaR of\", size.index),\nrep(\"Approxmated ES of\", size.index)), rep(index, 2))\n\nfor (i in 1:size.index) {\n# Assign the first size.index columns with theoretical VaR of the mixture model\nTheoretical_VaR.ES[, i] = sapply(alpha, mix_VaR_tau, index[i], tau[i], scale[i])\n\n# Assign the second size.index columns with theoretical ES of the mixture model\nTheoretical_VaR.ES[, i + size.index] = sapply(alpha, mix_ES_tau, index[i],\ntau[i], scale[i])\n\n# Assign the first size.index columns with approximations of VaR of the mixture model\n# based on the pre-supposed standard normal distribution\nApproximation_VaR.ES[, i] = sapply(alpha, mix_VaR, index[i], 0, scale[i])\n# Assign the second size.index columns with approximations of ES of the mixture model\nApproximation_VaR.ES[, i + size.index] = sapply(alpha, mix_ES, index[i], 0,\nscale[i])\npar(mfrow = c(2, size.index))\nplot(alpha, Theoretical_VaR.ES[, 1], col = 1, type = \"l\", lwd = 2, xlab = \"\",\nylab = \"VaR\", main = index[1])\nlines(alpha, Approximation_VaR.ES[, 1], col = 2, lty = 2, lwd = 2)\nfor (i in 2:size.index) {\nplot(alpha, Theoretical_VaR.ES[, i], col = 1, type = \"l\", lwd = 2, xlab = \"\",\nylab = \"\", main = index[i])\nlines(alpha, Approximation_VaR.ES[, i], col = 2, lty = 2, lwd = 2)\nplot(alpha, Theoretical_VaR.ES[, size.index + 1], col = 1, type = \"l\", lwd = 2,\nxlab = expression(alpha), ylab = \"ES\", main = \"\")\nlines(alpha, Approximation_VaR.ES[, size.index + 1], col = 2, lty = 2, lwd = 2)\nfor (i in size.index + 2:size.index) {\nplot(alpha, Theoretical_VaR.ES[, i], col = 1, type = \"l\", lwd = 2, xlab = expression(alpha),\nlist(`Theoretical value` = Theoretical_VaR.ES, `Approximation value` = Approximation_VaR.ES)\n#' Gives approximation based on contamination model at alpha / epsilon\napproximation.VaR.ES.epsilon.alpha_index = function(index, scale, tau, alpha) {\n# based on the contamination model at alpha / epsilon\nApproximation_VaR.ES[, i] = sapply(alpha^(1 - tau[i]), mix_VaR, index[i],\n1, scale[i])\nApproximation_VaR.ES[, i + size.index] = sapply(alpha^(1 - tau[i]), mix_ES,\nindex[i], 1, scale[i])\n# Assign values to parameters\nindex = c(\"Normal\", \"Laplace\", \"Power-like\")\nscale = c(1.1, 1.6, 1)\nalpha = seq(from = 5e-04, to = 0.1, by = 1e-04)\n# Call the function to give the graph SRMCappr_epsilon_alpha_1.png\napproximation.VaR.ES.epsilon.alpha_normal(index, scale, tau, alpha)\nscale = c(1.6, 0.95, 1)\ntau = c(0.5, 0.1, 0.5)\n# Call the function to give the graph SRMCappr_epsilon_alpha_2.png\napproximation.VaR.ES.epsilon.alpha_index(index, scale, tau, alpha)\n", "output_sequence": "Shows the approximations of VaR, ES with varying alpha and epsilon, a power function of alpha with exponent tau,\nbased on the pre-supposed ideal standard normal model, or the contamination model H"}, {"input_sequence": "Author: Zografia Anastasiadou ; # clear all variables\nrm(list = ls(all = TRUE))\ngraphics.off()\n# load data\nibm = read.csv(\"ibm.csv\")\n# compute returns for IBM\ny1 = ibm[, 5]\na = 0\nwhile (i <= 120) {\ni = i + 1\na[i] = (y1[i] - y1[i - 1])/y1[i]\n}\nx1 = a[2:121]\n# compute returns for Apple\ny2 = apple[, 5]\nb = 0\nb[i] = (y2[i] - y2[i - 1])/y2[i]\nx2 = b[2:121]\n# compute returns for Bank of America Corporation\ny3 = bac[, 5]\nd = 0\nd[i] = (y3[i] - y3[i - 1])/y3[i]\nx3 = d[2:121]\n# compute returns for Forward Industries\ny4 = ford[, 5]\nf = 0\nf[i] = (y4[i] - y4[i - 1])/y4[i]\nx4 = f[2:121]\n# compute returns for Consolidated Edison\ny5 = ed[, 5]\ng = 0\ng[i] = (y5[i] - y5[i - 1])/y5[i]\nx5 = g[2:121]\n# compute returns for Morgan Stanley\ny6 = ms[, 5]\nh = 0\nh[i] = (y6[i] - y6[i - 1])/y6[i]\nx6 = h[2:121]\nix = c(1:6) # choose assets to use\nx = cbind(x1, x2, x6) # MVAportfolio of all assets\ns1 = solve(cov(x)) # inverse of empirical variance\none = rep(1, length(ix)) # vector of ones\nc2 = (s1 %*% one)/rep(t(one) %*% s1 %*% one, length(s1 %*% one)) # c2 weight\nc1 = one/rep(sum(one), length(one)) # c1 weight\nq1 = x %*% c1 # Optimal MVAportfol returns\nt = c(1:120)\nd1 = cbind(t, q1)\n# plot\npar(mfrow = c(2, 1))\npar(oma = c(0, 0, 8))\nplot(d1, type = \"l\", col = \"blue\", ylab = \"Y\", xlab = \"X\", main = \"Equally Weighted Portfolio\",\nylim = c(-0.6, 0.6), cex.lab = 1.4, cex.axis = 1.4, cex.main = 1.4)\nmtext(\"Weights\", side = 4, line = 4, at = 1.2, las = 2, font = 1)\nmtext(toString(c(sprintf(\"%.3f\", c1[1]), \"IBM\")), side = 4, line = 4, at = 0.6, las = 2)\nlas = 2)\nmtext(toString(c(sprintf(\"%.3f\", c1[3]), \"BAC\")), side = 4, line = 4, at = 0.3, las = 2)\nplot(d2, type = \"l\", col = \"blue\", ylab = \"Y\", xlab = \"X\", main = \"Optimal Weighted Portfolio\",\nmtext(toString(c(sprintf(\"%.3f\", c2[1]), \"IBM\")), side = 4, line = 4, at = 0.6, las = 2)\nlas = 2)\n", "output_sequence": "Computes the optimal portfolio weights with monthly returns of six US firms from Jan 2000 to Dec 2009. The optimal portfolio is compared with an equally weighted one."}, {"input_sequence": "Author: Zografia Anastasiadou ; % ------------------------------------------------------------------------------\n% Book: MVA\n% ------------------------------------------------------------------------------\n% Quantlet: MVAportfol\n% Description: MVAportfol computes the optimal portfolio weights with monthly\n% returns of six US firms from Jan 2000 to Dec 2009. The optimal\n% portfolio is compared with an equally weighted one. Corresponds\n% to example 18.2 in MVA.\n% Usage: -\n% Inputs: None\n% Output: Computes the optimal portfolio weights with monthly returns of\n% six US firms from Jan 2000 to Dec 2009. The optimal portfolio is\n% compared with an equally weighted one.\n% Example: -\n% Author: Zografia Anastasiadou 20101120\n% Matlab: Awdesch Melzer 20120404\n%clear variables and close windows\nclear all;\nclc;\n%load data\nIBM = importdata('ibm.csv');\n% Create new variables in the base workspace from those fields.\nvars = fieldnames(IBM);\nfor i = 1:length(vars)\nassignin('base', vars{i}, IBM.(vars{i}));\nend\nibm = IBM.data;\n\nAPPLE = importdata('apple.csv');\nvars = fieldnames(APPLE);\nassignin('base', vars{i}, APPLE.(vars{i}));\napple = APPLE.data;\nBAC = importdata('bac.csv');\nvars = fieldnames(BAC);\nassignin('base', vars{i}, BAC.(vars{i}));\nbac = BAC.data;\nFORD = importdata('ford.csv');\nvars = fieldnames(FORD);\nassignin('base', vars{i}, FORD.(vars{i}));\nford = FORD.data;\nED = importdata('ed.csv');\nvars = fieldnames(ED);\nassignin('base', vars{i}, ED.(vars{i}));\ned = ED.data;\nMS = importdata('ms.csv');\nvars = fieldnames(MS);\nassignin('base', vars{i}, MS.(vars{i}));\nms = MS.data;\n%compute the returns from assets\ny1 = ibm(:,4);\na = 0;\nwhile i<=120\ni = i+1;\na(i) = (y1(i)-y1(i-1))./y1(i);\nend\n%returns for IBM\nx1 = a(2:121)';\ny2 = apple(:,4);\nb=0;\ni=1;\nwhile i<=120\ni=i+1;\nb(i) = (y2(i)-y2(i-1))./y2(i);\n%returns for Apple\nx2 = b(2:121)';\n\ny3 = bac(:,4);\nd=0;\nd(i) = (y3(i)-y3(i-1))./y3(i);\n%returns for Bank of America Corporation\nx3 = d(2:121)';\n\ny4 = ford(:,4);\nf=0;\nf(i) = (y4(i)-y4(i-1))./y4(i);\n%returns for Forward Industries\nx4 = f(2:121)';\ny5 = ed(:,4);\ng=0;\ng(i) = (y5(i)-y5(i-1))./y5(i);\n%returns for Consolidated Edison\nx5 = g(2:121)';\ny6 = ms(:,4);\nh=0;\nh(i) = (y6(i)-y6(i-1))./y6(i);\n%returns for Morgan Stanley\nx6 = h(2:121)';\n%choose assets to use\nix = [1:6]';\n%MVAportfolio of all assets\nx = [x1,x2,x3,x4,x5,x6];\n%inverse of empirical variance\ns1 = inv(cov(x));\n%vector of ones\none = ones(length(ix),1);\nc2 = (s1*one)./repmat(one'*s1*one,length(s1*one),1); % c1,c2 weights\nc1 = one./repmat(sum(one),length(one),1);\nq1 = x*c1; % Optimal MVAportfol returns\nt = [1:120]';\nd1 = [t,q1];\nfigure(1)\nsubplot(2,2,1)\nsubplot('Position',[0.15 0.61 0.4 0.3])\nhold on\nfor i=1:length(t)-1\nline([d1(i,1) d1(i+1,1)],[d1(i,2) d1(i+1,2)]);\ntitle('Equally Weighted Portfolio')\nxlabel('X')\nylim([-0.4 0.4])\nxlim([0,121])\nbox on\nsubplot(2,2,3)\nsubplot('Position',[0.15 0.1 0.3])\nline([d2(i,1) d2(i+1,1)],[d2(i,2) d2(i+1,2)])\ntitle('Optimal Weighted Portfolio')\nxlim([0 121])\nsubplot(2,2,2)\naxis off\ntext(0.5,1.06,'Weights')\nw1=num2str(c1(1),'%10.3f');\ntext(0.5,0.85,w1)\ntext(0.9,0.75,'Apple')\nsubplot(2,2,4)\ntext(0.5,0.96,'Weights')\nw1=num2str(c2(1),'%10.3f');\ntext(0.5,0.75,w1)\ntext(0.9,0.65,'Apple')\nhold off\n", "output_sequence": "Computes the optimal portfolio weights with monthly returns of six US firms from Jan 2000 to Dec 2009. The optimal portfolio is compared with an equally weighted one."}, {"input_sequence": "Author: Vladimir Georgescu, Jorge Patron, Song Song ; # clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# install and load packages\nlibraries = c(\"KernSmooth\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# load data\nx = read.table(\"bank2.dat\")\nx1 = x[1:100, 6]\n# Compute kernel density estimate\nfh1 = bkde(x1, kernel = \"biweight\")\n# plot\nplot(fh1, type = \"l\", lwd = 2, xlab = \"Counterfeit / Genuine\",\nylab = \"Density estimates for diagonals\", col = \"black\", main = \"Swiss bank notes\",\nxlim = c(137, 143), ylim = c(0, 0.85))\nlines(fh2, lty = \"dotted\", lwd = 2, col = \"red3\")\n", "output_sequence": "Computes kernel density estimates of the diagonal of the genuine and forged swiss bank notes. The bandwidth parameter are chosen by Silverman rule of thumb."}, {"input_sequence": "Author: Vladimir Georgescu, Jorge Patron, Song Song ; %% clear variables and close windows\nclear\nclose all\nclc\n\n%% load data\nx = load('bank2.dat');\nx1 = x(1:100,6);\n[f1,xi1] = ksdensity(x1);\n%% plot\nhold on\nplot(xi1,f1,'LineWidth',2,'Color','k')\nplot(xi2,f2,'LineWidth',2,'Color','r','LineStyle',':')\ntitle('Swiss bank notes')\nxlabel('Counterfeit / Genuine')\nylabel('Density estimates for diagonals')\n", "output_sequence": "Computes kernel density estimates of the diagonal of the genuine and forged swiss bank notes. The bandwidth parameter are chosen by Silverman rule of thumb."}, {"input_sequence": "Author: Wolfgang K. Haerdle ; # clear all variables\nrm(list = ls(all = TRUE))\ngraphics.off()\nxx = seq(-5, 5, by = 0.1)\n# Pdf of t-distribution with df=3,\nplot(xx, df = 3), type = \"l\", ylim = c(0, 0.4), ylab = \"Y\", xlab = \"X\", col = \"green\",\nlwd = 3, cex.lab = 2, cex.axis = 2)\nlines(xx, dt(xx, df = 6), type = \"l\", col = \"blue\", lwd = 3)\nlegend(x = 2, y = 0.3, legend = c(\"t3\", \"t6\", \"t30\"), pch = c(20, 20, col = c(\"green\",\n\"blue\", \"red\"), bty = \"n\")\ntitle(\"PDF of t-distribution\")\n# Cdf of t-distribution with df=3,\ndev.new()\nplot(xx, df = 3), type = \"l\", ylab = \"Y\", xlab = \"X\", col = \"green\", lwd = 3,\ncex.lab = 2, cex.axis = 2)\nlines(xx, pt(xx, df = 6), type = \"l\", col = \"blue\", lwd = 3)\nlegend(x = -5, y = 0.74, legend = c(\"t3\", \"t6\", \"t30\"), pch = c(20, 20, col = c(\"green\",\ntitle(\"CDF of t-distribution\")\n", "output_sequence": "Plots three probability density functions and three cumulative density functions of the t-distribution with different degrees of freedom (t3 stands for t-distribution with degree of freedom 3, etc.)"}, {"input_sequence": "Author: Wolfgang K. Haerdle ; \n%% clear variables and close windows\nclear all\nclc\n\n%% set input\nxx = -5:0.1:5;\ntpdf3 = pdf('T',xx,3);\n%% plot\nsubplot(1,2,1)\nhold on\nplot(xx, tpdf3, 'Color','g','Linewidth',2.5)\nxlabel('X')\nylim([0,0.40])\ntitle('PDF of t-distribution')\nlegend('t3','t6','t30','Location','NorthWest')\nhold off\nsubplot(1,2,2)\nplot(xx, tcdf3, 'Color','g','Linewidth',2.5)\nylim([0,1])\ntitle('CDF of t-distribution')\n", "output_sequence": "Plots three probability density functions and three cumulative density functions of the t-distribution with different degrees of freedom (t3 stands for t-distribution with degree of freedom 3, etc.)"}, {"input_sequence": "Author: Zografia Anastasiadou ; # clear all variables\nrm(list = ls(all = TRUE))\ngraphics.off()\nber = c(0, 2, 4, 12, 3)\nkob = c(12, 10, 8, 0, 9, 13)\nros = c(3, 5, 1, 13, 0)\ndist = rbind(ber, dre, ham, kob, mue, ros)\n# a,b,h,i are matrices\na = (dist^2) * (-0.5)\ni = diag(6)\nu = rep(1, 6)\nh = i - (1/6 * (u %*% t(u)))\nb = h %*% a %*% h # Determine the inner product matrix\ne = eigen(b)\ng1 = cbind(e$vectors[, 1], e$vectors[, 2])\ng2 = diag(e$values[1:2])\nx = g1 %*% (g2^0.5) # Determine the coordinate matrix\n# Determine the dissimilarities\nd12 = ((x[1, 1] - x[2, 1])^2 + (x[1, 2] - x[2, 2])^2)^0.5\nd14 = (d14 + d15)/2\nd15 = d14\nd16 = (d14 + d15 + d16 + d23 + d24 + d25 + d26)/7\nd14 = d16\nd35 = (d35 + d36)/2\nd36 = d35\ndd = rbind(c(0, d12, d16), c(d12, 0, d23, d26), c(d13,\nd23, 0, d34, d36), c(d14, d24, 0, d45, d46), c(d15, d25,\n0, d56), c(d16, d26, 0))\nf = cbind(c(1:15), c(d12, d13, d24, d35,\nd45, d56))\nl = cbind(c(15, 1), c(14.4, 3.17))\n# Plot\nplot(f, xlim = c(0, 16), xlab = \"Rank\", ylab = \"Distance\", main = \"Pool-Adjacent-Violator-Algorithm\",\ncex.axis = 1.2, cex.lab = 1.2, cex.main = 1.8)\nlines(f, lty = 2, lwd = 3, col = \"blue\")\nsegments(l[1, 1], l[1, 2], l[2, 1], l[2, 2], lwd = 3, col = \"blue\")\n", "output_sequence": "Computes the pool adjacent violator algorithm (PAV)."}, {"input_sequence": "Author: Zografia Anastasiadou ; %% clear all variables and console and close windows\nclear\nclc\nclose all\n%% data\nber = [0 2 4 12 11 3 ];\nkob = [12 10 8 0 9 13];\nmue = [11 7 15 9 0 14];\nros = [3 5 1 13 14 0 ];\ndist=[ber', dre', ham', kob', mue', ros'];\n%% matrices aa, bb, hh, ii\naa = (dist .^ 2) .* (-0.5);\nii = eye(6, 6);\nhh = ii - (1/6 * (u' * u));\nbb = hh * aa * hh; % Determine the inner product matrix\n[g1, g2] = eigs(bb, 2);\nx = g1 * (g2 .^ 0.5); % Determine the coordinate matrix\n%% Determine the dissimilarities\nd12 = ((x(1, 1) - x(2, 1))^2 + (x(1, 2) - x(2, 2))^2)^0.5;\nd14 = (d14 + d15)/2;\nd15 = d14;\nd16 = (d14 + d15 + d16 + d23 + d24 + d25 + d26)/7;\nd14 = d16;\nd35 = (d35 + d36)/2;\nd36 = d35;\ndd = [[0 d12 d16];\n[d13 d23 0 d34 d36];\nff = [[1:15]',...\n[d12;d13;d14;d15;d16;d23;d24;d25;d26;d34;d35;d36;d45;d46;d56]];\nll = [[15;1],[14.4;3.17]];\n%% plot\nscatter(ff(:, 1), ff(:, 2), 'k')\ntitle('Pool-Adjacent-Violator-Algorithm')\nxlabel('Rank')\nylabel('Distance')\nxlim([0 16])\nset(gca, 'box', 'on')\nfor i=1:14\nline([ff(i, 1) ff(i + 1, 1)], [ff(i, 2) ff(i + 1, 2)],...\n'Color', 'b', 'LineWidth', 1.5, 'LineStyle', '--')\nend\nline([ll(1,1) ll(2,1)],[ll(1,2) ll(2,2)],'Color','b','LineWidth',2)\n", "output_sequence": "Computes the pool adjacent violator algorithm (PAV)."}, {"input_sequence": "Author: Jorge Patron, Vladimir Georgescu, Song Song, Awdesch Melzer ; # clear all variables\nrm(list = ls(all = TRUE))\ngraphics.off()\n# load data\nx = read.table(\"pullover.dat\")\n# plot\nplot(x[, 4], x[, 1], xlab = \"Sales Assistants (X4)\", ylab = \"Sales (X1)\", xlim = c(70,\n120), ylim = c(80, 240), frame = TRUE, axes = FALSE)\ntitle(\"Pullovers Data\")\naxis(side = 2, seq(80, 240, 40), seq(80, 240, 40))\n", "output_sequence": "Computes a two dimensional scatterplot of assistants and sales from the pullovers data set."}, {"input_sequence": "Author: Jorge Patron, Vladimir Georgescu, Song Song, Awdesch Melzer ; %% clear all variables\nclear all\nclc\n\n%% load data\nx = load('pullover.dat');\n%% plot\nscatter(x(:,4),x(:,1),75,'k')\ntitle('Pullovers Data')\nxlabel('Sales Assistants (X4)')\n", "output_sequence": "Computes a two dimensional scatterplot of assistants and sales from the pullovers data set."}, {"input_sequence": "Author: Vladimir Georgescu, Jorge Patron, Song Song, Awdesch Melzer ; # clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# load data\nx = read.table(\"pullover.dat\")\ndat = cbind(x[, 2], x[, 1])\ncv = cov(dat)\nsxy = cv[2, 1]\nbeta = sxy/sxx\nalpha = mean(dat[, 2]) - beta * mean(dat[, 1])\ny = dat[, 2]\nyhat = alpha + beta * dat[, 1]\nyquer = NULL\nsteps = dim(x)[1]\nfor (i in 1:steps) {\nyquer[i] = mean(dat[, 2])\n}\n# Generating line1\nline11 = NULL\nm = 1\nline11[m] = dat[i, 1]\nm = m + 3\nline12[m] = yquer[i]\nline1 = cbind(line11, line12)\n# Generating line2\nline21 = NULL\nline21[m] = dat[i, 1] - 0.3\nline22[m] = yquer[i]\nline2 = cbind(line21, line22)\n# Generating line3\nline31 = NULL\nline31[m] = dat[i, 1] + 0.3\nline32[m] = yquer[i]\nline3 = cbind(line31, line32)\nt = (round(min(dat[, 1])) - 5):(round(max(dat[, 1])) + 5)\nfit = alpha + beta * t\nrl = cbind(t, fit)\naux = mean(dat[, 2]) * matrix(1, length(t),\naux1 = aux[, 1]\nybar = cbind(t, aux1)\n# Chart\nplot(dat[, 1], dat[, 2], xlab = \"Price (X2)\", ylab = \"Sales (X1)\", xlim = c(88,\n102), ylim = c(162, 198))\ntitle(\"Pullover Data\")\nlines(rl[, 1], rl[, 2], lwd = 2)\nlines(ybar[, 1], ybar[, 2], lty = \"dashed\", lwd = 2)\n# redline\nredline = rbind(line1[1, ], line1[2, ], line1[4, ], line1[5, ], line1[7, ], line1[8,\n], line1[28, ], line1[29, ])\ni = 1\ns = dim(redline)[1]\nwhile (i < s) {\nlines(c(redline[i, 1], redline[i + 1, 1]), c(redline[i, 2], redline[i +\n1, 2]), col = \"red3\", lwd = 2, lty = \"dashed\")\ni = i + 2\n# Greenline\ngreenline = rbind(line2[1, ], line2[3, ], line2[4, ], line2[6, ], line2[7, ], line2[9,\n], line2[28, ], line2[30, ])\nlines(c(greenline[i, 1], greenline[i + 1, 1]), c(greenline[i, 2], greenline[i +\n1, 2]), col = \"green4\", lwd = 2)\n# blueline\nblueline = rbind(line3[1, ], line3[3, ], line3[5, ], line3[6, ], line3[7, ], line3[8,\n])\nlines(c(blueline[i, 1], blueline[i + 1, 1]), c(blueline[i, 2], blueline[i + 1,\n2]), col = \"blue3\", lwd = 2, lty = 4)\n", "output_sequence": "Plots a section of the linear regression of the sales (X1) on price (X2) for the pullovers data. Graphical representation of the relationship: total variation = explained variation + unexplained variation."}, {"input_sequence": "Author: Vladimir Georgescu, Jorge Patron, Song Song, Awdesch Melzer ; %% clear all variables\nclear\nclc\nclose all\n\n%% load data\nx = load('pullover.dat');\ndat(:,1) = x(:,2); % extracts colum 2\ncv = cov(dat); % compute var-cov matrix\nsxy = cv(2,1); % extracts covariance\nbeta = sxy/sxx;\nalpha = mean(dat(:,2)) - beta*mean(dat(:,1));\ny = dat(:,2);\nyhat = alpha + beta * dat(:,1);\nfor i = 1:length(x)\nyquer(i) = mean(dat(:,2));\nend\n%% plot\n% Generate line1\nm = 1;\nline1(m,1) = dat(i,1);\n\nm = m+3;\nline1(m,2) = yquer(i);\n%Generating line2\nline2(m,1) = dat(i,1)-0.3;\n%Generating line3\nline3(m,1) = dat(i,1)+0.3;\nt = (round(min(dat(:,1)))-5):(round(max(dat(:,1))) +5);\nfit = alpha + beta*t;\nrl(:,1) = t;\naux = mean(dat(:,2))*ones(length(t));\naux1 = aux(:,1);\nybar(:,1) = t;\nhold on\n%Blueline\nblueline(1,:) = line1(1,:);\ni = 1;\nwhile i<length(blueline)\nline([blueline(i,1) blueline(i+1,1)],[blueline(i,2) blueline(i+1,2)],'Color','b','LineWidth',2,'LineStyle','--');\ni = i+2;\n%Greenline\ngreenline(1,:) = line2(1,:);\nwhile i<length(greenline)\nline([greenline(i,1) greenline(i+1,1)],[greenline(i,2) greenline(i+1,2)],'Color','g','LineWidth',2);\n%Redline\nredline(1,:) = line3(1,:);\nwhile i<length(redline)\nline([redline(i,1) redline(i+1,1)],[redline(i,2) redline(i+1,2)],'Color','r','LineWidth',2,'LineStyle','-.');\nscatter(dat(:,1),dat(:,2),75,'k');\ntitle('Pullover Data')\nxlabel('Price (X2)')\nxlim([88 102]);\nline(rl(:,1),rl(:,2),'Color','k','LineWidth',2)\nline(ybar(:,1),ybar(:,2),'Color','k','LineStyle','--','LineWidth',2)\n", "output_sequence": "Plots a section of the linear regression of the sales (X1) on price (X2) for the pullovers data. Graphical representation of the relationship: total variation = explained variation + unexplained variation."}, {"input_sequence": "Author: Zografia Anastasiadou ; %% clear all variables and console and close windows\nclear\nclc\nclose all\n%% load data\ndata = load('bostonh.dat');\n%% Transformations\nxt = data;\nxt(:, 1) = log(data(:, 1));\nxt(:, 7) = (power(data(:, 7), 2.5))/10000;\nxt(:, 8) = log(data(:, 8));\nxt(:, 11) = exp(0.4 * data(:, 11))/1000;\nxt(:, 13) = sqrt(data(:, 13));\ndata = xt;\ndata(:, 4) = [];\nn1 = length(data);\nb = sqrt((n1-1) * var(data)/n1);\nx = (data - repmat(m,n1,1))./repmat(b,n1,1);\nadjust = (n1 - 1) * cov(x)/n1;\n[v, e] = eigs(adjust, 13, 'la');\n% change the sign of the eigenvectors . This is done only to make easier\n% the comparison with R results.\nv(:, [1 2 4 7 8 ]) = -v(:, [1 2 4 7 8 ]);\nm = mean(x);\nx1 = x - repmat(m,n1,1);\nr1 = x1 * v;\nr = corr([r1 x]);\n%% correlations between variables and the first three pc's\nr123 = r(14:end, 1:3);\n%% plots\n% First and Second PC\nsubplot(2, 2, 1)\nhold on\nxlim([-1.2 1.2])\nline([-1.2 1.2], [0 0], 'Color', 'k')\ntitle('Boston Housing', 'FontSize', 12, 'FontWeight', 'bold')\nset(gca, 'FontSize', 12, 'FontWeight', 'bold', 'Box', 'on')\ncircle = rsmak('circle');\nfnplt(circle)\nfor i=1:length(r123)\nif i<4\ntext(r123(i, 1), r123(i, 2), strcat('X', int2str(i)), 'FontSize', 10, 'FontWeight', 'bold')\nelse\ntext(r123(i, 1), r123(i, 2), strcat('X', int2str(i + 1)), 'FontSize', 10, 'FontWeight', 'bold')\nend\nend\n% Third and Second PC\nsubplot(2, 2, 2)\nxlabel('Third PC', 'FontSize', 12, 'FontWeight', 'bold')\ntext(r123(i, 3), r123(i, 2), strcat('X', int2str(i)), 'FontSize', 10, 'FontWeight', 'bold')\n% First and Third PC\nsubplot(2, 2, 3)\nylabel('Third PC', 'FontSize', 12, 'FontWeight', 'bold')\ntext(r123(i, 1), r123(i, 3), strcat('X', int2str(i)), 'FontSize', 10, 'FontWeight', 'bold')\ndisp('Correlations of the first three PCs with the original variables')\ndisp(' PC1 PC3')\ndisp(r123)\n", "output_sequence": "Calculates and plots the correlations of the first three PCs with the original variables for the standardized Boston housing data."}, {"input_sequence": "Author: Zografia Anastasiadou ; # clear all variables\nrm(list = ls(all = TRUE))\ngraphics.off()\n# load data\ndata = read.table(\"bostonh.dat\")\n# transform data\nxt = data\nxt[, 1] = log(data[, 1])\nxt[, 7] = (data[, 7]^(2.5))/10000\nxt[, 8] = log(data[, 8])\nxt[, 11] = exp(0.4 * data[, 11])/1000\nxt[, 13] = sqrt(data[, 13])\nxt[, 14] = log(as.numeric(data[, 14]))\ndata = xt[, -4]\n\nn1 = nrow(data)\nx = (data - matrix(apply(data,2,mean), n1, byrow = T))/matrix(sqrt((n1 - 1) *\napply(data, 2, var)/n1), n1, byrow = T) # standardizes the data\neig = eigen((n1 - 1) * cov(x)/n1) # spectral decomposition\ne = eig$values\nx1 = as.matrix(x - matrix(apply(x,2,mean), n1, byrow = T))\nr1 = x1 %*% v\nr = cor(cbind(r1, x))\n# correlations between variables and the first three pc's\nr123 = r[14:26, 1:3]\n# plot\npar(mfrow = c(2, 2))\nucircle = cbind(cos((0:360)/180 * pi), sin((0:360)/180 * pi))\nplot(ucircle, type = \"l\", lty = \"solid\", col = \"blue\", xlab = \"First PC\", ylab = \"Second PC\",\nmain = \"Boston Housing\", cex.lab = 1.2, cex.axis = 1.2, cex.main = 1.6, lwd = 2)\nabline(h = 0, v = 0)\nlabel = c(\"X1\", \"X2\", \"X10\",\n\"X14\")\ntext(r123[,c(1,2)], label)\nplot(ucircle, type = \"l\", lty = \"solid\", col = \"blue\", xlab = \"Third PC\", ylab = \"Second PC\",\ntext(r123[,c(3,2)], label)\nplot(ucircle, type = \"l\", lty = \"solid\", col = \"blue\", xlab = \"First PC\", ylab = \"Third PC\",\ntext(r123[,c(1,3)], label)\n", "output_sequence": "Calculates and plots the correlations of the first three PCs with the original variables for the standardized Boston housing data."}, {"input_sequence": "Author: Vladimir Georgescu, Song Song, Jorge Patron ; %% clear variables and close windows\nclose all\nclear\nclc\n\n%% load data\nload carc.txt\nx = carc;\n%% parameter settings\nk = 0;\nM = carc(:,2);\nfor i = 1:length(x)\nif x(i,13) == 1\nk = k + 1;\nus(k,: ) = x(i,:);\nelseif x(i,13) == 2\nl = l + 1;\njapan(l,:) = x(i,:);\nelseif x(i,13) == 3\nm = m + 1;\neurope(m,:) = x(i,:);\nend\nend\nm1 = mean(us(:,2));\n%% plot\nhold on\nline([0.775 1.225],[m1 m1],'Color','k','LineStyle',':','LineWidth',1.2)\nboxplot(M,C,'Symbol','o','labels',{'US','JAPAN','EU'})\ntitle('Car Data')\n", "output_sequence": "Computes the five-number summary and boxplots for the mileage (X14 variable) of US, Japanese and European cars."}, {"input_sequence": "Author: Vladimir Georgescu, Song Song, Jorge Patron ; # clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# load data\nx = read.table(\"carc.txt\")\n# parameter settings\nk = 0\nus = NULL\neurope = NULL\nM = x[, 2]\nfor (i in 1:dim(x)[1]) {\nif (x[i, 13] == 1) {\nk = k + 1\nus[k] = x[i, 2]\n} else if (x[i, 13] == 2) {\nl = l + 1\njapan[l] = x[i, 2]\n} else if (x[i, 13] == 3) {\nm = m + 1\neurope[m] = x[i, 2]\n}\n}\nm1 = mean(us)\nm2 = mean(japan)\nm3 = mean(europe)\n# plot\nboxplot(us, japan, europe, axes = FALSE, frame = TRUE)\naxis(side = 1, at = seq(1, 3), label = c(\"US\", \"JAPAN\", \"EU\"))\ntitle(\"Car Data\")\nlines(c(0.6, 1.4), c(m1, m1), lty = \"dotted\", lwd = 1.2)\n", "output_sequence": "Computes the five-number summary and boxplots for the mileage (X14 variable) of US, Japanese and European cars."}, {"input_sequence": "Author: Zografia Anastasiadou ; %% clear all variables and console and close windows\nclear\nclc\nclose all\n%% the reported preference orderings\nx = (1:6)';\n% the estimated preference orderings according to the additive model (16.1)\n% and the metric solution (Table 16.6) in book\ny = [0.84, 3.16, 3.34, 5.66, 5.16]';\nz = [x, y];\n%% use PAV algorithm\ngp = pava(y);\ngp = [x, gp];\n%% plot\nfigure(1)\nhold on\nplot(gp, z, 'w')\nxlabel('Revealed rankings', 'FontSize', 12)\nbox on\nline(gp(:, 1), gp(:, 2), 'LineWidth', 2)\nscatter(x, y, 'MarkerEdgeColor', 'red', 'MarkerFaceColor', 'red')%'filled', true)\nxlim([0.5, 6.5])\nlabels = {'car1', 'car6'};\ntext(x, y - 0.1, labels, 'FontSize',12)\nhold off\n", "output_sequence": "Performs a monotone transformation to the estimated stimulus utilities of the car example by applying the pool-adjacent-violators (PAV) algorithm."}, {"input_sequence": "Author: Zografia Anastasiadou ; # clear all variables\nrm(list = ls(all = TRUE))\ngraphics.off()\n# install and load packages\nlibraries = c(\"isotone\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# the reported preference orderings\nx = c(1:6)\n# the estimated preference orderings according to the additive model (16.1) and the metric solution (Table 16.6) in MVA\ny = c(0.84, 2.84, 3.16, 3.34, 5.66, 5.16)\nz = cbind(x, y)\n# PAV algorithm\ngp = gpava(x, y)\na = gp$z # the reported preference orderings\nb = gp$x # the estimated preference orderings after PAV algorithm\ngp = cbind(a, b)\n# Plot of car rankings\nplot(gp, z, type = \"n\", xlab = \"revealed rankings\", ylab = \"estimated rankings\",\nmain = \"Car rankings\", cex.lab = 1.4, cex.axis = 1.4, cex.main = 1.6)\nlines(a, b, lwd = 2)\npoints(x, y, pch = 19, col = \"red\")\ntext(x[1], y[1] - 0.1, \"car1\", font = 2)\n", "output_sequence": "Performs a monotone transformation to the estimated stimulus utilities of the car example by applying the pool-adjacent-violators (PAV) algorithm."}, {"input_sequence": "Author: Zografia Anastasiadou, Awdesch Melzer ; %% clear all variables and console and close windows\nclear\nclc\nclose all\n%% load data\nformatSpec = '%s%f%f%f%f%f%f%s';\ndata = readtable('uscomp2.dat','Delimiter',' ', 'Format',formatSpec);\n%% symbols for the sectors\ndata.Properties.VariableNames{'Var8'} = 'Sector';\ndata.Sector = categorical(data.Sector);\ndata.Sector = mergecats(data.Sector, {'Communication','HiTech'},'H');\ndata.Sector = renamecats(data.Sector,{'Energy','Finance',...\n'Manufacturing','Retail'},{'E','F','M','R'});\ndata.Sector = mergecats(data.Sector, {'Medical','Other',...\n'Transportation'}, '*');\ndata.Sector = char(data.Sector);\n%% omit the observations for IBM and General Electric\ndata.Var1 = nominal(data.Var1);\ndata = data(data.Var1~='IBM',:);\nx = data;\nx.Var1 = [];\nx.Sector = [];\nx = table2array(x);\n[n,p] = size(x);\nm = mean(x);\nx = (x-repmat(m,n,1)).*repmat(1./sqrt((n-1)*var(x)/n),n,1); % standardizes the data matrix\n[v,e] = eigs((n-1)*cov(x)/n,p,'la'); % eigenvalues sorted by size from largest to smallest(Note: Command generates a Warning(Disregard it))\ne1 = diag(e); % creates a vector of eigenvalues\nx = x*v;\n%% plots\n%Eigenvalues\nsubplot(2,1,2)\nsubplot('Position',[0.1 0.1 0.15])\nnr=1:6;\nscatter(nr,e1,'k')\nxlabel('Index')\nylabel('Lambda')\ntitle('Eigenvalues of S')\nxlim([0.5 6.5])\nbox on\n%First vs Second PC\nsubplot(2,1,1)\nsubplot('Position',[0.1 0.4 0.5])\ntitle('First vs. Second PC')\nxlabel('PC 1')\nxlim([-3 9])\nhold on\ntext(x(:,1),x(:,2),data.Sector);\n", "output_sequence": "Performs a PCA for the standardized US company data without IBM and General Motors. It shows the first two principal components and screeplot of the eigenvalues."}, {"input_sequence": "Author: Zografia Anastasiadou, Awdesch Melzer ; # clear all variables\nrm(list = ls(all = TRUE))\ngraphics.off()\n# Load data\nx = read.table(\"uscomp2.dat\")\n# Without IBM and General Electric\nx = rbind(x[1:37, ], x[39, ], x[41:79, ])\ncolnames(x) = c(\"Company\", \"A\", \"MV\", \"P\", \"CF\", \"E\", \"Sector\")\nattach(x)\nSector = as.character(Sector)\nSector[1:2] = \"H\"\nx = x[, 2:7]\nn = nrow(x)\nx = (x - matrix(apply(x, 2, mean), n, 6, byrow = T))/matrix(sqrt((n - 1) * apply(x,\n2, var)/n), n, 6, byrow = T) # standardizes the data\neig = eigen((n - 1) * cov(x)/n) # spectral decomposition\ne = eig$values\nx = as.matrix(x) %*% v # principal components\nlayout(matrix(c(2, 1), 2, 1, byrow = T), c(2, 1), c(2, 1), T)\n# plot\nplot(e, xlab = \"Index\", ylab = \"Lambda\", main = \"Eigenvalues of S\")\nplot(-x[, 1], x[, 2], xlim = c(-2, 8), ylim = c(-8, 8), type = \"n\", xlab = \"PC 1\",\nylab = \"PC 2\", main = \"First vs. Second PC\")\ntext(-x[, 1], x[, 2], Sector)\n", "output_sequence": "Performs a PCA for the standardized US company data without IBM and General Motors. It shows the first two principal components and screeplot of the eigenvalues."}, {"input_sequence": "Author: Piedad Castro ; %% clear all variables and console and close windows\nclear\nclc\nclose all\n%% set directory\n% cd('\\\\tsclient\\D\\Trabajo HU\\AAA-MVA')\n%% data import\ndata1 = load('XFGvolsurf01.dat'); % 1 month maturity data\n%% sample covariance matrices\nC = cov(data1);\nK = size(C,3); % number of groups\n%% CPCA computation\nL = 15; % number of iterations\nB = eye(P);\nd1 = zeros(K, 1);\nfor l = 1:L\nfor p = 1:(P - 1)\nfor e = (p + 1):P\nQ = eye(2);\nfor k = 1:K\nH = B(: ,[p, e]);\nT(:, :, k) = H' * C(:, :, k) * H;\nd1(k) = Q(:, 1)' * T(:, :, k) * Q(:, 1);\nM = M + (d1(k) - d2(k))/(d1(k) * d2(k)) * T(:, :, k);\nend\n[EigVec, EigVal] = eig(M);\nQ = EigVec;\nB(: ,[p, e]) = H * Q;\nend\nend\n%% sort eigenvectors\nNewB = [];\nabsi = abs(sum(B, 1));\nwhile sum(absi) > -size(B,2)\nmaxi = max(absi);\nindex = find(absi == maxi);\nNewB = [NewB B(:,index)];\nabsi(index) = -1;\n%% plot\nfigure\nset(gcf,'color','w') % set the background color to white\nplot(-NewB(:, 1), 'black', 'LineWidth', 5)\nylim([-0.8 0.8])\ntitle('PCP for CPCA, 3 eigenvectors')\nxlabel('moneyness')\nylabel('loading')\nhold on\nplot(NewB(:, 2), 'black', 'LineWidth', 3)\n%% test statistic\ntest = 0;\nn = [size(data1, 1), size(data2, 1), size(data3, 1)]; % number of observations per group\nfor k = 1:K\nlambda(:, :, k) = diag(diag(NewB' * C(:, :, k) * NewB));\nSig(:, :, k) = NewB * lambda(:, :, k) * NewB';\ntest = test + (n(k)-1) * det(Sig(:, :, k))/det(C(:, :, k));\n%% p-value\ndf = 1/2 * (K - 1) * P * (P - 1);\npval = 1 - chi2cdf(test, df);\n", "output_sequence": "Estimates a common principal components model for the implied volatility data and computes a likelihood ratio test. The computation of the common principal components uses a part of the code of the function \"FCPC\" that belongs to the R-Package \"multigroup\" (https://CRAN.R-project.org/package=multigroup)."}, {"input_sequence": "Author: Piedad Castro ; # clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# set working directory\n# setwd(\"C:/...\")\n# load data\ndata1 = read.table(\"XFGvolsurf01.dat\") # 1 month maturity data\nK = length(C) # number of groups\nP = dim(data1)[2] # number of variables\n# sample covariance matrices\nC = vector(\"list\", K)\nC[[1]] = cov(data1)\n# CPCA computation\nL = 15 # number of iterations\nB = diag(P)\nT = vector(\"list\", K)\nd1 = rep(0, K)\nfor (l in 1:L) {\nfor (p in 1:(P - 1)) {\nQ = diag(2)\nfor (k in 1:K) {\nH = B[, c(p, e)]\nT[[k]] = t(H) %*% C[[k]] %*% H\nd1[k] = t(Q[, 1]) %*% T[[k]] %*% Q[, 1]\nM = M + (d1[k] - d2[k])/(d1[k] * d2[k]) * T[[k]]\n}\neig <- eigen(M)\nQ = eig$vectors\nB[, c(p, e)] = H %*% Q\n}\n}\n# plot the first three eigenvectors\nplot(-B[, 1], type = \"l\", lwd = 5,\nylim = c(-0.8, 0.8), xlab = \"moneyness\", ylab = \"loading\",\nmain = \"PCP for CPCA, 3 eigenvectors\")\nlines(B[, 2], type = \"l\", lwd = 3)\n# test statistic\ntest = 0\nn = c(nrow(data1), # number of observations per group\nlambda = array(NA, c(P, P, K))# 3D array to store the diagonal matrices of eigenvalues\nfor(k in 1:K){\nlambda[, , k] = diag(diag(t(B) %*% C[[k]] %*% B))\nSig[, , k] = B %*% lambda[, , k] %*% t(B)\ntest = test + (n[k]-1) * det(Sig[, , k])/det(C[[k]])\n# p-value\ndf = 1/2 * (K - 1) * P * (P - 1)\npval = 1 - pchisq(test, df)\n", "output_sequence": "Estimates a common principal components model for the implied volatility data and computes a likelihood ratio test. The computation of the common principal components uses a part of the code of the function \"FCPC\" that belongs to the R-Package \"multigroup\" (https://CRAN.R-project.org/package=multigroup)."}, {"input_sequence": "Author: Zografia Anastasiadou ; # clear all variables\nrm(list = ls(all = TRUE))\ngraphics.off()\n# load data\ndata = read.table(\"bostonh.dat\")\n# transform data\nxt = data\nxt[, 1] = log(data[, 1])\nxt[, 7] = (data[, 7]^(2.5))/10000\nxt[, 8] = log(data[, 8])\nxt[, 11] = exp(0.4 * data[, 11])/1000\nxt[, 13] = sqrt(data[, 13])\ndata = xt[, -4]\nda = scale(data) # standardize variables\nd = dist(da, \"euclidean\", p = 2) # euclidean distance matrix\nw = hclust(d, method = \"ward.D\") # cluster analysis with ward algorithm\ntree = cutree(w, 2) # define the clusters, tree=1 if cluster=1\n# the following two lines under comments are for price of Boston houses\n# tree=(xt[,14]>median(xt[,14]))+1\n# da=da[,1:12]\nt1 = subset(da, tree == 1)\nm1 = colMeans(t1) # mean of first cluster\ns = ((nrow(t1) - 1) * cov(t1) + (nrow(t2) - 1) * cov(t2))/(nrow(xt) - 2) # common variance matrix\nalpha = solve(s) %*% (m1 - m2) # alpha for the discrimination rule\n# APER for clusters of Boston houses\nmis1 = sum((t1 - m) %*% alpha < 0) # misclassified 1\naper = (mis1 + mis2)/nrow(xt) # APER (apparent error rate)\nalph = (da - matrix(m, nrow(da), byrow = T)) %*% alpha\nset.seed(1)\n# discrimination scores\np = cbind(alph, tree + 0.05 * rnorm(NROW(tree)))\ntree[tree == 1] = 16\ntr = tree\ntr[tr == 16] = \"red\"\n# plot of discrimination scores\nplot(p[, 1], p[, 2], pch = tree, col = tr, xaxt = \"n\", yaxt = \"n\", xlab = \"\", ylab = \"\",\nbty = \"n\")\nabline(v = 0, lwd = 3)\ntitle(paste(\"Discrimination scores\"))\n", "output_sequence": "Demonstrates maximum likelihood discrimination rule (ML rule) for the Boston housing data."}, {"input_sequence": "Author: Zografia Anastasiadou ; % ------------------------------------------------------------------------------\n% Book: MVA\n% ------------------------------------------------------------------------------\n% Quantlet: MVAdiscbh\n% Description: MVAdiscbh demonstrates the maximum likelihood (ML) discrimination\n% rule for the Boston housing data (bostonh.dat).\n% Usage: hclust\n% Inputs: None\n% Output: Application of the linear discriminant rule on the variables of\n% the transformed bostonh.dat, calculation of the APER (apparent\n% error rate) for price and clusters of Boston houses and plot of\n% the linear discriminant scores for the two clusters created from\n% the transformed bostonh.dat.\n% Example: -\n% Author: Zografia Anastasiadou 20100924\n% Matlab: Awdesch Melzer 20120407\n% clear loaded variables and close windows\nclear all;\nclc;\n% load data\ndata = load('bostonh.dat');\n%transform data\nxt = data;\nxt(:,1) = log(data(:,1));\nxt(:,7) = (data(:,7).^(2.5))./10000;\nxt(:,8) = log(data(:,8));\nxt(:,11) = exp(0.4*data(:,11))./1000;\nxt(:,13) = sqrt(data(:,13));\ndata = xt;\ndata(:,4) = [];\n%standardize variables\n[n m] = size(data);\nfor i = 1:n\nda(i,:) = (data(i,:)-mean(data))./sqrt((n-1)*var(data)/n);\nend\n%euclidean distance matrix\nd = pdist(da,'euclidean',2);\n%cluster analysis with ward algorithm\nw = hclust(d);\n%define the clusters, tree=1 if cluster=1\ntree = cluster(w,'maxclust',2);\nt1 = da(find(tree==1),:);\n%mean of first cluster\nm1 = mean(t1);\n%mean of second cluster\nm2 = mean(t2);\n%mean of both clusters\nm = (m1+m2)/2;\n%common variance matrix\ns = ((length(t1)-1)*cov(t1)+(length(t2)-1)*cov(t2))/(length(xt)-2);\n%alpha for the discrimination rule\nalpha = inv(s)*(m1-m2)';\n%APER for clusters of Boston houses\nmis1 = sum((t1-repmat(m,length(t1),1))*alpha<0) %misclassified 1\ndisp('Apparent error rate for clusters of Boston houses')\naper = (mis1+mis2)./length(xt) %APER (apparent error rate)\nalph = (da-repmat(m,length(da),1))*alpha;\n%linear discriminant scores\np = [alph,tree+0.05*normrnd(0,1,length(tree),1)];\n%plot of the linear discriminant scores\ngscatter(p(:,1),p(:,2),tree,'rk','o^',8)\nline([0,0],[0.7,2.3],'Color','k','LineWidth',2.4)\nylim([0.7,2.3])\nlegend off\nbox on\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n% Calculation of APER for price of Boston houses.\n% Demonstration of the ML discrimination rule and\n% calculation the apparent error rate for Boston housing\nfor i=1:length(xt)\nif xt(i,14)>median(xt(:,14));\ntrue(i)=1;\nelse\ntrue(i)=0;\nend\nj=0;\nif true(i)==1\nj=j+1;\nxg0(j,:)=xt(i,:);\nk=k+1;\nxf0(k,:)=xt(i,:);\na0=xg0(:,1:3);\nxg=[a0,b0];\nmg=mean(xg);\nm=(mf+mg)/2;\ns=((length(xg)-1)*cov(xg)+(length(xf)-1)*cov(xf))/(length(xt)-2);\nalpha=inv(s)*(mg-mf)';\nmiss1=0;\ncorr1=0;\nfor i=1:length(xg)\naux1(i)=(xg(i,:)-m)*alpha;\nfor i=1:length(aux1)\nif aux1(i)<0\nmiss1=miss1+1;\nfor i=1:length(xf)\naux2(i)=(xf(i,:)-m)*alpha;\nfor i=1:length(aux2)\nif aux2(i)>0\nmiss2=miss2+1;\ndisp('Group 1 classified as 2: ')\nmiss1\ndisp('Group 2 classified as 1: ')\nmiss2\ndisp('Group 1 classified as 1: ')\ncorr1\ndisp('Group 2 classified as 2: ')\ncorr2\ndisp('APER (apparent error rate) for price of Boston houses:')\naper=(miss1+miss2)/length(xt);\n", "output_sequence": "Demonstrates maximum likelihood discrimination rule (ML rule) for the Boston housing data."}, {"input_sequence": "Author: Zografia Anastasiadou, Awdesch Melzer ; # clear all variables\nrm(list = ls(all = TRUE))\ngraphics.off()\n# load data\nx = read.table(\"food.dat\")\np = ncol(x)\nx = x[, 2:p]\nx1 = sqrt((n - 1) * apply(x, 2, var)/n)\nx2 = x - matrix(apply(as.matrix(x), 2, mean), nrow = n, ncol = p - 1, byrow = T)\nx = as.matrix(x2/matrix(x1, nrow = n, ncol = p - 1, byrow = T)) # standardizes the data matrix\n# compute eigenvalues\ne = eigen(x %*% t(x)/n)\ne1 = e$values\ne2 = e$vectors\na = e2[, 1:2]\nw = -a * sqrt(matrix(e1[1:2], nrow(a), byrow = TRUE))\n# Plot 1: the representation of the individuals\ndev.new()\nplot(w, type = \"n\", xlab = \"First Factor - Families\", ylab = \"Second Factor - Families\",\nmain = \"French Food data\", cex.lab = 1.2, cex.axis = 1.2, cex.main = 1.8, lwd = 2)\ntext(w, c(\"MA2\", \"EM2\", \"MA3\", \"EM4\", \"MA5\",\n\"CA5\"), cex = 1.2)\nabline(h = 0, v = 0)\nsegments(w[1, 1], w[1, 2], w[2, 1], w[2, 2], lty = 3, lwd = 2)\ng = eigen(t(x) %*% x/n)\ng1 = g$values\ng2 = g$vectors\nb = g2[, 1:2]\nz = b * sqrt(matrix(g1[1:2], nrow(b), byrow = TRUE))\n# Plot 2: the representation of the variables\nucircle = cbind(cos((0:360)/180 * pi), sin((0:360)/180 * pi))\nplot(ucircle, type = \"l\", lty = \"solid\", col = \"blue\", xlim = c(-1.05, 1.05), ylim = c(-1.05,\n1.05), xlab = \"First Factor - Goods\", ylab = \"Second Factor - Goods\", main = \"French Food data\",\ncex.lab = 1.2, cex.axis = 1.2, cex.main = 1.8, lwd = 2)\nlabel = c(\"bread\", \"vegetables\", \"fruits\", \"meat\", \"poultry\", \"milk\", \"wine\")\ntext(z, label)\n", "output_sequence": "Performs a PCA for the standardized French food data and shows the first two principal components for the individuals and the variables. The normalization corresponds to that of Lebart/Morineau/Fenelon."}, {"input_sequence": "Author: Zografia Anastasiadou, Awdesch Melzer ; %% clear all variables and console and close windows\nclear\nclc\nclose all\n%% load data\nformatSpec = '%s%f%f%f%f%f%f%f';\ndata = readtable('food.dat', 'Delimiter', ' ', 'Format', formatSpec);\ndata.Var1 = [];\nx = table2array(data);\n[n,p] = size(x); % records dimensions of data matrix\nm = mean(x);\nx = (x - repmat(m, n, 1)).*repmat(1./sqrt((n - 1) * var(x)/n), n, 1); % standardizes the data matrix\na = (x * x')/n;\n%% compute eigenvalues\n[gamma, lambda] = eigs(a, 2, 'la'); % two largest eigenvalues and the respective eigenvectors\nlambda = diag(lambda);\nw = -[gamma(:, 1) * sqrt(lambda(1)), gamma(:, 2) * sqrt(lambda(2))];\nnamew = ['MA2'; 'EM2'; 'MA3'; 'EM4';\n'MA5';\n\n%% Plot1: the representation of the indivuals\nfigure\nline([-1.5 1], [0 0], 'Color', 'k')\ntitle('French Food Data')\nxlabel('First Factor - Families')\ntext(w(:, 1), w(:, 2), namew)\nline([w(1,1) w(2,1)], [w(1,2) w(2,2)], 'Color', 'k', 'LineStyle', ':')\nnamez = ['bread '; 'vegetables'; 'fruit '; 'meat ';\n'poultry '; 'milk '; 'wine '];\nb = (x' * x)/n;\n[gam,lamb] = eigs(b, 2, 'la');\nlamb = diag(lamb);\n% change the sign of some eigenvectors . This is done only to make\n% easier the comparison with R results.\ngam(:, 2) = - gam(:, 2);\nz = [gam(:, 1) * sqrt(lamb(1)), gam(:, 2) * sqrt(lamb(2))];\n%% Plot1: the representation of the variables\nline([-1.2 1.2], [0 0], 'Color', 'k')\nhold on\ncircle = rsmak('circle');\nfnplt(circle)\nxlabel('First Factor - Goods')\nxlim([-1.2 1.2])\ntext(z(:,1), z(:,2), namez)\n", "output_sequence": "Performs a PCA for the standardized French food data and shows the first two principal components for the individuals and the variables. The normalization corresponds to that of Lebart/Morineau/Fenelon."}, {"input_sequence": "Author: Mengmeng Guo ; %% clear all variables and console and close windows\nclear\nclc\nclose all\n\n%% load data\ndata = load('bostonh.dat');\n%% First we use the ANCOVA model to do the regression\n%% fit1\nx = [ones(size(data(:, 1))), data(:, 4:6), data(:, 8:13)];\ny = data(:, 14);\n[m, n] = size(x);\ndf = m - n;\nb = inv(x' * x) * x' * y;\nyhat = x * b;\nr = y - yhat;\nmse = r' * r./df;\ncovar = inv(x' * x).*mse;\nse = sqrt(diag(covar));\nt = b./se;\nk = t2.^2./(df + t2.^2);\np = 0.5.*(1+sign(t2).*betainc( k, 0.5, * df)) ;\nPvalues = 2 * (1 - p);\ntablex = [b se t Pvalues];\ndisp('Table with coefficient estimates, Standard error, value of the t-statistic and ');\ndisp('p-value (for the intercept (first line) and the 9 variables (4, 5, 6, 8, 10, 12 and 13))');\ndisp(tablex)\n%% fit2, we create x15 and replace x9 with x15, we also transform x4.\ndata(:, 15) = ones(size(data(:, 1)));\nfor i = 1:length(data)\nif data(i, 4)==0\nend\nif data(i, 9) < median(data(:, 9))\ndata(i, 15) = -1;\nend\nend\nx = [ones(size(data(:, 1))) data(:, 4:6) data(:, 8) data(:, 10:13) data(:, 15)];\nb = inv(x' * x) *x' *y;\np = 0.5.*(1 + sign(t2).*betainc(k, 0.5, * df)) ;\ndisp('p-value (for the intercept (first line) and the 10 variables (transformed 4, 5, 6, 8, 10, 12 ,13 and 15))');\n\n%% fit3, testing the interactions of x4 with x12 and x15\nxx = [x, data(:, 4).*data(:, 12), data(:, 4).*data(:, 15)];\n[m, n] = size(xx);\nb = inv(xx' * xx) * xx' * y;\nyhat = xx * b;\ncovar = inv(xx' * xx).*mse;\np = 0.5.*(1+sign(t2).*betainc( k, 0.5, 0.5*df)) ;\ndisp('p-value (for the intercept (first line) and the 12 variables ')\ndisp('(transformed 4, 5, 6, 8, 10, 12 ,13, 15 and the interactions of 4 with 12 and 4 with 15))');\n", "output_sequence": "Computes the ANCOVA model with Boston housing data. We add binary variable to the ANCOVA model, and try to check the effect of the new factors on the dependent variable."}, {"input_sequence": "Author: Mengmeng Guo ; # Clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# load data\ndata = read.table(\"bostonh.dat\")\n# First we use the ANOVA model to do the regression\nx1 = data[, 1]\nfit = lm(x14 ~ x4 + x5 + x6 + x8 + x9 + x10 + x11 + x12 + x13)\nsummary(fit)\n# We replace x9 by x15, and also transform x4.\nn = length(x4)\nfor (i in 1:n) {\nif (x4[i] == 0)\n}\ny = 0\nif (x9[i] >= median(x9))\ny[i] = 1 else y[i] = -1\nx15 = y\nfit2 = lm(x14 ~ x4 + x5 + x6 + x8 + x10 + x11 + x12 + x13 + x15)\nsummary(fit2)\n# This is used for testing the interaction of x4 and x12, and also x4 and x15.\nfit3 = lm(x14 ~ x4 + x5 + x6 + x8 + x10 + x11 + x12 + x13 + x15 + x4 * x12 + x4 * x15)\nsummary(fit3)\n", "output_sequence": "Computes the ANCOVA model with Boston housing data. We add binary variable to the ANCOVA model, and try to check the effect of the new factors on the dependent variable."}, {"input_sequence": "Author: Song Song ; # clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# install and load packages\nlibraries = c(\"KernSmooth\", \"graphics\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# load data\nxx = read.table(\"bank2.dat\")\n# Compute a kernel density estimates\ndj = bkde2D(xx[, 4:5], bandwidth = 1.06 * c(sd(xx[, 4]), sd(xx[, 5])) * 200^(-1/5))\nd1 = bkde(xx[, 4], gridsize = 51)\ndp = (d1$y) %*% t(d2$y)\n# plot\npersp(d1$x, d2$x, dp, box = FALSE, main = \"Joint estimate\")\n", "output_sequence": "Gives plots of the product of univariate and joint kernel density estimates of variables X4 and X5 of the Swiss bank notes."}, {"input_sequence": "Author: Song Song ; \n%% clear loaded variables and close windows\nclear all;\nclc;\n\nformat long;\n%% load dataset\nxx = load('bank2.dat');\nh = 1.06*[std(xx(:, 4)) std(xx(:, 5))].* 200.^(-1/5);\n% Compute a kernel density estimates\n[f1,xi1] = ksdensity(xx(:, 4));\n%% plot\n[xxi,yyi] = meshgrid(xi1,xi2);\npdfxy = ff1.*ff2;\nfigure(1)\nmesh(xxi,yyi,pdfxy)\nset(gca, 'XLim',[min(xi1) max(xi1)])\nxrange = min(xx(:,4)):(max(xx(:,4))-min(xx(:,4)))./99:max(xx(:,4));\n% steps\nendx = length(xrange);\nndata = length(xx(:,4));\nfor xxxi = 1:endx\nfor yyyi = 1:endy\nu1 =(xrange(xxxi)-xx(:,4))/h(1);\nu = [u1,u2]';\nfor is = 1:ndata\nKD(is) = 1/(2*pi)^(2/2)*exp(-1/2*u(:,is)'*u(:,is));\nend\nfhat(xxxi,yyyi) = mean(KD)/prod(h);\nend\nend\nfigure(2)\nmesh(yyi,xxi,fhat)\n", "output_sequence": "Gives plots of the product of univariate and joint kernel density estimates of variables X4 and X5 of the Swiss bank notes."}, {"input_sequence": "Author: Song Song ; function [xhat, yhat] = PAVAlgo(x,y) %Define the PVA algorithm as a function\nfor i = 1:length(x)\ndata = [x y];\nsdata = sortrows(data,1); % Sort the bivariate data by the first variable\nxhat = sdata(:,1);\nfor n = 1:length(x)-1 % checking from left to right\nif yhat(n) > yhat(n+1)\nyhat(n:n+1) = mean(yhat(n:n+1))*ones(2,1);\np = 0;\nwhile p < n-1 % checking from right to left\nif yhat(n-p-1) > yhat(n-p)\nyhat(n-p-1:n+1) = mean(yhat(n-p-1:n+1))*ones(p+3,1);\np = p+1;\nelse\np = n-1;\nend\nend\nend\n", "output_sequence": "Gives plots of the product of univariate and joint kernel density estimates of variables X4 and X5 of the Swiss bank notes."}, {"input_sequence": "Author: Song Song ; # Clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# PVA algorithm as function\nPAV = function(X, Y, FUN = mean, ..., left = TRUE, ordered = FALSE){\nn = length(Y)\nstopifnot(n > 1, n == length(X), !any(is.na(c(X, Y))))\n\nif(!ordered){\n.order = order(X);\nY = Y[.order]; X = X[.order]\n}\nY.hat = Y; work.index = 1:n;\n\nif(left){\nmain.index = 1:(n-1)\nrepeat{\n.increasing = (diff(Y.hat) < 0) # looking from the left to the right\nif(!any(.increasing)){\nbreak\ni = min(main.index[.increasing])\n.i.and.ii = work.index %in% work.index[i:(i + 1)]\nY.hat[.i.and.ii] = FUN(Y.hat[.i.and.ii], ...)\nwork.index[.i.and.ii] = work.index[i]\n}\n}else{\nmain.index = 2:n\n.decreasing = (diff(Y.hat) < 0) # looking from the right to the left\nif(!any(.decreasing)){\ni = max(main.index[.decreasing])\n.i.and.ii = work.index %in% work.index[(i - 1):i]\nlist(Y.hat = Y.hat, Y = Y, X = X)\n}\n# Sample data from a bivariate t-distribution with df = 4\n#install.packages(\"copula\")\nlibrary(\"copula\")\nset.seed(1)\nData = apply(X = rCopula(250, dim = 2)), 2, FUN = qt, df = 4);\nresult.left = PAV(Data[, 1], Data[, 2], FUN = mean)\n# Scatterplot of simulated data\npar(mai = c(0.9, 0, 0) + 0.1, cex.axis = 1.5, cex.lab = 1.5)\nplot(Data, axes = FALSE, xlab = expression(X), ylab = expression(Y), pch = 20)\ngrid(lwd = 1.5)\npoints(result.left$X, pch = 20, lwd = 1.75)\naxis(1)\nbox(lwd = 1.5)\n# Scatterplot of simulated data together with regression line\nplot(result.left$X, axes = FALSE, xlab = expression(X), ylab = expression(Y), pch = 20)\npoints(result.left$X, result.left$Y.hat, type = \"l\", pch = 20, lwd = 2.5, col = \"darkgray\")\n", "output_sequence": "Gives plots of the product of univariate and joint kernel density estimates of variables X4 and X5 of the Swiss bank notes."}, {"input_sequence": "Author: Zografia Anastasiadou ; % ---------------------------------------------------------------------\n% Book: MVA\n% ---------------------------------------------------------------------\n% Quantlet: MVAdisfbank\n% Description: MVAdisfbank performs a Fisher discrimination analysis\n% of the Swiss bank notes (bank2.dat), computes the\n% miss-classification rates for the whole dataset and\n% displays nonparametric density estimates of the\n% projected data.\n% Corresponds to example 13.6 in MVA.\n% Usage: -\n% Inputs: None\n% Output: Fisher discrimination analysis of the Swiss bank notes\n% (bank2.dat), miss-classification rates for the whole\n% dataset and plot of the nonparametric density\n% estimates of the projected data.\n% Example: -\n% Author: Wolfgang Haerdle, Vladimir Georgescu, Jorge Patron,\n% Song\n%% clear all variables and console and close windows\nclear\nclc\nclose all\n%% load data\nx = load('bank2.dat');\nxg = x(1:100,:); % Group first 100 observations\n% Determine the mean for the seperate groups and overall sample\nmg = mean(xg);\nmf = mean(xf);\nm = (mf+mg)/2;\nw = 100*(cov(xg)+cov(xf));\nd = mg-mf; % Difference in means\na = inv(w)*d'; % Determine the factors for linear combinations\nyg = (xg-repmat(m,100,1))*a; %Discriminant rule\n% Number of misclassified genuine notes\nsumg = 0;\nfor i=1:length(yg)\nif yg(i)<0\nsumg = sumg+1;\nend\nend\ndisp('Number of missclassified genuine notes');\ndisp(sumg)\n% Number of misclassified forged notes\nsumf = 0;\nfor i=1:length(yf)\nif yf(i)>0\nsumf = sumf+1;\ndisp('Number of missclassified forged notes');\ndisp(sumf)\n% Show densities of projections of genuine and counterfeit bank %notes by Fisher\u2019s discrimination rule.\n[f1,ygi1] = ksdensity(yg);\nhold on\ntitle('Swiss Bank Notes');\nylabel('Densities of Projections');\nxlim([-0.2 0.2]);\nplot(ygi1,f1,'color','b','LineWidth',2,'LineStyle','--');\ntext(-0.14,3.5,'Forged','Color','r');\nbox on\nhold off\n", "output_sequence": "Performs a Fisher discrimination analysis of the Swiss bank notes, computes the misclassification rates for the whole dataset and displays nonparametric density estimates of the projected data."}, {"input_sequence": "Author: Zografia Anastasiadou ; # clear all variables\nrm(list = ls(all = TRUE))\ngraphics.off()\n# reads the bank data\nx = read.table(\"bank2.dat\")\nxg = x[1:100, ] # Group first 100 observations\nmg = colMeans(xg) # Determine the mean for the seperate groups and overall sample\nmf = colMeans(xf)\nm = (mg + mf)/2\nw = 100 * (cov(xg) + cov(xf)) # matrix w for within sum-of-squares\nd = mg - mf # Difference in means\na = solve(w) %*% d # Determine the factors for linear combinations\nyg = as.matrix(xg - matrix(m, nrow = 100, ncol = 6, byrow = T)) %*% a # Discriminant rule for genuine notes\nxgtest = yg\nsg = sum(xgtest < 0) # Number of misclassified genuine notes\nsf = sum(xftest > 0)\nfg = density(yg) # density of projection of genuine notes\n# plot\nplot(ff, lwd = 3, col = \"red\", xlab = \"\", ylab = \"Densities of Projections\", main = \"Densities of Projections of Swiss bank notes\",\nxlim = c(-0.2, 0.2), cex.lab = 1.2, cex.axis = 1.2, cex.main = 1.8)\nlines(fg, lwd = 3, col = \"blue\", lty = 2)\ntext(mean(yf), 3.72, \"Forged\", col = \"red\")\n", "output_sequence": "Performs a Fisher discrimination analysis of the Swiss bank notes, computes the misclassification rates for the whole dataset and displays nonparametric density estimates of the projected data."}, {"input_sequence": "Author: Anastasia Stepanchenko ; # Clear variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# Install and load packages\nlibraries = c(\"ggplot2\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# Load data\noptions(stringsAsFactors=FALSE)\nobj.names = load(\"yamlQN.RData\", .GlobalEnv)\nyaml.DocNumChars = function(yaml_doc) {\nrow_name = names(yaml_doc)\nfield_name = c()\n\nfor (i in 1:length(row_name)){\nfield_name = c(field_name, row_name[i])\nfield_nchar = c(field_nchar, nchar(yaml_doc[i]))\n}\nField_nChars = data.frame(fields = field_name, nchars = field_nchar)\nField_nChars = Field_nChars[order(-Field_nChars$nchars),]\nreturn (Field_nChars)\n}\n# Plot number of char symbols of all different fields in a given document\ndoc = 1\nyaml.plotDocNumChars = function(yaml_doc) {\nField_nChar = yaml.DocNumChars(yaml_doc)\nField_nChar = transform(Field_nChar, fields = factor(fields, levels = Field_nChar$fields))\nplot = ggplot(data = Field_nChar, aes(x = fields, y = nchars, fill = fields)) +\ngeom_bar(colour = \"black\", fill = \"#DD8888\", width = .8, stat = \"identity\") +\nguides(fill=FALSE) + xlab(\"Fields\") + ylab(\"Number of char symbols\") +\nggtitle(\"Number of char symbols of all different fields in a given document\") + theme(axis.text.x = element_text(angle = 50, hjust = 1))\nreturn(plot)\nyaml.plotDocNumChars(yaml_list[[doc]])\nyaml.NumChars = function(yaml_dset) {\nfor (i in 1:length(yaml_dset)){\nrow_name = names(yaml_dset[[i]])\nfor (j in 1:length(row_name)){\nif(row_name[j] %in% field_name) {\nm = match(row_name[j], field_name)\nfield_nchar[m] = field_nchar[m] + nchar(yaml_dset[[i]][j])}\nelse {\nfield_name = c(field_name, row_name[j])\nfield_nchar = c(field_nchar, nchar(yaml_dset[[i]][j]))}\n}\nyaml.plotNumChars = function(yaml_dset){\nField_nChar = yaml.NumChars(yaml_dset)\nField_nChar = Field_nChar[order(-Field_nChar$nchars),]\nggtitle(\"Number of char symbols of all different fields in the dataset\") + theme(axis.text.x = element_text(angle = 50, hjust = 1))\n# Plot number of char symbols of all different fields in the data set\nyaml.plotNumChars(yaml_list)\n", "output_sequence": "Plots the number of char symbols of all different fields in YAML text corpus containing Quantnet metainfo.txt files."}, {"input_sequence": "Author: Ramona Steck ; # clear all variables and close windows\nrm(list = ls(all = TRUE))\ngraphics.off()\n# specify working directory and load results with pre-simulated critical values\n# setwd(\"...\")\nAGumbel = read.table(\"AGumbel\")\n# results adaptively simulated critical values\nBGumbel = read.table(\"BGumbel\")\n# dates\ndates = read.table(\"dates\")\ndates = dates[-c(1:5793), ]\ndates = as.matrix(dates)\nlabels = as.numeric(format(as.Date(dates, \"%Y-%m-%d\"), \"%Y\"))\nwhere.put = c(which(diff(labels) == 1) + 1)\na = c(1:9)\nfor (k in 0:8) {\ng[k + 1] = trunc(40 * 1.25^k)\n}\n# no. of conducted test steps at corresponding timepoint for pre-simulated critical values\ntstep = matrix(0, nrow = dim(AGumbel)[1], ncol = 1)\nfor (i in 1:dim(AGumbel)[1]) {\nfor (j in 1:9) {\nif (AGumbel[i, 1] == g[j]) {\ntstep[i] = (a[j] + 1)\n}\n}\n# no. of conducted test steps at corresponding timepoint for adaptively simulated critical values\ntstep1 = matrix(0, nrow = dim(AGumbel)[1], ncol = 1)\nif (BGumbel[i, 1] == g[j]) {\ntstep1[i] = (a[j] + 1)\n# differences of no. of test steps and ML for the two approaches\ndiff.test = tstep - tstep1\ndiff.ml = AGumbel[, 5] - BGumbel[, 5]\n# the plots\nlayout(matrix(1:2, nrow = 2, byrow = T))\npar(mai = (c(0, 0.8, -0.3, 0.1) + 0.4), mgp = c(3, 0.3, 0))\nplot(diff.test, type = \"l\", lwd = 1, col = \"blue3\", lty = \"solid\", axes = F, frame = T, xlab = \"\", ylab = \"D.steps\",\ncex.lab = 1, yaxt = \"n\", xaxt = \"n\")\naxis(2, tck = -0.02, las = 1, cex.axis = 1)\naxis(1, at = where.put, labels = labels[where.put], tck = -0.02, cex.axis = 1)\nplot(diff.ml, type = \"l\", lwd = 1, col = \"red3\", lty = \"solid\", axes = F, frame = T, xlab = \"\", ylab = \"D.ML\",\naxis(1, at = where.put, labels = labels[where.put], tck = -0.02, cex.axis = 1)\n", "output_sequence": "Calculates and plots the no. of different test steps per time point for the two approaches of LCP. The results from LCP correspond to the approach with pre-simulated critical values and the approach with adaptively simulated critical values. In addition, the dynamics of the difference in the ML criterion are plotted as well. The estimated model is a three-dimensional HAC with Gumbel generators."}, {"input_sequence": "Author: Alex Truesdale ; # -*- coding: utf-8 -*-\n\"\"\"\nCreated Jan 27, 2018\n@author: Alex Truesdale\nJupyter Notebook for running regression on data.\nimport os\nimport time\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\n# Introduction.\nprint('==============================================================================')\nprint(\"\\n This script reads in the final data collected for this project. The data \\n \\\nselection here contains a number of features relating to distances to \\n \\\ncertain amenities, area activity, ratings of nearby locations, and more. \\n \\\nThis data is regressed (OLS) against listing prices for ~5000+ apartment \\n \\\nlistings to study the effects of these social components. \\n\")\nprint()\ntime.sleep(8)\n# Read in data.\nprint(\"------ reading data from 'final_listings.csv'\")\nlistings = pd.read_csv('final_listings.csv')\nX, y = listings.iloc[:, :-3], np.log(listings.iloc[:, -1]).values.reshape((-1, 1))\ntime.sleep(1)\n# Fit regression model.\nprint(\"------ defining model and fitting data \\n\")\nregression_model = sm.OLS(y, X)\nregression_fit = regression_model.fit()\ntime.sleep(2.5)\n# Summary of model.\nsummary = regression_fit.summary()\nprint(summary)\ntime.sleep(1.5)\n# Output summary at HTML and read back in as DataFrame.\nprint(\"------ extracting R sq. and P values to 'regression_summary.csv' \\n\")\nresults_as_html = summary.tables[1].as_html()\nsummary_df = pd.read_html(results_as_html, header = 0, index_col = 0)[0]\nsummary_df = summary_df.loc[:, ['coef', 'P>|t|']]\nsummary_df['P>|t|'] = round(summary_df['P>|t|'], 3)\nsummary_df['coef'] = round((np.exp(summary_df['coef']) - 1) * 100, 4)\nsummary_df.columns = ['Unit Coef %', 'P>|t|']\nsummary_df.to_csv('regression_summary.csv')\n", "output_sequence": "Ordinary Least Squares regression (OLS) for analysis of correlation between real estate listing prices and social / location-based features for real estate within Berlin, DE."}, {"input_sequence": "Author: Awdesch Melzer ; # clear variables and close windows\nrm(list=ls(all=TRUE))\ngraphics.off()\n\n# install packages\n# install.packages(\"simba\")\nlibrary(simba)\n# setwd(\"C:/...\") # please change working directory\nx = read.table(\"carmean2.dat\") # load data\nx = as.matrix(x[,2:9]) # retrieve Renault, Rover, Toyota\nx1719 = x[c(17:19),]\nx.mu = apply(x1719,2,mean) # column means\ny = matrix(0,nrow(x1719),ncol(x1719)) # empty matrix\n# fill binary matrix\nfor (i in 1:nrow(x1719)){ # if x(i,k)>x_bar(k): 1, else 0\nfor(k in 1:ncol(x1719)){\nif(x1719[i,k]>x.mu[k]){\ny[i,k]=1\n}else{\ny[i,k]=0\n}\n}\n# similarity coefficients for binary data\nsim(y,method=\"jaccard\") # jaccard\nsim(y,method=\"simplematching\") # simple matching\n", "output_sequence": "computes the Jaccard, simple matching and Tanimoto proximity coefficients for binary car data"}, {"input_sequence": "Author: Awdesch Melzer ; clear all\nclose all\nclc\n\n% load data\nload carc.txt\nx = carc;\n[n1 n2] = size(x); % data size\nrange = max(x) - min(x) + (max(x)==min(x)); % data range\nrange_mat = ones(n1,1)*range;\nt=(x-min_mat)./(range_mat); %standardize data\nC = x(:,13);\n% extract observations for USA, Japan, Europe\nk=0;\nfor i=1:length(x)\nif x(i,13)==1\nk=k+1;\nus(k,:)=t(i,:);\n\nelseif x(i,13)==2\nl=l+1;\nja(l,:)=t(i,:);\n\nelseif x(i,13)==3\nm=m+1;\neu(m,:)=t(i,:);\nend\n% plot\nhold on\np1 = plot(us','linewidth',1.6,'Color','k','LineStyle','-');\ntitle('Parallel Coordinate Plot (Car Data)','FontSize',16,'FontWeight','Bold')\nP1 = hggroup;\nset(p1,'Parent',P1)\nset(get(get(P1,'Annotation'),'LegendInformation'),...\n'IconDisplayStyle','on'); % Include this hggroup in the legend\nset(get(get(P2,'Annotation'),'LegendInformation'),...\nlegend('us','japan','europe','Location','SouthOutside','Orientation','horizontal')\nset(gca,'LineWidth',1.6,'FontSize',16,'FontWeight','Bold')\nhold off\n% to save plot please uncomment following lines\n% print -painters -dpng -r600 SMSpcpcar.png\n", "output_sequence": "computes the Jaccard, simple matching and Tanimoto proximity coefficients for binary car data"}, {"input_sequence": "Author: Awdesch Melzer ; rm(list=ls(all=TRUE))\ngraphics.off()\n\n# load data\nload(\"carc.rda\")\n# install and load packages\nlibraries = c(\"MASS\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)\n})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n", "output_sequence": "computes the Jaccard, simple matching and Tanimoto proximity coefficients for binary car data"}, {"input_sequence": "Author: Awdesch Melzer ; % ----------------------------------------------------------------------------\n% Book: Multivariate Statistics: Exercises and Solutions Series\n% ----------------------------------------------------------------------------\n% Quantlet: SMSjordandec\n% Description: SMSjordandec performs a Jordan decomposition of a symmetric\n% (3 x 3) matrix using its eigenvalues and eigenvectors.\n% Usage: -\n% Keywords: spectral decomposition, eigenvalues, eigenvectors,\n% decomposition, eigenvalue decomposition\n% See also: SMSellipse, SMSjordandec\n% Author: Awdesch Melzer 20121126\n% clear variables and close windows\nclear all\nclc\n% main calculation\nA = [[1;2;3],[2;1;2],[3;2;1]]; % matrix A\n[vec val] = eigs(A); % eigenvalues and eigenvectors of A\ngama = vec % matrix of eigenvectors\nlambda = val % diagonal matrix containing the eigenvalues\ngama*lambda*gama' % Jordan decomposition\n", "output_sequence": "computes the Jaccard, simple matching and Tanimoto proximity coefficients for binary car data"}, {"input_sequence": "Author: Awdesch Melzer ; # ----------------------------------------------------------------------------\n# Book: Multivariate Statistics: Exercises and Solutions Series\n# ----------------------------------------------------------------------------\n# Quantlet: SMSjordandec\n# Description: SMSjordandec performs a Jordan decomposition of a symmetric\n# (3 x 3) matrix using its eigenvalues and eigenvectors.\n# Usage: -\n# Keywords: spectral decomposition, eigenvalues, eigenvectors,\n# decomposition, eigenvalue decomposition\n# See also: SMSellipse, SMSjordandec\n# Author: Dana Chromikova\n# clear variables and close windows\nrm(list=ls(all=TRUE))\ngraphics.off()\n# main calculation\nA = cbind(c(1,2,3),c(2,1,2),c(3,2,1)) # matrix A\nu = eigen(A) # eigenvalues and eigenvectors of A\ngama\nlambda = diag(u$val) # diagonal matrix containing the eigenvalues\nlambda\ngama%*%lambda%*%t(gama) # Jordan decomposition\n", "output_sequence": "computes the Jaccard, simple matching and Tanimoto proximity coefficients for binary car data"}, {"input_sequence": "Author: Zdenek Hlavka ; # Clear workspace\nrm(list=ls(all=TRUE))\ngraphics.off()\n# install and load packages\nlibraries = c(\"stats\",\"MASS\")\nlapply(libraries, function(x) if (!(x %in% installed.packages())) {\ninstall.packages(x)})\nlapply(libraries, library, quietly = TRUE, character.only = TRUE)\n# setwd(\"C:/...\") # please change the working directory\nload(\"wais.rda\")\nfisher.w=lda(group~.-subject,prior=c(0.5,0.5),data=wais)\nprediction=predict(fisher.w, wais)$class\nt=table(wais$group,prediction)\nprint(t)\naper=(sum(t)-sum(diag(t)))/sum(t)\nprint(aper)\ncorrect=0\nfor (i in 1:nrow(wais)) {\nfisher.t=lda(group~.-subject,prior=c(0.5,0.5),subset=-i,data=wais)\npredict=predict(fisher.t, wais[i,])$class\nif (predict==wais[i,\"group\"]) correct=correct+1\n}\naer=1-correct/nrow(wais)\nprint(aer)\n", "output_sequence": "computes Fisher's linear discrimination function for the wais data set. The apparent error rate (APER) and the actual error rate (AER) are computed. APER = 0.2449, AER = 0.3061."}, {"input_sequence": "Author: Zdenek Hlavka ; clear all\nclose all\nclc\n\n%% Fisher's LDA for WAIS data\nload('wais.dat')\nmdl = ClassificationDiscriminant.fit(wais(:,3:end),wais(:,1),'Prior','uniform');\n% LDA model prediction\npred = predict(mdl,wais(:,3:end));\n% truth-prediction\nlabel = wais(:,1);\n[C,order] = confusionmat(label,pred);\ncols = cellstr(strvcat(' ',' ','Prediction',' '));\nrows1 = cellstr(strvcat(' ',order));\ncols1 = cellstr(strvcat(order(1,:), num2str(C(:,1))));\ntable = [rows, cols1, cols2];\ntable = [cols'; table];\ndisp(table)\n%% compute apparent error rate (APER)\naper = (sum(sum(C))-sum(diag(C)))/sum(sum(C));\ndisp(aper)\n%% compute actual error rate (AER)\ncorrect=0;\nfor i = 1:size(wais,1)\nsubset = wais;\nmdl = ClassificationDiscriminant.fit(subset(:,3:end),subset(:,1),'Prior','uniform');\npred = predict(mdl,wais(i,3:end));\nif (pred==wais(i,1))\ncorrect=correct+1;\nend\nend\naer = 1-correct/size(wais,1);\ndisp(aer)\n", "output_sequence": "computes Fisher's linear discrimination function for the wais data set. The apparent error rate (APER) and the actual error rate (AER) are computed. APER = 0.2449, AER = 0.3061."}, {"input_sequence": "Author: V\u00edctor G\u00f3mez ; %\n% Practical example for the paper ''A new State Space Methodology to\n% Disaggregate Multivariate Time Series'', Journal of Time Series Analysis,\n% 30, 97-124, by Gomez and Aparicio, (2009).\n% The basic solution is specified by setting theta=0\n% The specific solution is specified by setting theta non equal to zero.\n%\n% Copyright (c) 21 July 2003 by Victor Gomez\n% Ministerio de Hacienda, Direccion Gral. de Presupuestos,\n% Subdireccion Gral. de Analisis y P.E.,\n% Alberto Alcocer 2, 1-P, D-34, 28046, Madrid, SPAIN.\n% Phone : +34-915835439\n% E-mail: VGomez@sepg.minhap.es\n% The author assumes no responsibility for errors or damage resulting from the use\n% of this code. Usage of this code in applications and/or alterations of it should\n% be referenced. This code may be redistributed if nothing has been added or\n% removed and no money is charged. Positive or negative feedback would be appreciated.\n\nclear\nnbeta = 0;\nipi_19951_200712 = load(fullfile('data', 'ipi_19951_200712.dat'));\nipi = ipi_19951_200712(:, 2);\np = 4;\nlam = 1;\n%plot both series\nbg_yearm = 1995;\nfreqm = 12;\ndateim = cal(bg_yearm, bg_perm, freqm);\ntsplot(ipi, dateim, 'IPI');\ndisp('strike a key to continue')\npause\nbg_yeart = 1995;\nfreqt = 4;\ndateit = cal(bg_yeart, bg_pert, freqt);\ntsplot(pib, dateit, 'GDP');\n%compute IPI quarterly figures\n[nq, junk] = size(pib);\nipiq = zeros(size(pib));\nfor i = 1:nq\nipiq(i) = mean(ipi((i - 1)*3+1:i*3));\nend\ntsplot(ipiq, dateit, 'IPIQ');\nyq = [pib, ipiq];\nvnames =char('GDP','Quarterly aggregated IPI');\ntsplot(yq, dateit, vnames);\ndisp('strike a key to continue');\n%compute GDP annual figures\nny = floor(nq/4);\npiby = zeros(ny, 1);\nfor i = 1:ny\npiby(i) = mean(pib((i - 1)*4+1:i*4));\nbg_yeary = 1995;\nfreqy = 1;\ndateiy = cal(bg_yeary, bg_pery, freqy);\ntsplot(piby, dateiy, 'GDPY');\nnq = 4 * ny;\nipiq = ipiq(1:nq, :);\n%compute bivariate series\nyq = [];\nyq = [yq; [ipiq(4*(i - 1)+1:4*i)', piby(i)]];\ntname = 'agtrimanss1';\nfname = fullfile('results', 'agtrimanss1.txt');\n%first, form univariate structural model and, from this, construct\n%bivariate structural model (this is easier than constructing the bivariate\n%structural model from scratch)\n%univariate structural model defined according to ssm_matlab\n% codes for the components of the univariate structural model:\n% level = -1 constant\n% 1 stochastic structural (Butterworth sine)\n% 2 Butterworth tan\n% slope = -1 constant\n% 0 zero\n% seas = -1 fixed dummy seasonality\n% 1 stochastic dummy seasonality\n% irreg = 1 stochastic\n% arp = k autoregressive of order k\n% (see ssm_matlab manual for further information)\nY = [];\ncomp.level = [1, NaN];\ncomp.conout = 'level';\ncomp.freq = freqt;\ncomp.datei = dateit;\nnpr = 0;\n[models, ferror] = suusm(comp, ipiq, Y, npr);\n%end of univariate structural model defined according to ssm_matlab\nnreg = 0;\nnr = length(models.pvar);\n% Bivariate structural model\nTb = kron(models.T, eye(2));\n[nt, mt] = size(models.T);\nib = [0, 2 * nt];\n% order in the standard deviations (conc):\n% 1 2 3 - level\n% 7 8 9 - seasonal\n% 10 11 12 - irregular\nx0 = [.1, ... %sigma level s21 (s11 one parameter is concentrated out)\n.1, ... %sigma slope s11 s22\n.1, 0, ... %sigma seasonal s11 s22 is not identified)\n.1, ... %sigma irreg. s11 s22\n];\npfix = [8]; %fixed parameters\npvar = [1:7, 9:11]; %free parameters\n%parameter for basic solution\n% theta=0.0;\n%parameter for specific solution\n% theta=0.5271; %parameter for proportional seasonality (0.2002/1.8767+0.0361/0.0381)/2\n% theta=.75232; %parameter for proportional seasonality, Fernandez\nxv = x0(pvar);\nstordb = [2, 3, 4, 5, 6, 7, 8, 9, 10, 12]; %standard deviations to be estimated\nconcb = 1; %standard deviation concentrated out\n[nhb, mhb] = size(Hb);\nxp = zeros(1, 12);\nxp(stordb) = x0;\nxp(concb) = 1; %concentrated parameter\nHb(1, 1) = xp(1);\nj = 5;\nfor i = 6:2:nhb\nHb(i, j) = xp(8);\nj = j + 2;\n% for i=6:2:nhb\n% Hb(i,5)=xp(8);\n% end\nGb(1, mhb-1) = xp(10);\nmodels.T = Tb;\nmodels.stord = stordb;\n% end of bivariate structural model\nclear Tb Hb Zb Gb ib insb stordb concb;\n%aggregation pattern\nD = [1, 0, 0; 0, 1, 0, 0; 0, 1, 0, 0; ...\nmodels.D = D;\ns = freqt;\nstcs = 1;\nchb = 0;\nQ = [];\nmodels.nalphao = [];\n[modelsn, H_p, Q] = modstr_agtrimanbs(xv, pfix, pvar, xf, models, Q, stcs, p, ny);\nmodels.nalphao = modelsn.nalphao;\nZ = modelsn.Z;\n[nalphao, junk] = size(T);\nsmname = 'agtrimanbsfun';\nmodelsb = models;\n%parameter optimization\n%Levenberg-Marquardt\ninfo.f = smname;\ninfo.tr = 1;\ninfo.tolf = 1e-4;\ninfo.tolx = sqrt(info.tolf);\ninfo.maxit = 300;\ntic\n[x, J, ff, g, iter, conf] = marqdt(info, xv, yq, pfix, pvar, xf, 0, modelsb, Q, p, ny);\ntoc\nxx = x0;\nxx(pvar) = x; %estimated parameters\n% get residuals and estimator\n[F, e, hb, Pevf, A, P] = eval([smname, '(x,yq,pfix,pvar,xf,1,modelsb,Q,p,ny);']);\n% compute residual diagnostics\nSs = e' * e;\nne = length(e); %residual sum of squares\ndisp('concentrated parameter:')\nconp = Ss / (ne - nr); %estimated sigma square\ndisp('square root of concentrated parameter:')\nsconp = sqrt(conp);\n% compute prediction error variance (finite sample)\nPevf = Pevf * conp;\ndisp('standard error (finite sample)')\nSPevf = sqrt(diag(Pevf));\n% lagl=min(36,max([floor(.2*ny) 3*s 10]));\nlagl = 36;\nndrs = ne + nbeta;\ninfr = rescomp(e, lagl, nr, Ss, conp, Ff, ndrs, nbeta);\n%standard errors via second derivatives of log likelihood\nxt = x';\nH = fdhess('logF', xt, smname, yq, pfix, pvar, xf, 0, modelsb, Q, p, ny);\nSS = inv(H/2) / (ne - nr);\nse = sqrt(abs(diag(SS)))';\n%t-values\ndisp('t-values:')\ntt = zeros(size(xx));\ntt(pfix) = NaN;\n% tt(pvar)=x./se\n%file for output\nfid = fopen(fname, 'w');\n% fid=1;\n%print estimation results\ninft = minft(fid, 1, 9, 3, 1);\nvnames = char('IPI','GDP');\nprtser(fid, vnames(1, :), ipiq, [], nq, dateit, inft, lam);\nfprintf(fid, 'Estimation results:\\n');\nstord = models.stord;\nnst = length(stord) - arp;\nif nst > 0, xx(1:end-arp) = xx(1:end-arp) * sconp;\nsse = zeros(size(xx));\nsse(pfix) = NaN;\nxx = [sconp, xx];\nsse = [NaN, sse];\nz = [xx', sse', tt'];\nclear in\nin.cnames = strvcat(' Estimate', 'Std. Error', ' T-ratio');\nin.rnames = ['Parameter '];\nrnamess = ['Sigma level 11 '; 'Sigma level 21 '; ...\nin.rnames = [in.rnames; rnamess(conc, :)];\nif nst > 0, for i = 1:nst, in.rnames = [in.rnames; rnamess(stord(i), :)];\nend,\nin.fmt = strvcat('%12.4f', '%12.4f',\nin.fid = fid;\nmprint(z, in);\nfprintf(fid, '\\nResidual standard error:%11.4f', SPevf);\nfprintf(fid, ['\\nParameter ', rnamess(conc, :), ' is concentrated out of the likelihood\\n']);\n%print residual diagnostics\nprintres(fid, infr);\n%end of residual diagnostics\nplotres([], [], 1.96, 'residuals', 1, 0, [], 0, [], infr, 1, 1);\n%smoothing starts\n% [Xt,Pt,g,M]=eval(['agtrimanbssmt' '(x,yq,s,pfix,pvar,xf,1,modelsb,Q,p,ny);']); %estados\n% iti=11; on=2;\n% format long;\n% Xt(on,:)\n% conp*Pt((on-1)*nalphao+1:on*nalphao,:)\n% format short;\n%interpolation. Update system matrices\nstcs = 0;\n[modelsn, H_p, Q] = modstr_agtrimanbs(x, pfix, pvar, xf, modelsb, Q, stcs, p, ny);\nCa = H_p * Q';\nD = J_p;\nif ~isempty(Y)\nU = ypr;\nelse\nU = [];\nC = Ca(:, end-nalphao+1:end);\n[mucd, junk] = size(C);\n%specific solution\nbeta = [0, theta, 0, 0; 0, theta, 0; 0, theta];\nQs = Q(end-nalphao+1:end, [1:5, 7, 9]) + Q(end-nalphao+1:end, [6, 8, 10]) * beta;\nCn = Cn / Qs;\nC = C + Ca(:, 1:end-nalphao) * Cn;\n%end of specific solution\n% [junk,ndd]=size(D);\n% mucd=nalphao; C=zeros(nalphao); U=ones(nalphao,1); %state vector\n% mucd=ndd; C=zeros(ndd,nalphao); D=eye(ndd); U=zeros(ndd,1); %innovations vector\n% U\n% C=sparse(C);\n[Xt, Pt, g, M] = eval(['agtrimanbsint', '(x,yq,pfix,pvar,xf,modelsb,Q,mucd,U,C,D,p,ny);']);\n% conp*Pt((on-1)*mucd+1:on*mucd,:)\nipiint = [];\nfor on = 1:ny\nyint = Xt(on, :);\nryint = sconp * sqrt((diag(Pt((on - 1)*mucd+1:on*mucd, :)))); %ojo, valores negativos\n% ipiint=[ipiint; yint(1);\nrpibint = [rpibint; ryint(2);\nfprintf(fid, '\\n');\n% dateib=dateim; inftb=inft; fnameb=' Interpolated IPI'; nint=length(ipiint);\n% prtser(fid,fnameb,ipiint,[],nint,dateib,inftb,lam);\ndateib = dateit;\ninftb = inft;\nfnameb = ' Interpolated GDP';\nnint = length(pibint);\nprtser(fid, fnameb, pibint, [], nint, dateib, inftb, lam);\n% fnameb=' Root Mean Squared Errors: IPI'; nint=length(ripiint);\n% prtser(fid,fnameb,ripiint,[],nint,dateib,inftb,lam);\nfnameb = ' Root Mean Squared Errors: GDP';\nnint = length(rpibint);\nprtser(fid, fnameb, rpibint, [], nint, dateib, inftb, lam);\n% error bands\n% lbripiint=ipiint-1.96*ripiint;\nubrpibint = pibint + 1.96 * rpibint;\n%close external file\nif fid ~= 1\nfclose(fid);\n%%To obtain the current ColorOrder, which may be set during startup, get\n%%the property value:\n% M=get(gca,'ColorOrder');\n%%after this, to restore the colors to the original values, do the\n%%following:\n% %set(0,'DefaultAxesColorOrder',M)\n%%the following sets the default ColorOrder to use only the color black and\n%%sets the LineStyleOrder to use solid, dash-dot, and dotted line styles.\n% set(0,'DefaultAxesColorOrder',[0 0 0],...\n% 'DefaultAxesLineStyleOrder','-|--|-.|:')\n% figure\n% vnames=[' IPI'\n% 'Interpolated IPI'];\n% tsplot([ipi ipiint],dateim,vnames);disp('strike a key to continue');\n% pause\n% ipiqint=zeros(size(ipiq)); % interpolated quarterly IPI. It should be equal\n% to IPIQ\n% for i=1:nq\n% ipiqint(i)=mean(ipiint((i-1)*3+1:i*3));\nfigure\nvnames = char('GDP','Interpolated GDP');\ntsplot([pib(1:nq), pibint], dateit, vnames);\n% vnames=['Interpolated IPI',\n% 'Interpolated GDP'];\n% tsplot([ipiint pibint],dateim,vnames);disp('strike a key to continue');\nvnames = char('Interpolated GDP','Lower band','Upper band');\ntsplot([pibint, lbrpibint, dateit, vnames);\n% mspesm = 100*sqrt(sum(((pibint-pib(1:nq))./pib(1:nq)).^2)./nq)\n% aesm = sum(abs((pibint-pib(1:nq))))./nq\nclose all\n", "output_sequence": "A multivariate state space model for the temporal disaggregation of the Spanish GDP is estimated."}, {"input_sequence": "Author: V\u00edctor G\u00f3mez ; % script file to simulate a series that follows a transfer function model.\n%The input series is assumed to follow an ARIMA model. The model is\n%\n% y_t = u_t + v_t\n% = (3.0B - 2.0B^2)x_t + (1 - 0.7B)/(1-B)A_t\n% where x_t follows the model\n% x_t = 1/(1-B)b_t\n% and std(a_t) = 1. and std(b_t)=sqrt(.5).\n\nclear\nfreq = 1;\nx = arimasimeasy(freq, '[p dr q]', [0, 1, 0], 'N', 150, 'discard', ...\n50, 'seed', 18, 'stda', sqrt(.5), 'gft', 0);\nv = arimasimeasy(freq, '[p dr q]', [0, 1, 1], 'thr', [-.7, 1], 'N', 150, 'discard', ...\n50, 'seed', 20, 'stda', 1., 'gft', 0);\n%third, filter inputs by Phi(B)^{-1}Gamma(B)\n%1) without the input model\nthp = [-2., 3., 0.];\nphip = 1.;\nu01 = varmafilp(x, phip, thp);\nomega(:, :, 1) = 0.;\nu02 = varmafilp(x, delta, omega);\n%2) with the input model\n%set up state space form input model\nphix = [-1., 1.];\nPhix = 1.;\nSigma = .5;\nu1 = varmafilp(x, phip, thp, phix, thx, Phix, Thx, Sigma, freq);\nphi(:, :, 1) = 1;\nu2 = varmafilp(x, delta, omega, phi, th, Phi, Th, Sigma, freq);\nu = u2;\ny = v + u;\n% y(6:10)=NaN(5,1); we add some missing values\n% out=arimaeasy(x,freq,'pr',1,'gft',1,'sname','myseries');\n%Identify and estimate the model\nout = tfeasy(y, x, freq, 'gft', 1, 'sname', 'mytfseries', 'tfident', 1, ...\n'autmid', 1);\n", "output_sequence": "Firstly, a time series following a transfer function model with one input is simulated. Then, the model is identified and estimated."}, {"input_sequence": "Author: V\u00edctor G\u00f3mez ; %*****************************************************************\n%\n% SPECTRAL ANALYSIS: US INDUSTRIAL PRODUCTION, CONSUMPTION AND HOURS\n% Series: Cycles of the US IPI, consumption and working hours (monthly data)\n% Time span: 1953.M4 - 2007.M9\n%*****************************************************************\n\nclear;\ny = load(fullfile('data', 'USc3.dat'));\n% Settings for spectral analysis:\nper = 12; %number of seasons\nwin = 2; %Parzen window\ncorlag = 40; %number of correlation and cross correlations\ngraph = 1; %plot results\nvnames = {'US IPI', 'US CONSUMPTION', 'US HOURS'}; %names for the series\nspr = spectralan(y, per, win, corlag, graph, vnames);\n", "output_sequence": "A spectral analysis of three series is performed to see if two of them lead or lag the third one."}, {"input_sequence": "Author: Yegor Klochkov ; import sys\nimport numpy as np\nfrom pandas import read_csv, DataFrame\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom math import ceil\nimport estimator\nfrom misc import *\nimport adaptation\n################################################################################\n#\n# the complete reproduction of the simulations is done in three steps:\n#\n# 1) simulate the data by simulations.py\n# 2) produce the bootstrap values using adaptation_full; script.py helps to run independent tasks in separate processes\n# 3) collect all the bvals, decide the interval lenght and conduct the one step ahead prediction using estimate_with_window; script_ahead.py helps to do it in parallel\ndef adaptation_full(dump=False):\nassert (len(sys.argv) == 1 or len(sys.argv) == 3)\ntau = .1\nquantile = norm.ppf(tau)\nY = read_csv('sim_ts.csv', header=None).values.T\n(tmax, n) = np.shape(Y)\nstep = 20\nmin_t = 60\nif len(sys.argv) == 1:\nend_points = range(tmax, min_t, -step)\nfilename = \"bvals_{}to{}step{}.csv\".format(tmax, min_t, step)\nelse:\nend = (int(sys.argv[1]) // step) * step\nstart = int(sys.argv[2])\nassert (start >= min_t)\nend_points = range(end, start, -step)\nfilename = \"bvals_{}to{}step{}.csv\".format(end, start, step)\nprint(\"doing {} .....\".format(filename))\nlengths = [ceil(60 * (1.25 ** k)) for k in range(8)]\nif not dump:\nresults = {}\nfor end_t in end_points:\nfor i in range(1, len(lengths)):\nif end_t < lengths[i]:\nbreak\nmessage = \"time{}length{}vs{}\".format(end_t, lengths[i], - 1])\nval, bvals = adaptation.bstrap_cp_test(\nY[(end_t - lengths[i]):end_t, :], tau, [lengths[i] - lengths[i - 1]], 30,\nmessage=message, prev_quantile=sigmas[end_t - lengths[i], :] * quantile\n)\nresults[message] = [val] + bvals\ndf = DataFrame.from_dict(results)\ndf.to_csv(filename)\ndef estimate_with_window():\nassert(len(sys.argv) == 4)\nend_t = int(sys.argv[1])\ntau= 0.1\npredict_ahead = []\nfor t in range(end_t, start_t, -1):\nres = estimator.train_mvcaviar(Y[end_t - length: t], tau, epochs=10000, verbose=False)\nquant_last = res.y_predict[[-1], :]\nx_last = np.abs(Y[[t-1], :])\nquant_next = np.matmul(quant_last, res.pars['recurrent']) + np.matmul(x_last, res.pars['kernel']) + res.pars['bias']\npredict_ahead.append(quant_next[0])\nfilename = \"ahead_{}to{}length{}.csv\".format(end_t, start_t, length)\nnp.savetxt(filename, np.array(predict_ahead), delimiter=',')\nif __name__ == \"__main__\":\nestimate_with_window()\n", "output_sequence": "Estimating a Multivariate Conditional Auto-Regressive Value at Risk model (MV-CAViaR) requires adaptation to possibly time varying parameter. Following the strategy of Spokoiny (2009) one can consider a sequence of included time intervals with the same end point, testing each of them agains the largest included one for homogeneity. Performing the tests subsequently one can choose the largest interval that is not rejected for estimation in order to have the least variance with moderately small bias. The standard way to test homogeneity is via Change Point detection. The critical values can be estimated using Multiplier Bootstrap procedure Spokoiny, Zhilova (2013). Based on a joint work with Xiu Xu and Wolfgang H\u00e4rdle we implement interval homogeneity test with bootstrap-simulated critical values."}, {"input_sequence": "Author: Yegor Klochkov ; import subprocess\ntasks = [\n[\"python\", \"test_tf.py\", str(t), - 20)] for t in range(500, 360, -20)\n]\nprocs = [subprocess.Popen(task) for task in tasks]\nfor i, proc in enumerate(procs):\nout, err = proc.communicate()\nif err is not None:\nprint(\"ERROR for TASK {}\".format(\" \".join(tasks[i])))\nprint(err)\n", "output_sequence": "Estimating a Multivariate Conditional Auto-Regressive Value at Risk model (MV-CAViaR) requires adaptation to possibly time varying parameter. Following the strategy of Spokoiny (2009) one can consider a sequence of included time intervals with the same end point, testing each of them agains the largest included one for homogeneity. Performing the tests subsequently one can choose the largest interval that is not rejected for estimation in order to have the least variance with moderately small bias. The standard way to test homogeneity is via Change Point detection. The critical values can be estimated using Multiplier Bootstrap procedure Spokoiny, Zhilova (2013). Based on a joint work with Xiu Xu and Wolfgang H\u00e4rdle we implement interval homogeneity test with bootstrap-simulated critical values."}, {"input_sequence": "Author: Yegor Klochkov ; # -*- coding: utf-8 -*-\n\"\"\"\nCreated on Sat Mar 14 14:19:07 2018\n\n@author: egor\n", "output_sequence": "Estimating a Multivariate Conditional Auto-Regressive Value at Risk model (MV-CAViaR) requires adaptation to possibly time varying parameter. Following the strategy of Spokoiny (2009) one can consider a sequence of included time intervals with the same end point, testing each of them agains the largest included one for homogeneity. Performing the tests subsequently one can choose the largest interval that is not rejected for estimation in order to have the least variance with moderately small bias. The standard way to test homogeneity is via Change Point detection. The critical values can be estimated using Multiplier Bootstrap procedure Spokoiny, Zhilova (2013). Based on a joint work with Xiu Xu and Wolfgang H\u00e4rdle we implement interval homogeneity test with bootstrap-simulated critical values."}, {"input_sequence": "Author: Yegor Klochkov ; import numpy as np\nfrom pandas import read_csv, DataFrame\nimport matplotlib.pyplot as plt\nimport subprocess\nfrom scipy.stats import norm\nfrom math import ceil\nimport sys\nfrom misc import *\ndef run(operation):\npoints = list(range(500, 40, -20))\nstep = 20\nlengths = [ceil(60 * (1.25 ** k)) for k in range(8)]\nselected_lengths = {}\ntasks = []\nfor (start, end) in zip(points[:-1], points[1:]):\ndf = read_csv(\"simu_bvals2/bvals_{}to{}step{}.csv\".format(start, end, step))\nfor t in range(start, end, -step):\nprint()\nprint(\"time point {}\".format(t))\nselected_lengths[t] = lengths[0]\nallpass = True\nfor k in range(1, 8):\nlen1 = lengths[k]\nbreak\nval = df[\"time{}length{}vs{}\".format(t, len1, len2)][0]\n#bvals_clean = clean_vec_from_nans(bvals)\ncr_val = two_moment_quantile(bvals, .95) * 0.92\nif val <= cr_val:\nres = \"PASS\"\ncomp = \"{} <= {}\".format(val, cr_val)\nelse:\nres = \"REJECT\"\ncomp = \"{} > {}\".format(val, cr_val)\nallpass = False\nselected_lengths[t] = len1\n#print(\"{} vs {}: {}\".format(len1, len2, res, comp))\ntasks.append([\"python\", \"test_tf.py\", str(t), - 20), str(selected_lengths[t])])\nif operation == \"DO\":\nprocs = [subprocess.Popen(task) for task in tasks]\nfor i, proc in enumerate(procs):\nout, err = proc.communicate()\nif err is not None:\nprint(\"ERROR for TASK {}\".format(\" \".join(tasks[i])))\nprint(err)\nelif operation == \"COLLECT\":\ntau = .1\nquantile1 = norm.ppf(tau)\nY = read_csv('sim_ts.csv', header=None).values.T\nlst = []\nfor (start, end) in zip(points[:-1], points[1:]):\nfilename = \"ahead/ahead_{}to{}length{}.csv\".format(start, end, selected_lengths[start])\nlst.append(np.loadtxt(filename, dtype=np.float32, delimiter=','))\nahead_quants = np.concatenate(lst, axis=0)\nfig, ax = plt.subplots()\nax.plot(list(range(61, 500)), Y[61:500, 1], color='g', linewidth=.5)\nax.plot(list(range(61, 500)), ahead_quants[list(range(439, 0, -1)), 1] * quantile2 / quantile1, color='r', linewidth=1.)\n# Turn off tick labels\n#ax.set_yticklabels([])\nplt.tick_params(direction='in', top=True, right=True)\nplt.show()\n", "output_sequence": "Estimating a Multivariate Conditional Auto-Regressive Value at Risk model (MV-CAViaR) requires adaptation to possibly time varying parameter. Following the strategy of Spokoiny (2009) one can consider a sequence of included time intervals with the same end point, testing each of them agains the largest included one for homogeneity. Performing the tests subsequently one can choose the largest interval that is not rejected for estimation in order to have the least variance with moderately small bias. The standard way to test homogeneity is via Change Point detection. The critical values can be estimated using Multiplier Bootstrap procedure Spokoiny, Zhilova (2013). Based on a joint work with Xiu Xu and Wolfgang H\u00e4rdle we implement interval homogeneity test with bootstrap-simulated critical values."}, {"input_sequence": "Author: Yegor Klochkov ; import numpy as np\nfrom math import isnan\nfrom scipy.stats import norm\ndef generate_random_weights(num):\n# SHOULD ONLY BE APPLIED IN MAIN PROCESS!\n#\n# let p > q and p + q = 1 then P(X = 1 - \\sqrt{q/p}) = p and P(X = 1 + \\sqrt{p/q}) = q\n# ensures that E(X) = 1 and Var(X) = 1 and X > 0 a.s.\n# moreover, taking p = 4/5 and q = 1/5 we have X \\in [1 - 1/2, 1 + 2] = [0.5, 3]\ndef trans(x):\nif x < .8:\nreturn 0.5\nelse:\nreturn 3.0\nreturn np.vectorize(trans)(np.random.rand(num))\ndef dict_of_pars(kernel, recurrent, bias):\nreturn {\n'kernel':\n'recurrent':\n'bias': bias\n}\ndef pars_diff(pars1, pars2):\nreturn {name: pars1[name] - pars2[name] for name in ['kernel', 'recurrent', 'bias']}\ndef pars_sum(pars1, pars2):\nreturn {name: pars1[name] + pars2[name] for name in ['kernel', 'recurrent', 'bias']}\ndef pars_left_right(pars_l, pars_r):\npars_left = {name + '_left': pars_l[name] for name in ['kernel', 'recurrent', 'bias']}\nreturn {**pars_left, **pars_right}\ndef get_left_pars(pars):\nif pars is None:\nreturn None\nreturn dict_of_pars(pars['kernel_left'], pars['recurrent_left'], pars['bias_left'])\ndef get_right_pars(pars):\nreturn dict_of_pars(pars['kernel_right'], pars['recurrent_right'], pars['bias_right'])\ndef clean_vec_from_nans(values):\nres = []\nfor x in values:\nif not isnan(x):\nres.append(x)\nreturn np.array(res)\ndef two_moment_quantile(x, tau):\nmu, std = norm.fit(np.sqrt(np.max(x, 0)))\nreturn (mu + std * norm.ppf(tau)) ** 2\n", "output_sequence": "Estimating a Multivariate Conditional Auto-Regressive Value at Risk model (MV-CAViaR) requires adaptation to possibly time varying parameter. Following the strategy of Spokoiny (2009) one can consider a sequence of included time intervals with the same end point, testing each of them agains the largest included one for homogeneity. Performing the tests subsequently one can choose the largest interval that is not rejected for estimation in order to have the least variance with moderately small bias. The standard way to test homogeneity is via Change Point detection. The critical values can be estimated using Multiplier Bootstrap procedure Spokoiny, Zhilova (2013). Based on a joint work with Xiu Xu and Wolfgang H\u00e4rdle we implement interval homogeneity test with bootstrap-simulated critical values."}, {"input_sequence": "Author: Yegor Klochkov ; import numpy as np\nfrom collections import namedtuple\nfrom scipy.stats import norm\n# local\nimport estimator\nfrom misc import *\nTestResult = namedtuple(\"TestResult\", [\"test_val\", \"res_joint\", \"res_sep\"])\nEPOCHS = 10000\nEPOCHS_BOOTSTRAP = 500\ndef cheat_estimation(train_func, is_separated, *args, **kwargs):\ntau = .1\nquantile = norm.ppf(tau)\ntheta1 = dict_of_pars(np.array([[0.0, 0.2], [0.2, 0.0]]) * quantile,\nnp.array([[0.5, 0.0], [0.0, 0.5]]),\ntheta2 = dict_of_pars(np.array([[0.0, 0.2], [0.2, 0.0]]) * quantile,\nnp.array([[-0.5, 0.0], [0.0, 0.5]]),\nif not is_separated:\ncheat_pars = [theta1,\nelse:\ncheat_pars = [pars_left_right(theta1, theta1), pars_left_right(theta1, theta2),\npars_left_right(theta2, theta2)]\nress = [train_func(*args, **kwargs, init_pars=pars) for pars in cheat_pars]\n_, idx = min([(res.loss, i) for (i, res) in enumerate(ress)])\nreturn ress[idx]\ndef _train_mvcaviar(*args, **kwargs):\nreturn cheat_estimation(estimator.train_mvcaviar, *args, **kwargs)\ndef _train_mvcaviar_w_shift(*args, **kwargs):\nreturn cheat_estimation(estimator.train_mvcaviar_w_shift, *args, **kwargs)\ndef _train_mvcaviar_separated(*args, **kwargs):\nreturn cheat_estimation(estimator.train_mvcaviar_separated2, *args, **kwargs)\nclass _CPTestWrapper:\ndef __init__(self, Y, tau, breaks, message=\"X\", prev_quantile=None):\nself.Y = Y\nself.breaks = breaks\nself.message = message\nself.counter = 0\n(self.tmax, self.n) = np.shape(Y)\nself.prev_quantile = prev_quantile\nself.do_test()\ndef do_test(self):\nself.res_joint = _train_mvcaviar(self.Y, self.tau, epochs=EPOCHS, prev_quantile=self.prev_quantile)\nself.res_breaks = [\n_train_mvcaviar_separated(self.Y, self.tau, break_point, epochs=EPOCHS,\n#init_pars=pars_left_right(self.res_joint.pars, self.res_joint.pars),\nprev_quantile=self.prev_quantile)\nfor break_point in self.breaks\n]\nself.test_val = self.res_joint.loss - min([res.loss for res in self.res_breaks])\nprint(\"Task {}: original test done\".format(self.message))\ndef __call__(self, weights):\nvals = []\nfor i, break_point in enumerate(self.breaks):\nshift = pars_diff(get_right_pars(self.res_breaks[i].pars),\nres_joint = estimator.train_mvcaviar_w_shift(\nself.Y, self.tau, break_point, shift,\nprev_quantile=self.res_breaks[i].prev_quantile,\ninit_pars=get_left_pars(self.res_breaks[i].pars),\nsample_weights=weights, epochs=EPOCHS_BOOTSTRAP\n)\nres_break = estimator.train_mvcaviar_separated(\nself.Y, self.tau, break_point,\ninit_pars=pars_left_right(res_joint.pars, pars_sum(res_joint.pars, shift)),\nvals.append(res_joint.loss - res_break.loss)\nself.counter += 1\nprint(\"Task {}: did {} bootstrap tests\".format(self.message, self.counter))\nreturn max(vals)\ndef bstrap_cp_test(Y, tau, breaks, SIM_NUM, message=\"X\", prev_quantile=None, cheat_pars=None):\nassert(len(breaks) > 0)\nwrapper = _CPTestWrapper(Y, tau, breaks, message=message, prev_quantile=prev_quantile)\nnp.random.seed(1)\nweights_list = [generate_random_weights(np.shape(Y)[0]-1) for x in range(SIM_NUM)]\nbtest_vals = list(map(wrapper, weights_list))\nreturn wrapper.test_val, btest_vals\ndef select_homogeneity_interval_with_single_break(Y, tau, lengths, SIM_NUM=400):\nreturn\n", "output_sequence": "Estimating a Multivariate Conditional Auto-Regressive Value at Risk model (MV-CAViaR) requires adaptation to possibly time varying parameter. Following the strategy of Spokoiny (2009) one can consider a sequence of included time intervals with the same end point, testing each of them agains the largest included one for homogeneity. Performing the tests subsequently one can choose the largest interval that is not rejected for estimation in order to have the least variance with moderately small bias. The standard way to test homogeneity is via Change Point detection. The critical values can be estimated using Multiplier Bootstrap procedure Spokoiny, Zhilova (2013). Based on a joint work with Xiu Xu and Wolfgang H\u00e4rdle we implement interval homogeneity test with bootstrap-simulated critical values."}, {"input_sequence": "Author: Yegor Klochkov ; import numpy as np\nimport mvcaviar\nimport matplotlib.pyplot as plt\nclass GARCH2DSimulator:\ndef __init__(self, pars, breaks):\nassert(len(pars) == len(breaks) + 1 )\nself.pars = pars\nself.sigma0 = 1.\ndef what_par(self, t):\nif len(self.breaks) == 0:\nreturn self.pars[0]\nif t >= self.breaks[-1]:\nfor (i, br) in enumerate(self.breaks):\nif br > t:\nreturn self.pars[i]\ndef get_realisation(self, T):\nY = np.random.randn(2, T)\nsigmas = np.empty((2, T))\nsigmas[:, 0] = (self.sigma0,\nY[:, 0] *= sigmas[:, 0]\nfor t in range(1, T):\npar = self.what_par(t)\n(B, A) = mvcaviar.parse_par(2, 1, par)\nc = A[:, [0]]\nx = np.matmul(B, sigmas[:, [t-1]]) + np.matmul(A, np.abs(Y[:, [t-1]]))\n#y = np.matmul(A, np.abs(Y[:, [t-1]]))\nsigmas[:, t] = np.reshape(c + x, (2,))\nY[:, t] *= sigmas[:, t]\nreturn Y, sigmas\n#theta0 = np.array([ 0.8, -0.3,\n# 0.0,\ntheta0 = np.array([\n.5, 0.,\n#\n0.5, .0,\n])\ntheta1 = np.array([\n-0.5, .0,\n.5, 0.2,\ntheta2 = np.array(theta0, copy=True)\nsim = GARCH2DSimulator([theta0, theta1], [250])\nY, sigmas = sim.get_realisation(500)\npsums = np.array(Y, copy=True)\nfor t in range(1, 500):\npsums[:, t] += psums[:, t-1]\nfig, ax = plt.subplots()\nax.plot(list(range(500)), sigmas[0, :500], color='g', linewidth=.5)\nplt.show()\nnp.savetxt(\"sim_ts.csv\", Y, delimiter=\",\")\nthetas = []\n", "output_sequence": "Estimating a Multivariate Conditional Auto-Regressive Value at Risk model (MV-CAViaR) requires adaptation to possibly time varying parameter. Following the strategy of Spokoiny (2009) one can consider a sequence of included time intervals with the same end point, testing each of them agains the largest included one for homogeneity. Performing the tests subsequently one can choose the largest interval that is not rejected for estimation in order to have the least variance with moderately small bias. The standard way to test homogeneity is via Change Point detection. The critical values can be estimated using Multiplier Bootstrap procedure Spokoiny, Zhilova (2013). Based on a joint work with Xiu Xu and Wolfgang H\u00e4rdle we implement interval homogeneity test with bootstrap-simulated critical values."}, {"input_sequence": "Author: Yegor Klochkov ; import tensorflow as tf\nimport numpy as np\nfrom collections import namedtuple\nfrom misc import *\ndef qr_loss(y, f, tau, sample_weights=None):\nerr = y - f\nrho = tf.reduce_sum(tf.maximum(tau*err, (tau-1)*err), axis=1, keepdims=True)\nif sample_weights is None:\nreturn tf.reduce_mean(rho, axis=None)\nelse:\nreturn tf.reduce_mean(\ntf.matmul(tf.constant(np.diag(sample_weights), dtype=tf.float32), rho),\naxis=None)\ndef predict_mvcaviar(x_train, prev_quantile, pars):\ntmax, n = np.shape(x_train)\ny_predict = np.matmul(x_train, pars['kernel']) + np.matmul(np.ones((tmax, 1)), np.reshape(pars['bias'], (1, n)))\ny_predict[0, :] += np.matmul(np.reshape(prev_quantile, (1, 2)), pars['recurrent'])[0, :]\nfor t in range(1, tmax):\ny_predict[t, :] += np.matmul(np.reshape(y_predict[t-1, :], (1, 2)), pars['recurrent'])[0, :]\nreturn y_predict\nResult = namedtuple(\"Result\", [\"y_predict\", \"loss\", \"pars\", \"prev_quantile\"])\ndef train_mvcaviar(Y, tau, init_pars=None, prev_quantile=None,\nsample_weights=None, epochs=2000, verbose=False):\ntf.reset_default_graph()\n(tmax, n) = np.shape(Y)\nassert (tmax > 1)\nx_train = np.abs(Y[:-1, :])\nx_train = np.reshape(x_train, (tmax - 1, n))\ninputs = tf.placeholder(tf.float32, shape=(tmax-1, n))\nparam_names = ['kernel', 'recurrent', 'bias']\nif init_pars is None:\ninitializers = {name: tf.initializers.zeros() for name in param_names}\nshapes = [(n, n), (n, n), (1, n)]\nparams = {name: tf.get_variable(name, shape, initializer=initializers[name])\nfor name, shape in zip(param_names, shapes)}\nif prev_quantile is None:\nprevq = tf.get_variable(\"prev_quantile\", (1, n),\ninitializer=tf.initializers.zeros())\nprevq = tf.constant(np.reshape(prev_quantile, (1, n)), dtype=tf.float32)\nquantiles = [prevq]\nfor t in range(0, tmax-1):\na = tf.matmul(quantiles[t], params['recurrent']) + \\\ntf.matmul(inputs[t : (t+1), :], params['kernel']) + params['bias']\nquantiles.append(a)\noutputs = tf.concat(quantiles[1:], axis=0)\nloss = qr_loss(answers, outputs, tau, sample_weights=sample_weights)\nopt = tf.train.MomentumOptimizer(learning_rate=0.001, momentum=0.05)\nopt_operation = opt.minimize(loss)\nwith tf.Session() as sess:\nsess.run(tf.global_variables_initializer())\nfor j in range(epochs):\n# Do gradient descent step\n_, loss_val = sess.run([opt_operation, loss], feed_dict={inputs: x_train, answers: y_train})\nif verbose:\nprint(\"Epoch {}/{}: {}\".format(j, epochs, loss_val))\ny_predict = sess.run(outputs, feed_dict={inputs: x_train})\npars = {}\nfor name in param_names:\npars[name] = sess.run(params[name])\nprevq_val = sess.run(prevq)\nloss_val = sess.run(loss, feed_dict={inputs: x_train, answers: y_train})\nreturn Result(y_predict, loss_val, pars, prevq_val)\ndef train_mvcaviar_separated(Y, tau, brek,\ninit_pars=None, prev_quantile=None,\nsample_weights=None, epochs=2000, verbose=False):\ninit_pars_left = get_left_pars(init_pars)\nsample_weights_left = None if sample_weights is None else sample_weights[:brek-1]\nres_left = train_mvcaviar(Y[:brek, :], tau,\nprev_quantile=prev_quantile, init_pars=init_pars_left,\nsample_weights=sample_weights_left, epochs=epochs, verbose=verbose)\ninit_pars_right = get_right_pars(init_pars)\nsample_weights_right = None if sample_weights is None else sample_weights[brek-1:]\nres_right = train_mvcaviar(Y[brek-1:, :], tau,\nprev_quantile=res_left.y_predict[-1, :], init_pars=init_pars_right,\nsample_weights=sample_weights_right, epochs=epochs, verbose=verbose)\npars_all = pars_left_right(res_left.pars, res_right.pars)\ny_predict = np.concatenate((res_left.y_predict, res_right.y_predict), axis=0)\ntmax = np.shape(Y)[0]\nloss = (res_left.loss * (brek-1) + res_right.loss * (tmax - brek)) / (tmax - 1)\nreturn Result(y_predict, loss, pars_all, res_left.prev_quantile)\ndef train_mvcaviar_w_shift(Y, tau, brek, shift,\ninit_pars=None, prev_quantile=None,\nsample_weights=None, epochs=2000, verbose=False):\ninputs = tf.placeholder(tf.float32, shape=(tmax - 1, n))\ninitializers = {}\ninitializers[name] = tf.initializers.zeros()\ninitializers[name] = tf.constant_initializer(init_pars[name])\nparams = {}\nfor name, shape in zip(param_names, shapes):\nparams[name] = tf.get_variable(name, shape,\ninitializer=initializers[name])\nshift_const = {}\nfor name in param_names:\nshift_const[name] = tf.constant(shift[name], dtype=tf.float32)\nfor t in range(0, tmax - 1):\ntf.matmul(inputs[t: (t + 1), :], params['kernel']) + params['bias']\nif t >= brek:\na = a + tf.matmul(quantiles[t], shift_const['recurrent']) + \\\ntf.matmul(inputs[t: (t + 1), :], shift_const['kernel']) + shift_const['bias']\n#err = answers - outputs\n#loss = tf.reduce_mean(tf.maximum(tau * err, (tau - 1) * err))\ndef train_mvcaviar_separated2(Y, tau, brek,\nparam_names = ['kernel_left', 'recurrent_left', 'bias_left',\n'kernel_right', 'recurrent_right', 'bias_right']\nshapes = [(n, n), (n, n), (1, n),\nif t < brek:\na = tf.matmul(quantiles[t], params['recurrent_left']) + \\\ntf.matmul(inputs[t: (t + 1), :], params['kernel_left']) + params['bias_left']\na = tf.matmul(quantiles[t], params['recurrent_right']) + \\\ntf.matmul(inputs[t: (t + 1), :], params['kernel_right']) + params['bias_right']\n", "output_sequence": "Estimating a Multivariate Conditional Auto-Regressive Value at Risk model (MV-CAViaR) requires adaptation to possibly time varying parameter. Following the strategy of Spokoiny (2009) one can consider a sequence of included time intervals with the same end point, testing each of them agains the largest included one for homogeneity. Performing the tests subsequently one can choose the largest interval that is not rejected for estimation in order to have the least variance with moderately small bias. The standard way to test homogeneity is via Change Point detection. The critical values can be estimated using Multiplier Bootstrap procedure Spokoiny, Zhilova (2013). Based on a joint work with Xiu Xu and Wolfgang H\u00e4rdle we implement interval homogeneity test with bootstrap-simulated critical values."}]}
{"version": "0.1.0", "data": [{"description": "\nIn this work, we propose to define Gaussian Processes indexed by multidimensional distributions.\nIn the framework where the distributions can be modeled as i.i.d realizations of a measure on\nthe set of distributions, we prove that the kernel defined as the quadratic distance between the\ntransportation maps, that transport each distribution to the barycenter of the distributions, provides\na valid covariance function. In this framework, we study the asymptotic properties of this process,\nproving micro ergodicity of the parameters.\n\n\n", "summary": ":\nGaussian Process, Kernel methods, Wasserstein Distance\n\nJEL classification:\n\n"}, {"description": "\nWe link the hiring of R&D scientists from industry competitors to the subsequent formation of collaborative agreements, namely technology-oriented alliances. By transferring technological knowledge as well as cognitive elements to the hiring firm, mobile inventors foster the alignment of decision frames applied by potential alliance partners in the process of alliance formation thereby making collaboration more likely. Using data on inventor mobility and alliance formation amongst 42 global pharmaceutical firms over 16 years, we show that inventor mobility is positively associated with the likelihood of alliance formation in periods following inventor movements. This relationship becomes more pronounced if mobile employees bring additional knowledge about their prior firm\u0092s technological capabilities and for alliances aimed at technology development rather than for agreements related to technology transfer. It is weakened, however, if the focal firm is already familiar with the competitor\u0092s technological capabilities. By revealing these relationships, our study contributes to research on alliance formation, employee mobility, and organizational frames.\n\n\n", "summary": ":\n\n\nJEL classication:\n\n"}, {"description": ":\nSoftware-as-a-service applications are experiencing immense growth as their\ncomparatively low cost makes them an important alternative to traditional\nsoftware. Following the initial adoption phase, vendors are now concerned with\nthe continued usage of their software. To analyze the influence of different\nmeasures to improve continued usage over time, a longitudinal study design using\ndata from a SaaS vendor was implemented. By employing a linear mixed model, the\nstudy finds several measures to have a positive effect on a software\u2019s usage\npenetration. In addition to these activation measures performed by the SaaS\nvendor, software as well as client characteristics were likewise examined but\ndid not display significant estimates. In summary the study contributes novel\ninsights into the scarcely researched field of influencing factors on SaaS usage\ncontinuance.\n\n", "summary": ":\nLinear Mixed Models Software-as-a-Service Usage Continuance\n\n"}, {"description": ":\n\nA copula model with flexibly specified dependence structure can be useful to capture the complexity and heterogeneity in economic and financial time series. However, there exists little methodological guidance for the specification process using copulas. This paper contributes to fill this gap by considering the recently proposed single-index copulas, for which we propose a simultaneous estimation and variable selection procedure. The proposed method allows to choose the most relevant state variables from a comprehensive set using a penalized estimation, and we derive its large sample properties. Simulation results demonstrate the good performance of the proposed method in selecting the appropriate state variables and estimating the unknown index coefficients and dependence parameters. An application of the new procedure identifies six macroeconomic driving factors for the dependence among U.S. housing markets.\n\n\n\n", "summary": ":\nSemiparametric Copula, Single-Index Copula, Variable Selection, SCAD\n\n\n\n"}, {"description": "\nHigh-dimensional, streaming datasets are ubiquitous in modern applications.\nExamples range from nance and e-commerce to the study of biomedical and\nneuroimaging data. As a result, many novel algorithms have been proposed to\naddress challenges posed by such datasets. In this work, we focus on the use of L1-\nregularized linear models in the context of (possibly non-stationary) streaming\ndata. Recently, it has been noted that the choice of the regularization parameter\nis fundamental in such models and several methods have been proposed which\niteratively tune such a parameter in a time-varying manner, thereby allowing\nthe underlying sparsity of estimated models to vary. Moreover, in many applications,\ninference on the regularization parameter may itself be of interest, as\nsuch a parameter is related to the underlying sparsity of the model. However, in\nthis work, we highlight and provide extensive empirical evidence regarding how\nvarious (often unrelated) statistical properties in the data can lead to changes\nin the regularization parameter. In particular, through various synthetic experiments,\nwe demonstrate that changes in the regularization parameter may be\ndriven by changes in the true underlying sparsity, signal-to-noise ratio or even\nmodel misspecication. The purpose of this letter is, therefore, to highlight and\ncatalog various statistical properties which induce changes in the associated regularization\nparameter. We conclude by presenting two applications: one relating\nto nancial data and another to neuroimaging data, where the aforementioned\ndiscussion is relevant.\n\n", "summary": ":\nLasso, penalty parameter, stock prices, neuroimaging\n\n"}, {"description": "\nWe investigate default probabilities and default correlations of Merton-type credit portfolio\nmodels in stress scenarios where a common risk factor is truncated. The analysis is\nperformed in the class of elliptical distributions, a family of light-tailed to heavy-tailed distributions\nencompassing many distributions commonly found in nancial modelling. It turns\nout that the asymptotic limit of default probabilities and default correlations depend on the\nmax-domain of the elliptical distribution's mixing variable. In case the mixing variable is\nregularly varying, default probabilities are strictly smaller than 1 and default correlations\nare in (0; 1). Both can be expressed in terms of the Student t-distribution function. In the\nrapidly varying case, default probabilities are 1 and default correlations are 0. We compare\nour results to the tail dependence function and discuss implications for credit portfolio\nmodelling.\n\n\n", "summary": ":\nfinancial risk management, credit portfolio modelling, stress testing, elliptic distribution, max-domain\n\nMSC classification:\n60G70, 91G40\n\n"}, {"description": "\nIn the work a characterization of difference of multivariate Gaussian measures is found on the\nfamily of centered Eucledian balls. In particular, it helps to derive (xx see paper).\n\n\n", "summary": ":\nmultivariate Gaussian measure, Kolmogorov distance, Gaussian comparison\n\nJEL classification:\n\n"}, {"description": ":\nWe investigate the relationship between underlying blockchain mechanism of\ncryptocurrencies and its distributional characteristics. In addition to price,\nwe emphasise on using actual block size and block time as the operational\nfeatures of cryptos. We use distributional characteristics such as fourier power\nspectrum, moments, quantiles, global we optimums, as well as the measures for\nlong term dependencies, risk and noise to summarise the information from crypto\ntime series. With the hypothesis that the blockchain structure explains the\ndistributional characteristics of cryptos, we use characteristic based spectral\nclustering to cluster the selected cryptos into \u2000five groups. We scrutinise\nthese clusters and \u2000find that indeed, the clusters of cryptos share similar\nmechanism such as origin of fork, difficulty adjustment frequency, and the\nnature of block size. This paper provides crypto creators and users with a\nbetter understanding toward the connection between the blockchain protocol\ndesign and distributional characteristics of cryptos.\n\n", "summary": ":\nCryptocurrency, price, blockchain mechanism, distributional characteristics,\nclustering\n\n"}, {"description": "\nWe distill sentiment from a huge assortment of NASDAQ news articles by means of machine\nlearning methods and examine its predictive power in single-stock option markets and equity\nmarkets. We provide evidence that single-stock options react to contemporaneous sentiment.\nNext, examining return predictability, we discover that while option variables indeed predict\nstock returns, sentiment variables add further informational content. In fact, both in a\nregression and a trading context, option variables orthogonalized to public and sentimental\nnews are even more informative predictors of stock returns. Distinguishing further between\novernight and trading-time news, we find the first to be more informative. From a statistical\ntopic model, we uncover that this is attributable to the differing thematic coverage of the\nalternate archives. Finally, we show that sentiment disagreement commands a strong positive\nrisk premium above and beyond market volatility and that lagged returns predict future\nreturns in concentrated sentiment environments.\n\n\n", "summary": ":\ninvestor disagreement; option markets; overnight information; stock return\npredictability; textual sentiment; topic model; trading-time information;\n\nJEL classification:\nC58, G12, G14, G41\n\n"}, {"description": ":\nCryptocurrencies\u2019 values often respond aggressively to major policy changes, but\nnone of the existing indices informs on the market risks associated with\nregulatory changes. In this paper, we quantify the risks originating from new\nregulations on FinTech and cryptocurrencies (CCs), and analyse their impact on\nmarket dynamics. Specifically, a Cryptocurrency Regulatory Risk IndeX (CRRIX) is\nconstructed based on policy-related news coverage frequency. The unlabeled news\ndata are collected from the top online CC news platforms and further classified\nusing a Latent Dirichlet Allocation model and Hellinger distance. Our results\nshow that the machine-learning-based CRRIX successfully captures major policy-\nchanging moments. The movements for both the VCRIX, a market volatility index,\nand the CRRIX are synchronous, meaning that the CRRIX could be helpful for all\nparticipants in the cryptocurrency market. The algorithms and Python code are\navailable for research purposes on www.quantlet.de.\n\n", "summary": ":\nCryptocurrency, Regulatory Risk, Index, LDA, News Classification\n\n"}, {"description": "\nMany Southeast European countries are currently undergoing a process of liberalization of electric power markets. The paper analyses day-ahead price dynamics on some of these new markets and in Germany as a benchmark of a completely decentralized Western European market. To that end, several price forecasting methods including autoregressive approaches, multiple linear regression, and neural networks are considered. These methods are tested on hourly day-ahead price data during four two-week periods corresponding to different seasons and varying levels of volatility in all selected markets. The most influential fundamental factors are determined and performance of forecasting techniques is analysed with respect to the age of the market, its degree of liberalization, and the level of volatility. A comparison of Southeast European electricity markets of different age with the older German market is made and clusters of similar Southeast European markets are identified.\n\n\n", "summary": ":\nARIMA models, energy forecasting, time series models, neural networks\n\nJEL classification:\n\n"}, {"description": "\nThe recent emergence of blockchain-based cryptocurrencies has received a\nconsiderable attention. The growing acceptance of cryptocurrencies has led\nmany to speculate that the blockchain technology can surpass a traditional\ncentralized monetary system. However, no monetary model has yet been de-\nveloped to study the economics of the blockchain. This paper builds a model\nof the economy with a single generally acepted blockchain-based currency. In\nthe spirit of the search and matching literature I use a matching function to\nmodel the operation of the blockchain. The formulation of the money demand\nis taken from a workhorse of monetary economics - Lagos and Wright (2005).\nI show that in a blockchain-based monetary system money demand features\na precautionary motive which is absent in the standard Lagos-Wright model.\nDue to this precautionary money demand the monetary equilibrium can be\nstable for some calibrations. I also used the developed model to study how\nthe equilibrium return on money is\n\n", "summary": ":\nBlockchain, Miners, Cryptocurrency, Matching function\n\nJEL classification:\nE40, E41, E42\n\n"}, {"description": "\nLet X1, . . . ,Xn be i.i.d. sample in Rp with zero mean and the\ncovariance matrix . The problem of recovering the projector onto\nan eigenspace of from these observations naturally arises in many\napplications. Recent technique from [9] helps to study the asymp-\ntotic distribution of the distance in the Frobenius norm kPr - bP\nrk2\nbetween the true projector Pr on the subspace of the rth eigenvalue\nand its empirical counterpart bP\nr in terms of the effective rank of .\nThis paper offers a bootstrap procedure for building sharp confidence\nsets for the true projector Pr from the given data. This procedure\ndoes not rely on the asymptotic distribution of kPr - bP\nrk2 and its\nmoments. It could be applied for small or moderate sample size n and\nlarge dimension p. The main result states the validity of the proposed\nprocedure for finite samples with an explicit error bound for the er-\nror of bootstrap approximation. This bound involves some new sharp\nresults on Gaussian comparison and Gaussian anti-concentration in\nhigh-dimensional spaces. Numeric results confirm a good performance\nof the method in realistic examples.\n\n\n", "summary": ":\n\n\nJEL classification:\n\n"}, {"description": "\nModeling the joint tails of multiple nancial time series has important im-\nplications for risk management. Classical models for dependence often encounter a lack\nof t in the joint tails, calling for additional exibility. In this paper we introduce a new\nnonparametric time-varying mixture copula model, in which both weights and depen-\ndence parameters are deterministic functions of time. We propose penalized trending\nmixture copula models with group smoothly clipped absolute deviation (SCAD) penal-\nty functions to do the estimation and copula selection simultaneously. Monte Carlo\nsimulation results suggest that the shrinkage estimation procedure performs well in s-\nelecting and estimating both constant and trending mixture copula models. Using the\nproposed model and method, we analyze the evolution of the dependence among four\ninternational stock markets, and nd substantial changes in the levels and patterns of\nthe dependence, in particular around crisis periods.\n\n", "summary": ":\nCopula, Time-Varying Copula, Mixture Copula, Copula Selection\n\n"}, {"description": "\nIn linear regression of Y on X(2 Rp) with parameters (2 Rp+1);\nstatistical inference is unreliable when observations are obtained\nfrom gross-error model, F;G = (1??)F +G; instead of the assumed\nprobability F;G is gross-error probability, 0 < < 1: When G is unit\nmass at (x; y); Residual's In uence Index, RINFIN(x; y; ; ), measures\nthe dierence in small x-perturbations of L2-residual, r(x; y);\nfor model F and for F;G via r's x-partial derivatives. Asymptotic\nproperties are presented for sample RINFIN that is successful in\nextracting indications for in uential and bad leverage cases in microarray\ndata and simulated, high dimensional data. Its performance\nimproves as p increases and can also be used in multiple response\nlinear regression. RINFIN's advantage is that, whereas in in uence\nfunctions of L2-regression coecients each x-coordinate and r(x; y)\nappear in a sum as product with moderate size when (x; y) is bad\nleverage case and masking makes r(x; y) nearly vanish, RINFIN's\nx-partial derivatives convert the product in sum allowing for unmasking.\n\n", "summary": ":\nBig Data, Data Science, In fluence Function, Leverage, Masking, Residual's In fluence Index (RINFIN)\n\n"}, {"description": ":\nThe Financial Risk Meter (FRM) is an established mechanism that, based on\nconditional Value at Risk (VaR) ideas, yields insight into the dynamics of\nnetwork risk. Originally, the FRM has been composed via Lasso based quantile\nregression, but we here extend it by incorporating the idea of expectiles, thus\nindicating not only the tail probability but rather the actual tail loss given a\nstress situation in the network. The expectile variant of the FRM enjoys several\nadvantages: Firstly, the coherent and multivariate tail risk indicator\nconditional expectile-based VaR (CoEVaR) can be derived, which is sensitive to\nthe magnitude of extreme losses. Next, FRM index is not restricted to an index\ncompared to the quantile based FRM mechanisms, but can be expanded to a set of\nsystemic tail risk indicators, which provide investors with numerous tools in\nterms of diverse risk preferences. The power of FRM also lies in displaying FRM\ndistribution across various entities every day. Two distinct patterns can be\ndiscovered under high stress and during stable periods from the empirical\nresults in the United States stock market. Furthermore, the framework is able to\nidentify individual risk characteristics and capture spillover effects in a\nnetwork.\n\n", "summary": ":\nexpectiles, EVaR, CoEVaR, expectile lasso regression, network analysis, systemic\nrisk, Financial Risk Meter\n\n"}, {"description": ":\nWe uncover networks from news articles to study cross-sectional stock returns.\nBy analyzing a huge dataset of more than 1 million news articles collected from\nthe internet, we construct time-varying directed networks of the S&P500 stocks.\nThe well-defined directed news networks are formed based on a modest assumption\nabout firm-specific news structure, and we propose an algorithm to tackle type-I\nerrors in identifying the stock tickers. We find strong evidence for the\ncomovement effect between the news-linked stocks returns and reversal effect\nfrom the lead stock return on the 1-day ahead follower stock return, after\ncontrolling for many known effects. Furthermore, a series of portfolio tests\nreveal that the news network attention proxy, network degree, provides a robust\nand significant cross-sectional predictability of the monthly stock returns.\nAmong different types of news linkages, the linkages of within-sector stocks,\nlarge size lead firms, and lead firms with lower stock liquidity are crucial for\ncross-sectional predictability.\n\n", "summary": ":\nNetworks, Textual News, Cross-Sectional Returns, Comovement, Network Degree\n\n"}, {"description": "\nThe conventional wisdom that housing prices are the present value of future\nrents ignores the fact that unlike dividends on stocks, rent is not discretionary.\nHousing price uncertainty can affect household property investments, which\nin turn affect rent. By extending the theory of investment under uncertainty, we\nmodel the renter\u0092s decision to buy a house and the landlord\u0092s decision to sell\nas the exercising of real options of waiting and examine real options effects on\nrent. Using data from Hong Kong and mainland China, we find a significant\neffect of housing price on rent and draw important policy implications.\n\n", "summary": ":\n\n\n"}, {"description": ":\nThis paper aims to model the joint dynamics of cryptocurrencies in a\nnonstationary setting. In particular, we analyze the role of cointegration\nrelationships within a large system of cryptocurrencies in a vector error\ncorrection model (VECM) framework. To enable analysis in a dynamic setting, we\npropose the COINtensity VECM, a nonlinear VECM specification accounting for a\nvarying systemwide cointegration exposure. Our results show that\ncryptocurrencies are indeed cointegrated with a cointegration rank of four. We\nalso find that all currencies are affected by these long term equilibrium\nrelations. A simple statistical arbitrage trading strategy is proposed showing a\ngreat in-sample performance.\n\n", "summary": ":\nCointegration, VECM, Nonstationarity, Cryptocurrencies\n\n"}, {"description": "\nThe market capitalization of cryptocurrencies has risen rapidly during the\nlast few years. Despite their high volatility, this fact has spurred growing\ninterest in cryptocurrencies as an alternative investment asset for portfolio and\nrisk management. We characterise the effects of adding cryptocurrencies in addition\nto traditional assets to the set of eligible assets in portfolio management.\nOut-of-sample performance and diversification benefits are studied for the most\npopular portfolio-construction rules, including mean-variance optimization,\nrisk-parity, and maximum-diversification strategies, as well as combined strategies.\nTo account for the frequently low liquidity of cryptocurrency markets\nwe incorporate the LIBRO method, which gives suitable liquidity constraints.\nOur results show that cryptocurrencies can improve the risk-return profile of\nportfolios. In particular, cryptocurrencies are more useful for portfolio strategies\nwith higher target returns; they do not play a role in minimum-variance\nportfolios. However, a maximum-diversification strategy (maximising the Portfolio\nDiversification Index, PDI) draws appreciably on cryptocurrencies, and\nspanning tests clearly indicate that cryptocurrency returns are non-redundant\nadditions to the investment universe.\n\n", "summary": ":\ncryptocurrency, CRIX, investments, portfolio management, asset\nclasses, blockchain, Bitcoin, altcoins, DLT\n\n"}, {"description": ":\nWhile attention is a predictor for digital asset prices, and jumps in Bitcoin\nprices are well-known, we know little about its alternatives. Studying high\nfrequency crypto data gives us the unique possibility to confirm that cross\nmarket digital asset returns are driven by high frequency jumps clustered around\nblack swan events, resembling volatility and trading volume seasonalities.\nRegressions show that intra-day jumps significantly influence end of day returns\nin size and direction. This provides fundamental research for crypto option\npricing models. However, we need better econometric methods for capturing the\nspecific market microstructure of cryptos. All calculations are reproducible via\nthe quantlet.com technology.\n\n", "summary": ":\njumps, market microstructure noise, high frequency data, cryptocurrencies, CRIX,\noption pricing\n\n"}, {"description": ":\nIn this paper, we develop an multi period overlapping generation framework to\ninvestigate agents' consumption and saving decisions, inequality and welfare\namong elderly. We assume that agents are heterogeneous in the non-asset income\nand the medical expenditure. In order to explicitly analyze the e ects of\nmedical expenditure, we conduct three counterfactual exercises. We successively\nshut down the heterogeneity in labor income, in the level and in the dispersion\nof medical expenses respectively. By comparing the benchmark with the\ncounterfactual results, we nd that in general wealth inequality decreases with\nage, and income uncertainty contributes the most to wealth inequality. Both\naverage consumption and consumption inequality increase with age. Consumption\ninequality largely tracks income inequality. Though uncertainty in medical\nexpenditures has little e ect on consumption inequality, a higher level of\nmedical expenditures may exacerbate consumption inequality. Meanwhile, the\naverage saving of elderly exhibits an inverse-U shape with age. The impacts on\naverage saving are similar both in benchmark and in counterfactual exercises.\nWelfare increases with age.\n\n", "summary": ":\nIncome Inequality, Social Mobility, Price-to-rent ratio\n\n"}, {"description": ":\nMarkowitz mean-variance portfolios with sample mean and covariance as input\nparameters feature numerous issues in practice. They perform poorly out of\nsample due to estimation error, they experience extreme weights together with\nhigh sensitivity to change in input parameters. The heavy-tail characteristics\nof \u2000financial time series are in fact the cause for these erratic fluctuations\nof weights that consequently create substantial transaction costs. In\nrobustifying the weights we present a toolbox for stabilizing costs and weights\nfor global minimum Markowitz portfolios. Utilizing a projected gradient descent\n(PGD) technique, we avoid the estimation and inversion of the covariance\noperator as a whole and concentrate on robust estimation of the gradient descent\nincrement. Using modern tools of robust statistics we construct a\ncomputationally efficient estimator with almost Gaussian properties based on\nmedian-of-means uniformly over weights. This robustified Markowitz approach is\nconfirmed by empirical studies on equity markets. We demonstrate that\nrobustified portfolios reach higher risk-adjusted performance and the lowest\nturnover compared to shrinkage based and constrained portfolios.\n\n", "summary": ":\n.\n\n"}, {"description": ":\nIn this paper, we conduct simultaneous inference of the non-parametric part of a\npartially linear model when the non-parametric component is a multivariate\nunknown function. Based on semi-parametric estimates of the model, we construct\na simultaneous confidence region of the multivariate function for simultaneous\ninference. The developed methodology is applied to perform simultaneous\ninference for the U.S. gasoline demand where the income and price variables are\ncontaminated by Berkson errors. The empirical results strongly suggest that the\nlinearity of the U.S. gasoline demand is rejected. The results are also used to\npropose an alternative form for the demand.\n\n", "summary": ":\nSimultaneous inference, Multivariate function, Simultaneous confidence region,\nBerkson error, Regression calibration\n\n"}, {"description": ":\nEstimating spot covariance is an important issue to study, especially with the\nincreasing availability of high-frequency nancial data. We study the estimation\nof spot covariance using a kernel method for high-frequency data. In particular,\nwe consider rst the kernel weighted version of realized covariance estimator\nfor the price process governed by a continuous multivariate semimartingale.\nNext, we extend it to the threshold kernel estimator of the spot covariances\nwhen the underlying price process is a discontinuous multivariate semimartingale\nwith nite activity jumps. We derive the asymptotic distribution of the\nestimators for both xed and shrinking bandwidth. The estimator in a setting\nwith jumps has the same rate of convergence as the estimator for di usion\nprocesses without jumps. A simulation study examines the nite sample properties\nof the estimators. In addition, we study an application of the estimator in the\ncontext of covariance forecasting. We discover that the forecasting model with\nour estimator outperforms a benchmark model in the literature.\n\n", "summary": ":\nhigh-frequency data; kernel estimation; jump; forecasting covariance matrix\n\n"}, {"description": ":\nAppropriate risk management is crucial to ensure the competitiveness of financial institutions\nand the stability of the economy. One widely used financial risk measure is Value-at-Risk\n(VaR). VaR estimates based on linear and parametric models can lead to biased results or\neven underestimation of risk due to time varying volatility, skewness and leptokurtosis of\nnancial return series. The paper proposes a nonlinear and nonparametric framework to\nforecast VaR. Mean and volatility are modeled via support vector regression (SVR) where\nthe volatility model is motivated by the standard generalized autoregressive conditional\nheteroscedasticity (GARCH) formulation. Based on this, VaR is derived by applying kernel\ndensity estimation (KDE). This approach allows for exible tail shapes of the profit and loss\ndistribution and adapts for a wide class of tail events.\nThe SVR-GARCH-KDE hybrid is compared to standard, exponential and threshold\nGARCH models coupled with different error distributions. To examine the performance in\ndifferent markets, one-day-ahead forecasts are produced for different financial indices. Model\nevaluation using a likelihood ratio based test framework for interval forecasts indicates that\nthe SVR-GARCH-KDE hybrid performs competitive to benchmark models. Especially models\nthat are coupled with a normal distribution are systematically outperformed.\n\n\n", "summary": ": Value-at-Risk, Support Vector Regression, Kernel Density Estimation, GARCH\n\n"}, {"description": ":\nWith growing economic globalization, the modern service sector is in great need\nof business intelligence for data analytics and computational statistics. The\njoint application of big data analytics, computational statistics and business\nintelligence has great potential to make the engineering of advanced service\nsystems more efficient. The purpose of this COST issue is to publish high-\nquality research papers (including reviews) that address the challenges of\nservice data analytics with business intelligence in the face of uncertainty and\nrisk. High quality contributions that are not yet published or that are not\nunder review by other journals or peer-reviewed conferences have been collected.\nThe resulting topic oriented special issue includes research on business\nintelligence and computational statistics, data-driven financial engineering,\nservice data analytics and algorithms for optimizing the business engineering.\nIt also covers implementation issues of managing the service process,\ncomputational statistics for risk analysis and novel theoretical and\ncomputational models, data mining algorithms for risk management related\nbusiness applications.\n\n", "summary": ":\nData Analytics, Business Intelligence Systems\n\n"}, {"description": ":\nThe fast-growing Emerging Market (EM) economies and their improved transparency\nand liquidity have attracted international investors. However, the external\nprice shocks can result in a higher level of volatility as well as domestic\npolicy instability. Therefore, an efficient risk measure and hedging strategies\nare needed to help investors protect their investments against this risk. In\nthis paper, a daily systemic risk measure, called FRM (Financial Risk Meter) is\nproposed. The FRM@ EM is applied to capture systemic risk behavior embedded in\nthe returns of the 25 largest EMs\u2019 FIs, covering the BRIMST (Brazil, Russia,\nIndia, Mexico, South Africa, and Turkey), and thereby reflects the financial\nlinkages between these economies. Concerning the Macro factors, in addition to\nthe Adrian & Brunnermeier (2016) Macro, we include the EM sovereign yield spread\nover respective US Treasuries and the above-mentioned countries\u2019 currencies. The\nresults indicated that the FRM of EMs\u2019 FIs reached its maximum during the US\nfinancial crisis following by COVID 19 crisis and the Macro factors explain the\nBRIMST\u2019 FIs with various degrees of sensibility. We then study the relationship\nbetween those factors and the tail event network behavior to build our policy\nrecommendations to help the investors to choose the suitable market for\ninvestment and tail-event optimized portfolios. For that purpose, an overlapping\nregion between portfolio optimization strategies and FRM network centrality is\ndeveloped. We propose a robust and well-diversified tail-event and cluster risk-\nsensitive portfolio allocation model and compare it to more classical\napproaches.\n\n", "summary": ":\nFRM (Financial Risk Meter), Lasso Quantile Regression, Network Dynamics,\nEmerging Markets, Hierarchical Risk Parity\n\n"}, {"description": "\nWe consider the estimation and inference in a system of high-dimensional regression equations\nallowing for temporal and cross-sectional dependency in covariates and error processes, covering\nrather general forms of weak dependence. A sequence of large-scale regressions with LASSO is\napplied to reduce the dimensionality, and an overall penalty level is carefully chosen by a block\nmultiplier bootstrap procedure to account for multiplicity of the equations and dependencies in the\ndata. Correspondingly, oracle properties with a jointly selected tuning parameter are derived. We\nfurther provide high-quality de-biased simultaneous inference on the many target parameters of\nthe system. We provide bootstrap consistency results of the test procedure, which are based on a\ngeneral Bahadur representation for the Z-estimators with dependent data. Simulations demonstrate\ngood performance of the proposed inference procedure. Finally, we apply the method to quantify\nspillover effects of textual sentiment indices in a financial market and to test the connectedness\namong sectors.\n\n\n", "summary": ":\nLASSO, time series, simultaneous inference, system of equations, Z-estimation, Bahadur representation, martingale decomposition\n\nJEL classification:\nC12, C22, C51, C53\n\n"}, {"description": ":\nAmong all the emerging markets, the cryptocurrency market is considered the most\ncontroversial and simultaneously the most interesting one. The visibly\nsignificant market capitalization of cryptos motivates modern financial\ninstruments such as futures and options. Those will depend on the dynamics,\nvolatility, or even the jumps of cryptos. In this paper, the risk\ncharacteristics for Bitcoin are analyzed from a realized volatility dynamics\nview. The realized variance RV is estimated with (threshold-)jump components\n(T)J, semivariance RSV+(\u2212) , and signed jumps (T)J+(\u2212) . Our empirical results\nshow that the Bitcoin market is far riskier than any other developed financial\nmarket. Up to 68% of the sample days are identified to entangle jumps. However,\nthe discontinuities do not contribute to the variance significantly. By\nemploying a 90-day rolling-window method, the in-sample evidence suggests that\nthe impacts of predictors change over time systematically under HAR-type models.\nThe out-of-sample forecasting results reveal that the forecasting horizon\nplays an important role in choosing forecasting models. For long-horizon risk\nforecast, a finer model calibrated with jumps gives extra utility up to 20 basis\npoints annually, while an approach based on the roughest estimators suits the\nshort-horizon risk forecast best. Last but not least, a simple equal-weighted\nportfolio not only significantly reduces the size and quantity of jumps but also\ngives investors higher utility in short-horizon risk forecast case.\n\n", "summary": ":\nCryptocurrency, Bitcoin, Realized Variance, Thresholded Jump, Signed Jumps,\nRealized Utility\n\n"}, {"description": "\nIn this paper we investigate the statistical properties of cryptocurrencies by using alpha-stable distributions. We also study the benefits of the Metcalfe's law (the value of a network is proportional to the square of the number of connected users of the system) for the evaluation of cryptocurrencies. As the results showed a potential for herding behaviour, we used LPPL models to capture the behaviour of cryptocurrencies exchange rates during an endogenous bubble and to predict the most probable time of the regime switching.\n\n", "summary": ":\ncryptocurrency, Bitcoin, CRIX, Log-Periodic Power Law, Metcalfe\u0092s law, stable distribution\n\n"}, {"description": "\nA trading rule that draws on the empirical similarity concept is proposed to simulate the\ntechnical trading mentality|one that selectively perceives structural resemblances between\nmarket scenarios of the present and the past. In more than half of the nineteen futures\nmarkets that we test against for protability of this similarity-based trading rule, we nd\nevidence of predictive ability that is robust to data-snooping and transaction-cost adjust-\nments. When aided by an exit strategy that liquidates the trader's positions across some\nevenly-spaced time points, this rule generates the most robust returns.\n\n", "summary": ":\nempirical similarity; technical trading; futures markets; analogical reasoning\n\n"}, {"description": ":\nK-means clustering is one of the most widely-used partitioning algorithm in\ncluster analysis due to its simplicity and computational efficiency, but it may\nnot provide ideal clustering results when applying to data with non-spherically\nshaped clusters. By considering the asymmetrically weighted distance, We propose\nthe K-expectile clustering and search the clusters via a greedy algorithm that\nminimizes the within cluster \u03c4-variance. We provide algorithms based on two\nschemes: the fixed \u03c4 clustering, and the adaptive \u03c4 clustering. Validated by\nsimulation results, our method has enhanced performance on data with asymmetric\nshaped clusters or clusters with a complicated structure. Applications of our\nmethod show that the fixed \u03c4 clustering can bring some flexibility on\nsegmentation with a decent accuracy, while the adaptive \u03c4 clustering may yield\nbetter performance. All calculation can be redone via quantlet.com.\n\n", "summary": ":\nclustering, expectiles, asymmetric quadratic loss, image segmentation\n\n"}, {"description": "\nIn this paper, we propose a new class of regime shift models with exible switching\nmechanism that relies on a nonparametric probability function of the observed thresh-\nold variables. The proposed models generally embrace traditional threshold models\nwith contaminated threshold variables or heterogeneous threshold values, thus gaining\nmore power in handling complicated data structure. We solve the identification issue by\nimposing either global shape restriction or boundary condition on the nonparametric\nprobability function. We utilize the natural connection between penalized splines and\nhierarchical Bayes to conduct smoothing. By adopting dierent priors, our procedure\ncould work well for estimations of smooth curve as well as discontinuous curves with\noccasionally structural breaks. Bayesian tests for the existence of threshold eects are\nalso conducted based on the posterior samples from Markov chain Monte Carlo (M-\nCMC) methods. Both simulation studies and an empirical application in predicting\nthe U.S. stock market returns demonstrate the validity of our methods.\n\n\n", "summary": ":\nThreshold Model, Nonparametric, Markov Chain Monte Carlo, Bayesian Inference, Spline.\n\nJEL classification:\n\n\n"}, {"description": "\nWe consider a problem of multiclass classification, where the\ntraining sample Sn = {(Xi, Yi)}n\ni=1 is generated from the model P(Y =\nm|X = x) = m(x), 1 6 m 6 M, and 1(x), . . . , M(x) are unknown Lip-\nschitz functions. Given a test point X, our goal is to estimate 1(X), . . . ,\nM(X). An approach based on nonparametric smoothing uses a localization\ntechnique, i.e. the weight of observation (Xi, Yi) depends on the distance\nbetween Xi and X. However, local estimates strongly depend on localiz-\ning scheme. In our solution we fix several schemes W1, . . . ,WK, compute\ncorresponding local estimates e(1), . . . , e(K) for each of them and apply an\naggregation procedure. We propose an algorithm, which constructs a con-\nvex combination of the estimates e(1), . . . , e(K) such that the aggregated\nestimate behaves approximately as well as the best one from the collection\ne(1), . . . , e(K). We also study theoretical properties of the procedure, prove\noracle results and establish rates of convergence under mild assumptions.\n\n\n", "summary": ":\n\n\nJEL classification:\n\n"}, {"description": ":\nStrategic planning in a corporate environment is often based on experience and\nintuition, although internal data is usually available and can be a valuable\nsource of information. Predicting merger & acquisition (M&A) events is at the\nheart of strategic management, yet not sufficiently motivated by data\nanalytics driven controlling. One of the main obstacles in using e.g. count data\ntime series for M&A seems to be the fact that the intensity of M&A is time\nvarying at least in certain business sectors, e.g. communications. We propose a\nnew automatic procedure to bridge this obstacle using novel statistical methods.\nThe proposed approach allows for a selection of adaptive windows in count data\nsets by detecting significant changes in the intensity of events. We test the\nefficacy of the proposed method on a simulated count data set and put it into\naction on various M&A data sets. It is robust to aberrant behaviour and\ngenerates accurate forecasts for the evaluated business sectors. It also\nprovides guidance for an a-priori selection of fixed windows for forecasting.\nFurthermore, it can be generalized to other business lines, e.g. for managing\nsupply chains, sales forecasts, or call center arrivals, thus giving managers\nnew ways for incorporating statistical modeling in strategic planning decisions.\n\n", "summary": ":\n-\n\n"}, {"description": ":\nModelling dynamic conditional heteroscedasticity is the daily routine in time\nseries econometrics. We propose a weighted conditional moment estimation to\npotentially improve the eciency of the QMLE (quasi maximum likelihood\nestimation). The weights of conditional moments are selected based on the\nanalytical form of optimal instruments, and we nominally decide the optimal\ninstrument based on the third and fourth moments of the underlying error term.\nThis approach is motivated by the idea of general estimation equations (GEE). We\nalso provide an analysis of the eciency of QMLE for the location and variance\nparameters. Simulations and applications are conducted to show the better\nperformance of our estimators.\n\n", "summary": ":\n-\n\n"}, {"description": "\nTrading of Bitcoin is spread about multiple venues where buying and selling is offered\nin various currencies. However, all markets trade one common good and by the law of\none price, the different prices should not deviate in the long run. In this context we are\ninterested in which platform is the most important one in terms of price discovery. To this\nend, we use a pairwise approach accounting for a potential impact of exchange rates. The\ncontribution to price discovery is measured by Hasbrouck's and Gonzalo and Granger's\ninformation share. We then derive an ordering with respect to the importance of each\nmarket which reveals that the Chinese OKCoin platform is the leader in price discovery\nof Bitcoin, followed by BTC China.\n\n\n", "summary": ":\nprice discovery; Bitcoin; Hasbrouck information shares;\n\nJEL classification:\nC58, C32, G23\n\n"}, {"description": "\nMarketing messages are most effective if they reach the right customers. Deciding which customers\nto contact is thus an important task in campaign planning. The paper focuses on empirical targeting\nmodels. We argue that common practices to develop such models do not account sufficiently for\nbusiness goals. To remedy this, we propose profit-conscious ensemble selection, a modeling framework\nthat integrates statistical learning principles and business objectives in the form of campaign profit\nmaximization. The results of a comprehensive empirical study confirm the business value of the\nproposed approach in that it recommends substantially more profitable target groups than several\nbenchmarks.\n\n\n", "summary": ":\nMarketing Decision Support, Business Value, Profit-Analytics, Machine Learning\n\nJEL classification:\nC00\n\n"}, {"description": "\nIn this article we develop a tractable procedure for testing strict stationarity in a\ndouble autoregressive model and formulate the problem as testing if the top Lyapunov\nexponent is negative. Without strict stationarity assumption, we construct a consistent\nestimator of the associated top Lyapunov exponent and employ a random weighting\napproach for its variance estimation, which in turn are used in a t-type test. We also\npropose a GLAD estimation for parameters of interest, relaxing key assumptions on\nthe commonly used QMLE. All estimators, except for the intercept, are shown to be\nconsistent and asymptotically normal in both stationary and explosive situations. The\nnite-sample performance of the proposed procedures is evaluated via Monte Carlo\nsimulation studies and a real dataset of interest rates is analyzed.\n\n", "summary": ":\nDAR model, GLAD estimation, Nonstationarity, Random weighting, Strict\nstationarity testing.\n\n"}, {"description": "\nData from social media has created opportunities to understand how and why\npeople move through their urban environment and how this relates to criminal\nactivity. To aid resource allocation decisions in the scope of predictive\npolicing, the paper proposes an approach to predict weekly crime counts. The\nnovel approach captures spatial dependency of criminal activity through approximating\nhuman dynamics. It integrates point of interest data in the form\nof Foursquare venues with Twitter activity and taxi trip data, and introduces a\nset of approaches to create features from these data sources. Empirical results\ndemonstrate the explanatory and predictive power of the novel features. Analysis\nof a six-month period of real-world crime data for the city of New York\nevidences that both temporal and static features are necessary to eectively account\nfor human dynamics and predict crime counts accurately. Furthermore,\nresults provide new evidence into the underlying mechanisms of crime and give\nimplications for crime analysis and intervention.\n\n\n", "summary": ":\nPredictive Policing, Crime Forecasting, Social Media Data, Spatial Econometrics\n\nJEL classification:\nC00\n\n"}, {"description": "\nThis paper studies the short-run impacts of temperature on human performance in the\ncomputer-mediated environment using server logs of a popular online game in China.\nTaking advantage of the quasi-experiment of winter central heating policy inChina, we\ndistinguish the impacts of outdoor and indoor temperature and find that low temperatures\nbelow 5 ?C decrease game performance significantly. Non-experienced players\nsuffered larger performance drop than experienced ones. Access to central heating\nattenuates negative impacts of low outdoor temperatures on gamers\u0092 performance.\nHigh temperatures above 21 ?C also lead to drops in game performance.We conclude\nthat expanding the current central heating zone will bring an increase in human performance\nby approximately 4% in Shanghai and surrounding provinces in the winter.\nWhile often perceived as a leisure activity, online gaming requires intense engagement\nand the deployment of cognitive, social, and motor skills, which are also key skills\nfor productive activities. Our results draw attention to potential damages of extreme\ntemperature on human performance in the modern computer-mediated environment.\n\n", "summary": ":\nTemperature, Human performance, Online game, Heating\n\n"}, {"description": "\nWe model the term structure of implied volatility (TSIV) with an adaptive approach\nto improve predictability, which treats dynamic time series models of globally time-\nvarying but locally constant parameters and uses a data-driven procedure to ?nd the\nlocal optimal interval. We choose two speci?cations of the adaptive models: a simple\nlocal AR (LAR) model for a univariate implied volatility series and an adaptive dynamic\nNelson-Siegel (ADNS) model of three factors, each based on an LAR, to model the cross-\nsection of the TSIV simultaneously with parsimony. Both LAR and ADNS models\nuniformly outperform more than a dozen alternative models with signi?cance across\nmaturities for 1-20 day forecast horizons. Measured by RMSE and MAE, the forecast\nerrors of the random walk model can be reduced by between 20% and 60% for the 5 to\n20 days ahead forecast. In terms of prediction accuracy of future directional changes,\nthe adaptive models achieve an accuracy range of 60%-90%, which strictly dominates\nthe range of 30%-59% of the alternative models.\n\n", "summary": ":\nTerm structure of implied volatility, local parametric models, forecasting\n\n"}, {"description": ":\nWe consider a new procedure for detecting structural breaks in mean for high-\ndimensional time series. We target breaks happening at unknown time points and\nlocations. In particular, at a fixed time point our method is concerned with\neither the biggest break in one location or aggregating simultaneous breaks over\nmultiple locations. We allow for both big or small sized breaks, so that we can\n1), stamp the dates and the locations of the breaks, 2), estimate the break\nsizes and 3), make inference on the break sizes as well as the break dates. Our\ntheoretical setup incorporates both temporal and crosssectional dependence, and\nis suitable for heavy-tailed innovations. We derive the asymptotic distribution\nfor the sizes of the breaks by extending the existing powerful theory on local\nlinear kernel estimation and high dimensional Gaussian approximation to allow\nfor trend stationary time series with jumps. A robust long-run covariance matrix\nestimation is proposed, which can be of independent interest. An application on\ndetecting structural changes of the US unemployment rate is considered to\nillustrate the usefulness of our method.\n\n", "summary": ":\nhigh-dimensional time series, multiple change-points, Gaussian approximation,\nnonparametric estimation, heavy tailed, long-run covariance matrix\n\n"}]}
{"version": "0.1.0", "data": [{"description": ":\nFor multivariate nonparametric regression models, existing variable selection\nmethods with penalization require high-dimensional nonparametric approximations\nin objective functions. When the dimension is high, none of methods with penalization\nin the literature are readily available. Also, ranking and screening approaches\ncannot have selection consistency when iterative algorithms cannot be used due to\ninefficient nonparametric approximation. In this paper, a novel and easily implemented\napproach is proposed to make existing methods feasible for selection with\nno need of nonparametric approximation. Selection consistency can be achieved.\nAs an application to additive regression models, we then suggest a two-stage procedure\nthat separates selection and estimation steps. An adaptive estimation to\nthe smoothness of underlying components can be constructed such that the consistency\ncan be even at parametric rate if the underlying model is really parametric.\nSimulations are carried out to examine the performance of our method, and a real\ndata example is analyzed for illustration.\n\n\n", "summary": ":\nAdaptive estimation; non-parametric additive model; purely nonparametric\nregression; variable selection\n\n"}, {"description": "\nWith option-implied volatility indices, we provide a new tool for event studies in a network setting and document systemic risk in the spillover networks across global financial markets. Network linkages are sufficiently asymmetric because the US stock and bond markets play as dominant volatility suppliers to other countries and markets. Shocks from the US generate systemic risk through intensifying volatility spillovers across countries and asset classes. The findings offer new evidence that asymmetric network linkages can lead to sizable aggregate fluctuations and thus potential systemic risk.\n\n\n", "summary": ":\nNetwork; Option-implied Volatility; Spillover; Asymmetric linkage; Systemic risk\n\n"}, {"description": "\nThe CRIX (CRyptocurrency IndeX) has been constructed based on a number of cryptos\nand provides a high coverage of market liquidity, hu.berlin/crix. The crypto currency\nmarket is a new asset market and attracts a lot of investors recently. Surprisingly a market\nfor contingent claims hat not been built up yet. A reason is certainly the lack of pricing\ntools that are based on solid financial econometric tools. Here a first step towards pricing of\nderivatives of this new asset class is presented. After a careful econometric pre-analysis we\nmotivate an affine jump diffusion model, i.e., the SVCJ (Stochastic Volatility with Correlated\nJumps) model. We calibrate SVCJ by MCMC and obtain interpretable jump processes\nand then via simulation price options. The jumps present in the cryptocurrency fluctutations\nare an essential component. Concrete examples are given to establish an OCRIX exchange\nplatform trading options on CRIX.\n\n\n", "summary": ":\nCRyptocurrency IndeX, CRIX, Bitcoin,Cryptocurrency, SVCJ, Option pricing,OCRIX\n\n"}, {"description": " The recent evolution of cryptocurrencies has been characterized by bubble-like behavior and extreme volatility. While it is difficult to assess an intrinsic value to a specific cryptocurrency, one can employ recently proposed bubble tests that rely on recursive applications of classical unit root tests. This paper extends this approach to the case where volatility is time varying, assuming a deterministic longrun component that may take into account a decrease of unconditional volatility when the cryptocurrency matures with a higher market dissemination. Volatility also includes a stochastic short-run component to capture volatility clustering. The wild bootstrap is shown to correctly adjust the size properties of the bubble test, which retains good power properties. In an empirical application using eleven of the largest cryptocurrencies and the CRIX index, the general evidence in favor of bubbles is confirmed, but much less pronounced than under constant volatility. ", "summary": ": cryptocurrencies, speculative bubbles, wild bootstrap, volatility JEL classification: C14, C43, Z11\n\n"}, {"description": "\nThe recent development of private cryptocurrencies has created a need to\nextend existing models of private currency provision and currency competi-\ntion. The outcome of cryptocurrency competition should be analyzed in a\nmodel which incorporates important features of the modern cryptocurren-\ncies. In this paper I focus on two such features. First, cryptocurrencies\noperate according to a protocol - a blockchain - and are, therefore, free from\nthe time-inconsistency problem. Second, the operation of the blockchain\ncosts real resources. I use the Lagos-Wright search theoretic monetary model\naugmented with privately issued currencies as in Fernandez-Villaverde and\nSanches (2016) and extend it by linear costs of private currency circulation. I\nshow that in contrast to Fernandez-Villaverde and Sanches (2016) cryptocur-\nrency competition 1) does not deliver price stability and 2) puts downward\npressure on the in ation in the public currency only when the costs private\ncurrency circulation (mining costs) are suciently low.\n\n\n", "summary": ":\nCurrency competition, Cryptocurrency, In ation, Blockchain\n\nJEL classication:\nE40, E42, E50, E58\n\n"}, {"description": "\nEstimation or mis-specification errors in the portfolio loss distribution can have a considerable impact\non risk measures. This paper investigates the sensitivity of tail-related risk measures including\nthe Value-at-Risk, expected shortfall and the expectile-quantile transformation level in an epsiloncontamination\nneighbourhood. The findings give the different approximations via the tail heaviness of\nthe contamination models and its contamination levels. Illustrating examples and an empirical study\non the dynamic CRIX capturing and displaying the market movements are given. The codes used to\nobtain the results in this paper are available via https://github.com/QuantLet/SRMC\n\n\n", "summary": ":\nSensitivity, expected shortfall, expectile, Value-at-Risk, risk management, influence function, CRIX\n\nJEL classification:\nC13, G10, G31\n\n"}, {"description": "\nNew Public Management helps universities and research institutions to perform in a highly competitive\nresearch environment. Evaluating publicly financed research results improves transparency, helps in reflection\nand self-assessment, and provides information for strategic decision making. In this paper we provide\nempirical evidence using data from a Collaborative Research Centre (CRC) on financial inputs and research\noutput from 2005 to 2016. After selecting performance indicators suitable for a CRC, we describe main\nproperties of the data using visualization techniques. To study the relationship between the dimensions of\nresearch performance, we use a time fixed effects panel data model and fixed effects Poisson model. With\nthe help of year dummy variables, we show how the pattern of research productivity changed over time after\ncontrolling for staff and travel costs. The joint depiction of the time fixed effects and the research project\u0092s\nlife cycle allows a better understanding of the development of the number of discussion papers over time.\n\n\n", "summary": ":\nResearch Performance, Time Fixed Effects Panel Data Model, Fixed Effects Poisson Model, Network, Collaborative Research Centre\n\nJEL classification:\nC00\n\n"}, {"description": "\nCryptocurrencies such as Bitcoin are establishing themselves as an investment asset and\nare often named the New Gold. This study, however, shows that the two assets could\nbarely be more dierent. Firstly, we analyze and compare conditional variance properties\nof Bitcoin and Gold as well as other assets and nd dierences in their structure.\nSecondly, we implement a BEKK-GARCH model to estimate time-varying conditional\ncorrelations. Gold plays an important role in nancial markets with ight-to-quality in\ntimes of market distress. Our results show that Bitcoin behaves as the exact opposite\nand it positively correlates with downward markets. Lastly, we analyze the properties of\nBitcoin as portfolio component and nd no evidence for hedging capabilities. We conclude\nthat Bitcoin and Gold feature fundamentally dierent properties as assets and linkages to\nequity markets. Our results hold for the broad cryptocurrency index CRIX. As of now,\nBitcoin does not re ect any distinctive properties of Gold other than asymmetric response\nin variance.\n\n\n", "summary": ":\nBEKK, Bitcoin, CRIX, Cryptocurrency, Gold, GARCH, Conditional Correlation, Asymmetry, Long memory\n\nJEL classification:\nC10; C58; G11\n\n"}, {"description": "\nThis paper analyzes the market impact of limit order books (LOB) taking crossstock\neffects into account. Based on penalized vector autoregressive approach, we\naim to identify significance and magnitude of the directed network channels within\nand between LOBs by bootstrapped impulse response functions. Moreover, information\non asymmetries and imbalances within the LOB over time would be derived. For\nthe sample of a NASDAQ blue-chip portfolio during 06-07/2016 we find that LOB\nnetwork effects crucially determine prices and bid-ask asymmetries are prevalent.\n\n\n", "summary": ":\nlimit order book, high dimension, generalized impulse response, high frequency, market risk, market impact, network, bootstrap\n\nJEL classification:\nC02, C13, C22, C45, G12\n\n"}, {"description": "\nWe investigate the concept of connectedness, which is important for risk\nmeasurement and management inGerman energy market. Understanding and\nlearning from these mechanisms are essential to avoid future systemic disasters.\nTo deal with large portfolio selection, we propose regularization approach\nto capture the spillover and contagion effects acrossGerman power derivatives.\nThis paper shows how network analysis can facilitate the monitoring of futures\nprice movements. Our methodology combines high-dimensional variable selection\ntechniques with network analysis, the results show that contracts like\nPhelix Base Year Options and Phelix Peak Year Futures are in the core of the\nEnergy futures market.\n\n\n", "summary": ":\nregularization, energy risk transmission, network, German energy market\n\nJEL classification:\nC1, Q41, Q47\n\n"}, {"description": "\nThis paper presents a new approach to non-parametric cluster analysis\ncalled Adaptive Weights Clustering (AWC). The idea is to identify the\nclustering structure by checking at different points and for dierent scales\non departure from local homogeneity. The proposed procedure describes\nthe clustering structure in terms of weights wij each of them measures\nthe degree of local inhomogeneity for two neighbor local clusters using\nstatistical tests of \no gap\" between them. The procedure starts from\nvery local scale, then the parameter of locality grows by some factor\nat each step. The method is fully adaptive and does not require to\nspecify the number of clusters or their structure. The clustering results\nare not sensitive to noise and outliers, the procedure is able to recover\ndierent clusters with sharp edges or manifold structure. The method\nis scalable and computationally feasible. An intensive numerical study\nshows a state-of-the-art performance of the method in various articial\nexamples and applications to text data. Our theoretical study states\noptimal sensitivity of AWC to local inhomogeneity.\n\n\n", "summary": ":\nadaptive weights, clustering, gap coecient, manifold clustering\n\nJEL classification:\n\nAMS 2000 Subject Classication:\nPrimary 62H30. Secondary 62G10\n\n"}, {"description": "\nGiven data y and k covariates xj one problem in linear regression\nis to decide which if any of the covariates to include when regressing\nthe dependent variable y on the covariates xj . In this paper three\nsuch methods, lasso, knockoff and Gaussian covariates are compared\nusing simulations and real data. The Gaussian covariate method is\nbased on exact probabilities which are valid for all y and xj making\nit model free. Moreover the probabilities agree with those based on\nthe F-distribution for the standard linear model with i.i.d. Gaussian\nerrors. It is conceptually, mathematically and algorithmically very\nsimple, it is very fast and makes no use of simulations. It outperforms\nlasso and knockoff in all respects by a considerable margin.\n\n\n", "summary": ":\n\n\nJEL classification:\n\n"}, {"description": "\nThe New Keynesian theory of inflation determination has been under scrutiny\ndue to identification issues, which rather have to do with the mechanism of inflation determination\nat its core (i.e. Cochrane (2011)). Moreover, similar identification problems\narise in the case of fiscal inflation (see for example Leeper and Leith (2016), Leeper and\nLi (2017) and Leeper and Walker (2012)). This paper makes a positive contribution.\nWe argue that statements about observational equivalence stem from referring to the\nequilibrium path, while this should not be our primary source of identifying restrictions.\nMoreover, policy identification (or lack thereof) relies on assumptions on the underlying\nshock structure, which is unobservable. We instead extract shocks using heterogeneous\nuncertain restrictions and external datasets, that is, we learn from errors. We are then\nable to recover deep and policy parameters irrespective of the prevailing equilibrium. We\nprovide time varying evidence on the efficacy of policy in stabilizing the US economy and\non the time varying plausibility of Ricardian versus non-Ricardian price determination.\nResults are work in progress.\n\n\n", "summary": ":\nMonetary and fiscal policy, Price Determination, Identification, Learning from errors\n\nJEL classification:\nC11, C13, E62, E63\n\n"}, {"description": "\nIn this paper, we consider a probabilistic setting where the probability measures\nare considered to be random objects. We propose a procedure of construction\nnon-asymptotic confidence sets for empirical barycenters in 2 -Wasserstein space and\ndevelop the idea further to construction of a non-parametric two-sample test that is\nthen applied to the detection of structural breaks in data with complex geometry. Both\nprocedures mainly rely on the idea of multiplier bootstrap (Spokoiny and Zhilova [29],\nChernozhukov, Chetverikov and Kato [13]). The main focus lies on probability measures\nthat have commuting covariance matrices and belong to the same scatter-location\nfamily: we proof the validity of a bootstrap procedure that allows to compute confidence\nsets and critical values for a Wasserstein-based two-sample test.\n\n\n", "summary": ":\nWasserstein barycenters, hypothesis testing, multiplier bootstrap,\nchange point detection, confidence sets.\n\nJEL classification:\n\n"}, {"description": "\nLet X1; : : : ;Xn be i.i.d. sample in Rp with zero mean and\nthe covariance matrix . The classic principal component analysis esti-\nmates the projector P\nJ onto the direct sum of some eigenspaces of\nby its empirical counterpart bPJ . Recent papers [20, 23] investigate the\nasymptotic distribution of the Frobenius distance between the projectors\nk bPJ ??P\nJ k2 . The problem arises when one tries to build a condence set\nfor the true projector eectively. We consider the problem from Bayesian\nperspective and derive an approximation for the posterior distribution of\nthe Frobenius distance between projectors. The derived theorems hold true\nfor non-Gaussian data: the only assumption that we impose is the con-\ncentration of the sample covariance b\nin a vicinity of . The obtained\nresults are applied to construction of sharp condence sets for the true pro-\njector. Numerical simulations illustrate good performance of the proposed\nprocedure even on non-Gaussian data in quite challenging regime.\n\n\n", "summary": ":\ncovariance matrix, spectral projector, principal\ncomponent analysis, Bernstein { von Mises theorem.\n\nJEL classification:\n\n"}, {"description": "\nIV regression in the context of a re-sampling is considered in the work. Comparatively, the contribution\nin the development is a structural identication in the IV model. The work also contains a\nmultiplier-bootstrap justication.\n\n\n", "summary": ":\nGaussian Process, Kernel methods, Wasserstein Distance\nJEL classification:\n\n"}, {"description": "\nIn this paper, we study the latent group structure in cryptocurrencies market\nby forming a dynamic return inferred network with coin attributions. We develop\na dynamic covariate-assisted spectral clustering method to detect the communities\nin dynamic network framework and prove its uniform consistency along the horizons.\nApplying our new method, we show the return inferred network structure and\ncoin attributions, including algorithm and proof types, jointly determine the market\nsegmentation. Based on the network model, we propose a novel \\hard-to-value\"\nmeasure using the centrality scores. Further analysis reveals that the group with a\nlower centrality score exhibits stronger short-term return reversals. Cross-sectional\nreturn predictability further conrms the economic meanings of our grouping results\nand reveal important portfolio management implications.\n\n\n", "summary": ":\nCommunity Detection, Dynamic Network, Return Predictability, Behavioural\nBias, Market Segmentation, Bitcoin\n\n"}, {"description": "\nIn a continuous-time setting where a risk-averse agent controls the drift of an output\nprocess driven by a Brownian motion, optimal contracts are linear in the terminal output;\nthis result is well-known in a setting with moral hazard and \u0096 under stronger assumptions\n\u0096 adverse selection. We show that this result continues to hold when in addition reser-\nvation utilities are type-dependent. This type of problem occurs in the study of optimal\ncompensation problems involving competing principals.\n\n\n", "summary": ":\nPrincipal-agent modelling; contract design; stochastic process; stochastic control\n\n"}, {"description": "\nIn 2012, JPMorgan accumulated a USD 6.2 billion loss on a credit derivatives portfolio,\nthe so-called \\London Whale\", partly as a consequence of de-correlations of non-perfectly\ncorrelated positions that were supposed to hedge each other. Motivated by this case, we\ndevise a factor model for correlations that allows for scenario-based stress-testing of correlations.\nWe derive a number of analytical results related to a portfolio of homogeneous\nassets. Using the concept of Mahalanobis distance, we show how to identify adverse scenarios\nof correlation risk. As an example, we apply the factor-model approach to the \\London\nWhale\" portfolio and determine the value-at-risk impact from correlation changes. Since our\nndings are particularly relevant for large portfolios, where even small correlation changes\ncan have a large impact, a further application would be to stress-test portfolios of central\ncounterparties, which are of systemically relevant size.\n\n\n", "summary": ":\nCorrelation stress testing, scenario selection, market risk, \"London Whale\"\n\nJEL Classication:\nC58, G15, G17, G18\n\n"}, {"description": "\nWe investigate correlations of asset returns in stress scenarios where a common risk\nfactor is truncated. Our analysis is performed in the class of normal variance mixture\n(NVM) models, which encompasses many distributions commonly used in nancial\nmodelling. For the special cases of jointly normally and t-distributed asset returns\nwe derive closed formulas for the correlation under stress. For the NVM distribution,\nwe calculate the asymptotic limit of the correlation under stress, which depends on\nwhether the variables are in the maximum domain of attraction of the Frechet or\nGumbel distribution. It turns out that correlations in heavy-tailed NVM models are\nless sensitive to stress than in medium- or light-tailed models. Our analysis sheds light\non the suitability of this model class to serve as a quantitative framework for stress\ntesting, and as such provides valuable information for risk and capital management\nin nancial institutions, where NVM models are frequently used for assessing capital\nadequacy. We also demonstrate how our results can be applied for more prudent stress\ntesting.\n\n\n", "summary": ":\nStress testing, risk management, correlation, normal variance mixture distribution, multivariate normal distribution, multivariate t-distribution.\n\n"}, {"description": "\nParalleling regulatory developments, we devise value-at-risk and expected shortfall type\nrisk measures for the potential losses arising from using misspecied models when pricing\nand hedging contingent claims. Essentially, losses from model risk correspond to losses realized\non a perfectly hedged position. Model uncertainty is expressed by a set of pricing\nmodels, relative to which potential losses are determined. Using market data, a unied\nloss distribution is attained by weighing models according to a relative likelihood criterion.\nExamples demonstrate the magnitude of model risk and corresponding capital buers necessary\nto suciently protect trading book positions against unexpected losses from model\nrisk.\n\n\n", "summary": ":\nModel risk, parameter uncertainty, hedge error, value-at-risk, expected shortfall\n\nJEL Clasification:\nG32, G13\n\n"}, {"description": "\nStarting from well-known empirical stylised facts of nancial time series, we develop\ndynamic portfolio protection trading strategies based on econometric methods. As a criterion\nfor riskiness we consider the evolution of the value-at-risk spread from a GARCH\nmodel with normal innovations relative to a GARCH model with generalised innovations.\nThese generalised innovations may for example follow a Student t, a generalised\nhyperbolic (GH), an alpha-stable or a Generalised Pareto (GPD) distribution. Our\nresults indicate that the GPD distribution provides the strongest signals for avoiding\ntail risks. This is not surprising as the GPD distribution arises as a limit of tail behaviour\nin extreme value theory and therefore is especially suited to deal with tail risks.\nOut-of-sample backtests on 11 years of DAX futures data, indicate that the dynamic\ntail-risk protection strategy eectively reduces the tail risk while outperforming traditional\nportfolio protection strategies. The results are further validated by calculating\nthe statistical signicance of the results obtained using bootstrap methods. A number of\nrobustness tests including application to other assets further underline the eectiveness\nof the strategy. Finally, by empirically testing for second order stochastic dominance,\nwe nd that risk averse investors would be willing to pay a positive premium to move\nfrom a static buy-and-hold investment in the DAX future to the tail-risk protection\nstrategy.\n\n", "summary": ":\ntail-risk protection, portfolio protection, extreme events, tail distributions\n\n"}, {"description": "\nIn the present paper we propose a new method, the Penalized Adaptive\nMethod (PAM), for a data driven detection of structural changes in sparse linear\nmodels. The method is able to allocate the longest homogeneous intervals over\nthe data sample and simultaneously choose the most proper variables with the\nhelp of penalized regression models. The method is simple yet exible and can\nbe safely applied in high-dimensional cases with dierent sources of parameter\nchanges. Comparing with the adaptive method in linear models, its combination\nwith dimension reduction yields a method which properly selects signicant\nvariables and detects structural breaks while steadily reduces the forecast error\nin high-dimensional data.\n\n", "summary": ":\nSCAD penalty, propagation-separation, adaptive window choice, multiplier bootstrap\n\n"}, {"description": "\nWe consider a generalization of Baum-Katz theorem for random vari-\nables satisfying some cover conditions. Consequently, we get the result for many\ndependent structure, such as END, -mixing, -mixing and -mixing, etc.\n\n", "summary": ":\nComplete convergence; Marcinkiewicz-Zygmund type SLLN; Extended negatively dependent; Mixing dependency; Weakly mean bounded.\n\n"}, {"description": "\nIn this paper, the complete convergence and complete moment convergence for maximal\nweighted sums of extended negatively dependent random variables are investigated. Some su\u00b1cient\nconditions for the convergence are provided. In addition, the Marcinkiewicz{Zygmund type strong law\nof large numbers for weighted sums of extended negatively dependent random variables is obtained.\nThe results obtained in the article extend the corresponding ones for independent random variables\nand some dependent random variables.\n\n", "summary": ":\nExtended negatively dependent, complete convergence, complete moment convergence, maximal weighted sums, strong law of large numbers\n\n"}, {"description": "\nIn this paper, the complete convergence for maximal weighted sums of extended negatively\ndependent (END, for short) random variables is investigated. Some sucient conditions\nfor the complete convergence and some applications to a nonparametric model are provided. The\nresults obtained in the paper generalise and improve the corresponding ones of Wang el al. (2014b)\nand Shen, Xue, and Wang (2017).\n\n", "summary": ":\nComplete convergence; Maximal weighted sums; Extended negatively dependent.\n\n"}, {"description": "\nNews move markets and contains incremental information about stock\nreactions. Future trading volumes, volatility and returns are a\nected by sentiments of texts and opinions expressed in articles. Earlier\nwork of sentiment distillation of stock news suggests that risk prole reactions\nmight differ across sectors.\nConventional asset pricing theory recognizes the role of a sector and its\nrisk uniqueness that differs from market or rm specic risk.\nOur research assesses whether incorporating the sentiment distilled from\nsector specic news carries information about risk proles. Textual analytics applied to about 600K\narticles leads us with lexical projection and machine learning to classication of sentiment polarities. The\ntexts are scraped from offcial NASDAQ web pages and with Natural Language Processing (NLP)\ntechniques, such as tokenization, lemmatization, a sector specic sentiment is extracted using a lexical\napproach and a nancial phrase bank. Predicted sentence-level polarities are aggregated into a bullishness\nmeasure on a daily basis and fed into a panel regression analysis with sector indicators. Supervised\nlearning with hinge or logistic loss and regularization yields good prediction results of polarity. Compared with\nstandard lexical projections, the supervised learning approach yields superior predictions of sentiment,\nleading to highly sector specic sentiment reactions. The Consumer Staples, Health Care and Materials\nsectors show strong risk prole reactions to negative polarity.\n\n", "summary": ":\nInvestor Sentiment, Attention Analysis, Sector-specic Reactions, Volatility, Text Mining, Polarity\n\n"}, {"description": "\nCryptocurrencies refer to a type of digital cash that use distributed ledger - or\nblockchain technology - to provide secure transactions. These currencies are generally\nmisunderstood. While initially dismissed as fads or bubbles, many large central\nbanks are considering launching their own version of national cryptocurrencies. In\ncontrast to most data in nancial economics, there is a plethora of detailed (free)\ndata on the history of every transaction for the cryptocurrency complex. Further,\nthere is little empirically-oriented research on this new asset class. This is an extraordinary\nresearch opportunity for academia. We provide a starting point by\ngiving an insight into cryptocurrency mechanisms and detailing summary statistics\nand focusing on potential future research avenues in nancial economics.\n\n", "summary": ":\nCryptocurrency, Blockchain, Bitcoin, Economic bubbles, Peer-to-Peer, Cryptographic\nhashing, Consensus, Proof-of-Work, Proof-of-stake, Volatility\n\n"}, {"description": "\nIn this article, we propose a new class of semiparametric instrumental variable models with partially varying\ncoefficients, in which the structural function has a partially linear form and the impact of endogenous\nstructural variables can vary over different levels of some exogenous variables. We propose a three-step\nestimation procedure to estimate both functional and constant coefficients. The consistency and asymptotic\nnormality of these proposed estimators are established. Moreover, a generalized F-test is developed to\ntest whether the functional coefficients are of particular parametric forms with some underlying economic\nintuitions, and furthermore, the limiting distribution of the proposed generalized F-test statistic under the\nnull hypothesis is established. Finally, we illustrate the finite sample performance of our approach with\nsimulations and two real data examples in economics.\n\n", "summary": ":\nEndogeneity; Functional coefficients; Generalized F-test; Instrumental variables models;\nNonparametric test; Profile least squares\n\n"}, {"description": "\nIn this paper, we propose a new class of regime shift models with flexible switching\nmechanism that relies on a nonparametric probability function of the observed threshold\nvariables. The proposed models generally embrace traditional threshold models\nwith contaminated threshold variables or heterogeneous threshold values, thus gaining\nmore power in handling complicated data structure. We solve the identification issue by\nimposing either global shape restriction or boundary condition on the nonparametric\nprobability function. We utilize the natural connection between penalized splines and\nhierarchical Bayes to conduct smoothing. By adopting different priors, our procedure\ncould work well for estimations of smooth curve as well as discontinuous curves with\noccasionally structural breaks. Bayesian tests for the existence of threshold effects are\nalso conducted based on the posterior samples from Markov chain Monte Carlo (MCMC)\nmethods. Both simulation studies and an empirical application in predicting\nthe U.S. stock market returns demonstrate the validity of our methods.\n\n", "summary": ":\nThreshold Model, Nonparametric, Markov Chain Monte Carlo, Bayesian\nInference, Spline.\n\n"}, {"description": "\nThis paper is concerned with selecting important covariates\nand estimating the index direction simultaneously for\nhigh dimensional single-index models. We develop an efficient\nThreshold Gradient Directed Regularization method\nvia maximizing Distance Covariance (DC-TGDR) between\nthe single index and response variable. Due to the appealing\nproperty of distance covariance which can measure nonlinear\ndependence between random variables, the proposed\nmethod avoids estimating the unknown link function of the\nsingle index and dramatically reduces computational complexity\ncompared to other methods that use smoothing techniques.\nIt keeps the model-free advantage from the view of\nsufficient dimension reduction and requires neither predictors\nnor response variable to be continuous. In addition, the\nDC-TGDR method encourages a grouping effect. That is,\nit is capable of choosing highly correlated covariates in or\nout of the model together. We examine finite-sample performance\nof the proposed method by Monte Carlo simulations.\nIn a real data analysis, we identify important copy number\nalterations (CNAs) for gene expression.\n\n", "summary": ":\nDistance covariance, Highdimensional\ndata, Threshold gradient directed regularization,\nSingle-index models, Variable selection.\n\n"}, {"description": "\nIn this article, we study a nonparametric approach regarding a general nonlinear reduced form equation\nto achieve a better approximation of the optimal instrument. Accordingly, we propose the nonparametric\nadditive instrumental variable estimator (NAIVE) with the adaptive group Lasso.We theoretically demonstrate\nthat the proposed estimator is root-n consistent and asymptotically normal. The adaptive group\nLasso helps us select the valid instruments while the dimensionality of potential instrumental variables is\nallowed to be greater than the sample size. In practice, the degree and knots of B-spline series are selected\nby minimizing the BIC or EBIC criteria for each nonparametric additive component in the reduced form\nequation. In Monte Carlo simulations, we show that the NAIVE has the same performance as the linear\ninstrumental variable (IV) estimator for the truly linear reduced form equation. On the other hand, the\nNAIVE performs much better in terms of bias and mean squared errors compared to other alternative\nestimators under the high-dimensional nonlinear reduced form equation. We further illustrate our method\nin an empirical study of international trade and growth. Our findings provide\n\n", "summary": ":\nAdaptive group Lasso; Instrumental variables; Nonparametric additive model; Optimal\nestimator; Variable selection.\n\n"}, {"description": "\nOpen-ended responses are widely used in market research studies. Processing of such\nresponses requires labor-intensive human coding. This paper focuses on unsupervised topic\nmodels and tests their ability to automate the analysis of open-ended responses. Since state-ofthe-\nart topic models struggle with the shortness of open-ended responses, the paper considers\nthree novel short text topic models: Latent Feature Latent Dirichlet Allocation, Biterm Topic\nModel and Word Network Topic Model. The models are fitted and evaluated on a set of realworld\nopen-ended responses provided by a market research company. Multiple components\nsuch as topic coherence and document classification are quantitatively and qualitatively\nevaluated to appraise whether topic models can replace human coding. The results suggest that\ntopic models are a viable alternative for open-ended response coding. However, their\nusefulness is limited when a correct one-to-one mapping of responses and topics or the exact\ntopic distribution is needed.\n\n", "summary": ":\nMarket research, open-ended responses, text analytics, short text topic models\n\n"}, {"description": "\nAn extensive empirical literature documents a generally negative relation, named the \u0093leverage\neffect,\u0094 between asset returns and changes of volatility. It is more challenging to establish\nsuch a return-volatility relationship for jumps in high-frequency data. We propose new nonparametric\nmethods to assess and test for a discontinuous leverage effect \u0097 i.e. a covariation\nbetween contemporaneous jumps in prices and volatility. The methods are robust to market\nmicrostructure noise and build on a newly developed price-jump localization and estimation\nprocedure. Our empirical investigation of six years of transaction data from 320 NASDAQ\nfirms displays no unconditional negative covariation between price and volatility cojumps.\nWe show, however, that there is a strong and significant discontinuous leverage effect if\none conditions on the sign of price jumps and whether the price jumps are market-wide or\nidiosyncratic.\n\n", "summary": ":\nHigh-frequency data, market microstructure, news impact, market-wide jumps, price jump, volatility jump\n\n"}, {"description": "\nIn deconvolution in Rd; d 1; with mixing density p(2 P) and kernel h; the mixture\ndensity fp(2 Fp) can always be estimated with f^pn; ^pn 2 P; via Minimum Distance\nEstimation approaches proposed herein, with calculation of f^pn's upper L1-error rate, an;\nin probability or in risk; h is either known or unknown, an decreases to zero with n: In\napplications, an is obtained when P consists either of products of d densities dened on\na compact, or L1 separable densities in R with their dierences changing sign at most J\ntimes; J is either known or unknown. When h is known and p is ~q-smooth, vanishing\noutside a compact in Rd; plug-in upper bounds are then also provided for the L2-error\nrate of ^pn and its derivatives, respectively, in probability or in risk; ~q 2 R+; d 1: These\nL2-upper bounds depend on h's Fourier transform, ~h(6= 0); and have rates (log a??1\nn )??N1 and aN2 n , respectively, for h super-smooth and smooth; N1 > 0; N2 > 0: For the typical\nan (log n) n??; the former (logarithmic) rate bound is optimal for any > 0 and\nthe latter misses the optimal rate by the factor (log n) when = :5; > 0; > 0: The\nexponents N1 and N2 appear also in optimal rates and lower error and risk bounds in the\ndeconvolution literature.\n\n", "summary": ":\n\n\n"}, {"description": "\nUplift modeling combines machine learning and experimental strategies to estimate the differential\neffect of a treatment on individuals\u0092 behavior. The paper considers uplift models in the scope of\nmarketing campaign targeting. Literature on uplift modeling strategies is fragmented across academic\ndisciplines and lacks an overarching empirical comparison. Using data from online retailers,\nwe fill this gap and contribute to literature through consolidating prior work on uplift modeling\nand systematically comparing the predictive performance and utility of available uplift modeling\nstrategies. Our empirical study includes three experiments in which we examine the interaction\nbetween an uplift modeling strategy and the underlying machine learning algorithm to implement\nthe strategy, quantify model performance in terms of business value and demonstrate the advantages\nof uplift models over response models, which are widely used in marketing. The results\nfacilitate making specific recommendations how to deploy uplift models in e-commerce applications.\n\n", "summary": ":\ne-commerce analytics, machine learning, uplift modeling, real-time targeting\n\n"}, {"description": "\nThe estimation of a causal parameter in a high-dimensional setting where the\nfunctions are potentially complex is a challenging task. Parametric and linear\nmodelling is not sufficient to generate unbiased and consistent estimators. Modern\napproaches, therefore, use machine learning (ML) algorithms to learn these nuisance\nfunctions. However, this leads to new problems like the regularization bias or\noverfitting that are common when using ML models.\nThis paper considers different novel methods that overcome these problems or at\nleast address them. These methods differ in terms of the target parameter, namely\nthe average treatment effect of the population, group heterogeneity or the conditional\naverage treatment effect for each individual. Each method is first investigated and\ntested separately and second, they are compared among each other. To do this in a\ndisciplined manner, simulations with synthetic data are used. This ensures that all\ndistributions of the generated treatment effect parameters are known. The findings\nare that each method has its limits in terms of unbiased estimation, the detection\nof heterogeneity and also the determination of which covariates are responsible for\ndifferent causal effects.\n\n", "summary": ":\ncausal inference, machine learning, simulation study, sample-splitting\ndouble machine learning, sorted group ATE (GATES), causal tree\n\n"}, {"description": ":\nSecond-hand car markets contribute to billions of Euro turnover each year but\nhardly generate profit for used car dealers. The paper examines the potential of\nsophisticated data-driven pricing systems to enhance supplier-side decision-\nmaking and escape the zero-profit-trap. Profit maximization requires an accurate\nunderstanding of demand. The paper identifies factors that characterize consumer\ndemand and proposes a framework to estimate demand functions using survival\nanalysis. Empirical analysis of a large data set of daily used car sales between\n2008 to 2012 confirm the merit of the new factors. Observed results also show\nthe value of survival analysis to explain and predict demand. Random survival\nforest emerges as the most suitable vehicle to develop price response functions\nas input for a dynamic pricing system.\n\n", "summary": ":\nAutomotive Industry, Price Optimization, Survival Analysis, Dynamic Pricing\n\n"}, {"description": ":\nWe study investor sentiment on a non-classical asset, cryptocurrencies using a\n\u201ccryptospecificlexicon\u201d recently proposed in Chen et al. (2018) and statistical\nlearning methods.We account for context-specific information and word similarity\nby learning word embeddingsvia neural network-based Word2Vec model. On top of\npre-trained word vectors, weapply popular machine learning methods such as\nrecursive neural networks for sentencelevelclassification and sentiment index\nconstruction. We perform this analysis on a noveldataset of 1220K messages\nrelated to 425 cryptocurrencies posted on a microblogging platformStockTwits\nduring the period between March 2013 and May 2018. The constructed sentiment\nindices are value-relevant in terms of its return and volatility predictability\nfor thecryptocurrency market index.\n\n", "summary": ":\nsentiment analysis, lexicon, social media, word embedding, deep learning\n\n"}, {"description": ": Excessive house price growth was at the heart of the financial crisis in 2007/08. Since then, many countries have added cooling measures to their regulatory frameworks. It has been found that these measures can indeed control price growth, but no one has examined whether this has adverse consequences for the housing wealth distribution. We examine this for Singapore, which started in 2009 to target price growth over ten rounds in total. We find that welfare from housing wealth in the last round might not be higher than before 2009. This depends on the deflator used to convert nominal into real prices. Irrespective of the deflator, we can reject that welfare increased monotonically over the different rounds. ", "summary": ": house price distribution, stochastic dominance tests "}, {"description": ":\nExcessive house price growth was at the heart of the financial crisis in\n2007/08. Since then, many countries have added cooling measures to their\nregulatory frameworks. It has been found that these measures can indeed control\nprice growth, but no one has examined whether this has adverse consequences for\nthe housing wealth distribution. We examine this for Singapore, which started in\n2009 to target price growth over ten rounds in total. We find that welfare from\nhousing wealth in the last round might not be higher than before 2009. This\ndepends on the deflator used to convert nominal into real prices. Irrespective\nof the deflator, we can reject that welfare increased monotonically over the\ndifferent rounds.\n\n", "summary": ":\nhouse price distribution, stochastic dominance tests\n\n"}, {"description": ":\nThis work aims to investigate the (inter)relations of information arrival, news\nsentiment, volatilities and jump dynamics of intraday returns. Two parametric\nGARCH-type jump models which explicitly incorporate both news arrival and news\nsentiment variables are proposed, among which one assumes news affecting\nfinancial markets through the jump component while the other postulating the\nGARCH component channel. In order to give the most-likely format of the\ninteractions between news arrival and stock market behaviors, these two models\nare compared with several other easier versions of GARCH-type models based on\nthe calibration results on DJIA 30 stocks. The necessity to include news\nprocesses in intraday stock volatility modeling is justified in our specific\ncalibration samples (2008 and 2013, respectively). While it is not as profitable\nto model jump process separately as using simpler GARCH process with error\ndistribution capable to capture fat tail behaviors of financial time series. In\nconclusion, our calibration results suggest GARCH-news model with skew-t\ninnovation distribution as the best candidate for intraday returns of large\nstocks in US market, which means one can probably avoid the complicatedness of\nmodelling jump behavior by using a simplier skew-t error distribution assumption\ninstead, but it\u2019s necessary to incorporate news variables.\n\n", "summary": ":\ninformation arrival, volatility modeling, jump, sentiment, GARCH\n\n"}, {"description": ":\nWeekly, quarterly and yearly risk measures are crucial for risk reporting\naccording to Basel III and Solvency II. For the respective data frequencies, the\nauthors show in a simulation and backtest study that available data series are\nnot sufficient in order to estimate Value at Risk and Expected Shortfall\nsufficiently, given confidence levels of 99.9% and 99.99%. Accordingly, this\npaper presents a semi-parametric estimation method, rescaling data from high- to\nlow-frequency which allows to obtain significantly more data points for the\nestimation of the respective risk measures. The presented methodology in the\n\u03b1-stable framework, which is able to mimic multifractal behavior in asset\nreturns, provides tail events which never occurred in the original low-frequency\ndataset.\n\n", "summary": ":\nhigh-frequency, multifractal, stable distribution, rescaling, risk management,\nValue at Risk, quantile distribution\n\n"}, {"description": ":\nThis paper provides a detailed framework for modeling portfolios, achieving the\nhighest growth rate under subjective risk constraints such as Value at Risk\n(VaR) in the presence of stable laws. Although the maximization of the expected\nlogarithm of wealth induces outperforming any other significantly different\nstrategy, the Kelly Criterion implies larger bets than a risk-averse investor\nwould accept. Restricting the Kelly optimization by spectral risk measures, the\nauthors provide a generalized mapping for different measures of growth and\nsecurity. Analyzing over 30 years of S&P 500 returns for different sampling\nfrequencies, the authors find evidence for leptokurtic behavior for all\nrespective sampling frequencies. Given that lower sampling frequencies imply a\nsmaller number of data points, this paper argues in favor of \u03b1-stable laws and\nits scaling behavior to model financial market returns for a given horizon in an\ni.i.d. world. Instead of simulating from the class of elliptically stable\ndistributions, a nonparametric scaling approximation, based on the data-set\nitself, is proposed. Our paper also uncovers that including long put options\ninto the portfolio optimization, improves the growth criterion for a given\nsecurity level, leading to a new Kelly portfolio providing the highest geometric\nmean.\n\n", "summary": ":\ngrowth-optimal, Kelly criterion, protective put, portfolio optimization, stable\ndistribution, Value at Risk\n\n"}, {"description": ":\nUnderstanding the topological structure of real world networks is of huge\ninterest in a variety of fields. One of the way to investigate this structure is\nto find the groups of densely connected nodes called communities. This paper\npresents a new non-parametric method of community detection in networks called\nAdaptive Weights Community Detection. The idea of the algorithm is to associate\na local community for each node. On every iteration the algorithm tests a\nhypothesis that two nodes are in the same community by comparing their local\ncommunities. The test rejects the hypothesis if the density of edges between\nthese two local communities is lower than the density inside each one. A\ndetailed performance analysis of the method shows its dominance over state-of-\nthe-art methods on well known artificial and real world benchmarks.\n\n", "summary": ":\nAdaptive weights, Gap coefficient, Graph clustering, Nonparametric, Overlapping\ncommunities\n\n"}, {"description": ":\nDeep learning has substantially advanced the state-of-the-art in computer\nvision, natural language processing and other elds. The paper examines the\npotential of contemporary recurrent deep learning architectures for nancial\ntime series forecasting. Considering the foreign exchange market as testbed, we\nsystematically compare long short-term memory networks and gated recurrent units\nto traditional recurrent architectures as well as feedforward networks in terms\nof their directional forecasting accuracy and the profitability of trading model\npredictions. Empirical results indicate the suitability of deep networks for\nexchange rate forecasting in general but also evidence the diculty of\nimplementing and tuning corresponding architectures. Especially with regard to\ntrading pro t, a simpler neural network may perform as well as if not better\nthan a more complex deep neural network.\n\n", "summary": ":\nDeep learning, Financial time series forecasting, Recurrent neural networks,\nForeign exchange rates\n\n"}, {"description": ":\nCryptocurrencies are becoming an attractive asset class and are the focus of\nrecent quantitative research. The joint dynamics of the cryptocurrency market\nyields information on network risk. Utilizing the adaptive LASSO approach, we\nbuild a dynamic network of cryptocurrencies and model the latent communities\nwith a dynamic stochastic blockmodel. We develop a dynamic covariate-assisted\nspectral clustering method to uniformly estimate the latent group membership of\ncryptocurrencies consistently. We show that return inter-predictability and\ncrypto characteristics, including hashing algorithms and proof types, jointly\ndetermine the crypto market segmentation. Based on this classification result,\nit is natural to employ eigenvector centrality to identify a cryptocurrency\u2019s\nidiosyncratic risk. An asset pricing analysis finds that a cross-sectional\nportfolio with a higher centrality earns a higher risk premium. Further tests\nconfirm that centrality serves as a risk factor well and delivers valuable\ninformation content on cryptocurrency markets.\n\n", "summary": ":\nCommunity Detection, Dynamic Stochastic Blockmodel, Spectral Clustering, Node\nCovariate, Return Predictability, Portfolio Management\n\n"}, {"description": ":\nHousing typically takes up a major proportion of households' expenditure, and\nthus it certainly plays a critical role in shaping the pattern of income in-\nequality and social mobility. Whether high housing price-to-rent ratio will am-\nplify inequality and inhibit social class upgrading is still a controversial\nissue in the existing literature. In this paper, we develop a partial\nequilibrium life- cycle framework to address these issues. Agents in our economy\nare divided into two social classes according to the initial human capital level\ninherited from their parents. Those who belong to upper class will draw their\ninnate abilities from a distribution that rst order stochastically dominates\nthose from lower class. Throughout the entire lifecycle, agents make endogenous\nhuman capital investment and housing tenure decisions. We calibrate the model to\nmimic some stylized facts in the the real world counter part. Our simulation\nresults indicate an inverse-U pattern between housing price-to-rent ratio and\nmeasures of income inequality, and as well as a U-shape pattern between price-\nto-rent ratio and social mobility measured by Shorrocks Index. The implication\nis that housing tends to amplify the inequality and slow down the social\nmobility when houses can only be purchased by a small group of agents in the\neconomy. Moreover, our results also suggest that better quality of education as\na result of a higher return to human capital investment tends to dampen the role\nof housing.\n\n", "summary": ":\nIncome Inequality, Social Mobility, Price-to-rent ratio\n\n"}, {"description": ":\nIn this paper, we build an overlapping generation model to examine the reason\nwhy developed countries with similar background have implemented different\nsocial health insurance systems. We propose two hypotheses to explain this\nphenomenon: (i) the different participation rates of the poor in the voting;\n(ii) the distinct attitudes towards the size of the government and the existence\nof a compulsory social health insurance system. Agents need to vote for one of\ntwo policies: Policy I without Social Health Insurance (SHI) but with the\nsubsidy for the poor, and Policy II with fully covered SHI. By comparing either\ntheir current utility or the expected life time utility, households will choose\none policy. We find that under Policy I, the derivative of the changes of\nexpected utility with respect to income is not monotonic. This means that both\nthe poorest and the richest dislike the social health insurance system. With the\ncalibrated parameters, we solve the benchmark and find that the public\u2019s\nattitude towards the size of the government and the lower representation of the\npoor affect the election result. The changes in the minimum consumption level\nunder Policy I affect the voting results most, followed by the attitude. Voting\nParticipant rate plays the most insignificant role in the voting outcome. The\nsensitivity analysis shows that our main findings are robust to the input\nparameters.\n\n", "summary": ":\nSocial Health Insurance, Voting\n\n"}, {"description": ":\nIncreasingly volatile and distributed energy production challenge traditional\nmechanisms to manage grid loads and price energy. Local energy markets (LEMs)\nmay be a response to those challenges as they can balance energy production and\nconsumption locally and may lower energy costs for consumers. Blockchain-based\nLEMs provide a decentralized market to local energy consumer and prosumers. They\nimplement a market mechanism in the form of a smart contract without the need\nfor a central authority coordinating the market. Recently proposed blockchain-\nbased LEMs use auction designs to match future demand and supply. Thus, such\nblockchain-based LEMs rely on accurate short-term forecasts of individual\nhouseholds\u2019 energy consumption and production. Often, such accurate forecasts\nare simply assumed to be given. The present research tests this assumption.\nFirst, by evaluating the forecast accuracy achievable with state-of-the-art\nenergy forecasting techniques for individual households and, second, by\nassessing the effect of prediction errors on market outcomes in three different\nsupply scenarios. The evaluation shows that, although a LASSO regression model\nis capable of achieving reasonably low forecasting errors, the costly settlement\nof prediction errors can offset and even surpass the savings brought to\nconsumers by a blockchain-based LEM. This shows, that due to prediction errors,\nparticipation in LEMs may be uneconomical for consumers, and thus, has to be\ntaken into consideration for pricing mechanisms in blockchain-based LEMs.\n\n", "summary": ":\nBlockchain; Local Energy Market; Smart Contract; Machine Learning; Household;\nEnergy Prediction; Prediction Errors; Market Mechanism\n\n"}, {"description": ":\nWe distill tone from a huge assortment of NASDAQ articles to examine the\npredictive power of media-expressed tone in single-stock option markets and\nequity markets. We find that (1) option markets are impacted by media tone; (2)\noption variables predict stock returns along with tone; (3) option variables\northogonalized to public information and tone are more effective predictors of\nstock returns; (4) overnight tone appears to be more informative than trading-\ntime tone, possibly due to a different thematic coverage of the trading versus\nthe overnight archive; (5) tone disagreement commands a strong positive risk\npremium above and beyond market volatility.\n\n", "summary": ":\noption markets, equity markets, stock return predictability, media tone, topic\nmodel\n\n"}, {"description": ":\nThe 2017 bubble on the cryptocurrency market recalls our memory in the dot-com\nbubble, during which hard-to-measure fundamentals and investors\u2019 illusion for\nbrand new technologies led to overvalued prices. Benefiting from the massive\nincrease in the volume of messages published on social media and message boards,\nwe examine the impact of investor sentiment, conditional on bubble regimes, on\ncryptocurrencies aggregate return prediction. Constructing a crypto-specific\nlexicon and using a local-momentum autoregression model, we find that the\nsentiment effect is prolonged and sustained during the bubble while it turns out\na reversal effect once the bubble collapsed. The out-of-sample analysis along\nwith portfolio analysis is conducted in this study. When measuring investor\nsentiment for a new type of asset such as cryptocurrencies, we highlight that\nthe impact of investor sentiment on cryptocurrency returns is conditional on\nbubble regimes.\n\n", "summary": ":\nCryptocurrency; Sentiment; Bubble; Return Predictability\n\n"}, {"description": ":\nThe paper presents a systematic theory for asymptotic inferences based on\nautocovariances of stationary processes. We consider nonparametric tests for se\nrial correlations using the maximum and the quadratic deviations of sample\nautocovariances. For these cases, with proper centering and rescaling, the\nasymptotic distributions of the deviations are Gumbel and Gaussian, respec\ntively. To establish such an asymptotic theory, as byproducts, we develop a\nnormal comparison principle and propose a sufficient condition for summability\nof joint cumulants of stationary processes. We adapt a blocks of blocks\nbootstrapping procedure proposed by Kuensch (1989) and Liu and Singh (1992) to\nthe maximum deviation based tests to improve the finite-sample performance.\n\n", "summary": ":\nAutocovariance, blocks of blocks bootstrapping, Box-Pierce test, extreme value\ndistribution, moderate deviation, normal comparison, physical dependence\nmeasure, short range dependence, stationary process, summability of cumulants\n\n"}, {"description": ":\nThe aim of this paper is to prove the phenotypic convergence of\ncryptocurrencies, in the sense that individual cryptocurrencies respond to\nsimilar selection pressures by developing similar characteristics. In order to\nretrieve the cryptocurrencies phenotype, we treat cryptocurrencies as financial\ninstruments (genus proximum) and find their specific difference (differentia\nspecifica) by using the daily time series of log-returns. In this sense, a daily time\nseries of asset returns (either cryptocurrencies or classical assets) can be\ncharacterized by a multidimensional vector with statistical components like\nvolatility, skewness, kurtosis, tail probability, quantiles, conditional tail\nexpectation or fractal dimension. By using dimension reduction techniques\n(Factor Analysis) and classification models (Binary Logistic Regression,\nDiscriminant Analysis, Support Vector Machines, K-means clustering, Variance\nComponents Split methods) for a representative sample of cryptocurrencies,\nstocks, exchange rates and commodities, we are able to classify cryptocurrencies\nas a new asset class with unique features in the tails of the log-returns\ndistribution. The main result of our paper is the complete separation of the\ncryptocurrencies from the other type of assets, by using the Maximum Variance\nComponents Split method. More, we observe a divergent evolution of the\ncryptocurrencies species, compared to the classical assets, mainly due to the\ntails behaviour of the log-returns distribution. The codes used here are\navailable via www.quantlet.de.\n\n", "summary": ":\ncryptocurrency, genus proximum, differentia specifica, classification,\nmultivariate analysis, factor models, phenotypic convergence, divergent\nevolution\n\n"}, {"description": ":\nWe propose an approach to calibrate the conditional value-at-risk (CoVaR) of\nfinancial institutions based on neural network quantile regression. Building on\nthe estimation results we model systemic risk spillover effects across banks by\nconsidering the marginal effects of the quantile regression procedure. We adopt a\ndropout regularization procedure to remedy the well-known issue of overfitting\nfor neural networks, and we provide empirical evidence for the favorable out-of-\nsample performance of a regularized neural network. We then propose three\nmeasures for systemic risk from our fitted results. We find that systemic risk\nincreases sharply during the height of the financial crisis in 2008 and again\nafter a short period of easing in 2011 and 2015. Our approach also allows\nidentifying systemically relevant firms during the financial crisis.\n\n", "summary": ":\nSystemic risk, CoVaR, Quantile regression, Neural networks\n\n"}, {"description": ":\nThis research analyses high-frequency data of the cryptocurrency market in\nregards to intraday trading patterns. We study trading quantitatives such as\nreturns, traded volumes, volatility periodicity, and provide summary statistics\nof return correlations to CRIX (CRyptocurrency IndeX), as well as respective\noverall high-frequency based market statistics. Our results provide mandatory\ninsight into a market, where the grand scale employment of automated trading\nalgorithms and the extremely rapid execution of trades might seem to be a\nstandard based on media reports. Our findings on intraday momentum of trading\npatterns lead to a new view on approaching the predictability of economic value\nin this new digital market.\n\n", "summary": ":\nCryptocurrency, High-Frequency Trading, Algorithmic Trading, Liquidity,\nVolatility, Price Impact, CRIX\n\n"}, {"description": ":\nA daily systemic risk measure is proposed accounting for links and mutual\ndependencies between financial institutions utilising tail event information. FRM\n(Financial Risk Meter) is based on Lasso quantile regression designed to capture\ntail event co-movements. The FRM focus lies on understanding active set data\ncharacteristics and the presentation of interdependencies in a network topology.\nTwo FRM indices are presented, namely, FRM@Americas and FRM@Europe. The FRM\nindices detect systemic risk at selected areas and identifies risk factors. In\npractice, FRM is applied to the return time series of selected financial\ninstitutions and macroeconomic risk factors. Using FRM on a daily basis, we\nidentify companies exhibiting extreme \"co-stress\", as well as \"activators\" of\nstress. With the SRM@EuroArea, we extend to the government bond asset class. FRM\nis a good predictor for recession probabilities, constituting the FRM-implied\nrecession probabilities. Thereby, FRM indicates tail event behaviour in a\nnetwork of financial risk factors.\n\n", "summary": ":\nSystemic Risk, Quantile Regression, Financial Markets, Risk Management, Network\nDynamics, Recession\n\n"}, {"description": ":\nA daily systemic risk measure is proposed accounting for links and mutual\ndependencies between financial institutions utilising tail event information. FRM\n(Financial Risk Meter) is based on Lasso quantile regression designed to capture\ntail event co-movements. The FRM focus lies on understanding active set data\ncharacteristics and the presentation of interdependencies in a network topology.\nTwo FRM indices are presented, namely, FRM@Americas and FRM@Europe. The FRM\nindices detect systemic risk at selected areas and identifies risk factors. In\npractice, FRM is applied to the return time series of selected financial\ninstitutions and macroeconomic risk factors. Using FRM on a daily basis, we\nidentify companies exhibiting extreme \"co-stress\", as well as \"activators\" of\nstress. With the SRM@EuroArea, we extend to the government bond asset class. FRM\nis a good predictor for recession probabilities, constituting the FRM-implied\nrecession probabilities. Thereby, FRM indicates tail event behaviour in a\nnetwork of financial risk factors.\n\n", "summary": ":\nSystemic Risk, Quantile Regression, Financial Markets, Risk Management, Network\nDynamics, Recession\n\n"}, {"description": ":\nThe importance of startups for a dynamic, innovative and competitive economy has\nalready been acknowledged in the scientific and business literature. The highly\nuncertain and volatile nature of the startup ecosystem makes the evaluation of\nstartup success through analysis and interpretation of information very time\nconsuming and computationally intensive. This prediction problem brings forward\nthe need for a quantitative model, which should enable an objective and fact-\nbased approach to startup success prediction. This paper presents a series of\nreproducible models for startup success prediction, using machine learning\nmethods. The data used for this purpose was received from the online investor\nplatform, crunchbase.com. The data has been pre-processed for sampling bias and\nimbalance by using the oversampling approach, ADASYN. A total of six different\nmodels are implemented to predict startup success. Using goodness-of-fit\nmeasures, applicable to each model case, the best models selected are the\nensemble methods, random forest and extreme gradient boosting with a test set\nprediction accuracy of 94.1% and 94.5% and AUC of 92.22% and 92.91%\nrespectively. Top variables in these models are last funding to date, first\nfunding lag and company age. The models presented in this study can be used to\npredict success rate for future new firms/ventures in a repeatable way.\n\n", "summary": ":\nMachine learning\n\n"}, {"description": ":\nThe paper examines the potential of deep learning to produce decision support\nmodels from structured, tabular data. Considering the context of financial risk\nmanagement, we develop a deep learning model for predicting whether individual\nspread traders are likely to secure profits from future trades. This embodies\ntypical modeling challenges faced in risk and behavior forecasting. Conventional\nmachine learning requires data that is representative of the feature-target\nrelationship and relies on the often costly development, maintenance, and\nrevision of handcrafted features. Consequently, modeling highly variable,\nheterogeneous patterns such as the behavior of traders is challenging. Deep\nlearning promises a remedy. Learning hierarchical distributed representations of\nthe raw data in an automatic manner (e.g. risk taking behavior), it uncovers\ngenerative features that determine the target (e.g., trader\u2019s profitability),\navoids manual feature engineering, and is more robust toward change (e.g.\ndynamic market conditions). The results of employing a deep network for\noperational risk forecasting confirm the feature learning capability of deep\nlearning, provide guidance on designing a suitable network architecture and\ndemonstrate the superiority of deep learning over machine learning and rule-\nbased benchmarks.\n\n", "summary": ":\nrisk management, retail finance, forecasting, deep learning\n\n"}, {"description": ":\nThe integration of social media characteristics into an econometric framework\nrequires modeling a high dimensional dynamic network with dimensions of\nparameter \u0398 typically much larger than the number of observations. To cope with\nthis problem, we introduce a new structural model \u2014 SONIC which assumes that\n(1) a few influencers drive the network dynamics; (2) the community structure of\nthe network is characterized as the homogeneity of response to the specific\ninfluencer, implying their underlying similarity. An estimation procedure is\nproposed based on a greedy algorithm and LASSO regularization. Through\ntheoretical study and simulations, we show that the matrix parameter can be\nestimated even when the observed time interval is smaller than the size of the\nnetwork. Using a novel dataset retrieved from a leading social media platform\u2013\nStockTwits and quantifying their opinions via natural language processing, we\nmodel the opinions network dynamics among a select group of users and further\ndetect the latent communities. With a sparsity regularization, we can identify\nimportant nodes in the network.\n\n", "summary": ":\nsocial media, network, community, opinion mining, natural language processing\n\n"}, {"description": ":\nCustomer scoring models are the core of scalable direct marketing. Uplift models\nprovide an estimate of the incremental benefit from a treatment that is used for\noperational decision-making. Training and monitoring of uplift models require\nexperimental data. However, the collection of data under randomized treatment\nassignment is costly, since random targeting deviates from an established\ntargeting policy. To increase the cost-efficiency of experimentation and\nfacilitate frequent data collection and model training, we introduce supervised\nrandomization. It is a novel approach that integrates existing scoring models\ninto randomized trials to target relevant customers, while ensuring consistent\nestimates of treatment effects through correction for active sample selection.\nAn empirical Monte Carlo study shows that data collection under supervised\nrandomization is cost-efficient, while downstream uplift models perform\ncompetitively.\n\n", "summary": ":\nUplift Modeling, Causal Inference, Experimental Design, Selection Bias\n\n"}, {"description": ":\nPublic interest, explosive returns, and diversification opportunities gave\nstimulus to the adoption of traditional financial tools to crypto-currencies.\nWhile the CRIX index offered the first scientifically-backed proxy to the\ncrypto- market (analogous to S&P 500), the introduction of Bitcoin futures by\nCboe became the milestone in the creation of the derivatives market for crypto-\ncurrencies. Following the intuition of the \"fear index\" VIX for the American\nstock market, the VCRIX volatility index was created to capture the investor\nexpectations about the crypto-currency ecosystem. VCRIX is built based on CRIX\nand offers a forecast for the mean annualized volatility of the next 30 days,\nre-estimated daily. The model was back-tested for its forecasting power,\nresulting in low MSE performance and further examined by the simulation of VIX\n(resulting in a correlation of 78% between the actual VIX and VIX estimated with\nthe VCRIX model). VCRIX provides forecasting functionality and serves as a proxy\nfor the investors\u2019 expectations in the absence of the de- veloped derivatives\nmarket. These features provide enhanced decision making capacities for market\nmonitoring, trading strategies, and potentially option pricing.\n\n", "summary": ":\nindex construction, volatility, crypto-currency, VCRIX\n\n"}, {"description": ":\nThe paper proposes an estimator to make inference of heterogeneous treatment effects sorted by impact groups (GATES) for non-randomised experiments. Observational studies are standard in policy evaluation from labour markets, educational surveys and other empirical studies. To control for a potential selection-bias we implement a doubly-robust estimator in the first stage. Keeping the flexibility, we can use any machine learning method to learn the conditional mean functions as well as the propensity score. We also use machine learning methods to learn a function for the conditional average treatment effect. The group average treatment effect, is then estimated via a parametric linear model to provide p-values and confidence intervals. To control for confounding in the linear model we use Neyman-orthogonal moments to partial out the effect that covariates have on both, the treatment assignment and the outcome. The result is a best linear predictor for effect heterogeneity based on impact groups. We introduce inclusion-probability weighting as a form of cross-splitting and averaging for each observation to avoid biases through sample splitting. The advantage of the proposed method is a robust linear estimation of heterogeneous group treatment effects in observational studies.\n\n", "summary": ": causal inference, machine learning, simulation study, confidence intervals, multiple splitting, sorted group ATE (GATES), doubly-robust estimator\n\n"}, {"description": ":\nThe shift of human communication to online platforms brings many benefits to\nsociety due to the ease of publication of opinions, sharing experience, getting\nimmediate feedback and the opportunity to discuss the hottest topics. Besides\nthat, it builds up a space for antisocial behavior such as harassment, insult\nand hate speech. This research is dedicated to detection of antisocial online\nbehavior detection (AOB) - an umbrella term for cyberbullying, hate speech,\ncyberaggression and use of any hateful textual content. First, we provide a\nbenchmark of deep learning models found in the literature on AOB detection. Deep\nlearning has already proved to be efficient in different types of decision\nsupport: decision support from financial disclosures, predicting process\nbehavior, text-based emoticon recognition. We compare methods of traditional\nmachine learning with deep learning, while applying important advancements of\nnatural language processing: we examine bidirectional encoding, compare\nattention mechanisms with simpler reduction techniques, and investigate whether\nthe hierarchical representation of the data and application of attention on\ndifferent layers might improve the predictive performance. As a partial\ncontribution of the final hierarchical part, we introduce pseudo-sentence\nhierarchical attention network, an extension of hierarchical attention network \u2013\na recent advancement in document classification.\n\n", "summary": ":\nDeep Learning, Cyberbullying, Antisocial Online Behavior, Attention Mechanism,\nText Classification\n\n"}, {"description": ":\nThe predictability of a high-dimensional time series model in forecasting with\nlarge information sets depends not only on the stability of parameters but also\ndepends heavily on the active covariates in the model. Since the true empirical\nenvironment can change as time goes by, the variables that function well at the\npresent may become useless in the future. Combined with the instable parameters,\nfinding the most active covariates in the parameter time-varying situations\nbecomes difficult. In this paper, we aim to propose a new method, the Penalized\nAdaptive Method (PAM), which can adaptively detect the parameter homogeneous\nintervals and simultaneously select the active variables in sparse models. The\nnewly developed method is able to identify the parameters stability at one hand\nand meanwhile, at the other hand, can manage of selecting the active forecasting\ncovariates at every different time point. Comparing with the classical models,\nthe method can be applied to high-dimensional cases with different sources of\nparameter changes while it steadily reduces the forecast error in high-\ndimensional data. In the out-of-sample bond risk premia forecasting, the\nPenalized Adaptive Method can reduce the forecasting error(RMSPE and MAPE)\naround 24% to 50% comparing with the other forecasting methods.\n\n", "summary": ":\nSCAD penalty, propagation-separation, adaptive window choice, multiplier\nbootstrap, bond risk premia\n\n"}, {"description": ":\nThe paper estimates banks\u2019 total factor efficiency (TFE) as well as TFE of each\nproduction factor by incorporating banks\u2019 overall risk endogenously into bank\u2019s\nproduction process as undesirable by-product in a Global-SMB Model. Our results\nshow that, compared with a model incorporated with banks\u2019 overall risk, a model\nconsidering only on-balance-sheet risk may over-estimate the integrated TFE\n(TFIE) and under-estimate TFE volatility. Significant heterogeneities of bank\nTFIE and TFE of each production factor exist among banks of different types and\nregions, as a result of still prominent unbalanced development of Chinese\ncommercial banks. Based on the estimated TFIE, the paper further investigates\nthe determinants of bank efficiency, and finds that shadow banking, bank size,\nNPL ratio, loan to deposit ratio, fiscal surplus to GDP ratio and banking sector\nconcentration are significant determinants of bank efficiency. Besides, a model\nwith risk-weighted assets as undesirable outputs can better capture the impact\nof shadow banking involvement.\n\n", "summary": ":\nNonparametric Methods, Commercial Banks, Shadow Bank, Financial Risk\n\n"}, {"description": ":\nRecently, a number of structured funds have emerged as public-private\npartnerships with the intent of promoting investment in renewable energy in\nemerging markets. These funds seek to attract institutional investors by\ntranching the asset pool and issuing senior notes with a high credit quality.\nFinancing of renewable energy (RE) projects is achieved via two channels: small\nRE projects are financed indirectly through local banks that draw loans from the\nfund\u2019s assets, whereas large RE projects are directly financed from the fund. In\na bottom-up Gaussian copula framework, we examine the diversification properties\nand RE exposure of the senior tranche. To this end, we introduce the LH++ model,\nwhich combines a homogeneous infinitely granular loan portfolio with a finite\nnumber of large loans. Using expected tranche percentage notional (which takes a\nsimilar role as the default probability of a loan), tranche prices and tranche\nsensitivities in RE loans, we analyse the risk profile of the senior tranche. We\nshow how the mix of indirect and direct RE investments in the asset pool affects\nthe sensitivity of the senior tranche to RE investments and how to balance a\ndesired sensitivity with a target credit quality and target tranche size.\n\n", "summary": ":\nRenewable energy financing, structured finance, CDO pricing, LH++ model\n\n"}, {"description": ":\nA multivariate quantile regression model with a factor structure is proposed to\nstudy data with many responses of interest. The factor structure is allowed to\nvary with the quantile levels, which makes our framework more flexible than the\nclassical factor models. The model is estimated with the nuclear norm\nregularization in order to accommodate the high dimensionality of data, but the\nincurred optimization problem can only be efficiently solved in an approximate\nmanner by off-the-shelf optimization methods. Such a scenario is often seen when\nthe empirical risk is non-smooth or the numerical procedure involves expensive\nsubroutines such as singular value decompo- sition. To ensure that the\napproximate estimator accurately estimates the model, non-asymptotic bounds on\nerror of the the approximate estimator is established. For implementation, a\nnumerical procedure that provably marginalizes the approximate error is\nproposed. The merits of our model and the proposed numerical procedures are\ndemonstrated through Monte Carlo experiments and an application to finance\ninvolving a large pool of asset returns.\n\n", "summary": ":\nFactor model, quantile regression, non-asymptotic analysis, multivariate\nregression, nuclear norm regularization\n\n"}, {"description": ":\nThis study provides a formal analysis of the customer targeting decision problem\nin settings where the cost for marketing action is stochastic and proposes a\nframework to efficiently estimate the decision variables for campaign profit\noptimization. Targeting a customer is profitable if the positive impact of the\nmarketing treatment on the customer and the associated profit to the company is\nhigher than the cost of the treatment. While there is a growing literature on\ndeveloping causal or uplift models to identify the customers who are impacted\nmost strongly by the marketing action, no research has investigated optimal\ntargeting when the costs of the action are uncertain at the time of the\ntargeting decision. Because marketing incentives are routinely conditioned on a\npositive response by the customer, e.g. a purchase or contract renewal,\nstochastic costs are ubiquitous in direct marketing and customer retention\ncampaigns. This study makes two contributions to the literature, which are\nevaluated on a coupon targeting campaign in an e-commerce setting. First, the\nauthors formally analyze the targeting decision problem under response-dependent\ncosts. Profit-optimal targeting requires an estimate of the treatment effect on\nthe customer and an estimate of the customer response probability under\ntreatment. The empirical results demonstrate that the consideration of treatment\ncost substantially increases campaign profit when used for customer targeting in\ncombination with the estimation of the average or customer- level treatment\neffect. Second, the authors propose a framework to jointly estimate the\ntreatment effect and the response probability combining methods for causal\ninference with a hurdle mixture model. The proposed causal hurdle model achieves\ncompetitive campaign profit while streamlining model building. The code for the\nempirical analysis is available on Github.\n\n", "summary": ":\nHeterogeneous Treatment Effect, Uplift Modeling, Coupon Targeting,\nChurn/Retention, Campaign Profit\n\n"}, {"description": ":\nDeep learning has substantially advanced the state of the art in computer\nvision, natural language processing, and other fields. The paper examines the\npotential of deep learning for exchange rate forecasting. We systematically\ncompare long short- term memory networks and gated recurrent units to\ntraditional recurrent network architectures as well as feedforward networks in\nterms of their directional forecasting accuracy and the profitability of trading\nmodel predictions. Empirical results indicate the suitability of deep networks\nfor exchange rate forecasting in general but also evidence the difficulty of\nimplementing and tuning corresponding architectures. Especially with regard to\ntrading profit, a simpler neural network may perform as well as if not better\nthan a more complex deep neural network.\n\n", "summary": ":\nDeep learning, Financial time series forecasting, Recurrent neural networks,\nForeign exchange rates\n\n"}, {"description": ":\nFinancial statement fraud is an area of significant consternation for potential\ninvestors, auditing companies, and state regulators. Intelligent systems\nfacilitate detecting financial statement fraud and assist the decision-making of\nrelevant stakeholders. Previous research detected instances in which financial\nstatements have been fraudulently misrepresented in managerial comments. The\npaper aims to investigate whether it is possible to develop an enhanced system\nfor detecting financial fraud through the combination of information sourced\nfrom financial ratios and managerial comments within corporate annual reports.\nWe employ a hierarchical attention network (HAN) with a long short-term memory\n(LSTM) encoder to extract text features from the Management Discussion and\nAnalysis (MD&A) section of annual reports. The model is designed to offer two\ndistinct features. First, it reflects the structured hierarchy of documents,\nwhich previous models were unable to capture. Second, the model embodies two\ndifferent attention mechanisms at the word and sentence level, which allows\ncontent to be differentiated in terms of its importance in the process of\nconstructing the document representation. As a result of its architecture, the\nmodel captures both content and context of managerial comments, which serve as\nsupplementary predictors to financial ratios in the detection of fraudulent\nreporting. Additionally, the model provides interpretable indicators denoted as\n\u201cred-flag\u201d sentences, which assist stakeholders in their process of determining\nwhether further investigation of a specific annual report is required. Empirical\nresults demonstrate that textual features of MD&A sections extracted by HAN\nyield promising classification results and substantially reinforce financial\nratios.\n\n", "summary": ":\nfraud detection, financial statements, deep learning, text analytics\n\n"}, {"description": ":\nThe cryptocurrency market is unique on many levels: Very volatile, frequently\nchanging market structure, emerging and vanishing of cryptocurrencies on a daily\nlevel. Following its development became a difficult task with the success of\ncryptocurrencies (CCs) other than Bitcoin. For fiat currency markets, the IMF\noffers the index SDR and, prior to the EUR, the ECU existed, which was an index\nrepresenting the development of European currencies. Index providers decide on a\nfixed number of index constituents which will represent the market segment. It\nis a challenge to fix a number and develop rules for the constituents in view of\nthe market changes. In the frequently changing CC market, this challenge is even\nmore severe. A method relying on the AIC is proposed to quickly react to market\nchanges and therefore enable us to create an index, referred to as CRIX, for the\ncryptocurrency market. CRIX is chosen by model selection such that it represents\nthe market well to enable each interested party studying economic questions in\nthis market and to invest into the market. The diversified nature of the CC\nmarket makes the inclusion of altcoins in the index product critical to improve\ntracking performance. We have shown that assigning optimal weights to altcoins\nhelps to reduce the tracking errors of a CC portfolio, despite the fact that\ntheir market cap is much smaller relative to Bitcoin. The codes used here are\navailable via www.quantlet.de.\n\n", "summary": ":\nIndex construction, Model selection, Bitcoin, Cryptocurrency, CRIX, Altcoin\n\n"}, {"description": ":\nAmong nonparametric smoothers, there is a well-known correspondence between\nkernel and Fourier series methods, pivoted by the Fourier transform of the\nkernel. This suggests a similar relationship between kernel and spline\nestimators. A known special case is the result of Silverman (1984) on the\neffective kernel for the classical Reinsch-Schoenberg smoothing spline in the\nnonparametric regression model. We present an extension by showing that a large\nclass of kernel estimators have a spline equivalent, in the sense of identical\nasymptotic local behaviour of the weighting coefficients. This general class of\nspline smoothers includes also the minimax linear estimator over Sobolev\nellipsoids. The analysis is carried out for piecewise linear splines and\nequidistant design.\n\n", "summary": ":\nKernel estimator, spline smoothing, filtering coefficients, differential\noperator, Green's function approximation, asymptotic minimax spline\n\n"}, {"description": ":\nMany countries have taken non-pharmaceutical interventions (NPIs) to contain the\nspread of the coronavirus (COVID-19) and push the recovery of national\neconomies. This paper investigates the effect of these control measures by\ncomparing five selected countries, China, Italy, Germany, the United Kingdom,\nand the United States. There is evidence that the degree of early intervention\nand efficacy of control measures are essential to contain the pandemic. China\nstands out because its early and strictly enforced interventions are effective\nto contain the virus spread. Furthermore, we quantify the causal effect of\ndifferent control measures on COVID-19 transmission and work resumption in\nChina. Surprisingly, digital contact tracing and delegating clear responsibility\nto the local community appear to be the two most effective policy measures for\ndisease containment and work resumption. Public information campaigns and social\ndistancing also help to flatten the peak significantly. Moreover, material\nlogistics that prevent medical supply shortages provide an additional\nconditioning factor for disease containment and work resumption. Fiscal policy,\nhowever, is less effective at the early to middle stage of the pandemic.\n\n", "summary": ":\nCOVID-19, coronavirus\n\n"}, {"description": ":\nWe investigate the finite sample performance of sample splitting, cross-fitting\nand averaging for the estimation of the conditional average treatment effect.\nRecently proposed methods, so-called meta- learners, make use of machine\nlearning to estimate different nuisance functions and hence allow for fewer\nrestrictions on the underlying structure of the data. To limit a potential\noverfitting bias that may result when using machine learning methods, cross-\nfitting estimators have been proposed. This includes the splitting of the data\nin different folds to reduce bias and averaging over folds to restore\nefficiency. To the best of our knowledge, it is not yet clear how exactly the\ndata should be split and averaged. We employ a Monte Carlo study with different\ndata generation processes and consider twelve different estimators that vary in\nsample-splitting, cross-fitting and averaging procedures. We investigate the\nperformance of each estimator independently on four different meta-learners: the\ndoubly-robust-learner, R-learner, T-learner and X-learner. We find that the\nperformance of all meta-learners heavily depends on the procedure of splitting\nand averaging. The best performance in terms of mean squared error (MSE) among\nthe sample split estimators can be achieved when applying cross-fitting plus\ntaking the median over multiple different sample-splitting iterations. Some\nmeta-learners exhibit a high variance when the lasso is included in the ML\nmethods. Excluding the lasso decreases the variance and leads to robust and at\nleast competitive results.\n\n", "summary": ":\ncausal inference, sample splitting, cross-fitting, sample averaging, machine\nlearning, simulation study\n\n"}, {"description": ":\nTail risk protection is in the focus of the financial industry and requires\nsolid mathematical and statistical tools, especially when a trading strategy is\nderived. Recent hype driven by machine learning (ML) mechanisms has raised the\nnecessity to display and understand the functionality of ML tools. In this\npaper, we present a dynamic tail risk protection strategy that targets a maximum\npredefined level of risk measured by Value-At-Risk while controlling for\nparticipation in bull market regimes. We propose different weak classifiers,\nparametric and non-parametric, that estimate the exceedance probability of the\nrisk level from which we derive trading signals in order to hedge tail events.\nWe then compare the different approaches both with statistical and trading\nstrategy performance, finally we propose an ensemble classifier that produces a\nmeta tail risk protection strategy improving both generalization and trading\nperformance.\n\n", "summary": ":\n-\n\n"}, {"description": ":\nThis paper proposes a dynamic spatial autoregressive quantile model. Using\npredetermined network information, we study dynamic tail event driven risk using\na system of conditional quantile equations. Extending Zhu, Wang, Wang and H\u00e4rdle\n(2019), we allow the contemporaneous dependency of nodal responses by\nincorporating a spatial lag in our model. For example, this is to allow a firm\u2019s\ntail behavior to be connected with a weighted aggregation of the simultaneous\nreturns of the other firms. In addition, we control for the common factor\neffects. The instrumental variable quantile regressive method is used for our\nmodel estimation, and the associated asymptotic theory for estimation is also\nprovided. Simulation results show that our model performs well at various\nquantile levels with different network structures, especially when the node size\nincreases. Finally, we illustrate our method with an empirical study. We uncover\nsignificant network effects in the spatial lag among financial institutions.\n\n", "summary": ":\nNetwork, Quantile autoregression, Instrumental variables, Dynamic models\n\n"}, {"description": ":\nInflation expectation (IE) is often considered to be an important determinant of\nactual inflation in modern economic theory, we are interested in investigating\nthe main risk factors that determine its dynamics. We fiirst apply a joint\narbitrage-free term structure model across different European countries to\nobtain estimate for country-specific IE. Then we use the two-component and\nthree-component models to capture the main risk factors. We discover that the\nextracted common trend for IE is an important driver for each country of\ninterest. Moreover a spatial-temporal copula model is tted to account for the\nnon-Gaussian dependency across countries. This paper aims to extract informative\nestimates for IE and provide good implications for monetary policies.\n\n", "summary": ":\nin ation expectation; joint yield-curve modeling; factor model; common trend;\nspatial-temporal copulas\n\n"}, {"description": ":\nA factor augmented dynamic model for analysing tail behaviour of high\ndimensional time series is proposed. As a first step, the tail event driven\nlatent factors are extracted. In the second step, a VAR (Vectorautoregression\nmodel) is carried out to analyse the interaction between these factors and the\nmacroeconomic variables. Furthermore, this methodology also provides the\npossibility for central banks to examine the sensitivity between macroeconomic\nvariables and financial shocks via impulse response analysis. Then the\npredictability of our estimator is illustrated. Finally, forecast error variance\ndecomposition is carried out to investigate the network effect of these\nvariables. The interesting findings are: firstly, GDP and Unemployment rate are\nvery much sensitive to the shock of financial tail event driven factors, while\nthese factors are more affected by inflation and short term interest rate.\nSecondly, financial tail event driven factors play important roles in the\nnetwork constructed by the extracted factors and the macroeconomic variables.\nThirdly, there is more connectedness during financial crisis than in the stable\nperiods. Compared with median case, the network is more dense in lower quantile\nlevel.\n\n", "summary": ":\nQuantile Regression, Expectile Regression, Dynamic Factor Model, Dynamic Network\n\n"}, {"description": ":\nWe propose a bivariate component GARCH-MIDAS model to estimate the long- and\nshort-run components of the variances and covariances. The advantage of our\nmodel to the existing DCC-based models is that it uses the same form for both\nthe variances and covariances and that it estimates these moments\nsimultaneously. We apply this model to obtain long- and short-run factor betas\nfor industry test portfolios, where the risk factors are the market, SMB, and\nHML portfolios. We use these betas in cross-sectional analysis of the risk\npremia. Among other things, we find that the risk premium related to the short-\nrun market beta is significantly positive, irrespective of the choice of test\nportfolio. Further, the risk premia for the short-run betas of all the risk\nfactors are significant outside recessions.\n\n", "summary": ":\nlong-run betas; short-run betas; risk premia; business cycles; component GARCH\nmodel; MIDAS\n\n"}, {"description": ":\nFor multiple change-points detection of high-dimensional time series, we provide\nasymptotic theory concerning the consistency and the asymptotic distribution of\nthe breakpoint statistics and estimated break sizes. The theory backs up a\nsimple two- step procedure for detecting and estimating multiple change-points.\nThe proposed two-step procedure involves the maximum of a MOSUM (moving sum)\ntype statistics in the rst step and a CUSUM (cumulative sum) re nement step on\nan aggregated time series in the second step. Thus, for a xed time-point, we\ncan capture both the biggest break across di erent coordinates and aggregating\nsimultaneous breaks over multiple coordinates. Extending the existing high-\ndimensional Gaussian approxima- tion theorem to dependent data with jumps, the\ntheory allows us to characterize the size and power of our multiple change-point\ntest asymptotically. Moreover, we can make inferences on the breakpoints\nestimates when the break sizes are small. Our theoretical setup incorporates\nboth weak temporal and strong or weak cross-sectional dependence and is suitable\nfor heavy-tailed innovations. A robust long-run covariance matrix estimation is\nproposed, which can be of independent interest. An application on detecting\nstructural changes of the U.S. unemployment rate is considered to illus- trate\nthe usefulness of our method.\n\n", "summary": ":\nmultiple change points detection; temporal and cross-sectional dependence;\nGaussian approximation; inference of break locations\n\n"}, {"description": ":\nWe develop a uniform test for detecting and dating explosive behavior of a\nstrictly stationary GARCH(r, s) (generalized autoregressive conditional\nheteroskedasticity) process. Namely, we test the null hypothesis of a globally\nstable GARCH process with constant parameters against an alternative where there\nis an \u2019abnormal\u2019 period with changed parameter values. During this period, the\nchange may lead to an explosive behavior of the volatility process. It is\nassumed that both the magnitude and the timing of the breaks are unknown. We\ndevelop a double supreme test for the existence of a break, and then provide an\nalgorithm to identify the period of change. Our theoretical results hold under\nmild moment assumptions on the innovations of the GARCH process. Technically,\nthe existing properties for the QMLE in the GARCH model need to be\nreinvestigated to hold uniformly over all possible periods of change. The key\nresults involve a uniform weak Bahadur representation for the estimated\nparameters, which leads to weak convergence of the test statistic to the supreme\nof a Gaussian Process. In simulations we show that the test has good size and\npower for reasonably large time series lengths. We apply the test to Apple asset\nreturns and Bitcoin returns.\n\n", "summary": ":\nGARCH, IGARCH, Change-point Analysis, Concentration Inequalities, Uniform Test\n\n"}, {"description": ":\nIn this paper, we study estimation of nonlinear models with cross sectional data\nusing two-step generalized estimating equations (GEE) in the quasi-maximum\nlikelihood estimation (QMLE) framework. In the interest of improving efficiency,\nwe propose a grouping estimator to account for the potential spatial correlation\nin the underlying innovations. We use a Poisson model and a Negative Binomial II\nmodel for count data and a Probit model for binary response data to demonstrate\nthe GEE procedure. Under mild weak dependency assumptions, results on estimation\nconsistency and asymptotic normality are provided. Monte Carlo simulations show\nefficiency gain of our approach in comparison of different estimation methods\nfor count data and binary response data. Finally we apply the GEE approach to\nstudy the determinants of the inflow foreign direct investment (FDI) to China.\n\n", "summary": ":\nquasi-maximum likelihood estimation; generalized estimating equations; nonlinear\nmodels; spatial dependence; count data; binary response data; FDI equation\n\n"}, {"description": ":\nCryptocurrencies are gaining momentum in investor attention, are about to become\na new asset class, and may provide a hedging alternative against the risk of\ndevaluation of fiat currencies following the COVID-19 crisis. In order to\nprovide a thorough understanding of this new asset class, risk indicators need\nto consider tail risk behaviour and the interdependencies between the\ncryptocurrencies not only for risk management but also for portfolio\noptimization. The tail risk network analysis framework proposed in the paper is\nable to identify individual risk characteristics and capture spillover effect in\na network topology. Finally we construct tail event sensitive portfolios and\nconsequently test the performance during an unforeseen COVID-19 pandemic.\n\n", "summary": ":\nCryptocurrencies, Network Dynamics, Portfolio Optimization, Quantile Regression,\nSystemic Risk, Financial Risk Meter\n\n"}, {"description": ": Surrogate models using a suitable orthogonal decomposition and radial basis functions have been proposed by many researchers to reduce the computational complexity of numerical solutions to optimization problems. However, these reduced-order models result in low accuracy, sometimes due to inappropriate initial sampling or the occurrence of optima at vertices. This paper provides an improved intelligent data-driven mechanism for constructing low-dimensional surrogate models using alternative memory-based sampling strategies in an iterative algorithm. Furthermore, the application of surrogate models to optimal control problems is extended.\nIt is shown that surrogate models with Latin hypercube sampling dominate variable-order methods in optimization computation time while maintaining accuracy. They are also shown to be robust to nonlinearities in the model. Therefore, these computationally efficient predictive surrogate models are applicable in various fields, especially for solving inverse problems and optimal control problems, some examples of which are shown in this paper. ", "summary": ": Proper Orthogonal Decomposition, SVD, Radial Basis Functions, Optimization, Surrogate Models, Smart Data Analytics, Parameter Estimation "}, {"description": ":\nSmart Contracts are commonly considered to be an important component or even a\nkey to many business solutions in an immense variety of sectors and promises to\nsecurely increase their individual efficiency in an ever more digitized\nenvironment. Introduced in the early 1990\u2019s, the technology has gained a lot of\nattention with its application to blockchain technology to an extent, that can\nbe considered a veritable hype. Reflecting the growing institutional interest,\nthis intertwined exploratory study between statistics, information technology,\nand law contrasts these idealistic stories with the data reality and provides a\nmandatory step of understanding the matter, before any further relevant\napplications are discussed as being \u201cfactually\u201d able to replace traditional\nconstructions. Besides fundamental flaws and application difficulties of\ncurrently employed Smart Contracts, the technological drive and enthusiasm\nbacking it may however serve as a jump-off board for future developments\nthrusting well in the presently unshakeable traditional structures.\n\n", "summary": ":\nCryptocurrency, Smart Contract, Ethereum, CRIX\n\n"}, {"description": ":\nFor treatment effects - one of the core issues in modern econometric analysis -\nprediction and estimation are flip-sides of the same coin. As it turns out,\nmachine learning methods are the tool for generalized prediction models.\nCombined with econometric theory allows us to estimate not only the average but\na personalized treatment effect - the conditional average treatment effect\n(CATE). In this tutorial, we give an overview of novel methods, explain them in\ndetail, and apply them via Quantlets in real data applications. We study the\neffect that microcredit availability has on the amount of money borrowed and if\nthe 401(k) pension plan eligibility has an impact on net financial assets, as\ntwo empirical examples. The presented toolbox of methods contains meta-\nlearners, like the Doubly-Robust, the R-, T- and X-learner, and methods that are\nspecially designed to estimate the CATE like the causal BART and the generalized\nrandom forest. In both, the microcredit and the 401(k) example, we find a\npositive treatment effect for all observations but diverse evidence of treatment\neffect heterogeneity. An additional simulation study, where the true treatment\neffect is known, allows us to compare the different methods and to observe\npatterns and similarities.\n\n", "summary": ":\nCausal Inference, CATE, Machine Learning, Tutorial\n\n"}, {"description": ":\nCryptocurrencies come with benefits, such as anonymity of payments and positive\nnetwork effects of user adoption, and transaction risks including unconfirmed\ntransactions, hacks, and frauds. They compete with central-bank-regulated money\nbut consumers may prefer one currency over the other. In our arbitrage-free\nworld utility from consumption depends on benefits, which are governed by\ndistinct stochastic processes, implying incomplete markets and distinct pricing\nkernels. We characterize the cryptocurrency kernels, evaluate the otherwise\nunobservable benefits, and show their contribution to pricing. The model\nexplains both the co-existence of the two currencies and the high volatility of\nthe cryptocurrency price.\n\n", "summary": ":\nBitcoin, cryptocurrency, pricing kernel, currency competition\n\n"}, {"description": ":\nThis paper sheds light on the dynamics of the cryptocurrency (CC) sector. By\nmodeling its dynamics via a stochastic volatility with correlated jumps (SVCJ)\nmodel in combination with several rolling windows, it is possible to capture the\nextreme ups and downs of the CC market and to understand its dynamics. Through\nthis approach, we obtain time series for each parameter of the model. Even\nthough parameter estimates change over time and depend on the window size,\nseveral recurring patterns are observable which are robust to changes of the\nwindow size and supported by clustering of parameter estimates: during bullish\nperiods, volatility stabilizes at low levels and the size and volatility of\njumps in mean decreases. In bearish periods though, volatility increases and\ntakes longer to return to its long-run trend. Furthermore, jumps in mean and\njumps in volatility are independent. With the rise of the CC market in 2017, a\nlevel shift of the volatility of volatility occurred.\n\n", "summary": ":\nCryptocurrency, SVCJ, Market Dynamics, Stochastic Volatility\n\n"}, {"description": ":\nDer Mathematiker und Statistiker E. J. Gumbel f\u00fchrte eine Doppelexistenz \u2013 als\nMathematiker und Statistiker von 1923 bis zu seiner Vertreibung 1932 an der\nUniversit\u00e4t Heidelberg und als politischer Autor. Auch im Exil in Frankreich\nbehielt er diese Doppelt\u00e4tigkeit bei, verfasste mathematische Arbeiten und\npublizierte Artikel gegen das NS-Regime in Exil-Zeitschriften. Sein Hauptwerk\n\u201eStatistics of Extremes\u201c erschien 1958 in New York (eine Reprint-Ausgabe 2013).\nDie \u201eWiederentdeckung\u201c des \u201epolitischen Gumbel\u201c begann 2012 und fast zeitgleich\ndie \u201eWiederentdeckung\u201c des \u201emathematischen Gumbel\u201c. Die Anwendungen der \u201eGumbel\nDistribution\u201c und der Gumbel-Copula zur Modellierung stochastischer\nAbh\u00e4ngigkeiten weckten das Interesse an der Person Gumbel und seinen Leistungen.\nIm Artikel werden neue Forschungsergebnisse zu E. J. Gumbel vorgestellt.\n\n", "summary": ":\nEmil J. Gumbel \u2013 Mathematiker, Pazifist und politischer Autor\n\n"}, {"description": ":\nThe rapid development of artificial intelligence methods contributes to their\nwide applications for forecasting various financial risks in recent years. This\nstudy introduces a novel explainable case-based reasoning (CBR) approach without\na requirement of rich expertise in financial risk. Compared with other black-box\nalgorithms, the explainable CBR system allows a natural economic interpretation\nof results. Indeed, the empirical results emphasize the interpretability of the\nCBR system in predicting financial risk, which is essential for both financial\ncompanies and their customers. In addition, results show that the proposed\nautomatic design CBR system has a good prediction performance compared to other\nartificial intelligence methods, overcoming the main drawback of a standard CBR\nsystem of highly depending on prior domain knowledge about the corresponding\nfield.\n\n", "summary": ":\nCase-based reasoning, Financial risk detection, Multiple-criteria decision-\nmaking, Feature scoring, Particle swarm optimization, Parallel computing\n\n"}, {"description": ":\nWe develop a general approach for stress testing correlations of financial asset\nportfolios. The correlation matrix of asset returns is specified in a parametric\nform, where correlations are represented as a function of risk factors, such as\ncountry and industry factors. A sparse factor structure linking assets and risk\nfactors is built using Bayesian variable selection methods. Regular calibration\nyields a joint distribution of economically meaningful stress scenarios of the\nfactors. As such, the method also lends itself as a reverse stress testing\nframework: using the Mahalanobis distance or highest density regions (HDR) on\nthe joint risk factor distribution allows to infer worst-case correlation\nscenarios. We give examples of stress tests on a large portfolio of European and\nNorth American stocks.\n\n", "summary": ":\nCorrelation stress testing, reverse stress testing, factor selection, scenario\nselection, Bayesian variable selection, market risk management\n\n"}, {"description": ":\nThe proportional subdistribution hazards (PSH) model is popularly used to deal\nwith competing risks data. Censored quantile regression provides an important\nsupplement as well as variable selection methods, due to large numbers of\nirrelevant covariates in practice. In this paper, we study variable selection\nprocedures based on penalized weighted quantile regression for competing risks\nmodels, which is conveniently applied by researchers. Asymptotic properties of\nthe proposed estimators including consistency and asymptotic normality of non-\npenalized estimator and consistency of variable selection are established. Monte\nCarlo simulation studies are conducted, showing that the proposed methods are\nconsiderably stable and efficient. A real data about bone marrow transplant\n(BMT) is also analyzed to illustrate the application of proposed procedure.\n\n", "summary": ":\nCompeting risks, Cumulative incidence function, Kaplan-Meier estimator,\nRedistribution method\n\n"}, {"description": ":\nSeveral cryptocurrency (CC) indices track the dynamics of the rising CC sector,\nand soon ETFs will be issued on them. We conduct a qualitative and quantitative\nevaluation of the currently existing CC indices. As the CC sector is not yet\nconsolidated, index issuers face the challenge of tracking the dynamics of a\nfast-growing sector that is under continuous transformation. We propose several\ncriteria and various measures to compare the indices under review. Major\ndifferences between the indices lie in their weighting schemes, their coverage\nof CCs and the number of constituents, the level of transparency, and thus their\naccuracy in mapping the dynamics of the CC sector. Our analysis reveals that\nindices that adapt dynamically to this rising sector outperform their\ncompetitors. Interestingly, increasing the number of constituents does not\nautomatically lead to a better fit of the CC sector. All codes are available on\nQuantlet.com\n\n", "summary": ":\nCryptocurrency, Index, Market Dynamics, Bitcoin\n\n"}, {"description": ":\nThe risk transmission among financial markets is time-evolving, especially for\nthe extreme risk scenarios. The possibly sudden time variations of these risk\nstructures ask for quantitative technology that is able to cope with such\nsituations. Here we present a novel localized multivariate CAViaR-type model to\nrespond to the challenge of time-varying risk contagion. For this purpose a\nlocal adaptive approach determines homogeneous intervals at each time point.\nCritical values for this technique are calculated via multiplier bootstrap, and\nthe statistical properties of this \u201dlocalized multivariate CAViaR\u201d are derived.\nA comprehensive simulation study supports the effectiveness of our approach in\ndetecting structural change in multivariate CAViaR. Finally, when applying for\nthe US and German financial markets, we can trace out the dynamic tail risk\nspillovers and find that the US market appears to play dominate role in risk\ntransmissions, especially in volatile market periods.\n\n", "summary": ":\nconditional quantile autoregression, local parametric approach, change point\ndetection, multiplier bootstrap\n\n"}, {"description": ":\nThis paper provides statistical learning techniques for determining the full\nown-price market impact and the relevance and effect of cross-price and cross-\nasset spillover channels from intraday transactions data. The novel tools allow\nextracting comprehensive information contained in the limit order books (LOB)\nand quantify their impacts on the size and structure of price interdependencies\nacross stocks. For correct empirical network determination of such dynamic\nliquidity price e\u2000ects even in small portfolios, we require high-dimensional\nstatistical learning methods with an integrated general bootstrap procedure. We\ndocument the importance of LOB liquidity network spillovers even for a small\nblue-chip NASDAQ portfolio.\n\n", "summary": ":\nlimit order book, high-dimensional statistical learning, liquidity networks,\nhigh frequency dynamics, market impact, bootstrap, network\n\n"}, {"description": ":\nCryptocurrencies return cross-predictability and technological similarity yield\ninformation on risk propagation and market segmentation. To investigate these\neffects, we build a time-varying network for cryptocurrencies, based on the\nevolution of return cross-predictability and technological similarities. We\ndevelop a dynamic covariate-assisted spectral clustering method to consistently\nestimate the latent community structure of cryptocurrencies network that\naccounts for both sets of information. We demonstrate that investors can achieve\nbetter risk diversification by investing in cryptocurrencies from different\ncommunities. A cross-sectional portfolio that implements an inter-crypto\nmomentum trading strategy earns a 1.08% daily return. By dissecting the\nportfolio returns on behavioral factors, we con\u2000rm that our results are not\ndriven by behavioral mechanisms.\n\n", "summary": ":\nCommunity detection, Dynamic stochastic blockmodel, Covariates, Co-clustering,\nNetwork risk, Momentum\n\n"}, {"description": ":\nPenalized spline smoothing of time series and its asymptotic properties are\nstudied. A data-driven algorithm for selecting the smoothing parameter is\ndeveloped. The proposal is applied to define a semiparametric extension of the\nwell-known Spline- GARCH, called a P-Spline-GARCH, based on the log-data\ntransformation of the squared returns. It is shown that now the errors process\nis exponentially strong mixing with finite moments of all orders. Asymptotic\nnormality of the P-spline smoother in this context is proved. Practical\nrelevance of the proposal is illustrated by data examples and simulation. The\nproposal is further applied to value at risk and expected shortfall.\n\n", "summary": ":\nP-spline smoother, smoothing parameter selection, P-Spline-GARCH, strong mixing,\nvalue at risk, expected shortfall\n\n"}, {"description": ":\nThis study seeks to evaluate the effect of green financial development,\nimproving energy efficiency and economic growth on Covid-19 tenure. For this,\nthe CPEC area is recommended to look into. Present study revealed the energy\neconomic negative repercussions of Covid-19 impacts. It is assumed that, in\nChina and Pakistan, economic expansion, trade openness, financial development,\nand urbanization coexist. To verify the postulated impacts of economic activity\non the environment, we do Johansen cointegration, error correction, and Granger\ncausality tests. We discovered that economic growth, energy consumption, trade\nopenness, financial development, and urbanization had a long-term relationship\nto CO2 emissions in Pakistan. Urbanization is the only macroeconomic factor with\na detrimental effect on carbon emissions. As with China, no cointegration is\nfound across variables, but unidirectional causality from energy consumption and\neconomic growth to economic growth is established. Economic growth, energy\nconsumption, and trade openness also each have bidirectional causal effect on\nfinancial development. According to statistical data, along with significant\nprojected economic development in CPEC countries, policymakers and regulators\nare urged to strengthen environmental protection laws in China and Pakistan.\n\n", "summary": ":\nGreen financial development, Energy Financing, Energy Efficiency, Economic\ngrowth, Covid-19 crises, Capital formation\n\n"}, {"description": ":\nShort Term Load Forecast (STLF) is necessary for effective scheduling, operation\noptimization trading, and decision-making for electricity consumers. Modern and\nefficient machine learning methods are recalled nowadays to manage complicated\nstructural big datasets, which are characterized by having a nonlinear temporal\ndependence structure. We propose different statistical nonlinear models to\nmanage these challenges of hard type datasets and forecast 15-min frequency\nelectricity load up to 2-days ahead. We show that the Long-short Term Memory\n(LSTM) and the Gated Recurrent Unit (GRU) models applied to the production line\nof a chemical production facility outperform several other predictive models in\nterms of out-of-sample forecasting accuracy by the Diebold-Mariano (DM) test\nwith several metrics. The predictive information is fundamental for the risk\nand production management of electricity consumers.\n\n", "summary": ":\nShort Term Load Forecast, Deep Neural Network, Hard Structure Load Process\n\n"}, {"description": ":\nThis paper develops a new risk meter specifically for China \u2013 FRM@China \u2013 to\ndetect systemic financial risk as well as tail-event (TE) dependencies among\nmajor financial institutions (FIs). Compared with the CBOE FIX VIX, which is\ncurrently the most popular financial risk measure, FRM@China has less noise. It\nalso emitted a risk signature much earlier than the CBOE FIX VIX index in the\n2020 COVID pandemic. In addition, FRM@China uses a single quantile-lasso\nregression model to allow both the assessment of risk transfer between different\nsectors in which FIs operate and the prediction of systemic risk. Because the\nrisk indicator in FRM@China is based on penalization terms, its relationship\nwith macro variables are unknown and non-linear. This paper further expands the\nexisting FRM approach by using Shapley values to identify the dynamic\ncontribution of different macro features in this type of \"black box\" situation.\nThe results show that short-term interest rates and forward guidance are\nsignificant risk drivers. This paper considers the interaction among FIs from\nmainland China, Hong Kong and Taiwan to provide an enhanced regional tool set\nfor regulators to evaluate financial policy responses. All quantlets are\navailable on quantlet.com.\n\n", "summary": ":\nFRM (Financial Risk Meter), Lasso Quantile Regression, Financial Network, China,\nShapley value\n\n"}, {"description": ":\nThe introduction of derivatives on Bitcoin enables investors to hedge risk\nexposures in cryptocurrencies. Because of volatility swings and jumps in\ncryptocurrency prices, the traditional variance-based approach to obtain hedge\nratios is infeasible. As a consequence, we consider two extensions of the\ntraditional approach: first, different dependence structures are modelled by\ndifferent copulae, such as the Gaussian, Student-t, Normal Inverse Gaussian and\nArchimedean copulae; second, different risk measures, such as value-at-risk,\nexpected shortfall and spectral risk measures are employed to and the optimal\nhedge ratio. Extensive out-of-sample tests give insights in the practice of\nhedging various cryptos and crypto indices, including Bitcoin, Ethereum,\nCardano, the CRIX index and a number of crypto-portfolios in the time period\nDecember 2017 until May 2021. Evidences show that BTC futures can\u2002effectively\nhedge BTC and BTC-involved indices. This promising result is consistent across\ndifferent risk measures and copulae except for Frank. On the other hand, we\nobserve complex and diverse dependence structures between BTC-not-involved\nassets and the futures. As a consequence, results of hedging other assets and\nindices are diverse and, in some occasions, not ideal.\n\n", "summary": ":\nCryptocurrencies, risk management, hedging, copulas\n\n"}]}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jd7S7ZIIS6Fn"
   },
   "source": [
    "## Preprocess Quantlet Data for the *Data Structure Type* experiment (4)\n",
    "\n",
    "Objective: preprocess Quantlet data for *Data Structure Type* experiment (4), for preprocessing of Experiments (1-3) see *Quantlets_Preprocessing.ipynb*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TNkVUfU21UsR",
    "tags": []
   },
   "outputs": [],
   "source": [
    "##### IMPORT DEPENDENCIES\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "import os\n",
    "\n",
    "os.chdir(\"./\")\n",
    "sys.path.append('../src')\n",
    "\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import importlib\n",
    "import preprocessing_utils\n",
    "\n",
    "importlib.reload(preprocessing_utils)\n",
    "from preprocessing_utils import *\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from Levenshtein import distance\n",
    "\n",
    "\n",
    "##### CONSTANTS AND PATHS\n",
    "QPATH = \"3-data-preprocessing\"\n",
    "DATE = \"20231119_normal\"\n",
    "RS = 111"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46Fkzpboq6sy"
   },
   "source": [
    "### 2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### PARSE AND POSTPROCESS META + CODE SNIPPETS\n",
    "\n",
    "with open(\n",
    "    f\"../../data/preprocessed/Quantlet/{DATE}/Quantlets_{DATE}.pkl\", \"rb\"\n",
    ") as file:\n",
    "    df = pickle.load(file)\n",
    "\n",
    "df = df_metainfo_parse(df=df,\n",
    "                    prepare_script=True,\n",
    "                    remove_other=True,\n",
    "                    remove_empty=False)\n",
    "\n",
    "df = clean_up(df)\n",
    "print(df.shape)\n",
    "\n",
    "df['script_name_no_ext'] = df.script_name.str.split('.', expand=True)[0]\n",
    "df['main_script'] = df['script_name_no_ext']==df['Quantlet']\n",
    "df = df.loc[df['main_script']==True, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADDITIONAL PREPROCESSING OF DESCRIPTIONS\n",
    "\n",
    "# remove parentheses\n",
    "df.Description = df.Description.str.replace(r\"\\(.+?\\)\", \"\", regex=True)\n",
    "\n",
    "# remove URL\n",
    "df.Description = df.Description.str.replace(\n",
    "r\"\"\"(?i)\\b((?:https?:(?:/{1,3}|[a-z0-9%])|[a-z0-9.\\-]+[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)/)(?:[^\\s()<>{}\\[\\]]+|\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\))+(?:\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’])|(?:(?<!@)[a-z0-9]+(?:[.\\-][a-z0-9]+)*[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)\\b/?(?!@)))\"\"\",\n",
    "\"\",\n",
    "regex=True)\n",
    "\n",
    "# ADDITIONAL PREPROCESSING OF CODE\n",
    "df.code_script = df.code_script.str.replace(r\"#\", \"\", regex=True)\n",
    "df.loc[df.type_script == \"m\", \"code_script\"] = df.loc[df.type_script == \"m\", \"code_script\"].str.replace(r\"\\%\", \" \", regex=True)\n",
    "\n",
    "df.loc[df.type_script == \"r\", \"code_script\"] = df.loc[df.type_script == \"r\", \"code_script\"].str.replace(r\"\\$\", \" \", regex=True)\n",
    "\n",
    "# remove the same sign repeated more than 4 times\n",
    "df.code_script = df.code_script.str.replace(r\"(.)\\1{4,}\", r\"\\1\", regex=True)\n",
    "\n",
    "df['Description_ID'] = df.groupby('Description').ngroup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENT PERTURBATIONS FUNCTIONS\n",
    "\n",
    "def shuffle_tokens(code_snippet):\n",
    "    cs_list = code_snippet.split()\n",
    "    shuffled_list = random.sample(cs_list, len(cs_list)) \n",
    "    return ' '.join(shuffled_list)\n",
    "\n",
    "def remove_parentheses(code_snippet):\n",
    "    regex = r'\\w+'\n",
    "    return ' '.join(re.findall(regex, code_snippet))\n",
    "\n",
    "def remove_non_text(code_snippet):\n",
    "    regex = r'\\w+'\n",
    "    cs_list = re.findall(regex, code_snippet)\n",
    "    cs_list = [word for word in cs_list if (len(word)>1) and not word.isdigit()]\n",
    "    return ' '.join(cs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE THE ORDER OF TOKENS\n",
    "# Choose and uncomment one by one\n",
    "\n",
    "# Random Order\n",
    "# df.code_script = df.code_script.apply(shuffle_tokens)\n",
    "\n",
    "# Operands removal\n",
    "# df.code_script = df.code_script.apply(remove_parentheses)\n",
    "\n",
    "# Remove nontext\n",
    "# df.code_script = df.code_script.apply(remove_non_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLIT THE DATA GROUP QUANTLET\n",
    "labelled_descr_id, test_descr_id = train_test_split(list(df.Description_ID.unique()),\n",
    "                test_size=0.1,\n",
    "                random_state=RS)\n",
    "train_descr_id, val_descr_id = train_test_split(labelled_descr_id,\n",
    "                test_size=0.1,\n",
    "                random_state=RS)\n",
    "\n",
    "full_train = df.loc[df.Description_ID.isin(labelled_descr_id)]\n",
    "train = df.loc[df.Description_ID.isin(train_descr_id)]\n",
    "val = df.loc[df.Description_ID.isin(val_descr_id)]\n",
    "test = df.loc[df.Description_ID.isin(test_descr_id)]\n",
    "\n",
    "save_datasets(full_train, train, val, test, \"20231119_normal\", RS, 'code_script', False, True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### CREATE BOOOTSTRAPED SAMPLES FOR EXPERIMENTS AND STAT TESTS\n",
    "\n",
    "SIZE = test.shape[0]\n",
    "indices = range(SIZE)\n",
    "N_SAMPLES = 35\n",
    "\n",
    "for sample in tqdm(range(1, N_SAMPLES)):\n",
    "    np.random.seed(sample)\n",
    "    sample_idx = np.random.choice(indices, size=SIZE, replace=True)\n",
    "    sample_df = test.iloc[sample_idx, : ].reset_index(drop=True)\n",
    "    sample_df.to_csv(f'../../data/preprocessed/Quantlet/20231119_normal/test_df_sample_{sample}.csv', index=False)\n",
    "\n",
    "    group_test = sample_df\n",
    "    test_dataset_json = {'version' : sample,\n",
    "                        'data' : [{'input_sequence'  : group_test['code_script'].iloc[i],\n",
    "                                'output_sequence'  : group_test['Description'].iloc[i]} for i in range(group_test.shape[0])]}\n",
    "    with open(f'../../data/preprocessed/Quantlet/20231119_normal/test_dataset_sample_{sample}.json', 'w') as f:\n",
    "        json.dump(test_dataset_json, f)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN1WASBG9SoZypHtCRaCZX/",
   "mount_file_id": "19c37YEU8LH5C0d1bxiNpE23xesKn01ad",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

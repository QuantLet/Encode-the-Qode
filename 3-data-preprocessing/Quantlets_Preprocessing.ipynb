{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jd7S7ZIIS6Fn"
   },
   "source": [
    "## Preprocess Quantlet Data \n",
    "Objective: preprocess Quantlet data for all Experiments (1-3) apart from *Data Structure Type* experiment (4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xz1ksIsd1SYO",
    "tags": []
   },
   "outputs": [],
   "source": [
    "##### IMPORT DEPENDENCIES\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import importlib\n",
    "import preprocessing_utils\n",
    "\n",
    "importlib.reload(preprocessing_utils)\n",
    "from preprocessing_utils import *\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "##### CONSTANTS AND PATHS\n",
    "QPATH = \"Quantlet/3-data-preprocessing\"\n",
    "DATE = \"20231104\"\n",
    "RS = 111\n",
    "folder_to_save = f\"../../data/preprocessed/Quantlet/{DATE}/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46Fkzpboq6sy"
   },
   "source": [
    "### 2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### PARSE AND POSTPROCESS META + CODE SNIPPETS\n",
    "\n",
    "with open(\n",
    "    f\"../../data/preprocessed/Quantlet/{DATE}/Quantlets_{DATE}.pkl\", \"rb\"\n",
    ") as file:\n",
    "    df = pickle.load(file)\n",
    "\n",
    "df = df_metainfo_parse(df=df,\n",
    "                    prepare_script=True,\n",
    "                    remove_other=True,\n",
    "                    remove_empty=False)\n",
    "\n",
    "df = clean_up(df)\n",
    "print(df.shape)\n",
    "\n",
    "df['script_name_no_ext'] = df.script_name.str.split('.', expand=True)[0]\n",
    "df['main_script'] = df['script_name_no_ext']==df['Quantlet']\n",
    "df = df.loc[df['main_script']==True, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ADDITIONAL PREPROCESSING OF DESCRIPTIONS\n",
    "\n",
    "# remove parentheses\n",
    "df.Description = df.Description.str.replace(r\"\\(.+?\\)\", \"\", regex=True)\n",
    "\n",
    "# remove URL\n",
    "df.Description = df.Description.str.replace(\n",
    "r\"\"\"(?i)\\b((?:https?:(?:/{1,3}|[a-z0-9%])|[a-z0-9.\\-]+[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)/)(?:[^\\s()<>{}\\[\\]]+|\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\))+(?:\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’])|(?:(?<!@)[a-z0-9]+(?:[.\\-][a-z0-9]+)*[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)\\b/?(?!@)))\"\"\",\n",
    "\"\",\n",
    "regex=True)\n",
    "\n",
    "# ADDITIONAL PREPROCESSING OF CODE\n",
    "df.code_script = df.code_script.str.replace(r\"#\", \"\", regex=True)\n",
    "df.loc[df.type_script == \"m\", \"code_script\"] = df.loc[df.type_script == \"m\", \"code_script\"].str.replace(r\"\\%\", \" \", regex=True)\n",
    "df.loc[df.type_script == \"r\", \"code_script\"] = df.loc[df.type_script == \"r\", \"code_script\"].str.replace(r\"\\$\", \" \", regex=True)\n",
    "\n",
    "# remove the same sign repeated more than 4 times\n",
    "df.code_script = df.code_script.str.replace(r\"(.)\\1{4,}\", r\"\\1\", regex=True)\n",
    "\n",
    "# Create IDs\n",
    "df['Description_ID'] = df.groupby('Description').ngroup()\n",
    "\n",
    "df[\"Q_ID\"] = df.index\n",
    "if not os.path.exists(folder_to_save):\n",
    "    os.mkdir(folder_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLIT THE DATA GROUP QUANTLET\n",
    "labelled_descr_id, test_descr_id = train_test_split(list(df.Description_ID.unique()),\n",
    "                test_size=0.1,\n",
    "                random_state=RS)\n",
    "train_descr_id, val_descr_id = train_test_split(labelled_descr_id,\n",
    "                test_size=0.1,\n",
    "                random_state=RS)\n",
    "\n",
    "full_train = df.loc[df.Description_ID.isin(labelled_descr_id)]\n",
    "train = df.loc[df.Description_ID.isin(train_descr_id)]\n",
    "val = df.loc[df.Description_ID.isin(val_descr_id)]\n",
    "test = df.loc[df.Description_ID.isin(test_descr_id)]\n",
    "\n",
    "c_len_t = train.Description.apply(lambda x: len(x.split()))\n",
    "c_len_t.describe()\n",
    "\n",
    "##### SAVE THE MAIN ANALYSIS SPLITS\n",
    "save_datasets(full_train, train, val, test, DATE, RS, 'code_script', False, True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### CREATE BOOOTSTRAPED SAMPLES FOR EXPERIMENTS AND STAT TESTS\n",
    "\n",
    "SIZE = test.shape[0]\n",
    "indices = range(SIZE)\n",
    "N_SAMPLES = 35\n",
    "\n",
    "for sample in tqdm(range(1, N_SAMPLES)):\n",
    "    np.random.seed(sample)\n",
    "    sample_idx = np.random.choice(indices, size=SIZE, replace=True)\n",
    "    sample_df = test.iloc[sample_idx, : ].reset_index(drop=True)\n",
    "    sample_df.to_csv(f'../../data/preprocessed/Quantlet/{DATE}/test_df_sample_{sample}.csv', index=False)\n",
    "\n",
    "    # PROGRAMMING LANGUAGE\n",
    "    for type_script in sample_df.type_script.unique():\n",
    "      group_test = sample_df.loc[sample_df.type_script == type_script, : ]\n",
    "      test_dataset_json = {'version' : type_script,\n",
    "                          'data' : [{'input_sequence'  : group_test['code_script'].iloc[i],\n",
    "                                    'output_sequence'  : group_test['Description'].iloc[i]} for i in range(group_test.shape[0])]}\n",
    "      with open(f'../../data/preprocessed/Quantlet/{DATE}/test_dataset_{type_script}_sample_{sample}.json', 'w') as f:\n",
    "        json.dump(test_dataset_json, f)\n",
    "\n",
    "\n",
    "# create bootstrap\n",
    "SIZE = val.shape[0]\n",
    "indices = range(SIZE)\n",
    "N_SAMPLES = 35\n",
    "\n",
    "for sample in tqdm(range(1, N_SAMPLES)):\n",
    "    np.random.seed(sample)\n",
    "    sample_idx = np.random.choice(indices, size=SIZE, replace=True)\n",
    "    sample_df = val.iloc[sample_idx, : ].reset_index(drop=True)\n",
    "    sample_df.to_csv(f'../../data/preprocessed/Quantlet/{DATE}/val_df_sample_{sample}.csv', index=False)\n",
    "\n",
    "    # PROGRAMMING LANGUAGE\n",
    "    for type_script in sample_df.type_script.unique():\n",
    "      group_val = sample_df.loc[sample_df.type_script == type_script, : ]\n",
    "      val_dataset_json = {'version' : type_script,\n",
    "                          'data' : [{'input_sequence'  : group_val['code_script'].iloc[i],\n",
    "                                    'output_sequence'  : group_val['Description'].iloc[i]} for i in range(group_val.shape[0])]}\n",
    "      with open(f'../../data/preprocessed/Quantlet/{DATE}/val_dataset_{type_script}_sample_{sample}.json', 'w') as f:\n",
    "        json.dump(val_dataset_json, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### CREATE FEW SHOT LEARNING SAMPLES RANDOM\n",
    "few_shot = train.groupby('type_script').sample(n=35, random_state=RS)\n",
    "few_shot_random_ids = few_shot.Description_ID\n",
    "not_few_shot_random_ids = set(train.Description_ID.values).difference(set(few_shot.Description_ID.values))\n",
    "\n",
    "few_shot_train_df = train.copy(deep=True)\n",
    "few_shot_train_df = few_shot_train_df.loc[~few_shot_train_df.Description_ID.isin(not_few_shot_random_ids)]\n",
    "print(few_shot_train_df.shape)\n",
    "\n",
    "few_shot_full_train_df = full_train.copy(deep=True)\n",
    "few_shot_full_train_df = few_shot_full_train_df.loc[~few_shot_full_train_df.Description_ID.isin(not_few_shot_random_ids)]\n",
    "print(few_shot_full_train_df.shape)\n",
    "\n",
    "fs_train_dataset_json = {'version' : '0',\n",
    "                        'data' : [{'input_sequence'  : few_shot_train_df['code_script'].iloc[i],\n",
    "                                    'output_sequence'  : few_shot_train_df['Description'].iloc[i]} for i in range(few_shot_train_df.shape[0])]}\n",
    "with open(f'../../data/preprocessed/Quantlet/{DATE}/fs_train_dataset_sample_0.json', 'w') as f:\n",
    "    json.dump(fs_train_dataset_json, f)\n",
    "\n",
    "fs_full_train_dataset_json = {'version' : '0',\n",
    "                        'data' : [{'input_sequence'  : few_shot_full_train_df['code_script'].iloc[i],\n",
    "                                    'output_sequence'  : few_shot_full_train_df['Description'].iloc[i]} for i in range(few_shot_full_train_df.shape[0])]}\n",
    "with open(f'../../data/preprocessed/Quantlet/{DATE}/fs_full_train_dataset_sample_0.json', 'w') as f:\n",
    "    json.dump(fs_full_train_dataset_json, f)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN1WASBG9SoZypHtCRaCZX/",
   "mount_file_id": "19c37YEU8LH5C0d1bxiNpE23xesKn01ad",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
